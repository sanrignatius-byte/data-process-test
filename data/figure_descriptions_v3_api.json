[
  {
    "doc_id": "1104.3913",
    "figure_id": "1104.3913_fig_1",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg",
    "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Two shapes S and T divided by horizontal dashed line\", \"S₀ and S₁ labels on left shape\", \"T₀ and T₁ labels on right elongated shape\", \"G₀ and G₁ labels on right side\", \"Horizontal dashed line separating upper and lower regions\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the first constraint require x to have S(x)-T(x) outgoing flow units from the regions shown?\",\n      \"answer\": \"The constraint ensures that flow f accounts for the difference between S and T values at each point x, represented by the spatial separation in the diagram where S and T occupy different regions divided by the horizontal line.\",\n      \"visual_anchor\": \"S₀, S₁ labels on left shape and T₀, T₁ labels on right shape separated by dashed line\",\n      \"text_evidence\": \"The first constraint requires that x has at least S(x) - T(x) outgoing units of flow in f.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the over-representation of S members in G₀ shown above the dashed line affect treatment differences?\",\n      \"answer\": \"Members of S being over-represented in G₀ and under-represented in G₁ causes the average treatment of S members to be very different from the average treatment of T members.\",\n      \"visual_anchor\": \"G₀ label above dashed line and G₁ label below dashed line\",\n      \"text_evidence\": \"members of S , on average, may therefore be very different from the treatment, on average, of members of T , since members of S are over-represented in G₀ and under-represented in G₁\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What concrete example does the spatial partition between S₀ and S₁ by the dashed line illustrate?\",\n      \"answer\": \"The partition illustrates a cardinal question being examined, showing how the intersection of groups G₀ and G₁ with sets S and T creates distinct subregions (S₀, S₁, T₀, T₁) that demonstrate spatial or categorical separation.\",\n      \"visual_anchor\": \"Horizontal dashed line separating S₀ above from S₁ below\",\n      \"text_evidence\": \"This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Two shapes S and T divided by horizontal dashed line",
        "S₀ and S₁ labels on left shape",
        "T₀ and T₁ labels on right elongated shape",
        "G₀ and G₁ labels on right side",
        "Horizontal dashed line separating upper and lower regions"
      ],
      "queries": [
        {
          "query": "Why does the first constraint require x to have S(x)-T(x) outgoing flow units from the regions shown?",
          "answer": "The constraint ensures that flow f accounts for the difference between S and T values at each point x, represented by the spatial separation in the diagram where S and T occupy different regions divided by the horizontal line.",
          "visual_anchor": "S₀, S₁ labels on left shape and T₀, T₁ labels on right shape separated by dashed line",
          "text_evidence": "The first constraint requires that x has at least S(x) - T(x) outgoing units of flow in f.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the over-representation of S members in G₀ shown above the dashed line affect treatment differences?",
          "answer": "Members of S being over-represented in G₀ and under-represented in G₁ causes the average treatment of S members to be very different from the average treatment of T members.",
          "visual_anchor": "G₀ label above dashed line and G₁ label below dashed line",
          "text_evidence": "members of S , on average, may therefore be very different from the treatment, on average, of members of T , since members of S are over-represented in G₀ and under-represented in G₁",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What concrete example does the spatial partition between S₀ and S₁ by the dashed line illustrate?",
          "answer": "The partition illustrates a cardinal question being examined, showing how the intersection of groups G₀ and G₁ with sets S and T creates distinct subregions (S₀, S₁, T₀, T₁) that demonstrate spatial or categorical separation.",
          "visual_anchor": "Horizontal dashed line separating S₀ above from S₁ below",
          "text_evidence": "This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the first constraint require x to have S(x)-T(x) outgoing flow units from the regions shown?",
        "answer": "The constraint ensures that flow f accounts for the difference between S and T values at each point x, represented by the spatial separation in the diagram where S and T occupy different regions divided by the horizontal line.",
        "visual_anchor": "S₀, S₁ labels on left shape and T₀, T₁ labels on right shape separated by dashed line",
        "text_evidence": "The first constraint requires that x has at least S(x) - T(x) outgoing units of flow in f.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the over-representation of S members in G₀ shown above the dashed line affect treatment differences?",
        "answer": "Members of S being over-represented in G₀ and under-represented in G₁ causes the average treatment of S members to be very different from the average treatment of T members.",
        "visual_anchor": "G₀ label above dashed line and G₁ label below dashed line",
        "text_evidence": "members of S , on average, may therefore be very different from the treatment, on average, of members of T , since members of S are over-represented in G₀ and under-represented in G₁",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What concrete example does the spatial partition between S₀ and S₁ by the dashed line illustrate?",
        "answer": "The partition illustrates a cardinal question being examined, showing how the intersection of groups G₀ and G₁ with sets S and T creates distinct subregions (S₀, S₁, T₀, T₁) that demonstrate spatial or categorical separation.",
        "visual_anchor": "Horizontal dashed line separating S₀ above from S₁ below",
        "text_evidence": "This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Two shapes S and T divided by horizontal dashed line",
      "S₀ and S₁ labels on left shape",
      "T₀ and T₁ labels on right elongated shape",
      "G₀ and G₁ labels on right side",
      "Horizontal dashed line separating upper and lower regions"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg",
    "caption": "Figure 1: Tag cloud of top terms from each dataset.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Large centered word 'syria' in black\",\n    \"Prominent term 'egitto3000' in black\",\n    \"Word 'revolution' in lower right\",\n    \"Arabic script mixed with English terms\",\n    \"Terms 'damascus', 'homs', 'aleppo' scattered around\",\n    \"Words 'bahrain', 'egypt', 'jordan', 'kuwait' visible\",\n    \"Term 'arab' and 'league' present\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does 'syria' dominate the tag cloud if researchers collected data using a boundary box with strong Asian bias?\",\n      \"answer\": \"The Asian boundary box captured Middle Eastern tweets, where Syria-related content was highly active during this period. The prominence reflects regional social media activity patterns within the geographic collection parameters.\",\n      \"visual_anchor\": \"Large centered word 'syria' dominating the tag cloud\",\n      \"text_evidence\": \"We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the presence of 'revolution' alongside city names like 'damascus' and 'homs' reflect the topic modeling applications researchers pursue with Twitter's Streaming API?\",\n      \"answer\": \"Yes, the clustering of political terms with geographic locations demonstrates typical topic modeling patterns. Researchers use the Streaming API specifically for such topic modeling to understand content themes and user behavior.\",\n      \"visual_anchor\": \"Word 'revolution' appearing near 'damascus', 'homs', and 'aleppo'\",\n      \"text_evidence\": \"Twitter's Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the mix of Arabic and English terms like 'bashar' and 'arab' illustrate the Streaming API's preset parameter matching?\",\n      \"answer\": \"The bilingual content shows the API captured tweets matching geographic or keyword parameters related to Middle Eastern events. The preset parameters allowed collection of both Arabic and English tweets discussing the same regional topics.\",\n      \"visual_anchor\": \"Mixed Arabic script and English terms including 'bashar', 'arab', and Arabic characters\",\n      \"text_evidence\": \"It provides a glance into its millions of users and billions of tweets through a 'Streaming API' which provides a sample of all tweets matching some parameters preset by the API user.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Large centered word 'syria' in black",
        "Prominent term 'egitto3000' in black",
        "Word 'revolution' in lower right",
        "Arabic script mixed with English terms",
        "Terms 'damascus', 'homs', 'aleppo' scattered around",
        "Words 'bahrain', 'egypt', 'jordan', 'kuwait' visible",
        "Term 'arab' and 'league' present"
      ],
      "queries": [
        {
          "query": "Why does 'syria' dominate the tag cloud if researchers collected data using a boundary box with strong Asian bias?",
          "answer": "The Asian boundary box captured Middle Eastern tweets, where Syria-related content was highly active during this period. The prominence reflects regional social media activity patterns within the geographic collection parameters.",
          "visual_anchor": "Large centered word 'syria' dominating the tag cloud",
          "text_evidence": "We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the presence of 'revolution' alongside city names like 'damascus' and 'homs' reflect the topic modeling applications researchers pursue with Twitter's Streaming API?",
          "answer": "Yes, the clustering of political terms with geographic locations demonstrates typical topic modeling patterns. Researchers use the Streaming API specifically for such topic modeling to understand content themes and user behavior.",
          "visual_anchor": "Word 'revolution' appearing near 'damascus', 'homs', and 'aleppo'",
          "text_evidence": "Twitter's Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the mix of Arabic and English terms like 'bashar' and 'arab' illustrate the Streaming API's preset parameter matching?",
          "answer": "The bilingual content shows the API captured tweets matching geographic or keyword parameters related to Middle Eastern events. The preset parameters allowed collection of both Arabic and English tweets discussing the same regional topics.",
          "visual_anchor": "Mixed Arabic script and English terms including 'bashar', 'arab', and Arabic characters",
          "text_evidence": "It provides a glance into its millions of users and billions of tweets through a 'Streaming API' which provides a sample of all tweets matching some parameters preset by the API user.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does 'syria' dominate the tag cloud if researchers collected data using a boundary box with strong Asian bias?",
        "answer": "The Asian boundary box captured Middle Eastern tweets, where Syria-related content was highly active during this period. The prominence reflects regional social media activity patterns within the geographic collection parameters.",
        "visual_anchor": "Large centered word 'syria' dominating the tag cloud",
        "text_evidence": "We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the presence of 'revolution' alongside city names like 'damascus' and 'homs' reflect the topic modeling applications researchers pursue with Twitter's Streaming API?",
        "answer": "Yes, the clustering of political terms with geographic locations demonstrates typical topic modeling patterns. Researchers use the Streaming API specifically for such topic modeling to understand content themes and user behavior.",
        "visual_anchor": "Word 'revolution' appearing near 'damascus', 'homs', and 'aleppo'",
        "text_evidence": "Twitter's Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the mix of Arabic and English terms like 'bashar' and 'arab' illustrate the Streaming API's preset parameter matching?",
        "answer": "The bilingual content shows the API captured tweets matching geographic or keyword parameters related to Middle Eastern events. The preset parameters allowed collection of both Arabic and English tweets discussing the same regional topics.",
        "visual_anchor": "Mixed Arabic script and English terms including 'bashar', 'arab', and Arabic characters",
        "text_evidence": "It provides a glance into its millions of users and billions of tweets through a 'Streaming API' which provides a sample of all tweets matching some parameters preset by the API user.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Large centered word 'syria' in black",
      "Prominent term 'egitto3000' in black",
      "Word 'revolution' in lower right",
      "Arabic script mixed with English terms",
      "Terms 'damascus', 'homs', 'aleppo' scattered around",
      "Words 'bahrain', 'egypt', 'jordan', 'kuwait' visible",
      "Term 'arab' and 'league' present"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
    "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Solid red line labeled Firehose\", \"Dotted blue line labeled Streaming\", \"Peak around 12-27 reaching 90000 tweets for Firehose\", \"Y-axis showing No. Tweets from 0 to 90000\", \"X-axis showing dates from 12-14 to 01-10\", \"Sharp decline in Streaming API around 12-22\", \"Firehose spike to approximately 72000 around 01-10\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dotted Streaming line drop below 10000 tweets after 12-22 despite matching parameters?\",\n      \"answer\": \"The Streaming API collected only 528,592 tweets total compared to 1,280,344 from the Firehose during the same period with exactly the same parameters, indicating significant data loss in the Streaming API after the drop-off point.\",\n      \"visual_anchor\": \"Dotted blue Streaming line dropping from ~43000 to below 10000 tweets around date 12-22\",\n      \"text_evidence\": \"During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the solid red peak reaching 90000 tweets around 12-27 support TweetTracker's collection methodology?\",\n      \"answer\": \"Yes, the Firehose peak demonstrates successful collection of the full dataset. TweetTracker collected from both sources using identical parameters, showing the Firehose captured significantly more data during high-volume periods.\",\n      \"visual_anchor\": \"Solid red Firehose line peaking at approximately 90000 tweets around date 12-27\",\n      \"text_evidence\": \"we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the gap between red and blue lines relate to social media analysis reliability?\",\n      \"answer\": \"The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.\",\n      \"visual_anchor\": \"Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period\",\n      \"text_evidence\": \"It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid red line labeled Firehose",
        "Dotted blue line labeled Streaming",
        "Peak around 12-27 reaching 90000 tweets for Firehose",
        "Y-axis showing No. Tweets from 0 to 90000",
        "X-axis showing dates from 12-14 to 01-10",
        "Sharp decline in Streaming API around 12-22",
        "Firehose spike to approximately 72000 around 01-10"
      ],
      "queries": [
        {
          "query": "Why does the dotted Streaming line drop below 10000 tweets after 12-22 despite matching parameters?",
          "answer": "The Streaming API collected only 528,592 tweets total compared to 1,280,344 from the Firehose during the same period with exactly the same parameters, indicating significant data loss in the Streaming API after the drop-off point.",
          "visual_anchor": "Dotted blue Streaming line dropping from ~43000 to below 10000 tweets around date 12-22",
          "text_evidence": "During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the solid red peak reaching 90000 tweets around 12-27 support TweetTracker's collection methodology?",
          "answer": "Yes, the Firehose peak demonstrates successful collection of the full dataset. TweetTracker collected from both sources using identical parameters, showing the Firehose captured significantly more data during high-volume periods.",
          "visual_anchor": "Solid red Firehose line peaking at approximately 90000 tweets around date 12-27",
          "text_evidence": "we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters.",
          "query_type": "value_context"
        },
        {
          "query": "How does the gap between red and blue lines relate to social media analysis reliability?",
          "answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.",
          "visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period",
          "text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dotted Streaming line drop below 10000 tweets after 12-22 despite matching parameters?",
        "answer": "The Streaming API collected only 528,592 tweets total compared to 1,280,344 from the Firehose during the same period with exactly the same parameters, indicating significant data loss in the Streaming API after the drop-off point.",
        "visual_anchor": "Dotted blue Streaming line dropping from ~43000 to below 10000 tweets around date 12-22",
        "text_evidence": "During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the solid red peak reaching 90000 tweets around 12-27 support TweetTracker's collection methodology?",
        "answer": "Yes, the Firehose peak demonstrates successful collection of the full dataset. TweetTracker collected from both sources using identical parameters, showing the Firehose captured significantly more data during high-volume periods.",
        "visual_anchor": "Solid red Firehose line peaking at approximately 90000 tweets around date 12-27",
        "text_evidence": "we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters.",
        "query_type": "value_context"
      },
      {
        "query": "How does the gap between red and blue lines relate to social media analysis reliability?",
        "answer": "The persistent gap demonstrates that researchers' reliance on the Streaming API is problematic, as it consistently undersamples compared to the Firehose. This affects topic modeling, network analysis, and statistical content analysis accuracy.",
        "visual_anchor": "Vertical gap between solid red Firehose line (higher) and dotted blue Streaming line (lower) throughout the time period",
        "text_evidence": "It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010)",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid red line labeled Firehose",
      "Dotted blue line labeled Streaming",
      "Peak around 12-27 reaching 90000 tweets for Firehose",
      "Y-axis showing No. Tweets from 0 to 90000",
      "X-axis showing dates from 12-14 to 01-10",
      "Sharp decline in Streaming API around 12-22",
      "Firehose spike to approximately 72000 around 01-10"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg",
    "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"box-and-whisker plot\", \"median line around 0.5\", \"upper quartile around 0.75\", \"lower quartile around 0.3\", \"whiskers extending to approximately 0.9 and 0.17\", \"blue box outline\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the median coverage near 0.5 explain why December 29th was selected for analysis?\",\n      \"answer\": \"Yes, December 29th was explicitly chosen because it represents the median coverage level shown in the box plot, making it useful for understanding typical Streaming API coverage characteristics.\",\n      \"visual_anchor\": \"median line at approximately 0.5 in the box plot\",\n      \"text_evidence\": \"we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the box extend from roughly 0.3 to 0.75 when only 528,592 tweets were collected?\",\n      \"answer\": \"The box spanning 0.3 to 0.75 represents the interquartile range of daily coverage proportions. Since 528,592 Streaming tweets were collected versus 1,280,344 Firehose tweets, the coverage varied substantially day-to-day within this range.\",\n      \"visual_anchor\": \"box boundaries at approximately 0.3 (lower quartile) and 0.75 (upper quartile)\",\n      \"text_evidence\": \"During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causes the upper whisker to reach approximately 0.9 despite investigating statistical properties of sampled data?\",\n      \"answer\": \"The upper whisker at 0.9 indicates that on some days, the Streaming API captured nearly 90% of Firehose tweets, representing extreme coverage values. This variation is important for understanding how sampling affects statistical characteristics being investigated.\",\n      \"visual_anchor\": \"upper whisker extending to approximately 0.9\",\n      \"text_evidence\": \"We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "box-and-whisker plot",
        "median line around 0.5",
        "upper quartile around 0.75",
        "lower quartile around 0.3",
        "whiskers extending to approximately 0.9 and 0.17",
        "blue box outline"
      ],
      "queries": [
        {
          "query": "Does the median coverage near 0.5 explain why December 29th was selected for analysis?",
          "answer": "Yes, December 29th was explicitly chosen because it represents the median coverage level shown in the box plot, making it useful for understanding typical Streaming API coverage characteristics.",
          "visual_anchor": "median line at approximately 0.5 in the box plot",
          "text_evidence": "we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)",
          "query_type": "value_context"
        },
        {
          "query": "Why does the box extend from roughly 0.3 to 0.75 when only 528,592 tweets were collected?",
          "answer": "The box spanning 0.3 to 0.75 represents the interquartile range of daily coverage proportions. Since 528,592 Streaming tweets were collected versus 1,280,344 Firehose tweets, the coverage varied substantially day-to-day within this range.",
          "visual_anchor": "box boundaries at approximately 0.3 (lower quartile) and 0.75 (upper quartile)",
          "text_evidence": "During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causes the upper whisker to reach approximately 0.9 despite investigating statistical properties of sampled data?",
          "answer": "The upper whisker at 0.9 indicates that on some days, the Streaming API captured nearly 90% of Firehose tweets, representing extreme coverage values. This variation is important for understanding how sampling affects statistical characteristics being investigated.",
          "visual_anchor": "upper whisker extending to approximately 0.9",
          "text_evidence": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the median coverage near 0.5 explain why December 29th was selected for analysis?",
        "answer": "Yes, December 29th was explicitly chosen because it represents the median coverage level shown in the box plot, making it useful for understanding typical Streaming API coverage characteristics.",
        "visual_anchor": "median line at approximately 0.5 in the box plot",
        "text_evidence": "we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)",
        "query_type": "value_context"
      },
      {
        "query": "Why does the box extend from roughly 0.3 to 0.75 when only 528,592 tweets were collected?",
        "answer": "The box spanning 0.3 to 0.75 represents the interquartile range of daily coverage proportions. Since 528,592 Streaming tweets were collected versus 1,280,344 Firehose tweets, the coverage varied substantially day-to-day within this range.",
        "visual_anchor": "box boundaries at approximately 0.3 (lower quartile) and 0.75 (upper quartile)",
        "text_evidence": "During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causes the upper whisker to reach approximately 0.9 despite investigating statistical properties of sampled data?",
        "answer": "The upper whisker at 0.9 indicates that on some days, the Streaming API captured nearly 90% of Firehose tweets, representing extreme coverage values. This variation is important for understanding how sampling affects statistical characteristics being investigated.",
        "visual_anchor": "upper whisker extending to approximately 0.9",
        "text_evidence": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "box-and-whisker plot",
      "median line around 0.5",
      "upper quartile around 0.75",
      "lower quartile around 0.3",
      "whiskers extending to approximately 0.9 and 0.17",
      "blue box outline"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_5",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg",
    "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid blue line labeled 'Min' starting around 0.3 and rising to ~0.5\",\n    \"Dashed green line labeled 'Q1' starting around 0.4 and rising to ~0.55\",\n    \"Dotted red line labeled 'Median' starting around 0.25 and rising to ~0.55\",\n    \"Dash-dot purple line labeled 'Q3' starting around 0.25 and rising to ~0.7\",\n    \"Solid black line labeled 'Max' starting around 0.8 and stabilizing at ~0.75\",\n    \"X-axis showing n from 0 to 1000\",\n    \"Y-axis showing tau_β from -0.4 to 1.0\",\n    \"Sharp initial fluctuations before n=100 for all curves\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid black Max curve start near 0.8 while investigating how sampled data characteristics match the Firehose?\",\n      \"answer\": \"The Max curve represents the highest rank correlation between top hashtags in Firehose and Streaming data. Starting at 0.8 indicates strong initial concordance in the most correlated ranking, which helps understand how well sampled data characteristics match the Firehose.\",\n      \"visual_anchor\": \"Solid black line labeled 'Max' starting around 0.8 at n near 0\",\n      \"text_evidence\": \"We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the dotted red Median curve stabilizing near 0.55 at n=1000 reflect concordant versus discordant pairs in the rank correlation?\",\n      \"answer\": \"Yes, the Median tau_β value near 0.55 represents the balance between concordant pairs (P_C) and discordant pairs (P_D) in the Kendall's tau calculation. This moderate positive correlation indicates reasonable agreement in hashtag rankings between datasets.\",\n      \"visual_anchor\": \"Dotted red line labeled 'Median' stabilizing around 0.55 at n=1000\",\n      \"text_evidence\": \"where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do all five curves show sharp fluctuations before n=100 when comparing top hashtags at different coverage levels?\",\n      \"answer\": \"The sharp initial fluctuations occur because with small n values, the rank correlation is highly sensitive to individual hashtag ranking differences. As n increases beyond 100, the correlation stabilizes because the statistical properties become more robust across different coverage levels.\",\n      \"visual_anchor\": \"Sharp fluctuations in all curves (Min, Q1, Median, Q3, Max) between n=0 and n=100\",\n      \"text_evidence\": \"We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid blue line labeled 'Min' starting around 0.3 and rising to ~0.5",
        "Dashed green line labeled 'Q1' starting around 0.4 and rising to ~0.55",
        "Dotted red line labeled 'Median' starting around 0.25 and rising to ~0.55",
        "Dash-dot purple line labeled 'Q3' starting around 0.25 and rising to ~0.7",
        "Solid black line labeled 'Max' starting around 0.8 and stabilizing at ~0.75",
        "X-axis showing n from 0 to 1000",
        "Y-axis showing tau_β from -0.4 to 1.0",
        "Sharp initial fluctuations before n=100 for all curves"
      ],
      "queries": [
        {
          "query": "Why does the solid black Max curve start near 0.8 while investigating how sampled data characteristics match the Firehose?",
          "answer": "The Max curve represents the highest rank correlation between top hashtags in Firehose and Streaming data. Starting at 0.8 indicates strong initial concordance in the most correlated ranking, which helps understand how well sampled data characteristics match the Firehose.",
          "visual_anchor": "Solid black line labeled 'Max' starting around 0.8 at n near 0",
          "text_evidence": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.",
          "query_type": "value_context"
        },
        {
          "query": "Does the dotted red Median curve stabilizing near 0.55 at n=1000 reflect concordant versus discordant pairs in the rank correlation?",
          "answer": "Yes, the Median tau_β value near 0.55 represents the balance between concordant pairs (P_C) and discordant pairs (P_D) in the Kendall's tau calculation. This moderate positive correlation indicates reasonable agreement in hashtag rankings between datasets.",
          "visual_anchor": "Dotted red line labeled 'Median' stabilizing around 0.55 at n=1000",
          "text_evidence": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total.",
          "query_type": "value_context"
        },
        {
          "query": "Why do all five curves show sharp fluctuations before n=100 when comparing top hashtags at different coverage levels?",
          "answer": "The sharp initial fluctuations occur because with small n values, the rank correlation is highly sensitive to individual hashtag ranking differences. As n increases beyond 100, the correlation stabilizes because the statistical properties become more robust across different coverage levels.",
          "visual_anchor": "Sharp fluctuations in all curves (Min, Q1, Median, Q3, Max) between n=0 and n=100",
          "text_evidence": "We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid black Max curve start near 0.8 while investigating how sampled data characteristics match the Firehose?",
        "answer": "The Max curve represents the highest rank correlation between top hashtags in Firehose and Streaming data. Starting at 0.8 indicates strong initial concordance in the most correlated ranking, which helps understand how well sampled data characteristics match the Firehose.",
        "visual_anchor": "Solid black line labeled 'Max' starting around 0.8 at n near 0",
        "text_evidence": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose.",
        "query_type": "value_context"
      },
      {
        "query": "Does the dotted red Median curve stabilizing near 0.55 at n=1000 reflect concordant versus discordant pairs in the rank correlation?",
        "answer": "Yes, the Median tau_β value near 0.55 represents the balance between concordant pairs (P_C) and discordant pairs (P_D) in the Kendall's tau calculation. This moderate positive correlation indicates reasonable agreement in hashtag rankings between datasets.",
        "visual_anchor": "Dotted red line labeled 'Median' stabilizing around 0.55 at n=1000",
        "text_evidence": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total.",
        "query_type": "value_context"
      },
      {
        "query": "Why do all five curves show sharp fluctuations before n=100 when comparing top hashtags at different coverage levels?",
        "answer": "The sharp initial fluctuations occur because with small n values, the rank correlation is highly sensitive to individual hashtag ranking differences. As n increases beyond 100, the correlation stabilizes because the statistical properties become more robust across different coverage levels.",
        "visual_anchor": "Sharp fluctuations in all curves (Min, Q1, Median, Q3, Max) between n=0 and n=100",
        "text_evidence": "We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid blue line labeled 'Min' starting around 0.3 and rising to ~0.5",
      "Dashed green line labeled 'Q1' starting around 0.4 and rising to ~0.55",
      "Dotted red line labeled 'Median' starting around 0.25 and rising to ~0.55",
      "Dash-dot purple line labeled 'Q3' starting around 0.25 and rising to ~0.7",
      "Solid black line labeled 'Max' starting around 0.8 and stabilizing at ~0.75",
      "X-axis showing n from 0 to 1000",
      "Y-axis showing tau_β from -0.4 to 1.0",
      "Sharp initial fluctuations before n=100 for all curves"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_6",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg",
    "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"solid blue line (Min)\", \"dashed green line (Q1)\", \"dotted brown line (Median)\", \"dash-dot purple line (Q3)\", \"solid black line (Max)\", \"y-axis labeled tau\", \"x-axis labeled n ranging 0-1000\", \"all lines declining from left to right\", \"blue Min line drops to ~0.6 at n=1000\", \"black Max line stays above 0.9\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid blue Min line drop to approximately 0.6 at n=1000 while computing concordant pairs PC?\",\n      \"answer\": \"The Min line represents the minimum correlation across different coverage levels. As n increases, the minimum tau_β decreases because the correlation coefficient is calculated using concordant pairs (PC) and discordant pairs (PD), showing worst-case correlation degradation at higher hashtag counts.\",\n      \"visual_anchor\": \"solid blue Min line reaching approximately 0.6 at n=1000\",\n      \"text_evidence\": \"where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the black Max line staying above 0.9 throughout align with tau_β ranging from perfect negative to perfect positive correlation?\",\n      \"answer\": \"Yes, the Max line remaining above 0.9 indicates that the best-case correlation stays near perfect positive correlation (value of 1) across all n values, consistent with the tau_β range from -1 to 1.\",\n      \"visual_anchor\": \"solid black Max line maintaining values above 0.9 across all n\",\n      \"text_evidence\": \"The $\\\\tau _ { \\\\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the steeper decline of the blue Min curve compared to purple Q3 relate to understanding n versus correlation?\",\n      \"answer\": \"The steeper Min decline demonstrates that minimum correlation degrades faster with increasing n than the third quartile, revealing the relationship between hashtag count and correlation variability that the analysis aims to understand.\",\n      \"visual_anchor\": \"blue Min line declining more steeply than purple Q3 dash-dot line\",\n      \"text_evidence\": \"To understand the relationship between $n$ and the resulting correlation, $\\\\tau _\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid blue line (Min)",
        "dashed green line (Q1)",
        "dotted brown line (Median)",
        "dash-dot purple line (Q3)",
        "solid black line (Max)",
        "y-axis labeled tau",
        "x-axis labeled n ranging 0-1000",
        "all lines declining from left to right",
        "blue Min line drops to ~0.6 at n=1000",
        "black Max line stays above 0.9"
      ],
      "queries": [
        {
          "query": "Why does the solid blue Min line drop to approximately 0.6 at n=1000 while computing concordant pairs PC?",
          "answer": "The Min line represents the minimum correlation across different coverage levels. As n increases, the minimum tau_β decreases because the correlation coefficient is calculated using concordant pairs (PC) and discordant pairs (PD), showing worst-case correlation degradation at higher hashtag counts.",
          "visual_anchor": "solid blue Min line reaching approximately 0.6 at n=1000",
          "text_evidence": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data",
          "query_type": "value_context"
        },
        {
          "query": "Does the black Max line staying above 0.9 throughout align with tau_β ranging from perfect negative to perfect positive correlation?",
          "answer": "Yes, the Max line remaining above 0.9 indicates that the best-case correlation stays near perfect positive correlation (value of 1) across all n values, consistent with the tau_β range from -1 to 1.",
          "visual_anchor": "solid black Max line maintaining values above 0.9 across all n",
          "text_evidence": "The $\\tau _ { \\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.",
          "query_type": "value_context"
        },
        {
          "query": "How does the steeper decline of the blue Min curve compared to purple Q3 relate to understanding n versus correlation?",
          "answer": "The steeper Min decline demonstrates that minimum correlation degrades faster with increasing n than the third quartile, revealing the relationship between hashtag count and correlation variability that the analysis aims to understand.",
          "visual_anchor": "blue Min line declining more steeply than purple Q3 dash-dot line",
          "text_evidence": "To understand the relationship between $n$ and the resulting correlation, $\\tau _",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid blue Min line drop to approximately 0.6 at n=1000 while computing concordant pairs PC?",
        "answer": "The Min line represents the minimum correlation across different coverage levels. As n increases, the minimum tau_β decreases because the correlation coefficient is calculated using concordant pairs (PC) and discordant pairs (PD), showing worst-case correlation degradation at higher hashtag counts.",
        "visual_anchor": "solid blue Min line reaching approximately 0.6 at n=1000",
        "text_evidence": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data",
        "query_type": "value_context"
      },
      {
        "query": "Does the black Max line staying above 0.9 throughout align with tau_β ranging from perfect negative to perfect positive correlation?",
        "answer": "Yes, the Max line remaining above 0.9 indicates that the best-case correlation stays near perfect positive correlation (value of 1) across all n values, consistent with the tau_β range from -1 to 1.",
        "visual_anchor": "solid black Max line maintaining values above 0.9 across all n",
        "text_evidence": "The $\\tau _ { \\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.",
        "query_type": "value_context"
      },
      {
        "query": "How does the steeper decline of the blue Min curve compared to purple Q3 relate to understanding n versus correlation?",
        "answer": "The steeper Min decline demonstrates that minimum correlation degrades faster with increasing n than the third quartile, revealing the relationship between hashtag count and correlation variability that the analysis aims to understand.",
        "visual_anchor": "blue Min line declining more steeply than purple Q3 dash-dot line",
        "text_evidence": "To understand the relationship between $n$ and the resulting correlation, $\\tau _",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid blue line (Min)",
      "dashed green line (Q1)",
      "dotted brown line (Median)",
      "dash-dot purple line (Q3)",
      "solid black line (Max)",
      "y-axis labeled tau",
      "x-axis labeled n ranging 0-1000",
      "all lines declining from left to right",
      "blue Min line drops to ~0.6 at n=1000",
      "black Max line stays above 0.9"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_10",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg",
    "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ",
    "original_quality_score": 0.55,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue histogram bars\", \"x-axis showing binned divergence from 0.00 to 0.15\", \"y-axis showing count up to 50\", \"tallest bar at leftmost position near 0.00\", \"rapidly decreasing bar heights\", \"no bars beyond 0.15\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the tallest blue bar near zero divergence support that matched topics have low average divergence?\",\n      \"answer\": \"The histogram shows most matched topic pairs cluster near zero Jensen-Shannon divergence, with the tallest bar at the leftmost position, confirming that μ (average divergence) is low across matched pairs.\",\n      \"visual_anchor\": \"tallest blue bar at leftmost position near 0.00 divergence\",\n      \"text_evidence\": \"We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the absence of histogram bars beyond 0.15 on the x-axis confirm the stated maximum divergence threshold?\",\n      \"answer\": \"Yes, the histogram shows no blue bars extending beyond the 0.15 mark on the x-axis, directly confirming that no matched topic pair exceeded this divergence threshold.\",\n      \"visual_anchor\": \"empty space on x-axis beyond 0.15 with no blue bars\",\n      \"text_evidence\": \"The $\\\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ .\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the rapid height decrease of blue bars from left to right relate to standard deviation?\",\n      \"answer\": \"The rapidly decreasing bar heights indicate most divergence values concentrate near zero with few outliers, suggesting a small standard deviation (σ) in the matched topic divergences.\",\n      \"visual_anchor\": \"rapidly decreasing blue bar heights from left to right\",\n      \"text_evidence\": \"$\\\\mu$ is the average divergence of the matched topics, $\\\\sigma$ is the standard deviation.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue histogram bars",
        "x-axis showing binned divergence from 0.00 to 0.15",
        "y-axis showing count up to 50",
        "tallest bar at leftmost position near 0.00",
        "rapidly decreasing bar heights",
        "no bars beyond 0.15"
      ],
      "queries": [
        {
          "query": "Why does the tallest blue bar near zero divergence support that matched topics have low average divergence?",
          "answer": "The histogram shows most matched topic pairs cluster near zero Jensen-Shannon divergence, with the tallest bar at the leftmost position, confirming that μ (average divergence) is low across matched pairs.",
          "visual_anchor": "tallest blue bar at leftmost position near 0.00 divergence",
          "text_evidence": "We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6.",
          "query_type": "value_context"
        },
        {
          "query": "Does the absence of histogram bars beyond 0.15 on the x-axis confirm the stated maximum divergence threshold?",
          "answer": "Yes, the histogram shows no blue bars extending beyond the 0.15 mark on the x-axis, directly confirming that no matched topic pair exceeded this divergence threshold.",
          "visual_anchor": "empty space on x-axis beyond 0.15 with no blue bars",
          "text_evidence": "The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ .",
          "query_type": "value_context"
        },
        {
          "query": "How does the rapid height decrease of blue bars from left to right relate to standard deviation?",
          "answer": "The rapidly decreasing bar heights indicate most divergence values concentrate near zero with few outliers, suggesting a small standard deviation (σ) in the matched topic divergences.",
          "visual_anchor": "rapidly decreasing blue bar heights from left to right",
          "text_evidence": "$\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the tallest blue bar near zero divergence support that matched topics have low average divergence?",
        "answer": "The histogram shows most matched topic pairs cluster near zero Jensen-Shannon divergence, with the tallest bar at the leftmost position, confirming that μ (average divergence) is low across matched pairs.",
        "visual_anchor": "tallest blue bar at leftmost position near 0.00 divergence",
        "text_evidence": "We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6.",
        "query_type": "value_context"
      },
      {
        "query": "Does the absence of histogram bars beyond 0.15 on the x-axis confirm the stated maximum divergence threshold?",
        "answer": "Yes, the histogram shows no blue bars extending beyond the 0.15 mark on the x-axis, directly confirming that no matched topic pair exceeded this divergence threshold.",
        "visual_anchor": "empty space on x-axis beyond 0.15 with no blue bars",
        "text_evidence": "The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ .",
        "query_type": "value_context"
      },
      {
        "query": "How does the rapid height decrease of blue bars from left to right relate to standard deviation?",
        "answer": "The rapidly decreasing bar heights indicate most divergence values concentrate near zero with few outliers, suggesting a small standard deviation (σ) in the matched topic divergences.",
        "visual_anchor": "rapidly decreasing blue bar heights from left to right",
        "text_evidence": "$\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue histogram bars",
      "x-axis showing binned divergence from 0.00 to 0.15",
      "y-axis showing count up to 50",
      "tallest bar at leftmost position near 0.00",
      "rapidly decreasing bar heights",
      "no bars beyond 0.15"
    ]
  },
  {
    "doc_id": "1306.5204",
    "figure_id": "1306.5204_fig_14",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg",
    "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue distribution curve centered around 0.01\", \"red vertical line at approximately 0.015\", \"x-axis ranging from 0.00 to 0.05\", \"y-axis showing probability density\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the red vertical line's position relative to the blue curve's mean demonstrate the Streaming data divergence anomaly?\",\n      \"answer\": \"Yes, the red vertical line representing Streaming data is positioned multiple standard deviations away from the blue curve's mean, indicating the Streaming data's average Jensen-Shannon divergence is significantly different from random samples.\",\n      \"visual_anchor\": \"red vertical line at approximately 0.015 compared to blue curve peak at 0.01\",\n      \"text_evidence\": \"The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Why does analyzing the blue distribution curve justify focusing only on the largest component in retweet networks?\",\n      \"answer\": \"The blue curve represents random baseline data for comparison, and since retweet networks create many small disconnected components with all smaller ones containing less than 1% of nodes, the main component analysis is justified.\",\n      \"visual_anchor\": \"blue distribution curve showing baseline random sample divergences\",\n      \"text_evidence\": \"Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1\\\\%$ of the nodes justify our focus on the main component for this data.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the red line's deviation from the blue curve peak support calculating z-Scores in this experiment?\",\n      \"answer\": \"The red line's position away from the blue curve's peak quantifies how many standard deviations the Streaming data diverges from random samples, which is precisely what z-Scores measure in this experimental setup.\",\n      \"visual_anchor\": \"red vertical line deviation from blue curve peak at 0.01\",\n      \"text_evidence\": \"Results of this experiment, including $z$ -Scores are shown in Figure 7.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue distribution curve centered around 0.01",
        "red vertical line at approximately 0.015",
        "x-axis ranging from 0.00 to 0.05",
        "y-axis showing probability density"
      ],
      "queries": [
        {
          "query": "Does the red vertical line's position relative to the blue curve's mean demonstrate the Streaming data divergence anomaly?",
          "answer": "Yes, the red vertical line representing Streaming data is positioned multiple standard deviations away from the blue curve's mean, indicating the Streaming data's average Jensen-Shannon divergence is significantly different from random samples.",
          "visual_anchor": "red vertical line at approximately 0.015 compared to blue curve peak at 0.01",
          "text_evidence": "The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Why does analyzing the blue distribution curve justify focusing only on the largest component in retweet networks?",
          "answer": "The blue curve represents random baseline data for comparison, and since retweet networks create many small disconnected components with all smaller ones containing less than 1% of nodes, the main component analysis is justified.",
          "visual_anchor": "blue distribution curve showing baseline random sample divergences",
          "text_evidence": "Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1\\%$ of the nodes justify our focus on the main component for this data.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the red line's deviation from the blue curve peak support calculating z-Scores in this experiment?",
          "answer": "The red line's position away from the blue curve's peak quantifies how many standard deviations the Streaming data diverges from random samples, which is precisely what z-Scores measure in this experimental setup.",
          "visual_anchor": "red vertical line deviation from blue curve peak at 0.01",
          "text_evidence": "Results of this experiment, including $z$ -Scores are shown in Figure 7.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the red vertical line's position relative to the blue curve's mean demonstrate the Streaming data divergence anomaly?",
        "answer": "Yes, the red vertical line representing Streaming data is positioned multiple standard deviations away from the blue curve's mean, indicating the Streaming data's average Jensen-Shannon divergence is significantly different from random samples.",
        "visual_anchor": "red vertical line at approximately 0.015 compared to blue curve peak at 0.01",
        "text_evidence": "The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Why does analyzing the blue distribution curve justify focusing only on the largest component in retweet networks?",
        "answer": "The blue curve represents random baseline data for comparison, and since retweet networks create many small disconnected components with all smaller ones containing less than 1% of nodes, the main component analysis is justified.",
        "visual_anchor": "blue distribution curve showing baseline random sample divergences",
        "text_evidence": "Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1\\%$ of the nodes justify our focus on the main component for this data.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the red line's deviation from the blue curve peak support calculating z-Scores in this experiment?",
        "answer": "The red line's position away from the blue curve's peak quantifies how many standard deviations the Streaming data diverges from random samples, which is precisely what z-Scores measure in this experimental setup.",
        "visual_anchor": "red vertical line deviation from blue curve peak at 0.01",
        "text_evidence": "Results of this experiment, including $z$ -Scores are shown in Figure 7.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue distribution curve centered around 0.01",
      "red vertical line at approximately 0.015",
      "x-axis ranging from 0.00 to 0.05",
      "y-axis showing probability density"
    ]
  },
  {
    "doc_id": "1403.7400",
    "figure_id": "1403.7400_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg",
    "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Sharp peak reaching approximately 4.5M around May 28\",\n    \"Steep decline from 4.5M to under 1M between May 28 and June 3\",\n    \"Relatively flat baseline below 1M from June 9 onwards\",\n    \"Blue line with data points showing frequency over time\",\n    \"Y-axis showing frequency from 0 to 5M\",\n    \"X-axis showing dates from May 22 to beyond June 15\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the spike reaching 4.5M on May 28 illustrate the denominator problem when analyzing who engaged?\",\n      \"answer\": \"The peak shows how many people clicked or retweeted, but the denominator problem means we don't know how many people saw these hashtags and chose not to engage, making it impossible to calculate true engagement rates.\",\n      \"visual_anchor\": \"Peak at approximately 4.5M frequency on May 28\",\n      \"text_evidence\": \"It's not enough to know how many people have \\\"liked\\\" a Facebook status update, clicked on a link, or \\\"retweeted\\\" a message without knowing how many people saw the item and chose not to take any action.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the sharp drop from 4.5M to under 1M between May 28 and June 3 reflect Twitter's disproportionate focus?\",\n      \"answer\": \"Yes, this dramatic decline in hashtag frequency exemplifies why Twitter data dominates research, as the platform provides accessible metrics that capture such rapid shifts in social movement activity.\",\n      \"visual_anchor\": \"Steep decline from 4.5M peak to below 1M between May 28 and June 3\",\n      \"text_evidence\": \"While there are many social media platforms, big data research focuses disproportionately on Twitter. For example, ICWSM, perhaps the leading selective conference in the field of social media, had 72 full papers last year (2013), almost half of which presented data that was primarily or solely drawn from Twitter.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the flat baseline below 1M after June 9 obscure understanding of sub-population characteristics?\",\n      \"answer\": \"The low, stable frequency after June 9 shows reduced hashtag usage, but without knowing the characteristics of who saw versus who engaged with these protest hashtags, researchers cannot determine if the movement lost visibility or simply shifted audiences.\",\n      \"visual_anchor\": \"Flat baseline consistently below 1M from June 9 onwards\",\n      \"text_evidence\": \"We rarely know the characteristics of the sub-population that sees the content even thoug\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Sharp peak reaching approximately 4.5M around May 28",
        "Steep decline from 4.5M to under 1M between May 28 and June 3",
        "Relatively flat baseline below 1M from June 9 onwards",
        "Blue line with data points showing frequency over time",
        "Y-axis showing frequency from 0 to 5M",
        "X-axis showing dates from May 22 to beyond June 15"
      ],
      "queries": [
        {
          "query": "Why does the spike reaching 4.5M on May 28 illustrate the denominator problem when analyzing who engaged?",
          "answer": "The peak shows how many people clicked or retweeted, but the denominator problem means we don't know how many people saw these hashtags and chose not to engage, making it impossible to calculate true engagement rates.",
          "visual_anchor": "Peak at approximately 4.5M frequency on May 28",
          "text_evidence": "It's not enough to know how many people have \"liked\" a Facebook status update, clicked on a link, or \"retweeted\" a message without knowing how many people saw the item and chose not to take any action.",
          "query_type": "value_context"
        },
        {
          "query": "Does the sharp drop from 4.5M to under 1M between May 28 and June 3 reflect Twitter's disproportionate focus?",
          "answer": "Yes, this dramatic decline in hashtag frequency exemplifies why Twitter data dominates research, as the platform provides accessible metrics that capture such rapid shifts in social movement activity.",
          "visual_anchor": "Steep decline from 4.5M peak to below 1M between May 28 and June 3",
          "text_evidence": "While there are many social media platforms, big data research focuses disproportionately on Twitter. For example, ICWSM, perhaps the leading selective conference in the field of social media, had 72 full papers last year (2013), almost half of which presented data that was primarily or solely drawn from Twitter.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the flat baseline below 1M after June 9 obscure understanding of sub-population characteristics?",
          "answer": "The low, stable frequency after June 9 shows reduced hashtag usage, but without knowing the characteristics of who saw versus who engaged with these protest hashtags, researchers cannot determine if the movement lost visibility or simply shifted audiences.",
          "visual_anchor": "Flat baseline consistently below 1M from June 9 onwards",
          "text_evidence": "We rarely know the characteristics of the sub-population that sees the content even thoug",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the spike reaching 4.5M on May 28 illustrate the denominator problem when analyzing who engaged?",
        "answer": "The peak shows how many people clicked or retweeted, but the denominator problem means we don't know how many people saw these hashtags and chose not to engage, making it impossible to calculate true engagement rates.",
        "visual_anchor": "Peak at approximately 4.5M frequency on May 28",
        "text_evidence": "It's not enough to know how many people have \"liked\" a Facebook status update, clicked on a link, or \"retweeted\" a message without knowing how many people saw the item and chose not to take any action.",
        "query_type": "value_context"
      },
      {
        "query": "Does the sharp drop from 4.5M to under 1M between May 28 and June 3 reflect Twitter's disproportionate focus?",
        "answer": "Yes, this dramatic decline in hashtag frequency exemplifies why Twitter data dominates research, as the platform provides accessible metrics that capture such rapid shifts in social movement activity.",
        "visual_anchor": "Steep decline from 4.5M peak to below 1M between May 28 and June 3",
        "text_evidence": "While there are many social media platforms, big data research focuses disproportionately on Twitter. For example, ICWSM, perhaps the leading selective conference in the field of social media, had 72 full papers last year (2013), almost half of which presented data that was primarily or solely drawn from Twitter.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the flat baseline below 1M after June 9 obscure understanding of sub-population characteristics?",
        "answer": "The low, stable frequency after June 9 shows reduced hashtag usage, but without knowing the characteristics of who saw versus who engaged with these protest hashtags, researchers cannot determine if the movement lost visibility or simply shifted audiences.",
        "visual_anchor": "Flat baseline consistently below 1M from June 9 onwards",
        "text_evidence": "We rarely know the characteristics of the sub-population that sees the content even thoug",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Sharp peak reaching approximately 4.5M around May 28",
      "Steep decline from 4.5M to under 1M between May 28 and June 3",
      "Relatively flat baseline below 1M from June 9 onwards",
      "Blue line with data points showing frequency over time",
      "Y-axis showing frequency from 0 to 5M",
      "X-axis showing dates from May 22 to beyond June 15"
    ]
  },
  {
    "doc_id": "1403.7400",
    "figure_id": "1403.7400_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig1.jpg",
    "caption": "Figure 2: Retweeted widely, but mostly in disgust",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Twitter post from @celebboutique account\",\n    \"Tweet text mentioning '#Aurora is trending' and 'Kim K inspired #Aurora dress'\",\n    \"Engagement metrics showing 1,404 retweets and 178 favorites\",\n    \"Row of user profile pictures below the tweet\",\n    \"Follow button and Celeb Boutique branding\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the 1,404 retweets shown contradict the claim that the same act can have multiple meanings?\",\n      \"answer\": \"The high retweet count appears to indicate popularity, but the figure caption reveals these retweets were 'mostly in disgust,' demonstrating that retweets can have contradictory meanings beyond simple endorsement.\",\n      \"visual_anchor\": \"1,404 retweets metric displayed below the tweet\",\n      \"text_evidence\": \"The same act can have multiple, even contradictory meanings. In many studies, for example, retweets or mentions\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the 178 favorites count illustrate the complexity of interpreting online imprints mentioned for different communities?\",\n      \"answer\": \"The favorites metric appears positive but requires contextual understanding of who favorited it and why, which could range from genuine appreciation to ironic mockery, demonstrating interpretive complexity.\",\n      \"visual_anchor\": \"178 favorites count shown in the engagement metrics\",\n      \"text_evidence\": \"What's a click? What does a retweet mean? In what context? By whom? How do different communities interpret these interactions? As with all human activities, interpreting online imprints engages layers of complexity.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the Aurora hashtag trending topic demonstrate how users can alter visibility to machine algorithms through subtweeting?\",\n      \"answer\": \"No, this example shows direct mention rather than subtweeting. It illustrates how algorithms may misinterpret engagement as positive when context reveals disgust, but not the visibility-altering practices like subtweeting discussed later.\",\n      \"visual_anchor\": \"#Aurora hashtag in the tweet text\",\n      \"text_evidence\": \"Social media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person's tweets via \"screen captures,\" and hatelinking.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": null
  },
  {
    "doc_id": "1403.7400",
    "figure_id": "1403.7400_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg",
    "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Two side-by-side Twitter posts\",\n    \"Left post by zekeriya öz\",\n    \"Right post by Şemil Tayyar\",\n    \"Both posts contain Turkish text\",\n    \"Engagement metrics visible below each tweet\",\n    \"Profile pictures and follow buttons shown\",\n    \"No @ mentions visible in either tweet\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the two Turkish tweets shown side-by-side lack @ mentions despite making engagement algorithmically invisible?\",\n      \"answer\": \"Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person by not using @ mentions. This allows users to discuss someone without triggering algorithmic visibility or notifications.\",\n      \"visual_anchor\": \"Absence of @ mentions in both the left tweet by zekeriya öz and right tweet by Şemil Tayyar\",\n      \"text_evidence\": \"Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and conseque\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the Turkish exchanges between zekeriya öz and Şemil Tayyar demonstrate how big data analyses can miss engagement modes?\",\n      \"answer\": \"Yes, these subtweets demonstrate practices that alter visibility to machine algorithms. Such exchanges blind big data analyses to this mode of activity and engagement despite being newsworthy human interactions.\",\n      \"visual_anchor\": \"The two Twitter posts without direct mentions but responding to each other\",\n      \"text_evidence\": \"Social media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person's tweets via \\\"screen captures,\\\" and hatelinking. All these practices can blind big data analyses to this mode of activity and engagement.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why would the subtweet exchange shown require intimate familiarity with broader conversation context to understand?\",\n      \"answer\": \"The exchange would be unintelligible to anyone who did not already follow the broader conversation and was not intimately familiar with the context. Despite lacking explicit mentions, the exchange was clear enough to be reported in newspapers for those with contextual knowledge.\",\n      \"visual_anchor\": \"The Turkish text in both tweets that refers to each other without explicit names\",\n      \"text_evidence\": \"In another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Two side-by-side Twitter posts",
        "Left post by zekeriya öz",
        "Right post by Şemil Tayyar",
        "Both posts contain Turkish text",
        "Engagement metrics visible below each tweet",
        "Profile pictures and follow buttons shown",
        "No @ mentions visible in either tweet"
      ],
      "queries": [
        {
          "query": "Why do the two Turkish tweets shown side-by-side lack @ mentions despite making engagement algorithmically invisible?",
          "answer": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person by not using @ mentions. This allows users to discuss someone without triggering algorithmic visibility or notifications.",
          "visual_anchor": "Absence of @ mentions in both the left tweet by zekeriya öz and right tweet by Şemil Tayyar",
          "text_evidence": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and conseque",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the Turkish exchanges between zekeriya öz and Şemil Tayyar demonstrate how big data analyses can miss engagement modes?",
          "answer": "Yes, these subtweets demonstrate practices that alter visibility to machine algorithms. Such exchanges blind big data analyses to this mode of activity and engagement despite being newsworthy human interactions.",
          "visual_anchor": "The two Twitter posts without direct mentions but responding to each other",
          "text_evidence": "Social media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person's tweets via \"screen captures,\" and hatelinking. All these practices can blind big data analyses to this mode of activity and engagement.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why would the subtweet exchange shown require intimate familiarity with broader conversation context to understand?",
          "answer": "The exchange would be unintelligible to anyone who did not already follow the broader conversation and was not intimately familiar with the context. Despite lacking explicit mentions, the exchange was clear enough to be reported in newspapers for those with contextual knowledge.",
          "visual_anchor": "The Turkish text in both tweets that refers to each other without explicit names",
          "text_evidence": "In another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the two Turkish tweets shown side-by-side lack @ mentions despite making engagement algorithmically invisible?",
        "answer": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person by not using @ mentions. This allows users to discuss someone without triggering algorithmic visibility or notifications.",
        "visual_anchor": "Absence of @ mentions in both the left tweet by zekeriya öz and right tweet by Şemil Tayyar",
        "text_evidence": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and conseque",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the Turkish exchanges between zekeriya öz and Şemil Tayyar demonstrate how big data analyses can miss engagement modes?",
        "answer": "Yes, these subtweets demonstrate practices that alter visibility to machine algorithms. Such exchanges blind big data analyses to this mode of activity and engagement despite being newsworthy human interactions.",
        "visual_anchor": "The two Twitter posts without direct mentions but responding to each other",
        "text_evidence": "Social media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person's tweets via \"screen captures,\" and hatelinking. All these practices can blind big data analyses to this mode of activity and engagement.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why would the subtweet exchange shown require intimate familiarity with broader conversation context to understand?",
        "answer": "The exchange would be unintelligible to anyone who did not already follow the broader conversation and was not intimately familiar with the context. Despite lacking explicit mentions, the exchange was clear enough to be reported in newspapers for those with contextual knowledge.",
        "visual_anchor": "The Turkish text in both tweets that refers to each other without explicit names",
        "text_evidence": "In another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Two side-by-side Twitter posts",
      "Left post by zekeriya öz",
      "Right post by Şemil Tayyar",
      "Both posts contain Turkish text",
      "Engagement metrics visible below each tweet",
      "Profile pictures and follow buttons shown",
      "No @ mentions visible in either tweet"
    ]
  },
  {
    "doc_id": "1403.7400",
    "figure_id": "1403.7400_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg",
    "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Twitter post by Niral Bengisu Karaca at top\",\n    \"Three embedded tweet screenshots below the main post\",\n    \"Engagement metrics showing 610 RETWEETS and 163 FAVORITES\",\n    \"Turkish language text in all tweets\",\n    \"Profile pictures and usernames visible in screenshots\",\n    \"Timestamp: 7:50 AM - 27 Dec 2013\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the columnist use three embedded screenshots instead of direct quotes to respond to critics?\",\n      \"answer\": \"Using screen captures rather than quotes makes the engagement algorithmically invisible, as this practice prevents Twitter's algorithms from detecting mentions, links, or quotes that would normally signal engagement.\",\n      \"visual_anchor\": \"Three embedded tweet screenshots in the middle section of the Twitter post\",\n      \"text_evidence\": \"A \\\"caps\\\" is done when Twitter users reference each other's tweets through screen captures rather than links, mentions or quotes\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How widespread is the practice shown in these embedded screenshots based on one hour of sampling?\",\n      \"answer\": \"The practice is extremely widespread, with a single hour of purposive sampling resulting in more than 300 instances where users employed such screen capture \\\"caps\\\" instead of direct engagement.\",\n      \"visual_anchor\": \"The three screenshot captures embedded within the main tweet dated 27 Dec 2013\",\n      \"text_evidence\": \"This practice is so widespread that a single hour following the same purposive sample resulted in more than 300 instances in which users employed such \\\"caps.\\\"\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What type of algorithmic detection does the screenshot-based response at 7:50 AM evade compared to traditional mentions?\",\n      \"answer\": \"The screen capture method adds to the invisibility of engagement to algorithms, preventing detection of references that would normally be captured through links, mentions, or quotes in Twitter's engagement tracking systems.\",\n      \"visual_anchor\": \"The main Twitter post timestamp of 7:50 AM - 27 Dec 2013 containing screenshot captures\",\n      \"text_evidence\": \"Using screen captures rather than quotes is another practice that adds to the invisibility of engagement to algorithms\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Twitter post by Niral Bengisu Karaca at top",
        "Three embedded tweet screenshots below the main post",
        "Engagement metrics showing 610 RETWEETS and 163 FAVORITES",
        "Turkish language text in all tweets",
        "Profile pictures and usernames visible in screenshots",
        "Timestamp: 7:50 AM - 27 Dec 2013"
      ],
      "queries": [
        {
          "query": "Why does the columnist use three embedded screenshots instead of direct quotes to respond to critics?",
          "answer": "Using screen captures rather than quotes makes the engagement algorithmically invisible, as this practice prevents Twitter's algorithms from detecting mentions, links, or quotes that would normally signal engagement.",
          "visual_anchor": "Three embedded tweet screenshots in the middle section of the Twitter post",
          "text_evidence": "A \"caps\" is done when Twitter users reference each other's tweets through screen captures rather than links, mentions or quotes",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How widespread is the practice shown in these embedded screenshots based on one hour of sampling?",
          "answer": "The practice is extremely widespread, with a single hour of purposive sampling resulting in more than 300 instances where users employed such screen capture \"caps\" instead of direct engagement.",
          "visual_anchor": "The three screenshot captures embedded within the main tweet dated 27 Dec 2013",
          "text_evidence": "This practice is so widespread that a single hour following the same purposive sample resulted in more than 300 instances in which users employed such \"caps.\"",
          "query_type": "value_context"
        },
        {
          "query": "What type of algorithmic detection does the screenshot-based response at 7:50 AM evade compared to traditional mentions?",
          "answer": "The screen capture method adds to the invisibility of engagement to algorithms, preventing detection of references that would normally be captured through links, mentions, or quotes in Twitter's engagement tracking systems.",
          "visual_anchor": "The main Twitter post timestamp of 7:50 AM - 27 Dec 2013 containing screenshot captures",
          "text_evidence": "Using screen captures rather than quotes is another practice that adds to the invisibility of engagement to algorithms",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the columnist use three embedded screenshots instead of direct quotes to respond to critics?",
        "answer": "Using screen captures rather than quotes makes the engagement algorithmically invisible, as this practice prevents Twitter's algorithms from detecting mentions, links, or quotes that would normally signal engagement.",
        "visual_anchor": "Three embedded tweet screenshots in the middle section of the Twitter post",
        "text_evidence": "A \"caps\" is done when Twitter users reference each other's tweets through screen captures rather than links, mentions or quotes",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How widespread is the practice shown in these embedded screenshots based on one hour of sampling?",
        "answer": "The practice is extremely widespread, with a single hour of purposive sampling resulting in more than 300 instances where users employed such screen capture \"caps\" instead of direct engagement.",
        "visual_anchor": "The three screenshot captures embedded within the main tweet dated 27 Dec 2013",
        "text_evidence": "This practice is so widespread that a single hour following the same purposive sample resulted in more than 300 instances in which users employed such \"caps.\"",
        "query_type": "value_context"
      },
      {
        "query": "What type of algorithmic detection does the screenshot-based response at 7:50 AM evade compared to traditional mentions?",
        "answer": "The screen capture method adds to the invisibility of engagement to algorithms, preventing detection of references that would normally be captured through links, mentions, or quotes in Twitter's engagement tracking systems.",
        "visual_anchor": "The main Twitter post timestamp of 7:50 AM - 27 Dec 2013 containing screenshot captures",
        "text_evidence": "Using screen captures rather than quotes is another practice that adds to the invisibility of engagement to algorithms",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Twitter post by Niral Bengisu Karaca at top",
      "Three embedded tweet screenshots below the main post",
      "Engagement metrics showing 610 RETWEETS and 163 FAVORITES",
      "Turkish language text in all tweets",
      "Profile pictures and usernames visible in screenshots",
      "Timestamp: 7:50 AM - 27 Dec 2013"
    ]
  },
  {
    "doc_id": "1403.7400",
    "figure_id": "1403.7400_fig_5",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg",
    "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Twitter post by Blake Hounshell (@blakehounshell)\",\n    \"Tweet text: 'Getting crowded under that bus.'\",\n    \"Timestamp: 11:18 AM - 1 Jan 2014\",\n    \"Engagement metrics: 64 RETWEETS, 55 FAVORITES\",\n    \"Profile avatars of users who engaged\",\n    \"Blue 'Following' button\",\n    \"Verified checkmark icon\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Blake Hounshell's January 2014 tweet demonstrate the assumption that information spreads like germs across platforms?\",\n      \"answer\": \"The tweet exemplifies how researchers import methods from other fields on the implicit assumption that social media networks operate like disease vectors or airline networks. However, such assumptions are rarely explicitly addressed in research papers.\",\n      \"visual_anchor\": \"Tweet dated '11:18 AM - 1 Jan 2014' with text 'Getting crowded under that bus.'\",\n      \"text_evidence\": \"Do social mediia networks opperate through similar mechaannisms as netwoorks of airliness? Does informmation work thhe wway germs doo? Such questiions are rarelyy explicitly addddressed even tthough many  papers importt methods fromm oother fields on  the implicit asssumption that  the answer is  a yyes.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the 64 retweets metric shown relate to studies limited to single platforms with restricted explanatory power?\",\n      \"answer\": \"The engagement metrics (64 retweets, 55 favorites) represent platform-specific measurements. Studies examining information spread like Romero et al. and Lerman et al. are often limited to single or few platforms, which constrains their explanatory power.\",\n      \"visual_anchor\": \"Engagement counter showing '64 RETWEETS' and '55 FAVORITES'\",\n      \"text_evidence\": \"Studies thaat do look at tthis question, ssuch as Romerro eet al. (2011) annd Lerman et aal. (2010), are  often limited tto ssingle, or few , platforms, wwhich limit thheir explanatorry ppower\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the verified checkmark and follower count exemplify how humans respond to the metrics researchers measure?\",\n      \"answer\": \"The verified badge and visible engagement metrics demonstrate reflexivity: humans understand, evaluate, and respond to the same measurements that big data researchers track. This includes deliberate attempts by activists to manipulate metrics like trending hashtags.\",\n      \"visual_anchor\": \"Blue verified checkmark icon next to Blake Hounshell's name\",\n      \"text_evidence\": \"Unlike disease  vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same  metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \\\"trend.\\\"\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Twitter post by Blake Hounshell (@blakehounshell)",
        "Tweet text: 'Getting crowded under that bus.'",
        "Timestamp: 11:18 AM - 1 Jan 2014",
        "Engagement metrics: 64 RETWEETS, 55 FAVORITES",
        "Profile avatars of users who engaged",
        "Blue 'Following' button",
        "Verified checkmark icon"
      ],
      "queries": [
        {
          "query": "Why does Blake Hounshell's January 2014 tweet demonstrate the assumption that information spreads like germs across platforms?",
          "answer": "The tweet exemplifies how researchers import methods from other fields on the implicit assumption that social media networks operate like disease vectors or airline networks. However, such assumptions are rarely explicitly addressed in research papers.",
          "visual_anchor": "Tweet dated '11:18 AM - 1 Jan 2014' with text 'Getting crowded under that bus.'",
          "text_evidence": "Do social mediia networks opperate through similar mechaannisms as netwoorks of airliness? Does informmation work thhe wway germs doo? Such questiions are rarelyy explicitly addddressed even tthough many  papers importt methods fromm oother fields on  the implicit asssumption that  the answer is  a yyes.",
          "query_type": "value_context"
        },
        {
          "query": "How does the 64 retweets metric shown relate to studies limited to single platforms with restricted explanatory power?",
          "answer": "The engagement metrics (64 retweets, 55 favorites) represent platform-specific measurements. Studies examining information spread like Romero et al. and Lerman et al. are often limited to single or few platforms, which constrains their explanatory power.",
          "visual_anchor": "Engagement counter showing '64 RETWEETS' and '55 FAVORITES'",
          "text_evidence": "Studies thaat do look at tthis question, ssuch as Romerro eet al. (2011) annd Lerman et aal. (2010), are  often limited tto ssingle, or few , platforms, wwhich limit thheir explanatorry ppower",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the verified checkmark and follower count exemplify how humans respond to the metrics researchers measure?",
          "answer": "The verified badge and visible engagement metrics demonstrate reflexivity: humans understand, evaluate, and respond to the same measurements that big data researchers track. This includes deliberate attempts by activists to manipulate metrics like trending hashtags.",
          "visual_anchor": "Blue verified checkmark icon next to Blake Hounshell's name",
          "text_evidence": "Unlike disease  vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same  metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \"trend.\"",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Blake Hounshell's January 2014 tweet demonstrate the assumption that information spreads like germs across platforms?",
        "answer": "The tweet exemplifies how researchers import methods from other fields on the implicit assumption that social media networks operate like disease vectors or airline networks. However, such assumptions are rarely explicitly addressed in research papers.",
        "visual_anchor": "Tweet dated '11:18 AM - 1 Jan 2014' with text 'Getting crowded under that bus.'",
        "text_evidence": "Do social mediia networks opperate through similar mechaannisms as netwoorks of airliness? Does informmation work thhe wway germs doo? Such questiions are rarelyy explicitly addddressed even tthough many  papers importt methods fromm oother fields on  the implicit asssumption that  the answer is  a yyes.",
        "query_type": "value_context"
      },
      {
        "query": "How does the 64 retweets metric shown relate to studies limited to single platforms with restricted explanatory power?",
        "answer": "The engagement metrics (64 retweets, 55 favorites) represent platform-specific measurements. Studies examining information spread like Romero et al. and Lerman et al. are often limited to single or few platforms, which constrains their explanatory power.",
        "visual_anchor": "Engagement counter showing '64 RETWEETS' and '55 FAVORITES'",
        "text_evidence": "Studies thaat do look at tthis question, ssuch as Romerro eet al. (2011) annd Lerman et aal. (2010), are  often limited tto ssingle, or few , platforms, wwhich limit thheir explanatorry ppower",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the verified checkmark and follower count exemplify how humans respond to the metrics researchers measure?",
        "answer": "The verified badge and visible engagement metrics demonstrate reflexivity: humans understand, evaluate, and respond to the same measurements that big data researchers track. This includes deliberate attempts by activists to manipulate metrics like trending hashtags.",
        "visual_anchor": "Blue verified checkmark icon next to Blake Hounshell's name",
        "text_evidence": "Unlike disease  vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same  metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \"trend.\"",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Twitter post by Blake Hounshell (@blakehounshell)",
      "Tweet text: 'Getting crowded under that bus.'",
      "Timestamp: 11:18 AM - 1 Jan 2014",
      "Engagement metrics: 64 RETWEETS, 55 FAVORITES",
      "Profile avatars of users who engaged",
      "Blue 'Following' button",
      "Verified checkmark icon"
    ]
  },
  {
    "doc_id": "1403.7400",
    "figure_id": "1403.7400_fig_6",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg",
    "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"photo\",\n  \"visual_elements\": [\n    \"Twitter interface showing İbrahim Melih Gökçek's profile with Following button\",\n    \"Tweet text in Turkish with #stoplyingCNN hashtag\",\n    \"Tweet metrics showing 4,206 retweets and 456 favorites\",\n    \"Row of user avatar icons below the tweet\",\n    \"Timestamp showing 8:41 AM - 25 Jun 13\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why did the Ankara Mayor's tweet with 4,206 retweets exemplify deliberate attempts to make hashtags trend?\",\n      \"answer\": \"Political activists in countries with less mainstream global media attention, like Bahrain, deliberately attempt to make hashtags trend to gain visibility. The Ankara Mayor's #stoplyingCNN campaign demonstrates this strategic use of social media metrics to amplify political messaging.\",\n      \"visual_anchor\": \"4,206 retweets metric displayed below the tweet\",\n      \"text_evidence\": \"political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \\\"trend.\\\"\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the #stoplyingCNN hashtag campaign illustrate how humans respond to the same big data metrics researchers measure?\",\n      \"answer\": \"Yes, the campaign demonstrates reflexivity where humans understand and respond to metrics like trending hashtags. This exemplifies how unlike disease vectors or gases, humans evaluate and strategically manipulate the same measurements that big data researchers study.\",\n      \"visual_anchor\": \"#stoplyingCNN hashtag visible in the tweet text\",\n      \"text_evidence\": \"UUnlike disease vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same metrics that biig ddata researcherrs are measurinng.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the June 25, 2013 timestamp relate to understanding context of human communications on social media?\",\n      \"answer\": \"The specific timestamp exemplifies the temporal context crucial for interpreting social media data. Understanding when communications occur is essential because meaning, context, and socio-cultural interactions are multi-faceted and complex, requiring careful methodological grounding beyond simple data collection.\",\n      \"visual_anchor\": \"Timestamp reading 8:41 AM - 25 Jun 13\",\n      \"text_evidence\": \"Meaniing of social mmedia imprints , context of huuman communiccations, and naature of socio -cultural interaactions are multi--faceted and ccomplex.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "photo",
      "visual_elements": [
        "Twitter interface showing İbrahim Melih Gökçek's profile with Following button",
        "Tweet text in Turkish with #stoplyingCNN hashtag",
        "Tweet metrics showing 4,206 retweets and 456 favorites",
        "Row of user avatar icons below the tweet",
        "Timestamp showing 8:41 AM - 25 Jun 13"
      ],
      "queries": [
        {
          "query": "Why did the Ankara Mayor's tweet with 4,206 retweets exemplify deliberate attempts to make hashtags trend?",
          "answer": "Political activists in countries with less mainstream global media attention, like Bahrain, deliberately attempt to make hashtags trend to gain visibility. The Ankara Mayor's #stoplyingCNN campaign demonstrates this strategic use of social media metrics to amplify political messaging.",
          "visual_anchor": "4,206 retweets metric displayed below the tweet",
          "text_evidence": "political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \"trend.\"",
          "query_type": "value_context"
        },
        {
          "query": "Does the #stoplyingCNN hashtag campaign illustrate how humans respond to the same big data metrics researchers measure?",
          "answer": "Yes, the campaign demonstrates reflexivity where humans understand and respond to metrics like trending hashtags. This exemplifies how unlike disease vectors or gases, humans evaluate and strategically manipulate the same measurements that big data researchers study.",
          "visual_anchor": "#stoplyingCNN hashtag visible in the tweet text",
          "text_evidence": "UUnlike disease vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same metrics that biig ddata researcherrs are measurinng.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the June 25, 2013 timestamp relate to understanding context of human communications on social media?",
          "answer": "The specific timestamp exemplifies the temporal context crucial for interpreting social media data. Understanding when communications occur is essential because meaning, context, and socio-cultural interactions are multi-faceted and complex, requiring careful methodological grounding beyond simple data collection.",
          "visual_anchor": "Timestamp reading 8:41 AM - 25 Jun 13",
          "text_evidence": "Meaniing of social mmedia imprints , context of huuman communiccations, and naature of socio -cultural interaactions are multi--faceted and ccomplex.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why did the Ankara Mayor's tweet with 4,206 retweets exemplify deliberate attempts to make hashtags trend?",
        "answer": "Political activists in countries with less mainstream global media attention, like Bahrain, deliberately attempt to make hashtags trend to gain visibility. The Ankara Mayor's #stoplyingCNN campaign demonstrates this strategic use of social media metrics to amplify political messaging.",
        "visual_anchor": "4,206 retweets metric displayed below the tweet",
        "text_evidence": "political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg \"trend.\"",
        "query_type": "value_context"
      },
      {
        "query": "Does the #stoplyingCNN hashtag campaign illustrate how humans respond to the same big data metrics researchers measure?",
        "answer": "Yes, the campaign demonstrates reflexivity where humans understand and respond to metrics like trending hashtags. This exemplifies how unlike disease vectors or gases, humans evaluate and strategically manipulate the same measurements that big data researchers study.",
        "visual_anchor": "#stoplyingCNN hashtag visible in the tweet text",
        "text_evidence": "UUnlike disease vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same metrics that biig ddata researcherrs are measurinng.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the June 25, 2013 timestamp relate to understanding context of human communications on social media?",
        "answer": "The specific timestamp exemplifies the temporal context crucial for interpreting social media data. Understanding when communications occur is essential because meaning, context, and socio-cultural interactions are multi-faceted and complex, requiring careful methodological grounding beyond simple data collection.",
        "visual_anchor": "Timestamp reading 8:41 AM - 25 Jun 13",
        "text_evidence": "Meaniing of social mmedia imprints , context of huuman communiccations, and naature of socio -cultural interaactions are multi--faceted and ccomplex.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "photo",
    "visual_elements": [
      "Twitter interface showing İbrahim Melih Gökçek's profile with Following button",
      "Tweet text in Turkish with #stoplyingCNN hashtag",
      "Tweet metrics showing 4,206 retweets and 456 favorites",
      "Row of user avatar icons below the tweet",
      "Timestamp showing 8:41 AM - 25 Jun 13"
    ]
  },
  {
    "doc_id": "1409.0575",
    "figure_id": "1409.0575_fig_1",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg",
    "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Seven horizontal rows labeled: Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size\",\n    \"Each row contains 4 example images with labels (e.g., Candle, Oyster, Cannon, Spider Web)\",\n    \"Bidirectional arrows spanning from 'Low' to 'High' at the ends of each row\",\n    \"Dog breeds examples in rows (Lizard visible in Number of Instances row)\",\n    \"Bottom row shows Orange, Laptop, Four-poster, Airliner with Real-world Size dimension\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the Lizard appear in the Number of Instances row when ILSVRC2012 replaced 90 synsets with dog breeds?\",\n      \"answer\": \"The Lizard example illustrates variation in instance count across object categories, while the 90 dog breed synsets replaced in ILSVRC2012 demonstrate fine-grained classification within the 1000 object classes, representing a different dimension of dataset complexity.\",\n      \"visual_anchor\": \"Lizard image in the Number of Instances row, second position from left\",\n      \"text_evidence\": \"In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Figure 2.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the Object Scale row spanning from Candle to Spider Web relate to the 1000 object classes annotation goal?\",\n      \"answer\": \"The Object Scale row demonstrates the scale variability challenge that algorithms must handle across the 1000 object classes, showing that the dataset requires systems to recognize objects ranging from small items like candles to large structures like spider webs.\",\n      \"visual_anchor\": \"Top row labeled 'Object Scale' with Candle on left and Spider Web on right\",\n      \"text_evidence\": \"The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the seven-dimensional organization visible in the rows support using multiple search engines and expanded multilingual queries during collection?\",\n      \"answer\": \"Yes, the seven dimensions (Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size) demonstrate the dataset's complexity, requiring diverse collection strategies including multiple search engines and multilingual queries to capture sufficient variation across all these dimensions.\",\n      \"visual_anchor\": \"Seven distinct horizontal rows with bidirectional arrows showing dimensional ranges\",\n      \"text_evidence\": \"collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Section 3.1.2), and finally filtering the millions of collected i\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Seven horizontal rows labeled: Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size",
        "Each row contains 4 example images with labels (e.g., Candle, Oyster, Cannon, Spider Web)",
        "Bidirectional arrows spanning from 'Low' to 'High' at the ends of each row",
        "Dog breeds examples in rows (Lizard visible in Number of Instances row)",
        "Bottom row shows Orange, Laptop, Four-poster, Airliner with Real-world Size dimension"
      ],
      "queries": [
        {
          "query": "Why does the Lizard appear in the Number of Instances row when ILSVRC2012 replaced 90 synsets with dog breeds?",
          "answer": "The Lizard example illustrates variation in instance count across object categories, while the 90 dog breed synsets replaced in ILSVRC2012 demonstrate fine-grained classification within the 1000 object classes, representing a different dimension of dataset complexity.",
          "visual_anchor": "Lizard image in the Number of Instances row, second position from left",
          "text_evidence": "In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Figure 2.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the Object Scale row spanning from Candle to Spider Web relate to the 1000 object classes annotation goal?",
          "answer": "The Object Scale row demonstrates the scale variability challenge that algorithms must handle across the 1000 object classes, showing that the dataset requires systems to recognize objects ranging from small items like candles to large structures like spider webs.",
          "visual_anchor": "Top row labeled 'Object Scale' with Candle on left and Spider Web on right",
          "text_evidence": "The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.",
          "query_type": "value_context"
        },
        {
          "query": "Does the seven-dimensional organization visible in the rows support using multiple search engines and expanded multilingual queries during collection?",
          "answer": "Yes, the seven dimensions (Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size) demonstrate the dataset's complexity, requiring diverse collection strategies including multiple search engines and multilingual queries to capture sufficient variation across all these dimensions.",
          "visual_anchor": "Seven distinct horizontal rows with bidirectional arrows showing dimensional ranges",
          "text_evidence": "collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Section 3.1.2), and finally filtering the millions of collected i",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the Lizard appear in the Number of Instances row when ILSVRC2012 replaced 90 synsets with dog breeds?",
        "answer": "The Lizard example illustrates variation in instance count across object categories, while the 90 dog breed synsets replaced in ILSVRC2012 demonstrate fine-grained classification within the 1000 object classes, representing a different dimension of dataset complexity.",
        "visual_anchor": "Lizard image in the Number of Instances row, second position from left",
        "text_evidence": "In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Figure 2.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the Object Scale row spanning from Candle to Spider Web relate to the 1000 object classes annotation goal?",
        "answer": "The Object Scale row demonstrates the scale variability challenge that algorithms must handle across the 1000 object classes, showing that the dataset requires systems to recognize objects ranging from small items like candles to large structures like spider webs.",
        "visual_anchor": "Top row labeled 'Object Scale' with Candle on left and Spider Web on right",
        "text_evidence": "The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.",
        "query_type": "value_context"
      },
      {
        "query": "Does the seven-dimensional organization visible in the rows support using multiple search engines and expanded multilingual queries during collection?",
        "answer": "Yes, the seven dimensions (Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size) demonstrate the dataset's complexity, requiring diverse collection strategies including multiple search engines and multilingual queries to capture sufficient variation across all these dimensions.",
        "visual_anchor": "Seven distinct horizontal rows with bidirectional arrows showing dimensional ranges",
        "text_evidence": "collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Section 3.1.2), and finally filtering the millions of collected i",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Seven horizontal rows labeled: Object Scale, Number of Instances, Image Clutter, Deformability, Amount of Texture, Color Distinctiveness, Shape Distinctiveness, Real-world Size",
      "Each row contains 4 example images with labels (e.g., Candle, Oyster, Cannon, Spider Web)",
      "Bidirectional arrows spanning from 'Low' to 'High' at the ends of each row",
      "Dog breeds examples in rows (Lizard visible in Number of Instances row)",
      "Bottom row shows Orange, Laptop, Four-poster, Airliner with Real-world Size dimension"
    ]
  },
  {
    "doc_id": "1409.0575",
    "figure_id": "1409.0575_fig_55",
    "figure_number": 12,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig54.jpg",
    "caption": "Figure 12.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"photo\",\n  \"visual_elements\": [\"metallic measuring scoops\", \"wooden cutting board\", \"red handled utensils\", \"white bowl in background\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the metallic measuring tools shown represent easiest or hardest classes in ILSVRC2012-2014 object detection?\",\n      \"answer\": \"The figure shows object detection results for ILSVRC2012-2014, displaying examples that represent either the easiest or hardest classes based on best performance entries. The specific classification depends on the detection accuracy achieved for these kitchen utensil categories.\",\n      \"visual_anchor\": \"metallic measuring scoops on wooden cutting board\",\n      \"text_evidence\": \"For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are the red handled utensils and white bowl included when only object detection results are referenced?\",\n      \"answer\": \"Figure 12 specifically displays object detection results for ILSVRC2012-2014, showing examples of the easiest and hardest object classes to detect. These items represent actual object categories evaluated in the detection task with varying performance levels.\",\n      \"visual_anchor\": \"red handled utensils and white bowl in background\",\n      \"text_evidence\": \"Object detection results are shown in Figure 12.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the wooden cutting board and kitchen items represent classes with best or worst detection performance?\",\n      \"answer\": \"The items shown represent examples from the optimistic results showing either easiest or hardest classes for the object detection task. The figure demonstrates the range of detection difficulty across different object categories in ILSVRC2012-2014.\",\n      \"visual_anchor\": \"wooden cutting board with various kitchen implements\",\n      \"text_evidence\": \"Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst result\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "photo",
      "visual_elements": [
        "metallic measuring scoops",
        "wooden cutting board",
        "red handled utensils",
        "white bowl in background"
      ],
      "queries": [
        {
          "query": "Do the metallic measuring tools shown represent easiest or hardest classes in ILSVRC2012-2014 object detection?",
          "answer": "The figure shows object detection results for ILSVRC2012-2014, displaying examples that represent either the easiest or hardest classes based on best performance entries. The specific classification depends on the detection accuracy achieved for these kitchen utensil categories.",
          "visual_anchor": "metallic measuring scoops on wooden cutting board",
          "text_evidence": "For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.",
          "query_type": "value_context"
        },
        {
          "query": "Why are the red handled utensils and white bowl included when only object detection results are referenced?",
          "answer": "Figure 12 specifically displays object detection results for ILSVRC2012-2014, showing examples of the easiest and hardest object classes to detect. These items represent actual object categories evaluated in the detection task with varying performance levels.",
          "visual_anchor": "red handled utensils and white bowl in background",
          "text_evidence": "Object detection results are shown in Figure 12.",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the wooden cutting board and kitchen items represent classes with best or worst detection performance?",
          "answer": "The items shown represent examples from the optimistic results showing either easiest or hardest classes for the object detection task. The figure demonstrates the range of detection difficulty across different object categories in ILSVRC2012-2014.",
          "visual_anchor": "wooden cutting board with various kitchen implements",
          "text_evidence": "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst result",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the metallic measuring tools shown represent easiest or hardest classes in ILSVRC2012-2014 object detection?",
        "answer": "The figure shows object detection results for ILSVRC2012-2014, displaying examples that represent either the easiest or hardest classes based on best performance entries. The specific classification depends on the detection accuracy achieved for these kitchen utensil categories.",
        "visual_anchor": "metallic measuring scoops on wooden cutting board",
        "text_evidence": "For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.",
        "query_type": "value_context"
      },
      {
        "query": "Why are the red handled utensils and white bowl included when only object detection results are referenced?",
        "answer": "Figure 12 specifically displays object detection results for ILSVRC2012-2014, showing examples of the easiest and hardest object classes to detect. These items represent actual object categories evaluated in the detection task with varying performance levels.",
        "visual_anchor": "red handled utensils and white bowl in background",
        "text_evidence": "Object detection results are shown in Figure 12.",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the wooden cutting board and kitchen items represent classes with best or worst detection performance?",
        "answer": "The items shown represent examples from the optimistic results showing either easiest or hardest classes for the object detection task. The figure demonstrates the range of detection difficulty across different object categories in ILSVRC2012-2014.",
        "visual_anchor": "wooden cutting board with various kitchen implements",
        "text_evidence": "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst result",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "photo",
    "visual_elements": [
      "metallic measuring scoops",
      "wooden cutting board",
      "red handled utensils",
      "white bowl in background"
    ]
  },
  {
    "doc_id": "1409.0575",
    "figure_id": "1409.0575_fig_75",
    "figure_number": 11,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg",
    "caption": "Figure 11.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"photo\",\n  \"visual_elements\": [\"bundle of cylindrical objects\", \"pink/red colored tips\", \"black/dark colored ends\", \"circular arrangement of tips\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the pink-tipped cylindrical objects shown represent one of the ten randomly selected object categories from ILSVRC2012-2014?\",\n      \"answer\": \"Yes, these objects represent one of the ten randomly selected object categories shown in Figure 11 (top), which displays a random set from the ILSVRC dataset used for evaluating classification and detection tasks.\",\n      \"visual_anchor\": \"pink/red colored tips arranged in circular bundle\",\n      \"text_evidence\": \"Figure 11 (top) shows a random set of 10 of them.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Are these bundled pink-tipped objects classified among the easiest or hardest categories based on optimistic ILSVRC2012-2014 performance?\",\n      \"answer\": \"The classification difficulty cannot be determined from the figure alone. The referenced analysis shows both easiest and hardest classes based on best submitted entry performance across multiple years.\",\n      \"visual_anchor\": \"entire bundle of pink-tipped cylindrical objects\",\n      \"text_evidence\": \"For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the average image area occupied by these pink-tipped objects relate to their detection performance on ILSVRC2014 validation?\",\n      \"answer\": \"The performance would correspond to a specific dot position in Figure 13, where x-axis represents the average fraction of image area occupied by instances of this object class, and y-axis shows the best detection performance achieved.\",\n      \"visual_anchor\": \"pink-tipped cylindrical objects occupying the frame\",\n      \"text_evidence\": \"Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. 'Optimistic' performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "photo",
      "visual_elements": [
        "bundle of cylindrical objects",
        "pink/red colored tips",
        "black/dark colored ends",
        "circular arrangement of tips"
      ],
      "queries": [
        {
          "query": "Do the pink-tipped cylindrical objects shown represent one of the ten randomly selected object categories from ILSVRC2012-2014?",
          "answer": "Yes, these objects represent one of the ten randomly selected object categories shown in Figure 11 (top), which displays a random set from the ILSVRC dataset used for evaluating classification and detection tasks.",
          "visual_anchor": "pink/red colored tips arranged in circular bundle",
          "text_evidence": "Figure 11 (top) shows a random set of 10 of them.",
          "query_type": "value_context"
        },
        {
          "query": "Are these bundled pink-tipped objects classified among the easiest or hardest categories based on optimistic ILSVRC2012-2014 performance?",
          "answer": "The classification difficulty cannot be determined from the figure alone. The referenced analysis shows both easiest and hardest classes based on best submitted entry performance across multiple years.",
          "visual_anchor": "entire bundle of pink-tipped cylindrical objects",
          "text_evidence": "For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the average image area occupied by these pink-tipped objects relate to their detection performance on ILSVRC2014 validation?",
          "answer": "The performance would correspond to a specific dot position in Figure 13, where x-axis represents the average fraction of image area occupied by instances of this object class, and y-axis shows the best detection performance achieved.",
          "visual_anchor": "pink-tipped cylindrical objects occupying the frame",
          "text_evidence": "Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. 'Optimistic' performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the pink-tipped cylindrical objects shown represent one of the ten randomly selected object categories from ILSVRC2012-2014?",
        "answer": "Yes, these objects represent one of the ten randomly selected object categories shown in Figure 11 (top), which displays a random set from the ILSVRC dataset used for evaluating classification and detection tasks.",
        "visual_anchor": "pink/red colored tips arranged in circular bundle",
        "text_evidence": "Figure 11 (top) shows a random set of 10 of them.",
        "query_type": "value_context"
      },
      {
        "query": "Are these bundled pink-tipped objects classified among the easiest or hardest categories based on optimistic ILSVRC2012-2014 performance?",
        "answer": "The classification difficulty cannot be determined from the figure alone. The referenced analysis shows both easiest and hardest classes based on best submitted entry performance across multiple years.",
        "visual_anchor": "entire bundle of pink-tipped cylindrical objects",
        "text_evidence": "For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these 'optimistic' results we show the easiest and harder classes for each task.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the average image area occupied by these pink-tipped objects relate to their detection performance on ILSVRC2014 validation?",
        "answer": "The performance would correspond to a specific dot position in Figure 13, where x-axis represents the average fraction of image area occupied by instances of this object class, and y-axis shows the best detection performance achieved.",
        "visual_anchor": "pink-tipped cylindrical objects occupying the frame",
        "text_evidence": "Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. 'Optimistic' performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "photo",
    "visual_elements": [
      "bundle of cylindrical objects",
      "pink/red colored tips",
      "black/dark colored ends",
      "circular arrangement of tips"
    ]
  },
  {
    "doc_id": "1409.0575",
    "figure_id": "1409.0575_fig_93",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg",
    "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue bars at four x-axis positions: None, Low, None, Low\",\n    \"Black circles showing average accuracy\",\n    \"Error bars showing 95% confidence intervals\",\n    \"Y-axis ranging from approximately 0.86 to 0.98\",\n    \"X-axis labeled with 'Artificial' and 'Natural' categories\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue bars show increasing accuracy from 0.90 to 0.97 despite object scale being normalized within each bin?\",\n      \"answer\": \"The scale normalization controls for size effects within each bin, but the increasing trend reflects the underlying difficulty differences between artificial and natural categories. The normalization ensures that the observed accuracy differences are not simply due to object size variations.\",\n      \"visual_anchor\": \"Blue bars increasing from approximately 0.90 (leftmost) to 0.97 (rightmost)\",\n      \"text_evidence\": \"We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the black circles positioned above the blue bars confirm that model accuracy varies across object classes within bins?\",\n      \"answer\": \"Yes, the black circles represent the average accuracy of the model on all object classes falling into each bin, showing variation across different bins. This demonstrates that accuracy differs depending on which classes are grouped together.\",\n      \"visual_anchor\": \"Black circles positioned at approximately 0.90, 0.93, 0.96, and 0.97 above respective blue bars\",\n      \"text_evidence\": \"The black circle is the average accuracy of the model on all object classes that fall into each bin.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are some bins missing color bars despite having blue bars and black circles at those positions?\",\n      \"answer\": \"Some bins have fewer than 5 object classes remaining after scale normalization, which is insufficient for displaying color bars. For example, the XL real-world object detection bin had only 3 classes (airplane, bus, train) and no classes remained after normalization.\",\n      \"visual_anchor\": \"Blue bars present at all four x-axis positions while color bars are selectively missing\",\n      \"text_evidence\": \"Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue bars at four x-axis positions: None, Low, None, Low",
        "Black circles showing average accuracy",
        "Error bars showing 95% confidence intervals",
        "Y-axis ranging from approximately 0.86 to 0.98",
        "X-axis labeled with 'Artificial' and 'Natural' categories"
      ],
      "queries": [
        {
          "query": "Why do the blue bars show increasing accuracy from 0.90 to 0.97 despite object scale being normalized within each bin?",
          "answer": "The scale normalization controls for size effects within each bin, but the increasing trend reflects the underlying difficulty differences between artificial and natural categories. The normalization ensures that the observed accuracy differences are not simply due to object size variations.",
          "visual_anchor": "Blue bars increasing from approximately 0.90 (leftmost) to 0.97 (rightmost)",
          "text_evidence": "We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the black circles positioned above the blue bars confirm that model accuracy varies across object classes within bins?",
          "answer": "Yes, the black circles represent the average accuracy of the model on all object classes falling into each bin, showing variation across different bins. This demonstrates that accuracy differs depending on which classes are grouped together.",
          "visual_anchor": "Black circles positioned at approximately 0.90, 0.93, 0.96, and 0.97 above respective blue bars",
          "text_evidence": "The black circle is the average accuracy of the model on all object classes that fall into each bin.",
          "query_type": "value_context"
        },
        {
          "query": "Why are some bins missing color bars despite having blue bars and black circles at those positions?",
          "answer": "Some bins have fewer than 5 object classes remaining after scale normalization, which is insufficient for displaying color bars. For example, the XL real-world object detection bin had only 3 classes (airplane, bus, train) and no classes remained after normalization.",
          "visual_anchor": "Blue bars present at all four x-axis positions while color bars are selectively missing",
          "text_evidence": "Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue bars show increasing accuracy from 0.90 to 0.97 despite object scale being normalized within each bin?",
        "answer": "The scale normalization controls for size effects within each bin, but the increasing trend reflects the underlying difficulty differences between artificial and natural categories. The normalization ensures that the observed accuracy differences are not simply due to object size variations.",
        "visual_anchor": "Blue bars increasing from approximately 0.90 (leftmost) to 0.97 (rightmost)",
        "text_evidence": "We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the black circles positioned above the blue bars confirm that model accuracy varies across object classes within bins?",
        "answer": "Yes, the black circles represent the average accuracy of the model on all object classes falling into each bin, showing variation across different bins. This demonstrates that accuracy differs depending on which classes are grouped together.",
        "visual_anchor": "Black circles positioned at approximately 0.90, 0.93, 0.96, and 0.97 above respective blue bars",
        "text_evidence": "The black circle is the average accuracy of the model on all object classes that fall into each bin.",
        "query_type": "value_context"
      },
      {
        "query": "Why are some bins missing color bars despite having blue bars and black circles at those positions?",
        "answer": "Some bins have fewer than 5 object classes remaining after scale normalization, which is insufficient for displaying color bars. For example, the XL real-world object detection bin had only 3 classes (airplane, bus, train) and no classes remained after normalization.",
        "visual_anchor": "Blue bars present at all four x-axis positions while color bars are selectively missing",
        "text_evidence": "Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue bars at four x-axis positions: None, Low, None, Low",
      "Black circles showing average accuracy",
      "Error bars showing 95% confidence intervals",
      "Y-axis ranging from approximately 0.86 to 0.98",
      "X-axis labeled with 'Artificial' and 'Natural' categories"
    ]
  },
  {
    "doc_id": "1412.3756",
    "figure_id": "1412.3756_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg",
    "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"red curve with peak at SAT score ~400\",\n    \"blue curve with peak at SAT score ~550\",\n    \"black curve with peak at SAT score ~475\",\n    \"y-axis showing density from 0 to 0.008\",\n    \"x-axis showing hypothetical SAT scores from 200 to 800\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black curve peak at 475 when male scores average 400 and female scores average 550?\",\n      \"answer\": \"The black curve represents the fully repaired distribution with μ=475, σ=75, which is the median distribution used to eliminate disparate impact between the original male (red, μ=400) and female (blue, μ=550) distributions.\",\n      \"visual_anchor\": \"black curve peak at SAT score 475\",\n      \"text_evidence\": \"The resulting fully repaired data is the distribution in black, with μ=475, σ=75\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the red curve's narrower spread with σ=50 compared to the blue curve's σ=100 affect repair?\",\n      \"answer\": \"The narrower red distribution (male, σ=50) means male students at the 95th percentile (score 500) are mapped to score 625 in the repaired distribution, a larger upward shift than for the wider female distribution, demonstrating how different variances impact quantile-based repair.\",\n      \"visual_anchor\": \"red curve narrower width compared to blue curve width\",\n      \"text_evidence\": \"Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the blue curve's higher peak score of 550 explain why women lose 125 points at their 95th percentile?\",\n      \"answer\": \"Yes, because the repair process moves inverse quantile distributions toward the median (μ=475), women with original scores of 750 (95th percentile of blue distribution with μ=550) are mapped down to 625 in the repaired distribution, resulting in a 125-point reduction.\",\n      \"visual_anchor\": \"blue curve peak at SAT score 550\",\n      \"text_evidence\": \"while women with scores of 625 in Ȳ originally had scores of 750\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "red curve with peak at SAT score ~400",
        "blue curve with peak at SAT score ~550",
        "black curve with peak at SAT score ~475",
        "y-axis showing density from 0 to 0.008",
        "x-axis showing hypothetical SAT scores from 200 to 800"
      ],
      "queries": [
        {
          "query": "Why does the black curve peak at 475 when male scores average 400 and female scores average 550?",
          "answer": "The black curve represents the fully repaired distribution with μ=475, σ=75, which is the median distribution used to eliminate disparate impact between the original male (red, μ=400) and female (blue, μ=550) distributions.",
          "visual_anchor": "black curve peak at SAT score 475",
          "text_evidence": "The resulting fully repaired data is the distribution in black, with μ=475, σ=75",
          "query_type": "value_context"
        },
        {
          "query": "How does the red curve's narrower spread with σ=50 compared to the blue curve's σ=100 affect repair?",
          "answer": "The narrower red distribution (male, σ=50) means male students at the 95th percentile (score 500) are mapped to score 625 in the repaired distribution, a larger upward shift than for the wider female distribution, demonstrating how different variances impact quantile-based repair.",
          "visual_anchor": "red curve narrower width compared to blue curve width",
          "text_evidence": "Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the blue curve's higher peak score of 550 explain why women lose 125 points at their 95th percentile?",
          "answer": "Yes, because the repair process moves inverse quantile distributions toward the median (μ=475), women with original scores of 750 (95th percentile of blue distribution with μ=550) are mapped down to 625 in the repaired distribution, resulting in a 125-point reduction.",
          "visual_anchor": "blue curve peak at SAT score 550",
          "text_evidence": "while women with scores of 625 in Ȳ originally had scores of 750",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black curve peak at 475 when male scores average 400 and female scores average 550?",
        "answer": "The black curve represents the fully repaired distribution with μ=475, σ=75, which is the median distribution used to eliminate disparate impact between the original male (red, μ=400) and female (blue, μ=550) distributions.",
        "visual_anchor": "black curve peak at SAT score 475",
        "text_evidence": "The resulting fully repaired data is the distribution in black, with μ=475, σ=75",
        "query_type": "value_context"
      },
      {
        "query": "How does the red curve's narrower spread with σ=50 compared to the blue curve's σ=100 affect repair?",
        "answer": "The narrower red distribution (male, σ=50) means male students at the 95th percentile (score 500) are mapped to score 625 in the repaired distribution, a larger upward shift than for the wider female distribution, demonstrating how different variances impact quantile-based repair.",
        "visual_anchor": "red curve narrower width compared to blue curve width",
        "text_evidence": "Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the blue curve's higher peak score of 550 explain why women lose 125 points at their 95th percentile?",
        "answer": "Yes, because the repair process moves inverse quantile distributions toward the median (μ=475), women with original scores of 750 (95th percentile of blue distribution with μ=550) are mapped down to 625 in the repaired distribution, resulting in a 125-point reduction.",
        "visual_anchor": "blue curve peak at SAT score 550",
        "text_evidence": "while women with scores of 625 in Ȳ originally had scores of 750",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "red curve with peak at SAT score ~400",
      "blue curve with peak at SAT score ~550",
      "black curve with peak at SAT score ~475",
      "y-axis showing density from 0 to 0.008",
      "x-axis showing hypothetical SAT scores from 200 to 800"
    ]
  },
  {
    "doc_id": "1412.3756",
    "figure_id": "1412.3756_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg",
    "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three faceted scatter plots labeled German Credit Data, Adult Income Data, and Ricci Data\",\n    \"Three methods shown by different markers: GNB (dark red circles), SVM (green triangles), LR (blue squares)\",\n    \"Horizontal gray line at Fairness (DI) = 0.8\",\n    \"Vertical gray line representing BER threshold around 0.47\",\n    \"X-axis: Lack of Predictability (BER) ranging from 0.30 to 0.50\",\n    \"Y-axis: Fairness (DI) ranging from approximately 0.4 to 1.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do no points appear above the vertical BER threshold yet below the 0.8 horizontal line?\",\n      \"answer\": \"The certification algorithm guarantees that points to the right of the BER threshold are also above τ = 0.8, the threshold for legal disparate impact, ensuring no violations occur in that region.\",\n      \"visual_anchor\": \"Vertical gray line at BER threshold (~0.47) and horizontal gray line at Fairness (DI) = 0.8\",\n      \"text_evidence\": \"The certification algorithm guarantees that points to the right of the BER threshold are also above $\\\\tau = 0 . 8$ , the threshold for legal disparate impact.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the BER value relate to disparate impact when predicting class from non-protected attributes?\",\n      \"answer\": \"The BER from predicting the protected attribute is compared to the disparate impact (DI) measured when classifiers predict the class given non-protected attributes, establishing the relationship between predictability and fairness.\",\n      \"visual_anchor\": \"Scatter points showing BER (x-axis) versus Fairness DI (y-axis) relationship across all three panels\",\n      \"text_evidence\": \"The resulting BER is compared to $\\\\mathsf { D } \\\\mathsf { I } ( g )$ where $g : Y \\\\to C ,$ i.e., the disparate impact value as measured when some classifier attempts to predict the class given the non-protected attributes.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the clustering of points near the upper-right region in all panels support the confusion matrix analysis?\",\n      \"answer\": \"Yes, the clustering near high BER and high fairness values aligns with the confusion matrix analysis of classifier g, showing that lower predictability of protected attributes correlates with better fairness outcomes.\",\n      \"visual_anchor\": \"Dense clustering of GNB (red circles), SVM (green triangles), and LR (blue squares) near BER=0.50 and Fairness=1.0\",\n      \"text_evidence\": \"Consider the confusion matrix associated with $g .$ , depicted in Table 2.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three faceted scatter plots labeled German Credit Data, Adult Income Data, and Ricci Data",
        "Three methods shown by different markers: GNB (dark red circles), SVM (green triangles), LR (blue squares)",
        "Horizontal gray line at Fairness (DI) = 0.8",
        "Vertical gray line representing BER threshold around 0.47",
        "X-axis: Lack of Predictability (BER) ranging from 0.30 to 0.50",
        "Y-axis: Fairness (DI) ranging from approximately 0.4 to 1.0"
      ],
      "queries": [
        {
          "query": "Why do no points appear above the vertical BER threshold yet below the 0.8 horizontal line?",
          "answer": "The certification algorithm guarantees that points to the right of the BER threshold are also above τ = 0.8, the threshold for legal disparate impact, ensuring no violations occur in that region.",
          "visual_anchor": "Vertical gray line at BER threshold (~0.47) and horizontal gray line at Fairness (DI) = 0.8",
          "text_evidence": "The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the BER value relate to disparate impact when predicting class from non-protected attributes?",
          "answer": "The BER from predicting the protected attribute is compared to the disparate impact (DI) measured when classifiers predict the class given non-protected attributes, establishing the relationship between predictability and fairness.",
          "visual_anchor": "Scatter points showing BER (x-axis) versus Fairness DI (y-axis) relationship across all three panels",
          "text_evidence": "The resulting BER is compared to $\\mathsf { D } \\mathsf { I } ( g )$ where $g : Y \\to C ,$ i.e., the disparate impact value as measured when some classifier attempts to predict the class given the non-protected attributes.",
          "query_type": "value_context"
        },
        {
          "query": "Does the clustering of points near the upper-right region in all panels support the confusion matrix analysis?",
          "answer": "Yes, the clustering near high BER and high fairness values aligns with the confusion matrix analysis of classifier g, showing that lower predictability of protected attributes correlates with better fairness outcomes.",
          "visual_anchor": "Dense clustering of GNB (red circles), SVM (green triangles), and LR (blue squares) near BER=0.50 and Fairness=1.0",
          "text_evidence": "Consider the confusion matrix associated with $g .$ , depicted in Table 2.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do no points appear above the vertical BER threshold yet below the 0.8 horizontal line?",
        "answer": "The certification algorithm guarantees that points to the right of the BER threshold are also above τ = 0.8, the threshold for legal disparate impact, ensuring no violations occur in that region.",
        "visual_anchor": "Vertical gray line at BER threshold (~0.47) and horizontal gray line at Fairness (DI) = 0.8",
        "text_evidence": "The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the BER value relate to disparate impact when predicting class from non-protected attributes?",
        "answer": "The BER from predicting the protected attribute is compared to the disparate impact (DI) measured when classifiers predict the class given non-protected attributes, establishing the relationship between predictability and fairness.",
        "visual_anchor": "Scatter points showing BER (x-axis) versus Fairness DI (y-axis) relationship across all three panels",
        "text_evidence": "The resulting BER is compared to $\\mathsf { D } \\mathsf { I } ( g )$ where $g : Y \\to C ,$ i.e., the disparate impact value as measured when some classifier attempts to predict the class given the non-protected attributes.",
        "query_type": "value_context"
      },
      {
        "query": "Does the clustering of points near the upper-right region in all panels support the confusion matrix analysis?",
        "answer": "Yes, the clustering near high BER and high fairness values aligns with the confusion matrix analysis of classifier g, showing that lower predictability of protected attributes correlates with better fairness outcomes.",
        "visual_anchor": "Dense clustering of GNB (red circles), SVM (green triangles), and LR (blue squares) near BER=0.50 and Fairness=1.0",
        "text_evidence": "Consider the confusion matrix associated with $g .$ , depicted in Table 2.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three faceted scatter plots labeled German Credit Data, Adult Income Data, and Ricci Data",
      "Three methods shown by different markers: GNB (dark red circles), SVM (green triangles), LR (blue squares)",
      "Horizontal gray line at Fairness (DI) = 0.8",
      "Vertical gray line representing BER threshold around 0.47",
      "X-axis: Lack of Predictability (BER) ranging from 0.30 to 0.50",
      "Y-axis: Fairness (DI) ranging from approximately 0.4 to 1.0"
    ]
  },
  {
    "doc_id": "1412.3756",
    "figure_id": "1412.3756_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
    "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red vertical line at DI=0.8 (legal threshold)\",\n    \"Green vertical line at DI=1.0 (full fairness)\",\n    \"Pink shaded region for illegal DI values\",\n    \"Green shaded region for legal DI values\",\n    \"Scatter points with color gradient showing repair amount\",\n    \"Dashed trend lines connecting points\",\n    \"Three rows: Adult Income, German Credit, Ricci Data\",\n    \"Two columns: Combinatorial Repair, Geometric Repair\",\n    \"Utility (1-BER) on y-axis\",\n    \"Disparate Impact on x-axis\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?\",\n      \"answer\": \"Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.\",\n      \"visual_anchor\": \"Dashed trend lines connecting scatter points showing downward slope from left to right\",\n      \"text_evidence\": \"The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do both repair methods show points in the pink region left of the red line at 0.8?\",\n      \"answer\": \"The pink region represents illegal disparate impact values (DI < 0.8). Points in this region indicate partial repair states that have not yet achieved the legal threshold of DI ≥ 0.8.\",\n      \"visual_anchor\": \"Pink shaded region left of red vertical line at DI=0.8\",\n      \"text_evidence\": \"Recall that only points with $\\\\mathsf { D } \\\\mathsf { I } \\\\ge \\\\tau = 0 . 8$ are legal.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the German Credit middle row results validate the SVM classification approach for partial repair processes?\",\n      \"answer\": \"Yes, the German Credit results show clear utility-fairness tradeoffs with points transitioning from the pink illegal region through the red threshold line to the green legal region, validating the SVM's effectiveness in controlling the repair process.\",\n      \"visual_anchor\": \"Middle row scatter points and trend lines for German Credit data\",\n      \"text_evidence\": \"Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red vertical line at DI=0.8 (legal threshold)",
        "Green vertical line at DI=1.0 (full fairness)",
        "Pink shaded region for illegal DI values",
        "Green shaded region for legal DI values",
        "Scatter points with color gradient showing repair amount",
        "Dashed trend lines connecting points",
        "Three rows: Adult Income, German Credit, Ricci Data",
        "Two columns: Combinatorial Repair, Geometric Repair",
        "Utility (1-BER) on y-axis",
        "Disparate Impact on x-axis"
      ],
      "queries": [
        {
          "query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
          "answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
          "visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
          "text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do both repair methods show points in the pink region left of the red line at 0.8?",
          "answer": "The pink region represents illegal disparate impact values (DI < 0.8). Points in this region indicate partial repair states that have not yet achieved the legal threshold of DI ≥ 0.8.",
          "visual_anchor": "Pink shaded region left of red vertical line at DI=0.8",
          "text_evidence": "Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal.",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the German Credit middle row results validate the SVM classification approach for partial repair processes?",
          "answer": "Yes, the German Credit results show clear utility-fairness tradeoffs with points transitioning from the pink illegal region through the red threshold line to the green legal region, validating the SVM's effectiveness in controlling the repair process.",
          "visual_anchor": "Middle row scatter points and trend lines for German Credit data",
          "text_evidence": "Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the utility decay pattern visible across all scatter points support the expected tradeoff between fairness and utility?",
        "answer": "Yes, the scatter points show declining utility (from ~1.0 to ~0.5-0.9) as disparate impact increases toward the green line at DI=1.0, demonstrating the expected decay over utility as fairness increases.",
        "visual_anchor": "Dashed trend lines connecting scatter points showing downward slope from left to right",
        "text_evidence": "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do both repair methods show points in the pink region left of the red line at 0.8?",
        "answer": "The pink region represents illegal disparate impact values (DI < 0.8). Points in this region indicate partial repair states that have not yet achieved the legal threshold of DI ≥ 0.8.",
        "visual_anchor": "Pink shaded region left of red vertical line at DI=0.8",
        "text_evidence": "Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal.",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the German Credit middle row results validate the SVM classification approach for partial repair processes?",
        "answer": "Yes, the German Credit results show clear utility-fairness tradeoffs with points transitioning from the pink illegal region through the red threshold line to the green legal region, validating the SVM's effectiveness in controlling the repair process.",
        "visual_anchor": "Middle row scatter points and trend lines for German Credit data",
        "text_evidence": "Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red vertical line at DI=0.8 (legal threshold)",
      "Green vertical line at DI=1.0 (full fairness)",
      "Pink shaded region for illegal DI values",
      "Green shaded region for legal DI values",
      "Scatter points with color gradient showing repair amount",
      "Dashed trend lines connecting points",
      "Three rows: Adult Income, German Credit, Ricci Data",
      "Two columns: Combinatorial Repair, Geometric Repair",
      "Utility (1-BER) on y-axis",
      "Disparate Impact on x-axis"
    ]
  },
  {
    "doc_id": "1412.3756",
    "figure_id": "1412.3756_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg",
    "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Red line with circles labeled 'Gender'\", \"Green line with triangles labeled 'Race'\", \"Blue line with squares labeled 'Race and Gender'\", \"Y-axis showing Utility (1-BER) from 0.714 to 0.738\", \"X-axis showing Disparate Impact from 0.7 to 1.05\", \"All three curves showing downward trends\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue Race and Gender curve reach utility 0.714 at disparate impact 1.05, near the maximum individual losses?\",\n      \"answer\": \"The joint distribution utility loss is close to the maximum of the utility loss over each protected attribute considered individually, as shown by the blue curve ending near the lowest points of the red and green curves.\",\n      \"visual_anchor\": \"Blue square line ending at approximately (1.05, 0.714), positioned near the convergence of red and green lines\",\n      \"text_evidence\": \"The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the Race green triangle curve maintaining utility above 0.732 across all disparate impacts reflect differences in repair performance?\",\n      \"answer\": \"Yes, the Race attribute maintains consistently higher utility than Gender or Race and Gender combined, demonstrating substantial differences in repair algorithm performance depending on the specific protected attributes chosen.\",\n      \"visual_anchor\": \"Green triangle line staying between 0.732 and 0.738 utility across the entire disparate impact range\",\n      \"text_evidence\": \"Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Do the red Gender and blue Race and Gender curves converging at utility 0.720 support using SVM as classifier?\",\n      \"answer\": \"The convergence demonstrates SVM classifier behavior under repair processes. The comparison uses SVM as a baseline against other methods like logistic regression, fair naive Bayes, and learned fair representations.\",\n      \"visual_anchor\": \"Red circle line and blue square line converging around disparate impact 1.0 at utility approximately 0.720\",\n      \"text_evidence\": \"Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.'s solution [26].\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red line with circles labeled 'Gender'",
        "Green line with triangles labeled 'Race'",
        "Blue line with squares labeled 'Race and Gender'",
        "Y-axis showing Utility (1-BER) from 0.714 to 0.738",
        "X-axis showing Disparate Impact from 0.7 to 1.05",
        "All three curves showing downward trends"
      ],
      "queries": [
        {
          "query": "Why does the blue Race and Gender curve reach utility 0.714 at disparate impact 1.05, near the maximum individual losses?",
          "answer": "The joint distribution utility loss is close to the maximum of the utility loss over each protected attribute considered individually, as shown by the blue curve ending near the lowest points of the red and green curves.",
          "visual_anchor": "Blue square line ending at approximately (1.05, 0.714), positioned near the convergence of red and green lines",
          "text_evidence": "The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the Race green triangle curve maintaining utility above 0.732 across all disparate impacts reflect differences in repair performance?",
          "answer": "Yes, the Race attribute maintains consistently higher utility than Gender or Race and Gender combined, demonstrating substantial differences in repair algorithm performance depending on the specific protected attributes chosen.",
          "visual_anchor": "Green triangle line staying between 0.732 and 0.738 utility across the entire disparate impact range",
          "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.",
          "query_type": "value_context"
        },
        {
          "query": "Do the red Gender and blue Race and Gender curves converging at utility 0.720 support using SVM as classifier?",
          "answer": "The convergence demonstrates SVM classifier behavior under repair processes. The comparison uses SVM as a baseline against other methods like logistic regression, fair naive Bayes, and learned fair representations.",
          "visual_anchor": "Red circle line and blue square line converging around disparate impact 1.0 at utility approximately 0.720",
          "text_evidence": "Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.'s solution [26].",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue Race and Gender curve reach utility 0.714 at disparate impact 1.05, near the maximum individual losses?",
        "answer": "The joint distribution utility loss is close to the maximum of the utility loss over each protected attribute considered individually, as shown by the blue curve ending near the lowest points of the red and green curves.",
        "visual_anchor": "Blue square line ending at approximately (1.05, 0.714), positioned near the convergence of red and green lines",
        "text_evidence": "The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the Race green triangle curve maintaining utility above 0.732 across all disparate impacts reflect differences in repair performance?",
        "answer": "Yes, the Race attribute maintains consistently higher utility than Gender or Race and Gender combined, demonstrating substantial differences in repair algorithm performance depending on the specific protected attributes chosen.",
        "visual_anchor": "Green triangle line staying between 0.732 and 0.738 utility across the entire disparate impact range",
        "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.",
        "query_type": "value_context"
      },
      {
        "query": "Do the red Gender and blue Race and Gender curves converging at utility 0.720 support using SVM as classifier?",
        "answer": "The convergence demonstrates SVM classifier behavior under repair processes. The comparison uses SVM as a baseline against other methods like logistic regression, fair naive Bayes, and learned fair representations.",
        "visual_anchor": "Red circle line and blue square line converging around disparate impact 1.0 at utility approximately 0.720",
        "text_evidence": "Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.'s solution [26].",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red line with circles labeled 'Gender'",
      "Green line with triangles labeled 'Race'",
      "Blue line with squares labeled 'Race and Gender'",
      "Y-axis showing Utility (1-BER) from 0.714 to 0.738",
      "X-axis showing Disparate Impact from 0.7 to 1.05",
      "All three curves showing downward trends"
    ]
  },
  {
    "doc_id": "1412.3756",
    "figure_id": "1412.3756_fig_5",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg",
    "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three columns: Geometric, Combinatorial, Previous Work\",\n    \"Two rows: German Credit Data, Adult Income Data\",\n    \"Blue squares labeled GNB\",\n    \"Green triangles labeled LR\",\n    \"Dark red circles labeled SVM\",\n    \"Purple squares labeled RLR\",\n    \"Pink plus signs labeled LFR\",\n    \"Orange triangles labeled FNB\",\n    \"Cyan circles labeled LR in Previous Work column\",\n    \"Accuracy on y-axis (0.3-0.8)\",\n    \"Zemel Fairness on x-axis (0.8-1.1)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the blue GNB squares clustering near 0.75-0.8 accuracy in Geometric and Combinatorial columns demonstrate flexibility with respect to classifier choice?\",\n      \"answer\": \"Yes, the consistent high accuracy of GNB (blue squares) across both repair methods, along with the varied performance of SVM (red circles) and LR (green triangles), demonstrates that the method can be flexible with respect to the chosen classifier.\",\n      \"visual_anchor\": \"Blue GNB squares at approximately 0.75-0.8 accuracy in both Geometric and Combinatorial columns\",\n      \"text_evidence\": \"Figure 5 shows that our method can be flexible with respect to the chosen classifier.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the red SVM circles span from 0.49 to 0.73 accuracy in Geometric repairs, illustrating substantial performance differences?\",\n      \"answer\": \"The wide accuracy range of SVM circles (0.49-0.73) reflects the substantial difference in repair algorithm performance depending on specific algorithms chosen, demonstrating the need for systematic study of the relationship between dataset features, algorithms, and repair performance.\",\n      \"visual_anchor\": \"Red SVM circles ranging from approximately 0.49 to 0.73 accuracy in the Geometric column\",\n      \"text_evidence\": \"Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How do the fairness-accuracy tradeoffs visible across lambda increments in both Geometric and Combinatorial columns align with the partial repair methodology?\",\n      \"answer\": \"The scattered points across different fairness values (0.8-1.1) represent different lambda values from 0 to 1 at 0.1 increments, showing how partial repairs balance fairness and accuracy tradeoffs for all three classifiers (SVM, LR, GNB).\",\n      \"visual_anchor\": \"Multiple points for each method (SVM, LR, GNB) distributed across fairness values 0.8-1.1 in Geometric and Combinatorial columns\",\n      \"text_evidence\": \"Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\\\lambda \\\\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three columns: Geometric, Combinatorial, Previous Work",
        "Two rows: German Credit Data, Adult Income Data",
        "Blue squares labeled GNB",
        "Green triangles labeled LR",
        "Dark red circles labeled SVM",
        "Purple squares labeled RLR",
        "Pink plus signs labeled LFR",
        "Orange triangles labeled FNB",
        "Cyan circles labeled LR in Previous Work column",
        "Accuracy on y-axis (0.3-0.8)",
        "Zemel Fairness on x-axis (0.8-1.1)"
      ],
      "queries": [
        {
          "query": "Do the blue GNB squares clustering near 0.75-0.8 accuracy in Geometric and Combinatorial columns demonstrate flexibility with respect to classifier choice?",
          "answer": "Yes, the consistent high accuracy of GNB (blue squares) across both repair methods, along with the varied performance of SVM (red circles) and LR (green triangles), demonstrates that the method can be flexible with respect to the chosen classifier.",
          "visual_anchor": "Blue GNB squares at approximately 0.75-0.8 accuracy in both Geometric and Combinatorial columns",
          "text_evidence": "Figure 5 shows that our method can be flexible with respect to the chosen classifier.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the red SVM circles span from 0.49 to 0.73 accuracy in Geometric repairs, illustrating substantial performance differences?",
          "answer": "The wide accuracy range of SVM circles (0.49-0.73) reflects the substantial difference in repair algorithm performance depending on specific algorithms chosen, demonstrating the need for systematic study of the relationship between dataset features, algorithms, and repair performance.",
          "visual_anchor": "Red SVM circles ranging from approximately 0.49 to 0.73 accuracy in the Geometric column",
          "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How do the fairness-accuracy tradeoffs visible across lambda increments in both Geometric and Combinatorial columns align with the partial repair methodology?",
          "answer": "The scattered points across different fairness values (0.8-1.1) represent different lambda values from 0 to 1 at 0.1 increments, showing how partial repairs balance fairness and accuracy tradeoffs for all three classifiers (SVM, LR, GNB).",
          "visual_anchor": "Multiple points for each method (SVM, LR, GNB) distributed across fairness values 0.8-1.1 in Geometric and Combinatorial columns",
          "text_evidence": "Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the blue GNB squares clustering near 0.75-0.8 accuracy in Geometric and Combinatorial columns demonstrate flexibility with respect to classifier choice?",
        "answer": "Yes, the consistent high accuracy of GNB (blue squares) across both repair methods, along with the varied performance of SVM (red circles) and LR (green triangles), demonstrates that the method can be flexible with respect to the chosen classifier.",
        "visual_anchor": "Blue GNB squares at approximately 0.75-0.8 accuracy in both Geometric and Combinatorial columns",
        "text_evidence": "Figure 5 shows that our method can be flexible with respect to the chosen classifier.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the red SVM circles span from 0.49 to 0.73 accuracy in Geometric repairs, illustrating substantial performance differences?",
        "answer": "The wide accuracy range of SVM circles (0.49-0.73) reflects the substantial difference in repair algorithm performance depending on specific algorithms chosen, demonstrating the need for systematic study of the relationship between dataset features, algorithms, and repair performance.",
        "visual_anchor": "Red SVM circles ranging from approximately 0.49 to 0.73 accuracy in the Geometric column",
        "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How do the fairness-accuracy tradeoffs visible across lambda increments in both Geometric and Combinatorial columns align with the partial repair methodology?",
        "answer": "The scattered points across different fairness values (0.8-1.1) represent different lambda values from 0 to 1 at 0.1 increments, showing how partial repairs balance fairness and accuracy tradeoffs for all three classifiers (SVM, LR, GNB).",
        "visual_anchor": "Multiple points for each method (SVM, LR, GNB) distributed across fairness values 0.8-1.1 in Geometric and Combinatorial columns",
        "text_evidence": "Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three columns: Geometric, Combinatorial, Previous Work",
      "Two rows: German Credit Data, Adult Income Data",
      "Blue squares labeled GNB",
      "Green triangles labeled LR",
      "Dark red circles labeled SVM",
      "Purple squares labeled RLR",
      "Pink plus signs labeled LFR",
      "Orange triangles labeled FNB",
      "Cyan circles labeled LR in Previous Work column",
      "Accuracy on y-axis (0.3-0.8)",
      "Zemel Fairness on x-axis (0.8-1.1)"
    ]
  },
  {
    "doc_id": "1511.00830",
    "figure_id": "1511.00830_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
    "caption": "Figure 1: Unsupervised model",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"node S (top-left)\", \"node Z (top-right)\", \"node X (bottom-center)\", \"arrow from S to X\", \"arrow from Z to X\", \"label N at bottom-right\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does node S point directly to X rather than through Z in this unsupervised representation learning model?\",\n      \"answer\": \"In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.\",\n      \"visual_anchor\": \"arrow from node S (top-left) directly to node X (bottom-center)\",\n      \"text_evidence\": \"Uninformative dimensions are often called \\\"noise\\\" or \\\"nuisance variables\\\" while informative dimensions are usually called latent or hidden factors of variation.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the arrow from Z to X indicate that hidden factors are informative for representation learning tasks?\",\n      \"answer\": \"Yes, the arrow from Z to X shows that Z represents latent or hidden factors of variation that are informative for the task. The model aims to find representations that preserve these informative dimensions while removing uninformative ones.\",\n      \"visual_anchor\": \"arrow from node Z (top-right) to node X (bottom-center)\",\n      \"text_evidence\": \"In \\\"Representation Learning\\\" one tries to find representations of the data that are informative for a particular task while removing the factors of variation that are uninformative\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What does the convergence of both S and Z arrows at node X represent for machine learning algorithms?\",\n      \"answer\": \"The convergence at X represents the observed data that results from both informative latent factors (Z) and uninformative nuisance variables (S). Many machine learning algorithms work to separate these influences, keeping informative dimensions while removing detrimental noise.\",\n      \"visual_anchor\": \"node X (bottom-center) receiving arrows from both S and Z\",\n      \"text_evidence\": \"Many machine learning algorithms can be understood in this way: pri\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "node S (top-left)",
        "node Z (top-right)",
        "node X (bottom-center)",
        "arrow from S to X",
        "arrow from Z to X",
        "label N at bottom-right"
      ],
      "queries": [
        {
          "query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
          "answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
          "visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
          "text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the arrow from Z to X indicate that hidden factors are informative for representation learning tasks?",
          "answer": "Yes, the arrow from Z to X shows that Z represents latent or hidden factors of variation that are informative for the task. The model aims to find representations that preserve these informative dimensions while removing uninformative ones.",
          "visual_anchor": "arrow from node Z (top-right) to node X (bottom-center)",
          "text_evidence": "In \"Representation Learning\" one tries to find representations of the data that are informative for a particular task while removing the factors of variation that are uninformative",
          "query_type": "value_context"
        },
        {
          "query": "What does the convergence of both S and Z arrows at node X represent for machine learning algorithms?",
          "answer": "The convergence at X represents the observed data that results from both informative latent factors (Z) and uninformative nuisance variables (S). Many machine learning algorithms work to separate these influences, keeping informative dimensions while removing detrimental noise.",
          "visual_anchor": "node X (bottom-center) receiving arrows from both S and Z",
          "text_evidence": "Many machine learning algorithms can be understood in this way: pri",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does node S point directly to X rather than through Z in this unsupervised representation learning model?",
        "answer": "In unsupervised representation learning, S represents noise or nuisance variables that directly affect the observed data X, independent of the informative latent factors Z. This direct connection models uninformative dimensions that are typically detrimental for the task under consideration.",
        "visual_anchor": "arrow from node S (top-left) directly to node X (bottom-center)",
        "text_evidence": "Uninformative dimensions are often called \"noise\" or \"nuisance variables\" while informative dimensions are usually called latent or hidden factors of variation.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the arrow from Z to X indicate that hidden factors are informative for representation learning tasks?",
        "answer": "Yes, the arrow from Z to X shows that Z represents latent or hidden factors of variation that are informative for the task. The model aims to find representations that preserve these informative dimensions while removing uninformative ones.",
        "visual_anchor": "arrow from node Z (top-right) to node X (bottom-center)",
        "text_evidence": "In \"Representation Learning\" one tries to find representations of the data that are informative for a particular task while removing the factors of variation that are uninformative",
        "query_type": "value_context"
      },
      {
        "query": "What does the convergence of both S and Z arrows at node X represent for machine learning algorithms?",
        "answer": "The convergence at X represents the observed data that results from both informative latent factors (Z) and uninformative nuisance variables (S). Many machine learning algorithms work to separate these influences, keeping informative dimensions while removing detrimental noise.",
        "visual_anchor": "node X (bottom-center) receiving arrows from both S and Z",
        "text_evidence": "Many machine learning algorithms can be understood in this way: pri",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "node S (top-left)",
      "node Z (top-right)",
      "node X (bottom-center)",
      "arrow from S to X",
      "arrow from Z to X",
      "label N at bottom-right"
    ]
  },
  {
    "doc_id": "1511.00830",
    "figure_id": "1511.00830_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg",
    "caption": "Figure 2: Semi-supervised model",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"node s (leftmost)\", \"node y (top left)\", \"node z2 (top right)\", \"node z1 (center right)\", \"node x (bottom, shaded)\", \"directed edges connecting nodes\", \"label N at bottom\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the shaded node x receive arrows from both z1 and s in this generative process?\",\n      \"answer\": \"The generative process defines p_θ(x|z,s), meaning x is generated conditioned on both the latent variable z (modeled through z1) and the observed variable s representing variations to remove.\",\n      \"visual_anchor\": \"shaded node x at bottom with incoming arrows from z1 and s\",\n      \"text_evidence\": \"This generative process can be formally defined as: where $p _ { \\\\theta } ( \\\\mathbf { x } | \\\\mathbf { z } , \\\\mathbf { s } )$ is an appropriate probabi\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the independent node z2 at the top right relate to modeling remaining information after removing undesired variations?\",\n      \"answer\": \"The continuous latent variable z (represented by z2 and z1) models all the remaining information after the observed variable s accounts for variations that need to be removed, as they are defined as independent sources.\",\n      \"visual_anchor\": \"node z2 at top right, separate from s\",\n      \"text_evidence\": \"admits two distinct (independent) \\\"sources\\\"; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does node y lack any outgoing arrows to x despite being part of the semi-supervised model?\",\n      \"answer\": \"In the semi-supervised formulation, y influences the latent structure through z1 but does not directly generate x. The model factors out undesired variations s while y provides supervisory information at the latent level.\",\n      \"visual_anchor\": \"node y at top left with arrow to z1 but no direct path to x\",\n      \"text_evidence\": \"Factoring out undesired variations from the data can be easily formulated as a general probabilistic model\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "node s (leftmost)",
        "node y (top left)",
        "node z2 (top right)",
        "node z1 (center right)",
        "node x (bottom, shaded)",
        "directed edges connecting nodes",
        "label N at bottom"
      ],
      "queries": [
        {
          "query": "Why does the shaded node x receive arrows from both z1 and s in this generative process?",
          "answer": "The generative process defines p_θ(x|z,s), meaning x is generated conditioned on both the latent variable z (modeled through z1) and the observed variable s representing variations to remove.",
          "visual_anchor": "shaded node x at bottom with incoming arrows from z1 and s",
          "text_evidence": "This generative process can be formally defined as: where $p _ { \\theta } ( \\mathbf { x } | \\mathbf { z } , \\mathbf { s } )$ is an appropriate probabi",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the independent node z2 at the top right relate to modeling remaining information after removing undesired variations?",
          "answer": "The continuous latent variable z (represented by z2 and z1) models all the remaining information after the observed variable s accounts for variations that need to be removed, as they are defined as independent sources.",
          "visual_anchor": "node z2 at top right, separate from s",
          "text_evidence": "admits two distinct (independent) \"sources\"; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does node y lack any outgoing arrows to x despite being part of the semi-supervised model?",
          "answer": "In the semi-supervised formulation, y influences the latent structure through z1 but does not directly generate x. The model factors out undesired variations s while y provides supervisory information at the latent level.",
          "visual_anchor": "node y at top left with arrow to z1 but no direct path to x",
          "text_evidence": "Factoring out undesired variations from the data can be easily formulated as a general probabilistic model",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the shaded node x receive arrows from both z1 and s in this generative process?",
        "answer": "The generative process defines p_θ(x|z,s), meaning x is generated conditioned on both the latent variable z (modeled through z1) and the observed variable s representing variations to remove.",
        "visual_anchor": "shaded node x at bottom with incoming arrows from z1 and s",
        "text_evidence": "This generative process can be formally defined as: where $p _ { \\theta } ( \\mathbf { x } | \\mathbf { z } , \\mathbf { s } )$ is an appropriate probabi",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the independent node z2 at the top right relate to modeling remaining information after removing undesired variations?",
        "answer": "The continuous latent variable z (represented by z2 and z1) models all the remaining information after the observed variable s accounts for variations that need to be removed, as they are defined as independent sources.",
        "visual_anchor": "node z2 at top right, separate from s",
        "text_evidence": "admits two distinct (independent) \"sources\"; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does node y lack any outgoing arrows to x despite being part of the semi-supervised model?",
        "answer": "In the semi-supervised formulation, y influences the latent structure through z1 but does not directly generate x. The model factors out undesired variations s while y provides supervisory information at the latent level.",
        "visual_anchor": "node y at top left with arrow to z1 but no direct path to x",
        "text_evidence": "Factoring out undesired variations from the data can be easily formulated as a general probabilistic model",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "node s (leftmost)",
      "node y (top left)",
      "node z2 (top right)",
      "node z1 (center right)",
      "node x (bottom, shaded)",
      "directed edges connecting nodes",
      "label N at bottom"
    ]
  },
  {
    "doc_id": "1511.00830",
    "figure_id": "1511.00830_fig_9",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg",
    "caption": "Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Blue bars for Random Forest (RF)\", \"Red bars for Logistic Regression (LR)\", \"Four groups labeled X, LFR, VAE, VFAE\", \"Horizontal dashed line at ~0.57\", \"Y-axis from 0.50 to 0.80\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red LR bar reach approximately 0.80 at X, given the objective requires low accuracy on S?\",\n      \"answer\": \"The red LR bar at X shows high accuracy (~0.80) on sensitive attribute S, which violates the fairness objective. This indicates that X does not provide a fair encoding, as a linear classifier can easily predict S.\",\n      \"visual_anchor\": \"Red bar at position X reaching approximately 0.80\",\n      \"text_evidence\": \"Note that the objective of a \\\"fair\\\" encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the blue RF bar's height of approximately 0.69 at LFR demonstrate lower nonlinear classification accuracy than linear?\",\n      \"answer\": \"Yes, the blue RF bar at LFR (~0.69) is higher than the red LR bar (~0.57), indicating that the nonlinear Random Forest classifier achieves higher accuracy on S than the linear Logistic Regression classifier at this evaluation scenario.\",\n      \"visual_anchor\": \"Blue RF bar at LFR position at approximately 0.69\",\n      \"text_evidence\": \"Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the VAE bars at approximately 0.66 compare to results shown for the other two datasets?\",\n      \"answer\": \"The VAE bars (both blue RF and red LR at ~0.66) are higher than most other scenarios except X, indicating moderate accuracy on S. Comparing across all three datasets shown, VAE performs better than LFR but worse than X in terms of S prediction.\",\n      \"visual_anchor\": \"Both blue and red bars at VAE position reaching approximately 0.66\",\n      \"text_evidence\": \"The results for all three datasets can be seen in Figure 3.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue bars for Random Forest (RF)",
        "Red bars for Logistic Regression (LR)",
        "Four groups labeled X, LFR, VAE, VFAE",
        "Horizontal dashed line at ~0.57",
        "Y-axis from 0.50 to 0.80"
      ],
      "queries": [
        {
          "query": "Why does the red LR bar reach approximately 0.80 at X, given the objective requires low accuracy on S?",
          "answer": "The red LR bar at X shows high accuracy (~0.80) on sensitive attribute S, which violates the fairness objective. This indicates that X does not provide a fair encoding, as a linear classifier can easily predict S.",
          "visual_anchor": "Red bar at position X reaching approximately 0.80",
          "text_evidence": "Note that the objective of a \"fair\" encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the blue RF bar's height of approximately 0.69 at LFR demonstrate lower nonlinear classification accuracy than linear?",
          "answer": "Yes, the blue RF bar at LFR (~0.69) is higher than the red LR bar (~0.57), indicating that the nonlinear Random Forest classifier achieves higher accuracy on S than the linear Logistic Regression classifier at this evaluation scenario.",
          "visual_anchor": "Blue RF bar at LFR position at approximately 0.69",
          "text_evidence": "Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the VAE bars at approximately 0.66 compare to results shown for the other two datasets?",
          "answer": "The VAE bars (both blue RF and red LR at ~0.66) are higher than most other scenarios except X, indicating moderate accuracy on S. Comparing across all three datasets shown, VAE performs better than LFR but worse than X in terms of S prediction.",
          "visual_anchor": "Both blue and red bars at VAE position reaching approximately 0.66",
          "text_evidence": "The results for all three datasets can be seen in Figure 3.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red LR bar reach approximately 0.80 at X, given the objective requires low accuracy on S?",
        "answer": "The red LR bar at X shows high accuracy (~0.80) on sensitive attribute S, which violates the fairness objective. This indicates that X does not provide a fair encoding, as a linear classifier can easily predict S.",
        "visual_anchor": "Red bar at position X reaching approximately 0.80",
        "text_evidence": "Note that the objective of a \"fair\" encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the blue RF bar's height of approximately 0.69 at LFR demonstrate lower nonlinear classification accuracy than linear?",
        "answer": "Yes, the blue RF bar at LFR (~0.69) is higher than the red LR bar (~0.57), indicating that the nonlinear Random Forest classifier achieves higher accuracy on S than the linear Logistic Regression classifier at this evaluation scenario.",
        "visual_anchor": "Blue RF bar at LFR position at approximately 0.69",
        "text_evidence": "Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the VAE bars at approximately 0.66 compare to results shown for the other two datasets?",
        "answer": "The VAE bars (both blue RF and red LR at ~0.66) are higher than most other scenarios except X, indicating moderate accuracy on S. Comparing across all three datasets shown, VAE performs better than LFR but worse than X in terms of S prediction.",
        "visual_anchor": "Both blue and red bars at VAE position reaching approximately 0.66",
        "text_evidence": "The results for all three datasets can be seen in Figure 3.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue bars for Random Forest (RF)",
      "Red bars for Logistic Regression (LR)",
      "Four groups labeled X, LFR, VAE, VFAE",
      "Horizontal dashed line at ~0.57",
      "Y-axis from 0.50 to 0.80"
    ]
  },
  {
    "doc_id": "1511.00830",
    "figure_id": "1511.00830_fig_13",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg",
    "caption": "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue scattered points representing males\", \"red scattered points representing females\", \"two-dimensional t-SNE embedding visualization\", \"overlapping clusters of blue and red points\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue and red points heavily overlap in this visualization, given the model uses both s and MMD?\",\n      \"answer\": \"The model successfully factors out domain (gender) information by incorporating both the sensitive attribute s and MMD regularization, resulting in representations where male and female samples are not separable and cluster together, demonstrating fairness through indistinguishability.\",\n      \"visual_anchor\": \"overlapping blue (male) and red (female) scattered point clusters\",\n      \"text_evidence\": \"Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the mixed distribution of blue and red points confirm successful removal of sensitive attribute information?\",\n      \"answer\": \"Yes, the visualization shows that male (blue) and female (red) samples are thoroughly intermixed without clear separation, which aligns with the model's success in making the learned representations indistinguishable with respect to gender.\",\n      \"visual_anchor\": \"intermixed blue and red points without clear cluster separation\",\n      \"text_evidence\": \"latent $\\\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What relationship between blue and red point distributions validates the two-dimensional Barnes-Hut SNE embedding approach?\",\n      \"answer\": \"The Barnes-Hut SNE embedding reveals that the latent representations z₁ achieve gender-invariance by producing overlapping distributions for males and females, allowing visual assessment of whether the model successfully disentangles sensitive attributes from the learned features.\",\n      \"visual_anchor\": \"two-dimensional scattered distribution showing blue and red point overlap\",\n      \"text_evidence\": \"In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\\\mathbf { z } _ { 1 }$ representations\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue scattered points representing males",
        "red scattered points representing females",
        "two-dimensional t-SNE embedding visualization",
        "overlapping clusters of blue and red points"
      ],
      "queries": [
        {
          "query": "Why do the blue and red points heavily overlap in this visualization, given the model uses both s and MMD?",
          "answer": "The model successfully factors out domain (gender) information by incorporating both the sensitive attribute s and MMD regularization, resulting in representations where male and female samples are not separable and cluster together, demonstrating fairness through indistinguishability.",
          "visual_anchor": "overlapping blue (male) and red (female) scattered point clusters",
          "text_evidence": "Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the mixed distribution of blue and red points confirm successful removal of sensitive attribute information?",
          "answer": "Yes, the visualization shows that male (blue) and female (red) samples are thoroughly intermixed without clear separation, which aligns with the model's success in making the learned representations indistinguishable with respect to gender.",
          "visual_anchor": "intermixed blue and red points without clear cluster separation",
          "text_evidence": "latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females",
          "query_type": "value_context"
        },
        {
          "query": "What relationship between blue and red point distributions validates the two-dimensional Barnes-Hut SNE embedding approach?",
          "answer": "The Barnes-Hut SNE embedding reveals that the latent representations z₁ achieve gender-invariance by producing overlapping distributions for males and females, allowing visual assessment of whether the model successfully disentangles sensitive attributes from the learned features.",
          "visual_anchor": "two-dimensional scattered distribution showing blue and red point overlap",
          "text_evidence": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue and red points heavily overlap in this visualization, given the model uses both s and MMD?",
        "answer": "The model successfully factors out domain (gender) information by incorporating both the sensitive attribute s and MMD regularization, resulting in representations where male and female samples are not separable and cluster together, demonstrating fairness through indistinguishability.",
        "visual_anchor": "overlapping blue (male) and red (female) scattered point clusters",
        "text_evidence": "Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the mixed distribution of blue and red points confirm successful removal of sensitive attribute information?",
        "answer": "Yes, the visualization shows that male (blue) and female (red) samples are thoroughly intermixed without clear separation, which aligns with the model's success in making the learned representations indistinguishable with respect to gender.",
        "visual_anchor": "intermixed blue and red points without clear cluster separation",
        "text_evidence": "latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females",
        "query_type": "value_context"
      },
      {
        "query": "What relationship between blue and red point distributions validates the two-dimensional Barnes-Hut SNE embedding approach?",
        "answer": "The Barnes-Hut SNE embedding reveals that the latent representations z₁ achieve gender-invariance by producing overlapping distributions for males and females, allowing visual assessment of whether the model successfully disentangles sensitive attributes from the learned features.",
        "visual_anchor": "two-dimensional scattered distribution showing blue and red point overlap",
        "text_evidence": "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue scattered points representing males",
      "red scattered points representing females",
      "two-dimensional t-SNE embedding visualization",
      "overlapping clusters of blue and red points"
    ]
  },
  {
    "doc_id": "1511.00830",
    "figure_id": "1511.00830_fig_16",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg",
    "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Two t-SNE visualizations labeled (a) and (b)\",\n    \"Person ID numbers (10, 17, 18, 19, 22, 26, etc.) labeling data points\",\n    \"Grayscale face images at each data point\",\n    \"Scattered cluster patterns in visualization (a)\",\n    \"Different spatial arrangement in visualization (b)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why are images clustered by lighting conditions in visualization (a) despite the goal of learning invariant representations?\",\n      \"answer\": \"Visualization (a) shows the original representation x where lighting conditions s are identifiable with almost perfect accuracy, demonstrating that invariance has not yet been achieved in the original space before the VFAE transformation.\",\n      \"visual_anchor\": \"Visualization (a) showing scattered clusters with person IDs 10, 17, 18, 19, 22, 26\",\n      \"text_evidence\": \"This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the spatial arrangement difference between visualizations (a) and (b) demonstrate latent variable z1's invariance to lighting conditions?\",\n      \"answer\": \"Yes, the different clustering patterns between (a) and (b) reflect that VFAE's latent z1 learns representations that are explicitly invariant with respect to lighting conditions while the original x clusters by lighting.\",\n      \"visual_anchor\": \"Contrasting cluster patterns between visualization (a) original x and visualization (b) latent z1\",\n      \"text_evidence\": \"We introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the person ID labeling in both visualizations support the claim about retaining remaining information beyond lighting?\",\n      \"answer\": \"The person ID labels (10, 17, 18, 19, etc.) visible in both visualizations demonstrate that VFAE retains identity information in z1 while removing lighting variance, fulfilling the goal of preserving remaining information.\",\n      \"visual_anchor\": \"Person ID numbers 10, 17, 18, 19, 22, 26 labeling points in both (a) and (b)\",\n      \"text_evidence\": \"learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Two t-SNE visualizations labeled (a) and (b)",
        "Person ID numbers (10, 17, 18, 19, 22, 26, etc.) labeling data points",
        "Grayscale face images at each data point",
        "Scattered cluster patterns in visualization (a)",
        "Different spatial arrangement in visualization (b)"
      ],
      "queries": [
        {
          "query": "Why are images clustered by lighting conditions in visualization (a) despite the goal of learning invariant representations?",
          "answer": "Visualization (a) shows the original representation x where lighting conditions s are identifiable with almost perfect accuracy, demonstrating that invariance has not yet been achieved in the original space before the VFAE transformation.",
          "visual_anchor": "Visualization (a) showing scattered clusters with person IDs 10, 17, 18, 19, 22, 26",
          "text_evidence": "This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the spatial arrangement difference between visualizations (a) and (b) demonstrate latent variable z1's invariance to lighting conditions?",
          "answer": "Yes, the different clustering patterns between (a) and (b) reflect that VFAE's latent z1 learns representations that are explicitly invariant with respect to lighting conditions while the original x clusters by lighting.",
          "visual_anchor": "Contrasting cluster patterns between visualization (a) original x and visualization (b) latent z1",
          "text_evidence": "We introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the person ID labeling in both visualizations support the claim about retaining remaining information beyond lighting?",
          "answer": "The person ID labels (10, 17, 18, 19, etc.) visible in both visualizations demonstrate that VFAE retains identity information in z1 while removing lighting variance, fulfilling the goal of preserving remaining information.",
          "visual_anchor": "Person ID numbers 10, 17, 18, 19, 22, 26 labeling points in both (a) and (b)",
          "text_evidence": "learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why are images clustered by lighting conditions in visualization (a) despite the goal of learning invariant representations?",
        "answer": "Visualization (a) shows the original representation x where lighting conditions s are identifiable with almost perfect accuracy, demonstrating that invariance has not yet been achieved in the original space before the VFAE transformation.",
        "visual_anchor": "Visualization (a) showing scattered clusters with person IDs 10, 17, 18, 19, 22, 26",
        "text_evidence": "This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the spatial arrangement difference between visualizations (a) and (b) demonstrate latent variable z1's invariance to lighting conditions?",
        "answer": "Yes, the different clustering patterns between (a) and (b) reflect that VFAE's latent z1 learns representations that are explicitly invariant with respect to lighting conditions while the original x clusters by lighting.",
        "visual_anchor": "Contrasting cluster patterns between visualization (a) original x and visualization (b) latent z1",
        "text_evidence": "We introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the person ID labeling in both visualizations support the claim about retaining remaining information beyond lighting?",
        "answer": "The person ID labels (10, 17, 18, 19, etc.) visible in both visualizations demonstrate that VFAE retains identity information in z1 while removing lighting variance, fulfilling the goal of preserving remaining information.",
        "visual_anchor": "Person ID numbers 10, 17, 18, 19, 22, 26 labeling points in both (a) and (b)",
        "text_evidence": "learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Two t-SNE visualizations labeled (a) and (b)",
      "Person ID numbers (10, 17, 18, 19, 22, 26, etc.) labeling data points",
      "Grayscale face images at each data point",
      "Scattered cluster patterns in visualization (a)",
      "Different spatial arrangement in visualization (b)"
    ]
  },
  {
    "doc_id": "1511.00830",
    "figure_id": "1511.00830_fig_18",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg",
    "caption": "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Left plot showing PAD on z1 from VFAE vs PAD on raw input\",\n    \"Right plot showing PAD on raw input from DANN\",\n    \"Dashed diagonal line from (0,0) to (2,2)\",\n    \"Clustered data points near 0.5 on both axes in left plot\",\n    \"Data points ranging from approximately 1.7 to 2.0 on x-axis in left plot\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the clustered points near 0.5 on both axes indicate domain invariance in VFAE's z1 representation?\",\n      \"answer\": \"The points clustered near 0.5 on both axes show that VFAE's z1 representations have low PAD scores, indicating the domains are less distinguishable compared to raw input, demonstrating successful domain adaptation.\",\n      \"visual_anchor\": \"Clustered data points near coordinate (0.5, 0.5) in the left plot\",\n      \"text_evidence\": \"we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the $\\\\mathbf { z } _ { 1 }$ representations of VFAE\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does VFAE's clustering near 0.5 compare to DANN's approach using a linear Support Vector Machine classifier?\",\n      \"answer\": \"VFAE achieves consistently low PAD scores near 0.5 for z1 representations across all raw input PAD values, while DANN's reference plot shows their domain adaptation performance using a linear SVM classifier.\",\n      \"visual_anchor\": \"Left plot points clustered at 0.5 vertical position versus right DANN reference plot\",\n      \"text_evidence\": \"we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What does the dashed diagonal line from origin to (2,2) represent for measuring domain distinguishability approximation?\",\n      \"answer\": \"The dashed diagonal line represents perfect correlation where PAD on z1 equals PAD on raw input. Points below this line indicate improved domain invariance, approximating the H-divergence measure of domain distinguishability.\",\n      \"visual_anchor\": \"Dashed diagonal line extending from (0,0) to (2,2)\",\n      \"text_evidence\": \"Proxy A-distance is an approximation to the $\\\\mathcal { H }$ -divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010)\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Left plot showing PAD on z1 from VFAE vs PAD on raw input",
        "Right plot showing PAD on raw input from DANN",
        "Dashed diagonal line from (0,0) to (2,2)",
        "Clustered data points near 0.5 on both axes in left plot",
        "Data points ranging from approximately 1.7 to 2.0 on x-axis in left plot"
      ],
      "queries": [
        {
          "query": "Why do the clustered points near 0.5 on both axes indicate domain invariance in VFAE's z1 representation?",
          "answer": "The points clustered near 0.5 on both axes show that VFAE's z1 representations have low PAD scores, indicating the domains are less distinguishable compared to raw input, demonstrating successful domain adaptation.",
          "visual_anchor": "Clustered data points near coordinate (0.5, 0.5) in the left plot",
          "text_evidence": "we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the $\\mathbf { z } _ { 1 }$ representations of VFAE",
          "query_type": "value_context"
        },
        {
          "query": "How does VFAE's clustering near 0.5 compare to DANN's approach using a linear Support Vector Machine classifier?",
          "answer": "VFAE achieves consistently low PAD scores near 0.5 for z1 representations across all raw input PAD values, while DANN's reference plot shows their domain adaptation performance using a linear SVM classifier.",
          "visual_anchor": "Left plot points clustered at 0.5 vertical position versus right DANN reference plot",
          "text_evidence": "we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What does the dashed diagonal line from origin to (2,2) represent for measuring domain distinguishability approximation?",
          "answer": "The dashed diagonal line represents perfect correlation where PAD on z1 equals PAD on raw input. Points below this line indicate improved domain invariance, approximating the H-divergence measure of domain distinguishability.",
          "visual_anchor": "Dashed diagonal line extending from (0,0) to (2,2)",
          "text_evidence": "Proxy A-distance is an approximation to the $\\mathcal { H }$ -divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010)",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the clustered points near 0.5 on both axes indicate domain invariance in VFAE's z1 representation?",
        "answer": "The points clustered near 0.5 on both axes show that VFAE's z1 representations have low PAD scores, indicating the domains are less distinguishable compared to raw input, demonstrating successful domain adaptation.",
        "visual_anchor": "Clustered data points near coordinate (0.5, 0.5) in the left plot",
        "text_evidence": "we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the $\\mathbf { z } _ { 1 }$ representations of VFAE",
        "query_type": "value_context"
      },
      {
        "query": "How does VFAE's clustering near 0.5 compare to DANN's approach using a linear Support Vector Machine classifier?",
        "answer": "VFAE achieves consistently low PAD scores near 0.5 for z1 representations across all raw input PAD values, while DANN's reference plot shows their domain adaptation performance using a linear SVM classifier.",
        "visual_anchor": "Left plot points clustered at 0.5 vertical position versus right DANN reference plot",
        "text_evidence": "we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What does the dashed diagonal line from origin to (2,2) represent for measuring domain distinguishability approximation?",
        "answer": "The dashed diagonal line represents perfect correlation where PAD on z1 equals PAD on raw input. Points below this line indicate improved domain invariance, approximating the H-divergence measure of domain distinguishability.",
        "visual_anchor": "Dashed diagonal line extending from (0,0) to (2,2)",
        "text_evidence": "Proxy A-distance is an approximation to the $\\mathcal { H }$ -divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010)",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Left plot showing PAD on z1 from VFAE vs PAD on raw input",
      "Right plot showing PAD on raw input from DANN",
      "Dashed diagonal line from (0,0) to (2,2)",
      "Clustered data points near 0.5 on both axes in left plot",
      "Data points ranging from approximately 1.7 to 2.0 on x-axis in left plot"
    ]
  },
  {
    "doc_id": "1602.05352",
    "figure_id": "1602.05352_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg",
    "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"table\",\n  \"visual_elements\": [\n    \"True rating matrix Y with Horror-lovers rating Horror=5\",\n    \"Propensity matrix P showing Horror-lovers have propensity=1 for Romance\",\n    \"Observation indicator matrix O with value 3 for Horror-lovers watching Drama\",\n    \"Prediction matrix Ŷ₁ showing Horror-lovers predicted rating of 5 for Horror\",\n    \"Prediction matrix Ŷ₂ showing Romance-lovers predicted rating of 5 for Romance and Drama\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does propensity matrix P show value 1 for Horror-lovers on Romance despite true rating Y being 1?\",\n      \"answer\": \"The propensity matrix P captures selection bias, showing that Horror-lovers have low propensity to watch Romance even though they would rate it poorly. This illustrates the MNAR (missing not at random) nature of recommendation data where observation patterns don't reflect true preferences.\",\n      \"visual_anchor\": \"Propensity matrix P top row showing Horror-lovers with value 1 for Romance column\",\n      \"text_evidence\": \"Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missing-data imputation based on the joint likelihood of the missing data model and the rating model\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does observation indicator O value 3 for Horror-lovers on Drama relate to evaluating prediction accuracy?\",\n      \"answer\": \"The observation indicator O shows which ratings are observed in the test set. The value 3 for Horror-lovers on Drama means this rating is available for evaluation, enabling calculation of error metrics like MAE or MSE on observed entries.\",\n      \"visual_anchor\": \"Observation indicator matrix O top row showing value 3 in Drama column\",\n      \"text_evidence\": \"Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as: for an appropriately chosen δu,i(Y,Ŷ)\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do prediction matrices Ŷ₁ and Ŷ₂ differ for Romance-lovers on Drama despite identical observation patterns?\",\n      \"answer\": \"Ŷ₁ predicts 5 while Ŷ₂ predicts 5 for Romance-lovers on Drama, demonstrating how different prediction models can produce varying estimates. This toy example illustrates the disastrous effect of selection bias on conventional evaluation using held-out ratings.\",\n      \"visual_anchor\": \"Bottom row matrices showing Ŷ₁ with value 5 and Ŷ₂ with value 5 for Romance-lovers on Drama\",\n      \"text_evidence\": \"Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "table",
      "visual_elements": [
        "True rating matrix Y with Horror-lovers rating Horror=5",
        "Propensity matrix P showing Horror-lovers have propensity=1 for Romance",
        "Observation indicator matrix O with value 3 for Horror-lovers watching Drama",
        "Prediction matrix Ŷ₁ showing Horror-lovers predicted rating of 5 for Horror",
        "Prediction matrix Ŷ₂ showing Romance-lovers predicted rating of 5 for Romance and Drama"
      ],
      "queries": [
        {
          "query": "Why does propensity matrix P show value 1 for Horror-lovers on Romance despite true rating Y being 1?",
          "answer": "The propensity matrix P captures selection bias, showing that Horror-lovers have low propensity to watch Romance even though they would rate it poorly. This illustrates the MNAR (missing not at random) nature of recommendation data where observation patterns don't reflect true preferences.",
          "visual_anchor": "Propensity matrix P top row showing Horror-lovers with value 1 for Romance column",
          "text_evidence": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missing-data imputation based on the joint likelihood of the missing data model and the rating model",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does observation indicator O value 3 for Horror-lovers on Drama relate to evaluating prediction accuracy?",
          "answer": "The observation indicator O shows which ratings are observed in the test set. The value 3 for Horror-lovers on Drama means this rating is available for evaluation, enabling calculation of error metrics like MAE or MSE on observed entries.",
          "visual_anchor": "Observation indicator matrix O top row showing value 3 in Drama column",
          "text_evidence": "Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as: for an appropriately chosen δu,i(Y,Ŷ)",
          "query_type": "value_context"
        },
        {
          "query": "Why do prediction matrices Ŷ₁ and Ŷ₂ differ for Romance-lovers on Drama despite identical observation patterns?",
          "answer": "Ŷ₁ predicts 5 while Ŷ₂ predicts 5 for Romance-lovers on Drama, demonstrating how different prediction models can produce varying estimates. This toy example illustrates the disastrous effect of selection bias on conventional evaluation using held-out ratings.",
          "visual_anchor": "Bottom row matrices showing Ŷ₁ with value 5 and Ŷ₂ with value 5 for Romance-lovers on Drama",
          "text_evidence": "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does propensity matrix P show value 1 for Horror-lovers on Romance despite true rating Y being 1?",
        "answer": "The propensity matrix P captures selection bias, showing that Horror-lovers have low propensity to watch Romance even though they would rate it poorly. This illustrates the MNAR (missing not at random) nature of recommendation data where observation patterns don't reflect true preferences.",
        "visual_anchor": "Propensity matrix P top row showing Horror-lovers with value 1 for Romance column",
        "text_evidence": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missing-data imputation based on the joint likelihood of the missing data model and the rating model",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does observation indicator O value 3 for Horror-lovers on Drama relate to evaluating prediction accuracy?",
        "answer": "The observation indicator O shows which ratings are observed in the test set. The value 3 for Horror-lovers on Drama means this rating is available for evaluation, enabling calculation of error metrics like MAE or MSE on observed entries.",
        "visual_anchor": "Observation indicator matrix O top row showing value 3 in Drama column",
        "text_evidence": "Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as: for an appropriately chosen δu,i(Y,Ŷ)",
        "query_type": "value_context"
      },
      {
        "query": "Why do prediction matrices Ŷ₁ and Ŷ₂ differ for Romance-lovers on Drama despite identical observation patterns?",
        "answer": "Ŷ₁ predicts 5 while Ŷ₂ predicts 5 for Romance-lovers on Drama, demonstrating how different prediction models can produce varying estimates. This toy example illustrates the disastrous effect of selection bias on conventional evaluation using held-out ratings.",
        "visual_anchor": "Bottom row matrices showing Ŷ₁ with value 5 and Ŷ₂ with value 5 for Romance-lovers on Drama",
        "text_evidence": "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "table",
    "visual_elements": [
      "True rating matrix Y with Horror-lovers rating Horror=5",
      "Propensity matrix P showing Horror-lovers have propensity=1 for Romance",
      "Observation indicator matrix O with value 3 for Horror-lovers watching Drama",
      "Prediction matrix Ŷ₁ showing Horror-lovers predicted rating of 5 for Horror",
      "Prediction matrix Ŷ₂ showing Romance-lovers predicted rating of 5 for Romance and Drama"
    ]
  },
  {
    "doc_id": "1602.05352",
    "figure_id": "1602.05352_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg",
    "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red line labeled 'Naive' showing high RMSE across all α values in MSE plot\",\n    \"Blue line labeled 'IPS' and green line labeled 'SNIPS' showing low RMSE in MSE plot\",\n    \"DCG plot showing red 'Naive' line decreasing from ~200 to ~1 as α increases\",\n    \"Blue 'IPS' and green 'SNIPS' lines converging to ~0.5 in DCG plot\",\n    \"Logarithmic y-axis scales in both plots\",\n    \"X-axis α ranging from 0 to 1.0 in both plots\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the red Naive line's high RMSE at α=0.5 support why propensity-scored MF-IPS substantially outperforms conventional matrix factorization?\",\n      \"answer\": \"Yes, the red Naive line maintains RMSE above 1.0 at α=0.5 while IPS/SNIPS stay near 0.01-0.02, demonstrating the substantial performance gap that Table 2 quantifies between propensity-scored and conventional approaches.\",\n      \"visual_anchor\": \"Red Naive line at RMSE > 1.0 versus blue IPS and green SNIPS lines at RMSE ≈ 0.01-0.02 when α=0.5 in MSE plot\",\n      \"text_evidence\": \"Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the blue IPS and green SNIPS curves remain stable below 0.1 RMSE across varying α, given cross-validation requires propensity scaling?\",\n      \"answer\": \"The propensity scaling by (k-1)/k for training folds and validation fold adjustments maintain consistent IPS/SNIPS estimation quality across selection bias degrees, keeping RMSE stable and low regardless of α.\",\n      \"visual_anchor\": \"Blue IPS and green SNIPS lines maintaining RMSE < 0.1 from α=0 to α=1.0 in MSE plot\",\n      \"text_evidence\": \"We randomly split the observed MNAR ratings into k folds k = 4 in all experiments), training on k - 1 and evaluating on the remaining one using the IPS estimator. Reflecting this additional split requires scaling the propensities in the training folds by\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the red Naive line's dramatic DCG drop from ~200 at α=0 to ~10 at α=0.7 in the experimental setting?\",\n      \"answer\": \"As α increases, observed ratings exhibit stronger selection bias, causing the naive estimator's DCG to degrade dramatically because it fails to correct for the increasingly non-random missing data pattern.\",\n      \"visual_anchor\": \"Red Naive line decreasing from ~200 at α=0 to ~10 at α=0.7 in DCG plot\",\n      \"text_evidence\": \"RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red line labeled 'Naive' showing high RMSE across all α values in MSE plot",
        "Blue line labeled 'IPS' and green line labeled 'SNIPS' showing low RMSE in MSE plot",
        "DCG plot showing red 'Naive' line decreasing from ~200 to ~1 as α increases",
        "Blue 'IPS' and green 'SNIPS' lines converging to ~0.5 in DCG plot",
        "Logarithmic y-axis scales in both plots",
        "X-axis α ranging from 0 to 1.0 in both plots"
      ],
      "queries": [
        {
          "query": "Does the red Naive line's high RMSE at α=0.5 support why propensity-scored MF-IPS substantially outperforms conventional matrix factorization?",
          "answer": "Yes, the red Naive line maintains RMSE above 1.0 at α=0.5 while IPS/SNIPS stay near 0.01-0.02, demonstrating the substantial performance gap that Table 2 quantifies between propensity-scored and conventional approaches.",
          "visual_anchor": "Red Naive line at RMSE > 1.0 versus blue IPS and green SNIPS lines at RMSE ≈ 0.01-0.02 when α=0.5 in MSE plot",
          "text_evidence": "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the blue IPS and green SNIPS curves remain stable below 0.1 RMSE across varying α, given cross-validation requires propensity scaling?",
          "answer": "The propensity scaling by (k-1)/k for training folds and validation fold adjustments maintain consistent IPS/SNIPS estimation quality across selection bias degrees, keeping RMSE stable and low regardless of α.",
          "visual_anchor": "Blue IPS and green SNIPS lines maintaining RMSE < 0.1 from α=0 to α=1.0 in MSE plot",
          "text_evidence": "We randomly split the observed MNAR ratings into k folds k = 4 in all experiments), training on k - 1 and evaluating on the remaining one using the IPS estimator. Reflecting this additional split requires scaling the propensities in the training folds by",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the red Naive line's dramatic DCG drop from ~200 at α=0 to ~10 at α=0.7 in the experimental setting?",
          "answer": "As α increases, observed ratings exhibit stronger selection bias, causing the naive estimator's DCG to degrade dramatically because it fails to correct for the increasingly non-random missing data pattern.",
          "visual_anchor": "Red Naive line decreasing from ~200 at α=0 to ~10 at α=0.7 in DCG plot",
          "text_evidence": "RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the red Naive line's high RMSE at α=0.5 support why propensity-scored MF-IPS substantially outperforms conventional matrix factorization?",
        "answer": "Yes, the red Naive line maintains RMSE above 1.0 at α=0.5 while IPS/SNIPS stay near 0.01-0.02, demonstrating the substantial performance gap that Table 2 quantifies between propensity-scored and conventional approaches.",
        "visual_anchor": "Red Naive line at RMSE > 1.0 versus blue IPS and green SNIPS lines at RMSE ≈ 0.01-0.02 when α=0.5 in MSE plot",
        "text_evidence": "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the blue IPS and green SNIPS curves remain stable below 0.1 RMSE across varying α, given cross-validation requires propensity scaling?",
        "answer": "The propensity scaling by (k-1)/k for training folds and validation fold adjustments maintain consistent IPS/SNIPS estimation quality across selection bias degrees, keeping RMSE stable and low regardless of α.",
        "visual_anchor": "Blue IPS and green SNIPS lines maintaining RMSE < 0.1 from α=0 to α=1.0 in MSE plot",
        "text_evidence": "We randomly split the observed MNAR ratings into k folds k = 4 in all experiments), training on k - 1 and evaluating on the remaining one using the IPS estimator. Reflecting this additional split requires scaling the propensities in the training folds by",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the red Naive line's dramatic DCG drop from ~200 at α=0 to ~10 at α=0.7 in the experimental setting?",
        "answer": "As α increases, observed ratings exhibit stronger selection bias, causing the naive estimator's DCG to degrade dramatically because it fails to correct for the increasingly non-random missing data pattern.",
        "visual_anchor": "Red Naive line decreasing from ~200 at α=0 to ~10 at α=0.7 in DCG plot",
        "text_evidence": "RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red line labeled 'Naive' showing high RMSE across all α values in MSE plot",
      "Blue line labeled 'IPS' and green line labeled 'SNIPS' showing low RMSE in MSE plot",
      "DCG plot showing red 'Naive' line decreasing from ~200 to ~1 as α increases",
      "Blue 'IPS' and green 'SNIPS' lines converging to ~0.5 in DCG plot",
      "Logarithmic y-axis scales in both plots",
      "X-axis α ranging from 0 to 1.0 in both plots"
    ]
  },
  {
    "doc_id": "1602.05352",
    "figure_id": "1602.05352_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg",
    "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"red curve labeled MF-Naive\", \"blue curve labeled MF-IPS\", \"MSE y-axis ranging from 0 to 1.75\", \"alpha x-axis ranging from 0 to 1\", \"shaded confidence intervals around both curves\", \"left panel showing MSE vs alpha\", \"curves starting high at low alpha and decreasing\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue MF-IPS curve achieve lower MSE than red MF-Naive across all alpha values?\",\n      \"answer\": \"MF-IPS uses propensity weighting to correct for selection bias, while MF-Naive does not account for biased observations. This allows MF-IPS to achieve better prediction accuracy through improved risk estimation.\",\n      \"visual_anchor\": \"blue MF-IPS curve consistently below red MF-Naive curve across alpha from 0 to 1\",\n      \"text_evidence\": \"Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the shaded confidence intervals around both curves support the claim of 95% confidence over 30 trials?\",\n      \"answer\": \"Yes, the shaded regions around both the red MF-Naive and blue MF-IPS curves represent 95% confidence intervals computed over 30 experimental trials, as stated in the methodology.\",\n      \"visual_anchor\": \"shaded regions surrounding red and blue curves throughout the alpha range\",\n      \"text_evidence\": \"The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\\\%$ confidence intervals over 30 trials.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the MSE reduction from 1.75 to near 0.1 as alpha increases demonstrate propensity estimation quality impact?\",\n      \"answer\": \"This left panel shows how selection bias severity affects learning as alpha varies. The right panel specifically examines propensity estimation quality degradation, which is a different experimental manipulation.\",\n      \"visual_anchor\": \"MSE values decreasing from approximately 1.75 at alpha=0 to near 0.1 at alpha=1 for both curves\",\n      \"text_evidence\": \"Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "red curve labeled MF-Naive",
        "blue curve labeled MF-IPS",
        "MSE y-axis ranging from 0 to 1.75",
        "alpha x-axis ranging from 0 to 1",
        "shaded confidence intervals around both curves",
        "left panel showing MSE vs alpha",
        "curves starting high at low alpha and decreasing"
      ],
      "queries": [
        {
          "query": "Why does the blue MF-IPS curve achieve lower MSE than red MF-Naive across all alpha values?",
          "answer": "MF-IPS uses propensity weighting to correct for selection bias, while MF-Naive does not account for biased observations. This allows MF-IPS to achieve better prediction accuracy through improved risk estimation.",
          "visual_anchor": "blue MF-IPS curve consistently below red MF-Naive curve across alpha from 0 to 1",
          "text_evidence": "Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the shaded confidence intervals around both curves support the claim of 95% confidence over 30 trials?",
          "answer": "Yes, the shaded regions around both the red MF-Naive and blue MF-IPS curves represent 95% confidence intervals computed over 30 experimental trials, as stated in the methodology.",
          "visual_anchor": "shaded regions surrounding red and blue curves throughout the alpha range",
          "text_evidence": "The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the MSE reduction from 1.75 to near 0.1 as alpha increases demonstrate propensity estimation quality impact?",
          "answer": "This left panel shows how selection bias severity affects learning as alpha varies. The right panel specifically examines propensity estimation quality degradation, which is a different experimental manipulation.",
          "visual_anchor": "MSE values decreasing from approximately 1.75 at alpha=0 to near 0.1 at alpha=1 for both curves",
          "text_evidence": "Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue MF-IPS curve achieve lower MSE than red MF-Naive across all alpha values?",
        "answer": "MF-IPS uses propensity weighting to correct for selection bias, while MF-Naive does not account for biased observations. This allows MF-IPS to achieve better prediction accuracy through improved risk estimation.",
        "visual_anchor": "blue MF-IPS curve consistently below red MF-Naive curve across alpha from 0 to 1",
        "text_evidence": "Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the shaded confidence intervals around both curves support the claim of 95% confidence over 30 trials?",
        "answer": "Yes, the shaded regions around both the red MF-Naive and blue MF-IPS curves represent 95% confidence intervals computed over 30 experimental trials, as stated in the methodology.",
        "visual_anchor": "shaded regions surrounding red and blue curves throughout the alpha range",
        "text_evidence": "The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the MSE reduction from 1.75 to near 0.1 as alpha increases demonstrate propensity estimation quality impact?",
        "answer": "This left panel shows how selection bias severity affects learning as alpha varies. The right panel specifically examines propensity estimation quality degradation, which is a different experimental manipulation.",
        "visual_anchor": "MSE values decreasing from approximately 1.75 at alpha=0 to near 0.1 at alpha=1 for both curves",
        "text_evidence": "Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "red curve labeled MF-Naive",
      "blue curve labeled MF-IPS",
      "MSE y-axis ranging from 0 to 1.75",
      "alpha x-axis ranging from 0 to 1",
      "shaded confidence intervals around both curves",
      "left panel showing MSE vs alpha",
      "curves starting high at low alpha and decreasing"
    ]
  },
  {
    "doc_id": "1602.05352",
    "figure_id": "1602.05352_fig_7",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg",
    "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red horizontal line at approximately 10^0 RMSE\",\n    \"Blue curve starting at ~10^-2 and rising to ~10^-1\",\n    \"Purple curve starting at ~10^0, dropping sharply around 10^2 MCAR ratings, then rising\",\n    \"Gray shaded region around purple curve\",\n    \"X-axis: Number of MCAR ratings seen by NB (log scale, 10^0 to 10^6)\",\n    \"Y-axis: Estimation error RMSE (log scale, 10^-2 to 10^1)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the purple curve drop sharply at 10^2 MCAR ratings when propensity estimates degrade?\",\n      \"answer\": \"The sharp drop at 10^2 MCAR ratings reflects the transition point where propensity estimates begin to degrade, yet IPS initially improves before deteriorating further as estimates worsen.\",\n      \"visual_anchor\": \"Purple curve with sharp drop at 10^2 on x-axis\",\n      \"text_evidence\": \"Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the blue curve remaining below 10^-1 RMSE across all MCAR ratings support robustness claims?\",\n      \"answer\": \"Yes, the blue curve stays consistently low (below 10^-1) across all MCAR rating counts, demonstrating robustness even with severely degraded propensity estimates.\",\n      \"visual_anchor\": \"Blue curve consistently below 10^-1 RMSE line\",\n      \"text_evidence\": \"severely degraded propensity estimates, demonstrating the robustness of the approach.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the red reference line at 10^0 RMSE relate to unweighted matrix factorization performance?\",\n      \"answer\": \"The red reference line at 10^0 RMSE represents the Naive baseline performance, corresponding to the traditional unweighted matrix factorization MF-Naive used for comparison.\",\n      \"visual_anchor\": \"Red horizontal line at 10^0 RMSE\",\n      \"text_evidence\": \"we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red horizontal line at approximately 10^0 RMSE",
        "Blue curve starting at ~10^-2 and rising to ~10^-1",
        "Purple curve starting at ~10^0, dropping sharply around 10^2 MCAR ratings, then rising",
        "Gray shaded region around purple curve",
        "X-axis: Number of MCAR ratings seen by NB (log scale, 10^0 to 10^6)",
        "Y-axis: Estimation error RMSE (log scale, 10^-2 to 10^1)"
      ],
      "queries": [
        {
          "query": "Why does the purple curve drop sharply at 10^2 MCAR ratings when propensity estimates degrade?",
          "answer": "The sharp drop at 10^2 MCAR ratings reflects the transition point where propensity estimates begin to degrade, yet IPS initially improves before deteriorating further as estimates worsen.",
          "visual_anchor": "Purple curve with sharp drop at 10^2 on x-axis",
          "text_evidence": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the blue curve remaining below 10^-1 RMSE across all MCAR ratings support robustness claims?",
          "answer": "Yes, the blue curve stays consistently low (below 10^-1) across all MCAR rating counts, demonstrating robustness even with severely degraded propensity estimates.",
          "visual_anchor": "Blue curve consistently below 10^-1 RMSE line",
          "text_evidence": "severely degraded propensity estimates, demonstrating the robustness of the approach.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the red reference line at 10^0 RMSE relate to unweighted matrix factorization performance?",
          "answer": "The red reference line at 10^0 RMSE represents the Naive baseline performance, corresponding to the traditional unweighted matrix factorization MF-Naive used for comparison.",
          "visual_anchor": "Red horizontal line at 10^0 RMSE",
          "text_evidence": "we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the purple curve drop sharply at 10^2 MCAR ratings when propensity estimates degrade?",
        "answer": "The sharp drop at 10^2 MCAR ratings reflects the transition point where propensity estimates begin to degrade, yet IPS initially improves before deteriorating further as estimates worsen.",
        "visual_anchor": "Purple curve with sharp drop at 10^2 on x-axis",
        "text_evidence": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the blue curve remaining below 10^-1 RMSE across all MCAR ratings support robustness claims?",
        "answer": "Yes, the blue curve stays consistently low (below 10^-1) across all MCAR rating counts, demonstrating robustness even with severely degraded propensity estimates.",
        "visual_anchor": "Blue curve consistently below 10^-1 RMSE line",
        "text_evidence": "severely degraded propensity estimates, demonstrating the robustness of the approach.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the red reference line at 10^0 RMSE relate to unweighted matrix factorization performance?",
        "answer": "The red reference line at 10^0 RMSE represents the Naive baseline performance, corresponding to the traditional unweighted matrix factorization MF-Naive used for comparison.",
        "visual_anchor": "Red horizontal line at 10^0 RMSE",
        "text_evidence": "we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red horizontal line at approximately 10^0 RMSE",
      "Blue curve starting at ~10^-2 and rising to ~10^-1",
      "Purple curve starting at ~10^0, dropping sharply around 10^2 MCAR ratings, then rising",
      "Gray shaded region around purple curve",
      "X-axis: Number of MCAR ratings seen by NB (log scale, 10^0 to 10^6)",
      "Y-axis: Estimation error RMSE (log scale, 10^-2 to 10^1)"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg",
    "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Username 'SirPatStew' in blue text\",\n    \"Comment score '8978 comments' below the main post\",\n    \"Nested comment structure showing reply indentation\",\n    \"Upvote/downvote arrows on left side of comments\",\n    \"Top 200 Comments section with 'show 500' link\",\n    \"Login form with username and password fields on right\",\n    \"SUBMIT AN AMA button in blue\",\n    \"Statistics showing '8,789 readers (8775 upvotes)'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the nested indentation below SirPatStew's comment demonstrate the focus on sharing versus discussion?\",\n      \"answer\": \"The indentation shows Patrick Stewart answering users' questions, illustrating that many subreddits are primarily about discussion rather than just sharing web content from other sites. The nested structure enables threaded conversations typical of discussion-focused communities.\",\n      \"visual_anchor\": \"Nested comment indentation below SirPatStew's blue username\",\n      \"text_evidence\": \"Many subreddits are primarily about sharing web content from other sites\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the 8,789 readers statistic relate to Reddit's position among top 35 worldwide sites?\",\n      \"answer\": \"The 8,789 readers shown for this single subreddit represents just a tiny fraction of Reddit's massive user base. This individual subreddit metric helps illustrate how Reddit's aggregation of 853,000 subreddits contributes to its ranking as a top 35 global website.\",\n      \"visual_anchor\": \"8,789 readers (8775 upvotes) displayed on right side\",\n      \"text_evidence\": \"According to Alexa, as of late 2015 Reddit is in the top 15 sites in the U.S. and the top 35 in the world in terms of monthly unique visitors\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the Top 200 Comments section with show 500 option reflect the particular purpose focus of subreddits?\",\n      \"answer\": \"Yes, the comment filtering demonstrates how IAmA subreddit serves its particular purpose of hosting Q&A sessions. The large volume requiring a 'show 500' option indicates this subreddit focuses specifically on facilitating celebrity question-and-answer interactions rather than general content sharing.\",\n      \"visual_anchor\": \"Top 200 Comments header with 'show 500' link\",\n      \"text_evidence\": \"It consists of a large number of subreddits (853,000 as of June 21st, 2015), each of which focuses on a particular purpose\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Username 'SirPatStew' in blue text",
        "Comment score '8978 comments' below the main post",
        "Nested comment structure showing reply indentation",
        "Upvote/downvote arrows on left side of comments",
        "Top 200 Comments section with 'show 500' link",
        "Login form with username and password fields on right",
        "SUBMIT AN AMA button in blue",
        "Statistics showing '8,789 readers (8775 upvotes)'"
      ],
      "queries": [
        {
          "query": "Why does the nested indentation below SirPatStew's comment demonstrate the focus on sharing versus discussion?",
          "answer": "The indentation shows Patrick Stewart answering users' questions, illustrating that many subreddits are primarily about discussion rather than just sharing web content from other sites. The nested structure enables threaded conversations typical of discussion-focused communities.",
          "visual_anchor": "Nested comment indentation below SirPatStew's blue username",
          "text_evidence": "Many subreddits are primarily about sharing web content from other sites",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the 8,789 readers statistic relate to Reddit's position among top 35 worldwide sites?",
          "answer": "The 8,789 readers shown for this single subreddit represents just a tiny fraction of Reddit's massive user base. This individual subreddit metric helps illustrate how Reddit's aggregation of 853,000 subreddits contributes to its ranking as a top 35 global website.",
          "visual_anchor": "8,789 readers (8775 upvotes) displayed on right side",
          "text_evidence": "According to Alexa, as of late 2015 Reddit is in the top 15 sites in the U.S. and the top 35 in the world in terms of monthly unique visitors",
          "query_type": "value_context"
        },
        {
          "query": "Does the Top 200 Comments section with show 500 option reflect the particular purpose focus of subreddits?",
          "answer": "Yes, the comment filtering demonstrates how IAmA subreddit serves its particular purpose of hosting Q&A sessions. The large volume requiring a 'show 500' option indicates this subreddit focuses specifically on facilitating celebrity question-and-answer interactions rather than general content sharing.",
          "visual_anchor": "Top 200 Comments header with 'show 500' link",
          "text_evidence": "It consists of a large number of subreddits (853,000 as of June 21st, 2015), each of which focuses on a particular purpose",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the nested indentation below SirPatStew's comment demonstrate the focus on sharing versus discussion?",
        "answer": "The indentation shows Patrick Stewart answering users' questions, illustrating that many subreddits are primarily about discussion rather than just sharing web content from other sites. The nested structure enables threaded conversations typical of discussion-focused communities.",
        "visual_anchor": "Nested comment indentation below SirPatStew's blue username",
        "text_evidence": "Many subreddits are primarily about sharing web content from other sites",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the 8,789 readers statistic relate to Reddit's position among top 35 worldwide sites?",
        "answer": "The 8,789 readers shown for this single subreddit represents just a tiny fraction of Reddit's massive user base. This individual subreddit metric helps illustrate how Reddit's aggregation of 853,000 subreddits contributes to its ranking as a top 35 global website.",
        "visual_anchor": "8,789 readers (8775 upvotes) displayed on right side",
        "text_evidence": "According to Alexa, as of late 2015 Reddit is in the top 15 sites in the U.S. and the top 35 in the world in terms of monthly unique visitors",
        "query_type": "value_context"
      },
      {
        "query": "Does the Top 200 Comments section with show 500 option reflect the particular purpose focus of subreddits?",
        "answer": "Yes, the comment filtering demonstrates how IAmA subreddit serves its particular purpose of hosting Q&A sessions. The large volume requiring a 'show 500' option indicates this subreddit focuses specifically on facilitating celebrity question-and-answer interactions rather than general content sharing.",
        "visual_anchor": "Top 200 Comments header with 'show 500' link",
        "text_evidence": "It consists of a large number of subreddits (853,000 as of June 21st, 2015), each of which focuses on a particular purpose",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Username 'SirPatStew' in blue text",
      "Comment score '8978 comments' below the main post",
      "Nested comment structure showing reply indentation",
      "Upvote/downvote arrows on left side of comments",
      "Top 200 Comments section with 'show 500' link",
      "Login form with username and password fields on right",
      "SUBMIT AN AMA button in blue",
      "Statistics showing '8,789 readers (8775 upvotes)'"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg",
    "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid dark purple line showing cumulative users\",\n    \"dashed teal line showing cumulative subreddits\",\n    \"logarithmic y-axis from 10^1 to 10^7\",\n    \"time axis from 2008 to 2014\",\n    \"legend box in bottom right\",\n    \"cumulative users reaching ~10^7 by 2014\",\n    \"cumulative subreddits reaching ~10^5 by 2014\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the solid purple curve reaching 10^7 by 2014 align with the 1.65 billion comments collected through May 2015?\",\n      \"answer\": \"The solid purple curve shows cumulative users reaching about 10 million by 2014, which is distinct from but consistent with the 1.65 billion comments dataset that extends through May 2015.\",\n      \"visual_anchor\": \"solid dark purple line reaching approximately 10^7 on y-axis by 2014\",\n      \"text_evidence\": \"Redditor Stuck In The Matrix used Reddit's API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the dashed teal line remain two orders of magnitude below the purple line throughout 2008-2014?\",\n      \"answer\": \"The dashed teal line represents cumulative subreddits while the purple line shows cumulative users. Reddit's structure allows many users to participate across relatively fewer subreddit communities.\",\n      \"visual_anchor\": \"dashed teal line staying around 10^5 while solid purple line reaches 10^7\",\n      \"text_evidence\": \"These datasets contain a total of 114 million submissions. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the sharp exponential rise in both curves before 2009 reflect the October 2007 starting point of data collection?\",\n      \"answer\": \"Yes, the steep initial rise in both curves starting around 2008 corresponds to the beginning of the dataset compilation from October 2007, capturing Reddit's early rapid growth phase.\",\n      \"visual_anchor\": \"steep exponential rise in both curves from 10^1-10^2 range in 2008\",\n      \"text_evidence\": \"He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid dark purple line showing cumulative users",
        "dashed teal line showing cumulative subreddits",
        "logarithmic y-axis from 10^1 to 10^7",
        "time axis from 2008 to 2014",
        "legend box in bottom right",
        "cumulative users reaching ~10^7 by 2014",
        "cumulative subreddits reaching ~10^5 by 2014"
      ],
      "queries": [
        {
          "query": "Does the solid purple curve reaching 10^7 by 2014 align with the 1.65 billion comments collected through May 2015?",
          "answer": "The solid purple curve shows cumulative users reaching about 10 million by 2014, which is distinct from but consistent with the 1.65 billion comments dataset that extends through May 2015.",
          "visual_anchor": "solid dark purple line reaching approximately 10^7 on y-axis by 2014",
          "text_evidence": "Redditor Stuck In The Matrix used Reddit's API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments",
          "query_type": "value_context"
        },
        {
          "query": "Why does the dashed teal line remain two orders of magnitude below the purple line throughout 2008-2014?",
          "answer": "The dashed teal line represents cumulative subreddits while the purple line shows cumulative users. Reddit's structure allows many users to participate across relatively fewer subreddit communities.",
          "visual_anchor": "dashed teal line staying around 10^5 while solid purple line reaches 10^7",
          "text_evidence": "These datasets contain a total of 114 million submissions. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the sharp exponential rise in both curves before 2009 reflect the October 2007 starting point of data collection?",
          "answer": "Yes, the steep initial rise in both curves starting around 2008 corresponds to the beginning of the dataset compilation from October 2007, capturing Reddit's early rapid growth phase.",
          "visual_anchor": "steep exponential rise in both curves from 10^1-10^2 range in 2008",
          "text_evidence": "He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the solid purple curve reaching 10^7 by 2014 align with the 1.65 billion comments collected through May 2015?",
        "answer": "The solid purple curve shows cumulative users reaching about 10 million by 2014, which is distinct from but consistent with the 1.65 billion comments dataset that extends through May 2015.",
        "visual_anchor": "solid dark purple line reaching approximately 10^7 on y-axis by 2014",
        "text_evidence": "Redditor Stuck In The Matrix used Reddit's API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments",
        "query_type": "value_context"
      },
      {
        "query": "Why does the dashed teal line remain two orders of magnitude below the purple line throughout 2008-2014?",
        "answer": "The dashed teal line represents cumulative subreddits while the purple line shows cumulative users. Reddit's structure allows many users to participate across relatively fewer subreddit communities.",
        "visual_anchor": "dashed teal line staying around 10^5 while solid purple line reaches 10^7",
        "text_evidence": "These datasets contain a total of 114 million submissions. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the sharp exponential rise in both curves before 2009 reflect the October 2007 starting point of data collection?",
        "answer": "Yes, the steep initial rise in both curves starting around 2008 corresponds to the beginning of the dataset compilation from October 2007, capturing Reddit's early rapid growth phase.",
        "visual_anchor": "steep exponential rise in both curves from 10^1-10^2 range in 2008",
        "text_evidence": "He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid dark purple line showing cumulative users",
      "dashed teal line showing cumulative subreddits",
      "logarithmic y-axis from 10^1 to 10^7",
      "time axis from 2008 to 2014",
      "legend box in bottom right",
      "cumulative users reaching ~10^7 by 2014",
      "cumulative subreddits reaching ~10^5 by 2014"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg",
    "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"purple/dark line showing average posts per user\", \"y-axis ranging from 0 to 30 posts\", \"x-axis showing time from 2008 to beyond 2014\", \"initial value around 13 posts in 2008\", \"plateau around 17-18 posts from 2011 onwards\", \"upward trend from 2008 to 2011\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the purple curve rise from 13 to 17 posts between 2008 and 2011 despite the 350,000 unavailable comments?\",\n      \"answer\": \"The 350,000 missing comments due to API call failures represent a negligible fraction of the 1.65 billion total comments, so they do not significantly impact the observed upward trend in average posts per active user during this period.\",\n      \"visual_anchor\": \"purple curve rising from approximately 13 posts per user in 2008 to 17 posts by 2011\",\n      \"text_evidence\": \"The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the plateau at 17 posts after 2011 reflect behavior only from users who remain active each month?\",\n      \"answer\": \"Yes, the plateau reflects only active users during each month. Users temporarily or permanently away from Reddit are excluded from the monthly average calculation, which stabilizes the metric around 17-18 posts per user.\",\n      \"visual_anchor\": \"plateau in the purple curve at approximately 17-18 posts per user from 2011 onwards\",\n      \"text_evidence\": \"we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does filtering bot-like usernames affect the consistency of the 17-post plateau visible from 2011 to 2014?\",\n      \"answer\": \"Filtering out bot-like usernames removes automated posting activity that could artificially inflate or add noise to the average. This preprocessing helps ensure the plateau represents genuine human user behavior, making the metric more stable and meaningful.\",\n      \"visual_anchor\": \"stable plateau around 17 posts per user maintained from 2011 through 2014\",\n      \"text_evidence\": \"we did light preprocessing to filter out posts by deleted users, posts with no creation time, and posts by authors with bot-like names\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "purple/dark line showing average posts per user",
        "y-axis ranging from 0 to 30 posts",
        "x-axis showing time from 2008 to beyond 2014",
        "initial value around 13 posts in 2008",
        "plateau around 17-18 posts from 2011 onwards",
        "upward trend from 2008 to 2011"
      ],
      "queries": [
        {
          "query": "Why does the purple curve rise from 13 to 17 posts between 2008 and 2011 despite the 350,000 unavailable comments?",
          "answer": "The 350,000 missing comments due to API call failures represent a negligible fraction of the 1.65 billion total comments, so they do not significantly impact the observed upward trend in average posts per active user during this period.",
          "visual_anchor": "purple curve rising from approximately 13 posts per user in 2008 to 17 posts by 2011",
          "text_evidence": "The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the plateau at 17 posts after 2011 reflect behavior only from users who remain active each month?",
          "answer": "Yes, the plateau reflects only active users during each month. Users temporarily or permanently away from Reddit are excluded from the monthly average calculation, which stabilizes the metric around 17-18 posts per user.",
          "visual_anchor": "plateau in the purple curve at approximately 17-18 posts per user from 2011 onwards",
          "text_evidence": "we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.",
          "query_type": "value_context"
        },
        {
          "query": "How does filtering bot-like usernames affect the consistency of the 17-post plateau visible from 2011 to 2014?",
          "answer": "Filtering out bot-like usernames removes automated posting activity that could artificially inflate or add noise to the average. This preprocessing helps ensure the plateau represents genuine human user behavior, making the metric more stable and meaningful.",
          "visual_anchor": "stable plateau around 17 posts per user maintained from 2011 through 2014",
          "text_evidence": "we did light preprocessing to filter out posts by deleted users, posts with no creation time, and posts by authors with bot-like names",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the purple curve rise from 13 to 17 posts between 2008 and 2011 despite the 350,000 unavailable comments?",
        "answer": "The 350,000 missing comments due to API call failures represent a negligible fraction of the 1.65 billion total comments, so they do not significantly impact the observed upward trend in average posts per active user during this period.",
        "visual_anchor": "purple curve rising from approximately 13 posts per user in 2008 to 17 posts by 2011",
        "text_evidence": "The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the plateau at 17 posts after 2011 reflect behavior only from users who remain active each month?",
        "answer": "Yes, the plateau reflects only active users during each month. Users temporarily or permanently away from Reddit are excluded from the monthly average calculation, which stabilizes the metric around 17-18 posts per user.",
        "visual_anchor": "plateau in the purple curve at approximately 17-18 posts per user from 2011 onwards",
        "text_evidence": "we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.",
        "query_type": "value_context"
      },
      {
        "query": "How does filtering bot-like usernames affect the consistency of the 17-post plateau visible from 2011 to 2014?",
        "answer": "Filtering out bot-like usernames removes automated posting activity that could artificially inflate or add noise to the average. This preprocessing helps ensure the plateau represents genuine human user behavior, making the metric more stable and meaningful.",
        "visual_anchor": "stable plateau around 17 posts per user maintained from 2011 through 2014",
        "text_evidence": "we did light preprocessing to filter out posts by deleted users, posts with no creation time, and posts by authors with bot-like names",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "purple/dark line showing average posts per user",
      "y-axis ranging from 0 to 30 posts",
      "x-axis showing time from 2008 to beyond 2014",
      "initial value around 13 posts in 2008",
      "plateau around 17-18 posts from 2011 onwards",
      "upward trend from 2008 to 2011"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_6",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg",
    "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"green solid line (2008)\", \"pink dotted line (2009)\", \"gray dotted line (2010)\", \"teal dashed line (2011)\", \"blue solid line (2012)\", \"black dotted line (2013)\", \"purple dashed line (2013)\", \"black solid line (Overall)\", \"y-axis: Average number of posts per user (0-30)\", \"x-axis: Time (2008-2014)\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green 2008 line reach 30 posts per user while later cohorts plateau lower, given exponential growth?\",\n      \"answer\": \"The exponential growth in user accounts from 2008-2009 shown in Figure 2a created a large influx of new users, which diluted the average posting rate for later cohorts despite the site's overall expansion.\",\n      \"visual_anchor\": \"green solid line (2008) reaching approximately 30 posts per user\",\n      \"text_evidence\": \"After an initial extremely rapid expansion from 2008–2009, the number of users and subreddits have grown exponentially. As of the end of 2014, about 16.2 million distinct users and 327 thousand subreddits made/received at least one post based on our data.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the black Overall line stabilizing around 18 posts per user support analyzing variation of posts over time?\",\n      \"answer\": \"Yes, the Overall line's stabilization around 18 posts per user demonstrates a consistent pattern of user activity variation over time, which is appropriate for analyzing how posting behavior changes temporally.\",\n      \"visual_anchor\": \"black solid Overall line stabilizing around 18 posts per user\",\n      \"text_evidence\": \"Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the 2013 purple dashed line reaching only 20 posts by late 2014 address measuring user activity quantity?\",\n      \"answer\": \"The 2013 cohort's limited growth to 20 posts per user by late 2014 illustrates that measuring user activity quantity through posts over time reveals how newer users contribute less individually, addressing the research question about activity changes.\",\n      \"visual_anchor\": \"purple dashed line (2013) reaching approximately 20 posts per user by late 2014\",\n      \"text_evidence\": \"In this section, we use this measure to address our first research question (RQ1): how does the amount of users' activity change ov\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "green solid line (2008)",
        "pink dotted line (2009)",
        "gray dotted line (2010)",
        "teal dashed line (2011)",
        "blue solid line (2012)",
        "black dotted line (2013)",
        "purple dashed line (2013)",
        "black solid line (Overall)",
        "y-axis: Average number of posts per user (0-30)",
        "x-axis: Time (2008-2014)"
      ],
      "queries": [
        {
          "query": "Why does the green 2008 line reach 30 posts per user while later cohorts plateau lower, given exponential growth?",
          "answer": "The exponential growth in user accounts from 2008-2009 shown in Figure 2a created a large influx of new users, which diluted the average posting rate for later cohorts despite the site's overall expansion.",
          "visual_anchor": "green solid line (2008) reaching approximately 30 posts per user",
          "text_evidence": "After an initial extremely rapid expansion from 2008–2009, the number of users and subreddits have grown exponentially. As of the end of 2014, about 16.2 million distinct users and 327 thousand subreddits made/received at least one post based on our data.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the black Overall line stabilizing around 18 posts per user support analyzing variation of posts over time?",
          "answer": "Yes, the Overall line's stabilization around 18 posts per user demonstrates a consistent pattern of user activity variation over time, which is appropriate for analyzing how posting behavior changes temporally.",
          "visual_anchor": "black solid Overall line stabilizing around 18 posts per user",
          "text_evidence": "Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks.",
          "query_type": "value_context"
        },
        {
          "query": "How does the 2013 purple dashed line reaching only 20 posts by late 2014 address measuring user activity quantity?",
          "answer": "The 2013 cohort's limited growth to 20 posts per user by late 2014 illustrates that measuring user activity quantity through posts over time reveals how newer users contribute less individually, addressing the research question about activity changes.",
          "visual_anchor": "purple dashed line (2013) reaching approximately 20 posts per user by late 2014",
          "text_evidence": "In this section, we use this measure to address our first research question (RQ1): how does the amount of users' activity change ov",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green 2008 line reach 30 posts per user while later cohorts plateau lower, given exponential growth?",
        "answer": "The exponential growth in user accounts from 2008-2009 shown in Figure 2a created a large influx of new users, which diluted the average posting rate for later cohorts despite the site's overall expansion.",
        "visual_anchor": "green solid line (2008) reaching approximately 30 posts per user",
        "text_evidence": "After an initial extremely rapid expansion from 2008–2009, the number of users and subreddits have grown exponentially. As of the end of 2014, about 16.2 million distinct users and 327 thousand subreddits made/received at least one post based on our data.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the black Overall line stabilizing around 18 posts per user support analyzing variation of posts over time?",
        "answer": "Yes, the Overall line's stabilization around 18 posts per user demonstrates a consistent pattern of user activity variation over time, which is appropriate for analyzing how posting behavior changes temporally.",
        "visual_anchor": "black solid Overall line stabilizing around 18 posts per user",
        "text_evidence": "Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks.",
        "query_type": "value_context"
      },
      {
        "query": "How does the 2013 purple dashed line reaching only 20 posts by late 2014 address measuring user activity quantity?",
        "answer": "The 2013 cohort's limited growth to 20 posts per user by late 2014 illustrates that measuring user activity quantity through posts over time reveals how newer users contribute less individually, addressing the research question about activity changes.",
        "visual_anchor": "purple dashed line (2013) reaching approximately 20 posts per user by late 2014",
        "text_evidence": "In this section, we use this measure to address our first research question (RQ1): how does the amount of users' activity change ov",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "green solid line (2008)",
      "pink dotted line (2009)",
      "gray dotted line (2010)",
      "teal dashed line (2011)",
      "blue solid line (2012)",
      "black dotted line (2013)",
      "purple dashed line (2013)",
      "black solid line (Overall)",
      "y-axis: Average number of posts per user (0-30)",
      "x-axis: Time (2008-2014)"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_8",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg",
    "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Green solid line labeled 0 declining from ~8 to near 0\",\n    \"Cyan dotted line labeled 1 declining from ~15 to near 0\",\n    \"Blue dashed line labeled 2 declining from ~17 to ~5\",\n    \"Dark blue solid line labeled 3 declining from ~17 to ~5\",\n    \"Black dotted line labeled 4 remaining steady around 30\",\n    \"Time in user referential on x-axis (0 to 5)\",\n    \"Average number of posts per user on y-axis (0 to 30)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black dotted line labeled 4 remain around 30 posts while all other groups decline?\",\n      \"answer\": \"Users who survived longer (labeled 4) started at higher activity levels than shorter-tenured ones and maintained their engagement throughout their tenure. The figure shows longer-tenured users' sustained posting behavior contrasts with the declining activity of users who churned earlier.\",\n      \"visual_anchor\": \"black dotted line labeled 4 staying at approximately 30 posts per user\",\n      \"text_evidence\": \"For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the green line labeled 0 dropping to near zero by time 1 support quantity-based activity measurement?\",\n      \"answer\": \"Yes, the green line's rapid decline to zero by time 1 demonstrates that the number of posts per user over time effectively captures user disengagement. This validates quantity as a measure of user activity in online communities.\",\n      \"visual_anchor\": \"green solid line labeled 0 declining from ~8 to approximately 0 by time 1\",\n      \"text_evidence\": \"One common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the blue dashed line labeled 2 declining from 17 to 5 relate to cohort posting averages?\",\n      \"answer\": \"The blue dashed line shows users who survived 2-3 years gradually decreasing their posting activity, illustrating the cohort effect where users level off at lower posting averages. This pattern contributes to understanding whether new users follow older users' behavior trajectories.\",\n      \"visual_anchor\": \"blue dashed line labeled 2 declining from approximately 17 to 5 posts\",\n      \"text_evidence\": \"We can already observe a significant cohort effect: users from later cohorts appear to level off at significantly lower posting averages than users from ear\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Green solid line labeled 0 declining from ~8 to near 0",
        "Cyan dotted line labeled 1 declining from ~15 to near 0",
        "Blue dashed line labeled 2 declining from ~17 to ~5",
        "Dark blue solid line labeled 3 declining from ~17 to ~5",
        "Black dotted line labeled 4 remaining steady around 30",
        "Time in user referential on x-axis (0 to 5)",
        "Average number of posts per user on y-axis (0 to 30)"
      ],
      "queries": [
        {
          "query": "Why does the black dotted line labeled 4 remain around 30 posts while all other groups decline?",
          "answer": "Users who survived longer (labeled 4) started at higher activity levels than shorter-tenured ones and maintained their engagement throughout their tenure. The figure shows longer-tenured users' sustained posting behavior contrasts with the declining activity of users who churned earlier.",
          "visual_anchor": "black dotted line labeled 4 staying at approximately 30 posts per user",
          "text_evidence": "For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the green line labeled 0 dropping to near zero by time 1 support quantity-based activity measurement?",
          "answer": "Yes, the green line's rapid decline to zero by time 1 demonstrates that the number of posts per user over time effectively captures user disengagement. This validates quantity as a measure of user activity in online communities.",
          "visual_anchor": "green solid line labeled 0 declining from ~8 to approximately 0 by time 1",
          "text_evidence": "One common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset",
          "query_type": "value_context"
        },
        {
          "query": "How does the blue dashed line labeled 2 declining from 17 to 5 relate to cohort posting averages?",
          "answer": "The blue dashed line shows users who survived 2-3 years gradually decreasing their posting activity, illustrating the cohort effect where users level off at lower posting averages. This pattern contributes to understanding whether new users follow older users' behavior trajectories.",
          "visual_anchor": "blue dashed line labeled 2 declining from approximately 17 to 5 posts",
          "text_evidence": "We can already observe a significant cohort effect: users from later cohorts appear to level off at significantly lower posting averages than users from ear",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black dotted line labeled 4 remain around 30 posts while all other groups decline?",
        "answer": "Users who survived longer (labeled 4) started at higher activity levels than shorter-tenured ones and maintained their engagement throughout their tenure. The figure shows longer-tenured users' sustained posting behavior contrasts with the declining activity of users who churned earlier.",
        "visual_anchor": "black dotted line labeled 4 staying at approximately 30 posts per user",
        "text_evidence": "For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the green line labeled 0 dropping to near zero by time 1 support quantity-based activity measurement?",
        "answer": "Yes, the green line's rapid decline to zero by time 1 demonstrates that the number of posts per user over time effectively captures user disengagement. This validates quantity as a measure of user activity in online communities.",
        "visual_anchor": "green solid line labeled 0 declining from ~8 to approximately 0 by time 1",
        "text_evidence": "One common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset",
        "query_type": "value_context"
      },
      {
        "query": "How does the blue dashed line labeled 2 declining from 17 to 5 relate to cohort posting averages?",
        "answer": "The blue dashed line shows users who survived 2-3 years gradually decreasing their posting activity, illustrating the cohort effect where users level off at lower posting averages. This pattern contributes to understanding whether new users follow older users' behavior trajectories.",
        "visual_anchor": "blue dashed line labeled 2 declining from approximately 17 to 5 posts",
        "text_evidence": "We can already observe a significant cohort effect: users from later cohorts appear to level off at significantly lower posting averages than users from ear",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Green solid line labeled 0 declining from ~8 to near 0",
      "Cyan dotted line labeled 1 declining from ~15 to near 0",
      "Blue dashed line labeled 2 declining from ~17 to ~5",
      "Dark blue solid line labeled 3 declining from ~17 to ~5",
      "Black dotted line labeled 4 remaining steady around 30",
      "Time in user referential on x-axis (0 to 5)",
      "Average number of posts per user on y-axis (0 to 30)"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_13",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg",
    "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Green solid line (cohort 0)\",\n    \"Light blue dotted line (cohort 1)\",\n    \"Dashed blue line (cohort 2)\",\n    \"Dark blue solid line (cohort 3)\",\n    \"Gray dotted line (cohort 4)\",\n    \"Overall darker line showing downward trend\",\n    \"Y-axis: Average comment length (160-230)\",\n    \"X-axis: Time in user referential (0-5)\",\n    \"Sharp initial drop followed by increase in individual cohorts\",\n    \"Peak around 210 for green line at time ~1\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the darker overall line's downward trend in Figure (b) support hypothesis H3 about decreasing user commitment?\",\n      \"answer\": \"The downward trend might suggest decreasing commitment (H3), but this interpretation may not be best according to the analysis, which directs attention to user-referential patterns instead.\",\n      \"visual_anchor\": \"darker overall line showing downward trend in Figure 6b\",\n      \"text_evidence\": \"Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users' commitment to the network is decreasing over time (H3)\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do individual cohort lines rise after time 1 despite the overall average length decreasing over time?\",\n      \"answer\": \"For any individual cohort, comment length increases after a sharp initial drop, even though the overall network average decreases. This reveals a compositional effect where cohort-level behavior differs from aggregate trends.\",\n      \"visual_anchor\": \"cohort lines (green, blue dashed, dark blue solid) rising after initial drop at time ~1\",\n      \"text_evidence\": \"The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the green cohort 0 peak at approximately 210 relate to user survival patterns?\",\n      \"answer\": \"Users who start as longer commenters (like those reaching ~210 length) are more likely to leave Reddit, opposite to the pattern observed for posting activity.\",\n      \"visual_anchor\": \"green solid line (cohort 0) peaking at approximately 210 around time 1\",\n      \"text_evidence\": \"Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Green solid line (cohort 0)",
        "Light blue dotted line (cohort 1)",
        "Dashed blue line (cohort 2)",
        "Dark blue solid line (cohort 3)",
        "Gray dotted line (cohort 4)",
        "Overall darker line showing downward trend",
        "Y-axis: Average comment length (160-230)",
        "X-axis: Time in user referential (0-5)",
        "Sharp initial drop followed by increase in individual cohorts",
        "Peak around 210 for green line at time ~1"
      ],
      "queries": [
        {
          "query": "Does the darker overall line's downward trend in Figure (b) support hypothesis H3 about decreasing user commitment?",
          "answer": "The downward trend might suggest decreasing commitment (H3), but this interpretation may not be best according to the analysis, which directs attention to user-referential patterns instead.",
          "visual_anchor": "darker overall line showing downward trend in Figure 6b",
          "text_evidence": "Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users' commitment to the network is decreasing over time (H3)",
          "query_type": "value_context"
        },
        {
          "query": "Why do individual cohort lines rise after time 1 despite the overall average length decreasing over time?",
          "answer": "For any individual cohort, comment length increases after a sharp initial drop, even though the overall network average decreases. This reveals a compositional effect where cohort-level behavior differs from aggregate trends.",
          "visual_anchor": "cohort lines (green, blue dashed, dark blue solid) rising after initial drop at time ~1",
          "text_evidence": "The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the green cohort 0 peak at approximately 210 relate to user survival patterns?",
          "answer": "Users who start as longer commenters (like those reaching ~210 length) are more likely to leave Reddit, opposite to the pattern observed for posting activity.",
          "visual_anchor": "green solid line (cohort 0) peaking at approximately 210 around time 1",
          "text_evidence": "Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the darker overall line's downward trend in Figure (b) support hypothesis H3 about decreasing user commitment?",
        "answer": "The downward trend might suggest decreasing commitment (H3), but this interpretation may not be best according to the analysis, which directs attention to user-referential patterns instead.",
        "visual_anchor": "darker overall line showing downward trend in Figure 6b",
        "text_evidence": "Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users' commitment to the network is decreasing over time (H3)",
        "query_type": "value_context"
      },
      {
        "query": "Why do individual cohort lines rise after time 1 despite the overall average length decreasing over time?",
        "answer": "For any individual cohort, comment length increases after a sharp initial drop, even though the overall network average decreases. This reveals a compositional effect where cohort-level behavior differs from aggregate trends.",
        "visual_anchor": "cohort lines (green, blue dashed, dark blue solid) rising after initial drop at time ~1",
        "text_evidence": "The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the green cohort 0 peak at approximately 210 relate to user survival patterns?",
        "answer": "Users who start as longer commenters (like those reaching ~210 length) are more likely to leave Reddit, opposite to the pattern observed for posting activity.",
        "visual_anchor": "green solid line (cohort 0) peaking at approximately 210 around time 1",
        "text_evidence": "Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Green solid line (cohort 0)",
      "Light blue dotted line (cohort 1)",
      "Dashed blue line (cohort 2)",
      "Dark blue solid line (cohort 3)",
      "Gray dotted line (cohort 4)",
      "Overall darker line showing downward trend",
      "Y-axis: Average comment length (160-230)",
      "X-axis: Time in user referential (0-5)",
      "Sharp initial drop followed by increase in individual cohorts",
      "Peak around 210 for green line at time ~1"
    ]
  },
  {
    "doc_id": "1603.07025",
    "figure_id": "1603.07025_fig_19",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg",
    "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"cohort 5 (dashed line with circles)\", \"cohort 1 (solid line)\", \"cohort 2 (dashed line)\", \"cohort 3 (dotted line)\", \"cohort 4 (dash-dot line)\", \"y-axis: Comments per submission (0-20)\", \"x-axis: Time in user referential (0-6)\", \"cohort 5 peaks near 20 at time 4\", \"cohort 1 remains below 6 throughout\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does cohort 5's dashed line reach nearly 20 while later cohorts stay below 10 throughout user-referential time?\",\n      \"answer\": \"Users who survive longer in each cohort start with a relatively higher comments per submission ratio than users who abandon Reddit sooner, explaining cohort 5's elevated trajectory.\",\n      \"visual_anchor\": \"cohort 5 dashed line peaking near 20 versus other cohorts below 10\",\n      \"text_evidence\": \"users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does cohort 1's position below cohort 5 at time point 3 contradict the general pattern of early adopters?\",\n      \"answer\": \"Yes, unlike the average posts per month analysis, the early 2008 cohort (cohort 1) ends up below the later cohorts, representing an anomaly.\",\n      \"visual_anchor\": \"cohort 1 solid line remaining below 6 while cohort 5 exceeds 15\",\n      \"text_evidence\": \"Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b)\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do users in cohort 5 who reach 18 comments per submission at time 4 demonstrate hitting the ground running?\",\n      \"answer\": \"Yes, the users who survive the longest in each cohort hit the ground running with a high comment-to-submission ratio relative to users who abandon Reddit more quickly.\",\n      \"visual_anchor\": \"cohort 5 reaching approximately 18 at time point 4\",\n      \"text_evidence\": \"the users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-tosubmission ratio relative to users in their cohort who abandon Reddit more quickly\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "cohort 5 (dashed line with circles)",
        "cohort 1 (solid line)",
        "cohort 2 (dashed line)",
        "cohort 3 (dotted line)",
        "cohort 4 (dash-dot line)",
        "y-axis: Comments per submission (0-20)",
        "x-axis: Time in user referential (0-6)",
        "cohort 5 peaks near 20 at time 4",
        "cohort 1 remains below 6 throughout"
      ],
      "queries": [
        {
          "query": "Why does cohort 5's dashed line reach nearly 20 while later cohorts stay below 10 throughout user-referential time?",
          "answer": "Users who survive longer in each cohort start with a relatively higher comments per submission ratio than users who abandon Reddit sooner, explaining cohort 5's elevated trajectory.",
          "visual_anchor": "cohort 5 dashed line peaking near 20 versus other cohorts below 10",
          "text_evidence": "users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does cohort 1's position below cohort 5 at time point 3 contradict the general pattern of early adopters?",
          "answer": "Yes, unlike the average posts per month analysis, the early 2008 cohort (cohort 1) ends up below the later cohorts, representing an anomaly.",
          "visual_anchor": "cohort 1 solid line remaining below 6 while cohort 5 exceeds 15",
          "text_evidence": "Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b)",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do users in cohort 5 who reach 18 comments per submission at time 4 demonstrate hitting the ground running?",
          "answer": "Yes, the users who survive the longest in each cohort hit the ground running with a high comment-to-submission ratio relative to users who abandon Reddit more quickly.",
          "visual_anchor": "cohort 5 reaching approximately 18 at time point 4",
          "text_evidence": "the users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-tosubmission ratio relative to users in their cohort who abandon Reddit more quickly",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does cohort 5's dashed line reach nearly 20 while later cohorts stay below 10 throughout user-referential time?",
        "answer": "Users who survive longer in each cohort start with a relatively higher comments per submission ratio than users who abandon Reddit sooner, explaining cohort 5's elevated trajectory.",
        "visual_anchor": "cohort 5 dashed line peaking near 20 versus other cohorts below 10",
        "text_evidence": "users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does cohort 1's position below cohort 5 at time point 3 contradict the general pattern of early adopters?",
        "answer": "Yes, unlike the average posts per month analysis, the early 2008 cohort (cohort 1) ends up below the later cohorts, representing an anomaly.",
        "visual_anchor": "cohort 1 solid line remaining below 6 while cohort 5 exceeds 15",
        "text_evidence": "Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b)",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do users in cohort 5 who reach 18 comments per submission at time 4 demonstrate hitting the ground running?",
        "answer": "Yes, the users who survive the longest in each cohort hit the ground running with a high comment-to-submission ratio relative to users who abandon Reddit more quickly.",
        "visual_anchor": "cohort 5 reaching approximately 18 at time point 4",
        "text_evidence": "the users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-tosubmission ratio relative to users in their cohort who abandon Reddit more quickly",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "cohort 5 (dashed line with circles)",
      "cohort 1 (solid line)",
      "cohort 2 (dashed line)",
      "cohort 3 (dotted line)",
      "cohort 4 (dash-dot line)",
      "y-axis: Comments per submission (0-20)",
      "x-axis: Time in user referential (0-6)",
      "cohort 5 peaks near 20 at time 4",
      "cohort 1 remains below 6 throughout"
    ]
  },
  {
    "doc_id": "1607.06520",
    "figure_id": "1607.06520_fig_1",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg",
    "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Scatter plot with dark blue dots representing occupation words\",\n    \"X-axis: she-he axis of w2vNEWS embedding ranging from -0.2 to 0.4\",\n    \"Y-axis: she-he axis of GloVe web-crawl embedding ranging from -0.2 to 0.4\",\n    \"Positive correlation pattern visible in dot distribution\",\n    \"Diagonal trend from bottom-left to top-right\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the diagonal pattern from bottom-left to top-right validate the Spearman correlation of 0.81 for occupation gender bias?\",\n      \"answer\": \"Yes, the strong diagonal pattern from approximately (-0.15, -0.15) to (0.35, 0.25) visually confirms the high positive correlation (ρ = 0.81), indicating that occupation gender biases are highly consistent across the two embedding methods.\",\n      \"visual_anchor\": \"Diagonal trend of blue dots from bottom-left (-0.15, -0.15) to top-right (0.35, 0.25)\",\n      \"text_evidence\": \"the gender bias of occupations is highly consistent across embeddings (Spearman $\\\\rho = 0 . 8 1$ )\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do most occupation dots cluster between -0.1 and 0.1 on both axes despite using 300-dimensional word2vec?\",\n      \"answer\": \"The clustering reflects that most occupations have relatively neutral gender bias when projected onto the she-he direction, with only a few occupations showing extreme bias. The dimensionality of 300 is the embedding space size, not the projection axis range.\",\n      \"visual_anchor\": \"Dense cluster of blue dots concentrated in the central region between -0.1 and 0.1 on both axes\",\n      \"text_evidence\": \"the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a $d = 3 0 0$ -dimensional word2vec [24, 25] embedding\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the cosine similarity normalization relate to the occupation word positions scattered between x=-0.2 and x=0.4?\",\n      \"answer\": \"The x-axis positions represent normalized cosine similarities (dot products) between occupation word vectors and the she-he direction in w2vNEWS embedding. Since words are normalized, the cosine similarity equals the dot product, producing values ranging from -0.2 to 0.4.\",\n      \"visual_anchor\": \"X-axis range from -0.2 to 0.4 showing scattered occupation word positions\",\n      \"text_evidence\": \"Since words are normalized $\\\\cos ( \\\\vec { w } _ { 1 } , \\\\vec { w } _ { 2 } ) = \\\\vec { w } _ { 1 } \\\\cdot \\\\vec { w } _ { 2 }$\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Scatter plot with dark blue dots representing occupation words",
        "X-axis: she-he axis of w2vNEWS embedding ranging from -0.2 to 0.4",
        "Y-axis: she-he axis of GloVe web-crawl embedding ranging from -0.2 to 0.4",
        "Positive correlation pattern visible in dot distribution",
        "Diagonal trend from bottom-left to top-right"
      ],
      "queries": [
        {
          "query": "Does the diagonal pattern from bottom-left to top-right validate the Spearman correlation of 0.81 for occupation gender bias?",
          "answer": "Yes, the strong diagonal pattern from approximately (-0.15, -0.15) to (0.35, 0.25) visually confirms the high positive correlation (ρ = 0.81), indicating that occupation gender biases are highly consistent across the two embedding methods.",
          "visual_anchor": "Diagonal trend of blue dots from bottom-left (-0.15, -0.15) to top-right (0.35, 0.25)",
          "text_evidence": "the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ )",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do most occupation dots cluster between -0.1 and 0.1 on both axes despite using 300-dimensional word2vec?",
          "answer": "The clustering reflects that most occupations have relatively neutral gender bias when projected onto the she-he direction, with only a few occupations showing extreme bias. The dimensionality of 300 is the embedding space size, not the projection axis range.",
          "visual_anchor": "Dense cluster of blue dots concentrated in the central region between -0.1 and 0.1 on both axes",
          "text_evidence": "the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a $d = 3 0 0$ -dimensional word2vec [24, 25] embedding",
          "query_type": "value_context"
        },
        {
          "query": "How does the cosine similarity normalization relate to the occupation word positions scattered between x=-0.2 and x=0.4?",
          "answer": "The x-axis positions represent normalized cosine similarities (dot products) between occupation word vectors and the she-he direction in w2vNEWS embedding. Since words are normalized, the cosine similarity equals the dot product, producing values ranging from -0.2 to 0.4.",
          "visual_anchor": "X-axis range from -0.2 to 0.4 showing scattered occupation word positions",
          "text_evidence": "Since words are normalized $\\cos ( \\vec { w } _ { 1 } , \\vec { w } _ { 2 } ) = \\vec { w } _ { 1 } \\cdot \\vec { w } _ { 2 }$",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the diagonal pattern from bottom-left to top-right validate the Spearman correlation of 0.81 for occupation gender bias?",
        "answer": "Yes, the strong diagonal pattern from approximately (-0.15, -0.15) to (0.35, 0.25) visually confirms the high positive correlation (ρ = 0.81), indicating that occupation gender biases are highly consistent across the two embedding methods.",
        "visual_anchor": "Diagonal trend of blue dots from bottom-left (-0.15, -0.15) to top-right (0.35, 0.25)",
        "text_evidence": "the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ )",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do most occupation dots cluster between -0.1 and 0.1 on both axes despite using 300-dimensional word2vec?",
        "answer": "The clustering reflects that most occupations have relatively neutral gender bias when projected onto the she-he direction, with only a few occupations showing extreme bias. The dimensionality of 300 is the embedding space size, not the projection axis range.",
        "visual_anchor": "Dense cluster of blue dots concentrated in the central region between -0.1 and 0.1 on both axes",
        "text_evidence": "the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a $d = 3 0 0$ -dimensional word2vec [24, 25] embedding",
        "query_type": "value_context"
      },
      {
        "query": "How does the cosine similarity normalization relate to the occupation word positions scattered between x=-0.2 and x=0.4?",
        "answer": "The x-axis positions represent normalized cosine similarities (dot products) between occupation word vectors and the she-he direction in w2vNEWS embedding. Since words are normalized, the cosine similarity equals the dot product, producing values ranging from -0.2 to 0.4.",
        "visual_anchor": "X-axis range from -0.2 to 0.4 showing scattered occupation word positions",
        "text_evidence": "Since words are normalized $\\cos ( \\vec { w } _ { 1 } , \\vec { w } _ { 2 } ) = \\vec { w } _ { 1 } \\cdot \\vec { w } _ { 2 }$",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Scatter plot with dark blue dots representing occupation words",
      "X-axis: she-he axis of w2vNEWS embedding ranging from -0.2 to 0.4",
      "Y-axis: she-he axis of GloVe web-crawl embedding ranging from -0.2 to 0.4",
      "Positive correlation pattern visible in dot distribution",
      "Diagonal trend from bottom-left to top-right"
    ]
  },
  {
    "doc_id": "1607.06520",
    "figure_id": "1607.06520_fig_2",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg",
    "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue bar chart\", \"first bar at height ~0.6\", \"second bar at height ~0.12\", \"ten bars total decreasing in height\", \"y-axis from 0 to 0.7\", \"x-axis from 0 to 10\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the first bar at 0.6 explain more variance than others in gender pair vector differences?\",\n      \"answer\": \"There is a single direction that explains the majority of variance in the normalized gender pair difference vectors, indicating that despite polysemy and sampling randomness, gender biases align along one dominant dimension.\",\n      \"visual_anchor\": \"first blue bar at height approximately 0.6\",\n      \"text_evidence\": \"As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the rapid drop from the first bar at 0.6 to subsequent bars below 0.15 contradict parallel gender differences?\",\n      \"answer\": \"No, the steep drop confirms that gender pair differences are not parallel in practice due to different biases, polysemy (like grandfather as in to grandfather a regulation), and randomness in finite samples, but they share one dominant direction.\",\n      \"visual_anchor\": \"steep decline from first bar (~0.6) to second bar (~0.12)\",\n      \"text_evidence\": \"However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the first component at 0.6 relate to measuring strict bias where c equals zero?\",\n      \"answer\": \"The dominant component at 0.6 captures the primary gender bias direction g. When c=0, the bias measure is binary: it equals 0 only if the word vector has no overlap with this dominant gender direction, otherwise 1.\",\n      \"visual_anchor\": \"top component bar at approximately 0.6 height\",\n      \"text_evidence\": \"where $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\\\mathrm { c o s } ( \\\\vec { w } - g ) | ^ { c } = 0$ only if $\\\\vec { w }$ has no overlap with $g$ and otherwise it is 1.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue bar chart",
        "first bar at height ~0.6",
        "second bar at height ~0.12",
        "ten bars total decreasing in height",
        "y-axis from 0 to 0.7",
        "x-axis from 0 to 10"
      ],
      "queries": [
        {
          "query": "Why does the first bar at 0.6 explain more variance than others in gender pair vector differences?",
          "answer": "There is a single direction that explains the majority of variance in the normalized gender pair difference vectors, indicating that despite polysemy and sampling randomness, gender biases align along one dominant dimension.",
          "visual_anchor": "first blue bar at height approximately 0.6",
          "text_evidence": "As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors.",
          "query_type": "value_context"
        },
        {
          "query": "Does the rapid drop from the first bar at 0.6 to subsequent bars below 0.15 contradict parallel gender differences?",
          "answer": "No, the steep drop confirms that gender pair differences are not parallel in practice due to different biases, polysemy (like grandfather as in to grandfather a regulation), and randomness in finite samples, but they share one dominant direction.",
          "visual_anchor": "steep decline from first bar (~0.6) to second bar (~0.12)",
          "text_evidence": "However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the first component at 0.6 relate to measuring strict bias where c equals zero?",
          "answer": "The dominant component at 0.6 captures the primary gender bias direction g. When c=0, the bias measure is binary: it equals 0 only if the word vector has no overlap with this dominant gender direction, otherwise 1.",
          "visual_anchor": "top component bar at approximately 0.6 height",
          "text_evidence": "where $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the first bar at 0.6 explain more variance than others in gender pair vector differences?",
        "answer": "There is a single direction that explains the majority of variance in the normalized gender pair difference vectors, indicating that despite polysemy and sampling randomness, gender biases align along one dominant dimension.",
        "visual_anchor": "first blue bar at height approximately 0.6",
        "text_evidence": "As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors.",
        "query_type": "value_context"
      },
      {
        "query": "Does the rapid drop from the first bar at 0.6 to subsequent bars below 0.15 contradict parallel gender differences?",
        "answer": "No, the steep drop confirms that gender pair differences are not parallel in practice due to different biases, polysemy (like grandfather as in to grandfather a regulation), and randomness in finite samples, but they share one dominant direction.",
        "visual_anchor": "steep decline from first bar (~0.6) to second bar (~0.12)",
        "text_evidence": "However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the first component at 0.6 relate to measuring strict bias where c equals zero?",
        "answer": "The dominant component at 0.6 captures the primary gender bias direction g. When c=0, the bias measure is binary: it equals 0 only if the word vector has no overlap with this dominant gender direction, otherwise 1.",
        "visual_anchor": "top component bar at approximately 0.6 height",
        "text_evidence": "where $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue bar chart",
      "first bar at height ~0.6",
      "second bar at height ~0.12",
      "ten bars total decreasing in height",
      "y-axis from 0 to 0.7",
      "x-axis from 0 to 10"
    ]
  },
  {
    "doc_id": "1607.06520",
    "figure_id": "1607.06520_fig_4",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg",
    "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"horizontal red dashed line separating gender-neutral (above) from gender-specific (below)\",\n    \"vertical axis labeled y representing gender neutrality direction\",\n    \"horizontal axis labeled x representing he-she difference projection\",\n    \"words positioned above horizontal line: homemaker, feminist, sassy, actresses, queen, sisters, ladies, daughters, grandmother\",\n    \"words positioned below horizontal line: babe, witch, witches, boys, dads, wives, girlfriend, wife, daddy, fiancee, cousin, sons, son, brothers, nephew, chap, lad, boyhood, beard, he\",\n    \"words near vertical center: folks, friend, priest, mate, pal, brass, buddies, burly\",\n    \"gender-neutral words like 'folks' and 'priest' positioned near vertical line after debiasing\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why would words above the horizontal red dashed line collapse to the vertical line after applying the debiasing algorithm?\",\n      \"answer\": \"The hard debiasing algorithm removes gender pair associations for gender neutral words, which are positioned above the horizontal line. Collapsing them to the vertical line eliminates their projection along the he-she axis while preserving gender neutrality.\",\n      \"visual_anchor\": \"horizontal red dashed line with words like 'homemaker', 'feminist', 'actresses' above it\",\n      \"text_evidence\": \"Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does receptionist's closer proximity to softball than football demonstrate indirect bias even after removing gender pairs from the embedding?\",\n      \"answer\": \"Yes, indirect gender association persists because gender-neutral words like receptionist remain closer to stereotypically female activities (softball) than male ones (football), even when explicit gender words are removed. This shows that simple word removal does not eliminate all gender bias in embeddings.\",\n      \"visual_anchor\": \"words positioned along x-axis showing distance relationships between gendered and neutral terms\",\n      \"text_evidence\": \"There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the y-axis direction separate words like daughters below the line from words like priest near the center?\",\n      \"answer\": \"The y-axis represents a learned direction capturing gender neutrality, with gender-specific words like 'daughters' positioned below the line and gender-neutral words like 'priest' positioned above or near the line. This learned direction enables the classifier to distinguish gender-specific from gender-neutral words.\",\n      \"visual_anchor\": \"y-axis with 'daughters' below horizontal line and 'priest' near center vertical line\",\n      \"text_evidence\": \"y is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "horizontal red dashed line separating gender-neutral (above) from gender-specific (below)",
        "vertical axis labeled y representing gender neutrality direction",
        "horizontal axis labeled x representing he-she difference projection",
        "words positioned above horizontal line: homemaker, feminist, sassy, actresses, queen, sisters, ladies, daughters, grandmother",
        "words positioned below horizontal line: babe, witch, witches, boys, dads, wives, girlfriend, wife, daddy, fiancee, cousin, sons, son, brothers, nephew, chap, lad, boyhood, beard, he",
        "words near vertical center: folks, friend, priest, mate, pal, brass, buddies, burly",
        "gender-neutral words like 'folks' and 'priest' positioned near vertical line after debiasing"
      ],
      "queries": [
        {
          "query": "Why would words above the horizontal red dashed line collapse to the vertical line after applying the debiasing algorithm?",
          "answer": "The hard debiasing algorithm removes gender pair associations for gender neutral words, which are positioned above the horizontal line. Collapsing them to the vertical line eliminates their projection along the he-she axis while preserving gender neutrality.",
          "visual_anchor": "horizontal red dashed line with words like 'homemaker', 'feminist', 'actresses' above it",
          "text_evidence": "Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does receptionist's closer proximity to softball than football demonstrate indirect bias even after removing gender pairs from the embedding?",
          "answer": "Yes, indirect gender association persists because gender-neutral words like receptionist remain closer to stereotypically female activities (softball) than male ones (football), even when explicit gender words are removed. This shows that simple word removal does not eliminate all gender bias in embeddings.",
          "visual_anchor": "words positioned along x-axis showing distance relationships between gendered and neutral terms",
          "text_evidence": "There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the y-axis direction separate words like daughters below the line from words like priest near the center?",
          "answer": "The y-axis represents a learned direction capturing gender neutrality, with gender-specific words like 'daughters' positioned below the line and gender-neutral words like 'priest' positioned above or near the line. This learned direction enables the classifier to distinguish gender-specific from gender-neutral words.",
          "visual_anchor": "y-axis with 'daughters' below horizontal line and 'priest' near center vertical line",
          "text_evidence": "y is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why would words above the horizontal red dashed line collapse to the vertical line after applying the debiasing algorithm?",
        "answer": "The hard debiasing algorithm removes gender pair associations for gender neutral words, which are positioned above the horizontal line. Collapsing them to the vertical line eliminates their projection along the he-she axis while preserving gender neutrality.",
        "visual_anchor": "horizontal red dashed line with words like 'homemaker', 'feminist', 'actresses' above it",
        "text_evidence": "Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does receptionist's closer proximity to softball than football demonstrate indirect bias even after removing gender pairs from the embedding?",
        "answer": "Yes, indirect gender association persists because gender-neutral words like receptionist remain closer to stereotypically female activities (softball) than male ones (football), even when explicit gender words are removed. This shows that simple word removal does not eliminate all gender bias in embeddings.",
        "visual_anchor": "words positioned along x-axis showing distance relationships between gendered and neutral terms",
        "text_evidence": "There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the y-axis direction separate words like daughters below the line from words like priest near the center?",
        "answer": "The y-axis represents a learned direction capturing gender neutrality, with gender-specific words like 'daughters' positioned below the line and gender-neutral words like 'priest' positioned above or near the line. This learned direction enables the classifier to distinguish gender-specific from gender-neutral words.",
        "visual_anchor": "y-axis with 'daughters' below horizontal line and 'priest' near center vertical line",
        "text_evidence": "y is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "horizontal red dashed line separating gender-neutral (above) from gender-specific (below)",
      "vertical axis labeled y representing gender neutrality direction",
      "horizontal axis labeled x representing he-she difference projection",
      "words positioned above horizontal line: homemaker, feminist, sassy, actresses, queen, sisters, ladies, daughters, grandmother",
      "words positioned below horizontal line: babe, witch, witches, boys, dads, wives, girlfriend, wife, daddy, fiancee, cousin, sons, son, brothers, nephew, chap, lad, boyhood, beard, he",
      "words near vertical center: folks, friend, priest, mate, pal, brass, buddies, burly",
      "gender-neutral words like 'folks' and 'priest' positioned near vertical line after debiasing"
    ]
  },
  {
    "doc_id": "1607.06520",
    "figure_id": "1607.06520_fig_5",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg",
    "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"blue circles with dashed line labeled 'before'\",\n    \"green line with x markers labeled 'hard-debiased'\",\n    \"red triangles with dashed line labeled 'soft-debiased'\",\n    \"x-axis: number of generated analogies (0-160)\",\n    \"y-axis: number of stereotypic analogies (0-30)\",\n    \"steep upward trend for 'before' and 'soft-debiased' curves\",\n    \"flat, minimal growth for 'hard-debiased' curve\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green hard-debiased curve remain below 10 stereotypic analogies across all 160 generated analogies?\",\n      \"answer\": \"The hard-debiasing algorithm successfully reduces both direct and indirect gender biases while preserving desirable embedding properties, keeping stereotypical analogies minimal even as more analogies are generated.\",\n      \"visual_anchor\": \"green line with x markers staying below y=10 across x-axis 0-160\",\n      \"text_evidence\": \"We evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the soft-debiased red curve reaching 25 stereotypic analogies at 160 generated analogies align with crowd-worker ratings?\",\n      \"answer\": \"Yes, the visual trend is consistent with the finding that 29 out of 150 analogies were rated as exhibiting gender stereotype by five or more crowd-workers, showing soft-debiasing still allows substantial stereotypical content.\",\n      \"visual_anchor\": \"red triangles with dashed line reaching approximately y=25 at x=160\",\n      \"text_evidence\": \"Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the blue before and red soft-debiased curves both exceed 20 stereotypic analogies despite different debiasing approaches?\",\n      \"answer\": \"Soft-debiasing does not eliminate stereotypical patterns as effectively as hard-debiasing, resulting in a trajectory similar to the original undebiased embedding, though with slightly lower stereotypical analogy counts.\",\n      \"visual_anchor\": \"blue circles and red triangles both rising above y=20 after x=120\",\n      \"text_evidence\": \"Figure 8 shows the results.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue circles with dashed line labeled 'before'",
        "green line with x markers labeled 'hard-debiased'",
        "red triangles with dashed line labeled 'soft-debiased'",
        "x-axis: number of generated analogies (0-160)",
        "y-axis: number of stereotypic analogies (0-30)",
        "steep upward trend for 'before' and 'soft-debiased' curves",
        "flat, minimal growth for 'hard-debiased' curve"
      ],
      "queries": [
        {
          "query": "Why does the green hard-debiased curve remain below 10 stereotypic analogies across all 160 generated analogies?",
          "answer": "The hard-debiasing algorithm successfully reduces both direct and indirect gender biases while preserving desirable embedding properties, keeping stereotypical analogies minimal even as more analogies are generated.",
          "visual_anchor": "green line with x markers staying below y=10 across x-axis 0-160",
          "text_evidence": "We evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the soft-debiased red curve reaching 25 stereotypic analogies at 160 generated analogies align with crowd-worker ratings?",
          "answer": "Yes, the visual trend is consistent with the finding that 29 out of 150 analogies were rated as exhibiting gender stereotype by five or more crowd-workers, showing soft-debiasing still allows substantial stereotypical content.",
          "visual_anchor": "red triangles with dashed line reaching approximately y=25 at x=160",
          "text_evidence": "Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8).",
          "query_type": "value_context"
        },
        {
          "query": "Why do the blue before and red soft-debiased curves both exceed 20 stereotypic analogies despite different debiasing approaches?",
          "answer": "Soft-debiasing does not eliminate stereotypical patterns as effectively as hard-debiasing, resulting in a trajectory similar to the original undebiased embedding, though with slightly lower stereotypical analogy counts.",
          "visual_anchor": "blue circles and red triangles both rising above y=20 after x=120",
          "text_evidence": "Figure 8 shows the results.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green hard-debiased curve remain below 10 stereotypic analogies across all 160 generated analogies?",
        "answer": "The hard-debiasing algorithm successfully reduces both direct and indirect gender biases while preserving desirable embedding properties, keeping stereotypical analogies minimal even as more analogies are generated.",
        "visual_anchor": "green line with x markers staying below y=10 across x-axis 0-160",
        "text_evidence": "We evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the soft-debiased red curve reaching 25 stereotypic analogies at 160 generated analogies align with crowd-worker ratings?",
        "answer": "Yes, the visual trend is consistent with the finding that 29 out of 150 analogies were rated as exhibiting gender stereotype by five or more crowd-workers, showing soft-debiasing still allows substantial stereotypical content.",
        "visual_anchor": "red triangles with dashed line reaching approximately y=25 at x=160",
        "text_evidence": "Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8).",
        "query_type": "value_context"
      },
      {
        "query": "Why do the blue before and red soft-debiased curves both exceed 20 stereotypic analogies despite different debiasing approaches?",
        "answer": "Soft-debiasing does not eliminate stereotypical patterns as effectively as hard-debiasing, resulting in a trajectory similar to the original undebiased embedding, though with slightly lower stereotypical analogy counts.",
        "visual_anchor": "blue circles and red triangles both rising above y=20 after x=120",
        "text_evidence": "Figure 8 shows the results.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue circles with dashed line labeled 'before'",
      "green line with x markers labeled 'hard-debiased'",
      "red triangles with dashed line labeled 'soft-debiased'",
      "x-axis: number of generated analogies (0-160)",
      "y-axis: number of stereotypic analogies (0-30)",
      "steep upward trend for 'before' and 'soft-debiased' curves",
      "flat, minimal growth for 'hard-debiased' curve"
    ]
  },
  {
    "doc_id": "1608.07187",
    "figure_id": "1608.07187_fig_1",
    "figure_number": null,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg",
    "caption": "",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red dots clustered at top right (y=1.5-1.7, x=80-100)\",\n    \"Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)\",\n    \"Yellow/green dots centered around middle (y=-0.5 to 0, x=30-60)\",\n    \"Orange dots in upper middle region (y=0.2-0.9, x=50-80)\",\n    \"Gradient color scheme from blue to red\",\n    \"Y-axis: Strength of association of occupation word vector with female gender\",\n    \"X-axis: Percentage of workers in occupation who are women\",\n    \"Horizontal line at y=0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do red dots at x=80-100 show y values above 1.5, given that female terms are less associated with sciences?\",\n      \"answer\": \"The red dots represent occupations with high female participation (80-100%), showing strong positive association with female gender. These likely represent arts/language fields, not sciences, where Nosek et al. found female terms less associated.\",\n      \"visual_anchor\": \"Red dots clustered at top right corner (y=1.5-1.7, x=80-100)\",\n      \"text_evidence\": \"In another laboratory study, Nosek et al. (2002b) found that female terms are less associated with the sciences, and male terms less associated with the arts.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the Pearson correlation coefficient of 0.90 explain why blue dots at x=0-20 correspond to y values below negative one?\",\n      \"answer\": \"Yes, the strong positive correlation (ρ=0.90, p<10^-18) indicates that as percentage of women workers decreases (x=0-20), the association with female gender becomes strongly negative (y<-1), showing male-dominated occupations have strong male associations.\",\n      \"visual_anchor\": \"Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)\",\n      \"text_evidence\": \"Pearson's correlation coefficient $\\\\rho = 0 . 9 0$ with $p$ -value $< 1 0 ^ { - 1 8 }$ .\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes the orange dots at x=50-80 to span y=0.2-0.9 despite the reported effect size of 1.47?\",\n      \"answer\": \"The effect size of 1.47 from Nosek et al. measures the strength of math/science vs. arts/language associations in IAT tests. The orange dots represent real-world occupations with moderate female participation, showing weaker but positive female associations, which differs from the controlled IAT experiment measuring category-level stereotypes.\",\n      \"visual_anchor\": \"Orange dots in upper middle region (y=0.2-0.9, x=50-80)\",\n      \"text_evidence\": \"83 subjects took the IAT with a combination of math/science and art/language attributes, and the expected associations were observed with an effect size of 1.47 and a $p$ -value of $1 0 ^ { - 2 4 }$ , Nosek et al. (2002b, p. 51).\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red dots clustered at top right (y=1.5-1.7, x=80-100)",
        "Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)",
        "Yellow/green dots centered around middle (y=-0.5 to 0, x=30-60)",
        "Orange dots in upper middle region (y=0.2-0.9, x=50-80)",
        "Gradient color scheme from blue to red",
        "Y-axis: Strength of association of occupation word vector with female gender",
        "X-axis: Percentage of workers in occupation who are women",
        "Horizontal line at y=0"
      ],
      "queries": [
        {
          "query": "Why do red dots at x=80-100 show y values above 1.5, given that female terms are less associated with sciences?",
          "answer": "The red dots represent occupations with high female participation (80-100%), showing strong positive association with female gender. These likely represent arts/language fields, not sciences, where Nosek et al. found female terms less associated.",
          "visual_anchor": "Red dots clustered at top right corner (y=1.5-1.7, x=80-100)",
          "text_evidence": "In another laboratory study, Nosek et al. (2002b) found that female terms are less associated with the sciences, and male terms less associated with the arts.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the Pearson correlation coefficient of 0.90 explain why blue dots at x=0-20 correspond to y values below negative one?",
          "answer": "Yes, the strong positive correlation (ρ=0.90, p<10^-18) indicates that as percentage of women workers decreases (x=0-20), the association with female gender becomes strongly negative (y<-1), showing male-dominated occupations have strong male associations.",
          "visual_anchor": "Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)",
          "text_evidence": "Pearson's correlation coefficient $\\rho = 0 . 9 0$ with $p$ -value $< 1 0 ^ { - 1 8 }$ .",
          "query_type": "value_context"
        },
        {
          "query": "What causes the orange dots at x=50-80 to span y=0.2-0.9 despite the reported effect size of 1.47?",
          "answer": "The effect size of 1.47 from Nosek et al. measures the strength of math/science vs. arts/language associations in IAT tests. The orange dots represent real-world occupations with moderate female participation, showing weaker but positive female associations, which differs from the controlled IAT experiment measuring category-level stereotypes.",
          "visual_anchor": "Orange dots in upper middle region (y=0.2-0.9, x=50-80)",
          "text_evidence": "83 subjects took the IAT with a combination of math/science and art/language attributes, and the expected associations were observed with an effect size of 1.47 and a $p$ -value of $1 0 ^ { - 2 4 }$ , Nosek et al. (2002b, p. 51).",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do red dots at x=80-100 show y values above 1.5, given that female terms are less associated with sciences?",
        "answer": "The red dots represent occupations with high female participation (80-100%), showing strong positive association with female gender. These likely represent arts/language fields, not sciences, where Nosek et al. found female terms less associated.",
        "visual_anchor": "Red dots clustered at top right corner (y=1.5-1.7, x=80-100)",
        "text_evidence": "In another laboratory study, Nosek et al. (2002b) found that female terms are less associated with the sciences, and male terms less associated with the arts.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the Pearson correlation coefficient of 0.90 explain why blue dots at x=0-20 correspond to y values below negative one?",
        "answer": "Yes, the strong positive correlation (ρ=0.90, p<10^-18) indicates that as percentage of women workers decreases (x=0-20), the association with female gender becomes strongly negative (y<-1), showing male-dominated occupations have strong male associations.",
        "visual_anchor": "Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)",
        "text_evidence": "Pearson's correlation coefficient $\\rho = 0 . 9 0$ with $p$ -value $< 1 0 ^ { - 1 8 }$ .",
        "query_type": "value_context"
      },
      {
        "query": "What causes the orange dots at x=50-80 to span y=0.2-0.9 despite the reported effect size of 1.47?",
        "answer": "The effect size of 1.47 from Nosek et al. measures the strength of math/science vs. arts/language associations in IAT tests. The orange dots represent real-world occupations with moderate female participation, showing weaker but positive female associations, which differs from the controlled IAT experiment measuring category-level stereotypes.",
        "visual_anchor": "Orange dots in upper middle region (y=0.2-0.9, x=50-80)",
        "text_evidence": "83 subjects took the IAT with a combination of math/science and art/language attributes, and the expected associations were observed with an effect size of 1.47 and a $p$ -value of $1 0 ^ { - 2 4 }$ , Nosek et al. (2002b, p. 51).",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red dots clustered at top right (y=1.5-1.7, x=80-100)",
      "Blue dots clustered at bottom left (y=-1.2 to -1.5, x=0-20)",
      "Yellow/green dots centered around middle (y=-0.5 to 0, x=30-60)",
      "Orange dots in upper middle region (y=0.2-0.9, x=50-80)",
      "Gradient color scheme from blue to red",
      "Y-axis: Strength of association of occupation word vector with female gender",
      "X-axis: Percentage of workers in occupation who are women",
      "Horizontal line at y=0"
    ]
  },
  {
    "doc_id": "1608.07187",
    "figure_id": "1608.07187_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg",
    "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"2D projection showing word vectors as points with labels\",\n    \"Dashed blue lines connecting related word pairs\",\n    \"Gender-related word pairs: brother-sister, nephew-niece, uncle-aunt\",\n    \"Royalty word pairs: king-queen, duke-duchess, emperor-empress\",\n    \"Family word pairs: man-woman, sir-madam, heir-heiress\",\n    \"Vertical axis ranging from -0.5 to 0.5\",\n    \"Horizontal axis ranging from -0.5 to 0.6\",\n    \"Consistent vector differences shown by parallel dashed lines\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the parallel dashed lines connecting brother-sister and nephew-niece demonstrate the algebraic relationships enabling the 0.84 correlation coefficient prediction?\",\n      \"answer\": \"Yes, the parallel vector differences between gender pairs in the 300-dimensional space (shown in 2D projection) demonstrate the algebraic relationships that WEFAT exploits to predict gender percentages with ρ=0.84 correlation.\",\n      \"visual_anchor\": \"Parallel dashed blue lines connecting brother-sister (left side, y≈-0.2 to 0.25) and nephew-niece (left side, y≈0.0 to 0.35)\",\n      \"text_evidence\": \"By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson's correlation coefficient of ρ = 0.84\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the king-queen and duke-duchess vector pairs at the right side exhibit roughly constant differences despite different semantic contexts?\",\n      \"answer\": \"The roughly constant vector differences reflect that pairs of words differing only by gender map to pairs of vectors with approximately constant vector difference, illustrating the algebraic relationships between related words independent of their specific royal or noble context.\",\n      \"visual_anchor\": \"Parallel dashed lines connecting king-queen (right side, y≈-0.5 to -0.13) and duke-duchess (right side, y≈-0.16 to 0.29)\",\n      \"text_evidence\": \"The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the heir-heiress vector relationship at horizontal position 0.1 exemplify the semantic relationships captured alongside countries-capitals and companies-CEOs?\",\n      \"answer\": \"Yes, the heir-heiress pair demonstrates the same type of algebraic vector relationship shown for gender pairs, which is analogous to other semantic relationships like countries and capitals or companies and CEOs mentioned in the methodology.\",\n      \"visual_anchor\": \"Dashed line connecting heir (y≈0.03) to heiress (y≈0.5) at horizontal position around 0.1\",\n      \"text_evidence\": \"Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "2D projection showing word vectors as points with labels",
        "Dashed blue lines connecting related word pairs",
        "Gender-related word pairs: brother-sister, nephew-niece, uncle-aunt",
        "Royalty word pairs: king-queen, duke-duchess, emperor-empress",
        "Family word pairs: man-woman, sir-madam, heir-heiress",
        "Vertical axis ranging from -0.5 to 0.5",
        "Horizontal axis ranging from -0.5 to 0.6",
        "Consistent vector differences shown by parallel dashed lines"
      ],
      "queries": [
        {
          "query": "Do the parallel dashed lines connecting brother-sister and nephew-niece demonstrate the algebraic relationships enabling the 0.84 correlation coefficient prediction?",
          "answer": "Yes, the parallel vector differences between gender pairs in the 300-dimensional space (shown in 2D projection) demonstrate the algebraic relationships that WEFAT exploits to predict gender percentages with ρ=0.84 correlation.",
          "visual_anchor": "Parallel dashed blue lines connecting brother-sister (left side, y≈-0.2 to 0.25) and nephew-niece (left side, y≈0.0 to 0.35)",
          "text_evidence": "By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson's correlation coefficient of ρ = 0.84",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the king-queen and duke-duchess vector pairs at the right side exhibit roughly constant differences despite different semantic contexts?",
          "answer": "The roughly constant vector differences reflect that pairs of words differing only by gender map to pairs of vectors with approximately constant vector difference, illustrating the algebraic relationships between related words independent of their specific royal or noble context.",
          "visual_anchor": "Parallel dashed lines connecting king-queen (right side, y≈-0.5 to -0.13) and duke-duchess (right side, y≈-0.16 to 0.29)",
          "text_evidence": "The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the heir-heiress vector relationship at horizontal position 0.1 exemplify the semantic relationships captured alongside countries-capitals and companies-CEOs?",
          "answer": "Yes, the heir-heiress pair demonstrates the same type of algebraic vector relationship shown for gender pairs, which is analogous to other semantic relationships like countries and capitals or companies and CEOs mentioned in the methodology.",
          "visual_anchor": "Dashed line connecting heir (y≈0.03) to heiress (y≈0.5) at horizontal position around 0.1",
          "text_evidence": "Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the parallel dashed lines connecting brother-sister and nephew-niece demonstrate the algebraic relationships enabling the 0.84 correlation coefficient prediction?",
        "answer": "Yes, the parallel vector differences between gender pairs in the 300-dimensional space (shown in 2D projection) demonstrate the algebraic relationships that WEFAT exploits to predict gender percentages with ρ=0.84 correlation.",
        "visual_anchor": "Parallel dashed blue lines connecting brother-sister (left side, y≈-0.2 to 0.25) and nephew-niece (left side, y≈0.0 to 0.35)",
        "text_evidence": "By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson's correlation coefficient of ρ = 0.84",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the king-queen and duke-duchess vector pairs at the right side exhibit roughly constant differences despite different semantic contexts?",
        "answer": "The roughly constant vector differences reflect that pairs of words differing only by gender map to pairs of vectors with approximately constant vector difference, illustrating the algebraic relationships between related words independent of their specific royal or noble context.",
        "visual_anchor": "Parallel dashed lines connecting king-queen (right side, y≈-0.5 to -0.13) and duke-duchess (right side, y≈-0.16 to 0.29)",
        "text_evidence": "The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the heir-heiress vector relationship at horizontal position 0.1 exemplify the semantic relationships captured alongside countries-capitals and companies-CEOs?",
        "answer": "Yes, the heir-heiress pair demonstrates the same type of algebraic vector relationship shown for gender pairs, which is analogous to other semantic relationships like countries and capitals or companies and CEOs mentioned in the methodology.",
        "visual_anchor": "Dashed line connecting heir (y≈0.03) to heiress (y≈0.5) at horizontal position around 0.1",
        "text_evidence": "Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "2D projection showing word vectors as points with labels",
      "Dashed blue lines connecting related word pairs",
      "Gender-related word pairs: brother-sister, nephew-niece, uncle-aunt",
      "Royalty word pairs: king-queen, duke-duchess, emperor-empress",
      "Family word pairs: man-woman, sir-madam, heir-heiress",
      "Vertical axis ranging from -0.5 to 0.5",
      "Horizontal axis ranging from -0.5 to 0.6",
      "Consistent vector differences shown by parallel dashed lines"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg",
    "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Achievable region (A=0) shown in blue/periwinkle\",\n    \"Achievable region (A=1) shown in green\",\n    \"Overlap region shown in dark teal\",\n    \"Plus symbol (+) marking result for Ŷ=Y\",\n    \"Cross symbol (×) marking result for Ŷ=1-Y\",\n    \"Red star marking Equal-odds optimum\",\n    \"Blue circle marking Equal opportunity (A=0)\",\n    \"Green circle marking Equal opportunity (A=1)\",\n    \"Left panel showing equalized odds predictor\",\n    \"Right panel showing equal opportunity predictor\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red star equal-odds optimum lie within the dark teal overlap region shared by both demographic groups?\",\n      \"answer\": \"The optimization problem seeks a derived predictor with equalized odds, which requires satisfying constraints from both demographics simultaneously. The overlap region represents feasible solutions meeting both groups' constraints, where the optimal point must reside.\",\n      \"visual_anchor\": \"red star in the dark teal overlap region (left panel)\",\n      \"text_evidence\": \"Combining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How do the blue and green circles in the right panel differ from the red star solution shown in the left panel?\",\n      \"answer\": \"The blue and green circles represent equal opportunity solutions for each demographic group separately (A=0 and A=1), while the red star represents the equalized odds optimum that jointly optimizes across both groups. Equal opportunity is a less restrictive constraint than equalized odds.\",\n      \"visual_anchor\": \"blue and green circles (right panel) versus red star (left panel)\",\n      \"text_evidence\": \"Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What do the first and second components of γₐ(Ŷ) represent geometrically in the achievable regions marked by blue and green?\",\n      \"answer\": \"The first component represents the false positive rate of Ŷ within demographic A=a, and the second component represents the true positive rate. These two rates define the coordinates of points within each demographic's achievable region.\",\n      \"visual_anchor\": \"blue achievable region (A=0) and green achievable region (A=1)\",\n      \"text_evidence\": \"The first component of $\\\\gamma _ { a } ( \\\\widehat { Y } )$ is the false positive rate of $\\\\widehat { Y }$ within the demographic satisfying $A = a$ . Similarly, the second component is the true\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Achievable region (A=0) shown in blue/periwinkle",
        "Achievable region (A=1) shown in green",
        "Overlap region shown in dark teal",
        "Plus symbol (+) marking result for Ŷ=Y",
        "Cross symbol (×) marking result for Ŷ=1-Y",
        "Red star marking Equal-odds optimum",
        "Blue circle marking Equal opportunity (A=0)",
        "Green circle marking Equal opportunity (A=1)",
        "Left panel showing equalized odds predictor",
        "Right panel showing equal opportunity predictor"
      ],
      "queries": [
        {
          "query": "Why does the red star equal-odds optimum lie within the dark teal overlap region shared by both demographic groups?",
          "answer": "The optimization problem seeks a derived predictor with equalized odds, which requires satisfying constraints from both demographics simultaneously. The overlap region represents feasible solutions meeting both groups' constraints, where the optimal point must reside.",
          "visual_anchor": "red star in the dark teal overlap region (left panel)",
          "text_evidence": "Combining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds",
          "query_type": "visual_definition"
        },
        {
          "query": "How do the blue and green circles in the right panel differ from the red star solution shown in the left panel?",
          "answer": "The blue and green circles represent equal opportunity solutions for each demographic group separately (A=0 and A=1), while the red star represents the equalized odds optimum that jointly optimizes across both groups. Equal opportunity is a less restrictive constraint than equalized odds.",
          "visual_anchor": "blue and green circles (right panel) versus red star (left panel)",
          "text_evidence": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What do the first and second components of γₐ(Ŷ) represent geometrically in the achievable regions marked by blue and green?",
          "answer": "The first component represents the false positive rate of Ŷ within demographic A=a, and the second component represents the true positive rate. These two rates define the coordinates of points within each demographic's achievable region.",
          "visual_anchor": "blue achievable region (A=0) and green achievable region (A=1)",
          "text_evidence": "The first component of $\\gamma _ { a } ( \\widehat { Y } )$ is the false positive rate of $\\widehat { Y }$ within the demographic satisfying $A = a$ . Similarly, the second component is the true",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red star equal-odds optimum lie within the dark teal overlap region shared by both demographic groups?",
        "answer": "The optimization problem seeks a derived predictor with equalized odds, which requires satisfying constraints from both demographics simultaneously. The overlap region represents feasible solutions meeting both groups' constraints, where the optimal point must reside.",
        "visual_anchor": "red star in the dark teal overlap region (left panel)",
        "text_evidence": "Combining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds",
        "query_type": "visual_definition"
      },
      {
        "query": "How do the blue and green circles in the right panel differ from the red star solution shown in the left panel?",
        "answer": "The blue and green circles represent equal opportunity solutions for each demographic group separately (A=0 and A=1), while the red star represents the equalized odds optimum that jointly optimizes across both groups. Equal opportunity is a less restrictive constraint than equalized odds.",
        "visual_anchor": "blue and green circles (right panel) versus red star (left panel)",
        "text_evidence": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What do the first and second components of γₐ(Ŷ) represent geometrically in the achievable regions marked by blue and green?",
        "answer": "The first component represents the false positive rate of Ŷ within demographic A=a, and the second component represents the true positive rate. These two rates define the coordinates of points within each demographic's achievable region.",
        "visual_anchor": "blue achievable region (A=0) and green achievable region (A=1)",
        "text_evidence": "The first component of $\\gamma _ { a } ( \\widehat { Y } )$ is the false positive rate of $\\widehat { Y }$ within the demographic satisfying $A = a$ . Similarly, the second component is the true",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Achievable region (A=0) shown in blue/periwinkle",
      "Achievable region (A=1) shown in green",
      "Overlap region shown in dark teal",
      "Plus symbol (+) marking result for Ŷ=Y",
      "Cross symbol (×) marking result for Ŷ=1-Y",
      "Red star marking Equal-odds optimum",
      "Blue circle marking Equal opportunity (A=0)",
      "Green circle marking Equal opportunity (A=1)",
      "Left panel showing equalized odds predictor",
      "Right panel showing equal opportunity predictor"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
    "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue curved ROC curve\",\n    \"Black tangent line touching the ROC curve\",\n    \"Horizontal axis labeled Pr[Ŷ = 1 | A, Y = 0]\",\n    \"Vertical axis labeled Pr[Ŷ = 1 | A, Y = 1]\",\n    \"Diagonal line from origin\",\n    \"Black arrow pointing to tangent point on upper left\",\n    \"Black arrow pointing downward to horizontal gap on lower right\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why is the black tangent line touching the blue ROC curve the profit-maximizing choice for each protected group?\",\n      \"answer\": \"The tangent line represents the optimal threshold that maximizes profit within each group before applying fairness constraints. The horizontal gap between the ROC curve and this tangent line determines the cost for achieving a given true positive rate.\",\n      \"visual_anchor\": \"Black tangent line touching the blue ROC curve in the upper left region\",\n      \"text_evidence\": \"within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?\",\n      \"answer\": \"The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.\",\n      \"visual_anchor\": \"Black arrow pointing to horizontal gap between blue curve and tangent line in lower right\",\n      \"text_evidence\": \"within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the area above the diagonal line and below the blue ROC curve represent the feasible equalized odds predictor space?\",\n      \"answer\": \"Yes, the feasible set of false/true positive rates for equalized odds predictors is the intersection of areas under the A-conditional ROC curves and above the main diagonal, as shown in the plot.\",\n      \"visual_anchor\": \"Area between the diagonal line from origin and the blue ROC curve\",\n      \"text_evidence\": \"The feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue curved ROC curve",
        "Black tangent line touching the ROC curve",
        "Horizontal axis labeled Pr[Ŷ = 1 | A, Y = 0]",
        "Vertical axis labeled Pr[Ŷ = 1 | A, Y = 1]",
        "Diagonal line from origin",
        "Black arrow pointing to tangent point on upper left",
        "Black arrow pointing downward to horizontal gap on lower right"
      ],
      "queries": [
        {
          "query": "Why is the black tangent line touching the blue ROC curve the profit-maximizing choice for each protected group?",
          "answer": "The tangent line represents the optimal threshold that maximizes profit within each group before applying fairness constraints. The horizontal gap between the ROC curve and this tangent line determines the cost for achieving a given true positive rate.",
          "visual_anchor": "Black tangent line touching the blue ROC curve in the upper left region",
          "text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
          "answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
          "visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
          "text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the area above the diagonal line and below the blue ROC curve represent the feasible equalized odds predictor space?",
          "answer": "Yes, the feasible set of false/true positive rates for equalized odds predictors is the intersection of areas under the A-conditional ROC curves and above the main diagonal, as shown in the plot.",
          "visual_anchor": "Area between the diagonal line from origin and the blue ROC curve",
          "text_evidence": "The feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why is the black tangent line touching the blue ROC curve the profit-maximizing choice for each protected group?",
        "answer": "The tangent line represents the optimal threshold that maximizes profit within each group before applying fairness constraints. The horizontal gap between the ROC curve and this tangent line determines the cost for achieving a given true positive rate.",
        "visual_anchor": "Black tangent line touching the blue ROC curve in the upper left region",
        "text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the horizontal gap indicated by the lower-right arrow relate to optimizing the equal opportunity predictor?",
        "answer": "The horizontal gap between the ROC curve and the profit-maximizing tangent line is proportional to the cost for achieving a given true positive rate, which becomes a convex function that can be efficiently optimized using ternary search.",
        "visual_anchor": "Black arrow pointing to horizontal gap between blue curve and tangent line in lower right",
        "text_evidence": "within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the area above the diagonal line and below the blue ROC curve represent the feasible equalized odds predictor space?",
        "answer": "Yes, the feasible set of false/true positive rates for equalized odds predictors is the intersection of areas under the A-conditional ROC curves and above the main diagonal, as shown in the plot.",
        "visual_anchor": "Area between the diagonal line from origin and the blue ROC curve",
        "text_evidence": "The feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue curved ROC curve",
      "Black tangent line touching the ROC curve",
      "Horizontal axis labeled Pr[Ŷ = 1 | A, Y = 0]",
      "Vertical axis labeled Pr[Ŷ = 1 | A, Y = 1]",
      "Diagonal line from origin",
      "Black arrow pointing to tangent point on upper left",
      "Black arrow pointing downward to horizontal gap on lower right"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_7",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg",
    "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node R\", \"Node Y\", \"Node X\", \"Node A\", \"Node X'\", \"Node Ỹ\", \"Node Y*\", \"Directed edges connecting nodes\", \"Central node A with multiple connections\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does node R connect directly to both A and Y in deriving the optimal equal opportunity threshold predictor?\",\n      \"answer\": \"The construction requires finding points on conditional ROC curves with the same true positive rates in both groups, necessitating R's connection to A and Y to capture group-conditional predictions.\",\n      \"visual_anchor\": \"Node R with directed edges to both node A and node Y\",\n      \"text_evidence\": \"We only need to find points on the conditional ROC curves that have the same true positive rates in both groups.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the central position of node A with connections to X, R, X', and Ỹ support the optimality characterization?\",\n      \"answer\": \"The protected attribute A must be accessible from the Bayes optimal regressor R and connected to other variables to derive optimal equalized odds or equal opportunity predictors.\",\n      \"visual_anchor\": \"Central node A with edges to nodes X, R, X', and Ỹ\",\n      \"text_evidence\": \"An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the conditional Kolmogorov distance measure involve nodes R and A given their graphical relationship to Y?\",\n      \"answer\": \"Yes, the conditional Kolmogorov distance is defined between random variables in the same probability space as A and Y, consistent with R's connections to both.\",\n      \"visual_anchor\": \"Nodes R and A both having paths to node Y\",\n      \"text_evidence\": \"We define the conditional Kolmogorov distance between two random variables R , R ^ { \\\\prime } \\\\in [ 0 , 1 ] in the same probability space as A and Y\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node R",
        "Node Y",
        "Node X",
        "Node A",
        "Node X'",
        "Node Ỹ",
        "Node Y*",
        "Directed edges connecting nodes",
        "Central node A with multiple connections"
      ],
      "queries": [
        {
          "query": "Why does node R connect directly to both A and Y in deriving the optimal equal opportunity threshold predictor?",
          "answer": "The construction requires finding points on conditional ROC curves with the same true positive rates in both groups, necessitating R's connection to A and Y to capture group-conditional predictions.",
          "visual_anchor": "Node R with directed edges to both node A and node Y",
          "text_evidence": "We only need to find points on the conditional ROC curves that have the same true positive rates in both groups.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the central position of node A with connections to X, R, X', and Ỹ support the optimality characterization?",
          "answer": "The protected attribute A must be accessible from the Bayes optimal regressor R and connected to other variables to derive optimal equalized odds or equal opportunity predictors.",
          "visual_anchor": "Central node A with edges to nodes X, R, X', and Ỹ",
          "text_evidence": "An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the conditional Kolmogorov distance measure involve nodes R and A given their graphical relationship to Y?",
          "answer": "Yes, the conditional Kolmogorov distance is defined between random variables in the same probability space as A and Y, consistent with R's connections to both.",
          "visual_anchor": "Nodes R and A both having paths to node Y",
          "text_evidence": "We define the conditional Kolmogorov distance between two random variables R , R ^ { \\prime } \\in [ 0 , 1 ] in the same probability space as A and Y",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does node R connect directly to both A and Y in deriving the optimal equal opportunity threshold predictor?",
        "answer": "The construction requires finding points on conditional ROC curves with the same true positive rates in both groups, necessitating R's connection to A and Y to capture group-conditional predictions.",
        "visual_anchor": "Node R with directed edges to both node A and node Y",
        "text_evidence": "We only need to find points on the conditional ROC curves that have the same true positive rates in both groups.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the central position of node A with connections to X, R, X', and Ỹ support the optimality characterization?",
        "answer": "The protected attribute A must be accessible from the Bayes optimal regressor R and connected to other variables to derive optimal equalized odds or equal opportunity predictors.",
        "visual_anchor": "Central node A with edges to nodes X, R, X', and Ỹ",
        "text_evidence": "An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the conditional Kolmogorov distance measure involve nodes R and A given their graphical relationship to Y?",
        "answer": "Yes, the conditional Kolmogorov distance is defined between random variables in the same probability space as A and Y, consistent with R's connections to both.",
        "visual_anchor": "Nodes R and A both having paths to node Y",
        "text_evidence": "We define the conditional Kolmogorov distance between two random variables R , R ^ { \\prime } \\in [ 0 , 1 ] in the same probability space as A and Y",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node R",
      "Node Y",
      "Node X",
      "Node A",
      "Node X'",
      "Node Ỹ",
      "Node Y*",
      "Directed edges connecting nodes",
      "Central node A with multiple connections"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_8",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig7.jpg",
    "caption": "Figure 4: Graphical model for Scenario I.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A with arrow to Y\", \"Node Y with arrow to X₂\", \"Node X₁ with arrow to R\", \"Node R with arrow to X₂\", \"Node R̄ isolated on right side\", \"Six nodes total in graphical model structure\"],\n  \"queries\": [\n    {\n      \"query\": \"Does node A's direct connection to Y in this graphical model relate to oblivious black box discrimination tests?\",\n      \"answer\": \"Yes, the dependency structure in this graphical model represents Scenario I, which is analyzed to determine whether oblivious tests can identify discriminatory predictions in different dependency scenarios.\",\n      \"visual_anchor\": \"Node A with direct arrow to node Y in upper left portion\",\n      \"text_evidence\": \"Before turning to analyzing data, we pause to consider to what extent \\\"black box\\\" oblivious tests like ours can identify discriminatory predictions.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does node R receive input from X₁ but node R̄ remains isolated in Scenario I's structure?\",\n      \"answer\": \"The isolated R̄ node and connected R node represent different aspects of the dependency structure being analyzed. This distinction is critical for understanding how these two scenarios differ in their interpretations from the discrimination identification perspective.\",\n      \"visual_anchor\": \"Node R̄ isolated on right side versus node R connected to X₁ and X₂\",\n      \"text_evidence\": \"we introduce two possible scenarios for the dependency structure of the score, the target and the protected attribute. We will argue that while these two scenarios can have fundamentally different interpretations from the point of view o\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causes both Y and R to point toward X₂ in the depicted dependency structure?\",\n      \"answer\": \"X₂ depends on both Y and R in this graphical model, representing how the score, target, and protected attribute interact in Scenario I's dependency structure.\",\n      \"visual_anchor\": \"Node X₂ receiving arrows from both node Y (top) and node R (bottom)\",\n      \"text_evidence\": \"Scenario I Consider the dependency structure depicted in Figure 4.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A with arrow to Y",
        "Node Y with arrow to X₂",
        "Node X₁ with arrow to R",
        "Node R with arrow to X₂",
        "Node R̄ isolated on right side",
        "Six nodes total in graphical model structure"
      ],
      "queries": [
        {
          "query": "Does node A's direct connection to Y in this graphical model relate to oblivious black box discrimination tests?",
          "answer": "Yes, the dependency structure in this graphical model represents Scenario I, which is analyzed to determine whether oblivious tests can identify discriminatory predictions in different dependency scenarios.",
          "visual_anchor": "Node A with direct arrow to node Y in upper left portion",
          "text_evidence": "Before turning to analyzing data, we pause to consider to what extent \"black box\" oblivious tests like ours can identify discriminatory predictions.",
          "query_type": "value_context"
        },
        {
          "query": "Why does node R receive input from X₁ but node R̄ remains isolated in Scenario I's structure?",
          "answer": "The isolated R̄ node and connected R node represent different aspects of the dependency structure being analyzed. This distinction is critical for understanding how these two scenarios differ in their interpretations from the discrimination identification perspective.",
          "visual_anchor": "Node R̄ isolated on right side versus node R connected to X₁ and X₂",
          "text_evidence": "we introduce two possible scenarios for the dependency structure of the score, the target and the protected attribute. We will argue that while these two scenarios can have fundamentally different interpretations from the point of view o",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causes both Y and R to point toward X₂ in the depicted dependency structure?",
          "answer": "X₂ depends on both Y and R in this graphical model, representing how the score, target, and protected attribute interact in Scenario I's dependency structure.",
          "visual_anchor": "Node X₂ receiving arrows from both node Y (top) and node R (bottom)",
          "text_evidence": "Scenario I Consider the dependency structure depicted in Figure 4.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does node A's direct connection to Y in this graphical model relate to oblivious black box discrimination tests?",
        "answer": "Yes, the dependency structure in this graphical model represents Scenario I, which is analyzed to determine whether oblivious tests can identify discriminatory predictions in different dependency scenarios.",
        "visual_anchor": "Node A with direct arrow to node Y in upper left portion",
        "text_evidence": "Before turning to analyzing data, we pause to consider to what extent \"black box\" oblivious tests like ours can identify discriminatory predictions.",
        "query_type": "value_context"
      },
      {
        "query": "Why does node R receive input from X₁ but node R̄ remains isolated in Scenario I's structure?",
        "answer": "The isolated R̄ node and connected R node represent different aspects of the dependency structure being analyzed. This distinction is critical for understanding how these two scenarios differ in their interpretations from the discrimination identification perspective.",
        "visual_anchor": "Node R̄ isolated on right side versus node R connected to X₁ and X₂",
        "text_evidence": "we introduce two possible scenarios for the dependency structure of the score, the target and the protected attribute. We will argue that while these two scenarios can have fundamentally different interpretations from the point of view o",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causes both Y and R to point toward X₂ in the depicted dependency structure?",
        "answer": "X₂ depends on both Y and R in this graphical model, representing how the score, target, and protected attribute interact in Scenario I's dependency structure.",
        "visual_anchor": "Node X₂ receiving arrows from both node Y (top) and node R (bottom)",
        "text_evidence": "Scenario I Consider the dependency structure depicted in Figure 4.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A with arrow to Y",
      "Node Y with arrow to X₂",
      "Node X₁ with arrow to R",
      "Node R with arrow to X₂",
      "Node R̄ isolated on right side",
      "Six nodes total in graphical model structure"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_9",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg",
    "caption": "Figure 5: Graphical model for Scenario II.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A\", \"Node X3\", \"Node Y\", \"Node R\", \"Node R*\", \"Arrow from A to X3\", \"Arrow from X3 to Y\", \"Arrow from A to R\", \"Arrow from X3 to R*\"],\n  \"queries\": [\n    {\n      \"query\": \"In Scenario II, does node X3 mediate between A and Y as shown by the arrows?\",\n      \"answer\": \"Yes, in Scenario II depicted in the graphical model, X3 acts as a mediator between A and Y, with A pointing to X3 and X3 pointing to Y.\",\n      \"visual_anchor\": \"Arrow from A to X3 and arrow from X3 to Y\",\n      \"text_evidence\": \"Scenario II Now consider the dependency structure depicted in Figure 5.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why is the optimal score R* connected only to X3 rather than directly to A in this model?\",\n      \"answer\": \"The optimal score R* is based only on a directly predictive feature (X3), not on A or its surrogate, distinguishing this scenario from one where R* depends directly on A.\",\n      \"visual_anchor\": \"Arrow from X3 to R* with no arrow from A to R*\",\n      \"text_evidence\": \"The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the dependency structure with R connected to A relate to unidentifiability under equalized odds?\",\n      \"answer\": \"The two scenarios with different dependency structures are indistinguishable using any oblivious test based only on target labels and protected attributes, suggesting a shortcoming of equalized odds.\",\n      \"visual_anchor\": \"Node R with arrow from A\",\n      \"text_evidence\": \"this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A",
        "Node X3",
        "Node Y",
        "Node R",
        "Node R*",
        "Arrow from A to X3",
        "Arrow from X3 to Y",
        "Arrow from A to R",
        "Arrow from X3 to R*"
      ],
      "queries": [
        {
          "query": "In Scenario II, does node X3 mediate between A and Y as shown by the arrows?",
          "answer": "Yes, in Scenario II depicted in the graphical model, X3 acts as a mediator between A and Y, with A pointing to X3 and X3 pointing to Y.",
          "visual_anchor": "Arrow from A to X3 and arrow from X3 to Y",
          "text_evidence": "Scenario II Now consider the dependency structure depicted in Figure 5.",
          "query_type": "visual_definition"
        },
        {
          "query": "Why is the optimal score R* connected only to X3 rather than directly to A in this model?",
          "answer": "The optimal score R* is based only on a directly predictive feature (X3), not on A or its surrogate, distinguishing this scenario from one where R* depends directly on A.",
          "visual_anchor": "Arrow from X3 to R* with no arrow from A to R*",
          "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the dependency structure with R connected to A relate to unidentifiability under equalized odds?",
          "answer": "The two scenarios with different dependency structures are indistinguishable using any oblivious test based only on target labels and protected attributes, suggesting a shortcoming of equalized odds.",
          "visual_anchor": "Node R with arrow from A",
          "text_evidence": "this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "In Scenario II, does node X3 mediate between A and Y as shown by the arrows?",
        "answer": "Yes, in Scenario II depicted in the graphical model, X3 acts as a mediator between A and Y, with A pointing to X3 and X3 pointing to Y.",
        "visual_anchor": "Arrow from A to X3 and arrow from X3 to Y",
        "text_evidence": "Scenario II Now consider the dependency structure depicted in Figure 5.",
        "query_type": "visual_definition"
      },
      {
        "query": "Why is the optimal score R* connected only to X3 rather than directly to A in this model?",
        "answer": "The optimal score R* is based only on a directly predictive feature (X3), not on A or its surrogate, distinguishing this scenario from one where R* depends directly on A.",
        "visual_anchor": "Arrow from X3 to R* with no arrow from A to R*",
        "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the dependency structure with R connected to A relate to unidentifiability under equalized odds?",
        "answer": "The two scenarios with different dependency structures are indistinguishable using any oblivious test based only on target labels and protected attributes, suggesting a shortcoming of equalized odds.",
        "visual_anchor": "Node R with arrow from A",
        "text_evidence": "this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A",
      "Node X3",
      "Node Y",
      "Node R",
      "Node R*",
      "Arrow from A to X3",
      "Arrow from X3 to Y",
      "Arrow from A to R",
      "Arrow from X3 to R*"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_10",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg",
    "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A connected to X3\", \"Node A connected to X1\", \"Node X3 connected to Y\", \"Node X2 connected to Y\", \"Node X1 connected to X2\", \"Directed edges between nodes\", \"Five nodes arranged in two rows\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does node A connect to both X3 and X1 when the optimal score is based directly on A or its surrogate?\",\n      \"answer\": \"In scenario I, the optimal score R* depends directly on A or its surrogate, requiring A to have direct influence on both pathways (through X3 to Y and through X1-X2 to Y) shown in the dependency structure.\",\n      \"visual_anchor\": \"Node A with directed edges to both X3 (top path) and X1 (bottom path)\",\n      \"text_evidence\": \"The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the directed edge from X2 to Y support the scenario where optimal score depends only on a directly predictive feature?\",\n      \"answer\": \"The X2 to Y edge represents scenario II where the optimal score R* is based only on a directly predictive feature (X2), independent of the protected attribute A's direct influence.\",\n      \"visual_anchor\": \"Directed edge from node X2 to node Y in the bottom-right portion\",\n      \"text_evidence\": \"The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are both directed structures shown when no test using only target labels and protected attribute can distinguish the scenarios?\",\n      \"answer\": \"The two directed structures represent fundamentally different causal mechanisms that produce indistinguishable distributions under oblivious tests, demonstrating that equalized odds cannot reveal whether the score depends directly on A or only on predictive features.\",\n      \"visual_anchor\": \"Two distinct directed dependency structures with identical node sets but different causal interpretations\",\n      \"text_evidence\": \"the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protecte\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A connected to X3",
        "Node A connected to X1",
        "Node X3 connected to Y",
        "Node X2 connected to Y",
        "Node X1 connected to X2",
        "Directed edges between nodes",
        "Five nodes arranged in two rows"
      ],
      "queries": [
        {
          "query": "Why does node A connect to both X3 and X1 when the optimal score is based directly on A or its surrogate?",
          "answer": "In scenario I, the optimal score R* depends directly on A or its surrogate, requiring A to have direct influence on both pathways (through X3 to Y and through X1-X2 to Y) shown in the dependency structure.",
          "visual_anchor": "Node A with directed edges to both X3 (top path) and X1 (bottom path)",
          "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the directed edge from X2 to Y support the scenario where optimal score depends only on a directly predictive feature?",
          "answer": "The X2 to Y edge represents scenario II where the optimal score R* is based only on a directly predictive feature (X2), independent of the protected attribute A's direct influence.",
          "visual_anchor": "Directed edge from node X2 to node Y in the bottom-right portion",
          "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion",
          "query_type": "value_context"
        },
        {
          "query": "Why are both directed structures shown when no test using only target labels and protected attribute can distinguish the scenarios?",
          "answer": "The two directed structures represent fundamentally different causal mechanisms that produce indistinguishable distributions under oblivious tests, demonstrating that equalized odds cannot reveal whether the score depends directly on A or only on predictive features.",
          "visual_anchor": "Two distinct directed dependency structures with identical node sets but different causal interpretations",
          "text_evidence": "the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protecte",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does node A connect to both X3 and X1 when the optimal score is based directly on A or its surrogate?",
        "answer": "In scenario I, the optimal score R* depends directly on A or its surrogate, requiring A to have direct influence on both pathways (through X3 to Y and through X1-X2 to Y) shown in the dependency structure.",
        "visual_anchor": "Node A with directed edges to both X3 (top path) and X1 (bottom path)",
        "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the directed edge from X2 to Y support the scenario where optimal score depends only on a directly predictive feature?",
        "answer": "The X2 to Y edge represents scenario II where the optimal score R* is based only on a directly predictive feature (X2), independent of the protected attribute A's direct influence.",
        "visual_anchor": "Directed edge from node X2 to node Y in the bottom-right portion",
        "text_evidence": "The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion",
        "query_type": "value_context"
      },
      {
        "query": "Why are both directed structures shown when no test using only target labels and protected attribute can distinguish the scenarios?",
        "answer": "The two directed structures represent fundamentally different causal mechanisms that produce indistinguishable distributions under oblivious tests, demonstrating that equalized odds cannot reveal whether the score depends directly on A or only on predictive features.",
        "visual_anchor": "Two distinct directed dependency structures with identical node sets but different causal interpretations",
        "text_evidence": "the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protecte",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A connected to X3",
      "Node A connected to X1",
      "Node X3 connected to Y",
      "Node X2 connected to Y",
      "Node X1 connected to X2",
      "Directed edges between nodes",
      "Five nodes arranged in two rows"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_12",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg",
    "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid blue line for Asian\",\n    \"Dashed green line for White\",\n    \"Dotted red line for Hispanic\",\n    \"Dotted cyan line for Black\",\n    \"X-axis: FICO score (300-800)\",\n    \"Y-axis: Non-default rate (0-100%)\",\n    \"Steep rise around FICO 500-700\",\n    \"All curves converge near 100% at FICO 800\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the steeper rise of the blue Asian curve between FICO 500-700 relate to optimal classification depending on summed variables?\",\n      \"answer\": \"The steeper Asian curve reflects optimal classification based on combined predictors, as optimal classification only depends on the sum x₁ + x₂, making R* = X₁ + X₂ optimal.\",\n      \"visual_anchor\": \"Solid blue line (Asian) showing steeper slope between FICO 500-700\",\n      \"text_evidence\": \"optimal classification only depends on $x _ { 1 } + x _ { 2 }$ and so $R ^ { * } = X _ { 1 } + X _ { 2 }$ is optimal\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the cyan Black curve lag below others around FICO 600, given monotone function optimality in scenario II?\",\n      \"answer\": \"The lag reflects that in scenario II, any monotone function of P(Y|X₃) is optimal and remains optimal even with dependence on A, suggesting the Black curve follows a different conditional distribution pattern.\",\n      \"visual_anchor\": \"Dotted cyan line (Black) positioned lower than other curves at FICO 600\",\n      \"text_evidence\": \"since $P ( Y | X _ { 3 } )$ is monotone in $X _ { 3 }$ , any monotone function of it is optimal (unconstrained), and the dependency structure implies its optimal even if we allow dependence on A\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the converging curves near 100% at FICO 800 constitute the marginal distributions that form the input data?\",\n      \"answer\": \"Yes, these marginal distributions by racial group, along with the number of people per group, constitute the input data as stated in the figure caption.\",\n      \"visual_anchor\": \"All four curves (Asian, White, Hispanic, Black) converging near 100% at FICO score 800\",\n      \"text_evidence\": \"These two marginals, and the number of people per group, constitute our input data\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid blue line for Asian",
        "Dashed green line for White",
        "Dotted red line for Hispanic",
        "Dotted cyan line for Black",
        "X-axis: FICO score (300-800)",
        "Y-axis: Non-default rate (0-100%)",
        "Steep rise around FICO 500-700",
        "All curves converge near 100% at FICO 800"
      ],
      "queries": [
        {
          "query": "Does the steeper rise of the blue Asian curve between FICO 500-700 relate to optimal classification depending on summed variables?",
          "answer": "The steeper Asian curve reflects optimal classification based on combined predictors, as optimal classification only depends on the sum x₁ + x₂, making R* = X₁ + X₂ optimal.",
          "visual_anchor": "Solid blue line (Asian) showing steeper slope between FICO 500-700",
          "text_evidence": "optimal classification only depends on $x _ { 1 } + x _ { 2 }$ and so $R ^ { * } = X _ { 1 } + X _ { 2 }$ is optimal",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the cyan Black curve lag below others around FICO 600, given monotone function optimality in scenario II?",
          "answer": "The lag reflects that in scenario II, any monotone function of P(Y|X₃) is optimal and remains optimal even with dependence on A, suggesting the Black curve follows a different conditional distribution pattern.",
          "visual_anchor": "Dotted cyan line (Black) positioned lower than other curves at FICO 600",
          "text_evidence": "since $P ( Y | X _ { 3 } )$ is monotone in $X _ { 3 }$ , any monotone function of it is optimal (unconstrained), and the dependency structure implies its optimal even if we allow dependence on A",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the converging curves near 100% at FICO 800 constitute the marginal distributions that form the input data?",
          "answer": "Yes, these marginal distributions by racial group, along with the number of people per group, constitute the input data as stated in the figure caption.",
          "visual_anchor": "All four curves (Asian, White, Hispanic, Black) converging near 100% at FICO score 800",
          "text_evidence": "These two marginals, and the number of people per group, constitute our input data",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the steeper rise of the blue Asian curve between FICO 500-700 relate to optimal classification depending on summed variables?",
        "answer": "The steeper Asian curve reflects optimal classification based on combined predictors, as optimal classification only depends on the sum x₁ + x₂, making R* = X₁ + X₂ optimal.",
        "visual_anchor": "Solid blue line (Asian) showing steeper slope between FICO 500-700",
        "text_evidence": "optimal classification only depends on $x _ { 1 } + x _ { 2 }$ and so $R ^ { * } = X _ { 1 } + X _ { 2 }$ is optimal",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the cyan Black curve lag below others around FICO 600, given monotone function optimality in scenario II?",
        "answer": "The lag reflects that in scenario II, any monotone function of P(Y|X₃) is optimal and remains optimal even with dependence on A, suggesting the Black curve follows a different conditional distribution pattern.",
        "visual_anchor": "Dotted cyan line (Black) positioned lower than other curves at FICO 600",
        "text_evidence": "since $P ( Y | X _ { 3 } )$ is monotone in $X _ { 3 }$ , any monotone function of it is optimal (unconstrained), and the dependency structure implies its optimal even if we allow dependence on A",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the converging curves near 100% at FICO 800 constitute the marginal distributions that form the input data?",
        "answer": "Yes, these marginal distributions by racial group, along with the number of people per group, constitute the input data as stated in the figure caption.",
        "visual_anchor": "All four curves (Asian, White, Hispanic, Black) converging near 100% at FICO score 800",
        "text_evidence": "These two marginals, and the number of people per group, constitute our input data",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid blue line for Asian",
      "Dashed green line for White",
      "Dotted red line for Hispanic",
      "Dotted cyan line for Black",
      "X-axis: FICO score (300-800)",
      "Y-axis: Non-default rate (0-100%)",
      "Steep rise around FICO 500-700",
      "All curves converge near 100% at FICO 800"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_14",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg",
    "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid blue line for Asian\",\n    \"dashed green line for White\",\n    \"dash-dot red line for Hispanic\",\n    \"dotted cyan line for Black\",\n    \"FICO score threshold of 620 on x-axis\",\n    \"non-default rate of 82% on y-axis\",\n    \"gray shaded region from FICO 620 to 800\",\n    \"curves showing cumulative distribution of non-default rates\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the gray shaded region start at FICO 620 if that threshold yields 82% non-default overall?\",\n      \"answer\": \"The single threshold of 620 is chosen because it corresponds to an 82% non-default rate overall across all groups, representing the common lending standard.\",\n      \"visual_anchor\": \"gray shaded region starting at FICO score 620 with 82% non-default rate\",\n      \"text_evidence\": \"Hence it will pick the single threshold at which 82% of people do not default overall, shown in Figure 8.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the dotted cyan curve reaching only 70% at FICO 620 demonstrate why race-blind thresholds violate fairness?\",\n      \"answer\": \"Yes, because black non-defaulters have much lower qualification rates at the 620 threshold compared to other groups, violating equal opportunity fairness definitions.\",\n      \"visual_anchor\": \"dotted cyan line (Black) at approximately 70% when other curves reach 82% at FICO 620\",\n      \"text_evidence\": \"This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the shaded area fraction relate to the probability that true non-defaulters qualify for loans?\",\n      \"answer\": \"The shaded area fraction represents Pr[Ŷ=1|Y=1,A], the probability that actual non-defaulters from each group are predicted to qualify, showing disparities across racial groups.\",\n      \"visual_anchor\": \"gray shaded region representing area under the curve from FICO 620 onwards\",\n      \"text_evidence\": \"Rescaling the x axis to represent the within-group thresholds (right), Pr[Ŷ=1|Y=1,A] is the fraction of the area under the curve that is shaded.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid blue line for Asian",
        "dashed green line for White",
        "dash-dot red line for Hispanic",
        "dotted cyan line for Black",
        "FICO score threshold of 620 on x-axis",
        "non-default rate of 82% on y-axis",
        "gray shaded region from FICO 620 to 800",
        "curves showing cumulative distribution of non-default rates"
      ],
      "queries": [
        {
          "query": "Why does the gray shaded region start at FICO 620 if that threshold yields 82% non-default overall?",
          "answer": "The single threshold of 620 is chosen because it corresponds to an 82% non-default rate overall across all groups, representing the common lending standard.",
          "visual_anchor": "gray shaded region starting at FICO score 620 with 82% non-default rate",
          "text_evidence": "Hence it will pick the single threshold at which 82% of people do not default overall, shown in Figure 8.",
          "query_type": "value_context"
        },
        {
          "query": "Does the dotted cyan curve reaching only 70% at FICO 620 demonstrate why race-blind thresholds violate fairness?",
          "answer": "Yes, because black non-defaulters have much lower qualification rates at the 620 threshold compared to other groups, violating equal opportunity fairness definitions.",
          "visual_anchor": "dotted cyan line (Black) at approximately 70% when other curves reach 82% at FICO 620",
          "text_evidence": "This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the shaded area fraction relate to the probability that true non-defaulters qualify for loans?",
          "answer": "The shaded area fraction represents Pr[Ŷ=1|Y=1,A], the probability that actual non-defaulters from each group are predicted to qualify, showing disparities across racial groups.",
          "visual_anchor": "gray shaded region representing area under the curve from FICO 620 onwards",
          "text_evidence": "Rescaling the x axis to represent the within-group thresholds (right), Pr[Ŷ=1|Y=1,A] is the fraction of the area under the curve that is shaded.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the gray shaded region start at FICO 620 if that threshold yields 82% non-default overall?",
        "answer": "The single threshold of 620 is chosen because it corresponds to an 82% non-default rate overall across all groups, representing the common lending standard.",
        "visual_anchor": "gray shaded region starting at FICO score 620 with 82% non-default rate",
        "text_evidence": "Hence it will pick the single threshold at which 82% of people do not default overall, shown in Figure 8.",
        "query_type": "value_context"
      },
      {
        "query": "Does the dotted cyan curve reaching only 70% at FICO 620 demonstrate why race-blind thresholds violate fairness?",
        "answer": "Yes, because black non-defaulters have much lower qualification rates at the 620 threshold compared to other groups, violating equal opportunity fairness definitions.",
        "visual_anchor": "dotted cyan line (Black) at approximately 70% when other curves reach 82% at FICO 620",
        "text_evidence": "This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the shaded area fraction relate to the probability that true non-defaulters qualify for loans?",
        "answer": "The shaded area fraction represents Pr[Ŷ=1|Y=1,A], the probability that actual non-defaulters from each group are predicted to qualify, showing disparities across racial groups.",
        "visual_anchor": "gray shaded region representing area under the curve from FICO 620 onwards",
        "text_evidence": "Rescaling the x axis to represent the within-group thresholds (right), Pr[Ŷ=1|Y=1,A] is the fraction of the area under the curve that is shaded.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid blue line for Asian",
      "dashed green line for White",
      "dash-dot red line for Hispanic",
      "dotted cyan line for Black",
      "FICO score threshold of 620 on x-axis",
      "non-default rate of 82% on y-axis",
      "gray shaded region from FICO 620 to 800",
      "curves showing cumulative distribution of non-default rates"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_16",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg",
    "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Asian (blue diamonds)\", \"White (green circles)\", \"Hispanic (red squares)\", \"Black (cyan circles)\", \"Five fairness definitions on y-axis\", \"FICO score thresholds on x-axis (300-800)\", \"Horizontal dotted lines separating definitions\", \"Connected lines for equal odds thresholds\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the Asian blue diamonds at Max profit and Demography confirm that optimal thresholds can be computed efficiently?\",\n      \"answer\": \"Yes, the figure displays computed thresholds for Asian applicants across all five fairness definitions (ranging from ~650 to ~720), demonstrating that Section 4's efficient computation method successfully produces results for multiple fairness criteria.\",\n      \"visual_anchor\": \"blue diamonds for Asian group spanning from Max profit (~650) to Demography (~720)\",\n      \"text_evidence\": \"As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the Hispanic red square at Max profit exceed 650 while the equal odds red square falls below 600?\",\n      \"answer\": \"The max profit threshold prioritizes lender revenue and lies between opportunity and demography thresholds as stated. Equal odds constrains to balance error rates across races, resulting in a lower threshold for Hispanic applicants to achieve fairness.\",\n      \"visual_anchor\": \"Hispanic red squares at Max profit (~670) and Equal odds (~590)\",\n      \"text_evidence\": \"Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the green circle range spanning from ~520 to ~680 address demographic parity's conceptual shortcomings?\",\n      \"answer\": \"Yes, the figure displays thresholds under multiple fairness definitions for White applicants. The proposed equal opportunity and equal odds measures (between Max profit and Demography) remedy demographic parity's shortcomings by lying between profit-maximizing and purely demographic criteria.\",\n      \"visual_anchor\": \"White green circles spanning from Equal odds (~520) to Demography (~680)\",\n      \"text_evidence\": \"We proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Asian (blue diamonds)",
        "White (green circles)",
        "Hispanic (red squares)",
        "Black (cyan circles)",
        "Five fairness definitions on y-axis",
        "FICO score thresholds on x-axis (300-800)",
        "Horizontal dotted lines separating definitions",
        "Connected lines for equal odds thresholds"
      ],
      "queries": [
        {
          "query": "Do the Asian blue diamonds at Max profit and Demography confirm that optimal thresholds can be computed efficiently?",
          "answer": "Yes, the figure displays computed thresholds for Asian applicants across all five fairness definitions (ranging from ~650 to ~720), demonstrating that Section 4's efficient computation method successfully produces results for multiple fairness criteria.",
          "visual_anchor": "blue diamonds for Asian group spanning from Max profit (~650) to Demography (~720)",
          "text_evidence": "As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the Hispanic red square at Max profit exceed 650 while the equal odds red square falls below 600?",
          "answer": "The max profit threshold prioritizes lender revenue and lies between opportunity and demography thresholds as stated. Equal odds constrains to balance error rates across races, resulting in a lower threshold for Hispanic applicants to achieve fairness.",
          "visual_anchor": "Hispanic red squares at Max profit (~670) and Equal odds (~590)",
          "text_evidence": "Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the green circle range spanning from ~520 to ~680 address demographic parity's conceptual shortcomings?",
          "answer": "Yes, the figure displays thresholds under multiple fairness definitions for White applicants. The proposed equal opportunity and equal odds measures (between Max profit and Demography) remedy demographic parity's shortcomings by lying between profit-maximizing and purely demographic criteria.",
          "visual_anchor": "White green circles spanning from Equal odds (~520) to Demography (~680)",
          "text_evidence": "We proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the Asian blue diamonds at Max profit and Demography confirm that optimal thresholds can be computed efficiently?",
        "answer": "Yes, the figure displays computed thresholds for Asian applicants across all five fairness definitions (ranging from ~650 to ~720), demonstrating that Section 4's efficient computation method successfully produces results for multiple fairness criteria.",
        "visual_anchor": "blue diamonds for Asian group spanning from Max profit (~650) to Demography (~720)",
        "text_evidence": "As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the Hispanic red square at Max profit exceed 650 while the equal odds red square falls below 600?",
        "answer": "The max profit threshold prioritizes lender revenue and lies between opportunity and demography thresholds as stated. Equal odds constrains to balance error rates across races, resulting in a lower threshold for Hispanic applicants to achieve fairness.",
        "visual_anchor": "Hispanic red squares at Max profit (~670) and Equal odds (~590)",
        "text_evidence": "Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the green circle range spanning from ~520 to ~680 address demographic parity's conceptual shortcomings?",
        "answer": "Yes, the figure displays thresholds under multiple fairness definitions for White applicants. The proposed equal opportunity and equal odds measures (between Max profit and Demography) remedy demographic parity's shortcomings by lying between profit-maximizing and purely demographic criteria.",
        "visual_anchor": "White green circles spanning from Equal odds (~520) to Demography (~680)",
        "text_evidence": "We proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Asian (blue diamonds)",
      "White (green circles)",
      "Hispanic (red squares)",
      "Black (cyan circles)",
      "Five fairness definitions on y-axis",
      "FICO score thresholds on x-axis (300-800)",
      "Horizontal dotted lines separating definitions",
      "Connected lines for equal odds thresholds"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_18",
    "figure_number": 10,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg",
    "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue solid line labeled Asian\",\n    \"Green dashed line labeled White\",\n    \"Red dash-dot line labeled Hispanic\",\n    \"Cyan dotted line labeled Black\",\n    \"Diagonal black line from (0,0) to (1,1)\",\n    \"Horizontal line at y=0.4\",\n    \"Vertical line at x=0.2\",\n    \"ROC curves showing true positive rate vs false positive rate\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does equality of opportunity select points along the horizontal line at y≈0.4 despite different group ROC curves?\",\n      \"answer\": \"Equality of opportunity enforces equal true positive rates across groups, constraining selections to the same horizontal line regardless of individual group ROC performance.\",\n      \"visual_anchor\": \"Horizontal line at approximately y=0.4 intersecting all four ROC curves\",\n      \"text_evidence\": \"Equality of opportunity picks points along the same horizontal line.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the ROC curves enable any convex combination of outcomes within each demographic group?\",\n      \"answer\": \"Points along each ROC curve represent different classification thresholds. Any weighted combination of two threshold choices produces a point on the line segment connecting them, creating convex combinations.\",\n      \"visual_anchor\": \"Four distinct ROC curves (blue solid, green dashed, red dash-dot, cyan dotted) showing continuous paths from origin\",\n      \"text_evidence\": \"Within a group, we can achieve any convex combination of these outcomes.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the green dashed curve reaching y≈0.9 at x≈0.2 support remedying demographic parity shortcomings?\",\n      \"answer\": \"Yes, the White group's superior ROC performance demonstrates that demographic parity would ignore predictive differences. The proposed fairness measure addresses this by considering true positive rates instead.\",\n      \"visual_anchor\": \"Green dashed line (White) reaching approximately y=0.9 at x=0.2, higher than other curves\",\n      \"text_evidence\": \"First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue solid line labeled Asian",
        "Green dashed line labeled White",
        "Red dash-dot line labeled Hispanic",
        "Cyan dotted line labeled Black",
        "Diagonal black line from (0,0) to (1,1)",
        "Horizontal line at y=0.4",
        "Vertical line at x=0.2",
        "ROC curves showing true positive rate vs false positive rate"
      ],
      "queries": [
        {
          "query": "Why does equality of opportunity select points along the horizontal line at y≈0.4 despite different group ROC curves?",
          "answer": "Equality of opportunity enforces equal true positive rates across groups, constraining selections to the same horizontal line regardless of individual group ROC performance.",
          "visual_anchor": "Horizontal line at approximately y=0.4 intersecting all four ROC curves",
          "text_evidence": "Equality of opportunity picks points along the same horizontal line.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the ROC curves enable any convex combination of outcomes within each demographic group?",
          "answer": "Points along each ROC curve represent different classification thresholds. Any weighted combination of two threshold choices produces a point on the line segment connecting them, creating convex combinations.",
          "visual_anchor": "Four distinct ROC curves (blue solid, green dashed, red dash-dot, cyan dotted) showing continuous paths from origin",
          "text_evidence": "Within a group, we can achieve any convex combination of these outcomes.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the green dashed curve reaching y≈0.9 at x≈0.2 support remedying demographic parity shortcomings?",
          "answer": "Yes, the White group's superior ROC performance demonstrates that demographic parity would ignore predictive differences. The proposed fairness measure addresses this by considering true positive rates instead.",
          "visual_anchor": "Green dashed line (White) reaching approximately y=0.9 at x=0.2, higher than other curves",
          "text_evidence": "First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does equality of opportunity select points along the horizontal line at y≈0.4 despite different group ROC curves?",
        "answer": "Equality of opportunity enforces equal true positive rates across groups, constraining selections to the same horizontal line regardless of individual group ROC performance.",
        "visual_anchor": "Horizontal line at approximately y=0.4 intersecting all four ROC curves",
        "text_evidence": "Equality of opportunity picks points along the same horizontal line.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the ROC curves enable any convex combination of outcomes within each demographic group?",
        "answer": "Points along each ROC curve represent different classification thresholds. Any weighted combination of two threshold choices produces a point on the line segment connecting them, creating convex combinations.",
        "visual_anchor": "Four distinct ROC curves (blue solid, green dashed, red dash-dot, cyan dotted) showing continuous paths from origin",
        "text_evidence": "Within a group, we can achieve any convex combination of these outcomes.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the green dashed curve reaching y≈0.9 at x≈0.2 support remedying demographic parity shortcomings?",
        "answer": "Yes, the White group's superior ROC performance demonstrates that demographic parity would ignore predictive differences. The proposed fairness measure addresses this by considering true positive rates instead.",
        "visual_anchor": "Green dashed line (White) reaching approximately y=0.9 at x=0.2, higher than other curves",
        "text_evidence": "First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue solid line labeled Asian",
      "Green dashed line labeled White",
      "Red dash-dot line labeled Hispanic",
      "Cyan dotted line labeled Black",
      "Diagonal black line from (0,0) to (1,1)",
      "Horizontal line at y=0.4",
      "Vertical line at x=0.2",
      "ROC curves showing true positive rate vs false positive rate"
    ]
  },
  {
    "doc_id": "1610.02413",
    "figure_id": "1610.02413_fig_20",
    "figure_number": 11,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg",
    "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue bars representing Max profit\",\n    \"Green bars representing Single threshold\",\n    \"Red bars representing Opportunity\",\n    \"Teal bars representing Equal odds\",\n    \"Yellow bars representing Demography\",\n    \"Four demographic groups: Asian, White, Hispanic, Black\",\n    \"Y-axis: Fraction non-defaulters getting loan (0.0 to 1.0)\",\n    \"Right side: Profit as a fraction of max profit\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue Max profit bar reach 0.85 for Asian borrowers if fairness classifiers are built with independency constraints?\",\n      \"answer\": \"The Max profit approach optimizes loan approval without fairness constraints, achieving 0.85 approval rate for Asian non-defaulters. Building classifiers with independency constraints, as mentioned in the references, would lower this rate to match fairness notions like the yellow Demography bar at approximately 0.63.\",\n      \"visual_anchor\": \"Blue bar at approximately 0.85 for Asian demographic group\",\n      \"text_evidence\": \"Building classifiers with independency constraints. In In Proc. IEEE International Conference on Data Mining Workshops, pages 13–18, 2009.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the green Single threshold bar at 0.55 for Black borrowers align with claims about disparate impact in big data?\",\n      \"answer\": \"Yes, the green Single threshold bar showing only 0.55 loan approval for Black non-defaulters, compared to 0.82 for Asian borrowers, demonstrates disparate impact. This significant disparity across demographics exemplifies how algorithmic decisions can produce unequal outcomes even when targeting non-defaulters.\",\n      \"visual_anchor\": \"Green bar at approximately 0.55 for Black demographic group\",\n      \"text_evidence\": \"Solon Barocas and Andrew Selbst. Big data's disparate impact. California Law Review, 104, 2016.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the yellow Demography bars show 0.63 to 0.90 range across demographics when learning fair classifiers?\",\n      \"answer\": \"The yellow Demography bars vary from approximately 0.63 for Asian to 0.90 for Black borrowers because demographic fairness constraints aim to equalize approval rates across groups. This approach, related to learning fair classifiers, adjusts thresholds per demographic to balance loan access, though perfect equalization is not achieved.\",\n      \"visual_anchor\": \"Yellow bars ranging from approximately 0.63 (Asian) to 0.90 (Black)\",\n      \"text_evidence\": \"Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Learning fair classifiers. CoRR, abs:1507.05259, 2015.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue bars representing Max profit",
        "Green bars representing Single threshold",
        "Red bars representing Opportunity",
        "Teal bars representing Equal odds",
        "Yellow bars representing Demography",
        "Four demographic groups: Asian, White, Hispanic, Black",
        "Y-axis: Fraction non-defaulters getting loan (0.0 to 1.0)",
        "Right side: Profit as a fraction of max profit"
      ],
      "queries": [
        {
          "query": "Why does the blue Max profit bar reach 0.85 for Asian borrowers if fairness classifiers are built with independency constraints?",
          "answer": "The Max profit approach optimizes loan approval without fairness constraints, achieving 0.85 approval rate for Asian non-defaulters. Building classifiers with independency constraints, as mentioned in the references, would lower this rate to match fairness notions like the yellow Demography bar at approximately 0.63.",
          "visual_anchor": "Blue bar at approximately 0.85 for Asian demographic group",
          "text_evidence": "Building classifiers with independency constraints. In In Proc. IEEE International Conference on Data Mining Workshops, pages 13–18, 2009.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the green Single threshold bar at 0.55 for Black borrowers align with claims about disparate impact in big data?",
          "answer": "Yes, the green Single threshold bar showing only 0.55 loan approval for Black non-defaulters, compared to 0.82 for Asian borrowers, demonstrates disparate impact. This significant disparity across demographics exemplifies how algorithmic decisions can produce unequal outcomes even when targeting non-defaulters.",
          "visual_anchor": "Green bar at approximately 0.55 for Black demographic group",
          "text_evidence": "Solon Barocas and Andrew Selbst. Big data's disparate impact. California Law Review, 104, 2016.",
          "query_type": "value_context"
        },
        {
          "query": "Why do the yellow Demography bars show 0.63 to 0.90 range across demographics when learning fair classifiers?",
          "answer": "The yellow Demography bars vary from approximately 0.63 for Asian to 0.90 for Black borrowers because demographic fairness constraints aim to equalize approval rates across groups. This approach, related to learning fair classifiers, adjusts thresholds per demographic to balance loan access, though perfect equalization is not achieved.",
          "visual_anchor": "Yellow bars ranging from approximately 0.63 (Asian) to 0.90 (Black)",
          "text_evidence": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Learning fair classifiers. CoRR, abs:1507.05259, 2015.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue Max profit bar reach 0.85 for Asian borrowers if fairness classifiers are built with independency constraints?",
        "answer": "The Max profit approach optimizes loan approval without fairness constraints, achieving 0.85 approval rate for Asian non-defaulters. Building classifiers with independency constraints, as mentioned in the references, would lower this rate to match fairness notions like the yellow Demography bar at approximately 0.63.",
        "visual_anchor": "Blue bar at approximately 0.85 for Asian demographic group",
        "text_evidence": "Building classifiers with independency constraints. In In Proc. IEEE International Conference on Data Mining Workshops, pages 13–18, 2009.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the green Single threshold bar at 0.55 for Black borrowers align with claims about disparate impact in big data?",
        "answer": "Yes, the green Single threshold bar showing only 0.55 loan approval for Black non-defaulters, compared to 0.82 for Asian borrowers, demonstrates disparate impact. This significant disparity across demographics exemplifies how algorithmic decisions can produce unequal outcomes even when targeting non-defaulters.",
        "visual_anchor": "Green bar at approximately 0.55 for Black demographic group",
        "text_evidence": "Solon Barocas and Andrew Selbst. Big data's disparate impact. California Law Review, 104, 2016.",
        "query_type": "value_context"
      },
      {
        "query": "Why do the yellow Demography bars show 0.63 to 0.90 range across demographics when learning fair classifiers?",
        "answer": "The yellow Demography bars vary from approximately 0.63 for Asian to 0.90 for Black borrowers because demographic fairness constraints aim to equalize approval rates across groups. This approach, related to learning fair classifiers, adjusts thresholds per demographic to balance loan access, though perfect equalization is not achieved.",
        "visual_anchor": "Yellow bars ranging from approximately 0.63 (Asian) to 0.90 (Black)",
        "text_evidence": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Learning fair classifiers. CoRR, abs:1507.05259, 2015.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue bars representing Max profit",
      "Green bars representing Single threshold",
      "Red bars representing Opportunity",
      "Teal bars representing Equal odds",
      "Yellow bars representing Demography",
      "Four demographic groups: Asian, White, Hispanic, Black",
      "Y-axis: Fraction non-defaulters getting loan (0.0 to 1.0)",
      "Right side: Profit as a fraction of max profit"
    ]
  },
  {
    "doc_id": "1610.07524",
    "figure_id": "1610.07524_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg",
    "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Black bars (gray) showing observed probability of recidivism\",\n    \"White bars (yellow/orange) showing observed probability of recidivism\",\n    \"X-axis: COMPAS decile score from 1 to 10\",\n    \"Y-axis: Observed probability of recidivism from 0.00 to 1.00\",\n    \"Error bars representing 95% confidence intervals\",\n    \"Overlapping bars at each decile score position\",\n    \"Increasing trend in probability as decile score increases\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the overlapping Black and White bars at each decile score verify the test fairness condition adherence?\",\n      \"answer\": \"Yes, the close overlap of Black and White bars across all decile scores demonstrates that P(Y=1|S=s,R) is similar for both races at each score level, which Flores et al. verified using logistic regression as adherence to test fairness.\",\n      \"visual_anchor\": \"overlapping gray and yellow bars at decile scores 1-10\",\n      \"text_evidence\": \"We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the observed probability rise from approximately 0.20 at decile 1 to 0.85 at decile 10?\",\n      \"answer\": \"The increasing probability across decile scores reflects that higher COMPAS scores indicate higher risk of recidivism, as the decile score is designed to stratify defendants by predicted recidivism risk.\",\n      \"visual_anchor\": \"Black bars rising from 0.20 at decile 1 to 0.85 at decile 10\",\n      \"text_evidence\": \"Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the error bars widening at decile 10 relate to creating the coarsened classifier mentioned for simplification?\",\n      \"answer\": \"The wider confidence intervals at extreme decile scores suggest uncertainty in fine-grained predictions, motivating the introduction of a coarsened binary score (high-risk vs. low-risk) that simplifies the classifier into a confusion matrix for error rate discussion.\",\n      \"visual_anchor\": \"wide error bars at decile 10 compared to middle decile scores\",\n      \"text_evidence\": \"To facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , wh\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black bars (gray) showing observed probability of recidivism",
        "White bars (yellow/orange) showing observed probability of recidivism",
        "X-axis: COMPAS decile score from 1 to 10",
        "Y-axis: Observed probability of recidivism from 0.00 to 1.00",
        "Error bars representing 95% confidence intervals",
        "Overlapping bars at each decile score position",
        "Increasing trend in probability as decile score increases"
      ],
      "queries": [
        {
          "query": "Do the overlapping Black and White bars at each decile score verify the test fairness condition adherence?",
          "answer": "Yes, the close overlap of Black and White bars across all decile scores demonstrates that P(Y=1|S=s,R) is similar for both races at each score level, which Flores et al. verified using logistic regression as adherence to test fairness.",
          "visual_anchor": "overlapping gray and yellow bars at decile scores 1-10",
          "text_evidence": "We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the observed probability rise from approximately 0.20 at decile 1 to 0.85 at decile 10?",
          "answer": "The increasing probability across decile scores reflects that higher COMPAS scores indicate higher risk of recidivism, as the decile score is designed to stratify defendants by predicted recidivism risk.",
          "visual_anchor": "Black bars rising from 0.20 at decile 1 to 0.85 at decile 10",
          "text_evidence": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score.",
          "query_type": "value_context"
        },
        {
          "query": "How do the error bars widening at decile 10 relate to creating the coarsened classifier mentioned for simplification?",
          "answer": "The wider confidence intervals at extreme decile scores suggest uncertainty in fine-grained predictions, motivating the introduction of a coarsened binary score (high-risk vs. low-risk) that simplifies the classifier into a confusion matrix for error rate discussion.",
          "visual_anchor": "wide error bars at decile 10 compared to middle decile scores",
          "text_evidence": "To facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , wh",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the overlapping Black and White bars at each decile score verify the test fairness condition adherence?",
        "answer": "Yes, the close overlap of Black and White bars across all decile scores demonstrates that P(Y=1|S=s,R) is similar for both races at each score level, which Flores et al. verified using logistic regression as adherence to test fairness.",
        "visual_anchor": "overlapping gray and yellow bars at decile scores 1-10",
        "text_evidence": "We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the observed probability rise from approximately 0.20 at decile 1 to 0.85 at decile 10?",
        "answer": "The increasing probability across decile scores reflects that higher COMPAS scores indicate higher risk of recidivism, as the decile score is designed to stratify defendants by predicted recidivism risk.",
        "visual_anchor": "Black bars rising from 0.20 at decile 1 to 0.85 at decile 10",
        "text_evidence": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score.",
        "query_type": "value_context"
      },
      {
        "query": "How do the error bars widening at decile 10 relate to creating the coarsened classifier mentioned for simplification?",
        "answer": "The wider confidence intervals at extreme decile scores suggest uncertainty in fine-grained predictions, motivating the introduction of a coarsened binary score (high-risk vs. low-risk) that simplifies the classifier into a confusion matrix for error rate discussion.",
        "visual_anchor": "wide error bars at decile 10 compared to middle decile scores",
        "text_evidence": "To facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , wh",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black bars (gray) showing observed probability of recidivism",
      "White bars (yellow/orange) showing observed probability of recidivism",
      "X-axis: COMPAS decile score from 1 to 10",
      "Y-axis: Observed probability of recidivism from 0.00 to 1.00",
      "Error bars representing 95% confidence intervals",
      "Overlapping bars at each decile score position",
      "Increasing trend in probability as decile score increases"
    ]
  },
  {
    "doc_id": "1610.07524",
    "figure_id": "1610.07524_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
    "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Gray bars representing Black defendants\",\n    \"Orange/yellow bars representing White defendants\",\n    \"Five groups on x-axis: 0, 1-3, 4-6, 7-10, >10 priors\",\n    \"Y-axis showing false positive rate from 0 to 1.00\",\n    \"Error bars showing 95% confidence intervals\",\n    \"Increasing trend in gray bars from left to right\",\n    \"Relatively flat trend in orange bars across groups\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the gray bar reach approximately 0.9 at seven to ten priors, supporting claims about higher prevalence groups?\",\n      \"answer\": \"When using a test-fair RPI in populations where recidivism prevalence differs, the higher recidivism prevalence group will generally have a higher FPR. This explains why Black defendants (gray bars) show escalating false positive rates reaching 0.9, resulting in greater penalties for defendants in the higher prevalence group.\",\n      \"visual_anchor\": \"Gray bar at 7-10 priors reaching approximately 0.9 false positive rate\",\n      \"text_evidence\": \"When using a test-fair RPI in populations where recidivism prevalence differs across groups, it will generally be the case that the higher recidivism prevalence group will have a higher FPR and lower FNR. From equations (3.2) and (3.3), we can see that this would result in greater penalties for defendants in the higher prevalence group\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the gray bar pattern from 0 to greater than ten priors illustrate disparate impact from predictive bias-free instruments?\",\n      \"answer\": \"Yes, the escalating gray bars demonstrate how disparate impact can result even when using a recidivism prediction instrument free from predictive bias. The primary contribution was showing this phenomenon in the simple binary risk assessment setting.\",\n      \"visual_anchor\": \"Increasing trend in gray bars across all five prior count categories from approximately 0.2 to 0.9\",\n      \"text_evidence\": \"The primary contribution of this paper was to show how disparate impact can result from the use of a recidivism prediction instrument that is known to be free from predictive bias. Our analysis focussed on the simple setting where a binary risk assessment was used to inform a binary penalty policy.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?\",\n      \"answer\": \"Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.\",\n      \"visual_anchor\": \"Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)\",\n      \"text_evidence\": \"That is, even if $\\\\mathrm { F P R } _ { b } = \\\\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Gray bars representing Black defendants",
        "Orange/yellow bars representing White defendants",
        "Five groups on x-axis: 0, 1-3, 4-6, 7-10, >10 priors",
        "Y-axis showing false positive rate from 0 to 1.00",
        "Error bars showing 95% confidence intervals",
        "Increasing trend in gray bars from left to right",
        "Relatively flat trend in orange bars across groups"
      ],
      "queries": [
        {
          "query": "Why does the gray bar reach approximately 0.9 at seven to ten priors, supporting claims about higher prevalence groups?",
          "answer": "When using a test-fair RPI in populations where recidivism prevalence differs, the higher recidivism prevalence group will generally have a higher FPR. This explains why Black defendants (gray bars) show escalating false positive rates reaching 0.9, resulting in greater penalties for defendants in the higher prevalence group.",
          "visual_anchor": "Gray bar at 7-10 priors reaching approximately 0.9 false positive rate",
          "text_evidence": "When using a test-fair RPI in populations where recidivism prevalence differs across groups, it will generally be the case that the higher recidivism prevalence group will have a higher FPR and lower FNR. From equations (3.2) and (3.3), we can see that this would result in greater penalties for defendants in the higher prevalence group",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the gray bar pattern from 0 to greater than ten priors illustrate disparate impact from predictive bias-free instruments?",
          "answer": "Yes, the escalating gray bars demonstrate how disparate impact can result even when using a recidivism prediction instrument free from predictive bias. The primary contribution was showing this phenomenon in the simple binary risk assessment setting.",
          "visual_anchor": "Increasing trend in gray bars across all five prior count categories from approximately 0.2 to 0.9",
          "text_evidence": "The primary contribution of this paper was to show how disparate impact can result from the use of a recidivism prediction instrument that is known to be free from predictive bias. Our analysis focussed on the simple setting where a binary risk assessment was used to inform a binary penalty policy.",
          "query_type": "value_context"
        },
        {
          "query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?",
          "answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.",
          "visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)",
          "text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the gray bar reach approximately 0.9 at seven to ten priors, supporting claims about higher prevalence groups?",
        "answer": "When using a test-fair RPI in populations where recidivism prevalence differs, the higher recidivism prevalence group will generally have a higher FPR. This explains why Black defendants (gray bars) show escalating false positive rates reaching 0.9, resulting in greater penalties for defendants in the higher prevalence group.",
        "visual_anchor": "Gray bar at 7-10 priors reaching approximately 0.9 false positive rate",
        "text_evidence": "When using a test-fair RPI in populations where recidivism prevalence differs across groups, it will generally be the case that the higher recidivism prevalence group will have a higher FPR and lower FNR. From equations (3.2) and (3.3), we can see that this would result in greater penalties for defendants in the higher prevalence group",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the gray bar pattern from 0 to greater than ten priors illustrate disparate impact from predictive bias-free instruments?",
        "answer": "Yes, the escalating gray bars demonstrate how disparate impact can result even when using a recidivism prediction instrument free from predictive bias. The primary contribution was showing this phenomenon in the simple binary risk assessment setting.",
        "visual_anchor": "Increasing trend in gray bars across all five prior count categories from approximately 0.2 to 0.9",
        "text_evidence": "The primary contribution of this paper was to show how disparate impact can result from the use of a recidivism prediction instrument that is known to be free from predictive bias. Our analysis focussed on the simple setting where a binary risk assessment was used to inform a binary penalty policy.",
        "query_type": "value_context"
      },
      {
        "query": "Why do differences persist between gray and orange bars within each prior category despite equal overall FPR claims?",
        "answer": "Even if overall FPR is equal between Black and White defendants, differences in error rates can still appear within individual prior record score categories. This explains the persistent gap between gray and orange bars across all five groupings shown.",
        "visual_anchor": "Consistent separation between gray and orange bars across all five x-axis categories (0, 1-3, 4-6, 7-10, >10)",
        "text_evidence": "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2).",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Gray bars representing Black defendants",
      "Orange/yellow bars representing White defendants",
      "Five groups on x-axis: 0, 1-3, 4-6, 7-10, >10 priors",
      "Y-axis showing false positive rate from 0 to 1.00",
      "Error bars showing 95% confidence intervals",
      "Increasing trend in gray bars from left to right",
      "Relatively flat trend in orange bars across groups"
    ]
  },
  {
    "doc_id": "1610.08452",
    "figure_id": "1610.08452_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg",
    "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"table\",\n  \"visual_elements\": [\"Gender column with Male/Female values\", \"Bulge column with 1/0 values\", \"Crime column with checkmark/X symbols\", \"C1, C2, C3 classifier decision columns\", \"Ground Truth column with 1/0 values\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does C1's decision to stop both Female 1 and Male 1 demonstrate disparate impact despite identical non-sensitive attributes?\",\n      \"answer\": \"C1 shows disparate impact because it stops individuals with the same bulge (1) and crime proximity (✓) values, but the decision disproportionately affects members based on their gender group, which is the sensitive attribute.\",\n      \"visual_anchor\": \"C1 column showing decision 1 for both Female 1 and Male 1 with identical Bulge=1 and Crime=✓\",\n      \"text_evidence\": \"there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does C2's decision of 0 for Male 2 with Bulge=0 minimize misclassifications given the training objective described?\",\n      \"answer\": \"No, C2's decision creates a false negative since Male 2's ground truth is 1 (actually possessing a weapon), which represents a misclassification error that training aims to minimize.\",\n      \"visual_anchor\": \"C2 column showing decision 0 for Male 2 with Ground Truth=1\",\n      \"text_evidence\": \"In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does C3's checkmark pattern across all three individuals illustrate the distinction between sensitive and non-sensitive attributes?\",\n      \"answer\": \"C3 stops all individuals regardless of Gender (sensitive attribute) and regardless of varying Bulge and Crime values (non-sensitive attributes), showing decisions that ignore the distinction between attribute types.\",\n      \"visual_anchor\": \"C3 column showing all checkmarks for Female 1, Male 1, and Male 2\",\n      \"text_evidence\": \"Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "table",
      "visual_elements": [
        "Gender column with Male/Female values",
        "Bulge column with 1/0 values",
        "Crime column with checkmark/X symbols",
        "C1, C2, C3 classifier decision columns",
        "Ground Truth column with 1/0 values"
      ],
      "queries": [
        {
          "query": "Why does C1's decision to stop both Female 1 and Male 1 demonstrate disparate impact despite identical non-sensitive attributes?",
          "answer": "C1 shows disparate impact because it stops individuals with the same bulge (1) and crime proximity (✓) values, but the decision disproportionately affects members based on their gender group, which is the sensitive attribute.",
          "visual_anchor": "C1 column showing decision 1 for both Female 1 and Male 1 with identical Bulge=1 and Crime=✓",
          "text_evidence": "there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does C2's decision of 0 for Male 2 with Bulge=0 minimize misclassifications given the training objective described?",
          "answer": "No, C2's decision creates a false negative since Male 2's ground truth is 1 (actually possessing a weapon), which represents a misclassification error that training aims to minimize.",
          "visual_anchor": "C2 column showing decision 0 for Male 2 with Ground Truth=1",
          "text_evidence": "In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does C3's checkmark pattern across all three individuals illustrate the distinction between sensitive and non-sensitive attributes?",
          "answer": "C3 stops all individuals regardless of Gender (sensitive attribute) and regardless of varying Bulge and Crime values (non-sensitive attributes), showing decisions that ignore the distinction between attribute types.",
          "visual_anchor": "C3 column showing all checkmarks for Female 1, Male 1, and Male 2",
          "text_evidence": "Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does C1's decision to stop both Female 1 and Male 1 demonstrate disparate impact despite identical non-sensitive attributes?",
        "answer": "C1 shows disparate impact because it stops individuals with the same bulge (1) and crime proximity (✓) values, but the decision disproportionately affects members based on their gender group, which is the sensitive attribute.",
        "visual_anchor": "C1 column showing decision 1 for both Female 1 and Male 1 with identical Bulge=1 and Crime=✓",
        "text_evidence": "there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does C2's decision of 0 for Male 2 with Bulge=0 minimize misclassifications given the training objective described?",
        "answer": "No, C2's decision creates a false negative since Male 2's ground truth is 1 (actually possessing a weapon), which represents a misclassification error that training aims to minimize.",
        "visual_anchor": "C2 column showing decision 0 for Male 2 with Ground Truth=1",
        "text_evidence": "In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does C3's checkmark pattern across all three individuals illustrate the distinction between sensitive and non-sensitive attributes?",
        "answer": "C3 stops all individuals regardless of Gender (sensitive attribute) and regardless of varying Bulge and Crime values (non-sensitive attributes), showing decisions that ignore the distinction between attribute types.",
        "visual_anchor": "C3 column showing all checkmarks for Female 1, Male 1, and Male 2",
        "text_evidence": "Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "table",
    "visual_elements": [
      "Gender column with Male/Female values",
      "Bulge column with 1/0 values",
      "Crime column with checkmark/X symbols",
      "C1, C2, C3 classifier decision columns",
      "Ground Truth column with 1/0 values"
    ]
  },
  {
    "doc_id": "1610.08452",
    "figure_id": "1610.08452_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg",
    "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue asterisk markers showing z=0 false positive rate decreasing from ~0.25 to ~0.13\",\n    \"Magenta square markers showing z=1 false positive rate increasing from ~0.05 to ~0.12\",\n    \"X-axis showing covariance multiplicative factor (m) from 1 to 0\",\n    \"Y-axis showing false positive rate from 0 to 0.3\",\n    \"Convergence of both marker types at m≈0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue asterisks decrease from 0.25 to 0.13 while magenta squares rise, achieving the scenario where disparate mistreatment exists only in false positive rate?\",\n      \"answer\": \"The fairness constraints rotate the decision boundary to move z=0 examples into the negative class (decreasing their false positives) while moving z=1 examples into the positive class (increasing their false positives), addressing the scenario where only false positive rate shows disparate mistreatment while false negative rate remains fair.\",\n      \"visual_anchor\": \"blue asterisk markers decreasing from ~0.25 to ~0.13 and magenta square markers increasing from ~0.05 to ~0.12\",\n      \"text_evidence\": \"The first scenario considers a case where a classifier trained on the ground truth data leads to disparate mistreatment in terms of only the false positive rate (false negative rate), while being fair with respect to false negative rate (false positive rate), i.e., $D _ { F P R } \\\\neq 0$ and $D _ { F N R } = 0$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the convergence of both marker types at m approaching zero demonstrate the relation between decision-boundary covariance and false positive rates?\",\n      \"answer\": \"Yes, as the covariance multiplicative factor decreases toward zero, the false positive rates for both groups converge to approximately 0.12-0.13, demonstrating that decreasing the covariance threshold causes the false positive rates to become similar across groups.\",\n      \"visual_anchor\": \"convergence point at m≈0 where blue asterisks and magenta squares meet at ~0.12-0.13\",\n      \"text_evidence\": \"Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the gap between blue asterisks and magenta squares narrowing from 0.20 to near-zero relate to the more complex disparate mistreatment scenario?\",\n      \"answer\": \"The narrowing gap demonstrates fairness enforcement in false positive rates, which is a prerequisite before addressing the more complex scenario where both false positive and false negative rates show non-zero disparate mistreatment simultaneously.\",\n      \"visual_anchor\": \"gap between blue asterisk and magenta square markers narrowing from ~0.20 at m=1 to ~0.01 at m=0\",\n      \"text_evidence\": \"In this section, we consider a more complex scenario, where the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive rate and false negative rate, i.e., both $D _ { F P R }$ and $D _ { F N R }$ are non-zero.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue asterisk markers showing z=0 false positive rate decreasing from ~0.25 to ~0.13",
        "Magenta square markers showing z=1 false positive rate increasing from ~0.05 to ~0.12",
        "X-axis showing covariance multiplicative factor (m) from 1 to 0",
        "Y-axis showing false positive rate from 0 to 0.3",
        "Convergence of both marker types at m≈0"
      ],
      "queries": [
        {
          "query": "Why do the blue asterisks decrease from 0.25 to 0.13 while magenta squares rise, achieving the scenario where disparate mistreatment exists only in false positive rate?",
          "answer": "The fairness constraints rotate the decision boundary to move z=0 examples into the negative class (decreasing their false positives) while moving z=1 examples into the positive class (increasing their false positives), addressing the scenario where only false positive rate shows disparate mistreatment while false negative rate remains fair.",
          "visual_anchor": "blue asterisk markers decreasing from ~0.25 to ~0.13 and magenta square markers increasing from ~0.05 to ~0.12",
          "text_evidence": "The first scenario considers a case where a classifier trained on the ground truth data leads to disparate mistreatment in terms of only the false positive rate (false negative rate), while being fair with respect to false negative rate (false positive rate), i.e., $D _ { F P R } \\neq 0$ and $D _ { F N R } = 0$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the convergence of both marker types at m approaching zero demonstrate the relation between decision-boundary covariance and false positive rates?",
          "answer": "Yes, as the covariance multiplicative factor decreases toward zero, the false positive rates for both groups converge to approximately 0.12-0.13, demonstrating that decreasing the covariance threshold causes the false positive rates to become similar across groups.",
          "visual_anchor": "convergence point at m≈0 where blue asterisks and magenta squares meet at ~0.12-0.13",
          "text_evidence": "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values",
          "query_type": "value_context"
        },
        {
          "query": "How does the gap between blue asterisks and magenta squares narrowing from 0.20 to near-zero relate to the more complex disparate mistreatment scenario?",
          "answer": "The narrowing gap demonstrates fairness enforcement in false positive rates, which is a prerequisite before addressing the more complex scenario where both false positive and false negative rates show non-zero disparate mistreatment simultaneously.",
          "visual_anchor": "gap between blue asterisk and magenta square markers narrowing from ~0.20 at m=1 to ~0.01 at m=0",
          "text_evidence": "In this section, we consider a more complex scenario, where the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive rate and false negative rate, i.e., both $D _ { F P R }$ and $D _ { F N R }$ are non-zero.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue asterisks decrease from 0.25 to 0.13 while magenta squares rise, achieving the scenario where disparate mistreatment exists only in false positive rate?",
        "answer": "The fairness constraints rotate the decision boundary to move z=0 examples into the negative class (decreasing their false positives) while moving z=1 examples into the positive class (increasing their false positives), addressing the scenario where only false positive rate shows disparate mistreatment while false negative rate remains fair.",
        "visual_anchor": "blue asterisk markers decreasing from ~0.25 to ~0.13 and magenta square markers increasing from ~0.05 to ~0.12",
        "text_evidence": "The first scenario considers a case where a classifier trained on the ground truth data leads to disparate mistreatment in terms of only the false positive rate (false negative rate), while being fair with respect to false negative rate (false positive rate), i.e., $D _ { F P R } \\neq 0$ and $D _ { F N R } = 0$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the convergence of both marker types at m approaching zero demonstrate the relation between decision-boundary covariance and false positive rates?",
        "answer": "Yes, as the covariance multiplicative factor decreases toward zero, the false positive rates for both groups converge to approximately 0.12-0.13, demonstrating that decreasing the covariance threshold causes the false positive rates to become similar across groups.",
        "visual_anchor": "convergence point at m≈0 where blue asterisks and magenta squares meet at ~0.12-0.13",
        "text_evidence": "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values",
        "query_type": "value_context"
      },
      {
        "query": "How does the gap between blue asterisks and magenta squares narrowing from 0.20 to near-zero relate to the more complex disparate mistreatment scenario?",
        "answer": "The narrowing gap demonstrates fairness enforcement in false positive rates, which is a prerequisite before addressing the more complex scenario where both false positive and false negative rates show non-zero disparate mistreatment simultaneously.",
        "visual_anchor": "gap between blue asterisk and magenta square markers narrowing from ~0.20 at m=1 to ~0.01 at m=0",
        "text_evidence": "In this section, we consider a more complex scenario, where the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive rate and false negative rate, i.e., both $D _ { F P R }$ and $D _ { F N R }$ are non-zero.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue asterisk markers showing z=0 false positive rate decreasing from ~0.25 to ~0.13",
      "Magenta square markers showing z=1 false positive rate increasing from ~0.05 to ~0.12",
      "X-axis showing covariance multiplicative factor (m) from 1 to 0",
      "Y-axis showing false positive rate from 0 to 0.3",
      "Convergence of both marker types at m≈0"
    ]
  },
  {
    "doc_id": "1610.08452",
    "figure_id": "1610.08452_fig_6",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg",
    "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid cyan decision boundary line\",\n    \"Dashed purple decision boundary line\",\n    \"Red circles and crosses (one protected group)\",\n    \"Green circles and crosses (another protected group)\",\n    \"Legend showing Acc=0.78, FPR=0.14:0.30, FNR=0.31:0.12 for solid line\",\n    \"Legend showing Acc=0.75, FPR=0.26:0.27, FNR=0.24:0.24 for dashed line\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid cyan boundary achieve opposite-signed FPR values 0.14 and 0.30 when disparate mistreatment signs are opposite?\",\n      \"answer\": \"The opposite signs of D_FPR and D_FNR in this scenario mean that removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR, resulting in the observed FPR disparity.\",\n      \"visual_anchor\": \"Solid cyan line with FPR=0.14:0.30 in legend\",\n      \"text_evidence\": \"$D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the dashed purple boundary's nearly equal FNR of 0.24:0.24 confirm that constraining both metrics simultaneously yields similar results?\",\n      \"answer\": \"Yes, the dashed line shows FNR=0.24:0.24, demonstrating that removing disparate mistreatment on both FPR and FNR at the same time leads to very similar fairness results across groups.\",\n      \"visual_anchor\": \"Dashed purple line with FNR=0.24:0.24 in legend\",\n      \"text_evidence\": \"Removing disparate mistreatment on both at the same time leads to very similar results.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the solid cyan and dashed purple boundaries illustrate the unconstrained versus constrained fair classifiers summarized for this scenario?\",\n      \"answer\": \"The solid cyan line represents the constrained classifier optimizing FPR fairness, while the dashed purple line represents constraining both FPR and FNR simultaneously, demonstrating different fairness-accuracy trade-offs.\",\n      \"visual_anchor\": \"Solid cyan and dashed purple decision boundaries separating red and green data points\",\n      \"text_evidence\": \"Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid cyan decision boundary line",
        "Dashed purple decision boundary line",
        "Red circles and crosses (one protected group)",
        "Green circles and crosses (another protected group)",
        "Legend showing Acc=0.78, FPR=0.14:0.30, FNR=0.31:0.12 for solid line",
        "Legend showing Acc=0.75, FPR=0.26:0.27, FNR=0.24:0.24 for dashed line"
      ],
      "queries": [
        {
          "query": "Why does the solid cyan boundary achieve opposite-signed FPR values 0.14 and 0.30 when disparate mistreatment signs are opposite?",
          "answer": "The opposite signs of D_FPR and D_FNR in this scenario mean that removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR, resulting in the observed FPR disparity.",
          "visual_anchor": "Solid cyan line with FPR=0.14:0.30 in legend",
          "text_evidence": "$D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the dashed purple boundary's nearly equal FNR of 0.24:0.24 confirm that constraining both metrics simultaneously yields similar results?",
          "answer": "Yes, the dashed line shows FNR=0.24:0.24, demonstrating that removing disparate mistreatment on both FPR and FNR at the same time leads to very similar fairness results across groups.",
          "visual_anchor": "Dashed purple line with FNR=0.24:0.24 in legend",
          "text_evidence": "Removing disparate mistreatment on both at the same time leads to very similar results.",
          "query_type": "value_context"
        },
        {
          "query": "How do the solid cyan and dashed purple boundaries illustrate the unconstrained versus constrained fair classifiers summarized for this scenario?",
          "answer": "The solid cyan line represents the constrained classifier optimizing FPR fairness, while the dashed purple line represents constraining both FPR and FNR simultaneously, demonstrating different fairness-accuracy trade-offs.",
          "visual_anchor": "Solid cyan and dashed purple decision boundaries separating red and green data points",
          "text_evidence": "Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid cyan boundary achieve opposite-signed FPR values 0.14 and 0.30 when disparate mistreatment signs are opposite?",
        "answer": "The opposite signs of D_FPR and D_FNR in this scenario mean that removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR, resulting in the observed FPR disparity.",
        "visual_anchor": "Solid cyan line with FPR=0.14:0.30 in legend",
        "text_evidence": "$D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the dashed purple boundary's nearly equal FNR of 0.24:0.24 confirm that constraining both metrics simultaneously yields similar results?",
        "answer": "Yes, the dashed line shows FNR=0.24:0.24, demonstrating that removing disparate mistreatment on both FPR and FNR at the same time leads to very similar fairness results across groups.",
        "visual_anchor": "Dashed purple line with FNR=0.24:0.24 in legend",
        "text_evidence": "Removing disparate mistreatment on both at the same time leads to very similar results.",
        "query_type": "value_context"
      },
      {
        "query": "How do the solid cyan and dashed purple boundaries illustrate the unconstrained versus constrained fair classifiers summarized for this scenario?",
        "answer": "The solid cyan line represents the constrained classifier optimizing FPR fairness, while the dashed purple line represents constraining both FPR and FNR simultaneously, demonstrating different fairness-accuracy trade-offs.",
        "visual_anchor": "Solid cyan and dashed purple decision boundaries separating red and green data points",
        "text_evidence": "Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid cyan decision boundary line",
      "Dashed purple decision boundary line",
      "Red circles and crosses (one protected group)",
      "Green circles and crosses (another protected group)",
      "Legend showing Acc=0.78, FPR=0.14:0.30, FNR=0.31:0.12 for solid line",
      "Legend showing Acc=0.75, FPR=0.26:0.27, FNR=0.24:0.24 for dashed line"
    ]
  },
  {
    "doc_id": "1610.08452",
    "figure_id": "1610.08452_fig_9",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
    "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid cyan/teal line representing unconstrained classifier boundary\",\n    \"dashed blue line representing fair constrained classifier boundary\",\n    \"red circles and x markers representing one class\",\n    \"green circles and x markers representing another class\",\n    \"legend showing Acc=0.80; FPR=0.33:0.08; FNR=0.26:0.12 for solid line\",\n    \"legend showing Acc=0.77; FPR=0.63:0.07; FNR=14:0.10 for dashed line\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dashed blue boundary achieve FPR=0.63:0.07 while the solid achieves 0.33:0.08?\",\n      \"answer\": \"The dashed boundary implements fairness constraints to avoid disparate mistreatment by controlling false positive rates between groups, causing it to shift and equalize FPR across protected attributes despite lower overall accuracy.\",\n      \"visual_anchor\": \"dashed blue line with FPR=0.63:0.07 versus solid cyan line with FPR=0.33:0.08 in legend\",\n      \"text_evidence\": \"Our method: implements our scheme to avoid disparate treatment and disparate mistreatment simultaneously. Disparate mistreatment is avoided by using fairness constraints\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the accuracy drop from 0.80 to 0.77 shown in the legend support controlling both rates simultaneously?\",\n      \"answer\": \"Yes, the accuracy decrease from the solid to dashed boundary demonstrates that removing disparate mistreatment on both FPR and FNR simultaneously causes a larger drop in accuracy, as stated in the methodology.\",\n      \"visual_anchor\": \"legend showing Acc=0.80 for solid line versus Acc=0.77 for dashed line\",\n      \"text_evidence\": \"Removing disparate mistreatment on both at the same time causes a larger drop in accuracy\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?\",\n      \"answer\": \"The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.\",\n      \"visual_anchor\": \"dashed blue line positioned differently from solid cyan line relative to red and green markers\",\n      \"text_evidence\": \"Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid cyan/teal line representing unconstrained classifier boundary",
        "dashed blue line representing fair constrained classifier boundary",
        "red circles and x markers representing one class",
        "green circles and x markers representing another class",
        "legend showing Acc=0.80; FPR=0.33:0.08; FNR=0.26:0.12 for solid line",
        "legend showing Acc=0.77; FPR=0.63:0.07; FNR=14:0.10 for dashed line"
      ],
      "queries": [
        {
          "query": "Why does the dashed blue boundary achieve FPR=0.63:0.07 while the solid achieves 0.33:0.08?",
          "answer": "The dashed boundary implements fairness constraints to avoid disparate mistreatment by controlling false positive rates between groups, causing it to shift and equalize FPR across protected attributes despite lower overall accuracy.",
          "visual_anchor": "dashed blue line with FPR=0.63:0.07 versus solid cyan line with FPR=0.33:0.08 in legend",
          "text_evidence": "Our method: implements our scheme to avoid disparate treatment and disparate mistreatment simultaneously. Disparate mistreatment is avoided by using fairness constraints",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the accuracy drop from 0.80 to 0.77 shown in the legend support controlling both rates simultaneously?",
          "answer": "Yes, the accuracy decrease from the solid to dashed boundary demonstrates that removing disparate mistreatment on both FPR and FNR simultaneously causes a larger drop in accuracy, as stated in the methodology.",
          "visual_anchor": "legend showing Acc=0.80 for solid line versus Acc=0.77 for dashed line",
          "text_evidence": "Removing disparate mistreatment on both at the same time causes a larger drop in accuracy",
          "query_type": "value_context"
        },
        {
          "query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
          "answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
          "visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
          "text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dashed blue boundary achieve FPR=0.63:0.07 while the solid achieves 0.33:0.08?",
        "answer": "The dashed boundary implements fairness constraints to avoid disparate mistreatment by controlling false positive rates between groups, causing it to shift and equalize FPR across protected attributes despite lower overall accuracy.",
        "visual_anchor": "dashed blue line with FPR=0.63:0.07 versus solid cyan line with FPR=0.33:0.08 in legend",
        "text_evidence": "Our method: implements our scheme to avoid disparate treatment and disparate mistreatment simultaneously. Disparate mistreatment is avoided by using fairness constraints",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the accuracy drop from 0.80 to 0.77 shown in the legend support controlling both rates simultaneously?",
        "answer": "Yes, the accuracy decrease from the solid to dashed boundary demonstrates that removing disparate mistreatment on both FPR and FNR simultaneously causes a larger drop in accuracy, as stated in the methodology.",
        "visual_anchor": "legend showing Acc=0.80 for solid line versus Acc=0.77 for dashed line",
        "text_evidence": "Removing disparate mistreatment on both at the same time causes a larger drop in accuracy",
        "query_type": "value_context"
      },
      {
        "query": "What causes the dashed boundary to separate more red circles from green ones compared to the solid boundary?",
        "answer": "The fair constrained classifier shifts the decision boundary to equalize false positive and negative rates across groups, which the research compares against unconstrained classifiers to demonstrate performance trade-offs when avoiding disparate mistreatment.",
        "visual_anchor": "dashed blue line positioned differently from solid cyan line relative to red and green markers",
        "text_evidence": "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid cyan/teal line representing unconstrained classifier boundary",
      "dashed blue line representing fair constrained classifier boundary",
      "red circles and x markers representing one class",
      "green circles and x markers representing another class",
      "legend showing Acc=0.80; FPR=0.33:0.08; FNR=0.26:0.12 for solid line",
      "legend showing Acc=0.77; FPR=0.63:0.07; FNR=14:0.10 for dashed line"
    ]
  },
  {
    "doc_id": "1611.07438",
    "figure_id": "1611.07438_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg",
    "caption": "Figure 1: Causal graph of an example university admission system.",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"node labeled 'major'\", \"node labeled 'gender'\", \"node labeled 'admission'\", \"node labeled 'test_score'\", \"arrow from major to admission\", \"arrow from gender to admission\", \"arrow from test_score to admission\", \"arrow from major to gender\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the direct arrow from gender to admission represent the discriminatory effect quantified by |ΔP|b < τ in each subpopulation?\",\n      \"answer\": \"Yes, the direct arrow from gender to admission represents potential direct discrimination. The criterion |ΔP|b < τ ensures each subpopulation b contains no discriminatory effect.\",\n      \"visual_anchor\": \"direct arrow from gender node to admission node\",\n      \"text_evidence\": \"if each block set B is examined and ensures that | ΔP | _ { \\\\bf b } | < τ holds for each subpopulation b, we can guarantee that the dataset contains no discriminatory effect and is not liable for any claim of direct discrimination\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does major point to both gender and admission when the dataset includes gender as a protected attribute?\",\n      \"answer\": \"Major influences both admission decisions and correlates with gender distribution. The dataset includes gender as the protected attribute along with admission as the decision attribute and major as a non-protected attribute.\",\n      \"visual_anchor\": \"two outgoing arrows from major node, one to gender and one to admission\",\n      \"text_evidence\": \"Each individual in $\\\\mathcal { D }$ is specified by a set of attributes V, which includes the protected attribute (e.g., gender), the decision attribute (e.g., admission), and the non-protected attributes (e.g., major)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Which three nodes have direct causal paths to admission in this university system's causal graph structure?\",\n      \"answer\": \"Major, gender, and test_score all have direct arrows pointing to the admission node, representing their direct causal influence on admission decisions in the university system.\",\n      \"visual_anchor\": \"three incoming arrows to the admission node from major, gender, and test_score\",\n      \"text_evidence\": \"Consider a dataset $\\\\mathcal { D }$ which may contain discrimination against a certain protected group. Each individual in $\\\\mathcal { D }$ is specified by a set of attributes V\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "node labeled 'major'",
        "node labeled 'gender'",
        "node labeled 'admission'",
        "node labeled 'test_score'",
        "arrow from major to admission",
        "arrow from gender to admission",
        "arrow from test_score to admission",
        "arrow from major to gender"
      ],
      "queries": [
        {
          "query": "Does the direct arrow from gender to admission represent the discriminatory effect quantified by |ΔP|b < τ in each subpopulation?",
          "answer": "Yes, the direct arrow from gender to admission represents potential direct discrimination. The criterion |ΔP|b < τ ensures each subpopulation b contains no discriminatory effect.",
          "visual_anchor": "direct arrow from gender node to admission node",
          "text_evidence": "if each block set B is examined and ensures that | ΔP | _ { \\bf b } | < τ holds for each subpopulation b, we can guarantee that the dataset contains no discriminatory effect and is not liable for any claim of direct discrimination",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does major point to both gender and admission when the dataset includes gender as a protected attribute?",
          "answer": "Major influences both admission decisions and correlates with gender distribution. The dataset includes gender as the protected attribute along with admission as the decision attribute and major as a non-protected attribute.",
          "visual_anchor": "two outgoing arrows from major node, one to gender and one to admission",
          "text_evidence": "Each individual in $\\mathcal { D }$ is specified by a set of attributes V, which includes the protected attribute (e.g., gender), the decision attribute (e.g., admission), and the non-protected attributes (e.g., major)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Which three nodes have direct causal paths to admission in this university system's causal graph structure?",
          "answer": "Major, gender, and test_score all have direct arrows pointing to the admission node, representing their direct causal influence on admission decisions in the university system.",
          "visual_anchor": "three incoming arrows to the admission node from major, gender, and test_score",
          "text_evidence": "Consider a dataset $\\mathcal { D }$ which may contain discrimination against a certain protected group. Each individual in $\\mathcal { D }$ is specified by a set of attributes V",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the direct arrow from gender to admission represent the discriminatory effect quantified by |ΔP|b < τ in each subpopulation?",
        "answer": "Yes, the direct arrow from gender to admission represents potential direct discrimination. The criterion |ΔP|b < τ ensures each subpopulation b contains no discriminatory effect.",
        "visual_anchor": "direct arrow from gender node to admission node",
        "text_evidence": "if each block set B is examined and ensures that | ΔP | _ { \\bf b } | < τ holds for each subpopulation b, we can guarantee that the dataset contains no discriminatory effect and is not liable for any claim of direct discrimination",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does major point to both gender and admission when the dataset includes gender as a protected attribute?",
        "answer": "Major influences both admission decisions and correlates with gender distribution. The dataset includes gender as the protected attribute along with admission as the decision attribute and major as a non-protected attribute.",
        "visual_anchor": "two outgoing arrows from major node, one to gender and one to admission",
        "text_evidence": "Each individual in $\\mathcal { D }$ is specified by a set of attributes V, which includes the protected attribute (e.g., gender), the decision attribute (e.g., admission), and the non-protected attributes (e.g., major)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Which three nodes have direct causal paths to admission in this university system's causal graph structure?",
        "answer": "Major, gender, and test_score all have direct arrows pointing to the admission node, representing their direct causal influence on admission decisions in the university system.",
        "visual_anchor": "three incoming arrows to the admission node from major, gender, and test_score",
        "text_evidence": "Consider a dataset $\\mathcal { D }$ which may contain discrimination against a certain protected group. Each individual in $\\mathcal { D }$ is specified by a set of attributes V",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "node labeled 'major'",
      "node labeled 'gender'",
      "node labeled 'admission'",
      "node labeled 'test_score'",
      "arrow from major to admission",
      "arrow from gender to admission",
      "arrow from test_score to admission",
      "arrow from major to gender"
    ]
  },
  {
    "doc_id": "1611.07438",
    "figure_id": "1611.07438_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg",
    "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"red node labeled 'sex'\", \"blue node labeled 'income'\", \"green nodes (race, age, edu, hour, work_class, marital, native_country, occupation, relationship)\", \"directed edges showing causal relationships\", \"arrows pointing from multiple green nodes to blue income node\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do multiple green nodes point to the blue income node while the red sex node lacks direct connection?\",\n      \"answer\": \"The green nodes represent set Q containing non-protected attributes that causally influence income. Sex is the protected attribute (red) whose effect is mediated through Q variables rather than directly impacting income.\",\n      \"visual_anchor\": \"blue income node at bottom with incoming arrows from green nodes but no direct arrow from red sex node\",\n      \"text_evidence\": \"the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does examining major alone versus major with test score explain why green nodes form set Q in this graph?\",\n      \"answer\": \"Yes. The example shows that examining individual attributes (like major) may miss discriminatory effects, but examining combinations reveals bias. Set Q contains attributes whose combinations expose conditional disparities, similar to how {major, test score} revealed discrimination.\",\n      \"visual_anchor\": \"multiple green nodes (edu, work_class, occupation) forming set Q with interconnected edges\",\n      \"text_evidence\": \"For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\\\Delta P | _ { \\\\mathrm { \\\\{ m a t h , B \\\\} } } | = 0\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the variance formula relate to edges connecting green nodes to the blue income decision node?\",\n      \"answer\": \"The variance formula σ²_B measures disparity across different combinations of attributes B. The edges from green nodes to income represent how different attribute combinations (b) produce varying discrimination levels ΔP|_b, which the variance formula aggregates.\",\n      \"visual_anchor\": \"directed edges from green nodes (race, age, edu, native_country, hour, work_class, marital, occupation, relationship) converging to blue income node\",\n      \"text_evidence\": \"where $\\\\begin{array} { r } { \\\\hat { \\\\mu } _ { \\\\mathbf { B } } = \\\\sum _ { \\\\mathbf { B } } \\\\operatorname* { P r } ( \\\\mathbf { b } ) \\\\cdot \\\\Delta P | _ { \\\\mathbf { b } } } \\\\end{array}$ and $\\\\begin{array} { r } { \\\\hat { \\\\sigma } _ { \\\\mathbf { B } } ^ { 2 } = \\\\sum _ { \\\\mathbf { B } } \\\\operatorname* { P r } ( \\\\mathbf { b } ) ( \\\\Delta P | _ { \\\\mathbf { b } } - \\\\hat { \\\\mu } _ { \\\\mathbf { B } } ) ^ { 2 } , } \\\\end{array}$\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "red node labeled 'sex'",
        "blue node labeled 'income'",
        "green nodes (race, age, edu, hour, work_class, marital, native_country, occupation, relationship)",
        "directed edges showing causal relationships",
        "arrows pointing from multiple green nodes to blue income node"
      ],
      "queries": [
        {
          "query": "Why do multiple green nodes point to the blue income node while the red sex node lacks direct connection?",
          "answer": "The green nodes represent set Q containing non-protected attributes that causally influence income. Sex is the protected attribute (red) whose effect is mediated through Q variables rather than directly impacting income.",
          "visual_anchor": "blue income node at bottom with incoming arrows from green nodes but no direct arrow from red sex node",
          "text_evidence": "the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does examining major alone versus major with test score explain why green nodes form set Q in this graph?",
          "answer": "Yes. The example shows that examining individual attributes (like major) may miss discriminatory effects, but examining combinations reveals bias. Set Q contains attributes whose combinations expose conditional disparities, similar to how {major, test score} revealed discrimination.",
          "visual_anchor": "multiple green nodes (edu, work_class, occupation) forming set Q with interconnected edges",
          "text_evidence": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the variance formula relate to edges connecting green nodes to the blue income decision node?",
          "answer": "The variance formula σ²_B measures disparity across different combinations of attributes B. The edges from green nodes to income represent how different attribute combinations (b) produce varying discrimination levels ΔP|_b, which the variance formula aggregates.",
          "visual_anchor": "directed edges from green nodes (race, age, edu, native_country, hour, work_class, marital, occupation, relationship) converging to blue income node",
          "text_evidence": "where $\\begin{array} { r } { \\hat { \\mu } _ { \\mathbf { B } } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) \\cdot \\Delta P | _ { \\mathbf { b } } } \\end{array}$ and $\\begin{array} { r } { \\hat { \\sigma } _ { \\mathbf { B } } ^ { 2 } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) ( \\Delta P | _ { \\mathbf { b } } - \\hat { \\mu } _ { \\mathbf { B } } ) ^ { 2 } , } \\end{array}$",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do multiple green nodes point to the blue income node while the red sex node lacks direct connection?",
        "answer": "The green nodes represent set Q containing non-protected attributes that causally influence income. Sex is the protected attribute (red) whose effect is mediated through Q variables rather than directly impacting income.",
        "visual_anchor": "blue income node at bottom with incoming arrows from green nodes but no direct arrow from red sex node",
        "text_evidence": "the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does examining major alone versus major with test score explain why green nodes form set Q in this graph?",
        "answer": "Yes. The example shows that examining individual attributes (like major) may miss discriminatory effects, but examining combinations reveals bias. Set Q contains attributes whose combinations expose conditional disparities, similar to how {major, test score} revealed discrimination.",
        "visual_anchor": "multiple green nodes (edu, work_class, occupation) forming set Q with interconnected edges",
        "text_evidence": "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the variance formula relate to edges connecting green nodes to the blue income decision node?",
        "answer": "The variance formula σ²_B measures disparity across different combinations of attributes B. The edges from green nodes to income represent how different attribute combinations (b) produce varying discrimination levels ΔP|_b, which the variance formula aggregates.",
        "visual_anchor": "directed edges from green nodes (race, age, edu, native_country, hour, work_class, marital, occupation, relationship) converging to blue income node",
        "text_evidence": "where $\\begin{array} { r } { \\hat { \\mu } _ { \\mathbf { B } } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) \\cdot \\Delta P | _ { \\mathbf { b } } } \\end{array}$ and $\\begin{array} { r } { \\hat { \\sigma } _ { \\mathbf { B } } ^ { 2 } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) ( \\Delta P | _ { \\mathbf { b } } - \\hat { \\mu } _ { \\mathbf { B } } ) ^ { 2 } , } \\end{array}$",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "red node labeled 'sex'",
      "blue node labeled 'income'",
      "green nodes (race, age, edu, hour, work_class, marital, native_country, occupation, relationship)",
      "directed edges showing causal relationships",
      "arrows pointing from multiple green nodes to blue income node"
    ]
  },
  {
    "doc_id": "1611.07438",
    "figure_id": "1611.07438_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg",
    "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"red node labeled 'sex'\",\n    \"blue node labeled 'edu'\",\n    \"green nodes labeled 'age' and 'edu'\",\n    \"black nodes including 'citizenship', 'prev_residence', 'occupation', 'economic_state', 'marital', 'household_position', 'househol_dsize', 'cur_eco_activity'\",\n    \"directed edges connecting nodes\",\n    \"node labeled 'country_of_birth' at top left\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why is sex marked as the red node when the graph represents the Dutch Census causal structure?\",\n      \"answer\": \"Sex is the protected attribute in this discrimination discovery analysis of the Dutch Census dataset, requiring special identification in the causal framework.\",\n      \"visual_anchor\": \"red node labeled 'sex' in top right area\",\n      \"text_evidence\": \"the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the green node age relate to the blue decision node edu in discovering conditional discrimination patterns?\",\n      \"answer\": \"Age is part of set Q, which helps partition the population to detect conditional discrimination in educational outcomes, as the authors extended prior conditional discrimination approaches.\",\n      \"visual_anchor\": \"green node 'age' with edge pointing to blue node 'edu'\",\n      \"text_evidence\": \"Zliobaitˇ e et˙ al. [28] proposed conditional discrimination. However, their approaches cannot determine whether a partition is meaning\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What discrimination detection methods do the black nodes like citizenship and prev_residence support beyond classification rule-based approaches?\",\n      \"answer\": \"These nodes represent additional variables beyond the protected attribute, decision, and set Q, enabling more comprehensive discrimination pattern discovery than simple elift or belift rule-based methods.\",\n      \"visual_anchor\": \"black nodes 'citizenship' and 'prev_residence' in middle section\",\n      \"text_evidence\": \"Classification rule-based methods such as elift [23] and belift [20] were proposed to represent certain discrimination patterns.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "red node labeled 'sex'",
        "blue node labeled 'edu'",
        "green nodes labeled 'age' and 'edu'",
        "black nodes including 'citizenship', 'prev_residence', 'occupation', 'economic_state', 'marital', 'household_position', 'househol_dsize', 'cur_eco_activity'",
        "directed edges connecting nodes",
        "node labeled 'country_of_birth' at top left"
      ],
      "queries": [
        {
          "query": "Why is sex marked as the red node when the graph represents the Dutch Census causal structure?",
          "answer": "Sex is the protected attribute in this discrimination discovery analysis of the Dutch Census dataset, requiring special identification in the causal framework.",
          "visual_anchor": "red node labeled 'sex' in top right area",
          "text_evidence": "the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the green node age relate to the blue decision node edu in discovering conditional discrimination patterns?",
          "answer": "Age is part of set Q, which helps partition the population to detect conditional discrimination in educational outcomes, as the authors extended prior conditional discrimination approaches.",
          "visual_anchor": "green node 'age' with edge pointing to blue node 'edu'",
          "text_evidence": "Zliobaitˇ e et˙ al. [28] proposed conditional discrimination. However, their approaches cannot determine whether a partition is meaning",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What discrimination detection methods do the black nodes like citizenship and prev_residence support beyond classification rule-based approaches?",
          "answer": "These nodes represent additional variables beyond the protected attribute, decision, and set Q, enabling more comprehensive discrimination pattern discovery than simple elift or belift rule-based methods.",
          "visual_anchor": "black nodes 'citizenship' and 'prev_residence' in middle section",
          "text_evidence": "Classification rule-based methods such as elift [23] and belift [20] were proposed to represent certain discrimination patterns.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why is sex marked as the red node when the graph represents the Dutch Census causal structure?",
        "answer": "Sex is the protected attribute in this discrimination discovery analysis of the Dutch Census dataset, requiring special identification in the causal framework.",
        "visual_anchor": "red node labeled 'sex' in top right area",
        "text_evidence": "the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the green node age relate to the blue decision node edu in discovering conditional discrimination patterns?",
        "answer": "Age is part of set Q, which helps partition the population to detect conditional discrimination in educational outcomes, as the authors extended prior conditional discrimination approaches.",
        "visual_anchor": "green node 'age' with edge pointing to blue node 'edu'",
        "text_evidence": "Zliobaitˇ e et˙ al. [28] proposed conditional discrimination. However, their approaches cannot determine whether a partition is meaning",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What discrimination detection methods do the black nodes like citizenship and prev_residence support beyond classification rule-based approaches?",
        "answer": "These nodes represent additional variables beyond the protected attribute, decision, and set Q, enabling more comprehensive discrimination pattern discovery than simple elift or belift rule-based methods.",
        "visual_anchor": "black nodes 'citizenship' and 'prev_residence' in middle section",
        "text_evidence": "Classification rule-based methods such as elift [23] and belift [20] were proposed to represent certain discrimination patterns.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "red node labeled 'sex'",
      "blue node labeled 'edu'",
      "green nodes labeled 'age' and 'edu'",
      "black nodes including 'citizenship', 'prev_residence', 'occupation', 'economic_state', 'marital', 'household_position', 'househol_dsize', 'cur_eco_activity'",
      "directed edges connecting nodes",
      "node labeled 'country_of_birth' at top left"
    ]
  },
  {
    "doc_id": "1611.07509",
    "figure_id": "1611.07509_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg",
    "caption": "Figure 1: The toy model.",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Race node with arrows to Zip_code, Income, and Loan\", \"Zip_code node with arrow to Loan\", \"Income node with arrow to Loan\", \"Bidirectional arrow between Zip_code and Income\", \"Directional arrows showing causal relationships\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the direct arrow from Race to Loan represent the path-specific effect modeling direct discrimination in historical data?\",\n      \"answer\": \"Yes, the direct arrow from Race to Loan models direct discrimination as a path-specific effect, which the authors use to distinguish and discover discriminatory patterns before building classifiers.\",\n      \"visual_anchor\": \"direct arrow from Race node to Loan node\",\n      \"text_evidence\": \"Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does Race connect to Loan through Zip_code when analyzing the expected change treating disadvantaged applicants as advantaged?\",\n      \"answer\": \"The Race-to-Zip_code-to-Loan path captures indirect discrimination, representing the expected change in loan approval when the disadvantaged group had the same racial makeups shown in Zip code as the advantaged group.\",\n      \"visual_anchor\": \"path from Race through Zip_code to Loan\",\n      \"text_evidence\": \"When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the arrow from Race directly to Loan capture treating black applicants as white in loan approval expectations?\",\n      \"answer\": \"Yes, the direct Race-to-Loan arrow models the expected change in loan approval when applications from the disadvantaged group (e.g., black) are instructed to be treated as from the advantaged group (e.g., white).\",\n      \"visual_anchor\": \"single arrow connecting Race directly to Loan\",\n      \"text_evidence\": \"When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Race node with arrows to Zip_code, Income, and Loan",
        "Zip_code node with arrow to Loan",
        "Income node with arrow to Loan",
        "Bidirectional arrow between Zip_code and Income",
        "Directional arrows showing causal relationships"
      ],
      "queries": [
        {
          "query": "Does the direct arrow from Race to Loan represent the path-specific effect modeling direct discrimination in historical data?",
          "answer": "Yes, the direct arrow from Race to Loan models direct discrimination as a path-specific effect, which the authors use to distinguish and discover discriminatory patterns before building classifiers.",
          "visual_anchor": "direct arrow from Race node to Loan node",
          "text_evidence": "Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does Race connect to Loan through Zip_code when analyzing the expected change treating disadvantaged applicants as advantaged?",
          "answer": "The Race-to-Zip_code-to-Loan path captures indirect discrimination, representing the expected change in loan approval when the disadvantaged group had the same racial makeups shown in Zip code as the advantaged group.",
          "visual_anchor": "path from Race through Zip_code to Loan",
          "text_evidence": "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the arrow from Race directly to Loan capture treating black applicants as white in loan approval expectations?",
          "answer": "Yes, the direct Race-to-Loan arrow models the expected change in loan approval when applications from the disadvantaged group (e.g., black) are instructed to be treated as from the advantaged group (e.g., white).",
          "visual_anchor": "single arrow connecting Race directly to Loan",
          "text_evidence": "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the direct arrow from Race to Loan represent the path-specific effect modeling direct discrimination in historical data?",
        "answer": "Yes, the direct arrow from Race to Loan models direct discrimination as a path-specific effect, which the authors use to distinguish and discover discriminatory patterns before building classifiers.",
        "visual_anchor": "direct arrow from Race node to Loan node",
        "text_evidence": "Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does Race connect to Loan through Zip_code when analyzing the expected change treating disadvantaged applicants as advantaged?",
        "answer": "The Race-to-Zip_code-to-Loan path captures indirect discrimination, representing the expected change in loan approval when the disadvantaged group had the same racial makeups shown in Zip code as the advantaged group.",
        "visual_anchor": "path from Race through Zip_code to Loan",
        "text_evidence": "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the arrow from Race directly to Loan capture treating black applicants as white in loan approval expectations?",
        "answer": "Yes, the direct Race-to-Loan arrow models the expected change in loan approval when applications from the disadvantaged group (e.g., black) are instructed to be treated as from the advantaged group (e.g., white).",
        "visual_anchor": "single arrow connecting Race directly to Loan",
        "text_evidence": "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Race node with arrows to Zip_code, Income, and Loan",
      "Zip_code node with arrow to Loan",
      "Income node with arrow to Loan",
      "Bidirectional arrow between Zip_code and Income",
      "Directional arrows showing causal relationships"
    ]
  },
  {
    "doc_id": "1611.07509",
    "figure_id": "1611.07509_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg",
    "caption": "Figure 2: An example with the recanting witness criterion satisfied.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node X with arrow to Z1\", \"Node Z1 with arrow to Z2\", \"Node Z2 with arrow to Y\", \"Node Z1 with direct arrow to Y\", \"Node Z2 with arrow to Y\", \"Five directed edges total\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the direct arrow from X to Z1 represent the intervention needed to compute the post-intervention distribution formula shown?\",\n      \"answer\": \"Yes, the arrow from X to Z1 corresponds to the intervention on attribute X in the post-intervention distribution formula P(Y|do(X=x)), which marginalizes over all other attributes.\",\n      \"visual_anchor\": \"Direct arrow from node X to node Z1\",\n      \"text_evidence\": \"Specifically, the post-intervention distribution of a single attribute Y given an intervention on a single attribute X is given by\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the path X→Z1→Y allow total causal effect calculation from X to Y as defined by do-calculus?\",\n      \"answer\": \"The path X→Z1→Y represents one causal pathway through which X affects Y. The total causal effect sums over all such pathways, including the indirect path through Z2, as computed using do-calculus.\",\n      \"visual_anchor\": \"Path from X through Z1 directly to Y\",\n      \"text_evidence\": \"By using the do-calculus, the total causal effect of X on Y is defined as fo\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the mediator Z2 receiving input from Z1 relate to attributes that cannot be objectively justified in decisions?\",\n      \"answer\": \"Z2 acts as a mediator between Z1 and Y, potentially representing non-protected attributes that should not be used in decision-making. The recanting witness criterion helps identify such unjustifiable pathways in discrimination analysis.\",\n      \"visual_anchor\": \"Arrow from Z1 to Z2 and arrow from Z2 to Y\",\n      \"text_evidence\": \"Among the non-protected attributes, assume there is a set of attributes that cannot be objectively justified if used in the decision making process, whi\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node X with arrow to Z1",
        "Node Z1 with arrow to Z2",
        "Node Z2 with arrow to Y",
        "Node Z1 with direct arrow to Y",
        "Node Z2 with arrow to Y",
        "Five directed edges total"
      ],
      "queries": [
        {
          "query": "Does the direct arrow from X to Z1 represent the intervention needed to compute the post-intervention distribution formula shown?",
          "answer": "Yes, the arrow from X to Z1 corresponds to the intervention on attribute X in the post-intervention distribution formula P(Y|do(X=x)), which marginalizes over all other attributes.",
          "visual_anchor": "Direct arrow from node X to node Z1",
          "text_evidence": "Specifically, the post-intervention distribution of a single attribute Y given an intervention on a single attribute X is given by",
          "query_type": "value_context"
        },
        {
          "query": "Why does the path X→Z1→Y allow total causal effect calculation from X to Y as defined by do-calculus?",
          "answer": "The path X→Z1→Y represents one causal pathway through which X affects Y. The total causal effect sums over all such pathways, including the indirect path through Z2, as computed using do-calculus.",
          "visual_anchor": "Path from X through Z1 directly to Y",
          "text_evidence": "By using the do-calculus, the total causal effect of X on Y is defined as fo",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the mediator Z2 receiving input from Z1 relate to attributes that cannot be objectively justified in decisions?",
          "answer": "Z2 acts as a mediator between Z1 and Y, potentially representing non-protected attributes that should not be used in decision-making. The recanting witness criterion helps identify such unjustifiable pathways in discrimination analysis.",
          "visual_anchor": "Arrow from Z1 to Z2 and arrow from Z2 to Y",
          "text_evidence": "Among the non-protected attributes, assume there is a set of attributes that cannot be objectively justified if used in the decision making process, whi",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the direct arrow from X to Z1 represent the intervention needed to compute the post-intervention distribution formula shown?",
        "answer": "Yes, the arrow from X to Z1 corresponds to the intervention on attribute X in the post-intervention distribution formula P(Y|do(X=x)), which marginalizes over all other attributes.",
        "visual_anchor": "Direct arrow from node X to node Z1",
        "text_evidence": "Specifically, the post-intervention distribution of a single attribute Y given an intervention on a single attribute X is given by",
        "query_type": "value_context"
      },
      {
        "query": "Why does the path X→Z1→Y allow total causal effect calculation from X to Y as defined by do-calculus?",
        "answer": "The path X→Z1→Y represents one causal pathway through which X affects Y. The total causal effect sums over all such pathways, including the indirect path through Z2, as computed using do-calculus.",
        "visual_anchor": "Path from X through Z1 directly to Y",
        "text_evidence": "By using the do-calculus, the total causal effect of X on Y is defined as fo",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the mediator Z2 receiving input from Z1 relate to attributes that cannot be objectively justified in decisions?",
        "answer": "Z2 acts as a mediator between Z1 and Y, potentially representing non-protected attributes that should not be used in decision-making. The recanting witness criterion helps identify such unjustifiable pathways in discrimination analysis.",
        "visual_anchor": "Arrow from Z1 to Z2 and arrow from Z2 to Y",
        "text_evidence": "Among the non-protected attributes, assume there is a set of attributes that cannot be objectively justified if used in the decision making process, whi",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node X with arrow to Z1",
      "Node Z1 with arrow to Z2",
      "Node Z2 with arrow to Y",
      "Node Z1 with direct arrow to Y",
      "Node Z2 with arrow to Y",
      "Five directed edges total"
    ]
  },
  {
    "doc_id": "1611.07509",
    "figure_id": "1611.07509_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg",
    "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"green path from sex to income\", \"blue paths through marital_status node\", \"nodes: race, age, sex, marital_status, edu_level, occupation, hours_per_week, workclass, relationship, income, native_country\", \"directed edges showing causal relationships\"],\n  \"queries\": [\n    {\n      \"query\": \"Why is the green path from sex to income considered direct when the Adult dataset contains 11 attributes?\",\n      \"answer\": \"The green path represents the direct causal effect from sex to income without mediating variables. The Adult dataset's 11 attributes were analyzed to construct this causal network, identifying sex as having both direct (green) and indirect (blue through marital_status) effects on income.\",\n      \"visual_anchor\": \"green path connecting sex node directly to income node\",\n      \"text_evidence\": \"The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the blue paths through marital_status differ from the green direct path given the binarization preprocessing?\",\n      \"answer\": \"The blue paths represent indirect causal effects where sex influences income through the mediating variable marital_status, while the green path shows direct influence. Binarization reduced domain sizes to two classes for computational feasibility, enabling the PC algorithm to discover both path types.\",\n      \"visual_anchor\": \"blue paths from sex through marital_status node versus green direct path to income\",\n      \"text_evidence\": \"we binarize each attribute's domain values into two classes to reduce the domain sizes\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does marital_status appear as a central node with multiple blue incoming edges despite temporal priority considerations?\",\n      \"answer\": \"Marital_status receives blue edges from age and sex because these attributes have temporal priority in the three-tier ordering. Sex and age are in the first tier and naturally precede and causally influence marital status, which then mediates their effects on income through indirect blue paths.\",\n      \"visual_anchor\": \"marital_status node with multiple blue incoming edges from age and sex\",\n      \"text_evidence\": \"We use three tiers in the partial order for temporal priority: sex, age, native\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "green path from sex to income",
        "blue paths through marital_status node",
        "nodes: race, age, sex, marital_status, edu_level, occupation, hours_per_week, workclass, relationship, income, native_country",
        "directed edges showing causal relationships"
      ],
      "queries": [
        {
          "query": "Why is the green path from sex to income considered direct when the Adult dataset contains 11 attributes?",
          "answer": "The green path represents the direct causal effect from sex to income without mediating variables. The Adult dataset's 11 attributes were analyzed to construct this causal network, identifying sex as having both direct (green) and indirect (blue through marital_status) effects on income.",
          "visual_anchor": "green path connecting sex node directly to income node",
          "text_evidence": "The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc.",
          "query_type": "value_context"
        },
        {
          "query": "How do the blue paths through marital_status differ from the green direct path given the binarization preprocessing?",
          "answer": "The blue paths represent indirect causal effects where sex influences income through the mediating variable marital_status, while the green path shows direct influence. Binarization reduced domain sizes to two classes for computational feasibility, enabling the PC algorithm to discover both path types.",
          "visual_anchor": "blue paths from sex through marital_status node versus green direct path to income",
          "text_evidence": "we binarize each attribute's domain values into two classes to reduce the domain sizes",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does marital_status appear as a central node with multiple blue incoming edges despite temporal priority considerations?",
          "answer": "Marital_status receives blue edges from age and sex because these attributes have temporal priority in the three-tier ordering. Sex and age are in the first tier and naturally precede and causally influence marital status, which then mediates their effects on income through indirect blue paths.",
          "visual_anchor": "marital_status node with multiple blue incoming edges from age and sex",
          "text_evidence": "We use three tiers in the partial order for temporal priority: sex, age, native",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why is the green path from sex to income considered direct when the Adult dataset contains 11 attributes?",
        "answer": "The green path represents the direct causal effect from sex to income without mediating variables. The Adult dataset's 11 attributes were analyzed to construct this causal network, identifying sex as having both direct (green) and indirect (blue through marital_status) effects on income.",
        "visual_anchor": "green path connecting sex node directly to income node",
        "text_evidence": "The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc.",
        "query_type": "value_context"
      },
      {
        "query": "How do the blue paths through marital_status differ from the green direct path given the binarization preprocessing?",
        "answer": "The blue paths represent indirect causal effects where sex influences income through the mediating variable marital_status, while the green path shows direct influence. Binarization reduced domain sizes to two classes for computational feasibility, enabling the PC algorithm to discover both path types.",
        "visual_anchor": "blue paths from sex through marital_status node versus green direct path to income",
        "text_evidence": "we binarize each attribute's domain values into two classes to reduce the domain sizes",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does marital_status appear as a central node with multiple blue incoming edges despite temporal priority considerations?",
        "answer": "Marital_status receives blue edges from age and sex because these attributes have temporal priority in the three-tier ordering. Sex and age are in the first tier and naturally precede and causally influence marital status, which then mediates their effects on income through indirect blue paths.",
        "visual_anchor": "marital_status node with multiple blue incoming edges from age and sex",
        "text_evidence": "We use three tiers in the partial order for temporal priority: sex, age, native",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "green path from sex to income",
      "blue paths through marital_status node",
      "nodes: race, age, sex, marital_status, edu_level, occupation, hours_per_week, workclass, relationship, income, native_country",
      "directed edges showing causal relationships"
    ]
  },
  {
    "doc_id": "1611.07509",
    "figure_id": "1611.07509_fig_5",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg",
    "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"green path from sex to occupation\", \"blue paths through marital_status\", \"nodes: sex, age, country_birth, marital_status, edu_level, occupation, economic_status, household_position, household_size, cur_eco_activity, prev_residence_place, citizenship\", \"directed edges connecting various nodes\"],\n  \"queries\": [\n    {\n      \"query\": \"Why is the green path from sex to occupation classified as direct rather than indirect in the network?\",\n      \"answer\": \"The green path represents the direct path from sex to occupation, bypassing mediating variables like marital status, which defines indirect paths shown in blue.\",\n      \"visual_anchor\": \"green path connecting sex node to occupation node\",\n      \"text_evidence\": \"the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the blue paths through marital_status align with temporal priority placing sex in the first tier?\",\n      \"answer\": \"Yes, sex is in the first tier and influences marital_status, which then affects other variables, consistent with the temporal ordering where sex, age, native country, and race are defined first.\",\n      \"visual_anchor\": \"blue paths passing through marital_status node\",\n      \"text_evidence\": \"We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the firs\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the causal network structure support elift's discrimination pattern representation in the Dutch dataset?\",\n      \"answer\": \"The network shows direct and indirect paths from protected attributes like sex and age to outcomes, enabling rule-based methods like elift to identify discrimination patterns through these causal relationships.\",\n      \"visual_anchor\": \"directed edges from sex and age nodes to multiple downstream nodes\",\n      \"text_evidence\": \"Classification rule-based methods such as elift (Pedreshi, Ruggieri, and Turini 2008) and belift (Mancuhan and Clifton 2014) were proposed to represent certain discrimination patterns\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "green path from sex to occupation",
        "blue paths through marital_status",
        "nodes: sex, age, country_birth, marital_status, edu_level, occupation, economic_status, household_position, household_size, cur_eco_activity, prev_residence_place, citizenship",
        "directed edges connecting various nodes"
      ],
      "queries": [
        {
          "query": "Why is the green path from sex to occupation classified as direct rather than indirect in the network?",
          "answer": "The green path represents the direct path from sex to occupation, bypassing mediating variables like marital status, which defines indirect paths shown in blue.",
          "visual_anchor": "green path connecting sex node to occupation node",
          "text_evidence": "the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the blue paths through marital_status align with temporal priority placing sex in the first tier?",
          "answer": "Yes, sex is in the first tier and influences marital_status, which then affects other variables, consistent with the temporal ordering where sex, age, native country, and race are defined first.",
          "visual_anchor": "blue paths passing through marital_status node",
          "text_evidence": "We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the firs",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the causal network structure support elift's discrimination pattern representation in the Dutch dataset?",
          "answer": "The network shows direct and indirect paths from protected attributes like sex and age to outcomes, enabling rule-based methods like elift to identify discrimination patterns through these causal relationships.",
          "visual_anchor": "directed edges from sex and age nodes to multiple downstream nodes",
          "text_evidence": "Classification rule-based methods such as elift (Pedreshi, Ruggieri, and Turini 2008) and belift (Mancuhan and Clifton 2014) were proposed to represent certain discrimination patterns",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why is the green path from sex to occupation classified as direct rather than indirect in the network?",
        "answer": "The green path represents the direct path from sex to occupation, bypassing mediating variables like marital status, which defines indirect paths shown in blue.",
        "visual_anchor": "green path connecting sex node to occupation node",
        "text_evidence": "the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the blue paths through marital_status align with temporal priority placing sex in the first tier?",
        "answer": "Yes, sex is in the first tier and influences marital_status, which then affects other variables, consistent with the temporal ordering where sex, age, native country, and race are defined first.",
        "visual_anchor": "blue paths passing through marital_status node",
        "text_evidence": "We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the firs",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the causal network structure support elift's discrimination pattern representation in the Dutch dataset?",
        "answer": "The network shows direct and indirect paths from protected attributes like sex and age to outcomes, enabling rule-based methods like elift to identify discrimination patterns through these causal relationships.",
        "visual_anchor": "directed edges from sex and age nodes to multiple downstream nodes",
        "text_evidence": "Classification rule-based methods such as elift (Pedreshi, Ruggieri, and Turini 2008) and belift (Mancuhan and Clifton 2014) were proposed to represent certain discrimination patterns",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "green path from sex to occupation",
      "blue paths through marital_status",
      "nodes: sex, age, country_birth, marital_status, edu_level, occupation, economic_status, household_position, household_size, cur_eco_activity, prev_residence_place, citizenship",
      "directed edges connecting various nodes"
    ]
  },
  {
    "doc_id": "1701.08230",
    "figure_id": "1701.08230_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
    "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Two probability distribution curves (red and blue) in top-right simulated panel\",\n    \"Vertical dashed line at 25% in simulated plot\",\n    \"X-axis labeled 'Probability of reoffending' from 0% to 100%\",\n    \"Y-axis showing probability density\",\n    \"Blue curve peaks earlier and higher than red curve\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?\",\n      \"answer\": \"No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.\",\n      \"visual_anchor\": \"Blue and red curves with vertical dashed line at 25% in top-right simulated panel\",\n      \"text_evidence\": \"simulated data drawn from two beta distributions with equal means (right)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does detaining 30% of defendants using a single threshold violate statistical parity despite the distributions shown?\",\n      \"answer\": \"Because the different shapes of the red and blue distributions mean a single threshold at any point will capture different proportions of each group, creating disparate detention rates between groups.\",\n      \"visual_anchor\": \"Different shapes of red and blue probability curves in simulated panel\",\n      \"text_evidence\": \"using a single threshold which detains $3 0 \\\\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Can the vertical dashed line at 25% serve as a threshold to evaluate average risk above thresholds for fairness?\",\n      \"answer\": \"No, detention rates and false positive rates are poor proxies for thresholds because these infra-marginal statistics consider average risk above the thresholds, and can differ even with identical thresholds as the distributions show.\",\n      \"visual_anchor\": \"Vertical dashed line at 25% probability in simulated plot\",\n      \"text_evidence\": \"detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Two probability distribution curves (red and blue) in top-right simulated panel",
        "Vertical dashed line at 25% in simulated plot",
        "X-axis labeled 'Probability of reoffending' from 0% to 100%",
        "Y-axis showing probability density",
        "Blue curve peaks earlier and higher than red curve"
      ],
      "queries": [
        {
          "query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?",
          "answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.",
          "visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel",
          "text_evidence": "simulated data drawn from two beta distributions with equal means (right)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does detaining 30% of defendants using a single threshold violate statistical parity despite the distributions shown?",
          "answer": "Because the different shapes of the red and blue distributions mean a single threshold at any point will capture different proportions of each group, creating disparate detention rates between groups.",
          "visual_anchor": "Different shapes of red and blue probability curves in simulated panel",
          "text_evidence": "using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Can the vertical dashed line at 25% serve as a threshold to evaluate average risk above thresholds for fairness?",
          "answer": "No, detention rates and false positive rates are poor proxies for thresholds because these infra-marginal statistics consider average risk above the thresholds, and can differ even with identical thresholds as the distributions show.",
          "visual_anchor": "Vertical dashed line at 25% probability in simulated plot",
          "text_evidence": "detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the two distributions drawn from beta distributions with equal means in the right panel share the same peak location?",
        "answer": "No, the blue curve peaks before 25% (near the dashed line) while the red curve peaks slightly after 25%, despite both distributions having equal means as stated.",
        "visual_anchor": "Blue and red curves with vertical dashed line at 25% in top-right simulated panel",
        "text_evidence": "simulated data drawn from two beta distributions with equal means (right)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does detaining 30% of defendants using a single threshold violate statistical parity despite the distributions shown?",
        "answer": "Because the different shapes of the red and blue distributions mean a single threshold at any point will capture different proportions of each group, creating disparate detention rates between groups.",
        "visual_anchor": "Different shapes of red and blue probability curves in simulated panel",
        "text_evidence": "using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Can the vertical dashed line at 25% serve as a threshold to evaluate average risk above thresholds for fairness?",
        "answer": "No, detention rates and false positive rates are poor proxies for thresholds because these infra-marginal statistics consider average risk above the thresholds, and can differ even with identical thresholds as the distributions show.",
        "visual_anchor": "Vertical dashed line at 25% probability in simulated plot",
        "text_evidence": "detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Two probability distribution curves (red and blue) in top-right simulated panel",
      "Vertical dashed line at 25% in simulated plot",
      "X-axis labeled 'Probability of reoffending' from 0% to 100%",
      "Y-axis showing probability density",
      "Blue curve peaks earlier and higher than red curve"
    ]
  },
  {
    "doc_id": "1701.08230",
    "figure_id": "1701.08230_fig_5",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg",
    "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red line representing Black defendants\",\n    \"Blue line representing White defendants\",\n    \"X-axis showing risk scores from 1 to 10\",\n    \"Y-axis showing likelihood of recidivism from 0% to 100%\",\n    \"Gray confidence interval bands around both lines\",\n    \"Both lines converging around 70-75% at higher risk scores\",\n    \"Both lines starting around 20-25% at risk score 1\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the red and blue lines track closely together across all risk scores if COMPAS scores are calibrated?\",\n      \"answer\": \"Calibrated scores mean that defendants of different races with the same risk score have equal recidivism rates. The closely tracking red (Black) and blue (White) lines demonstrate this calibration property across all risk levels.\",\n      \"visual_anchor\": \"Red and blue lines tracking closely together from risk score 1 to 10\",\n      \"text_evidence\": \"White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the overlapping gray confidence bands around both curves at risk score 8 support claims about statistical certainty of calibration?\",\n      \"answer\": \"Yes, the overlapping 95% confidence intervals at risk score 8 indicate no statistically significant difference in recidivism rates between Black and White defendants at that risk level, supporting the calibration claim.\",\n      \"visual_anchor\": \"Gray confidence interval bands overlapping around risk score 8 where both lines reach approximately 75%\",\n      \"text_evidence\": \"the gray bands show $9 5 \\\\%$ condence intervals\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the convergence of both lines near 70-75% at high risk scores align with any-crime rearrest definitions?\",\n      \"answer\": \"Yes, the high recidivism rates (70-75%) at elevated risk scores reflect the broad definition used, which includes re-arrests for any crime, not just violent offenses, capturing a wider range of recidivism events.\",\n      \"visual_anchor\": \"Both red and blue lines converging at approximately 70-75% likelihood between risk scores 8-10\",\n      \"text_evidence\": \"the $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red line representing Black defendants",
        "Blue line representing White defendants",
        "X-axis showing risk scores from 1 to 10",
        "Y-axis showing likelihood of recidivism from 0% to 100%",
        "Gray confidence interval bands around both lines",
        "Both lines converging around 70-75% at higher risk scores",
        "Both lines starting around 20-25% at risk score 1"
      ],
      "queries": [
        {
          "query": "Why do the red and blue lines track closely together across all risk scores if COMPAS scores are calibrated?",
          "answer": "Calibrated scores mean that defendants of different races with the same risk score have equal recidivism rates. The closely tracking red (Black) and blue (White) lines demonstrate this calibration property across all risk levels.",
          "visual_anchor": "Red and blue lines tracking closely together from risk score 1 to 10",
          "text_evidence": "White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the overlapping gray confidence bands around both curves at risk score 8 support claims about statistical certainty of calibration?",
          "answer": "Yes, the overlapping 95% confidence intervals at risk score 8 indicate no statistically significant difference in recidivism rates between Black and White defendants at that risk level, supporting the calibration claim.",
          "visual_anchor": "Gray confidence interval bands overlapping around risk score 8 where both lines reach approximately 75%",
          "text_evidence": "the gray bands show $9 5 \\%$ condence intervals",
          "query_type": "value_context"
        },
        {
          "query": "Does the convergence of both lines near 70-75% at high risk scores align with any-crime rearrest definitions?",
          "answer": "Yes, the high recidivism rates (70-75%) at elevated risk scores reflect the broad definition used, which includes re-arrests for any crime, not just violent offenses, capturing a wider range of recidivism events.",
          "visual_anchor": "Both red and blue lines converging at approximately 70-75% likelihood between risk scores 8-10",
          "text_evidence": "the $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the red and blue lines track closely together across all risk scores if COMPAS scores are calibrated?",
        "answer": "Calibrated scores mean that defendants of different races with the same risk score have equal recidivism rates. The closely tracking red (Black) and blue (White) lines demonstrate this calibration property across all risk levels.",
        "visual_anchor": "Red and blue lines tracking closely together from risk score 1 to 10",
        "text_evidence": "White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the overlapping gray confidence bands around both curves at risk score 8 support claims about statistical certainty of calibration?",
        "answer": "Yes, the overlapping 95% confidence intervals at risk score 8 indicate no statistically significant difference in recidivism rates between Black and White defendants at that risk level, supporting the calibration claim.",
        "visual_anchor": "Gray confidence interval bands overlapping around risk score 8 where both lines reach approximately 75%",
        "text_evidence": "the gray bands show $9 5 \\%$ condence intervals",
        "query_type": "value_context"
      },
      {
        "query": "Does the convergence of both lines near 70-75% at high risk scores align with any-crime rearrest definitions?",
        "answer": "Yes, the high recidivism rates (70-75%) at elevated risk scores reflect the broad definition used, which includes re-arrests for any crime, not just violent offenses, capturing a wider range of recidivism events.",
        "visual_anchor": "Both red and blue lines converging at approximately 70-75% likelihood between risk scores 8-10",
        "text_evidence": "the $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red line representing Black defendants",
      "Blue line representing White defendants",
      "X-axis showing risk scores from 1 to 10",
      "Y-axis showing likelihood of recidivism from 0% to 100%",
      "Gray confidence interval bands around both lines",
      "Both lines converging around 70-75% at higher risk scores",
      "Both lines starting around 20-25% at risk score 1"
    ]
  },
  {
    "doc_id": "1701.08230",
    "figure_id": "1701.08230_fig_6",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg",
    "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"black line showing distribution of risk\", \"red line showing strategically altered risk estimates\", \"dashed vertical line indicating detention threshold at approximately 25%\", \"x-axis labeled 'Risk score' from 0% to 100%\", \"left plot showing probability distributions\", \"right plot showing calibration\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red distribution peak left of the dashed detention threshold despite algorithms outputting scores claimed to indicate defendant risk?\",\n      \"answer\": \"The red line shows strategically altered risk scores that are less informative than true risk (black line), ensuring no defendants fall above the detention threshold while maintaining calibration, demonstrating how scores can be discriminatory even when calibrated.\",\n      \"visual_anchor\": \"red distribution peak positioned left of the dashed vertical detention threshold line\",\n      \"text_evidence\": \"In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant's risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the red line demonstrate the concern that undisclosed score procedures might be discriminatory?\",\n      \"answer\": \"The red line represents strategically altered risk estimates that guarantee no defendants exceed the detention threshold, illustrating how opaque scoring procedures can produce discriminatory outcomes while appearing calibrated.\",\n      \"visual_anchor\": \"red line showing altered distribution with no mass beyond the dashed vertical threshold\",\n      \"text_evidence\": \"In some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"What relationship between the black and red distributions illustrates the general method for constructing discriminatory scores from true risk?\",\n      \"answer\": \"The black line shows true risk distribution while the red line shows altered scores that shift the distribution leftward, demonstrating how true risk estimates can be manipulated to produce discriminatory yet calibrated scores.\",\n      \"visual_anchor\": \"comparison between black distribution centered around 25% and red distribution peaked left of detention threshold\",\n      \"text_evidence\": \"Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "black line showing distribution of risk",
        "red line showing strategically altered risk estimates",
        "dashed vertical line indicating detention threshold at approximately 25%",
        "x-axis labeled 'Risk score' from 0% to 100%",
        "left plot showing probability distributions",
        "right plot showing calibration"
      ],
      "queries": [
        {
          "query": "Why does the red distribution peak left of the dashed detention threshold despite algorithms outputting scores claimed to indicate defendant risk?",
          "answer": "The red line shows strategically altered risk scores that are less informative than true risk (black line), ensuring no defendants fall above the detention threshold while maintaining calibration, demonstrating how scores can be discriminatory even when calibrated.",
          "visual_anchor": "red distribution peak positioned left of the dashed vertical detention threshold line",
          "text_evidence": "In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant's risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the red line demonstrate the concern that undisclosed score procedures might be discriminatory?",
          "answer": "The red line represents strategically altered risk estimates that guarantee no defendants exceed the detention threshold, illustrating how opaque scoring procedures can produce discriminatory outcomes while appearing calibrated.",
          "visual_anchor": "red line showing altered distribution with no mass beyond the dashed vertical threshold",
          "text_evidence": "In some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory.",
          "query_type": "visual_definition"
        },
        {
          "query": "What relationship between the black and red distributions illustrates the general method for constructing discriminatory scores from true risk?",
          "answer": "The black line shows true risk distribution while the red line shows altered scores that shift the distribution leftward, demonstrating how true risk estimates can be manipulated to produce discriminatory yet calibrated scores.",
          "visual_anchor": "comparison between black distribution centered around 25% and red distribution peaked left of detention threshold",
          "text_evidence": "Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red distribution peak left of the dashed detention threshold despite algorithms outputting scores claimed to indicate defendant risk?",
        "answer": "The red line shows strategically altered risk scores that are less informative than true risk (black line), ensuring no defendants fall above the detention threshold while maintaining calibration, demonstrating how scores can be discriminatory even when calibrated.",
        "visual_anchor": "red distribution peak positioned left of the dashed vertical detention threshold line",
        "text_evidence": "In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant's risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the red line demonstrate the concern that undisclosed score procedures might be discriminatory?",
        "answer": "The red line represents strategically altered risk estimates that guarantee no defendants exceed the detention threshold, illustrating how opaque scoring procedures can produce discriminatory outcomes while appearing calibrated.",
        "visual_anchor": "red line showing altered distribution with no mass beyond the dashed vertical threshold",
        "text_evidence": "In some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory.",
        "query_type": "visual_definition"
      },
      {
        "query": "What relationship between the black and red distributions illustrates the general method for constructing discriminatory scores from true risk?",
        "answer": "The black line shows true risk distribution while the red line shows altered scores that shift the distribution leftward, demonstrating how true risk estimates can be manipulated to produce discriminatory yet calibrated scores.",
        "visual_anchor": "comparison between black distribution centered around 25% and red distribution peaked left of detention threshold",
        "text_evidence": "Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "black line showing distribution of risk",
      "red line showing strategically altered risk estimates",
      "dashed vertical line indicating detention threshold at approximately 25%",
      "x-axis labeled 'Risk score' from 0% to 100%",
      "left plot showing probability distributions",
      "right plot showing calibration"
    ]
  },
  {
    "doc_id": "1703.06856",
    "figure_id": "1703.06856_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg",
    "caption": "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A (protected attribute)\", \"Node X\", \"Node Y (outcome)\", \"Node U\", \"Directed edges between nodes\", \"Gray filled nodes X and Y\", \"White filled nodes A and U\"],\n  \"queries\": [\n    {\n      \"query\": \"How does node A in the DAG relate to the protected attribute in the insurance pricing scenario?\",\n      \"answer\": \"Node A represents the protected attribute (such as gender or race) in the causal model. In the insurance pricing scenario, the company must predict accident rate Y while considering fairness with respect to this protected attribute A.\",\n      \"visual_anchor\": \"Node A (white circle, top-left position) with directed edge to node X\",\n      \"text_evidence\": \"A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does node U connect to both X and Y while node A only connects to X?\",\n      \"answer\": \"Node U represents independent background variables that can directly affect both intermediate variables and outcomes. Node A (protected attribute) influences Y only through X, reflecting the causal pathway where the protected attribute affects outcomes through intermediate factors rather than directly.\",\n      \"visual_anchor\": \"Node U (white circle, top-right) with edges to both X and Y, versus node A with single edge to X\",\n      \"text_evidence\": \"Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What real-world scenarios do the two different causal graph structures in parts a and b represent?\",\n      \"answer\": \"The two causal graphs correspond to insurance pricing and crime prediction scenarios. These different structures reflect how protected attributes and background variables causally influence outcomes differently across real-world fair prediction problems.\",\n      \"visual_anchor\": \"Two separate graph structures labeled (a) and (b) showing different node arrangements\",\n      \"text_evidence\": \"To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b).\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A (protected attribute)",
        "Node X",
        "Node Y (outcome)",
        "Node U",
        "Directed edges between nodes",
        "Gray filled nodes X and Y",
        "White filled nodes A and U"
      ],
      "queries": [
        {
          "query": "How does node A in the DAG relate to the protected attribute in the insurance pricing scenario?",
          "answer": "Node A represents the protected attribute (such as gender or race) in the causal model. In the insurance pricing scenario, the company must predict accident rate Y while considering fairness with respect to this protected attribute A.",
          "visual_anchor": "Node A (white circle, top-left position) with directed edge to node X",
          "text_evidence": "A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$.",
          "query_type": "value_context"
        },
        {
          "query": "Why does node U connect to both X and Y while node A only connects to X?",
          "answer": "Node U represents independent background variables that can directly affect both intermediate variables and outcomes. Node A (protected attribute) influences Y only through X, reflecting the causal pathway where the protected attribute affects outcomes through intermediate factors rather than directly.",
          "visual_anchor": "Node U (white circle, top-right) with edges to both X and Y, versus node A with single edge to X",
          "text_evidence": "Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What real-world scenarios do the two different causal graph structures in parts a and b represent?",
          "answer": "The two causal graphs correspond to insurance pricing and crime prediction scenarios. These different structures reflect how protected attributes and background variables causally influence outcomes differently across real-world fair prediction problems.",
          "visual_anchor": "Two separate graph structures labeled (a) and (b) showing different node arrangements",
          "text_evidence": "To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b).",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "How does node A in the DAG relate to the protected attribute in the insurance pricing scenario?",
        "answer": "Node A represents the protected attribute (such as gender or race) in the causal model. In the insurance pricing scenario, the company must predict accident rate Y while considering fairness with respect to this protected attribute A.",
        "visual_anchor": "Node A (white circle, top-left position) with directed edge to node X",
        "text_evidence": "A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$.",
        "query_type": "value_context"
      },
      {
        "query": "Why does node U connect to both X and Y while node A only connects to X?",
        "answer": "Node U represents independent background variables that can directly affect both intermediate variables and outcomes. Node A (protected attribute) influences Y only through X, reflecting the causal pathway where the protected attribute affects outcomes through intermediate factors rather than directly.",
        "visual_anchor": "Node U (white circle, top-right) with edges to both X and Y, versus node A with single edge to X",
        "text_evidence": "Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What real-world scenarios do the two different causal graph structures in parts a and b represent?",
        "answer": "The two causal graphs correspond to insurance pricing and crime prediction scenarios. These different structures reflect how protected attributes and background variables causally influence outcomes differently across real-world fair prediction problems.",
        "visual_anchor": "Two separate graph structures labeled (a) and (b) showing different node arrangements",
        "text_evidence": "To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b).",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A (protected attribute)",
      "Node X",
      "Node Y (outcome)",
      "Node U",
      "Directed edges between nodes",
      "Gray filled nodes X and Y",
      "White filled nodes A and U"
    ]
  },
  {
    "doc_id": "1703.06856",
    "figure_id": "1703.06856_fig_7",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg",
    "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four pairs of distribution plots comparing black↔white, asian↔white, mexican↔white, female↔male\",\n    \"Two rows labeled 'Full' and 'Unaware'\",\n    \"Cyan and pink/red overlapping density curves\",\n    \"X-axis labeled FŶA ranging from -6 to 10\",\n    \"Y-axis showing density values up to 0.2-0.3\",\n    \"Causal model diagram on left showing R, S, εG, εL, GPA, LSAT, FYA nodes\",\n    \"Legend showing 'actual' and 'counterfactual'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the Full row distributions show greater cyan-pink separation than Unaware row for black↔white predictions?\",\n      \"answer\": \"The Full model uses race and sex directly to predict FYA, creating larger counterfactual differences between groups. The Unaware model excludes these attributes, reducing the distributional gap between actual and counterfactual predictions.\",\n      \"visual_anchor\": \"Cyan and pink density curves in top-left 'Full' panel versus bottom-left 'Unaware' panel for black↔white comparison\",\n      \"text_evidence\": \"We estimate the error terms εG, εL by first fitting two models that each use race and sex to individually predict GPA and LSAT.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the nearly overlapping cyan and pink curves in the Unaware female↔male panel support Fair Add's use of residuals?\",\n      \"answer\": \"Yes, the overlapping distributions demonstrate that using residual estimates εG and εL removes the direct influence of sex on predictions, achieving fairness by making actual and counterfactual predictions nearly identical.\",\n      \"visual_anchor\": \"Bottom-right panel showing cyan and pink curves with minimal separation in Unaware row for female↔male\",\n      \"text_evidence\": \"We then compute the residuals of each model (e.g., εG = GPA − ŶG PA(R,S)). We use these residual estimates of εG, εL to predict FYA. We call this Fair Add.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the causal path from R through GPA and LSAT explain the Full row's bimodal pattern in black↔white?\",\n      \"answer\": \"The causal model shows race R influences both GPA and LSAT (via εG and εL), which then determine FYA. This creates distinct prediction distributions for actual versus counterfactual racial groups, manifesting as separated cyan and pink peaks.\",\n      \"visual_anchor\": \"Left causal diagram with R→εG→GPA→FYA and R→εL→LSAT→FYA paths, corresponding to separated peaks in top-left Full panel\",\n      \"text_evidence\": \"The causal graph corresponding to this model is shown in Figure 2, (Level 2).\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four pairs of distribution plots comparing black↔white, asian↔white, mexican↔white, female↔male",
        "Two rows labeled 'Full' and 'Unaware'",
        "Cyan and pink/red overlapping density curves",
        "X-axis labeled FŶA ranging from -6 to 10",
        "Y-axis showing density values up to 0.2-0.3",
        "Causal model diagram on left showing R, S, εG, εL, GPA, LSAT, FYA nodes",
        "Legend showing 'actual' and 'counterfactual'"
      ],
      "queries": [
        {
          "query": "Why do the Full row distributions show greater cyan-pink separation than Unaware row for black↔white predictions?",
          "answer": "The Full model uses race and sex directly to predict FYA, creating larger counterfactual differences between groups. The Unaware model excludes these attributes, reducing the distributional gap between actual and counterfactual predictions.",
          "visual_anchor": "Cyan and pink density curves in top-left 'Full' panel versus bottom-left 'Unaware' panel for black↔white comparison",
          "text_evidence": "We estimate the error terms εG, εL by first fitting two models that each use race and sex to individually predict GPA and LSAT.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the nearly overlapping cyan and pink curves in the Unaware female↔male panel support Fair Add's use of residuals?",
          "answer": "Yes, the overlapping distributions demonstrate that using residual estimates εG and εL removes the direct influence of sex on predictions, achieving fairness by making actual and counterfactual predictions nearly identical.",
          "visual_anchor": "Bottom-right panel showing cyan and pink curves with minimal separation in Unaware row for female↔male",
          "text_evidence": "We then compute the residuals of each model (e.g., εG = GPA − ŶG PA(R,S)). We use these residual estimates of εG, εL to predict FYA. We call this Fair Add.",
          "query_type": "value_context"
        },
        {
          "query": "How does the causal path from R through GPA and LSAT explain the Full row's bimodal pattern in black↔white?",
          "answer": "The causal model shows race R influences both GPA and LSAT (via εG and εL), which then determine FYA. This creates distinct prediction distributions for actual versus counterfactual racial groups, manifesting as separated cyan and pink peaks.",
          "visual_anchor": "Left causal diagram with R→εG→GPA→FYA and R→εL→LSAT→FYA paths, corresponding to separated peaks in top-left Full panel",
          "text_evidence": "The causal graph corresponding to this model is shown in Figure 2, (Level 2).",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the Full row distributions show greater cyan-pink separation than Unaware row for black↔white predictions?",
        "answer": "The Full model uses race and sex directly to predict FYA, creating larger counterfactual differences between groups. The Unaware model excludes these attributes, reducing the distributional gap between actual and counterfactual predictions.",
        "visual_anchor": "Cyan and pink density curves in top-left 'Full' panel versus bottom-left 'Unaware' panel for black↔white comparison",
        "text_evidence": "We estimate the error terms εG, εL by first fitting two models that each use race and sex to individually predict GPA and LSAT.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the nearly overlapping cyan and pink curves in the Unaware female↔male panel support Fair Add's use of residuals?",
        "answer": "Yes, the overlapping distributions demonstrate that using residual estimates εG and εL removes the direct influence of sex on predictions, achieving fairness by making actual and counterfactual predictions nearly identical.",
        "visual_anchor": "Bottom-right panel showing cyan and pink curves with minimal separation in Unaware row for female↔male",
        "text_evidence": "We then compute the residuals of each model (e.g., εG = GPA − ŶG PA(R,S)). We use these residual estimates of εG, εL to predict FYA. We call this Fair Add.",
        "query_type": "value_context"
      },
      {
        "query": "How does the causal path from R through GPA and LSAT explain the Full row's bimodal pattern in black↔white?",
        "answer": "The causal model shows race R influences both GPA and LSAT (via εG and εL), which then determine FYA. This creates distinct prediction distributions for actual versus counterfactual racial groups, manifesting as separated cyan and pink peaks.",
        "visual_anchor": "Left causal diagram with R→εG→GPA→FYA and R→εL→LSAT→FYA paths, corresponding to separated peaks in top-left Full panel",
        "text_evidence": "The causal graph corresponding to this model is shown in Figure 2, (Level 2).",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four pairs of distribution plots comparing black↔white, asian↔white, mexican↔white, female↔male",
      "Two rows labeled 'Full' and 'Unaware'",
      "Cyan and pink/red overlapping density curves",
      "X-axis labeled FŶA ranging from -6 to 10",
      "Y-axis showing density values up to 0.2-0.3",
      "Causal model diagram on left showing R, S, εG, εL, GPA, LSAT, FYA nodes",
      "Legend showing 'actual' and 'counterfactual'"
    ]
  },
  {
    "doc_id": "1703.06856",
    "figure_id": "1703.06856_fig_8",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg",
    "caption": "Figure 3: A causal model for the stop and frisk dataset.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Race node\", \"Arrest node\", \"Searched node\", \"Frisked node\", \"Force node\", \"Criminality node\", \"Weapon node\", \"Directed edges connecting nodes\", \"Gray-filled oval nodes\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does Race have directed edges to Arrest, Searched, Frisked, and Force in the 2014 NYPD dataset model?\",\n      \"answer\": \"The directed edges from Race to multiple outcome nodes represent potential causal pathways through which race may influence police actions in the 38,609 records of males stopped during 2014.\",\n      \"visual_anchor\": \"Race node with four outgoing directed edges to Arrest, Searched, Frisked, and Force\",\n      \"text_evidence\": \"We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the edge from Searched to Arrest represent a path in the controlled direct effects definition?\",\n      \"answer\": \"Yes, this edge would be part of the paths in P_G_A that are considered 'direct' paths when intervening on some paths from A to Y but not others.\",\n      \"visual_anchor\": \"Directed edge from Searched node to Arrest node\",\n      \"text_evidence\": \"This notion is related to controlled direct effects [29], where we intervene on some paths from A to Y, but not others. Paths in P_G_A are considered here to be the 'direct' paths\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the Weapon node's single incoming edge from Criminality relate to counterfactual fairness conditioning?\",\n      \"answer\": \"The single edge indicates Weapon is conditionally independent of Race given Criminality, which aligns with conditioning on X and A similarly to the probability of sufficiency definition.\",\n      \"visual_anchor\": \"Weapon node with only one incoming directed edge from Criminality node\",\n      \"text_evidence\": \"and we condition on X and A similarly to the definition of probability of sufficiency (3). This definition is the same as the original counterfactual fairness definition\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Race node",
        "Arrest node",
        "Searched node",
        "Frisked node",
        "Force node",
        "Criminality node",
        "Weapon node",
        "Directed edges connecting nodes",
        "Gray-filled oval nodes"
      ],
      "queries": [
        {
          "query": "Why does Race have directed edges to Arrest, Searched, Frisked, and Force in the 2014 NYPD dataset model?",
          "answer": "The directed edges from Race to multiple outcome nodes represent potential causal pathways through which race may influence police actions in the 38,609 records of males stopped during 2014.",
          "visual_anchor": "Race node with four outgoing directed edges to Arrest, Searched, Frisked, and Force",
          "text_evidence": "We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the edge from Searched to Arrest represent a path in the controlled direct effects definition?",
          "answer": "Yes, this edge would be part of the paths in P_G_A that are considered 'direct' paths when intervening on some paths from A to Y but not others.",
          "visual_anchor": "Directed edge from Searched node to Arrest node",
          "text_evidence": "This notion is related to controlled direct effects [29], where we intervene on some paths from A to Y, but not others. Paths in P_G_A are considered here to be the 'direct' paths",
          "query_type": "value_context"
        },
        {
          "query": "How does the Weapon node's single incoming edge from Criminality relate to counterfactual fairness conditioning?",
          "answer": "The single edge indicates Weapon is conditionally independent of Race given Criminality, which aligns with conditioning on X and A similarly to the probability of sufficiency definition.",
          "visual_anchor": "Weapon node with only one incoming directed edge from Criminality node",
          "text_evidence": "and we condition on X and A similarly to the definition of probability of sufficiency (3). This definition is the same as the original counterfactual fairness definition",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Race have directed edges to Arrest, Searched, Frisked, and Force in the 2014 NYPD dataset model?",
        "answer": "The directed edges from Race to multiple outcome nodes represent potential causal pathways through which race may influence police actions in the 38,609 records of males stopped during 2014.",
        "visual_anchor": "Race node with four outgoing directed edges to Arrest, Searched, Frisked, and Force",
        "text_evidence": "We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the edge from Searched to Arrest represent a path in the controlled direct effects definition?",
        "answer": "Yes, this edge would be part of the paths in P_G_A that are considered 'direct' paths when intervening on some paths from A to Y but not others.",
        "visual_anchor": "Directed edge from Searched node to Arrest node",
        "text_evidence": "This notion is related to controlled direct effects [29], where we intervene on some paths from A to Y, but not others. Paths in P_G_A are considered here to be the 'direct' paths",
        "query_type": "value_context"
      },
      {
        "query": "How does the Weapon node's single incoming edge from Criminality relate to counterfactual fairness conditioning?",
        "answer": "The single edge indicates Weapon is conditionally independent of Race given Criminality, which aligns with conditioning on X and A similarly to the probability of sufficiency definition.",
        "visual_anchor": "Weapon node with only one incoming directed edge from Criminality node",
        "text_evidence": "and we condition on X and A similarly to the definition of probability of sufficiency (3). This definition is the same as the original counterfactual fairness definition",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Race node",
      "Arrest node",
      "Searched node",
      "Frisked node",
      "Force node",
      "Criminality node",
      "Weapon node",
      "Directed edges connecting nodes",
      "Gray-filled oval nodes"
    ]
  },
  {
    "doc_id": "1703.06856",
    "figure_id": "1703.06856_fig_9",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg",
    "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four maps of NYC showing race distribution and arrest patterns\",\n    \"Yellow circles highlighting northern region\",\n    \"Magenta circles highlighting southern region\",\n    \"Blue dots representing White population\",\n    \"Orange dots representing Black Hispanic population\",\n    \"Red dots indicating arrests (Yes)\",\n    \"White dots indicating no arrests (No)\",\n    \"Black background representing water/non-populated areas\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the 38,609 male stops from 2014 explain why White and Black Hispanic distributions appear in distinct geographic clusters?\",\n      \"answer\": \"Yes, the analysis is limited to males stopped during 2014 constituting 38,609 records, which forms the basis for visualizing the distribution of White and Black Hispanic populations shown in the leftmost map.\",\n      \"visual_anchor\": \"Blue and orange dots in leftmost 'Race (data)' map showing population distribution\",\n      \"text_evidence\": \"We consider the data collected on males stopped during 2014 which constitutes 38,609 records.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the third plot show predominantly white dots with red outlines compared to the second plot's arrest distribution?\",\n      \"answer\": \"The third plot displays a counterfactual scenario where the model computed whether every individual would be arrested if they had been White, showing fewer predicted arrests than the actual data.\",\n      \"visual_anchor\": \"Third plot 'Arrest if White (counterfactual)' showing mostly white dots versus second plot 'Arrested (data)' showing mixed red and white dots\",\n      \"text_evidence\": \"Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the fourth plot's increased red dot density in the yellow-circled region demonstrate racial effects on arrest outcomes?\",\n      \"answer\": \"The fourth plot shows the counterfactual where everyone is treated as Black Hispanic, resulting in more arrests (red dots) compared to other scenarios, demonstrating that race significantly affects arrest likelihood in the model.\",\n      \"visual_anchor\": \"Fourth plot 'Arrest if Black Hispanic (counterfactual)' showing higher concentration of red dots in yellow-circled northern region\",\n      \"text_evidence\": \"Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four maps of NYC showing race distribution and arrest patterns",
        "Yellow circles highlighting northern region",
        "Magenta circles highlighting southern region",
        "Blue dots representing White population",
        "Orange dots representing Black Hispanic population",
        "Red dots indicating arrests (Yes)",
        "White dots indicating no arrests (No)",
        "Black background representing water/non-populated areas"
      ],
      "queries": [
        {
          "query": "Do the 38,609 male stops from 2014 explain why White and Black Hispanic distributions appear in distinct geographic clusters?",
          "answer": "Yes, the analysis is limited to males stopped during 2014 constituting 38,609 records, which forms the basis for visualizing the distribution of White and Black Hispanic populations shown in the leftmost map.",
          "visual_anchor": "Blue and orange dots in leftmost 'Race (data)' map showing population distribution",
          "text_evidence": "We consider the data collected on males stopped during 2014 which constitutes 38,609 records.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the third plot show predominantly white dots with red outlines compared to the second plot's arrest distribution?",
          "answer": "The third plot displays a counterfactual scenario where the model computed whether every individual would be arrested if they had been White, showing fewer predicted arrests than the actual data.",
          "visual_anchor": "Third plot 'Arrest if White (counterfactual)' showing mostly white dots versus second plot 'Arrested (data)' showing mixed red and white dots",
          "text_evidence": "Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the fourth plot's increased red dot density in the yellow-circled region demonstrate racial effects on arrest outcomes?",
          "answer": "The fourth plot shows the counterfactual where everyone is treated as Black Hispanic, resulting in more arrests (red dots) compared to other scenarios, demonstrating that race significantly affects arrest likelihood in the model.",
          "visual_anchor": "Fourth plot 'Arrest if Black Hispanic (counterfactual)' showing higher concentration of red dots in yellow-circled northern region",
          "text_evidence": "Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the 38,609 male stops from 2014 explain why White and Black Hispanic distributions appear in distinct geographic clusters?",
        "answer": "Yes, the analysis is limited to males stopped during 2014 constituting 38,609 records, which forms the basis for visualizing the distribution of White and Black Hispanic populations shown in the leftmost map.",
        "visual_anchor": "Blue and orange dots in leftmost 'Race (data)' map showing population distribution",
        "text_evidence": "We consider the data collected on males stopped during 2014 which constitutes 38,609 records.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the third plot show predominantly white dots with red outlines compared to the second plot's arrest distribution?",
        "answer": "The third plot displays a counterfactual scenario where the model computed whether every individual would be arrested if they had been White, showing fewer predicted arrests than the actual data.",
        "visual_anchor": "Third plot 'Arrest if White (counterfactual)' showing mostly white dots versus second plot 'Arrested (data)' showing mixed red and white dots",
        "text_evidence": "Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the fourth plot's increased red dot density in the yellow-circled region demonstrate racial effects on arrest outcomes?",
        "answer": "The fourth plot shows the counterfactual where everyone is treated as Black Hispanic, resulting in more arrests (red dots) compared to other scenarios, demonstrating that race significantly affects arrest likelihood in the model.",
        "visual_anchor": "Fourth plot 'Arrest if Black Hispanic (counterfactual)' showing higher concentration of red dots in yellow-circled northern region",
        "text_evidence": "Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four maps of NYC showing race distribution and arrest patterns",
      "Yellow circles highlighting northern region",
      "Magenta circles highlighting southern region",
      "Blue dots representing White population",
      "Orange dots representing Black Hispanic population",
      "Red dots indicating arrests (Yes)",
      "White dots indicating no arrests (No)",
      "Black background representing water/non-populated areas"
    ]
  },
  {
    "doc_id": "1705.10378",
    "figure_id": "1705.10378_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
    "caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node C at top-left\", \"Node Y at top-right\", \"Node A at bottom-left\", \"Node M at bottom-right\", \"Blue arrows connecting C→Y, C→A, A→M, A→Y, M→Y\"],\n  \"queries\": [\n    {\n      \"query\": \"How many mediators exist between A and Y in this graph with vertices representing observed random variables?\",\n      \"answer\": \"There is a single mediator M between A and Y, as shown by the paths A→M→Y in the causal graph where vertices represent observed random variables.\",\n      \"visual_anchor\": \"Node M at bottom-right with incoming arrow from A and outgoing arrow to Y\",\n      \"text_evidence\": \"In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the path A→Y represent a direct causal pathway distinct from the A→M→Y mediated pathway?\",\n      \"answer\": \"Yes, the direct arrow A→Y represents one causal pathway, while A→M→Y represents a mediated pathway through M, allowing decomposition of effects along particular causal pathways.\",\n      \"visual_anchor\": \"Blue arrow directly connecting node A to node Y, bypassing node M\",\n      \"text_evidence\": \"we may wish to decompose the effect of $A$ on $Y$ into the contribution of the path $A  W  Y$ , and the path bundle $A  Y$ and $A \\\\to M \\\\to W \\\\to Y .$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does C point to both A and Y if causal models generalize Bayesian networks?\",\n      \"answer\": \"C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.\",\n      \"visual_anchor\": \"Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)\",\n      \"text_evidence\": \"Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node C at top-left",
        "Node Y at top-right",
        "Node A at bottom-left",
        "Node M at bottom-right",
        "Blue arrows connecting C→Y, C→A, A→M, A→Y, M→Y"
      ],
      "queries": [
        {
          "query": "How many mediators exist between A and Y in this graph with vertices representing observed random variables?",
          "answer": "There is a single mediator M between A and Y, as shown by the paths A→M→Y in the causal graph where vertices represent observed random variables.",
          "visual_anchor": "Node M at bottom-right with incoming arrow from A and outgoing arrow to Y",
          "text_evidence": "In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships.",
          "query_type": "value_context"
        },
        {
          "query": "Does the path A→Y represent a direct causal pathway distinct from the A→M→Y mediated pathway?",
          "answer": "Yes, the direct arrow A→Y represents one causal pathway, while A→M→Y represents a mediated pathway through M, allowing decomposition of effects along particular causal pathways.",
          "visual_anchor": "Blue arrow directly connecting node A to node Y, bypassing node M",
          "text_evidence": "we may wish to decompose the effect of $A$ on $Y$ into the contribution of the path $A  W  Y$ , and the path bundle $A  Y$ and $A \\to M \\to W \\to Y .$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does C point to both A and Y if causal models generalize Bayesian networks?",
          "answer": "C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.",
          "visual_anchor": "Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)",
          "text_evidence": "Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "How many mediators exist between A and Y in this graph with vertices representing observed random variables?",
        "answer": "There is a single mediator M between A and Y, as shown by the paths A→M→Y in the causal graph where vertices represent observed random variables.",
        "visual_anchor": "Node M at bottom-right with incoming arrow from A and outgoing arrow to Y",
        "text_evidence": "In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships.",
        "query_type": "value_context"
      },
      {
        "query": "Does the path A→Y represent a direct causal pathway distinct from the A→M→Y mediated pathway?",
        "answer": "Yes, the direct arrow A→Y represents one causal pathway, while A→M→Y represents a mediated pathway through M, allowing decomposition of effects along particular causal pathways.",
        "visual_anchor": "Blue arrow directly connecting node A to node Y, bypassing node M",
        "text_evidence": "we may wish to decompose the effect of $A$ on $Y$ into the contribution of the path $A  W  Y$ , and the path bundle $A  Y$ and $A \\to M \\to W \\to Y .$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does C point to both A and Y if causal models generalize Bayesian networks?",
        "answer": "C pointing to both A and Y indicates C is a common cause (confounder) that creates conditional dependencies, which causal models encode beyond standard Bayesian network independence structures.",
        "visual_anchor": "Node C at top-left with blue arrows pointing to both A (bottom-left) and Y (top-right)",
        "text_evidence": "Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node C at top-left",
      "Node Y at top-right",
      "Node A at bottom-left",
      "Node M at bottom-right",
      "Blue arrows connecting C→Y, C→A, A→M, A→Y, M→Y"
    ]
  },
  {
    "doc_id": "1705.10378",
    "figure_id": "1705.10378_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg",
    "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A with blue arrow to M\", \"Node M with blue arrow to Y\", \"Node C with blue arrows to both A and M\", \"Node M with green arrow to Y\", \"Circular nodes labeled A, M, Y, C\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue arrow from A to M represent prior convictions as the mediator in COMPAS?\",\n      \"answer\": \"In the COMPAS dataset, A denotes race and M represents prior convictions as the mediator between race and recidivism outcome Y, showing how race indirectly affects recidivism through prior convictions.\",\n      \"visual_anchor\": \"Blue arrow from node A to node M in diagram (a)\",\n      \"text_evidence\": \"The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the green arrow from M to Y in diagram b represent a different causal pathway than COMPAS?\",\n      \"answer\": \"Yes, the green arrow indicates a distinct mediating pathway in the Adult dataset where M represents different mediating variables (like education and job-related factors) affecting income outcome Y, unlike COMPAS where M is prior convictions.\",\n      \"visual_anchor\": \"Green arrow from node M to node Y in diagram (b)\",\n      \"text_evidence\": \"The \\\"adult\\\" dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does estimating NDE using the causal paths shown enable robust fair inference despite Y model misspecification?\",\n      \"answer\": \"The NDE (Natural Direct Effect) quantifying discrimination can be estimated through the causal paths without relying on correct specification of the Y model, allowing fair inferences to remain valid even when the outcome model is misspecified.\",\n      \"visual_anchor\": \"Direct and indirect paths from A through M to Y in both diagrams\",\n      \"text_evidence\": \"We also illustrate how a part of the model involving the outcome $Y$ may be regularized without compromising fair inferences if the NDE quantifying discrimination is estimated using methods that are robust to misspecification of the $Y$ model.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A with blue arrow to M",
        "Node M with blue arrow to Y",
        "Node C with blue arrows to both A and M",
        "Node M with green arrow to Y",
        "Circular nodes labeled A, M, Y, C"
      ],
      "queries": [
        {
          "query": "Why does the blue arrow from A to M represent prior convictions as the mediator in COMPAS?",
          "answer": "In the COMPAS dataset, A denotes race and M represents prior convictions as the mediator between race and recidivism outcome Y, showing how race indirectly affects recidivism through prior convictions.",
          "visual_anchor": "Blue arrow from node A to node M in diagram (a)",
          "text_evidence": "The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the green arrow from M to Y in diagram b represent a different causal pathway than COMPAS?",
          "answer": "Yes, the green arrow indicates a distinct mediating pathway in the Adult dataset where M represents different mediating variables (like education and job-related factors) affecting income outcome Y, unlike COMPAS where M is prior convictions.",
          "visual_anchor": "Green arrow from node M to node Y in diagram (b)",
          "text_evidence": "The \"adult\" dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does estimating NDE using the causal paths shown enable robust fair inference despite Y model misspecification?",
          "answer": "The NDE (Natural Direct Effect) quantifying discrimination can be estimated through the causal paths without relying on correct specification of the Y model, allowing fair inferences to remain valid even when the outcome model is misspecified.",
          "visual_anchor": "Direct and indirect paths from A through M to Y in both diagrams",
          "text_evidence": "We also illustrate how a part of the model involving the outcome $Y$ may be regularized without compromising fair inferences if the NDE quantifying discrimination is estimated using methods that are robust to misspecification of the $Y$ model.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue arrow from A to M represent prior convictions as the mediator in COMPAS?",
        "answer": "In the COMPAS dataset, A denotes race and M represents prior convictions as the mediator between race and recidivism outcome Y, showing how race indirectly affects recidivism through prior convictions.",
        "visual_anchor": "Blue arrow from node A to node M in diagram (a)",
        "text_evidence": "The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the green arrow from M to Y in diagram b represent a different causal pathway than COMPAS?",
        "answer": "Yes, the green arrow indicates a distinct mediating pathway in the Adult dataset where M represents different mediating variables (like education and job-related factors) affecting income outcome Y, unlike COMPAS where M is prior convictions.",
        "visual_anchor": "Green arrow from node M to node Y in diagram (b)",
        "text_evidence": "The \"adult\" dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does estimating NDE using the causal paths shown enable robust fair inference despite Y model misspecification?",
        "answer": "The NDE (Natural Direct Effect) quantifying discrimination can be estimated through the causal paths without relying on correct specification of the Y model, allowing fair inferences to remain valid even when the outcome model is misspecified.",
        "visual_anchor": "Direct and indirect paths from A through M to Y in both diagrams",
        "text_evidence": "We also illustrate how a part of the model involving the outcome $Y$ may be regularized without compromising fair inferences if the NDE quantifying discrimination is estimated using methods that are robust to misspecification of the $Y$ model.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A with blue arrow to M",
      "Node M with blue arrow to Y",
      "Node C with blue arrows to both A and M",
      "Node M with green arrow to Y",
      "Circular nodes labeled A, M, Y, C"
    ]
  },
  {
    "doc_id": "1706.02409",
    "figure_id": "1706.02409_fig_4",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg",
    "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"MSE on y-axis ranging from 0.14 to 0.26\",\n    \"Fairness Loss on x-axis ranging from 0 to 0.05\",\n    \"Six curves: Group single (red), Group separate (blue), Hybrid single (magenta), Hybrid separate (purple), Individual single (cyan), Individual separate (green)\",\n    \"Cyan and green curves showing gradual decline from left to right\",\n    \"Purple curve showing sharp drop near fairness loss = 0\",\n    \"All curves converging to MSE range 0.14-0.16\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the cyan and green Individual curves decline gradually across fairness loss 0 to 0.05?\",\n      \"answer\": \"The curves are obtained by varying the weight λ on the fairness regularizer, showing the trade-off between accuracy (MSE) and fairness as λ increases across different values.\",\n      \"visual_anchor\": \"cyan (Individual, single) and green (Individual, separate) curves showing gradual decline\",\n      \"text_evidence\": \"These curves are shown in Figure 1, and are obtained by varying the weight $\\\\lambda$ on the fairness regularizer, and for each value of $\\\\lambda$ finding the model which minimizes the associated regularized loss function.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the purple Hybrid separate curve dropping sharply near zero fairness loss support qualitative cross-dataset comparison?\",\n      \"answer\": \"Yes, the sharp drop demonstrates that in some datasets the fairness penalty can be substantially decreased with little cost to accuracy, enabling qualitative comparisons across datasets.\",\n      \"visual_anchor\": \"purple curve (Hybrid, separate) showing sharp drop from MSE ~0.17 to ~0.15 near fairness loss = 0\",\n      \"text_evidence\": \"The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are only six curves shown when race or gender protected features define the subgroups?\",\n      \"answer\": \"For binary-valued targets, three fairness notions (group, individual, hybrid) are examined with either single or separate models per group, yielding six curves total (3 notions × 2 model types).\",\n      \"visual_anchor\": \"six distinct curves in the legend: three fairness types (Group, Hybrid, Individual) each with single and separate variants\",\n      \"text_evidence\": \"For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "MSE on y-axis ranging from 0.14 to 0.26",
        "Fairness Loss on x-axis ranging from 0 to 0.05",
        "Six curves: Group single (red), Group separate (blue), Hybrid single (magenta), Hybrid separate (purple), Individual single (cyan), Individual separate (green)",
        "Cyan and green curves showing gradual decline from left to right",
        "Purple curve showing sharp drop near fairness loss = 0",
        "All curves converging to MSE range 0.14-0.16"
      ],
      "queries": [
        {
          "query": "Why do the cyan and green Individual curves decline gradually across fairness loss 0 to 0.05?",
          "answer": "The curves are obtained by varying the weight λ on the fairness regularizer, showing the trade-off between accuracy (MSE) and fairness as λ increases across different values.",
          "visual_anchor": "cyan (Individual, single) and green (Individual, separate) curves showing gradual decline",
          "text_evidence": "These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the purple Hybrid separate curve dropping sharply near zero fairness loss support qualitative cross-dataset comparison?",
          "answer": "Yes, the sharp drop demonstrates that in some datasets the fairness penalty can be substantially decreased with little cost to accuracy, enabling qualitative comparisons across datasets.",
          "visual_anchor": "purple curve (Hybrid, separate) showing sharp drop from MSE ~0.17 to ~0.15 near fairness loss = 0",
          "text_evidence": "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy.",
          "query_type": "value_context"
        },
        {
          "query": "Why are only six curves shown when race or gender protected features define the subgroups?",
          "answer": "For binary-valued targets, three fairness notions (group, individual, hybrid) are examined with either single or separate models per group, yielding six curves total (3 notions × 2 model types).",
          "visual_anchor": "six distinct curves in the legend: three fairness types (Group, Hybrid, Individual) each with single and separate variants",
          "text_evidence": "For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the cyan and green Individual curves decline gradually across fairness loss 0 to 0.05?",
        "answer": "The curves are obtained by varying the weight λ on the fairness regularizer, showing the trade-off between accuracy (MSE) and fairness as λ increases across different values.",
        "visual_anchor": "cyan (Individual, single) and green (Individual, separate) curves showing gradual decline",
        "text_evidence": "These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the purple Hybrid separate curve dropping sharply near zero fairness loss support qualitative cross-dataset comparison?",
        "answer": "Yes, the sharp drop demonstrates that in some datasets the fairness penalty can be substantially decreased with little cost to accuracy, enabling qualitative comparisons across datasets.",
        "visual_anchor": "purple curve (Hybrid, separate) showing sharp drop from MSE ~0.17 to ~0.15 near fairness loss = 0",
        "text_evidence": "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy.",
        "query_type": "value_context"
      },
      {
        "query": "Why are only six curves shown when race or gender protected features define the subgroups?",
        "answer": "For binary-valued targets, three fairness notions (group, individual, hybrid) are examined with either single or separate models per group, yielding six curves total (3 notions × 2 model types).",
        "visual_anchor": "six distinct curves in the legend: three fairness types (Group, Hybrid, Individual) each with single and separate variants",
        "text_evidence": "For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "MSE on y-axis ranging from 0.14 to 0.26",
      "Fairness Loss on x-axis ranging from 0 to 0.05",
      "Six curves: Group single (red), Group separate (blue), Hybrid single (magenta), Hybrid separate (purple), Individual single (cyan), Individual separate (green)",
      "Cyan and green curves showing gradual decline from left to right",
      "Purple curve showing sharp drop near fairness loss = 0",
      "All curves converging to MSE range 0.14-0.16"
    ]
  },
  {
    "doc_id": "1706.02409",
    "figure_id": "1706.02409_fig_10",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
    "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bars in six color pairs (blue, red, magenta, yellow, cyan, green)\",\n    \"X-axis showing alpha values from 0.5 to 0.01\",\n    \"Y-axis showing Price of Fairness from 0 to 3\",\n    \"Legend indicating Group separate/single, Hybrid separate/single, Individual separate/single\",\n    \"Bar heights ranging approximately from 1.0 to 1.6\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the cyan and green bars representing individual fairness regularizers reach the highest Price of Fairness at alpha 0.01?\",\n      \"answer\": \"Yes, the cyan (individual, separate) and green (individual, single) bars at alpha 0.01 reach approximately 1.6, the highest values shown for individual fairness regularizers across all alpha values in the plot.\",\n      \"visual_anchor\": \"cyan and green bars at rightmost position (alpha=0.01)\",\n      \"text_evidence\": \"Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the blue and red bars for group fairness remain near 1.0 across all alpha values compared to other regularizers?\",\n      \"answer\": \"The group fairness regularizers (blue and red bars) consistently show the lowest Price of Fairness near 1.0, suggesting that group-based fairness constraints impose minimal performance cost compared to individual or hybrid approaches across different regularization strengths.\",\n      \"visual_anchor\": \"blue (Group, separate) and red (Group, single) bars consistently at approximately 1.0 height\",\n      \"text_evidence\": \"The \\\"Price of Fairness\\\" across data sets, for each type of fairness regularizer, in both the single and separate model case.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?\",\n      \"answer\": \"As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.\",\n      \"visual_anchor\": \"magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right\",\n      \"text_evidence\": \"In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bars in six color pairs (blue, red, magenta, yellow, cyan, green)",
        "X-axis showing alpha values from 0.5 to 0.01",
        "Y-axis showing Price of Fairness from 0 to 3",
        "Legend indicating Group separate/single, Hybrid separate/single, Individual separate/single",
        "Bar heights ranging approximately from 1.0 to 1.6"
      ],
      "queries": [
        {
          "query": "Do the cyan and green bars representing individual fairness regularizers reach the highest Price of Fairness at alpha 0.01?",
          "answer": "Yes, the cyan (individual, separate) and green (individual, single) bars at alpha 0.01 reach approximately 1.6, the highest values shown for individual fairness regularizers across all alpha values in the plot.",
          "visual_anchor": "cyan and green bars at rightmost position (alpha=0.01)",
          "text_evidence": "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case.",
          "query_type": "value_context"
        },
        {
          "query": "Why do the blue and red bars for group fairness remain near 1.0 across all alpha values compared to other regularizers?",
          "answer": "The group fairness regularizers (blue and red bars) consistently show the lowest Price of Fairness near 1.0, suggesting that group-based fairness constraints impose minimal performance cost compared to individual or hybrid approaches across different regularization strengths.",
          "visual_anchor": "blue (Group, separate) and red (Group, single) bars consistently at approximately 1.0 height",
          "text_evidence": "The \"Price of Fairness\" across data sets, for each type of fairness regularizer, in both the single and separate model case.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?",
          "answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.",
          "visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right",
          "text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the cyan and green bars representing individual fairness regularizers reach the highest Price of Fairness at alpha 0.01?",
        "answer": "Yes, the cyan (individual, separate) and green (individual, single) bars at alpha 0.01 reach approximately 1.6, the highest values shown for individual fairness regularizers across all alpha values in the plot.",
        "visual_anchor": "cyan and green bars at rightmost position (alpha=0.01)",
        "text_evidence": "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case.",
        "query_type": "value_context"
      },
      {
        "query": "Why do the blue and red bars for group fairness remain near 1.0 across all alpha values compared to other regularizers?",
        "answer": "The group fairness regularizers (blue and red bars) consistently show the lowest Price of Fairness near 1.0, suggesting that group-based fairness constraints impose minimal performance cost compared to individual or hybrid approaches across different regularization strengths.",
        "visual_anchor": "blue (Group, separate) and red (Group, single) bars consistently at approximately 1.0 height",
        "text_evidence": "The \"Price of Fairness\" across data sets, for each type of fairness regularizer, in both the single and separate model case.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the progressive height increase in magenta and yellow hybrid bars as alpha decreases from 0.5 to 0.01?",
        "answer": "As alpha decreases (stronger regularization), the hybrid fairness regularizers show increasing Price of Fairness, indicating that stricter hybrid fairness constraints progressively sacrifice more model performance to achieve fairness goals.",
        "visual_anchor": "magenta (Hybrid, separate) and yellow (Hybrid, single) bars increasing from left to right",
        "text_evidence": "In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016. [2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bars in six color pairs (blue, red, magenta, yellow, cyan, green)",
      "X-axis showing alpha values from 0.5 to 0.01",
      "Y-axis showing Price of Fairness from 0 to 3",
      "Legend indicating Group separate/single, Hybrid separate/single, Individual separate/single",
      "Bar heights ranging approximately from 1.0 to 1.6"
    ]
  },
  {
    "doc_id": "1706.02744",
    "figure_id": "1706.02744_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg",
    "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A (protected attribute)\", \"Node X (proxy/feature)\", \"Node R (decision)\", \"Arrow from A to X\", \"Arrow from A to R\", \"Arrow from X to R\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the arrow from A to X matter when evaluating the direct effect of gender on admission R?\",\n      \"answer\": \"The direct effect of gender A on admission R cannot be ascribed to department choice X when X is on the causal path from A to R. The arrow from A to X shows that gender influences department choice, which then affects admission.\",\n      \"visual_anchor\": \"Arrow from node A to node X\",\n      \"text_evidence\": \"From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the arrow from X to R support the claim that admission depends on department choice?\",\n      \"answer\": \"Yes, the arrow from X to R directly represents that the admission decision R depends on department choice X, as stated in the context that R depends on X.\",\n      \"visual_anchor\": \"Arrow from node X to node R\",\n      \"text_evidence\": \"The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What role do nodes A, X, and R play in organizing assumptions about the data generating process?\",\n      \"answer\": \"Node A represents the protected attribute (gender), node X represents features (department choice), and node R represents the predictor (admission decision). These nodes organize causal assumptions about how gender affects admission both directly and through department choice.\",\n      \"visual_anchor\": \"Three nodes A, X, and R with connecting arrows\",\n      \"text_evidence\": \"Causal graphs are a convenient way of organizing assumptions about the data generating process. We will generally consider causal graphs involving a protected attribute $A$ , a set of proxy variables $P _ { - }$ , features $X$ , a predictor $R$\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A (protected attribute)",
        "Node X (proxy/feature)",
        "Node R (decision)",
        "Arrow from A to X",
        "Arrow from A to R",
        "Arrow from X to R"
      ],
      "queries": [
        {
          "query": "Why does the arrow from A to X matter when evaluating the direct effect of gender on admission R?",
          "answer": "The direct effect of gender A on admission R cannot be ascribed to department choice X when X is on the causal path from A to R. The arrow from A to X shows that gender influences department choice, which then affects admission.",
          "visual_anchor": "Arrow from node A to node X",
          "text_evidence": "From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the arrow from X to R support the claim that admission depends on department choice?",
          "answer": "Yes, the arrow from X to R directly represents that the admission decision R depends on department choice X, as stated in the context that R depends on X.",
          "visual_anchor": "Arrow from node X to node R",
          "text_evidence": "The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$",
          "query_type": "value_context"
        },
        {
          "query": "What role do nodes A, X, and R play in organizing assumptions about the data generating process?",
          "answer": "Node A represents the protected attribute (gender), node X represents features (department choice), and node R represents the predictor (admission decision). These nodes organize causal assumptions about how gender affects admission both directly and through department choice.",
          "visual_anchor": "Three nodes A, X, and R with connecting arrows",
          "text_evidence": "Causal graphs are a convenient way of organizing assumptions about the data generating process. We will generally consider causal graphs involving a protected attribute $A$ , a set of proxy variables $P _ { - }$ , features $X$ , a predictor $R$",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the arrow from A to X matter when evaluating the direct effect of gender on admission R?",
        "answer": "The direct effect of gender A on admission R cannot be ascribed to department choice X when X is on the causal path from A to R. The arrow from A to X shows that gender influences department choice, which then affects admission.",
        "visual_anchor": "Arrow from node A to node X",
        "text_evidence": "From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the arrow from X to R support the claim that admission depends on department choice?",
        "answer": "Yes, the arrow from X to R directly represents that the admission decision R depends on department choice X, as stated in the context that R depends on X.",
        "visual_anchor": "Arrow from node X to node R",
        "text_evidence": "The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$",
        "query_type": "value_context"
      },
      {
        "query": "What role do nodes A, X, and R play in organizing assumptions about the data generating process?",
        "answer": "Node A represents the protected attribute (gender), node X represents features (department choice), and node R represents the predictor (admission decision). These nodes organize causal assumptions about how gender affects admission both directly and through department choice.",
        "visual_anchor": "Three nodes A, X, and R with connecting arrows",
        "text_evidence": "Causal graphs are a convenient way of organizing assumptions about the data generating process. We will generally consider causal graphs involving a protected attribute $A$ , a set of proxy variables $P _ { - }$ , features $X$ , a predictor $R$",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A (protected attribute)",
      "Node X (proxy/feature)",
      "Node R (decision)",
      "Arrow from A to X",
      "Arrow from A to R",
      "Arrow from X to R"
    ]
  },
  {
    "doc_id": "1706.02744",
    "figure_id": "1706.02744_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg",
    "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A\", \"Node X₁ (green)\", \"Node X₂\", \"Node Y\", \"Node R*\", \"Green arrow from A to X₁\", \"Red paths from X₁ to Y and X₂\", \"Directed edges forming causal graph\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the green node X₁ exhibit unresolved discrimination in the right graph along the red paths?\",\n      \"answer\": \"Yes, if X₁ is a resolving variable, R* exhibits unresolved discrimination in the right graph along the red paths, but not in the left one.\",\n      \"visual_anchor\": \"green node X₁ with red paths to Y and X₂ in right graph\",\n      \"text_evidence\": \"If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why can both graphs generate identical joint distributions for R* despite different edge structures between X₁ and other nodes?\",\n      \"answer\": \"Both graphs represent different causal structures that may produce the same joint distribution for the Bayes optimal unconstrained predictor R*, illustrating observational equivalence despite different causal mechanisms.\",\n      \"visual_anchor\": \"two distinct graph structures with node R* at top\",\n      \"text_evidence\": \"Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What makes interventions on variables like those represented by node A difficult to perform in randomized trials?\",\n      \"answer\": \"Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize at an individual level and impossible to perform in randomized trials.\",\n      \"visual_anchor\": \"node A with outgoing arrows to X₁ and X₂\",\n      \"text_evidence\": \"Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize—especially at an individual level, and impossible to perform in a randomized trial.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A",
        "Node X₁ (green)",
        "Node X₂",
        "Node Y",
        "Node R*",
        "Green arrow from A to X₁",
        "Red paths from X₁ to Y and X₂",
        "Directed edges forming causal graph"
      ],
      "queries": [
        {
          "query": "Does the green node X₁ exhibit unresolved discrimination in the right graph along the red paths?",
          "answer": "Yes, if X₁ is a resolving variable, R* exhibits unresolved discrimination in the right graph along the red paths, but not in the left one.",
          "visual_anchor": "green node X₁ with red paths to Y and X₂ in right graph",
          "text_evidence": "If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
          "query_type": "value_context"
        },
        {
          "query": "Why can both graphs generate identical joint distributions for R* despite different edge structures between X₁ and other nodes?",
          "answer": "Both graphs represent different causal structures that may produce the same joint distribution for the Bayes optimal unconstrained predictor R*, illustrating observational equivalence despite different causal mechanisms.",
          "visual_anchor": "two distinct graph structures with node R* at top",
          "text_evidence": "Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What makes interventions on variables like those represented by node A difficult to perform in randomized trials?",
          "answer": "Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize at an individual level and impossible to perform in randomized trials.",
          "visual_anchor": "node A with outgoing arrows to X₁ and X₂",
          "text_evidence": "Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize—especially at an individual level, and impossible to perform in a randomized trial.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the green node X₁ exhibit unresolved discrimination in the right graph along the red paths?",
        "answer": "Yes, if X₁ is a resolving variable, R* exhibits unresolved discrimination in the right graph along the red paths, but not in the left one.",
        "visual_anchor": "green node X₁ with red paths to Y and X₂ in right graph",
        "text_evidence": "If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
        "query_type": "value_context"
      },
      {
        "query": "Why can both graphs generate identical joint distributions for R* despite different edge structures between X₁ and other nodes?",
        "answer": "Both graphs represent different causal structures that may produce the same joint distribution for the Bayes optimal unconstrained predictor R*, illustrating observational equivalence despite different causal mechanisms.",
        "visual_anchor": "two distinct graph structures with node R* at top",
        "text_evidence": "Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What makes interventions on variables like those represented by node A difficult to perform in randomized trials?",
        "answer": "Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize at an individual level and impossible to perform in randomized trials.",
        "visual_anchor": "node A with outgoing arrows to X₁ and X₂",
        "text_evidence": "Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize—especially at an individual level, and impossible to perform in a randomized trial.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A",
      "Node X₁ (green)",
      "Node X₂",
      "Node Y",
      "Node R*",
      "Green arrow from A to X₁",
      "Red paths from X₁ to Y and X₂",
      "Directed edges forming causal graph"
    ]
  },
  {
    "doc_id": "1706.02744",
    "figure_id": "1706.02744_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg",
    "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Template graph with green node A, red node P, black nodes NP, NX, X, R\",\n    \"Intervened graph on right with removed edges from green A to red P\",\n    \"Directed edges showing causal relationships\",\n    \"Gray node G label below R in both graphs\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the intervened graph remove the edge from green A to red P when proxy discrimination is being avoided?\",\n      \"answer\": \"The benevolent viewpoint aims to guarantee that proxy P has no overall influence on prediction by adjusting P→R to cancel influence along P→X→R, requiring intervention on the A→P edge.\",\n      \"visual_anchor\": \"removed edge from green node A to red node P in the right graph\",\n      \"text_evidence\": \"we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red node P receiving input from green A and black NP align with proxies having additional unobserved causes?\",\n      \"answer\": \"Yes, the diagram shows P receives inputs from both protected attribute A and unobserved cause NP, where NP, NX, and A are pairwise independent as stated.\",\n      \"visual_anchor\": \"red node P with incoming edges from green node A and black node NP\",\n      \"text_evidence\": \"A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What structural change between left and right graphs illustrates the procedure for the example in Figure 3?\",\n      \"answer\": \"The right intervened graph removes the A→P edge while preserving other relationships, demonstrating the intervention step of the general procedure being illustrated through this example.\",\n      \"visual_anchor\": \"comparison between left template graph and right intervened graph structure\",\n      \"text_evidence\": \"While presenting the general procedure, we illustrate each step in the example shown in Figure 3\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Template graph with green node A, red node P, black nodes NP, NX, X, R",
        "Intervened graph on right with removed edges from green A to red P",
        "Directed edges showing causal relationships",
        "Gray node G label below R in both graphs"
      ],
      "queries": [
        {
          "query": "Why does the intervened graph remove the edge from green A to red P when proxy discrimination is being avoided?",
          "answer": "The benevolent viewpoint aims to guarantee that proxy P has no overall influence on prediction by adjusting P→R to cancel influence along P→X→R, requiring intervention on the A→P edge.",
          "visual_anchor": "removed edge from green node A to red node P in the right graph",
          "text_evidence": "we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red node P receiving input from green A and black NP align with proxies having additional unobserved causes?",
          "answer": "Yes, the diagram shows P receives inputs from both protected attribute A and unobserved cause NP, where NP, NX, and A are pairwise independent as stated.",
          "visual_anchor": "red node P with incoming edges from green node A and black node NP",
          "text_evidence": "A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent",
          "query_type": "value_context"
        },
        {
          "query": "What structural change between left and right graphs illustrates the procedure for the example in Figure 3?",
          "answer": "The right intervened graph removes the A→P edge while preserving other relationships, demonstrating the intervention step of the general procedure being illustrated through this example.",
          "visual_anchor": "comparison between left template graph and right intervened graph structure",
          "text_evidence": "While presenting the general procedure, we illustrate each step in the example shown in Figure 3",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the intervened graph remove the edge from green A to red P when proxy discrimination is being avoided?",
        "answer": "The benevolent viewpoint aims to guarantee that proxy P has no overall influence on prediction by adjusting P→R to cancel influence along P→X→R, requiring intervention on the A→P edge.",
        "visual_anchor": "removed edge from green node A to red node P in the right graph",
        "text_evidence": "we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red node P receiving input from green A and black NP align with proxies having additional unobserved causes?",
        "answer": "Yes, the diagram shows P receives inputs from both protected attribute A and unobserved cause NP, where NP, NX, and A are pairwise independent as stated.",
        "visual_anchor": "red node P with incoming edges from green node A and black node NP",
        "text_evidence": "A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent",
        "query_type": "value_context"
      },
      {
        "query": "What structural change between left and right graphs illustrates the procedure for the example in Figure 3?",
        "answer": "The right intervened graph removes the A→P edge while preserving other relationships, demonstrating the intervention step of the general procedure being illustrated through this example.",
        "visual_anchor": "comparison between left template graph and right intervened graph structure",
        "text_evidence": "While presenting the general procedure, we illustrate each step in the example shown in Figure 3",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Template graph with green node A, red node P, black nodes NP, NX, X, R",
      "Intervened graph on right with removed edges from green A to red P",
      "Directed edges showing causal relationships",
      "Gray node G label below R in both graphs"
    ]
  },
  {
    "doc_id": "1706.02744",
    "figure_id": "1706.02744_fig_5",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg",
    "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Template graph labeled \\\\tilde{\\\\mathcal{G}} on the left\",\n    \"Intervened graph labeled \\\\mathcal{G} on the right\",\n    \"Green node E with outgoing arrows to X and R in left graph\",\n    \"Green node E with interrupted paths (no outgoing arrows) in right graph\",\n    \"Red node A with arrow to X in left graph, arrow to R in right graph\",\n    \"Black arrows showing causal dependencies\",\n    \"Nodes N_E, N_X, X, R, A in both graphs\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the left graph show green E with arrows to both X and R while requiring A's influence on R to be canceled?\",\n      \"answer\": \"The skeptical viewpoint generically does not want A to influence R, but first intervenes on E to interrupt all paths through E, then cancels only the remaining influence from A to R.\",\n      \"visual_anchor\": \"Green node E with outgoing arrows to X and R in the left template graph \\\\tilde{\\\\mathcal{G}}\",\n      \"text_evidence\": \"While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the right graph's structure enable writing R_θ(P,X) using only root nodes from the intervened version?\",\n      \"answer\": \"By intervening on E (eliminating its outgoing arrows in the right graph), variables can be iteratively substituted until R_θ is expressed as R_θ(P, g(t\\\\bar{a}^{\\\\mathcal{G}}(X))) with only root nodes {A, P, N_X} remaining.\",\n      \"visual_anchor\": \"Right intervened graph \\\\mathcal{G} with green node E having no outgoing arrows\",\n      \"text_evidence\": \"Iteratively substitute variables in the equation for $R _ { \\\\theta }$ from their structural equations until only root nodes of the intervened graph are left, i.e. write $R _ { \\\\theta } ( P , X )$ as $R _ { \\\\theta } ( P , g ( t \\\\bar { a } ^ { \\\\mathcal { G } } ( X ) ) )$ for some function $g$ . In the example, $t \\\\bar { a ( X ) } = \\\\{ A , P , N _ { X } \\\\}$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What procedure modification does the right graph implement by fixing green E to a random variable η?\",\n      \"answer\": \"The procedure adjusts to avoid unresolved discrimination by intervening on E, fixing it to a random variable η with distribution matching E's marginal distribution in the template graph.\",\n      \"visual_anchor\": \"Green node E in the right intervened graph \\\\mathcal{G}\",\n      \"text_evidence\": \"Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination. 1. Intervene on $E$ by fixing it to a random variable $\\\\eta$ with $\\\\mathbb { P } ( \\\\eta ) = \\\\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\\\tilde { \\\\mathcal { G } }$\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Template graph labeled \\tilde{\\mathcal{G}} on the left",
        "Intervened graph labeled \\mathcal{G} on the right",
        "Green node E with outgoing arrows to X and R in left graph",
        "Green node E with interrupted paths (no outgoing arrows) in right graph",
        "Red node A with arrow to X in left graph, arrow to R in right graph",
        "Black arrows showing causal dependencies",
        "Nodes N_E, N_X, X, R, A in both graphs"
      ],
      "queries": [
        {
          "query": "Why does the left graph show green E with arrows to both X and R while requiring A's influence on R to be canceled?",
          "answer": "The skeptical viewpoint generically does not want A to influence R, but first intervenes on E to interrupt all paths through E, then cancels only the remaining influence from A to R.",
          "visual_anchor": "Green node E with outgoing arrows to X and R in the left template graph \\tilde{\\mathcal{G}}",
          "text_evidence": "While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the right graph's structure enable writing R_θ(P,X) using only root nodes from the intervened version?",
          "answer": "By intervening on E (eliminating its outgoing arrows in the right graph), variables can be iteratively substituted until R_θ is expressed as R_θ(P, g(t\\bar{a}^{\\mathcal{G}}(X))) with only root nodes {A, P, N_X} remaining.",
          "visual_anchor": "Right intervened graph \\mathcal{G} with green node E having no outgoing arrows",
          "text_evidence": "Iteratively substitute variables in the equation for $R _ { \\theta }$ from their structural equations until only root nodes of the intervened graph are left, i.e. write $R _ { \\theta } ( P , X )$ as $R _ { \\theta } ( P , g ( t \\bar { a } ^ { \\mathcal { G } } ( X ) ) )$ for some function $g$ . In the example, $t \\bar { a ( X ) } = \\{ A , P , N _ { X } \\}$",
          "query_type": "value_context"
        },
        {
          "query": "What procedure modification does the right graph implement by fixing green E to a random variable η?",
          "answer": "The procedure adjusts to avoid unresolved discrimination by intervening on E, fixing it to a random variable η with distribution matching E's marginal distribution in the template graph.",
          "visual_anchor": "Green node E in the right intervened graph \\mathcal{G}",
          "text_evidence": "Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination. 1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the left graph show green E with arrows to both X and R while requiring A's influence on R to be canceled?",
        "answer": "The skeptical viewpoint generically does not want A to influence R, but first intervenes on E to interrupt all paths through E, then cancels only the remaining influence from A to R.",
        "visual_anchor": "Green node E with outgoing arrows to X and R in the left template graph \\tilde{\\mathcal{G}}",
        "text_evidence": "While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the right graph's structure enable writing R_θ(P,X) using only root nodes from the intervened version?",
        "answer": "By intervening on E (eliminating its outgoing arrows in the right graph), variables can be iteratively substituted until R_θ is expressed as R_θ(P, g(t\\bar{a}^{\\mathcal{G}}(X))) with only root nodes {A, P, N_X} remaining.",
        "visual_anchor": "Right intervened graph \\mathcal{G} with green node E having no outgoing arrows",
        "text_evidence": "Iteratively substitute variables in the equation for $R _ { \\theta }$ from their structural equations until only root nodes of the intervened graph are left, i.e. write $R _ { \\theta } ( P , X )$ as $R _ { \\theta } ( P , g ( t \\bar { a } ^ { \\mathcal { G } } ( X ) ) )$ for some function $g$ . In the example, $t \\bar { a ( X ) } = \\{ A , P , N _ { X } \\}$",
        "query_type": "value_context"
      },
      {
        "query": "What procedure modification does the right graph implement by fixing green E to a random variable η?",
        "answer": "The procedure adjusts to avoid unresolved discrimination by intervening on E, fixing it to a random variable η with distribution matching E's marginal distribution in the template graph.",
        "visual_anchor": "Green node E in the right intervened graph \\mathcal{G}",
        "text_evidence": "Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination. 1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Template graph labeled \\tilde{\\mathcal{G}} on the left",
      "Intervened graph labeled \\mathcal{G} on the right",
      "Green node E with outgoing arrows to X and R in left graph",
      "Green node E with interrupted paths (no outgoing arrows) in right graph",
      "Red node A with arrow to X in left graph, arrow to R in right graph",
      "Black arrows showing causal dependencies",
      "Nodes N_E, N_X, X, R, A in both graphs"
    ]
  },
  {
    "doc_id": "1706.02744",
    "figure_id": "1706.02744_fig_6",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg",
    "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Node A with solid arrow to P\",\n    \"Node P with dashed arrow to DAG\",\n    \"Node R with solid arrow from X\",\n    \"Node X with dashed arrow to DAG\",\n    \"Dashed arrows between nodes\",\n    \"Circle labeled DAG at bottom\",\n    \"Label 'G tilde' on left side\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why must the solid arrow from A to P remain when requiring distribution of R_θ to be independent of p?\",\n      \"answer\": \"The solid arrow from A to P represents a direct causal path that must be considered when ensuring R_θ's distribution independence from p, as this structural relationship affects how the predictor separates P from other roots in the hypothesis class specification.\",\n      \"visual_anchor\": \"solid arrow from node A to node P\",\n      \"text_evidence\": \"We seek to write the predictor as a function of P and all the other roots of G separately. If our hypothesis class is such that there exists θ̃ such that R_θ(P,g(ta(X)))=R_θ̃(P,g̃(ta(X)∖{P})), we call the structural equation model and hypothesis class spec\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the dashed arrow from P to DAG enable blocking all directed paths from ancestors of P to X?\",\n      \"answer\": \"The dashed arrow from P to DAG indicates optional paths that can be blocked by P. When P blocks all directed paths from its ancestors to X, predictors using adjusted features X̃:=X-E[X|P] exhibit no proxy discrimination.\",\n      \"visual_anchor\": \"dashed arrow from node P to DAG circle\",\n      \"text_evidence\": \"Under the assumptions of Theorem 2, if all directed paths from any ancestor of P to X in the graph G are blocked by P, then any predictor based on the adjusted features X̃:=X-E[X|P] exhibits no proxy discrimination\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the dashed arrow from X to DAG matter for individual proxy discrimination comparing examples with same features X?\",\n      \"answer\": \"The dashed arrow from X to DAG represents potential paths affecting unobserved non-feature variables. Individual proxy discrimination compares examples with identical X but different P values, which can involve different unobserved variables connected through this DAG structure.\",\n      \"visual_anchor\": \"dashed arrow from node X to DAG circle\",\n      \"text_evidence\": \"Individual proxy discrimination aims at comparing examples with the same features X, for different values of P. Note that this can be individuals with different values for the unobserved non-feature variables.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A with solid arrow to P",
        "Node P with dashed arrow to DAG",
        "Node R with solid arrow from X",
        "Node X with dashed arrow to DAG",
        "Dashed arrows between nodes",
        "Circle labeled DAG at bottom",
        "Label 'G tilde' on left side"
      ],
      "queries": [
        {
          "query": "Why must the solid arrow from A to P remain when requiring distribution of R_θ to be independent of p?",
          "answer": "The solid arrow from A to P represents a direct causal path that must be considered when ensuring R_θ's distribution independence from p, as this structural relationship affects how the predictor separates P from other roots in the hypothesis class specification.",
          "visual_anchor": "solid arrow from node A to node P",
          "text_evidence": "We seek to write the predictor as a function of P and all the other roots of G separately. If our hypothesis class is such that there exists θ̃ such that R_θ(P,g(ta(X)))=R_θ̃(P,g̃(ta(X)∖{P})), we call the structural equation model and hypothesis class spec",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the dashed arrow from P to DAG enable blocking all directed paths from ancestors of P to X?",
          "answer": "The dashed arrow from P to DAG indicates optional paths that can be blocked by P. When P blocks all directed paths from its ancestors to X, predictors using adjusted features X̃:=X-E[X|P] exhibit no proxy discrimination.",
          "visual_anchor": "dashed arrow from node P to DAG circle",
          "text_evidence": "Under the assumptions of Theorem 2, if all directed paths from any ancestor of P to X in the graph G are blocked by P, then any predictor based on the adjusted features X̃:=X-E[X|P] exhibits no proxy discrimination",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the dashed arrow from X to DAG matter for individual proxy discrimination comparing examples with same features X?",
          "answer": "The dashed arrow from X to DAG represents potential paths affecting unobserved non-feature variables. Individual proxy discrimination compares examples with identical X but different P values, which can involve different unobserved variables connected through this DAG structure.",
          "visual_anchor": "dashed arrow from node X to DAG circle",
          "text_evidence": "Individual proxy discrimination aims at comparing examples with the same features X, for different values of P. Note that this can be individuals with different values for the unobserved non-feature variables.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why must the solid arrow from A to P remain when requiring distribution of R_θ to be independent of p?",
        "answer": "The solid arrow from A to P represents a direct causal path that must be considered when ensuring R_θ's distribution independence from p, as this structural relationship affects how the predictor separates P from other roots in the hypothesis class specification.",
        "visual_anchor": "solid arrow from node A to node P",
        "text_evidence": "We seek to write the predictor as a function of P and all the other roots of G separately. If our hypothesis class is such that there exists θ̃ such that R_θ(P,g(ta(X)))=R_θ̃(P,g̃(ta(X)∖{P})), we call the structural equation model and hypothesis class spec",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the dashed arrow from P to DAG enable blocking all directed paths from ancestors of P to X?",
        "answer": "The dashed arrow from P to DAG indicates optional paths that can be blocked by P. When P blocks all directed paths from its ancestors to X, predictors using adjusted features X̃:=X-E[X|P] exhibit no proxy discrimination.",
        "visual_anchor": "dashed arrow from node P to DAG circle",
        "text_evidence": "Under the assumptions of Theorem 2, if all directed paths from any ancestor of P to X in the graph G are blocked by P, then any predictor based on the adjusted features X̃:=X-E[X|P] exhibits no proxy discrimination",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the dashed arrow from X to DAG matter for individual proxy discrimination comparing examples with same features X?",
        "answer": "The dashed arrow from X to DAG represents potential paths affecting unobserved non-feature variables. Individual proxy discrimination compares examples with identical X but different P values, which can involve different unobserved variables connected through this DAG structure.",
        "visual_anchor": "dashed arrow from node X to DAG circle",
        "text_evidence": "Individual proxy discrimination aims at comparing examples with the same features X, for different values of P. Note that this can be individuals with different values for the unobserved non-feature variables.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A with solid arrow to P",
      "Node P with dashed arrow to DAG",
      "Node R with solid arrow from X",
      "Node X with dashed arrow to DAG",
      "Dashed arrows between nodes",
      "Circle labeled DAG at bottom",
      "Label 'G tilde' on left side"
    ]
  },
  {
    "doc_id": "1707.00574",
    "figure_id": "1707.00574_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg",
    "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Heatmap in panel (a) showing average quality q with color gradient from yellow (low) to dark purple/black (high)\",\n    \"Y-axis labeled α ranging from 0.0 to 3.0\",\n    \"X-axis labeled β ranging from 0.0 to 1.0\",\n    \"Color bar labeled T at top showing gradient from 0.0 to 1.0\",\n    \"Dark purple/black region concentrated in upper-right corner at high α and high β values\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dark purple region shift leftward toward lower β values as α increases from 1.0 to 3.0?\",\n      \"answer\": \"The maximum average quality q occurs at approximately β=0.4 when α=1, but for α=3 the maximum shifts to a lower β value, indicating that higher popularity bias requires reduced β to maintain optimal quality.\",\n      \"visual_anchor\": \"Dark purple region in heatmap showing maximum q location shifting from center-right (α=1.0) to left (α=3.0)\",\n      \"text_evidence\": \"Heatmap of average quality $q$ as a function of $\\\\alpha$ and $\\\\beta$ , showing that $q$ reaches a maximum for $\\\\alpha = 1$ and $\\\\beta \\\\approx 0 . 4$ , while for $\\\\alpha = 3$ the maximum is attained for a lower $\\\\beta$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the predominantly yellow coloring across most β values at low α support algorithm faithfulness optimization?\",\n      \"answer\": \"The yellow coloring indicates lower quality values. Panel (c) shows faithfulness τ, which is a separate metric from quality q shown in panel (a), so the yellow region reflects quality rather than faithfulness performance.\",\n      \"visual_anchor\": \"Yellow-colored region dominating the heatmap at α values between 0.0 and 1.5 across most β values\",\n      \"text_evidence\": \"Faithfulness $\\\\tau$ of the algorithm as a function of $\\\\alpha$ and $\\\\beta$ . ( $d$ ) $\\\\tau$ as a function of $\\\\beta$ for the same three values of $\\\\alpha$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the sharp transition from yellow to dark purple at the upper-right corner near α=3.0 and β=1.0?\",\n      \"answer\": \"This represents a dramatic increase in average quality q at extreme parameter combinations. The location of maximum q depends on α, with the relationship between these parameters determining optimal quality as popularity bias intensifies.\",\n      \"visual_anchor\": \"Abrupt color change from yellow to dark purple/black in top-right corner at α≈3.0, β≈1.0\",\n      \"text_evidence\": \"The location of the maximum $q$ as a function of $\\\\beta$ depends on $\\\\alpha$ , here shown for $\\\\alpha = 0 , 0 . 5 , 1 . 0$\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Heatmap in panel (a) showing average quality q with color gradient from yellow (low) to dark purple/black (high)",
        "Y-axis labeled α ranging from 0.0 to 3.0",
        "X-axis labeled β ranging from 0.0 to 1.0",
        "Color bar labeled T at top showing gradient from 0.0 to 1.0",
        "Dark purple/black region concentrated in upper-right corner at high α and high β values"
      ],
      "queries": [
        {
          "query": "Why does the dark purple region shift leftward toward lower β values as α increases from 1.0 to 3.0?",
          "answer": "The maximum average quality q occurs at approximately β=0.4 when α=1, but for α=3 the maximum shifts to a lower β value, indicating that higher popularity bias requires reduced β to maintain optimal quality.",
          "visual_anchor": "Dark purple region in heatmap showing maximum q location shifting from center-right (α=1.0) to left (α=3.0)",
          "text_evidence": "Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the predominantly yellow coloring across most β values at low α support algorithm faithfulness optimization?",
          "answer": "The yellow coloring indicates lower quality values. Panel (c) shows faithfulness τ, which is a separate metric from quality q shown in panel (a), so the yellow region reflects quality rather than faithfulness performance.",
          "visual_anchor": "Yellow-colored region dominating the heatmap at α values between 0.0 and 1.5 across most β values",
          "text_evidence": "Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$",
          "query_type": "value_context"
        },
        {
          "query": "What explains the sharp transition from yellow to dark purple at the upper-right corner near α=3.0 and β=1.0?",
          "answer": "This represents a dramatic increase in average quality q at extreme parameter combinations. The location of maximum q depends on α, with the relationship between these parameters determining optimal quality as popularity bias intensifies.",
          "visual_anchor": "Abrupt color change from yellow to dark purple/black in top-right corner at α≈3.0, β≈1.0",
          "text_evidence": "The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dark purple region shift leftward toward lower β values as α increases from 1.0 to 3.0?",
        "answer": "The maximum average quality q occurs at approximately β=0.4 when α=1, but for α=3 the maximum shifts to a lower β value, indicating that higher popularity bias requires reduced β to maintain optimal quality.",
        "visual_anchor": "Dark purple region in heatmap showing maximum q location shifting from center-right (α=1.0) to left (α=3.0)",
        "text_evidence": "Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the predominantly yellow coloring across most β values at low α support algorithm faithfulness optimization?",
        "answer": "The yellow coloring indicates lower quality values. Panel (c) shows faithfulness τ, which is a separate metric from quality q shown in panel (a), so the yellow region reflects quality rather than faithfulness performance.",
        "visual_anchor": "Yellow-colored region dominating the heatmap at α values between 0.0 and 1.5 across most β values",
        "text_evidence": "Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$",
        "query_type": "value_context"
      },
      {
        "query": "What explains the sharp transition from yellow to dark purple at the upper-right corner near α=3.0 and β=1.0?",
        "answer": "This represents a dramatic increase in average quality q at extreme parameter combinations. The location of maximum q depends on α, with the relationship between these parameters determining optimal quality as popularity bias intensifies.",
        "visual_anchor": "Abrupt color change from yellow to dark purple/black in top-right corner at α≈3.0, β≈1.0",
        "text_evidence": "The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Heatmap in panel (a) showing average quality q with color gradient from yellow (low) to dark purple/black (high)",
      "Y-axis labeled α ranging from 0.0 to 3.0",
      "X-axis labeled β ranging from 0.0 to 1.0",
      "Color bar labeled T at top showing gradient from 0.0 to 1.0",
      "Dark purple/black region concentrated in upper-right corner at high α and high β values"
    ]
  },
  {
    "doc_id": "1707.00574",
    "figure_id": "1707.00574_fig_5",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg",
    "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red curve with error bars for α=1\",\n    \"Black curve with error bars for α=2\",\n    \"X-axis showing Time from 10³ to 10⁶\",\n    \"Y-axis showing average quality q̄ from 0.56 to 0.68\",\n    \"Both curves converge around 0.67\",\n    \"Label β=0.1 in lower right\",\n    \"Red curve reaches higher values earlier than black curve\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black curve for lower exploration converge earlier than the red one, supporting sub-optimal outcomes?\",\n      \"answer\": \"With α=2 (lower exploration), the system converges early to sub-optimal quality around 0.67, while α=1 (higher exploration) allows continued quality improvement, demonstrating that reduced exploration traps the system in local optima.\",\n      \"visual_anchor\": \"Black curve (α=2) plateaus before 10⁴ while red curve (α=1) continues rising\",\n      \"text_evidence\": \"With less exploration the system converges early to sub-optimal quality.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red curve reaching 0.67 at 10⁵ reflect difficulty in establishing inherent quality in cultural markets?\",\n      \"answer\": \"Yes, the gradual convergence of the α=1 curve demonstrates that systems must explore extensively over time to approach optimal quality when inherent product quality is difficult to establish objectively.\",\n      \"visual_anchor\": \"Red curve (α=1) gradually approaching 0.67 at time 10⁵\",\n      \"text_evidence\": \"The inherent quality of cultural products is often difficult to establish, therefore relying on measurable quantitative features like the popularity of an item is hugely advantageous\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the popularity bias β=0.1 relate to the structural features causing both curves to plateau?\",\n      \"answer\": \"The β=0.1 popularity bias creates competition for limited attention, which constrains both exploration levels from achieving higher quality beyond 0.67, reflecting structural limitations in cultural markets where popularity competes with quality discovery.\",\n      \"visual_anchor\": \"Both curves plateau near 0.67 with β=0.1 label shown\",\n      \"text_evidence\": \"Success in these markets may strongly depend on structural or subjective features, like competition for limited attention\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red curve with error bars for α=1",
        "Black curve with error bars for α=2",
        "X-axis showing Time from 10³ to 10⁶",
        "Y-axis showing average quality q̄ from 0.56 to 0.68",
        "Both curves converge around 0.67",
        "Label β=0.1 in lower right",
        "Red curve reaches higher values earlier than black curve"
      ],
      "queries": [
        {
          "query": "Why does the black curve for lower exploration converge earlier than the red one, supporting sub-optimal outcomes?",
          "answer": "With α=2 (lower exploration), the system converges early to sub-optimal quality around 0.67, while α=1 (higher exploration) allows continued quality improvement, demonstrating that reduced exploration traps the system in local optima.",
          "visual_anchor": "Black curve (α=2) plateaus before 10⁴ while red curve (α=1) continues rising",
          "text_evidence": "With less exploration the system converges early to sub-optimal quality.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red curve reaching 0.67 at 10⁵ reflect difficulty in establishing inherent quality in cultural markets?",
          "answer": "Yes, the gradual convergence of the α=1 curve demonstrates that systems must explore extensively over time to approach optimal quality when inherent product quality is difficult to establish objectively.",
          "visual_anchor": "Red curve (α=1) gradually approaching 0.67 at time 10⁵",
          "text_evidence": "The inherent quality of cultural products is often difficult to establish, therefore relying on measurable quantitative features like the popularity of an item is hugely advantageous",
          "query_type": "value_context"
        },
        {
          "query": "How does the popularity bias β=0.1 relate to the structural features causing both curves to plateau?",
          "answer": "The β=0.1 popularity bias creates competition for limited attention, which constrains both exploration levels from achieving higher quality beyond 0.67, reflecting structural limitations in cultural markets where popularity competes with quality discovery.",
          "visual_anchor": "Both curves plateau near 0.67 with β=0.1 label shown",
          "text_evidence": "Success in these markets may strongly depend on structural or subjective features, like competition for limited attention",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black curve for lower exploration converge earlier than the red one, supporting sub-optimal outcomes?",
        "answer": "With α=2 (lower exploration), the system converges early to sub-optimal quality around 0.67, while α=1 (higher exploration) allows continued quality improvement, demonstrating that reduced exploration traps the system in local optima.",
        "visual_anchor": "Black curve (α=2) plateaus before 10⁴ while red curve (α=1) continues rising",
        "text_evidence": "With less exploration the system converges early to sub-optimal quality.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red curve reaching 0.67 at 10⁵ reflect difficulty in establishing inherent quality in cultural markets?",
        "answer": "Yes, the gradual convergence of the α=1 curve demonstrates that systems must explore extensively over time to approach optimal quality when inherent product quality is difficult to establish objectively.",
        "visual_anchor": "Red curve (α=1) gradually approaching 0.67 at time 10⁵",
        "text_evidence": "The inherent quality of cultural products is often difficult to establish, therefore relying on measurable quantitative features like the popularity of an item is hugely advantageous",
        "query_type": "value_context"
      },
      {
        "query": "How does the popularity bias β=0.1 relate to the structural features causing both curves to plateau?",
        "answer": "The β=0.1 popularity bias creates competition for limited attention, which constrains both exploration levels from achieving higher quality beyond 0.67, reflecting structural limitations in cultural markets where popularity competes with quality discovery.",
        "visual_anchor": "Both curves plateau near 0.67 with β=0.1 label shown",
        "text_evidence": "Success in these markets may strongly depend on structural or subjective features, like competition for limited attention",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red curve with error bars for α=1",
      "Black curve with error bars for α=2",
      "X-axis showing Time from 10³ to 10⁶",
      "Y-axis showing average quality q̄ from 0.56 to 0.68",
      "Both curves converge around 0.67",
      "Label β=0.1 in lower right",
      "Red curve reaches higher values earlier than black curve"
    ]
  },
  {
    "doc_id": "1707.09457",
    "figure_id": "1707.09457_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg",
    "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Five cooking images with semantic role tables\",\n    \"AGENT role filled with WOMAN in first four images\",\n    \"AGENT role filled with MAN in fifth image\",\n    \"ROLE-VALUE pairs showing FOOD, HEAT, TOOL, PLACE attributes\",\n    \"Orange box highlighting AGENT-WOMAN pair in fourth image\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the fifth image show MAN as agent when training data contains 33% men in cooking roles?\",\n      \"answer\": \"The fifth image represents the minority case in the imSitu dataset where men appear as cooking agents. This 33% training distribution becomes amplified to only 16% after CRF training, demonstrating bias amplification.\",\n      \"visual_anchor\": \"Fifth image table showing AGENT-MAN pair\",\n      \"text_evidence\": \"In the imSitu training set, $33 \\\\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\\\%$ of agent roles in cooking images.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the orange-boxed AGENT-WOMAN pair in the fourth image relate to bias amplification from 67% to 84%?\",\n      \"answer\": \"The highlighted AGENT-WOMAN pair represents the majority class that becomes over-represented after CRF training. The original 67% (100%-33%) of women agents amplifies to 84% (100%-16%), showing the model's tendency to reinforce existing biases.\",\n      \"visual_anchor\": \"Orange box highlighting AGENT-WOMAN in fourth image table\",\n      \"text_evidence\": \"After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\\\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the SPATULA tool value appearing in multiple image tables support predictions based on cooking activity correlations?\",\n      \"answer\": \"Yes, spatula appears as the TOOL value in multiple cooking images, demonstrating that structured prediction models can leverage correlations between activities and common tools. This illustrates how co-occurring labels help recognition but may also encode biases.\",\n      \"visual_anchor\": \"TOOL-SPATULA pairs visible in first and fifth image tables\",\n      \"text_evidence\": \"Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Five cooking images with semantic role tables",
        "AGENT role filled with WOMAN in first four images",
        "AGENT role filled with MAN in fifth image",
        "ROLE-VALUE pairs showing FOOD, HEAT, TOOL, PLACE attributes",
        "Orange box highlighting AGENT-WOMAN pair in fourth image"
      ],
      "queries": [
        {
          "query": "Why does the fifth image show MAN as agent when training data contains 33% men in cooking roles?",
          "answer": "The fifth image represents the minority case in the imSitu dataset where men appear as cooking agents. This 33% training distribution becomes amplified to only 16% after CRF training, demonstrating bias amplification.",
          "visual_anchor": "Fifth image table showing AGENT-MAN pair",
          "text_evidence": "In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the orange-boxed AGENT-WOMAN pair in the fourth image relate to bias amplification from 67% to 84%?",
          "answer": "The highlighted AGENT-WOMAN pair represents the majority class that becomes over-represented after CRF training. The original 67% (100%-33%) of women agents amplifies to 84% (100%-16%), showing the model's tendency to reinforce existing biases.",
          "visual_anchor": "Orange box highlighting AGENT-WOMAN in fourth image table",
          "text_evidence": "After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the SPATULA tool value appearing in multiple image tables support predictions based on cooking activity correlations?",
          "answer": "Yes, spatula appears as the TOOL value in multiple cooking images, demonstrating that structured prediction models can leverage correlations between activities and common tools. This illustrates how co-occurring labels help recognition but may also encode biases.",
          "visual_anchor": "TOOL-SPATULA pairs visible in first and fifth image tables",
          "text_evidence": "Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the fifth image show MAN as agent when training data contains 33% men in cooking roles?",
        "answer": "The fifth image represents the minority case in the imSitu dataset where men appear as cooking agents. This 33% training distribution becomes amplified to only 16% after CRF training, demonstrating bias amplification.",
        "visual_anchor": "Fifth image table showing AGENT-MAN pair",
        "text_evidence": "In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the orange-boxed AGENT-WOMAN pair in the fourth image relate to bias amplification from 67% to 84%?",
        "answer": "The highlighted AGENT-WOMAN pair represents the majority class that becomes over-represented after CRF training. The original 67% (100%-33%) of women agents amplifies to 84% (100%-16%), showing the model's tendency to reinforce existing biases.",
        "visual_anchor": "Orange box highlighting AGENT-WOMAN in fourth image table",
        "text_evidence": "After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the SPATULA tool value appearing in multiple image tables support predictions based on cooking activity correlations?",
        "answer": "Yes, spatula appears as the TOOL value in multiple cooking images, demonstrating that structured prediction models can leverage correlations between activities and common tools. This illustrates how co-occurring labels help recognition but may also encode biases.",
        "visual_anchor": "TOOL-SPATULA pairs visible in first and fifth image tables",
        "text_evidence": "Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Five cooking images with semantic role tables",
      "AGENT role filled with WOMAN in first four images",
      "AGENT role filled with MAN in fifth image",
      "ROLE-VALUE pairs showing FOOD, HEAT, TOOL, PLACE attributes",
      "Orange box highlighting AGENT-WOMAN pair in fourth image"
    ]
  },
  {
    "doc_id": "1707.09457",
    "figure_id": "1707.09457_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg",
    "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"scatter plot with red dots\", \"blue diagonal trend line\", \"x-axis: training gender ratio\", \"y-axis: predicted gender ratio\", \"labeled points: shopping, cooking, serving, microwaving, washing, combing, twisting at lower left\", \"labeled points: tuning, coaching, shoveling, shooting, aiming, pumping, driving at upper right\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does shopping appear near 0.2 on both axes when the dataset shows 64.6% of verbs favor males?\",\n      \"answer\": \"Shopping is one of the few female-biased verbs in the dataset. While 64.6% of verbs overall favor males with average bias of 0.707, shopping is positioned at approximately 0.2 training bias, indicating it strongly favors females (values near zero indicate bias toward women).\",\n      \"visual_anchor\": \"shopping label positioned at approximately (0.2, 0.2) in lower left quadrant\",\n      \"text_evidence\": \"Overall, the dataset is heavily biased toward male agents, with $6 4 . 6 \\\\%$ of verbs favoring a male agent by an average bias of 0.707 (roughly 3:1 male).\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do verbs like tuning and coaching above the diagonal line demonstrate that models amplify existing gender bias?\",\n      \"answer\": \"Yes, verbs positioned above the diagonal line (like tuning and coaching at approximately 0.95 predicted ratio versus 0.85-0.90 training ratio) show the model predicting higher male bias than present in training data, confirming bias amplification. This supports the hypothesis that models trained on biased data amplify the existing gender bias.\",\n      \"visual_anchor\": \"tuning and coaching labels positioned above the blue diagonal trend line in upper right\",\n      \"text_evidence\": \"We confirmed our hypothesis that (a) both the im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased and that (b) models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the clustering of cooking, serving, microwaving, and washing below 0.4 support the claim about extremely biased verbs?\",\n      \"answer\": \"Yes, these verbs clustered below 0.4 on the training axis represent extremely female-biased categories (values near zero favor women). This supports the finding that 46.95% of verbs favor a gender with a bias of at least 0.7, showing polarized gender associations in both male and female directions.\",\n      \"visual_anchor\": \"cooking, serving, microwaving, washing labels clustered between 0.25-0.35 on x-axis in lower portion\",\n      \"text_evidence\": \"Nearly half of verbs are extremely biased in the male or female direction: $4 6 . 9 5 \\\\%$ of verbs favor a gender with a bias of at least 0.7.6\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "scatter plot with red dots",
        "blue diagonal trend line",
        "x-axis: training gender ratio",
        "y-axis: predicted gender ratio",
        "labeled points: shopping, cooking, serving, microwaving, washing, combing, twisting at lower left",
        "labeled points: tuning, coaching, shoveling, shooting, aiming, pumping, driving at upper right"
      ],
      "queries": [
        {
          "query": "Why does shopping appear near 0.2 on both axes when the dataset shows 64.6% of verbs favor males?",
          "answer": "Shopping is one of the few female-biased verbs in the dataset. While 64.6% of verbs overall favor males with average bias of 0.707, shopping is positioned at approximately 0.2 training bias, indicating it strongly favors females (values near zero indicate bias toward women).",
          "visual_anchor": "shopping label positioned at approximately (0.2, 0.2) in lower left quadrant",
          "text_evidence": "Overall, the dataset is heavily biased toward male agents, with $6 4 . 6 \\%$ of verbs favoring a male agent by an average bias of 0.707 (roughly 3:1 male).",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do verbs like tuning and coaching above the diagonal line demonstrate that models amplify existing gender bias?",
          "answer": "Yes, verbs positioned above the diagonal line (like tuning and coaching at approximately 0.95 predicted ratio versus 0.85-0.90 training ratio) show the model predicting higher male bias than present in training data, confirming bias amplification. This supports the hypothesis that models trained on biased data amplify the existing gender bias.",
          "visual_anchor": "tuning and coaching labels positioned above the blue diagonal trend line in upper right",
          "text_evidence": "We confirmed our hypothesis that (a) both the im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased and that (b) models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the clustering of cooking, serving, microwaving, and washing below 0.4 support the claim about extremely biased verbs?",
          "answer": "Yes, these verbs clustered below 0.4 on the training axis represent extremely female-biased categories (values near zero favor women). This supports the finding that 46.95% of verbs favor a gender with a bias of at least 0.7, showing polarized gender associations in both male and female directions.",
          "visual_anchor": "cooking, serving, microwaving, washing labels clustered between 0.25-0.35 on x-axis in lower portion",
          "text_evidence": "Nearly half of verbs are extremely biased in the male or female direction: $4 6 . 9 5 \\%$ of verbs favor a gender with a bias of at least 0.7.6",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does shopping appear near 0.2 on both axes when the dataset shows 64.6% of verbs favor males?",
        "answer": "Shopping is one of the few female-biased verbs in the dataset. While 64.6% of verbs overall favor males with average bias of 0.707, shopping is positioned at approximately 0.2 training bias, indicating it strongly favors females (values near zero indicate bias toward women).",
        "visual_anchor": "shopping label positioned at approximately (0.2, 0.2) in lower left quadrant",
        "text_evidence": "Overall, the dataset is heavily biased toward male agents, with $6 4 . 6 \\%$ of verbs favoring a male agent by an average bias of 0.707 (roughly 3:1 male).",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do verbs like tuning and coaching above the diagonal line demonstrate that models amplify existing gender bias?",
        "answer": "Yes, verbs positioned above the diagonal line (like tuning and coaching at approximately 0.95 predicted ratio versus 0.85-0.90 training ratio) show the model predicting higher male bias than present in training data, confirming bias amplification. This supports the hypothesis that models trained on biased data amplify the existing gender bias.",
        "visual_anchor": "tuning and coaching labels positioned above the blue diagonal trend line in upper right",
        "text_evidence": "We confirmed our hypothesis that (a) both the im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased and that (b) models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the clustering of cooking, serving, microwaving, and washing below 0.4 support the claim about extremely biased verbs?",
        "answer": "Yes, these verbs clustered below 0.4 on the training axis represent extremely female-biased categories (values near zero favor women). This supports the finding that 46.95% of verbs favor a gender with a bias of at least 0.7, showing polarized gender associations in both male and female directions.",
        "visual_anchor": "cooking, serving, microwaving, washing labels clustered between 0.25-0.35 on x-axis in lower portion",
        "text_evidence": "Nearly half of verbs are extremely biased in the male or female direction: $4 6 . 9 5 \\%$ of verbs favor a gender with a bias of at least 0.7.6",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "scatter plot with red dots",
      "blue diagonal trend line",
      "x-axis: training gender ratio",
      "y-axis: predicted gender ratio",
      "labeled points: shopping, cooking, serving, microwaving, washing, combing, twisting at lower left",
      "labeled points: tuning, coaching, shoveling, shooting, aiming, pumping, driving at upper right"
    ]
  },
  {
    "doc_id": "1707.09457",
    "figure_id": "1707.09457_fig_7",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg",
    "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"red dots representing violations\", \"green dots representing points meeting margin\", \"dotted blue lines indicating 0.05 margin\", \"x-axis: training gender ratio\", \"y-axis: predicted gender ratio\", \"solid diagonal line\"],\n  \"queries\": [\n    {\n      \"query\": \"How does the 0.05 margin shown by dotted blue lines relate to the 40% reduction in violations?\",\n      \"answer\": \"The dotted blue lines define the acceptable bias range. RBA reduced violations (red dots exceeding this margin) by 40%, demonstrating successful constraint enforcement.\",\n      \"visual_anchor\": \"dotted blue lines indicating 0.05 margin, with red dots as violations\",\n      \"text_evidence\": \"Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\\\%$ , by $40 \\\\%$ (Viol.).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do green dots dominate the upper-right region while red dots cluster at lower training ratios?\",\n      \"answer\": \"RBA successfully reduces bias amplification across all initial training biases. Lower training ratios initially showed more violations, but after RBA application, most points (green) meet the margin constraint.\",\n      \"visual_anchor\": \"green dots in upper-right region versus red dots at lower x-axis values\",\n      \"text_evidence\": \"Across all initial training biases, RBA is able to reduce the bias amplification.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the shift from red to green dots support the 39% decrease in training set distribution distance?\",\n      \"answer\": \"Yes, the transition from red violations to green compliant points visually demonstrates reduced bias amplification. This corresponds to the overall 39% decrease in distance to the training distribution after applying RBA.\",\n      \"visual_anchor\": \"red dots violating margin versus green dots meeting margin\",\n      \"text_evidence\": \"In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\\\%$ .\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "red dots representing violations",
        "green dots representing points meeting margin",
        "dotted blue lines indicating 0.05 margin",
        "x-axis: training gender ratio",
        "y-axis: predicted gender ratio",
        "solid diagonal line"
      ],
      "queries": [
        {
          "query": "How does the 0.05 margin shown by dotted blue lines relate to the 40% reduction in violations?",
          "answer": "The dotted blue lines define the acceptable bias range. RBA reduced violations (red dots exceeding this margin) by 40%, demonstrating successful constraint enforcement.",
          "visual_anchor": "dotted blue lines indicating 0.05 margin, with red dots as violations",
          "text_evidence": "Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do green dots dominate the upper-right region while red dots cluster at lower training ratios?",
          "answer": "RBA successfully reduces bias amplification across all initial training biases. Lower training ratios initially showed more violations, but after RBA application, most points (green) meet the margin constraint.",
          "visual_anchor": "green dots in upper-right region versus red dots at lower x-axis values",
          "text_evidence": "Across all initial training biases, RBA is able to reduce the bias amplification.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the shift from red to green dots support the 39% decrease in training set distribution distance?",
          "answer": "Yes, the transition from red violations to green compliant points visually demonstrates reduced bias amplification. This corresponds to the overall 39% decrease in distance to the training distribution after applying RBA.",
          "visual_anchor": "red dots violating margin versus green dots meeting margin",
          "text_evidence": "In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\%$ .",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the 0.05 margin shown by dotted blue lines relate to the 40% reduction in violations?",
        "answer": "The dotted blue lines define the acceptable bias range. RBA reduced violations (red dots exceeding this margin) by 40%, demonstrating successful constraint enforcement.",
        "visual_anchor": "dotted blue lines indicating 0.05 margin, with red dots as violations",
        "text_evidence": "Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do green dots dominate the upper-right region while red dots cluster at lower training ratios?",
        "answer": "RBA successfully reduces bias amplification across all initial training biases. Lower training ratios initially showed more violations, but after RBA application, most points (green) meet the margin constraint.",
        "visual_anchor": "green dots in upper-right region versus red dots at lower x-axis values",
        "text_evidence": "Across all initial training biases, RBA is able to reduce the bias amplification.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the shift from red to green dots support the 39% decrease in training set distribution distance?",
        "answer": "Yes, the transition from red violations to green compliant points visually demonstrates reduced bias amplification. This corresponds to the overall 39% decrease in distance to the training distribution after applying RBA.",
        "visual_anchor": "red dots violating margin versus green dots meeting margin",
        "text_evidence": "In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\%$ .",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "red dots representing violations",
      "green dots representing points meeting margin",
      "dotted blue lines indicating 0.05 margin",
      "x-axis: training gender ratio",
      "y-axis: predicted gender ratio",
      "solid diagonal line"
    ]
  },
  {
    "doc_id": "1709.02012",
    "figure_id": "1709.02012_fig_3",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg",
    "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Black triangular boundary from top-left to bottom-right\",\n    \"Blue line labeled h₁\",\n    \"Red/orange line labeled h₂\",\n    \"Horizontal dashed line intersecting both h₁ and h₂\",\n    \"Gray shaded triangular region\",\n    \"Origin point labeled O\",\n    \"Axes labeled 'Generalized FP Rate' and 'Generalized FN Rate'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue line h₁ start at the origin while representing a trivial classifier μ₁?\",\n      \"answer\": \"The blue line represents the trivial classifier h^μ₁, which starts at the origin (0,0) in the false-positive/false-negative plane, indicating zero error rates when the classifier always predicts according to the base rate of group 1.\",\n      \"visual_anchor\": \"Blue line h₁ originating at point O (origin)\",\n      \"text_evidence\": \"Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. h^μ₁, h^μ₂ are trivial classifiers.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the calibrated classifier sets H₁* and H₂* both lie on the black diagonal boundary?\",\n      \"answer\": \"Yes, the sets H₁* and H₂* of calibrated classifiers for the two groups both lie on the black diagonal boundary, representing the Pareto frontier of achievable false-positive and false-negative trade-offs for each group.\",\n      \"visual_anchor\": \"Black diagonal line from top-left to bottom-right\",\n      \"text_evidence\": \"H₁*, H₂* are the set of cal. classifiers for the two groups\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the horizontal dashed line crossing h₁ and h₂ represent an equal-cost constraint across both groups?\",\n      \"answer\": \"Yes, the horizontal dashed line represents an equal-cost constraint, intersecting both the blue line h₁ and red line h₂ at the same generalized false-negative rate, enforcing fairness by equalizing costs across the two groups.\",\n      \"visual_anchor\": \"Horizontal dashed line intersecting blue h₁ and red h₂\",\n      \"text_evidence\": \"Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black triangular boundary from top-left to bottom-right",
        "Blue line labeled h₁",
        "Red/orange line labeled h₂",
        "Horizontal dashed line intersecting both h₁ and h₂",
        "Gray shaded triangular region",
        "Origin point labeled O",
        "Axes labeled 'Generalized FP Rate' and 'Generalized FN Rate'"
      ],
      "queries": [
        {
          "query": "Why does the blue line h₁ start at the origin while representing a trivial classifier μ₁?",
          "answer": "The blue line represents the trivial classifier h^μ₁, which starts at the origin (0,0) in the false-positive/false-negative plane, indicating zero error rates when the classifier always predicts according to the base rate of group 1.",
          "visual_anchor": "Blue line h₁ originating at point O (origin)",
          "text_evidence": "Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. h^μ₁, h^μ₂ are trivial classifiers.",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the calibrated classifier sets H₁* and H₂* both lie on the black diagonal boundary?",
          "answer": "Yes, the sets H₁* and H₂* of calibrated classifiers for the two groups both lie on the black diagonal boundary, representing the Pareto frontier of achievable false-positive and false-negative trade-offs for each group.",
          "visual_anchor": "Black diagonal line from top-left to bottom-right",
          "text_evidence": "H₁*, H₂* are the set of cal. classifiers for the two groups",
          "query_type": "value_context"
        },
        {
          "query": "Does the horizontal dashed line crossing h₁ and h₂ represent an equal-cost constraint across both groups?",
          "answer": "Yes, the horizontal dashed line represents an equal-cost constraint, intersecting both the blue line h₁ and red line h₂ at the same generalized false-negative rate, enforcing fairness by equalizing costs across the two groups.",
          "visual_anchor": "Horizontal dashed line intersecting blue h₁ and red h₂",
          "text_evidence": "Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue line h₁ start at the origin while representing a trivial classifier μ₁?",
        "answer": "The blue line represents the trivial classifier h^μ₁, which starts at the origin (0,0) in the false-positive/false-negative plane, indicating zero error rates when the classifier always predicts according to the base rate of group 1.",
        "visual_anchor": "Blue line h₁ originating at point O (origin)",
        "text_evidence": "Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. h^μ₁, h^μ₂ are trivial classifiers.",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the calibrated classifier sets H₁* and H₂* both lie on the black diagonal boundary?",
        "answer": "Yes, the sets H₁* and H₂* of calibrated classifiers for the two groups both lie on the black diagonal boundary, representing the Pareto frontier of achievable false-positive and false-negative trade-offs for each group.",
        "visual_anchor": "Black diagonal line from top-left to bottom-right",
        "text_evidence": "H₁*, H₂* are the set of cal. classifiers for the two groups",
        "query_type": "value_context"
      },
      {
        "query": "Does the horizontal dashed line crossing h₁ and h₂ represent an equal-cost constraint across both groups?",
        "answer": "Yes, the horizontal dashed line represents an equal-cost constraint, intersecting both the blue line h₁ and red line h₂ at the same generalized false-negative rate, enforcing fairness by equalizing costs across the two groups.",
        "visual_anchor": "Horizontal dashed line intersecting blue h₁ and red h₂",
        "text_evidence": "Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black triangular boundary from top-left to bottom-right",
      "Blue line labeled h₁",
      "Red/orange line labeled h₂",
      "Horizontal dashed line intersecting both h₁ and h₂",
      "Gray shaded triangular region",
      "Origin point labeled O",
      "Axes labeled 'Generalized FP Rate' and 'Generalized FN Rate'"
    ]
  },
  {
    "doc_id": "1709.02012",
    "figure_id": "1709.02012_fig_6",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg",
    "caption": "Figure 2: Calibration-Preserving Parity through interpolation.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Generalized FP Rate axis (horizontal)\", \"Generalized FN Rate axis (vertical)\", \"Blue point labeled h1\", \"Red point labeled h̃2\", \"Black point labeled h'^μ2\", \"Dashed lines connecting points\", \"Solid blue line from origin to h1\", \"Solid red line from origin to h̃2\", \"Triangular feasible region\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue point h1 lie above the dashed line connecting origin to the black point h'^μ2?\",\n      \"answer\": \"The diagram illustrates interpolation for Calibration-Preserving Parity, where h1 represents a different group's classifier position while maintaining calibration through the interpolation scheme between points.\",\n      \"visual_anchor\": \"Blue point h1 positioned above the dashed diagonal line\",\n      \"text_evidence\": \"We introduce a general relaxation that seeks to satisfy a single equal-cost constraint while maintaining calibration for each group G_t\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the red point h̃2 position relative to h1 demonstrate the equalization of false-positive and false-negative rates?\",\n      \"answer\": \"Yes, the red point h̃2 and blue point h1 are positioned to show different trade-offs between generalized FP and FN rates, illustrating the constraint equalization that Equalized Odds seeks to achieve.\",\n      \"visual_anchor\": \"Red point h̃2 positioned lower and right of blue point h1\",\n      \"text_evidence\": \"Equalized Odds sets constraints to equalize false-positives c_fp(h_t) and false-negatives c_fn(h_t)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the solid lines from origin to h1 and h̃2 represent calibration preservation for each group?\",\n      \"answer\": \"The solid lines represent the calibration-preserving paths for different groups, showing how interpolation maintains calibration while achieving parity constraints through the relaxation approach.\",\n      \"visual_anchor\": \"Solid blue and red lines radiating from origin (0,0)\",\n      \"text_evidence\": \"a substantially simplified notion of Equalized Odds is compatible with calibration\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Generalized FP Rate axis (horizontal)",
        "Generalized FN Rate axis (vertical)",
        "Blue point labeled h1",
        "Red point labeled h̃2",
        "Black point labeled h'^μ2",
        "Dashed lines connecting points",
        "Solid blue line from origin to h1",
        "Solid red line from origin to h̃2",
        "Triangular feasible region"
      ],
      "queries": [
        {
          "query": "Why does the blue point h1 lie above the dashed line connecting origin to the black point h'^μ2?",
          "answer": "The diagram illustrates interpolation for Calibration-Preserving Parity, where h1 represents a different group's classifier position while maintaining calibration through the interpolation scheme between points.",
          "visual_anchor": "Blue point h1 positioned above the dashed diagonal line",
          "text_evidence": "We introduce a general relaxation that seeks to satisfy a single equal-cost constraint while maintaining calibration for each group G_t",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the red point h̃2 position relative to h1 demonstrate the equalization of false-positive and false-negative rates?",
          "answer": "Yes, the red point h̃2 and blue point h1 are positioned to show different trade-offs between generalized FP and FN rates, illustrating the constraint equalization that Equalized Odds seeks to achieve.",
          "visual_anchor": "Red point h̃2 positioned lower and right of blue point h1",
          "text_evidence": "Equalized Odds sets constraints to equalize false-positives c_fp(h_t) and false-negatives c_fn(h_t)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the solid lines from origin to h1 and h̃2 represent calibration preservation for each group?",
          "answer": "The solid lines represent the calibration-preserving paths for different groups, showing how interpolation maintains calibration while achieving parity constraints through the relaxation approach.",
          "visual_anchor": "Solid blue and red lines radiating from origin (0,0)",
          "text_evidence": "a substantially simplified notion of Equalized Odds is compatible with calibration",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue point h1 lie above the dashed line connecting origin to the black point h'^μ2?",
        "answer": "The diagram illustrates interpolation for Calibration-Preserving Parity, where h1 represents a different group's classifier position while maintaining calibration through the interpolation scheme between points.",
        "visual_anchor": "Blue point h1 positioned above the dashed diagonal line",
        "text_evidence": "We introduce a general relaxation that seeks to satisfy a single equal-cost constraint while maintaining calibration for each group G_t",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the red point h̃2 position relative to h1 demonstrate the equalization of false-positive and false-negative rates?",
        "answer": "Yes, the red point h̃2 and blue point h1 are positioned to show different trade-offs between generalized FP and FN rates, illustrating the constraint equalization that Equalized Odds seeks to achieve.",
        "visual_anchor": "Red point h̃2 positioned lower and right of blue point h1",
        "text_evidence": "Equalized Odds sets constraints to equalize false-positives c_fp(h_t) and false-negatives c_fn(h_t)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the solid lines from origin to h1 and h̃2 represent calibration preservation for each group?",
        "answer": "The solid lines represent the calibration-preserving paths for different groups, showing how interpolation maintains calibration while achieving parity constraints through the relaxation approach.",
        "visual_anchor": "Solid blue and red lines radiating from origin (0,0)",
        "text_evidence": "a substantially simplified notion of Equalized Odds is compatible with calibration",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Generalized FP Rate axis (horizontal)",
      "Generalized FN Rate axis (vertical)",
      "Blue point labeled h1",
      "Red point labeled h̃2",
      "Black point labeled h'^μ2",
      "Dashed lines connecting points",
      "Solid blue line from origin to h1",
      "Solid red line from origin to h̃2",
      "Triangular feasible region"
    ]
  },
  {
    "doc_id": "1709.02012",
    "figure_id": "1709.02012_fig_9",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg",
    "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue curve labeled H₁* representing group 1\",\n    \"Red curve labeled H₂* representing group 2\",\n    \"Filled circle points h₁ and h₂\",\n    \"Diamond points h̃₁ and h̃₂\",\n    \"Left panel: Equal Odds (Derived)\",\n    \"Right panel: Calib. + Equal F.N.\",\n    \"Generalized F.P. Rate on x-axis\",\n    \"Generalized F.N. Rate on y-axis\",\n    \"Gray dashed horizontal line at approximately 0.5\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the diamond points h̃₁ and h̃₂ lie on their respective blue and red curves in both panels?\",\n      \"answer\": \"The diamond points represent post-processed classifiers that interpolate between the original classifiers and optimal ones. This interpolation ensures they lie on the cost curves connecting h₁ to h^μ₁ and h₂ to h^μ₂.\",\n      \"visual_anchor\": \"Diamond points h̃₁ and h̃₂ positioned on the blue and red curves\",\n      \"text_evidence\": \"g₂(h̃₂) = (1 - α)g₂(h₂) + αg₂(h^μ₂), and thus setting α = [J₁(h₁) - g₂(h₂)] / [g₂(h^μ₂) - g₂(h₂)]\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"What performance impact do the experiments aim to reveal when moving from filled circles to diamonds in these plots?\",\n      \"answer\": \"The experiments aim to show that imposing calibration and equal-cost constraints (represented by the diamond points) often results in performance degradation while simultaneously increasing other notions of disparity.\",\n      \"visual_anchor\": \"Transition from filled circle points (h₁, h₂) to diamond points (h̃₁, h̃₂)\",\n      \"text_evidence\": \"our goal is to understand the impact of imposing calibration and an equalcost constraint on real-world datasets. We will empirically show that, in many cases, this will result in performance degradation, while simultaneously increasing other notions of disparity.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the left panel's blue curve trajectory differ from the right panel's regarding constraint modification?\",\n      \"answer\": \"Points on the Equal Odds (left panel) curves represent classifiers achieved by modifying constraint hyperparameters during training, while the right panel shows calibrated relaxation with post-processing represented by diamonds.\",\n      \"visual_anchor\": \"Blue curve in left panel (Equal Odds Derived) versus right panel (Calib. + Equal F.N.)\",\n      \"text_evidence\": \"Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue curve labeled H₁* representing group 1",
        "Red curve labeled H₂* representing group 2",
        "Filled circle points h₁ and h₂",
        "Diamond points h̃₁ and h̃₂",
        "Left panel: Equal Odds (Derived)",
        "Right panel: Calib. + Equal F.N.",
        "Generalized F.P. Rate on x-axis",
        "Generalized F.N. Rate on y-axis",
        "Gray dashed horizontal line at approximately 0.5"
      ],
      "queries": [
        {
          "query": "Why do the diamond points h̃₁ and h̃₂ lie on their respective blue and red curves in both panels?",
          "answer": "The diamond points represent post-processed classifiers that interpolate between the original classifiers and optimal ones. This interpolation ensures they lie on the cost curves connecting h₁ to h^μ₁ and h₂ to h^μ₂.",
          "visual_anchor": "Diamond points h̃₁ and h̃₂ positioned on the blue and red curves",
          "text_evidence": "g₂(h̃₂) = (1 - α)g₂(h₂) + αg₂(h^μ₂), and thus setting α = [J₁(h₁) - g₂(h₂)] / [g₂(h^μ₂) - g₂(h₂)]",
          "query_type": "visual_definition"
        },
        {
          "query": "What performance impact do the experiments aim to reveal when moving from filled circles to diamonds in these plots?",
          "answer": "The experiments aim to show that imposing calibration and equal-cost constraints (represented by the diamond points) often results in performance degradation while simultaneously increasing other notions of disparity.",
          "visual_anchor": "Transition from filled circle points (h₁, h₂) to diamond points (h̃₁, h̃₂)",
          "text_evidence": "our goal is to understand the impact of imposing calibration and an equalcost constraint on real-world datasets. We will empirically show that, in many cases, this will result in performance degradation, while simultaneously increasing other notions of disparity.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the left panel's blue curve trajectory differ from the right panel's regarding constraint modification?",
          "answer": "Points on the Equal Odds (left panel) curves represent classifiers achieved by modifying constraint hyperparameters during training, while the right panel shows calibrated relaxation with post-processing represented by diamonds.",
          "visual_anchor": "Blue curve in left panel (Equal Odds Derived) versus right panel (Calib. + Equal F.N.)",
          "text_evidence": "Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the diamond points h̃₁ and h̃₂ lie on their respective blue and red curves in both panels?",
        "answer": "The diamond points represent post-processed classifiers that interpolate between the original classifiers and optimal ones. This interpolation ensures they lie on the cost curves connecting h₁ to h^μ₁ and h₂ to h^μ₂.",
        "visual_anchor": "Diamond points h̃₁ and h̃₂ positioned on the blue and red curves",
        "text_evidence": "g₂(h̃₂) = (1 - α)g₂(h₂) + αg₂(h^μ₂), and thus setting α = [J₁(h₁) - g₂(h₂)] / [g₂(h^μ₂) - g₂(h₂)]",
        "query_type": "visual_definition"
      },
      {
        "query": "What performance impact do the experiments aim to reveal when moving from filled circles to diamonds in these plots?",
        "answer": "The experiments aim to show that imposing calibration and equal-cost constraints (represented by the diamond points) often results in performance degradation while simultaneously increasing other notions of disparity.",
        "visual_anchor": "Transition from filled circle points (h₁, h₂) to diamond points (h̃₁, h̃₂)",
        "text_evidence": "our goal is to understand the impact of imposing calibration and an equalcost constraint on real-world datasets. We will empirically show that, in many cases, this will result in performance degradation, while simultaneously increasing other notions of disparity.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the left panel's blue curve trajectory differ from the right panel's regarding constraint modification?",
        "answer": "Points on the Equal Odds (left panel) curves represent classifiers achieved by modifying constraint hyperparameters during training, while the right panel shows calibrated relaxation with post-processing represented by diamonds.",
        "visual_anchor": "Blue curve in left panel (Equal Odds Derived) versus right panel (Calib. + Equal F.N.)",
        "text_evidence": "Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue curve labeled H₁* representing group 1",
      "Red curve labeled H₂* representing group 2",
      "Filled circle points h₁ and h₂",
      "Diamond points h̃₁ and h̃₂",
      "Left panel: Equal Odds (Derived)",
      "Right panel: Calib. + Equal F.N.",
      "Generalized F.P. Rate on x-axis",
      "Generalized F.N. Rate on y-axis",
      "Gray dashed horizontal line at approximately 0.5"
    ]
  },
  {
    "doc_id": "1710.08615",
    "figure_id": "1710.08615_fig_1",
    "figure_number": null,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg",
    "caption": "",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Blue curve showing probability to retweet from 0 to ~0.23\", \"X-axis: Number of Tweeting Friends (0-150)\", \"Y-axis: Probability to Retweet (0-0.3)\", \"Curve plateaus around 0.18-0.20 after 50 friends\", \"Initial steep rise from 0 to ~0.15 in first 50 friends\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the plateau at 0.18-0.20 probability after 50 friends illustrate heterogeneity in observational data about human behavior?\",\n      \"answer\": \"Yes, the plateau suggests different subgroups within the population vary in behavior and size. Users with fewer friends show rapid probability increases, while those with more friends stabilize, demonstrating heterogeneous behavioral patterns across population subgroups.\",\n      \"visual_anchor\": \"Blue curve plateau at probability 0.18-0.20 occurring after 50 tweeting friends\",\n      \"text_evidence\": \"Observational data about human behavior is often heterogeneous, i.e., generated by subgroups within the population under study that vary in size and behavior.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does probability rise steeply to 0.15 before 50 friends if responsive users with fewer friends quickly drop out?\",\n      \"answer\": \"The steep initial rise reflects the behavior of responsive users who are still active in the analysis at lower friend counts. These users show high retweet probability before they drop out of the dataset due to fewer exposures.\",\n      \"visual_anchor\": \"Steep curve rise from 0 to approximately 0.15 probability in the 0-50 tweeting friends range\",\n      \"text_evidence\": \"responsive users (with fewer friends) quickly drop out of analysis (since they are generally exposed fewer times), leaving only the highly connecte\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Could aggregating the blue curve from 0 to 150 friends mask opposing trends predicted by Simpson's paradox?\",\n      \"answer\": \"Yes, Simpson's paradox warns that aggregated trends can differ substantially from subgroup trends. The overall increasing curve may hide that different friend-count subgroups exhibit reversed or different retweet probability patterns when analyzed separately.\",\n      \"visual_anchor\": \"Overall blue curve trend spanning entire x-axis range from 0 to 150 tweeting friends\",\n      \"text_evidence\": \"Heterogeneity predisposes analysis to Simpson's paradox, whereby the trends observed in data that has been aggregated over the entire population may be substantially different from those of the underlying subgroups.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue curve showing probability to retweet from 0 to ~0.23",
        "X-axis: Number of Tweeting Friends (0-150)",
        "Y-axis: Probability to Retweet (0-0.3)",
        "Curve plateaus around 0.18-0.20 after 50 friends",
        "Initial steep rise from 0 to ~0.15 in first 50 friends"
      ],
      "queries": [
        {
          "query": "Does the plateau at 0.18-0.20 probability after 50 friends illustrate heterogeneity in observational data about human behavior?",
          "answer": "Yes, the plateau suggests different subgroups within the population vary in behavior and size. Users with fewer friends show rapid probability increases, while those with more friends stabilize, demonstrating heterogeneous behavioral patterns across population subgroups.",
          "visual_anchor": "Blue curve plateau at probability 0.18-0.20 occurring after 50 tweeting friends",
          "text_evidence": "Observational data about human behavior is often heterogeneous, i.e., generated by subgroups within the population under study that vary in size and behavior.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does probability rise steeply to 0.15 before 50 friends if responsive users with fewer friends quickly drop out?",
          "answer": "The steep initial rise reflects the behavior of responsive users who are still active in the analysis at lower friend counts. These users show high retweet probability before they drop out of the dataset due to fewer exposures.",
          "visual_anchor": "Steep curve rise from 0 to approximately 0.15 probability in the 0-50 tweeting friends range",
          "text_evidence": "responsive users (with fewer friends) quickly drop out of analysis (since they are generally exposed fewer times), leaving only the highly connecte",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Could aggregating the blue curve from 0 to 150 friends mask opposing trends predicted by Simpson's paradox?",
          "answer": "Yes, Simpson's paradox warns that aggregated trends can differ substantially from subgroup trends. The overall increasing curve may hide that different friend-count subgroups exhibit reversed or different retweet probability patterns when analyzed separately.",
          "visual_anchor": "Overall blue curve trend spanning entire x-axis range from 0 to 150 tweeting friends",
          "text_evidence": "Heterogeneity predisposes analysis to Simpson's paradox, whereby the trends observed in data that has been aggregated over the entire population may be substantially different from those of the underlying subgroups.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the plateau at 0.18-0.20 probability after 50 friends illustrate heterogeneity in observational data about human behavior?",
        "answer": "Yes, the plateau suggests different subgroups within the population vary in behavior and size. Users with fewer friends show rapid probability increases, while those with more friends stabilize, demonstrating heterogeneous behavioral patterns across population subgroups.",
        "visual_anchor": "Blue curve plateau at probability 0.18-0.20 occurring after 50 tweeting friends",
        "text_evidence": "Observational data about human behavior is often heterogeneous, i.e., generated by subgroups within the population under study that vary in size and behavior.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does probability rise steeply to 0.15 before 50 friends if responsive users with fewer friends quickly drop out?",
        "answer": "The steep initial rise reflects the behavior of responsive users who are still active in the analysis at lower friend counts. These users show high retweet probability before they drop out of the dataset due to fewer exposures.",
        "visual_anchor": "Steep curve rise from 0 to approximately 0.15 probability in the 0-50 tweeting friends range",
        "text_evidence": "responsive users (with fewer friends) quickly drop out of analysis (since they are generally exposed fewer times), leaving only the highly connecte",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Could aggregating the blue curve from 0 to 150 friends mask opposing trends predicted by Simpson's paradox?",
        "answer": "Yes, Simpson's paradox warns that aggregated trends can differ substantially from subgroup trends. The overall increasing curve may hide that different friend-count subgroups exhibit reversed or different retweet probability patterns when analyzed separately.",
        "visual_anchor": "Overall blue curve trend spanning entire x-axis range from 0 to 150 tweeting friends",
        "text_evidence": "Heterogeneity predisposes analysis to Simpson's paradox, whereby the trends observed in data that has been aggregated over the entire population may be substantially different from those of the underlying subgroups.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue curve showing probability to retweet from 0 to ~0.23",
      "X-axis: Number of Tweeting Friends (0-150)",
      "Y-axis: Probability to Retweet (0-0.3)",
      "Curve plateaus around 0.18-0.20 after 50 friends",
      "Initial steep rise from 0 to ~0.15 in first 50 friends"
    ]
  },
  {
    "doc_id": "1710.11214",
    "figure_id": "1710.11214_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg",
    "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"PLATFORM box with list interface on left\",\n    \"USER icon (purple figure) on right\",\n    \"curved arrow labeled 'interaction' from USER to PLATFORM\",\n    \"curved arrow labeled 'recommendation' from PLATFORM to USER with star icon below\",\n    \"model icon (green gears) connected to PLATFORM\",\n    \"preference icon (pink thought bubble) connected to USER\",\n    \"cursor icon at top\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the curved arrow labeled recommendation with the star icon relate to systems being retrained regularly?\",\n      \"answer\": \"The recommendation arrow shows how live systems are updated using new data influenced by the system itself, forming the feedback loop where recommendations affect future training data.\",\n      \"visual_anchor\": \"curved arrow labeled 'recommendation' from PLATFORM to USER with yellow star icon below it\",\n      \"text_evidence\": \"Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the pink thought bubble labeled preference connect to USER rather than directly to the PLATFORM box?\",\n      \"answer\": \"User preferences act as confounding factors that influence both current interactions and recommendations through past interactions, rather than being directly observable by the platform model.\",\n      \"visual_anchor\": \"pink thought bubble labeled 'preference' connected to USER icon on the right side\",\n      \"text_evidence\": \"User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What problem arises when the green gear model attempts to predict the interaction arrow without considering recommendations?\",\n      \"answer\": \"Confounding occurs because the model attempts to predict user behavior without accounting for algorithmic recommendations from previously deployed systems that influenced the training data.\",\n      \"visual_anchor\": \"green gears icon labeled 'model' connected to PLATFORM box on left side\",\n      \"text_evidence\": \"the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "PLATFORM box with list interface on left",
        "USER icon (purple figure) on right",
        "curved arrow labeled 'interaction' from USER to PLATFORM",
        "curved arrow labeled 'recommendation' from PLATFORM to USER with star icon below",
        "model icon (green gears) connected to PLATFORM",
        "preference icon (pink thought bubble) connected to USER",
        "cursor icon at top"
      ],
      "queries": [
        {
          "query": "How does the curved arrow labeled recommendation with the star icon relate to systems being retrained regularly?",
          "answer": "The recommendation arrow shows how live systems are updated using new data influenced by the system itself, forming the feedback loop where recommendations affect future training data.",
          "visual_anchor": "curved arrow labeled 'recommendation' from PLATFORM to USER with yellow star icon below it",
          "text_evidence": "Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1).",
          "query_type": "value_context"
        },
        {
          "query": "Why does the pink thought bubble labeled preference connect to USER rather than directly to the PLATFORM box?",
          "answer": "User preferences act as confounding factors that influence both current interactions and recommendations through past interactions, rather than being directly observable by the platform model.",
          "visual_anchor": "pink thought bubble labeled 'preference' connected to USER icon on the right side",
          "text_evidence": "User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What problem arises when the green gear model attempts to predict the interaction arrow without considering recommendations?",
          "answer": "Confounding occurs because the model attempts to predict user behavior without accounting for algorithmic recommendations from previously deployed systems that influenced the training data.",
          "visual_anchor": "green gears icon labeled 'model' connected to PLATFORM box on left side",
          "text_evidence": "the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the curved arrow labeled recommendation with the star icon relate to systems being retrained regularly?",
        "answer": "The recommendation arrow shows how live systems are updated using new data influenced by the system itself, forming the feedback loop where recommendations affect future training data.",
        "visual_anchor": "curved arrow labeled 'recommendation' from PLATFORM to USER with yellow star icon below it",
        "text_evidence": "Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1).",
        "query_type": "value_context"
      },
      {
        "query": "Why does the pink thought bubble labeled preference connect to USER rather than directly to the PLATFORM box?",
        "answer": "User preferences act as confounding factors that influence both current interactions and recommendations through past interactions, rather than being directly observable by the platform model.",
        "visual_anchor": "pink thought bubble labeled 'preference' connected to USER icon on the right side",
        "text_evidence": "User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What problem arises when the green gear model attempts to predict the interaction arrow without considering recommendations?",
        "answer": "Confounding occurs because the model attempts to predict user behavior without accounting for algorithmic recommendations from previously deployed systems that influenced the training data.",
        "visual_anchor": "green gears icon labeled 'model' connected to PLATFORM box on left side",
        "text_evidence": "the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "PLATFORM box with list interface on left",
      "USER icon (purple figure) on right",
      "curved arrow labeled 'interaction' from USER to PLATFORM",
      "curved arrow labeled 'recommendation' from PLATFORM to USER with star icon below",
      "model icon (green gears) connected to PLATFORM",
      "preference icon (pink thought bubble) connected to USER",
      "cursor icon at top"
    ]
  },
  {
    "doc_id": "1710.11214",
    "figure_id": "1710.11214_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg",
    "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"utility matrix heatmap with darker shading for higher values\", \"x-axis labeled 'items' ranging 0-10000\", \"y-axis labeled 'users' ranging 0-100\", \"sparse darker blocks scattered across matrix\", \"predominantly light/white background indicating low utility\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the darker blocks appear sparse across the matrix despite user preferences being disproportionate in the real world?\",\n      \"answer\": \"The sparse darker blocks reflect that user preferences are disproportionately distributed, meaning most users have strong preferences for only a few items. This sparsity matches commonly accepted intuitions about user behavior where users enjoy limited subsets of available content.\",\n      \"visual_anchor\": \"sparse darker blocks scattered across predominantly light matrix background\",\n      \"text_evidence\": \"With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the visible structure in darker regions support the claim that matrix factorization can easily capture this data?\",\n      \"answer\": \"Yes, the darker regions show clustered patterns rather than random noise, indicating underlying structure in user-item relationships. This structured sparsity makes the utility matrix amenable to low-rank approximation through matrix factorization techniques.\",\n      \"visual_anchor\": \"clustered darker blocks showing structured patterns in utility values\",\n      \"text_evidence\": \"The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the darker vertical bands between items 6000-8000 relate to the underlying score approach for recommendations?\",\n      \"answer\": \"The vertical bands indicate items with higher utility across multiple users, which would produce higher underlying scores in the recommendation framework. These high-utility items would be prioritized by algorithms when computing scores of how much users will enjoy items at any given time.\",\n      \"visual_anchor\": \"darker vertical bands concentrated in the region between items 6000-8000\",\n      \"text_evidence\": \"The core idea of this framework is that each recommendation system provides some underlying score $s _ { u i } ( t )$ of how much a user $u$ uiwill enjoy item  at time .\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "utility matrix heatmap with darker shading for higher values",
        "x-axis labeled 'items' ranging 0-10000",
        "y-axis labeled 'users' ranging 0-100",
        "sparse darker blocks scattered across matrix",
        "predominantly light/white background indicating low utility"
      ],
      "queries": [
        {
          "query": "Why do the darker blocks appear sparse across the matrix despite user preferences being disproportionate in the real world?",
          "answer": "The sparse darker blocks reflect that user preferences are disproportionately distributed, meaning most users have strong preferences for only a few items. This sparsity matches commonly accepted intuitions about user behavior where users enjoy limited subsets of available content.",
          "visual_anchor": "sparse darker blocks scattered across predominantly light matrix background",
          "text_evidence": "With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the visible structure in darker regions support the claim that matrix factorization can easily capture this data?",
          "answer": "Yes, the darker regions show clustered patterns rather than random noise, indicating underlying structure in user-item relationships. This structured sparsity makes the utility matrix amenable to low-rank approximation through matrix factorization techniques.",
          "visual_anchor": "clustered darker blocks showing structured patterns in utility values",
          "text_evidence": "The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.",
          "query_type": "value_context"
        },
        {
          "query": "How do the darker vertical bands between items 6000-8000 relate to the underlying score approach for recommendations?",
          "answer": "The vertical bands indicate items with higher utility across multiple users, which would produce higher underlying scores in the recommendation framework. These high-utility items would be prioritized by algorithms when computing scores of how much users will enjoy items at any given time.",
          "visual_anchor": "darker vertical bands concentrated in the region between items 6000-8000",
          "text_evidence": "The core idea of this framework is that each recommendation system provides some underlying score $s _ { u i } ( t )$ of how much a user $u$ uiwill enjoy item  at time .",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the darker blocks appear sparse across the matrix despite user preferences being disproportionate in the real world?",
        "answer": "The sparse darker blocks reflect that user preferences are disproportionately distributed, meaning most users have strong preferences for only a few items. This sparsity matches commonly accepted intuitions about user behavior where users enjoy limited subsets of available content.",
        "visual_anchor": "sparse darker blocks scattered across predominantly light matrix background",
        "text_evidence": "With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the visible structure in darker regions support the claim that matrix factorization can easily capture this data?",
        "answer": "Yes, the darker regions show clustered patterns rather than random noise, indicating underlying structure in user-item relationships. This structured sparsity makes the utility matrix amenable to low-rank approximation through matrix factorization techniques.",
        "visual_anchor": "clustered darker blocks showing structured patterns in utility values",
        "text_evidence": "The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.",
        "query_type": "value_context"
      },
      {
        "query": "How do the darker vertical bands between items 6000-8000 relate to the underlying score approach for recommendations?",
        "answer": "The vertical bands indicate items with higher utility across multiple users, which would produce higher underlying scores in the recommendation framework. These high-utility items would be prioritized by algorithms when computing scores of how much users will enjoy items at any given time.",
        "visual_anchor": "darker vertical bands concentrated in the region between items 6000-8000",
        "text_evidence": "The core idea of this framework is that each recommendation system provides some underlying score $s _ { u i } ( t )$ of how much a user $u$ uiwill enjoy item  at time .",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "utility matrix heatmap with darker shading for higher values",
      "x-axis labeled 'items' ranging 0-10000",
      "y-axis labeled 'users' ranging 0-100",
      "sparse darker blocks scattered across matrix",
      "predominantly light/white background indicating low utility"
    ]
  },
  {
    "doc_id": "1710.11214",
    "figure_id": "1710.11214_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig2.jpg",
    "caption": "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Two panel line plot with iteration on x-axis and change in Jaccard index on y-axis\",\n    \"Case 1 (left panel) shows lines near zero with slight increase then decline\",\n    \"Case 2 (right panel) shows lines rising to 0.3 by iteration 100\",\n    \"Five colored lines: content (yellow dotted), ideal (black solid), MF (red dashed), popularity (blue dotted), random (gray dash-dot), social (cyan dashed)\",\n    \"Ideal line remains at zero in both panels\",\n    \"Popularity line reaches approximately 0.16 in Case 2 while other algorithms reach 0.3\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the ideal black line stay at zero in both cases while homogenization effects occur?\",\n      \"answer\": \"The ideal line represents the baseline behavior without recommendation influence. Since the Jaccard index measures change relative to ideal behavior, the ideal algorithm itself shows zero change by definition.\",\n      \"visual_anchor\": \"Black solid line labeled 'ideal' at y=0 across all iterations in both panels\",\n      \"text_evidence\": \"Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the popularity algorithm's lower peak around 0.16 in Case 2 contradict the claim about homogenization?\",\n      \"answer\": \"No, popularity-based systems still homogenize behavior but less than MF and social algorithms. All non-ideal algorithms show substantial increases compared to Case 1, confirming that repeated training increases homogenization.\",\n      \"visual_anchor\": \"Blue dotted popularity line reaching approximately 0.16 while red MF and cyan social lines reach 0.3 at iteration 100 in right panel\",\n      \"text_evidence\": \"Recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do all recommendation lines decline after iteration 50 in Case 1 despite initial mild homogenization?\",\n      \"answer\": \"After a single training event, the homogenization effect is temporary and diminishes over time as users return to more diverse behavior patterns without further algorithmic reinforcement.\",\n      \"visual_anchor\": \"All colored lines (content, MF, popularity, social) showing slight increase to ~0.03 near iteration 50, then declining toward zero by iteration 100 in left panel\",\n      \"text_evidence\": \"On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Two panel line plot with iteration on x-axis and change in Jaccard index on y-axis",
        "Case 1 (left panel) shows lines near zero with slight increase then decline",
        "Case 2 (right panel) shows lines rising to 0.3 by iteration 100",
        "Five colored lines: content (yellow dotted), ideal (black solid), MF (red dashed), popularity (blue dotted), random (gray dash-dot), social (cyan dashed)",
        "Ideal line remains at zero in both panels",
        "Popularity line reaches approximately 0.16 in Case 2 while other algorithms reach 0.3"
      ],
      "queries": [
        {
          "query": "Why does the ideal black line stay at zero in both cases while homogenization effects occur?",
          "answer": "The ideal line represents the baseline behavior without recommendation influence. Since the Jaccard index measures change relative to ideal behavior, the ideal algorithm itself shows zero change by definition.",
          "visual_anchor": "Black solid line labeled 'ideal' at y=0 across all iterations in both panels",
          "text_evidence": "Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the popularity algorithm's lower peak around 0.16 in Case 2 contradict the claim about homogenization?",
          "answer": "No, popularity-based systems still homogenize behavior but less than MF and social algorithms. All non-ideal algorithms show substantial increases compared to Case 1, confirming that repeated training increases homogenization.",
          "visual_anchor": "Blue dotted popularity line reaching approximately 0.16 while red MF and cyan social lines reach 0.3 at iteration 100 in right panel",
          "text_evidence": "Recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do all recommendation lines decline after iteration 50 in Case 1 despite initial mild homogenization?",
          "answer": "After a single training event, the homogenization effect is temporary and diminishes over time as users return to more diverse behavior patterns without further algorithmic reinforcement.",
          "visual_anchor": "All colored lines (content, MF, popularity, social) showing slight increase to ~0.03 near iteration 50, then declining toward zero by iteration 100 in left panel",
          "text_evidence": "On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the ideal black line stay at zero in both cases while homogenization effects occur?",
        "answer": "The ideal line represents the baseline behavior without recommendation influence. Since the Jaccard index measures change relative to ideal behavior, the ideal algorithm itself shows zero change by definition.",
        "visual_anchor": "Black solid line labeled 'ideal' at y=0 across all iterations in both panels",
        "text_evidence": "Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the popularity algorithm's lower peak around 0.16 in Case 2 contradict the claim about homogenization?",
        "answer": "No, popularity-based systems still homogenize behavior but less than MF and social algorithms. All non-ideal algorithms show substantial increases compared to Case 1, confirming that repeated training increases homogenization.",
        "visual_anchor": "Blue dotted popularity line reaching approximately 0.16 while red MF and cyan social lines reach 0.3 at iteration 100 in right panel",
        "text_evidence": "Recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do all recommendation lines decline after iteration 50 in Case 1 despite initial mild homogenization?",
        "answer": "After a single training event, the homogenization effect is temporary and diminishes over time as users return to more diverse behavior patterns without further algorithmic reinforcement.",
        "visual_anchor": "All colored lines (content, MF, popularity, social) showing slight increase to ~0.03 near iteration 50, then declining toward zero by iteration 100 in left panel",
        "text_evidence": "On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Two panel line plot with iteration on x-axis and change in Jaccard index on y-axis",
      "Case 1 (left panel) shows lines near zero with slight increase then decline",
      "Case 2 (right panel) shows lines rising to 0.3 by iteration 100",
      "Five colored lines: content (yellow dotted), ideal (black solid), MF (red dashed), popularity (blue dotted), random (gray dash-dot), social (cyan dashed)",
      "Ideal line remains at zero in both panels",
      "Popularity line reaches approximately 0.16 in Case 2 while other algorithms reach 0.3"
    ]
  },
  {
    "doc_id": "1710.11214",
    "figure_id": "1710.11214_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg",
    "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue dotted line labeled 'popularity' reaching ~0.17\", \"teal dashed line labeled 'social' reaching ~0.08\", \"orange dotted line labeled 'content' staying near 0.025\", \"gray dash-dot line labeled 'random' near 0\", \"black solid line labeled 'ideal' at 0\", \"x-axis showing iterations 0-100\", \"y-axis showing change in Jaccard index 0-0.15\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue popularity curve reach 0.17 while social reaches only 0.08 by iteration 100?\",\n      \"answer\": \"Popularity increases homogenization the most globally among all non-random recommendation algorithms, causing the highest change in Jaccard index relative to ideal behavior.\",\n      \"visual_anchor\": \"blue dotted line at ~0.17 versus teal dashed line at ~0.08 at iteration 100\",\n      \"text_evidence\": \"Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do minority users experience the same utility changes as the orange content curve suggests at 0.025?\",\n      \"answer\": \"No, minority users whose preferences are not well-captured by low dimensional representations may see lesser improvements or even decreases in utility when homogenization occurs.\",\n      \"visual_anchor\": \"orange dotted line labeled 'content' at ~0.025\",\n      \"text_evidence\": \"These minority users may see lesser improvements or even decreases in utility when homogenization occurs.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the utility breakdown by user relate to the teal social line reaching 0.08 at iteration 100?\",\n      \"answer\": \"The relationship between homogenization and utility varies by user, with Figure 5 breaking down how different users are affected by the homogenization shown in the social algorithm's curve.\",\n      \"visual_anchor\": \"teal dashed line labeled 'social' at ~0.08 at iteration 100\",\n      \"text_evidence\": \"Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorith\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue dotted line labeled 'popularity' reaching ~0.17",
        "teal dashed line labeled 'social' reaching ~0.08",
        "orange dotted line labeled 'content' staying near 0.025",
        "gray dash-dot line labeled 'random' near 0",
        "black solid line labeled 'ideal' at 0",
        "x-axis showing iterations 0-100",
        "y-axis showing change in Jaccard index 0-0.15"
      ],
      "queries": [
        {
          "query": "Why does the blue popularity curve reach 0.17 while social reaches only 0.08 by iteration 100?",
          "answer": "Popularity increases homogenization the most globally among all non-random recommendation algorithms, causing the highest change in Jaccard index relative to ideal behavior.",
          "visual_anchor": "blue dotted line at ~0.17 versus teal dashed line at ~0.08 at iteration 100",
          "text_evidence": "Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do minority users experience the same utility changes as the orange content curve suggests at 0.025?",
          "answer": "No, minority users whose preferences are not well-captured by low dimensional representations may see lesser improvements or even decreases in utility when homogenization occurs.",
          "visual_anchor": "orange dotted line labeled 'content' at ~0.025",
          "text_evidence": "These minority users may see lesser improvements or even decreases in utility when homogenization occurs.",
          "query_type": "value_context"
        },
        {
          "query": "How does the utility breakdown by user relate to the teal social line reaching 0.08 at iteration 100?",
          "answer": "The relationship between homogenization and utility varies by user, with Figure 5 breaking down how different users are affected by the homogenization shown in the social algorithm's curve.",
          "visual_anchor": "teal dashed line labeled 'social' at ~0.08 at iteration 100",
          "text_evidence": "Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorith",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue popularity curve reach 0.17 while social reaches only 0.08 by iteration 100?",
        "answer": "Popularity increases homogenization the most globally among all non-random recommendation algorithms, causing the highest change in Jaccard index relative to ideal behavior.",
        "visual_anchor": "blue dotted line at ~0.17 versus teal dashed line at ~0.08 at iteration 100",
        "text_evidence": "Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do minority users experience the same utility changes as the orange content curve suggests at 0.025?",
        "answer": "No, minority users whose preferences are not well-captured by low dimensional representations may see lesser improvements or even decreases in utility when homogenization occurs.",
        "visual_anchor": "orange dotted line labeled 'content' at ~0.025",
        "text_evidence": "These minority users may see lesser improvements or even decreases in utility when homogenization occurs.",
        "query_type": "value_context"
      },
      {
        "query": "How does the utility breakdown by user relate to the teal social line reaching 0.08 at iteration 100?",
        "answer": "The relationship between homogenization and utility varies by user, with Figure 5 breaking down how different users are affected by the homogenization shown in the social algorithm's curve.",
        "visual_anchor": "teal dashed line labeled 'social' at ~0.08 at iteration 100",
        "text_evidence": "Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorith",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue dotted line labeled 'popularity' reaching ~0.17",
      "teal dashed line labeled 'social' reaching ~0.08",
      "orange dotted line labeled 'content' staying near 0.025",
      "gray dash-dot line labeled 'random' near 0",
      "black solid line labeled 'ideal' at 0",
      "x-axis showing iterations 0-100",
      "y-axis showing change in Jaccard index 0-0.15"
    ]
  },
  {
    "doc_id": "1710.11214",
    "figure_id": "1710.11214_fig_5",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig4.jpg",
    "caption": "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Five scatter plot panels labeled content, MF, popularity, random, and social\",\n    \"Black diagonal trend lines showing negative slopes in content, MF, popularity, and social panels\",\n    \"Nearly horizontal black trend line in random panel\",\n    \"Green point clouds distributed across different utility ranges\",\n    \"X-axis: utility relative to ideal (ranging from -0.6 to 0.0)\",\n    \"Y-axis: change in Jaccard index (ranging from -0.25 to 0.50)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the random panel show a nearly horizontal trend line despite users paired by cosine similarity of theta?\",\n      \"answer\": \"Random recommendations do not leverage user preferences or similarity, so utility losses are unrelated to behavioral homogenization. The pairing by cosine similarity of theta does not create the homogenization pattern seen in other recommendation algorithms.\",\n      \"visual_anchor\": \"Nearly horizontal black trend line in random panel at y≈0\",\n      \"text_evidence\": \"users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the negative slopes in content, MF, popularity, and social panels support that lower utility users have higher homogenization?\",\n      \"answer\": \"Yes, the negative slopes indicate that as utility relative to ideal decreases (moves left on x-axis), the change in Jaccard index increases, confirming higher homogenization for users experiencing utility losses.\",\n      \"visual_anchor\": \"Black diagonal trend lines with negative slopes in content, MF, popularity, and social panels\",\n      \"text_evidence\": \"for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the inequality measured by Gini coefficient relate to the green point distributions across the five recommendation systems?\",\n      \"answer\": \"The Gini coefficient measures inequality in item consumption rates, which drives the different utility-homogenization patterns. Systems like random show minimal inequality impact, while content, MF, popularity, and social create stronger inequality leading to the negative trend correlations visible in their point distributions.\",\n      \"visual_anchor\": \"Green point cloud distributions varying across five panels with different spread patterns\",\n      \"text_evidence\": \"To explore the impacts of these systems on item consumption, we computed the Gini coefficient on the distribution of consumed items, which measures the inequality of the distribution\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Five scatter plot panels labeled content, MF, popularity, random, and social",
        "Black diagonal trend lines showing negative slopes in content, MF, popularity, and social panels",
        "Nearly horizontal black trend line in random panel",
        "Green point clouds distributed across different utility ranges",
        "X-axis: utility relative to ideal (ranging from -0.6 to 0.0)",
        "Y-axis: change in Jaccard index (ranging from -0.25 to 0.50)"
      ],
      "queries": [
        {
          "query": "Why does the random panel show a nearly horizontal trend line despite users paired by cosine similarity of theta?",
          "answer": "Random recommendations do not leverage user preferences or similarity, so utility losses are unrelated to behavioral homogenization. The pairing by cosine similarity of theta does not create the homogenization pattern seen in other recommendation algorithms.",
          "visual_anchor": "Nearly horizontal black trend line in random panel at y≈0",
          "text_evidence": "users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the negative slopes in content, MF, popularity, and social panels support that lower utility users have higher homogenization?",
          "answer": "Yes, the negative slopes indicate that as utility relative to ideal decreases (moves left on x-axis), the change in Jaccard index increases, confirming higher homogenization for users experiencing utility losses.",
          "visual_anchor": "Black diagonal trend lines with negative slopes in content, MF, popularity, and social panels",
          "text_evidence": "for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the inequality measured by Gini coefficient relate to the green point distributions across the five recommendation systems?",
          "answer": "The Gini coefficient measures inequality in item consumption rates, which drives the different utility-homogenization patterns. Systems like random show minimal inequality impact, while content, MF, popularity, and social create stronger inequality leading to the negative trend correlations visible in their point distributions.",
          "visual_anchor": "Green point cloud distributions varying across five panels with different spread patterns",
          "text_evidence": "To explore the impacts of these systems on item consumption, we computed the Gini coefficient on the distribution of consumed items, which measures the inequality of the distribution",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the random panel show a nearly horizontal trend line despite users paired by cosine similarity of theta?",
        "answer": "Random recommendations do not leverage user preferences or similarity, so utility losses are unrelated to behavioral homogenization. The pairing by cosine similarity of theta does not create the homogenization pattern seen in other recommendation algorithms.",
        "visual_anchor": "Nearly horizontal black trend line in random panel at y≈0",
        "text_evidence": "users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the negative slopes in content, MF, popularity, and social panels support that lower utility users have higher homogenization?",
        "answer": "Yes, the negative slopes indicate that as utility relative to ideal decreases (moves left on x-axis), the change in Jaccard index increases, confirming higher homogenization for users experiencing utility losses.",
        "visual_anchor": "Black diagonal trend lines with negative slopes in content, MF, popularity, and social panels",
        "text_evidence": "for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the inequality measured by Gini coefficient relate to the green point distributions across the five recommendation systems?",
        "answer": "The Gini coefficient measures inequality in item consumption rates, which drives the different utility-homogenization patterns. Systems like random show minimal inequality impact, while content, MF, popularity, and social create stronger inequality leading to the negative trend correlations visible in their point distributions.",
        "visual_anchor": "Green point cloud distributions varying across five panels with different spread patterns",
        "text_evidence": "To explore the impacts of these systems on item consumption, we computed the Gini coefficient on the distribution of consumed items, which measures the inequality of the distribution",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Five scatter plot panels labeled content, MF, popularity, random, and social",
      "Black diagonal trend lines showing negative slopes in content, MF, popularity, and social panels",
      "Nearly horizontal black trend line in random panel",
      "Green point clouds distributed across different utility ranges",
      "X-axis: utility relative to ideal (ranging from -0.6 to 0.0)",
      "Y-axis: change in Jaccard index (ranging from -0.25 to 0.50)"
    ]
  },
  {
    "doc_id": "1710.11214",
    "figure_id": "1710.11214_fig_6",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig5.jpg",
    "caption": "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items). Each point is a single simulation. Similar homogenization can result in different item consumption distributions.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Yellow diamonds labeled 'content' clustered around Gini 0.5-0.65\",\n    \"Pink squares labeled 'MF' spanning Gini 0.65-0.8\",\n    \"Blue triangles labeled 'popularity' at Gini 0.7-0.8 with low Jaccard change\",\n    \"Green triangles labeled 'social' distributed across Gini 0.6-0.8\",\n    \"Black stars labeled 'ideal' at Jaccard change near 0.0\",\n    \"Shaded elliptical regions around content, popularity, and social clusters\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do pink MF squares span Gini 0.65-0.8 while causing similar homogenization as content algorithms?\",\n      \"answer\": \"Similar homogenization (Jaccard index change) can result in different item consumption distributions (Gini coefficients), as MF and content both increase homogeneity around 0.25-0.35 but produce distinct inequality patterns.\",\n      \"visual_anchor\": \"Pink MF squares at higher Gini values (0.65-0.8) compared to yellow content diamonds (0.5-0.65)\",\n      \"text_evidence\": \"Similar homogenization can result in different item consumption distributions.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do blue popularity triangles at Gini 0.7-0.8 relate to bias toward confounding algorithms in held-out data?\",\n      \"answer\": \"Popularity algorithms create lower homogenization (Jaccard change 0.1-0.22) but high inequality (Gini 0.7-0.8). Evaluation using confounded held-out data biases results toward systems similar to the confounding algorithm, potentially favoring popularity-based approaches.\",\n      \"visual_anchor\": \"Blue popularity triangles clustered at low Jaccard change (0.1-0.22) and high Gini (0.7-0.8)\",\n      \"text_evidence\": \"When a recommendation system is evaluated using confounded held-out data, results are biased toward recommendation systems similar to the confounding algorithm.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Do black ideal stars at zero Jaccard change support claims about researchers selecting data sets to highlight specific results?\",\n      \"answer\": \"Yes, ideal algorithms show no homogenization (Jaccard change ≈0.0), contrasting sharply with all other methods. This demonstrates how data set choice can intentionally or unintentionally highlight particular algorithm characteristics.\",\n      \"visual_anchor\": \"Black stars labeled 'ideal' positioned at Jaccard change near 0.0\",\n      \"text_evidence\": \"Researchers can, intentionally or unintentionally, select data sets that highlight\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Yellow diamonds labeled 'content' clustered around Gini 0.5-0.65",
        "Pink squares labeled 'MF' spanning Gini 0.65-0.8",
        "Blue triangles labeled 'popularity' at Gini 0.7-0.8 with low Jaccard change",
        "Green triangles labeled 'social' distributed across Gini 0.6-0.8",
        "Black stars labeled 'ideal' at Jaccard change near 0.0",
        "Shaded elliptical regions around content, popularity, and social clusters"
      ],
      "queries": [
        {
          "query": "Why do pink MF squares span Gini 0.65-0.8 while causing similar homogenization as content algorithms?",
          "answer": "Similar homogenization (Jaccard index change) can result in different item consumption distributions (Gini coefficients), as MF and content both increase homogeneity around 0.25-0.35 but produce distinct inequality patterns.",
          "visual_anchor": "Pink MF squares at higher Gini values (0.65-0.8) compared to yellow content diamonds (0.5-0.65)",
          "text_evidence": "Similar homogenization can result in different item consumption distributions.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do blue popularity triangles at Gini 0.7-0.8 relate to bias toward confounding algorithms in held-out data?",
          "answer": "Popularity algorithms create lower homogenization (Jaccard change 0.1-0.22) but high inequality (Gini 0.7-0.8). Evaluation using confounded held-out data biases results toward systems similar to the confounding algorithm, potentially favoring popularity-based approaches.",
          "visual_anchor": "Blue popularity triangles clustered at low Jaccard change (0.1-0.22) and high Gini (0.7-0.8)",
          "text_evidence": "When a recommendation system is evaluated using confounded held-out data, results are biased toward recommendation systems similar to the confounding algorithm.",
          "query_type": "value_context"
        },
        {
          "query": "Do black ideal stars at zero Jaccard change support claims about researchers selecting data sets to highlight specific results?",
          "answer": "Yes, ideal algorithms show no homogenization (Jaccard change ≈0.0), contrasting sharply with all other methods. This demonstrates how data set choice can intentionally or unintentionally highlight particular algorithm characteristics.",
          "visual_anchor": "Black stars labeled 'ideal' positioned at Jaccard change near 0.0",
          "text_evidence": "Researchers can, intentionally or unintentionally, select data sets that highlight",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do pink MF squares span Gini 0.65-0.8 while causing similar homogenization as content algorithms?",
        "answer": "Similar homogenization (Jaccard index change) can result in different item consumption distributions (Gini coefficients), as MF and content both increase homogeneity around 0.25-0.35 but produce distinct inequality patterns.",
        "visual_anchor": "Pink MF squares at higher Gini values (0.65-0.8) compared to yellow content diamonds (0.5-0.65)",
        "text_evidence": "Similar homogenization can result in different item consumption distributions.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do blue popularity triangles at Gini 0.7-0.8 relate to bias toward confounding algorithms in held-out data?",
        "answer": "Popularity algorithms create lower homogenization (Jaccard change 0.1-0.22) but high inequality (Gini 0.7-0.8). Evaluation using confounded held-out data biases results toward systems similar to the confounding algorithm, potentially favoring popularity-based approaches.",
        "visual_anchor": "Blue popularity triangles clustered at low Jaccard change (0.1-0.22) and high Gini (0.7-0.8)",
        "text_evidence": "When a recommendation system is evaluated using confounded held-out data, results are biased toward recommendation systems similar to the confounding algorithm.",
        "query_type": "value_context"
      },
      {
        "query": "Do black ideal stars at zero Jaccard change support claims about researchers selecting data sets to highlight specific results?",
        "answer": "Yes, ideal algorithms show no homogenization (Jaccard change ≈0.0), contrasting sharply with all other methods. This demonstrates how data set choice can intentionally or unintentionally highlight particular algorithm characteristics.",
        "visual_anchor": "Black stars labeled 'ideal' positioned at Jaccard change near 0.0",
        "text_evidence": "Researchers can, intentionally or unintentionally, select data sets that highlight",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Yellow diamonds labeled 'content' clustered around Gini 0.5-0.65",
      "Pink squares labeled 'MF' spanning Gini 0.65-0.8",
      "Blue triangles labeled 'popularity' at Gini 0.7-0.8 with low Jaccard change",
      "Green triangles labeled 'social' distributed across Gini 0.6-0.8",
      "Black stars labeled 'ideal' at Jaccard change near 0.0",
      "Shaded elliptical regions around content, popularity, and social clusters"
    ]
  },
  {
    "doc_id": "1711.05144",
    "figure_id": "1711.05144_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg",
    "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Multiple colored curves showing error values from 0 to 8000 iterations\",\n    \"Y-axis labeled 'error' ranging from 0.1 to 0.4\",\n    \"X-axis showing iteration count up to 8000\",\n    \"Title 'Learner: error, vs. t'\",\n    \"Curves converging to different steady-state error levels\",\n    \"Initial error spike around 0.25 at iteration 0\",\n    \"Several curves stabilizing around 0.12-0.22 range\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do curves stabilize at different error levels between 0.12 and 0.22 when varying γ parameter?\",\n      \"answer\": \"Each curve corresponds to a different choice of the fairness parameter γ, and Learner chooses a linear threshold function over all 122 features at each round, producing different error-fairness tradeoffs.\",\n      \"visual_anchor\": \"Multiple colored curves converging to distinct steady-state values between 0.12 and 0.22 on the y-axis\",\n      \"text_evidence\": \"at each round Learner chooses a linear threshold function over all 122 features\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the curves reaching lowest error around 0.12 correspond to models with better cost sensitive classification implementation?\",\n      \"answer\": \"No, all curves use the same cost sensitive classification oracle implemented via a two stage regression procedure; the different error levels reflect different γ parameter choices, not implementation differences.\",\n      \"visual_anchor\": \"Lowest curves stabilizing near 0.12 error level after 2000 iterations\",\n      \"text_evidence\": \"We implement the cost sensitive classification oracle via a two stage regression procedure\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the paired unfairness γt relate to curves stabilizing at 0.16 error after 4000 iterations?\",\n      \"answer\": \"Each iteration t yields a pair of values ⟨εt, γt⟩, where εt is the error shown in the plot. Curves at 0.16 error represent specific Learner models with corresponding worst-case unfairness values from the auditing process.\",\n      \"visual_anchor\": \"Curves stabilizing at approximately 0.16 error level after 4000 iterations\",\n      \"text_evidence\": \"each iteration t, the two panels of Figure 1 yield a pair of realized values ⟨εt, γt⟩ from the experiment, corresponding to a Learner model whose error is εt\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Multiple colored curves showing error values from 0 to 8000 iterations",
        "Y-axis labeled 'error' ranging from 0.1 to 0.4",
        "X-axis showing iteration count up to 8000",
        "Title 'Learner: error, vs. t'",
        "Curves converging to different steady-state error levels",
        "Initial error spike around 0.25 at iteration 0",
        "Several curves stabilizing around 0.12-0.22 range"
      ],
      "queries": [
        {
          "query": "Why do curves stabilize at different error levels between 0.12 and 0.22 when varying γ parameter?",
          "answer": "Each curve corresponds to a different choice of the fairness parameter γ, and Learner chooses a linear threshold function over all 122 features at each round, producing different error-fairness tradeoffs.",
          "visual_anchor": "Multiple colored curves converging to distinct steady-state values between 0.12 and 0.22 on the y-axis",
          "text_evidence": "at each round Learner chooses a linear threshold function over all 122 features",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the curves reaching lowest error around 0.12 correspond to models with better cost sensitive classification implementation?",
          "answer": "No, all curves use the same cost sensitive classification oracle implemented via a two stage regression procedure; the different error levels reflect different γ parameter choices, not implementation differences.",
          "visual_anchor": "Lowest curves stabilizing near 0.12 error level after 2000 iterations",
          "text_evidence": "We implement the cost sensitive classification oracle via a two stage regression procedure",
          "query_type": "value_context"
        },
        {
          "query": "How does the paired unfairness γt relate to curves stabilizing at 0.16 error after 4000 iterations?",
          "answer": "Each iteration t yields a pair of values ⟨εt, γt⟩, where εt is the error shown in the plot. Curves at 0.16 error represent specific Learner models with corresponding worst-case unfairness values from the auditing process.",
          "visual_anchor": "Curves stabilizing at approximately 0.16 error level after 4000 iterations",
          "text_evidence": "each iteration t, the two panels of Figure 1 yield a pair of realized values ⟨εt, γt⟩ from the experiment, corresponding to a Learner model whose error is εt",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do curves stabilize at different error levels between 0.12 and 0.22 when varying γ parameter?",
        "answer": "Each curve corresponds to a different choice of the fairness parameter γ, and Learner chooses a linear threshold function over all 122 features at each round, producing different error-fairness tradeoffs.",
        "visual_anchor": "Multiple colored curves converging to distinct steady-state values between 0.12 and 0.22 on the y-axis",
        "text_evidence": "at each round Learner chooses a linear threshold function over all 122 features",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the curves reaching lowest error around 0.12 correspond to models with better cost sensitive classification implementation?",
        "answer": "No, all curves use the same cost sensitive classification oracle implemented via a two stage regression procedure; the different error levels reflect different γ parameter choices, not implementation differences.",
        "visual_anchor": "Lowest curves stabilizing near 0.12 error level after 2000 iterations",
        "text_evidence": "We implement the cost sensitive classification oracle via a two stage regression procedure",
        "query_type": "value_context"
      },
      {
        "query": "How does the paired unfairness γt relate to curves stabilizing at 0.16 error after 4000 iterations?",
        "answer": "Each iteration t yields a pair of values ⟨εt, γt⟩, where εt is the error shown in the plot. Curves at 0.16 error represent specific Learner models with corresponding worst-case unfairness values from the auditing process.",
        "visual_anchor": "Curves stabilizing at approximately 0.16 error level after 4000 iterations",
        "text_evidence": "each iteration t, the two panels of Figure 1 yield a pair of realized values ⟨εt, γt⟩ from the experiment, corresponding to a Learner model whose error is εt",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Multiple colored curves showing error values from 0 to 8000 iterations",
      "Y-axis labeled 'error' ranging from 0.1 to 0.4",
      "X-axis showing iteration count up to 8000",
      "Title 'Learner: error, vs. t'",
      "Curves converging to different steady-state error levels",
      "Initial error spike around 0.25 at iteration 0",
      "Several curves stabilizing around 0.12-0.22 range"
    ]
  },
  {
    "doc_id": "1711.05144",
    "figure_id": "1711.05144_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
    "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Pareto frontier curves color-coded by gamma parameter\",\n    \"gamma values ranging from approximately 0.01 to 0.026 on y-axis\",\n    \"x-axis ranging from 0.1 to 0.3\",\n    \"Multiple colored trajectories (blue, red, green, purple, orange, cyan, yellow)\",\n    \"Dense clustering of points in the 0.18-0.22 x-axis range\",\n    \"Descending trend from upper-left to lower-right\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?\",\n      \"answer\": \"Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.\",\n      \"visual_anchor\": \"Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left\",\n      \"text_evidence\": \"Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the dense clustering of points between x-values 0.18-0.22 reflect the smoother sampling mentioned for aggregate analysis?\",\n      \"answer\": \"Yes, the dense clustering reflects the denser gamma sampling used to achieve a smoother Pareto frontier. This sampling strategy creates more closely-spaced optimal points in that region.\",\n      \"visual_anchor\": \"Dense cluster of colored points concentrated in the 0.18-0.22 x-axis range\",\n      \"text_evidence\": \"Here the γ values cover the same range but are sampled more densely to get a smoother frontier\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the descending pattern from gamma 0.026 to lower values relate to fairness protection rules?\",\n      \"answer\": \"The descending pattern shows the error-unfairness trade-off as gamma varies, which is relevant to understanding how algorithmic fairness rules balance accuracy against disparate impact, similar to concerns about differential protection.\",\n      \"visual_anchor\": \"Descending curves from y-value 0.026 at upper-left to lower values at right\",\n      \"text_evidence\": \"Facebooks secret censorship rules protect white men from hate speech but not black children\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Pareto frontier curves color-coded by gamma parameter",
        "gamma values ranging from approximately 0.01 to 0.026 on y-axis",
        "x-axis ranging from 0.1 to 0.3",
        "Multiple colored trajectories (blue, red, green, purple, orange, cyan, yellow)",
        "Dense clustering of points in the 0.18-0.22 x-axis range",
        "Descending trend from upper-left to lower-right"
      ],
      "queries": [
        {
          "query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
          "answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
          "visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
          "text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the dense clustering of points between x-values 0.18-0.22 reflect the smoother sampling mentioned for aggregate analysis?",
          "answer": "Yes, the dense clustering reflects the denser gamma sampling used to achieve a smoother Pareto frontier. This sampling strategy creates more closely-spaced optimal points in that region.",
          "visual_anchor": "Dense cluster of colored points concentrated in the 0.18-0.22 x-axis range",
          "text_evidence": "Here the γ values cover the same range but are sampled more densely to get a smoother frontier",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the descending pattern from gamma 0.026 to lower values relate to fairness protection rules?",
          "answer": "The descending pattern shows the error-unfairness trade-off as gamma varies, which is relevant to understanding how algorithmic fairness rules balance accuracy against disparate impact, similar to concerns about differential protection.",
          "visual_anchor": "Descending curves from y-value 0.026 at upper-left to lower values at right",
          "text_evidence": "Facebooks secret censorship rules protect white men from hate speech but not black children",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the color-coded trajectories form distinct curves rather than a single frontier, given the reductions approach?",
        "answer": "Each colored trajectory represents Pareto-optimal solutions for a specific gamma parameter value. The reductions approach by Agarwal et al. generates different error-unfairness trade-offs as gamma varies.",
        "visual_anchor": "Multiple distinct colored curves (blue, red, green, purple, orange) descending from upper-left",
        "text_evidence": "Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the dense clustering of points between x-values 0.18-0.22 reflect the smoother sampling mentioned for aggregate analysis?",
        "answer": "Yes, the dense clustering reflects the denser gamma sampling used to achieve a smoother Pareto frontier. This sampling strategy creates more closely-spaced optimal points in that region.",
        "visual_anchor": "Dense cluster of colored points concentrated in the 0.18-0.22 x-axis range",
        "text_evidence": "Here the γ values cover the same range but are sampled more densely to get a smoother frontier",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the descending pattern from gamma 0.026 to lower values relate to fairness protection rules?",
        "answer": "The descending pattern shows the error-unfairness trade-off as gamma varies, which is relevant to understanding how algorithmic fairness rules balance accuracy against disparate impact, similar to concerns about differential protection.",
        "visual_anchor": "Descending curves from y-value 0.026 at upper-left to lower values at right",
        "text_evidence": "Facebooks secret censorship rules protect white men from hate speech but not black children",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Pareto frontier curves color-coded by gamma parameter",
      "gamma values ranging from approximately 0.01 to 0.026 on y-axis",
      "x-axis ranging from 0.1 to 0.3",
      "Multiple colored trajectories (blue, red, green, purple, orange, cyan, yellow)",
      "Dense clustering of points in the 0.18-0.22 x-axis range",
      "Descending trend from upper-left to lower-right"
    ]
  },
  {
    "doc_id": "1711.07076",
    "figure_id": "1711.07076_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg",
    "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"vertical cyan line at approximately 10 years work experience\",\n    \"dashed diagonal blue line descending from top-left to bottom-right\",\n    \"orange triangles representing one group in upper-left region\",\n    \"blue circles and inverted triangles distributed across the plot\",\n    \"hair length on y-axis (0-30+ cm)\",\n    \"work experience on x-axis (0-20 years)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the dashed diagonal line achieve near-parity compared to the vertical cyan line in hiring rates?\",\n      \"answer\": \"The dashed diagonal DLP achieves near-parity by differentiating based on hair length (an irrelevant attribute) rather than only work experience, which the vertical unconstrained classifier uses. This allows it to balance hiring rates between men and women through treatment disparity.\",\n      \"visual_anchor\": \"dashed diagonal blue line versus vertical cyan line\",\n      \"text_evidence\": \"An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why are orange triangles concentrated above 20 cm hair length while blue markers span the full range?\",\n      \"answer\": \"The orange triangles represent women who generally have longer hair (20-30+ cm), while blue markers represent men with varying hair lengths. This clustering demonstrates how the DLP uses an irrelevant attribute (hair length) correlated with gender to achieve impact parity.\",\n      \"visual_anchor\": \"orange triangles clustered above 20 cm versus blue markers spanning 0-30 cm\",\n      \"text_evidence\": \"The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the dashed line's negative slope reconcile treatment and impact disparity as described for disparate learning processes?\",\n      \"answer\": \"Yes, the negative slope shows the DLP achieving impact parity through purposeful treatment disparity by incorporating hair length. This directly demonstrates the approach of reconciling the two parities mentioned in the fairness literature.\",\n      \"visual_anchor\": \"dashed diagonal line with negative slope\",\n      \"text_evidence\": \"Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning pro\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "vertical cyan line at approximately 10 years work experience",
        "dashed diagonal blue line descending from top-left to bottom-right",
        "orange triangles representing one group in upper-left region",
        "blue circles and inverted triangles distributed across the plot",
        "hair length on y-axis (0-30+ cm)",
        "work experience on x-axis (0-20 years)"
      ],
      "queries": [
        {
          "query": "How does the dashed diagonal line achieve near-parity compared to the vertical cyan line in hiring rates?",
          "answer": "The dashed diagonal DLP achieves near-parity by differentiating based on hair length (an irrelevant attribute) rather than only work experience, which the vertical unconstrained classifier uses. This allows it to balance hiring rates between men and women through treatment disparity.",
          "visual_anchor": "dashed diagonal blue line versus vertical cyan line",
          "text_evidence": "An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why are orange triangles concentrated above 20 cm hair length while blue markers span the full range?",
          "answer": "The orange triangles represent women who generally have longer hair (20-30+ cm), while blue markers represent men with varying hair lengths. This clustering demonstrates how the DLP uses an irrelevant attribute (hair length) correlated with gender to achieve impact parity.",
          "visual_anchor": "orange triangles clustered above 20 cm versus blue markers spanning 0-30 cm",
          "text_evidence": "The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the dashed line's negative slope reconcile treatment and impact disparity as described for disparate learning processes?",
          "answer": "Yes, the negative slope shows the DLP achieving impact parity through purposeful treatment disparity by incorporating hair length. This directly demonstrates the approach of reconciling the two parities mentioned in the fairness literature.",
          "visual_anchor": "dashed diagonal line with negative slope",
          "text_evidence": "Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning pro",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the dashed diagonal line achieve near-parity compared to the vertical cyan line in hiring rates?",
        "answer": "The dashed diagonal DLP achieves near-parity by differentiating based on hair length (an irrelevant attribute) rather than only work experience, which the vertical unconstrained classifier uses. This allows it to balance hiring rates between men and women through treatment disparity.",
        "visual_anchor": "dashed diagonal blue line versus vertical cyan line",
        "text_evidence": "An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why are orange triangles concentrated above 20 cm hair length while blue markers span the full range?",
        "answer": "The orange triangles represent women who generally have longer hair (20-30+ cm), while blue markers represent men with varying hair lengths. This clustering demonstrates how the DLP uses an irrelevant attribute (hair length) correlated with gender to achieve impact parity.",
        "visual_anchor": "orange triangles clustered above 20 cm versus blue markers spanning 0-30 cm",
        "text_evidence": "The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the dashed line's negative slope reconcile treatment and impact disparity as described for disparate learning processes?",
        "answer": "Yes, the negative slope shows the DLP achieving impact parity through purposeful treatment disparity by incorporating hair length. This directly demonstrates the approach of reconciling the two parities mentioned in the fairness literature.",
        "visual_anchor": "dashed diagonal line with negative slope",
        "text_evidence": "Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning pro",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "vertical cyan line at approximately 10 years work experience",
      "dashed diagonal blue line descending from top-left to bottom-right",
      "orange triangles representing one group in upper-left region",
      "blue circles and inverted triangles distributed across the plot",
      "hair length on y-axis (0-30+ cm)",
      "work experience on x-axis (0-20 years)"
    ]
  },
  {
    "doc_id": "1711.07076",
    "figure_id": "1711.07076_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg",
    "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Downward yellow triangles indicating rejected individuals\",\n    \"Upward blue triangles indicating accepted individuals\",\n    \"Blue and yellow dots for unchanged decisions\",\n    \"Horizontal dashed line at y=0.5\",\n    \"X-axis labeled p(female)\",\n    \"Y-axis labeled p(admit|unconstrained)\",\n    \"Scatter plot with approximately 4000 dots\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why are downward triangles clustered near the horizontal dashed line at y=0.5 admission probability?\",\n      \"answer\": \"Students whose decisions are flipped after applying the fairness constraint tend to be those close to the decision boundary, which is represented by the y=0.5 threshold on the admission probability axis.\",\n      \"visual_anchor\": \"Downward triangles near horizontal dashed line at y=0.5\",\n      \"text_evidence\": \"Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are 'flipped' (after applying the fairness constraint) tend to be those close to the decision boundary.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the upward blue triangles support the claim that males looking like females benefit from the DLP?\",\n      \"answer\": \"Yes, the upward triangles indicate individuals accepted only by the DLP. Many of these are males with high p(female) values on the x-axis, confirming that males who 'look like' females based on other features benefit from the DLP.\",\n      \"visual_anchor\": \"Upward blue triangles at high x-axis p(female) values\",\n      \"text_evidence\": \"Many students benefiting from the DLP are males who 'look like' females based on other features, whereas females who 'look like' males are hurt by the DLP.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What percentage of the total 4000 blue and yellow dots were used for training versus testing?\",\n      \"answer\": \"70% of the observations (approximately 2800 dots) were used for training, while the remaining 30% (approximately 1200 dots) were reserved for model testing on the unseen test data shown in the plot.\",\n      \"visual_anchor\": \"Blue and yellow dots representing approximately 4000 unchanged decisions\",\n      \"text_evidence\": \"To construct the data, we sample $n _ { \\\\mathrm { a l l } } = 2 0 0 0$ total observations from the data-generating process described below. $70 \\\\%$ of the observations are used for training, and the remaining $30 \\\\%$ are reserved for model testing.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Downward yellow triangles indicating rejected individuals",
        "Upward blue triangles indicating accepted individuals",
        "Blue and yellow dots for unchanged decisions",
        "Horizontal dashed line at y=0.5",
        "X-axis labeled p(female)",
        "Y-axis labeled p(admit|unconstrained)",
        "Scatter plot with approximately 4000 dots"
      ],
      "queries": [
        {
          "query": "Why are downward triangles clustered near the horizontal dashed line at y=0.5 admission probability?",
          "answer": "Students whose decisions are flipped after applying the fairness constraint tend to be those close to the decision boundary, which is represented by the y=0.5 threshold on the admission probability axis.",
          "visual_anchor": "Downward triangles near horizontal dashed line at y=0.5",
          "text_evidence": "Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are 'flipped' (after applying the fairness constraint) tend to be those close to the decision boundary.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the upward blue triangles support the claim that males looking like females benefit from the DLP?",
          "answer": "Yes, the upward triangles indicate individuals accepted only by the DLP. Many of these are males with high p(female) values on the x-axis, confirming that males who 'look like' females based on other features benefit from the DLP.",
          "visual_anchor": "Upward blue triangles at high x-axis p(female) values",
          "text_evidence": "Many students benefiting from the DLP are males who 'look like' females based on other features, whereas females who 'look like' males are hurt by the DLP.",
          "query_type": "value_context"
        },
        {
          "query": "What percentage of the total 4000 blue and yellow dots were used for training versus testing?",
          "answer": "70% of the observations (approximately 2800 dots) were used for training, while the remaining 30% (approximately 1200 dots) were reserved for model testing on the unseen test data shown in the plot.",
          "visual_anchor": "Blue and yellow dots representing approximately 4000 unchanged decisions",
          "text_evidence": "To construct the data, we sample $n _ { \\mathrm { a l l } } = 2 0 0 0$ total observations from the data-generating process described below. $70 \\%$ of the observations are used for training, and the remaining $30 \\%$ are reserved for model testing.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why are downward triangles clustered near the horizontal dashed line at y=0.5 admission probability?",
        "answer": "Students whose decisions are flipped after applying the fairness constraint tend to be those close to the decision boundary, which is represented by the y=0.5 threshold on the admission probability axis.",
        "visual_anchor": "Downward triangles near horizontal dashed line at y=0.5",
        "text_evidence": "Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are 'flipped' (after applying the fairness constraint) tend to be those close to the decision boundary.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the upward blue triangles support the claim that males looking like females benefit from the DLP?",
        "answer": "Yes, the upward triangles indicate individuals accepted only by the DLP. Many of these are males with high p(female) values on the x-axis, confirming that males who 'look like' females based on other features benefit from the DLP.",
        "visual_anchor": "Upward blue triangles at high x-axis p(female) values",
        "text_evidence": "Many students benefiting from the DLP are males who 'look like' females based on other features, whereas females who 'look like' males are hurt by the DLP.",
        "query_type": "value_context"
      },
      {
        "query": "What percentage of the total 4000 blue and yellow dots were used for training versus testing?",
        "answer": "70% of the observations (approximately 2800 dots) were used for training, while the remaining 30% (approximately 1200 dots) were reserved for model testing on the unseen test data shown in the plot.",
        "visual_anchor": "Blue and yellow dots representing approximately 4000 unchanged decisions",
        "text_evidence": "To construct the data, we sample $n _ { \\mathrm { a l l } } = 2 0 0 0$ total observations from the data-generating process described below. $70 \\%$ of the observations are used for training, and the remaining $30 \\%$ are reserved for model testing.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Downward yellow triangles indicating rejected individuals",
      "Upward blue triangles indicating accepted individuals",
      "Blue and yellow dots for unchanged decisions",
      "Horizontal dashed line at y=0.5",
      "X-axis labeled p(female)",
      "Y-axis labeled p(admit|unconstrained)",
      "Scatter plot with approximately 4000 dots"
    ]
  },
  {
    "doc_id": "1711.08536",
    "figure_id": "1711.08536_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg",
    "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"US: 32.1% (large yellow-green segment)\", \"GB: 12.9% (green segment)\", \"FR: 4.3%\", \"ES: 4.1%\", \"CA: 3.7%\", \"MX: 1.1%\", \"AT: 0.7%\", \"IE: 0.5%\", \"WF: 0.0%\", \"Pie chart with country-level distribution\", \"Two-letter ISO country codes as labels\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the 32.1% US dominance in the yellow segment support concerns about limited geo-diversity affecting developing world applications?\",\n      \"answer\": \"Yes, the US dominance at 32.1% indicates limited geo-diversity, which is problematic since practitioners use these datasets for developing world applications where resources are limited but local representation may be insufficient.\",\n      \"visual_anchor\": \"US: 32.1% (large yellow-green segment)\",\n      \"text_evidence\": \"This is particularly desirable when using ML for the developing world, where resources for creating new data sets may be limited. However, if these data sets are not representative of the locations of interest, predictive performance of models may suffer.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the top two segments totaling 45% contrast with the claim about difficulty identifying geo-location for every image?\",\n      \"answer\": \"Despite difficulty in identifying geo-location for every image, proxy information like textual metadata and URL data allowed recovery of reasonably reliable country-level location information for a large number of images, enabling this distribution analysis.\",\n      \"visual_anchor\": \"US (32.1%) and GB (12.9%) segments combining for approximately 45%\",\n      \"text_evidence\": \"It is naturally difficult to identify the geo-location of every image in the open source data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the concentration in few large segments explain why ImageNet shows a similar distribution pattern at country level?\",\n      \"answer\": \"Yes, the concentration in a small number of countries (US, GB, FR dominating) mirrors ImageNet's pattern. Both datasets are similarly dominated by the same few countries despite ImageNet having lower coverage.\",\n      \"visual_anchor\": \"Large segments for US (32.1%), GB (12.9%), FR (4.3%) versus many small segments\",\n      \"text_evidence\": \"We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "US: 32.1% (large yellow-green segment)",
        "GB: 12.9% (green segment)",
        "FR: 4.3%",
        "ES: 4.1%",
        "CA: 3.7%",
        "MX: 1.1%",
        "AT: 0.7%",
        "IE: 0.5%",
        "WF: 0.0%",
        "Pie chart with country-level distribution",
        "Two-letter ISO country codes as labels"
      ],
      "queries": [
        {
          "query": "Does the 32.1% US dominance in the yellow segment support concerns about limited geo-diversity affecting developing world applications?",
          "answer": "Yes, the US dominance at 32.1% indicates limited geo-diversity, which is problematic since practitioners use these datasets for developing world applications where resources are limited but local representation may be insufficient.",
          "visual_anchor": "US: 32.1% (large yellow-green segment)",
          "text_evidence": "This is particularly desirable when using ML for the developing world, where resources for creating new data sets may be limited. However, if these data sets are not representative of the locations of interest, predictive performance of models may suffer.",
          "query_type": "value_context"
        },
        {
          "query": "Why do the top two segments totaling 45% contrast with the claim about difficulty identifying geo-location for every image?",
          "answer": "Despite difficulty in identifying geo-location for every image, proxy information like textual metadata and URL data allowed recovery of reasonably reliable country-level location information for a large number of images, enabling this distribution analysis.",
          "visual_anchor": "US (32.1%) and GB (12.9%) segments combining for approximately 45%",
          "text_evidence": "It is naturally difficult to identify the geo-location of every image in the open source data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the concentration in few large segments explain why ImageNet shows a similar distribution pattern at country level?",
          "answer": "Yes, the concentration in a small number of countries (US, GB, FR dominating) mirrors ImageNet's pattern. Both datasets are similarly dominated by the same few countries despite ImageNet having lower coverage.",
          "visual_anchor": "Large segments for US (32.1%), GB (12.9%), FR (4.3%) versus many small segments",
          "text_evidence": "We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 32.1% US dominance in the yellow segment support concerns about limited geo-diversity affecting developing world applications?",
        "answer": "Yes, the US dominance at 32.1% indicates limited geo-diversity, which is problematic since practitioners use these datasets for developing world applications where resources are limited but local representation may be insufficient.",
        "visual_anchor": "US: 32.1% (large yellow-green segment)",
        "text_evidence": "This is particularly desirable when using ML for the developing world, where resources for creating new data sets may be limited. However, if these data sets are not representative of the locations of interest, predictive performance of models may suffer.",
        "query_type": "value_context"
      },
      {
        "query": "Why do the top two segments totaling 45% contrast with the claim about difficulty identifying geo-location for every image?",
        "answer": "Despite difficulty in identifying geo-location for every image, proxy information like textual metadata and URL data allowed recovery of reasonably reliable country-level location information for a large number of images, enabling this distribution analysis.",
        "visual_anchor": "US (32.1%) and GB (12.9%) segments combining for approximately 45%",
        "text_evidence": "It is naturally difficult to identify the geo-location of every image in the open source data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the concentration in few large segments explain why ImageNet shows a similar distribution pattern at country level?",
        "answer": "Yes, the concentration in a small number of countries (US, GB, FR dominating) mirrors ImageNet's pattern. Both datasets are similarly dominated by the same few countries despite ImageNet having lower coverage.",
        "visual_anchor": "Large segments for US (32.1%), GB (12.9%), FR (4.3%) versus many small segments",
        "text_evidence": "We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "US: 32.1% (large yellow-green segment)",
      "GB: 12.9% (green segment)",
      "FR: 4.3%",
      "ES: 4.1%",
      "CA: 3.7%",
      "MX: 1.1%",
      "AT: 0.7%",
      "IE: 0.5%",
      "WF: 0.0%",
      "Pie chart with country-level distribution",
      "Two-letter ISO country codes as labels"
    ]
  },
  {
    "doc_id": "1711.08536",
    "figure_id": "1711.08536_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg",
    "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue curve (imagenet images) with peak around log-likelihood 0\",\n    \"Green curve (rater images) with peak around log-likelihood -6\",\n    \"Density on y-axis ranging from 0.0 to 0.5\",\n    \"Log-likelihood on x-axis ranging from -12 to 2\",\n    \"Two distinct distribution peaks separated by approximately 6 units\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green rater curve peak at log-likelihood -6 while the blue standard test curve peaks near 0?\",\n      \"answer\": \"Hyderabad-located crowdsourced groom/bridegroom images are dramatically less likely to be recognized correctly by ImageNet-trained models compared to standard test set images, showing a 6-unit log-likelihood gap.\",\n      \"visual_anchor\": \"Green curve peak at -6 versus blue curve peak at 0\",\n      \"text_evidence\": \"the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the blue density peak reaching 0.47 at log-likelihood 0 indicate better model confidence than the green peak at 0.32?\",\n      \"answer\": \"Yes, the higher blue peak at 0.47 indicates ImageNet models assign much higher confidence (log-likelihood near 0) to standard test images compared to crowdsourced Hyderabad images which peak lower at 0.32 around log-likelihood -6.\",\n      \"visual_anchor\": \"Blue peak density of 0.47 versus green peak density of 0.32\",\n      \"text_evidence\": \"ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the minimal overlap between the green and blue distributions across the -12 to 2 log-likelihood range?\",\n      \"answer\": \"The lack of geo-diversity in ImageNet training data causes poor classification performance on geographically diverse images. Hyderabad-sourced images receive systematically lower log-likelihoods, creating separated distributions with minimal overlap.\",\n      \"visual_anchor\": \"Separation between green and blue curves with minimal overlap from -12 to 2\",\n      \"text_evidence\": \"We examined how lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue curve (imagenet images) with peak around log-likelihood 0",
        "Green curve (rater images) with peak around log-likelihood -6",
        "Density on y-axis ranging from 0.0 to 0.5",
        "Log-likelihood on x-axis ranging from -12 to 2",
        "Two distinct distribution peaks separated by approximately 6 units"
      ],
      "queries": [
        {
          "query": "Why does the green rater curve peak at log-likelihood -6 while the blue standard test curve peaks near 0?",
          "answer": "Hyderabad-located crowdsourced groom/bridegroom images are dramatically less likely to be recognized correctly by ImageNet-trained models compared to standard test set images, showing a 6-unit log-likelihood gap.",
          "visual_anchor": "Green curve peak at -6 versus blue curve peak at 0",
          "text_evidence": "the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the blue density peak reaching 0.47 at log-likelihood 0 indicate better model confidence than the green peak at 0.32?",
          "answer": "Yes, the higher blue peak at 0.47 indicates ImageNet models assign much higher confidence (log-likelihood near 0) to standard test images compared to crowdsourced Hyderabad images which peak lower at 0.32 around log-likelihood -6.",
          "visual_anchor": "Blue peak density of 0.47 versus green peak density of 0.32",
          "text_evidence": "ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the minimal overlap between the green and blue distributions across the -12 to 2 log-likelihood range?",
          "answer": "The lack of geo-diversity in ImageNet training data causes poor classification performance on geographically diverse images. Hyderabad-sourced images receive systematically lower log-likelihoods, creating separated distributions with minimal overlap.",
          "visual_anchor": "Separation between green and blue curves with minimal overlap from -12 to 2",
          "text_evidence": "We examined how lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green rater curve peak at log-likelihood -6 while the blue standard test curve peaks near 0?",
        "answer": "Hyderabad-located crowdsourced groom/bridegroom images are dramatically less likely to be recognized correctly by ImageNet-trained models compared to standard test set images, showing a 6-unit log-likelihood gap.",
        "visual_anchor": "Green curve peak at -6 versus blue curve peak at 0",
        "text_evidence": "the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the blue density peak reaching 0.47 at log-likelihood 0 indicate better model confidence than the green peak at 0.32?",
        "answer": "Yes, the higher blue peak at 0.47 indicates ImageNet models assign much higher confidence (log-likelihood near 0) to standard test images compared to crowdsourced Hyderabad images which peak lower at 0.32 around log-likelihood -6.",
        "visual_anchor": "Blue peak density of 0.47 versus green peak density of 0.32",
        "text_evidence": "ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the minimal overlap between the green and blue distributions across the -12 to 2 log-likelihood range?",
        "answer": "The lack of geo-diversity in ImageNet training data causes poor classification performance on geographically diverse images. Hyderabad-sourced images receive systematically lower log-likelihoods, creating separated distributions with minimal overlap.",
        "visual_anchor": "Separation between green and blue curves with minimal overlap from -12 to 2",
        "text_evidence": "We examined how lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue curve (imagenet images) with peak around log-likelihood 0",
      "Green curve (rater images) with peak around log-likelihood -6",
      "Density on y-axis ranging from 0.0 to 0.5",
      "Log-likelihood on x-axis ranging from -12 to 2",
      "Two distinct distribution peaks separated by approximately 6 units"
    ]
  },
  {
    "doc_id": "1711.08536",
    "figure_id": "1711.08536_fig_7",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg",
    "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Density curves for eight countries (United_States, Australia, Ethiopia, Mexico, China, Nigeria, Pakistan, Brazil)\",\n    \"Log-likelihood values on x-axis ranging from -10 to 2\",\n    \"Density values on y-axis ranging from 0.0 to 0.6\",\n    \"Peak density around 0.5 for United_States curve\",\n    \"Non-US curves generally shifted left with lower log-likelihood peaks\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do non-US bridegroom density peaks fall between -2 and 0 while the US peak reaches 0.5 density?\",\n      \"answer\": \"Groom images with non-US location tags tend to have lower likelihoods than groom images from the US, reflecting lack of geo-diversity in the Open Images training data that impacts classification performance on images from broader locations.\",\n      \"visual_anchor\": \"US curve peak at approximately 0.5 density versus non-US curves peaking at lower log-likelihood values between -2 and 0\",\n      \"text_evidence\": \"Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the leftward shift of Ethiopia and Nigeria curves below log-likelihood -2 support concerns about geo-diversity?\",\n      \"answer\": \"Yes, the leftward shift indicates lower model likelihoods for these regions, directly supporting the finding that standard open source data sets like Open Images may not have sufficient geo-diversity for broad representation across the developing world.\",\n      \"visual_anchor\": \"Ethiopia and Nigeria density curves concentrated in the -4 to -2 log-likelihood range\",\n      \"text_evidence\": \"It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the clustering of all eight country curves between -4 and 2 relate to crowdsourced testing?\",\n      \"answer\": \"The clustering demonstrates geographic variation in model performance that was examined through crowdsourced data collection. Raters in Hyderabad were asked to find and return URLs of images matching particular labels to create stress-test data for assessing geo-diversity impact.\",\n      \"visual_anchor\": \"All eight country density curves clustered between log-likelihood values -4 and 2\",\n      \"text_evidence\": \"Our first method of collecting stress-test data was to ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Density curves for eight countries (United_States, Australia, Ethiopia, Mexico, China, Nigeria, Pakistan, Brazil)",
        "Log-likelihood values on x-axis ranging from -10 to 2",
        "Density values on y-axis ranging from 0.0 to 0.6",
        "Peak density around 0.5 for United_States curve",
        "Non-US curves generally shifted left with lower log-likelihood peaks"
      ],
      "queries": [
        {
          "query": "Why do non-US bridegroom density peaks fall between -2 and 0 while the US peak reaches 0.5 density?",
          "answer": "Groom images with non-US location tags tend to have lower likelihoods than groom images from the US, reflecting lack of geo-diversity in the Open Images training data that impacts classification performance on images from broader locations.",
          "visual_anchor": "US curve peak at approximately 0.5 density versus non-US curves peaking at lower log-likelihood values between -2 and 0",
          "text_evidence": "Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the leftward shift of Ethiopia and Nigeria curves below log-likelihood -2 support concerns about geo-diversity?",
          "answer": "Yes, the leftward shift indicates lower model likelihoods for these regions, directly supporting the finding that standard open source data sets like Open Images may not have sufficient geo-diversity for broad representation across the developing world.",
          "visual_anchor": "Ethiopia and Nigeria density curves concentrated in the -4 to -2 log-likelihood range",
          "text_evidence": "It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.",
          "query_type": "value_context"
        },
        {
          "query": "How does the clustering of all eight country curves between -4 and 2 relate to crowdsourced testing?",
          "answer": "The clustering demonstrates geographic variation in model performance that was examined through crowdsourced data collection. Raters in Hyderabad were asked to find and return URLs of images matching particular labels to create stress-test data for assessing geo-diversity impact.",
          "visual_anchor": "All eight country density curves clustered between log-likelihood values -4 and 2",
          "text_evidence": "Our first method of collecting stress-test data was to ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do non-US bridegroom density peaks fall between -2 and 0 while the US peak reaches 0.5 density?",
        "answer": "Groom images with non-US location tags tend to have lower likelihoods than groom images from the US, reflecting lack of geo-diversity in the Open Images training data that impacts classification performance on images from broader locations.",
        "visual_anchor": "US curve peak at approximately 0.5 density versus non-US curves peaking at lower log-likelihood values between -2 and 0",
        "text_evidence": "Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the leftward shift of Ethiopia and Nigeria curves below log-likelihood -2 support concerns about geo-diversity?",
        "answer": "Yes, the leftward shift indicates lower model likelihoods for these regions, directly supporting the finding that standard open source data sets like Open Images may not have sufficient geo-diversity for broad representation across the developing world.",
        "visual_anchor": "Ethiopia and Nigeria density curves concentrated in the -4 to -2 log-likelihood range",
        "text_evidence": "It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.",
        "query_type": "value_context"
      },
      {
        "query": "How does the clustering of all eight country curves between -4 and 2 relate to crowdsourced testing?",
        "answer": "The clustering demonstrates geographic variation in model performance that was examined through crowdsourced data collection. Raters in Hyderabad were asked to find and return URLs of images matching particular labels to create stress-test data for assessing geo-diversity impact.",
        "visual_anchor": "All eight country density curves clustered between log-likelihood values -4 and 2",
        "text_evidence": "Our first method of collecting stress-test data was to ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Density curves for eight countries (United_States, Australia, Ethiopia, Mexico, China, Nigeria, Pakistan, Brazil)",
      "Log-likelihood values on x-axis ranging from -10 to 2",
      "Density values on y-axis ranging from 0.0 to 0.6",
      "Peak density around 0.5 for United_States curve",
      "Non-US curves generally shifted left with lower log-likelihood peaks"
    ]
  },
  {
    "doc_id": "1711.08536",
    "figure_id": "1711.08536_fig_10",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg",
    "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four horizontal timelines for United States, Australia, Ethiopia, and Pakistan\",\n    \"Log-likelihood scale from 3e-3 to 4e-1\",\n    \"Circular photo thumbnails of bridegrooms positioned along each timeline\",\n    \"Dense clustering of photos at right end (higher likelihood) for United States and Australia\",\n    \"More dispersed photo distribution across timeline for Ethiopia and Pakistan\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why are Ethiopia and Pakistan photos more dispersed across the likelihood scale than the dense right-end clustering in United States?\",\n      \"answer\": \"Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia, resulting in lower and more variable log-likelihood scores rather than concentrated high-confidence predictions.\",\n      \"visual_anchor\": \"dispersed photo distribution from 1e-2 to 3e-1 for Ethiopia and Pakistan versus dense clustering at 2e-1 to 4e-1 for United States\",\n      \"text_evidence\": \"Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the sparse left-side positioning of Ethiopia bridegroom photos at 4e-3 to 1e-2 illustrate insufficient geo-diversity in training data?\",\n      \"answer\": \"Yes, the low log-likelihood assignments for Ethiopian images demonstrate that standard open source data sets like Open Images lack sufficient geo-diversity for broad representation across the developing world.\",\n      \"visual_anchor\": \"left-positioned photos at 3e-3 to 4e-3 and 4e-3 on Ethiopia timeline\",\n      \"text_evidence\": \"It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes Pakistan photos to scatter across 3e-3 to 4e-1 despite the classifier being trained on Open Images bridegroom class?\",\n      \"answer\": \"The classifier was trained on data sets designed for specific purposes, and later adoption for other geographic contexts introduces problems when the training data lacks representation from regions like Pakistan.\",\n      \"visual_anchor\": \"Pakistan timeline showing photos distributed from 3e-3 through 1e-1 to 4e-1 with no clear concentration\",\n      \"text_evidence\": \"these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four horizontal timelines for United States, Australia, Ethiopia, and Pakistan",
        "Log-likelihood scale from 3e-3 to 4e-1",
        "Circular photo thumbnails of bridegrooms positioned along each timeline",
        "Dense clustering of photos at right end (higher likelihood) for United States and Australia",
        "More dispersed photo distribution across timeline for Ethiopia and Pakistan"
      ],
      "queries": [
        {
          "query": "Why are Ethiopia and Pakistan photos more dispersed across the likelihood scale than the dense right-end clustering in United States?",
          "answer": "Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia, resulting in lower and more variable log-likelihood scores rather than concentrated high-confidence predictions.",
          "visual_anchor": "dispersed photo distribution from 1e-2 to 3e-1 for Ethiopia and Pakistan versus dense clustering at 2e-1 to 4e-1 for United States",
          "text_evidence": "Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the sparse left-side positioning of Ethiopia bridegroom photos at 4e-3 to 1e-2 illustrate insufficient geo-diversity in training data?",
          "answer": "Yes, the low log-likelihood assignments for Ethiopian images demonstrate that standard open source data sets like Open Images lack sufficient geo-diversity for broad representation across the developing world.",
          "visual_anchor": "left-positioned photos at 3e-3 to 4e-3 and 4e-3 on Ethiopia timeline",
          "text_evidence": "It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.",
          "query_type": "value_context"
        },
        {
          "query": "What causes Pakistan photos to scatter across 3e-3 to 4e-1 despite the classifier being trained on Open Images bridegroom class?",
          "answer": "The classifier was trained on data sets designed for specific purposes, and later adoption for other geographic contexts introduces problems when the training data lacks representation from regions like Pakistan.",
          "visual_anchor": "Pakistan timeline showing photos distributed from 3e-3 through 1e-1 to 4e-1 with no clear concentration",
          "text_evidence": "these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why are Ethiopia and Pakistan photos more dispersed across the likelihood scale than the dense right-end clustering in United States?",
        "answer": "Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia, resulting in lower and more variable log-likelihood scores rather than concentrated high-confidence predictions.",
        "visual_anchor": "dispersed photo distribution from 1e-2 to 3e-1 for Ethiopia and Pakistan versus dense clustering at 2e-1 to 4e-1 for United States",
        "text_evidence": "Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the sparse left-side positioning of Ethiopia bridegroom photos at 4e-3 to 1e-2 illustrate insufficient geo-diversity in training data?",
        "answer": "Yes, the low log-likelihood assignments for Ethiopian images demonstrate that standard open source data sets like Open Images lack sufficient geo-diversity for broad representation across the developing world.",
        "visual_anchor": "left-positioned photos at 3e-3 to 4e-3 and 4e-3 on Ethiopia timeline",
        "text_evidence": "It is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world.",
        "query_type": "value_context"
      },
      {
        "query": "What causes Pakistan photos to scatter across 3e-3 to 4e-1 despite the classifier being trained on Open Images bridegroom class?",
        "answer": "The classifier was trained on data sets designed for specific purposes, and later adoption for other geographic contexts introduces problems when the training data lacks representation from regions like Pakistan.",
        "visual_anchor": "Pakistan timeline showing photos distributed from 3e-3 through 1e-1 to 4e-1 with no clear concentration",
        "text_evidence": "these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four horizontal timelines for United States, Australia, Ethiopia, and Pakistan",
      "Log-likelihood scale from 3e-3 to 4e-1",
      "Circular photo thumbnails of bridegrooms positioned along each timeline",
      "Dense clustering of photos at right end (higher likelihood) for United States and Australia",
      "More dispersed photo distribution across timeline for Ethiopia and Pakistan"
    ]
  },
  {
    "doc_id": "1801.04385",
    "figure_id": "1801.04385_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg",
    "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"x-axis showing Answer Position from 1 to 8\", \"y-axis showing Acceptance Probability from 0.32 to 0.37\", \"black dots representing data points\", \"dashed black trend line with upward slope\", \"gray grid background\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the upward dashed trend line from position 1 to 8 support the claim that later answers are more likely accepted?\",\n      \"answer\": \"The upward trend in aggregated data suggests later answers are more likely accepted, but this is Simpson's paradox. When disaggregated by session length, the trend reverses.\",\n      \"visual_anchor\": \"dashed black trend line showing upward slope from acceptance probability ~0.34 at position 1 to ~0.35 at position 8\",\n      \"text_evidence\": \"Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does acceptance probability at position 7 reach approximately 0.352 despite the previously reported finding about answer position?\",\n      \"answer\": \"The previously reported finding that acceptance probability decreases with answer position is itself an instance of Simpson's paradox revealed by the method, contradicting the aggregated upward trend shown.\",\n      \"visual_anchor\": \"black dot at position 7 showing acceptance probability approximately 0.352\",\n      \"text_evidence\": \"Our approach reveals that the previously reported nding that acceptance probability decreases with answer position [9] is an instance of Sim\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How many Simpson's pairs were identified from the 110 possible ones to explain the acceptance probability pattern from 0.34 to 0.36?\",\n      \"answer\": \"Seven Simpson's pairs were identified among the 110 possible pairs from eleven variables in Stack Exchange data, which explain the paradoxical acceptance probability pattern.\",\n      \"visual_anchor\": \"acceptance probability range spanning from approximately 0.34 to 0.36 across positions 1 to 8\",\n      \"text_evidence\": \"e eleven variables in Stack Exchange data, result in 110 possible Simpson's pairs. Among these, our method identies seven as instance of paradox\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "x-axis showing Answer Position from 1 to 8",
        "y-axis showing Acceptance Probability from 0.32 to 0.37",
        "black dots representing data points",
        "dashed black trend line with upward slope",
        "gray grid background"
      ],
      "queries": [
        {
          "query": "Does the upward dashed trend line from position 1 to 8 support the claim that later answers are more likely accepted?",
          "answer": "The upward trend in aggregated data suggests later answers are more likely accepted, but this is Simpson's paradox. When disaggregated by session length, the trend reverses.",
          "visual_anchor": "dashed black trend line showing upward slope from acceptance probability ~0.34 at position 1 to ~0.35 at position 8",
          "text_evidence": "Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does acceptance probability at position 7 reach approximately 0.352 despite the previously reported finding about answer position?",
          "answer": "The previously reported finding that acceptance probability decreases with answer position is itself an instance of Simpson's paradox revealed by the method, contradicting the aggregated upward trend shown.",
          "visual_anchor": "black dot at position 7 showing acceptance probability approximately 0.352",
          "text_evidence": "Our approach reveals that the previously reported nding that acceptance probability decreases with answer position [9] is an instance of Sim",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How many Simpson's pairs were identified from the 110 possible ones to explain the acceptance probability pattern from 0.34 to 0.36?",
          "answer": "Seven Simpson's pairs were identified among the 110 possible pairs from eleven variables in Stack Exchange data, which explain the paradoxical acceptance probability pattern.",
          "visual_anchor": "acceptance probability range spanning from approximately 0.34 to 0.36 across positions 1 to 8",
          "text_evidence": "e eleven variables in Stack Exchange data, result in 110 possible Simpson's pairs. Among these, our method identies seven as instance of paradox",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the upward dashed trend line from position 1 to 8 support the claim that later answers are more likely accepted?",
        "answer": "The upward trend in aggregated data suggests later answers are more likely accepted, but this is Simpson's paradox. When disaggregated by session length, the trend reverses.",
        "visual_anchor": "dashed black trend line showing upward slope from acceptance probability ~0.34 at position 1 to ~0.35 at position 8",
        "text_evidence": "Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does acceptance probability at position 7 reach approximately 0.352 despite the previously reported finding about answer position?",
        "answer": "The previously reported finding that acceptance probability decreases with answer position is itself an instance of Simpson's paradox revealed by the method, contradicting the aggregated upward trend shown.",
        "visual_anchor": "black dot at position 7 showing acceptance probability approximately 0.352",
        "text_evidence": "Our approach reveals that the previously reported nding that acceptance probability decreases with answer position [9] is an instance of Sim",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How many Simpson's pairs were identified from the 110 possible ones to explain the acceptance probability pattern from 0.34 to 0.36?",
        "answer": "Seven Simpson's pairs were identified among the 110 possible pairs from eleven variables in Stack Exchange data, which explain the paradoxical acceptance probability pattern.",
        "visual_anchor": "acceptance probability range spanning from approximately 0.34 to 0.36 across positions 1 to 8",
        "text_evidence": "e eleven variables in Stack Exchange data, result in 110 possible Simpson's pairs. Among these, our method identies seven as instance of paradox",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "x-axis showing Answer Position from 1 to 8",
      "y-axis showing Acceptance Probability from 0.32 to 0.37",
      "black dots representing data points",
      "dashed black trend line with upward slope",
      "gray grid background"
    ]
  },
  {
    "doc_id": "1801.04385",
    "figure_id": "1801.04385_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg",
    "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Downward sloping trend lines in multiple colors\",\n    \"X-axis: Number of answers (0 to 1800)\",\n    \"Y-axis: Acceptance Probability (0 to 0.5)\",\n    \"Multiple colored markers (circles, diamonds, triangles, squares, plus signs)\",\n    \"Dashed and solid lines representing different reputation bins\",\n    \"Green circles showing highest acceptance probability at low answer counts\",\n    \"Gray triangles showing relatively flat trend\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do all colored trend lines slope downward when users with more lifetime answers should have higher acceptance probability?\",\n      \"answer\": \"This demonstrates Simpson's paradox: aggregated data shows upward trend, but when disaggregated by reputation bins (different colors), the trend reverses within each group.\",\n      \"visual_anchor\": \"Multiple downward-sloping colored trend lines across all reputation bins\",\n      \"text_evidence\": \"Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the green circles dropping from 0.24 at 100 answers to 0.10 at 150 answers contradict experience improving acceptance rates?\",\n      \"answer\": \"No, because among users with the same reputation level (green bin), those who posted more answers are less likely to have answers accepted as best.\",\n      \"visual_anchor\": \"Green circles showing steep decline from approximately 0.24 to 0.10 between 100-150 answers\",\n      \"text_evidence\": \"Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Are answers at 1000 lifetime posts with 0.06 acceptance probability from longer or shorter user sessions than expected?\",\n      \"answer\": \"The plot shows lifetime answer count, not session length. However, answers from longer sessions are more likely to be accepted than this data point suggests.\",\n      \"visual_anchor\": \"Cyan 'x' marker at approximately 1000 answers with acceptance probability near 0.06\",\n      \"text_evidence\": \"answers produced during longer sessions are more likely to be accepted than answers\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Downward sloping trend lines in multiple colors",
        "X-axis: Number of answers (0 to 1800)",
        "Y-axis: Acceptance Probability (0 to 0.5)",
        "Multiple colored markers (circles, diamonds, triangles, squares, plus signs)",
        "Dashed and solid lines representing different reputation bins",
        "Green circles showing highest acceptance probability at low answer counts",
        "Gray triangles showing relatively flat trend"
      ],
      "queries": [
        {
          "query": "Why do all colored trend lines slope downward when users with more lifetime answers should have higher acceptance probability?",
          "answer": "This demonstrates Simpson's paradox: aggregated data shows upward trend, but when disaggregated by reputation bins (different colors), the trend reverses within each group.",
          "visual_anchor": "Multiple downward-sloping colored trend lines across all reputation bins",
          "text_evidence": "Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the green circles dropping from 0.24 at 100 answers to 0.10 at 150 answers contradict experience improving acceptance rates?",
          "answer": "No, because among users with the same reputation level (green bin), those who posted more answers are less likely to have answers accepted as best.",
          "visual_anchor": "Green circles showing steep decline from approximately 0.24 to 0.10 between 100-150 answers",
          "text_evidence": "Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Are answers at 1000 lifetime posts with 0.06 acceptance probability from longer or shorter user sessions than expected?",
          "answer": "The plot shows lifetime answer count, not session length. However, answers from longer sessions are more likely to be accepted than this data point suggests.",
          "visual_anchor": "Cyan 'x' marker at approximately 1000 answers with acceptance probability near 0.06",
          "text_evidence": "answers produced during longer sessions are more likely to be accepted than answers",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do all colored trend lines slope downward when users with more lifetime answers should have higher acceptance probability?",
        "answer": "This demonstrates Simpson's paradox: aggregated data shows upward trend, but when disaggregated by reputation bins (different colors), the trend reverses within each group.",
        "visual_anchor": "Multiple downward-sloping colored trend lines across all reputation bins",
        "text_evidence": "Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the green circles dropping from 0.24 at 100 answers to 0.10 at 150 answers contradict experience improving acceptance rates?",
        "answer": "No, because among users with the same reputation level (green bin), those who posted more answers are less likely to have answers accepted as best.",
        "visual_anchor": "Green circles showing steep decline from approximately 0.24 to 0.10 between 100-150 answers",
        "text_evidence": "Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Are answers at 1000 lifetime posts with 0.06 acceptance probability from longer or shorter user sessions than expected?",
        "answer": "The plot shows lifetime answer count, not session length. However, answers from longer sessions are more likely to be accepted than this data point suggests.",
        "visual_anchor": "Cyan 'x' marker at approximately 1000 answers with acceptance probability near 0.06",
        "text_evidence": "answers produced during longer sessions are more likely to be accepted than answers",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Downward sloping trend lines in multiple colors",
      "X-axis: Number of answers (0 to 1800)",
      "Y-axis: Acceptance Probability (0 to 0.5)",
      "Multiple colored markers (circles, diamonds, triangles, squares, plus signs)",
      "Dashed and solid lines representing different reputation bins",
      "Green circles showing highest acceptance probability at low answer counts",
      "Gray triangles showing relatively flat trend"
    ]
  },
  {
    "doc_id": "1801.04385",
    "figure_id": "1801.04385_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
    "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Color gradient from blue (0.0) to red (0.7) representing Acceptance Probability\",\n    \"Logarithmic x-axis showing Number of Answers from 10^0 to 10^4\",\n    \"Logarithmic y-axis showing Reputation from 10^0 to 10^6\",\n    \"Dense blue region at lower reputation values\",\n    \"Red-shaded region at high reputation (10^5-10^6) and high answer counts (10^3-10^4)\",\n    \"Diagonal band pattern from lower-left to upper-right\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue region at reputation 10^1 show near-zero acceptance probability despite constant probability mass along same-length sessions?\",\n      \"answer\": \"The probability mass at Xp = a+1 is normalized to account for sessions of length a, causing the rate of change of probability masses with respect to Xp to affect acceptance probability differently at lower reputation values.\",\n      \"visual_anchor\": \"Blue region (acceptance probability near 0.0) at reputation values around 10^1\",\n      \"text_evidence\": \"the probability mass at Xp = a + 1 c is the same as that at Xp = a (as the num-Xp a Xp aber of data points is constant along sessions of same length) but normalized to account for the sessions of length a\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?\",\n      \"answer\": \"Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.\",\n      \"visual_anchor\": \"Diagonal gradient transitioning from blue (lower-left) to red (upper-right)\",\n      \"text_evidence\": \"e rate of change of these probability masses with respect to Xp is\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red concentration at reputation above 10^5 align with the probability mass function decreasing for higher Xc values?\",\n      \"answer\": \"Yes, the red region (high acceptance probability 0.6-0.7) at reputation above 10^5 corresponds to the stated behavior where the probability mass function Pr(Xc = xc | Xp = a) decreases for Xc.\",\n      \"visual_anchor\": \"Red region at reputation values 10^5 to 10^6 with acceptance probability 0.6-0.7\",\n      \"text_evidence\": \"e probability mass function Pr ( Xc = xc | Xp = a ) decreases for Xc\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Color gradient from blue (0.0) to red (0.7) representing Acceptance Probability",
        "Logarithmic x-axis showing Number of Answers from 10^0 to 10^4",
        "Logarithmic y-axis showing Reputation from 10^0 to 10^6",
        "Dense blue region at lower reputation values",
        "Red-shaded region at high reputation (10^5-10^6) and high answer counts (10^3-10^4)",
        "Diagonal band pattern from lower-left to upper-right"
      ],
      "queries": [
        {
          "query": "Why does the blue region at reputation 10^1 show near-zero acceptance probability despite constant probability mass along same-length sessions?",
          "answer": "The probability mass at Xp = a+1 is normalized to account for sessions of length a, causing the rate of change of probability masses with respect to Xp to affect acceptance probability differently at lower reputation values.",
          "visual_anchor": "Blue region (acceptance probability near 0.0) at reputation values around 10^1",
          "text_evidence": "the probability mass at Xp = a + 1 c is the same as that at Xp = a (as the num-Xp a Xp aber of data points is constant along sessions of same length) but normalized to account for the sessions of length a",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?",
          "answer": "Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.",
          "visual_anchor": "Diagonal gradient transitioning from blue (lower-left) to red (upper-right)",
          "text_evidence": "e rate of change of these probability masses with respect to Xp is",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red concentration at reputation above 10^5 align with the probability mass function decreasing for higher Xc values?",
          "answer": "Yes, the red region (high acceptance probability 0.6-0.7) at reputation above 10^5 corresponds to the stated behavior where the probability mass function Pr(Xc = xc | Xp = a) decreases for Xc.",
          "visual_anchor": "Red region at reputation values 10^5 to 10^6 with acceptance probability 0.6-0.7",
          "text_evidence": "e probability mass function Pr ( Xc = xc | Xp = a ) decreases for Xc",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue region at reputation 10^1 show near-zero acceptance probability despite constant probability mass along same-length sessions?",
        "answer": "The probability mass at Xp = a+1 is normalized to account for sessions of length a, causing the rate of change of probability masses with respect to Xp to affect acceptance probability differently at lower reputation values.",
        "visual_anchor": "Blue region (acceptance probability near 0.0) at reputation values around 10^1",
        "text_evidence": "the probability mass at Xp = a + 1 c is the same as that at Xp = a (as the num-Xp a Xp aber of data points is constant along sessions of same length) but normalized to account for the sessions of length a",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does acceptance probability change from the blue diagonal band to the red upper region as both reputation and answer count increase?",
        "answer": "Acceptance probability increases from near 0.0 (blue) to 0.7 (red) as reputation grows from 10^2 to 10^6 and answer count increases from 10^1 to 10^4, following the diagonal pattern.",
        "visual_anchor": "Diagonal gradient transitioning from blue (lower-left) to red (upper-right)",
        "text_evidence": "e rate of change of these probability masses with respect to Xp is",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red concentration at reputation above 10^5 align with the probability mass function decreasing for higher Xc values?",
        "answer": "Yes, the red region (high acceptance probability 0.6-0.7) at reputation above 10^5 corresponds to the stated behavior where the probability mass function Pr(Xc = xc | Xp = a) decreases for Xc.",
        "visual_anchor": "Red region at reputation values 10^5 to 10^6 with acceptance probability 0.6-0.7",
        "text_evidence": "e probability mass function Pr ( Xc = xc | Xp = a ) decreases for Xc",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Color gradient from blue (0.0) to red (0.7) representing Acceptance Probability",
      "Logarithmic x-axis showing Number of Answers from 10^0 to 10^4",
      "Logarithmic y-axis showing Reputation from 10^0 to 10^6",
      "Dense blue region at lower reputation values",
      "Red-shaded region at high reputation (10^5-10^6) and high answer counts (10^3-10^4)",
      "Diagonal band pattern from lower-left to upper-right"
    ]
  },
  {
    "doc_id": "1801.04385",
    "figure_id": "1801.04385_fig_7",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg",
    "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Six colored lines representing different reputation score ranges\",\n    \"Blue circles for Rep ≤ 10\",\n    \"Red diamonds for Rep ≥ 10^5\",\n    \"Black solid line showing Average\",\n    \"X-axis: Reputation per number of answers (log scale from 10^-1 to 10^3)\",\n    \"Y-axis: Acceptance Probability (0 to 0.8)\",\n    \"Purple and magenta lines showing intermediate reputation ranges\",\n    \"Much smaller variance across subgroups compared to referenced Fig. 2b\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do red diamonds reach 0.57 acceptance probability while blue circles plateau at 0.20, despite both groups showing upward trends?\",\n      \"answer\": \"The probability mass function increases at a greater rate for higher reputation subgroups (X_c > a) than the rate at which acceptance probability decreases for the smallest value subgroup, resulting in divergent upward trends when aggregated.\",\n      \"visual_anchor\": \"red diamonds (Rep ≥ 10^5) reaching 0.57 versus blue circles (Rep ≤ 10) at 0.20\",\n      \"text_evidence\": \"the rate of increase of this probability mass is greater than the rate at which the acceptance probability decreases, resulting in an upward trend when the data is aggregated\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the much smaller variance among the six colored subgroup lines validate Reputation Rate as a performance proxy?\",\n      \"answer\": \"Yes, the caption explicitly states that the much smaller variance compared to Fig. 2b suggests that Reputation Rate (reputation per number of answers) is a good proxy of answerer performance.\",\n      \"visual_anchor\": \"small variance among six colored lines (blue, purple, magenta, red) between 0.2 and 0.6\",\n      \"text_evidence\": \"e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does disaggregating by reputation ranges reveal the paradox that the method aims to uncover systematically?\",\n      \"answer\": \"The method identifies variable pairs where a trend in outcome disappears or reverses when data is disaggregated by conditioning on a second variable, creating subgroups. The plot shows such disaggregation with different reputation ranges revealing distinct acceptance probability patterns.\",\n      \"visual_anchor\": \"six distinct colored lines showing different trends when disaggregated by reputation ranges\",\n      \"text_evidence\": \"e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Six colored lines representing different reputation score ranges",
        "Blue circles for Rep ≤ 10",
        "Red diamonds for Rep ≥ 10^5",
        "Black solid line showing Average",
        "X-axis: Reputation per number of answers (log scale from 10^-1 to 10^3)",
        "Y-axis: Acceptance Probability (0 to 0.8)",
        "Purple and magenta lines showing intermediate reputation ranges",
        "Much smaller variance across subgroups compared to referenced Fig. 2b"
      ],
      "queries": [
        {
          "query": "Why do red diamonds reach 0.57 acceptance probability while blue circles plateau at 0.20, despite both groups showing upward trends?",
          "answer": "The probability mass function increases at a greater rate for higher reputation subgroups (X_c > a) than the rate at which acceptance probability decreases for the smallest value subgroup, resulting in divergent upward trends when aggregated.",
          "visual_anchor": "red diamonds (Rep ≥ 10^5) reaching 0.57 versus blue circles (Rep ≤ 10) at 0.20",
          "text_evidence": "the rate of increase of this probability mass is greater than the rate at which the acceptance probability decreases, resulting in an upward trend when the data is aggregated",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the much smaller variance among the six colored subgroup lines validate Reputation Rate as a performance proxy?",
          "answer": "Yes, the caption explicitly states that the much smaller variance compared to Fig. 2b suggests that Reputation Rate (reputation per number of answers) is a good proxy of answerer performance.",
          "visual_anchor": "small variance among six colored lines (blue, purple, magenta, red) between 0.2 and 0.6",
          "text_evidence": "e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance",
          "query_type": "value_context"
        },
        {
          "query": "How does disaggregating by reputation ranges reveal the paradox that the method aims to uncover systematically?",
          "answer": "The method identifies variable pairs where a trend in outcome disappears or reverses when data is disaggregated by conditioning on a second variable, creating subgroups. The plot shows such disaggregation with different reputation ranges revealing distinct acceptance probability patterns.",
          "visual_anchor": "six distinct colored lines showing different trends when disaggregated by reputation ranges",
          "text_evidence": "e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do red diamonds reach 0.57 acceptance probability while blue circles plateau at 0.20, despite both groups showing upward trends?",
        "answer": "The probability mass function increases at a greater rate for higher reputation subgroups (X_c > a) than the rate at which acceptance probability decreases for the smallest value subgroup, resulting in divergent upward trends when aggregated.",
        "visual_anchor": "red diamonds (Rep ≥ 10^5) reaching 0.57 versus blue circles (Rep ≤ 10) at 0.20",
        "text_evidence": "the rate of increase of this probability mass is greater than the rate at which the acceptance probability decreases, resulting in an upward trend when the data is aggregated",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the much smaller variance among the six colored subgroup lines validate Reputation Rate as a performance proxy?",
        "answer": "Yes, the caption explicitly states that the much smaller variance compared to Fig. 2b suggests that Reputation Rate (reputation per number of answers) is a good proxy of answerer performance.",
        "visual_anchor": "small variance among six colored lines (blue, purple, magenta, red) between 0.2 and 0.6",
        "text_evidence": "e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance",
        "query_type": "value_context"
      },
      {
        "query": "How does disaggregating by reputation ranges reveal the paradox that the method aims to uncover systematically?",
        "answer": "The method identifies variable pairs where a trend in outcome disappears or reverses when data is disaggregated by conditioning on a second variable, creating subgroups. The plot shows such disaggregation with different reputation ranges revealing distinct acceptance probability patterns.",
        "visual_anchor": "six distinct colored lines showing different trends when disaggregated by reputation ranges",
        "text_evidence": "e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Six colored lines representing different reputation score ranges",
      "Blue circles for Rep ≤ 10",
      "Red diamonds for Rep ≥ 10^5",
      "Black solid line showing Average",
      "X-axis: Reputation per number of answers (log scale from 10^-1 to 10^3)",
      "Y-axis: Acceptance Probability (0 to 0.8)",
      "Purple and magenta lines showing intermediate reputation ranges",
      "Much smaller variance across subgroups compared to referenced Fig. 2b"
    ]
  },
  {
    "doc_id": "1801.04385",
    "figure_id": "1801.04385_fig_8",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg",
    "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Heatmap with red and blue coloring representing acceptance probability\",\n    \"Y-axis: Answer Position (logarithmic scale from 10^0 to 10^2)\",\n    \"X-axis: Time Since Previous Answer (logarithmic scale from 10^1 to 10^8)\",\n    \"Color bar showing acceptance probability scale from 0.05 to 0.6\",\n    \"Dense cluster of data points in mid-range Answer Position (10^1 to 10^2)\",\n    \"Blue points concentrated at lower Answer Position values\",\n    \"Red points concentrated at higher Answer Position values\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue concentration at Answer Position below 10^1 illustrate a pair that multivariate logistic regression cannot find?\",\n      \"answer\": \"The method identifies pairs of variables where a trend in an outcome as a function of one variable disappears or reverses when conditioned on the second variable. Multivariate logistic regression fails to detect such Simpson's paradox instances in this variable pair.\",\n      \"visual_anchor\": \"blue concentration at Answer Position below 10^1\",\n      \"text_evidence\": \"We presented a method for systematically uncovering instances of Simpson's paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the red region at Answer Position 10^1 to 10^2 relate to subgroups within the population generating the data?\",\n      \"answer\": \"The disaggregated data shown in the red region corresponds to subgroups within the population, revealing different acceptance probability patterns when conditioned on Time Since Previous Answer.\",\n      \"visual_anchor\": \"red region at Answer Position 10^1 to 10^2\",\n      \"text_evidence\": \"e disaggregated data corresponds to subgroups within the population generating the data.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the acceptance probability gradient from blue to red across Answer Position values support funding for paradox detection research?\",\n      \"answer\": \"Yes, the systematic uncovering of Simpson's paradox instances demonstrated by the color gradient pattern received funding support from the James S. McDonnell Foundation, Air Force Office of Scientific Research, and Army Research Office.\",\n      \"visual_anchor\": \"acceptance probability gradient from blue (0.05-0.2) to red (0.4-0.6)\",\n      \"text_evidence\": \"We acknowledge funding support from the James S. McDonnell Foundation, Air Force Oce of Scientic Research (FA9550-17-1- 0327), and by the Army Research Oce (W911NF-15-1-0142).\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Heatmap with red and blue coloring representing acceptance probability",
        "Y-axis: Answer Position (logarithmic scale from 10^0 to 10^2)",
        "X-axis: Time Since Previous Answer (logarithmic scale from 10^1 to 10^8)",
        "Color bar showing acceptance probability scale from 0.05 to 0.6",
        "Dense cluster of data points in mid-range Answer Position (10^1 to 10^2)",
        "Blue points concentrated at lower Answer Position values",
        "Red points concentrated at higher Answer Position values"
      ],
      "queries": [
        {
          "query": "Why does the blue concentration at Answer Position below 10^1 illustrate a pair that multivariate logistic regression cannot find?",
          "answer": "The method identifies pairs of variables where a trend in an outcome as a function of one variable disappears or reverses when conditioned on the second variable. Multivariate logistic regression fails to detect such Simpson's paradox instances in this variable pair.",
          "visual_anchor": "blue concentration at Answer Position below 10^1",
          "text_evidence": "We presented a method for systematically uncovering instances of Simpson's paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the red region at Answer Position 10^1 to 10^2 relate to subgroups within the population generating the data?",
          "answer": "The disaggregated data shown in the red region corresponds to subgroups within the population, revealing different acceptance probability patterns when conditioned on Time Since Previous Answer.",
          "visual_anchor": "red region at Answer Position 10^1 to 10^2",
          "text_evidence": "e disaggregated data corresponds to subgroups within the population generating the data.",
          "query_type": "value_context"
        },
        {
          "query": "Does the acceptance probability gradient from blue to red across Answer Position values support funding for paradox detection research?",
          "answer": "Yes, the systematic uncovering of Simpson's paradox instances demonstrated by the color gradient pattern received funding support from the James S. McDonnell Foundation, Air Force Office of Scientific Research, and Army Research Office.",
          "visual_anchor": "acceptance probability gradient from blue (0.05-0.2) to red (0.4-0.6)",
          "text_evidence": "We acknowledge funding support from the James S. McDonnell Foundation, Air Force Oce of Scientic Research (FA9550-17-1- 0327), and by the Army Research Oce (W911NF-15-1-0142).",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue concentration at Answer Position below 10^1 illustrate a pair that multivariate logistic regression cannot find?",
        "answer": "The method identifies pairs of variables where a trend in an outcome as a function of one variable disappears or reverses when conditioned on the second variable. Multivariate logistic regression fails to detect such Simpson's paradox instances in this variable pair.",
        "visual_anchor": "blue concentration at Answer Position below 10^1",
        "text_evidence": "We presented a method for systematically uncovering instances of Simpson's paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the red region at Answer Position 10^1 to 10^2 relate to subgroups within the population generating the data?",
        "answer": "The disaggregated data shown in the red region corresponds to subgroups within the population, revealing different acceptance probability patterns when conditioned on Time Since Previous Answer.",
        "visual_anchor": "red region at Answer Position 10^1 to 10^2",
        "text_evidence": "e disaggregated data corresponds to subgroups within the population generating the data.",
        "query_type": "value_context"
      },
      {
        "query": "Does the acceptance probability gradient from blue to red across Answer Position values support funding for paradox detection research?",
        "answer": "Yes, the systematic uncovering of Simpson's paradox instances demonstrated by the color gradient pattern received funding support from the James S. McDonnell Foundation, Air Force Office of Scientific Research, and Army Research Office.",
        "visual_anchor": "acceptance probability gradient from blue (0.05-0.2) to red (0.4-0.6)",
        "text_evidence": "We acknowledge funding support from the James S. McDonnell Foundation, Air Force Oce of Scientic Research (FA9550-17-1- 0327), and by the Army Research Oce (W911NF-15-1-0142).",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Heatmap with red and blue coloring representing acceptance probability",
      "Y-axis: Answer Position (logarithmic scale from 10^0 to 10^2)",
      "X-axis: Time Since Previous Answer (logarithmic scale from 10^1 to 10^8)",
      "Color bar showing acceptance probability scale from 0.05 to 0.6",
      "Dense cluster of data points in mid-range Answer Position (10^1 to 10^2)",
      "Blue points concentrated at lower Answer Position values",
      "Red points concentrated at higher Answer Position values"
    ]
  },
  {
    "doc_id": "1801.07593",
    "figure_id": "1801.07593_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg",
    "caption": "Figure 1: The architecture of the adversarial network.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"architecture\",\n  \"visual_elements\": [\"Predictor Weights: W\", \"Adversary Weights: U\", \"Loss L_P(ŷ,y)\", \"Loss L_A(ẑ,z)\", \"Input x\", \"Output ŷ\", \"Output ẑ\", \"Arrow from Predictor to Adversary\"],\n  \"queries\": [\n    {\n      \"query\": \"How does the Predictor block with weights W minimize loss L_P(ŷ,y) during training?\",\n      \"answer\": \"The predictor modifies weights W to minimize loss L_P(ŷ,y) using a gradient-based method such as stochastic gradient descent.\",\n      \"visual_anchor\": \"Predictor Weights: W box with upward arrow to L_P(ŷ,y)\",\n      \"text_evidence\": \"we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ,y), using a gradient-based method such as stochastic gradient descent\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the output ŷ from Predictor Weights W flow as input to the Adversary Weights U block?\",\n      \"answer\": \"The output layer of the predictor is used as input to the adversary network, which then attempts to predict Z from the predictor's output.\",\n      \"visual_anchor\": \"Arrow connecting Predictor output ŷ to Adversary Weights: U input\",\n      \"text_evidence\": \"The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What problem motivates the two-network architecture with separate losses L_P and L_A for predictor and adversary?\",\n      \"answer\": \"Training data frequently contains undesirable biases, and machine learning models faithful to such data can perpetuate these biases in decision making systems.\",\n      \"visual_anchor\": \"Two separate loss functions L_P(ŷ,y) and L_A(ẑ,z) above their respective blocks\",\n      \"text_evidence\": \"the available training data frequently contains biases with respect to things that we would rather not use for decision making. Machine learning builds models faithful to training data and can lead to perpetuating these undesirable biases\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "architecture",
      "visual_elements": [
        "Predictor Weights: W",
        "Adversary Weights: U",
        "Loss L_P(ŷ,y)",
        "Loss L_A(ẑ,z)",
        "Input x",
        "Output ŷ",
        "Output ẑ",
        "Arrow from Predictor to Adversary"
      ],
      "queries": [
        {
          "query": "How does the Predictor block with weights W minimize loss L_P(ŷ,y) during training?",
          "answer": "The predictor modifies weights W to minimize loss L_P(ŷ,y) using a gradient-based method such as stochastic gradient descent.",
          "visual_anchor": "Predictor Weights: W box with upward arrow to L_P(ŷ,y)",
          "text_evidence": "we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ,y), using a gradient-based method such as stochastic gradient descent",
          "query_type": "value_context"
        },
        {
          "query": "Why does the output ŷ from Predictor Weights W flow as input to the Adversary Weights U block?",
          "answer": "The output layer of the predictor is used as input to the adversary network, which then attempts to predict Z from the predictor's output.",
          "visual_anchor": "Arrow connecting Predictor output ŷ to Adversary Weights: U input",
          "text_evidence": "The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What problem motivates the two-network architecture with separate losses L_P and L_A for predictor and adversary?",
          "answer": "Training data frequently contains undesirable biases, and machine learning models faithful to such data can perpetuate these biases in decision making systems.",
          "visual_anchor": "Two separate loss functions L_P(ŷ,y) and L_A(ẑ,z) above their respective blocks",
          "text_evidence": "the available training data frequently contains biases with respect to things that we would rather not use for decision making. Machine learning builds models faithful to training data and can lead to perpetuating these undesirable biases",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the Predictor block with weights W minimize loss L_P(ŷ,y) during training?",
        "answer": "The predictor modifies weights W to minimize loss L_P(ŷ,y) using a gradient-based method such as stochastic gradient descent.",
        "visual_anchor": "Predictor Weights: W box with upward arrow to L_P(ŷ,y)",
        "text_evidence": "we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ,y), using a gradient-based method such as stochastic gradient descent",
        "query_type": "value_context"
      },
      {
        "query": "Why does the output ŷ from Predictor Weights W flow as input to the Adversary Weights U block?",
        "answer": "The output layer of the predictor is used as input to the adversary network, which then attempts to predict Z from the predictor's output.",
        "visual_anchor": "Arrow connecting Predictor output ŷ to Adversary Weights: U input",
        "text_evidence": "The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What problem motivates the two-network architecture with separate losses L_P and L_A for predictor and adversary?",
        "answer": "Training data frequently contains undesirable biases, and machine learning models faithful to such data can perpetuate these biases in decision making systems.",
        "visual_anchor": "Two separate loss functions L_P(ŷ,y) and L_A(ẑ,z) above their respective blocks",
        "text_evidence": "the available training data frequently contains biases with respect to things that we would rather not use for decision making. Machine learning builds models faithful to training data and can lead to perpetuating these undesirable biases",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "architecture",
    "visual_elements": [
      "Predictor Weights: W",
      "Adversary Weights: U",
      "Loss L_P(ŷ,y)",
      "Loss L_A(ẑ,z)",
      "Input x",
      "Output ŷ",
      "Output ẑ",
      "Arrow from Predictor to Adversary"
    ]
  },
  {
    "doc_id": "1801.07593",
    "figure_id": "1801.07593_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg",
    "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"g - proj_h g + h (upper left arrow)\",\n    \"g - proj_h g (upper center arrow)\",\n    \"g + h (upper right arrow)\",\n    \"g = ∇_W L_P (right arrow)\",\n    \"h = -α ∇_W L_A (left arrow)\",\n    \"proj_h g (bottom right arrow)\",\n    \"central point where all vectors originate\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the g+h direction help the adversary, when h equals negative alpha times the gradient of adversary loss?\",\n      \"answer\": \"Since h = -α∇_W L_A moves opposite to the adversary's gradient, adding it to g creates a direction that reduces adversary loss, effectively helping the adversary predict the protected attribute Z.\",\n      \"visual_anchor\": \"upper right arrow labeled 'g + h' and left arrow labeled 'h = -α ∇_W L_A'\",\n      \"text_evidence\": \"The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the upward vertical arrow labeled g minus proj_h g prevent movement that minimizes the predictor loss L_P?\",\n      \"answer\": \"No, it removes only the component of g that aligns with h, preserving the orthogonal part that still reduces L_P without helping the adversary minimize its loss.\",\n      \"visual_anchor\": \"upward vertical arrow labeled 'g - proj_h g'\",\n      \"text_evidence\": \"We begin with a model, which we call the predictor, trained to accomplish the task of predicting Y given X. As in Figure 1, we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ, y), using a gradient-based method such as stochastic gradient descent.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the projection term proj_h g shown as the bottom right arrow ensure the desired equality definitions are satisfied?\",\n      \"answer\": \"By removing the component that helps the adversary predict from Ŷ, the projection ensures the adversary gains no advantage from information about Ŷ, which is the exact condition needed to guarantee desired equality definitions.\",\n      \"visual_anchor\": \"bottom right arrow labeled 'proj_h g'\",\n      \"text_evidence\": \"We will show in the next propositions that the adversary gaining no advantage from information about Ŷ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "g - proj_h g + h (upper left arrow)",
        "g - proj_h g (upper center arrow)",
        "g + h (upper right arrow)",
        "g = ∇_W L_P (right arrow)",
        "h = -α ∇_W L_A (left arrow)",
        "proj_h g (bottom right arrow)",
        "central point where all vectors originate"
      ],
      "queries": [
        {
          "query": "Why does the g+h direction help the adversary, when h equals negative alpha times the gradient of adversary loss?",
          "answer": "Since h = -α∇_W L_A moves opposite to the adversary's gradient, adding it to g creates a direction that reduces adversary loss, effectively helping the adversary predict the protected attribute Z.",
          "visual_anchor": "upper right arrow labeled 'g + h' and left arrow labeled 'h = -α ∇_W L_A'",
          "text_evidence": "The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the upward vertical arrow labeled g minus proj_h g prevent movement that minimizes the predictor loss L_P?",
          "answer": "No, it removes only the component of g that aligns with h, preserving the orthogonal part that still reduces L_P without helping the adversary minimize its loss.",
          "visual_anchor": "upward vertical arrow labeled 'g - proj_h g'",
          "text_evidence": "We begin with a model, which we call the predictor, trained to accomplish the task of predicting Y given X. As in Figure 1, we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ, y), using a gradient-based method such as stochastic gradient descent.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the projection term proj_h g shown as the bottom right arrow ensure the desired equality definitions are satisfied?",
          "answer": "By removing the component that helps the adversary predict from Ŷ, the projection ensures the adversary gains no advantage from information about Ŷ, which is the exact condition needed to guarantee desired equality definitions.",
          "visual_anchor": "bottom right arrow labeled 'proj_h g'",
          "text_evidence": "We will show in the next propositions that the adversary gaining no advantage from information about Ŷ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the g+h direction help the adversary, when h equals negative alpha times the gradient of adversary loss?",
        "answer": "Since h = -α∇_W L_A moves opposite to the adversary's gradient, adding it to g creates a direction that reduces adversary loss, effectively helping the adversary predict the protected attribute Z.",
        "visual_anchor": "upper right arrow labeled 'g + h' and left arrow labeled 'h = -α ∇_W L_A'",
        "text_evidence": "The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict Z.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the upward vertical arrow labeled g minus proj_h g prevent movement that minimizes the predictor loss L_P?",
        "answer": "No, it removes only the component of g that aligns with h, preserving the orthogonal part that still reduces L_P without helping the adversary minimize its loss.",
        "visual_anchor": "upward vertical arrow labeled 'g - proj_h g'",
        "text_evidence": "We begin with a model, which we call the predictor, trained to accomplish the task of predicting Y given X. As in Figure 1, we assume that the model is trained by attempting to modify weights W to minimize some loss L_P(ŷ, y), using a gradient-based method such as stochastic gradient descent.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the projection term proj_h g shown as the bottom right arrow ensure the desired equality definitions are satisfied?",
        "answer": "By removing the component that helps the adversary predict from Ŷ, the projection ensures the adversary gains no advantage from information about Ŷ, which is the exact condition needed to guarantee desired equality definitions.",
        "visual_anchor": "bottom right arrow labeled 'proj_h g'",
        "text_evidence": "We will show in the next propositions that the adversary gaining no advantage from information about Ŷ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "g - proj_h g + h (upper left arrow)",
      "g - proj_h g (upper center arrow)",
      "g + h (upper right arrow)",
      "g = ∇_W L_P (right arrow)",
      "h = -α ∇_W L_A (left arrow)",
      "proj_h g (bottom right arrow)",
      "central point where all vectors originate"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg",
    "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"node C at top\", \"node A at bottom left\", \"node Y at bottom right\", \"directed edge from C to A\", \"directed edge from C to Y\", \"directed edge from A to Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does node C have edges pointing to both A and Y in this graphical structure?\",\n      \"answer\": \"Node C represents a confounder that affects both the sensitive attribute A and the decision Y, creating statistical dependence through both fair and unfair pathways that must be distinguished in fair decision systems.\",\n      \"visual_anchor\": \"node C with two outgoing directed edges to nodes A and Y\",\n      \"text_evidence\": \"We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the direct edge from A to Y relate to correcting observations adversely affected by sensitive attributes?\",\n      \"answer\": \"The direct edge from A to Y represents a causal pathway that must be evaluated when correcting observations, as the method aims to disregard effects along unfair pathways while preserving fair information conveyed through legitimate causal relationships.\",\n      \"visual_anchor\": \"horizontal directed edge connecting node A to node Y at the bottom\",\n      \"text_evidence\": \"Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causal modeling framework allows nodes A, C, and Y to represent variables with directed links between them?\",\n      \"answer\": \"Graphical Causal Models (GCMs) comprise nodes representing variables and links describing statistical and causal relationships, restricted here to directed acyclic graphs where no node can be an ancestor of itself.\",\n      \"visual_anchor\": \"three circular nodes labeled C, A, Y connected by directed edges forming a triangle\",\n      \"text_evidence\": \"Causal relationships among random variables can be visually expressed using Graphical Causal Models (GCMs), which are a special case of Graphical Models. GCMs comprise nodes representing the variables, and links describing statistical and causal relationships among them.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "node C at top",
        "node A at bottom left",
        "node Y at bottom right",
        "directed edge from C to A",
        "directed edge from C to Y",
        "directed edge from A to Y"
      ],
      "queries": [
        {
          "query": "Why does node C have edges pointing to both A and Y in this graphical structure?",
          "answer": "Node C represents a confounder that affects both the sensitive attribute A and the decision Y, creating statistical dependence through both fair and unfair pathways that must be distinguished in fair decision systems.",
          "visual_anchor": "node C with two outgoing directed edges to nodes A and Y",
          "text_evidence": "We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the direct edge from A to Y relate to correcting observations adversely affected by sensitive attributes?",
          "answer": "The direct edge from A to Y represents a causal pathway that must be evaluated when correcting observations, as the method aims to disregard effects along unfair pathways while preserving fair information conveyed through legitimate causal relationships.",
          "visual_anchor": "horizontal directed edge connecting node A to node Y at the bottom",
          "text_evidence": "Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causal modeling framework allows nodes A, C, and Y to represent variables with directed links between them?",
          "answer": "Graphical Causal Models (GCMs) comprise nodes representing variables and links describing statistical and causal relationships, restricted here to directed acyclic graphs where no node can be an ancestor of itself.",
          "visual_anchor": "three circular nodes labeled C, A, Y connected by directed edges forming a triangle",
          "text_evidence": "Causal relationships among random variables can be visually expressed using Graphical Causal Models (GCMs), which are a special case of Graphical Models. GCMs comprise nodes representing the variables, and links describing statistical and causal relationships among them.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does node C have edges pointing to both A and Y in this graphical structure?",
        "answer": "Node C represents a confounder that affects both the sensitive attribute A and the decision Y, creating statistical dependence through both fair and unfair pathways that must be distinguished in fair decision systems.",
        "visual_anchor": "node C with two outgoing directed edges to nodes A and Y",
        "text_evidence": "We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the direct edge from A to Y relate to correcting observations adversely affected by sensitive attributes?",
        "answer": "The direct edge from A to Y represents a causal pathway that must be evaluated when correcting observations, as the method aims to disregard effects along unfair pathways while preserving fair information conveyed through legitimate causal relationships.",
        "visual_anchor": "horizontal directed edge connecting node A to node Y at the bottom",
        "text_evidence": "Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causal modeling framework allows nodes A, C, and Y to represent variables with directed links between them?",
        "answer": "Graphical Causal Models (GCMs) comprise nodes representing variables and links describing statistical and causal relationships, restricted here to directed acyclic graphs where no node can be an ancestor of itself.",
        "visual_anchor": "three circular nodes labeled C, A, Y connected by directed edges forming a triangle",
        "text_evidence": "Causal relationships among random variables can be visually expressed using Graphical Causal Models (GCMs), which are a special case of Graphical Models. GCMs comprise nodes representing the variables, and links describing statistical and causal relationships among them.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "node C at top",
      "node A at bottom left",
      "node Y at bottom right",
      "directed edge from C to A",
      "directed edge from C to Y",
      "directed edge from A to Y"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg",
    "caption": "Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node M\", \"Node C\", \"Node Y\", \"Node A\", \"Node W\", \"Green path from A to Y\", \"Directed edges between nodes\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green path from A to Y exclude node M when studying path-specific fairness?\",\n      \"answer\": \"The green path represents the specific causal route of interest for path-specific counterfactual fairness analysis, isolating direct effects from A to Y while excluding indirect effects through mediator M.\",\n      \"visual_anchor\": \"green path connecting node A to node Y\",\n      \"text_evidence\": \"To gain insights into the problem of path-specific fairness, consider the following linear model\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Which variables shown as nodes M, C, A, L, Y are observed according to the model specification?\",\n      \"answer\": \"All variables A, C, M, L, and Y are observed, while the epsilon terms (ε_a, ε_c, ε_m, ε_l) representing unobserved zero-mean Gaussian noise are not shown as nodes.\",\n      \"visual_anchor\": \"nodes labeled M, C, A, L, and Y\",\n      \"text_evidence\": \"The variables $A , C , M , L$ and $Y$ are observed, whilst $\\\\epsilon _ { a }$ , $\\\\epsilon _ { c }$ , $\\\\epsilon _ { m }$ and $\\\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does node C require latent variable correction given it is a descendant of the sensitive attribute?\",\n      \"answer\": \"Yes, descendants of the sensitive attribute that need correction require latent variables to address general data-generation process mismatch, as explicitly stated for the extended GCM framework.\",\n      \"visual_anchor\": \"node C with incoming edge from sensitive attribute\",\n      \"text_evidence\": \"we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node M",
        "Node C",
        "Node Y",
        "Node A",
        "Node W",
        "Green path from A to Y",
        "Directed edges between nodes"
      ],
      "queries": [
        {
          "query": "Why does the green path from A to Y exclude node M when studying path-specific fairness?",
          "answer": "The green path represents the specific causal route of interest for path-specific counterfactual fairness analysis, isolating direct effects from A to Y while excluding indirect effects through mediator M.",
          "visual_anchor": "green path connecting node A to node Y",
          "text_evidence": "To gain insights into the problem of path-specific fairness, consider the following linear model",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Which variables shown as nodes M, C, A, L, Y are observed according to the model specification?",
          "answer": "All variables A, C, M, L, and Y are observed, while the epsilon terms (ε_a, ε_c, ε_m, ε_l) representing unobserved zero-mean Gaussian noise are not shown as nodes.",
          "visual_anchor": "nodes labeled M, C, A, L, and Y",
          "text_evidence": "The variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms",
          "query_type": "value_context"
        },
        {
          "query": "Does node C require latent variable correction given it is a descendant of the sensitive attribute?",
          "answer": "Yes, descendants of the sensitive attribute that need correction require latent variables to address general data-generation process mismatch, as explicitly stated for the extended GCM framework.",
          "visual_anchor": "node C with incoming edge from sensitive attribute",
          "text_evidence": "we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green path from A to Y exclude node M when studying path-specific fairness?",
        "answer": "The green path represents the specific causal route of interest for path-specific counterfactual fairness analysis, isolating direct effects from A to Y while excluding indirect effects through mediator M.",
        "visual_anchor": "green path connecting node A to node Y",
        "text_evidence": "To gain insights into the problem of path-specific fairness, consider the following linear model",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Which variables shown as nodes M, C, A, L, Y are observed according to the model specification?",
        "answer": "All variables A, C, M, L, and Y are observed, while the epsilon terms (ε_a, ε_c, ε_m, ε_l) representing unobserved zero-mean Gaussian noise are not shown as nodes.",
        "visual_anchor": "nodes labeled M, C, A, L, and Y",
        "text_evidence": "The variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms",
        "query_type": "value_context"
      },
      {
        "query": "Does node C require latent variable correction given it is a descendant of the sensitive attribute?",
        "answer": "Yes, descendants of the sensitive attribute that need correction require latent variables to address general data-generation process mismatch, as explicitly stated for the extended GCM framework.",
        "visual_anchor": "node C with incoming edge from sensitive attribute",
        "text_evidence": "we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node M",
      "Node C",
      "Node Y",
      "Node A",
      "Node W",
      "Green path from A to Y",
      "Directed edges between nodes"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_7",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg",
    "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Black curve with x markers showing tall narrow peak centered near 0\",\n    \"Red curve with x markers showing shorter wider peak centered near -3\",\n    \"Legend indicating A=0 (black) and A=1 (red)\",\n    \"X-axis ranging from -10 to 10\",\n    \"Y-axis showing probability density\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black A=0 distribution peak near zero while the latent variable H_m follows a Gaussian?\",\n      \"answer\": \"The black curve represents the empirical distribution of ε_m^n from Eq. (3) with a non-linear term f(A,C), while H_m is an explicit Gaussian latent variable added to the modified GCM for generating M.\",\n      \"visual_anchor\": \"Black curve with x markers showing narrow peak centered near 0\",\n      \"text_evidence\": \"Empirical distribution of $\\\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\\\tilde { p } ( H _ { m } | A )$ (crossed lines)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red A=1 histogram at position -3 demonstrate the small dependence on A encouraged by MMD criterion?\",\n      \"answer\": \"Yes, the red histogram represents $\\\\tilde{p}(H_m|A=1)$, and the MMD criterion is used during training to encourage small dependence of $\\\\tilde{p}(H_m|A=a)$ on A, though the visual separation suggests some dependence remains.\",\n      \"visual_anchor\": \"Red curve with x markers showing peak centered near -3\",\n      \"text_evidence\": \"We encourage $\\\\tilde { p } ( H _ { m } | A = a )$ to have small dependence on $A$ during training through the maximum mean discrepancy (MMD) criterion (Gretton et al., 2012).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the crossed-line histogram pattern relate to the joint Gaussian distribution over Z including H_m?\",\n      \"answer\": \"The crossed-line histograms show $\\\\tilde{p}(H_m|A)$, which are marginals of the joint Gaussian distribution $p(Z=\\\\{Y,L,M,C,H_m\\\\}|A)$ where the latent variable H_m follows $\\\\mathcal{N}(\\\\theta^h, \\\\sigma_h^2)$ in the modified GCM.\",\n      \"visual_anchor\": \"Crossed lines overlaying both black and red distribution curves\",\n      \"text_evidence\": \"Consider the GCM in Fig. 3(a), corresponding to Eq. (3) with the addition of a Gaussian latent variable $H _ { m } \\\\sim \\\\mathcal { N } ( \\\\theta ^ { h } , \\\\sigma _ { h } ^ { 2 } )$ in the equation for $M$ . The joint distribution $p ( Z = \\\\{ Y , L , M , C , H _ { m } \\\\} | A )$ is Gaussian\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black curve with x markers showing tall narrow peak centered near 0",
        "Red curve with x markers showing shorter wider peak centered near -3",
        "Legend indicating A=0 (black) and A=1 (red)",
        "X-axis ranging from -10 to 10",
        "Y-axis showing probability density"
      ],
      "queries": [
        {
          "query": "Why does the black A=0 distribution peak near zero while the latent variable H_m follows a Gaussian?",
          "answer": "The black curve represents the empirical distribution of ε_m^n from Eq. (3) with a non-linear term f(A,C), while H_m is an explicit Gaussian latent variable added to the modified GCM for generating M.",
          "visual_anchor": "Black curve with x markers showing narrow peak centered near 0",
          "text_evidence": "Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red A=1 histogram at position -3 demonstrate the small dependence on A encouraged by MMD criterion?",
          "answer": "Yes, the red histogram represents $\\tilde{p}(H_m|A=1)$, and the MMD criterion is used during training to encourage small dependence of $\\tilde{p}(H_m|A=a)$ on A, though the visual separation suggests some dependence remains.",
          "visual_anchor": "Red curve with x markers showing peak centered near -3",
          "text_evidence": "We encourage $\\tilde { p } ( H _ { m } | A = a )$ to have small dependence on $A$ during training through the maximum mean discrepancy (MMD) criterion (Gretton et al., 2012).",
          "query_type": "value_context"
        },
        {
          "query": "How does the crossed-line histogram pattern relate to the joint Gaussian distribution over Z including H_m?",
          "answer": "The crossed-line histograms show $\\tilde{p}(H_m|A)$, which are marginals of the joint Gaussian distribution $p(Z=\\{Y,L,M,C,H_m\\}|A)$ where the latent variable H_m follows $\\mathcal{N}(\\theta^h, \\sigma_h^2)$ in the modified GCM.",
          "visual_anchor": "Crossed lines overlaying both black and red distribution curves",
          "text_evidence": "Consider the GCM in Fig. 3(a), corresponding to Eq. (3) with the addition of a Gaussian latent variable $H _ { m } \\sim \\mathcal { N } ( \\theta ^ { h } , \\sigma _ { h } ^ { 2 } )$ in the equation for $M$ . The joint distribution $p ( Z = \\{ Y , L , M , C , H _ { m } \\} | A )$ is Gaussian",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black A=0 distribution peak near zero while the latent variable H_m follows a Gaussian?",
        "answer": "The black curve represents the empirical distribution of ε_m^n from Eq. (3) with a non-linear term f(A,C), while H_m is an explicit Gaussian latent variable added to the modified GCM for generating M.",
        "visual_anchor": "Black curve with x markers showing narrow peak centered near 0",
        "text_evidence": "Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red A=1 histogram at position -3 demonstrate the small dependence on A encouraged by MMD criterion?",
        "answer": "Yes, the red histogram represents $\\tilde{p}(H_m|A=1)$, and the MMD criterion is used during training to encourage small dependence of $\\tilde{p}(H_m|A=a)$ on A, though the visual separation suggests some dependence remains.",
        "visual_anchor": "Red curve with x markers showing peak centered near -3",
        "text_evidence": "We encourage $\\tilde { p } ( H _ { m } | A = a )$ to have small dependence on $A$ during training through the maximum mean discrepancy (MMD) criterion (Gretton et al., 2012).",
        "query_type": "value_context"
      },
      {
        "query": "How does the crossed-line histogram pattern relate to the joint Gaussian distribution over Z including H_m?",
        "answer": "The crossed-line histograms show $\\tilde{p}(H_m|A)$, which are marginals of the joint Gaussian distribution $p(Z=\\{Y,L,M,C,H_m\\}|A)$ where the latent variable H_m follows $\\mathcal{N}(\\theta^h, \\sigma_h^2)$ in the modified GCM.",
        "visual_anchor": "Crossed lines overlaying both black and red distribution curves",
        "text_evidence": "Consider the GCM in Fig. 3(a), corresponding to Eq. (3) with the addition of a Gaussian latent variable $H _ { m } \\sim \\mathcal { N } ( \\theta ^ { h } , \\sigma _ { h } ^ { 2 } )$ in the equation for $M$ . The joint distribution $p ( Z = \\{ Y , L , M , C , H _ { m } \\} | A )$ is Gaussian",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black curve with x markers showing tall narrow peak centered near 0",
      "Red curve with x markers showing shorter wider peak centered near -3",
      "Legend indicating A=0 (black) and A=1 (red)",
      "X-axis ranging from -10 to 10",
      "Y-axis showing probability density"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_9",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg",
    "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node M with incoming edges from A, C, and H_m\", \"Node L with incoming edges from M, C, and H_l\", \"Node Y with incoming edge from R\", \"Green solid edges connecting nodes\", \"Dashed green edges between M, L, and R\", \"Nodes A, M, L, R, Y arranged in bottom row\", \"Nodes H_m, C, H_l, H_r arranged in top row\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does node M receive three incoming solid green edges when its distribution depends on A, C, and H_m?\",\n      \"answer\": \"The three incoming edges to M from A, C, and H_m directly represent the conditional probability distribution p_θ(M|A,C,H_m), which can be modeled by any function f_θ(A,C,H_m) such as a neural network.\",\n      \"visual_anchor\": \"Three solid green edges pointing to node M from nodes A, C, and H_m\",\n      \"text_evidence\": \"If M is categorical, p_{θ}(M|A,C,H_{m}) = f_{θ}(A,C,H_{m}), where f_{θ}(A,C,H_{m}) can be any function, e.g. a neural network.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the dashed green edges between M, L, and R indicate dependencies captured in the model likelihood expression?\",\n      \"answer\": \"Yes, the dashed edges represent probabilistic dependencies among M, L, and R that are encoded in the joint model likelihood p_θ(A,C,M,L,Y), which captures the complete factorization of the GCM.\",\n      \"visual_anchor\": \"Dashed green edges connecting nodes M, L, and R in the bottom row\",\n      \"text_evidence\": \"The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does node L's position between M and R reflect the posterior distribution requiring inputs from both neighbors?\",\n      \"answer\": \"Yes, node L's central position with connections to M reflects that the posterior distribution p_θ(H_l|...) requires information from neighboring nodes M and L, as indicated by the incomplete expression mentioning H_l conditioning.\",\n      \"visual_anchor\": \"Node L positioned centrally in bottom row between nodes M and R with bidirectional dashed edges\",\n      \"text_evidence\": \"The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node M with incoming edges from A, C, and H_m",
        "Node L with incoming edges from M, C, and H_l",
        "Node Y with incoming edge from R",
        "Green solid edges connecting nodes",
        "Dashed green edges between M, L, and R",
        "Nodes A, M, L, R, Y arranged in bottom row",
        "Nodes H_m, C, H_l, H_r arranged in top row"
      ],
      "queries": [
        {
          "query": "Why does node M receive three incoming solid green edges when its distribution depends on A, C, and H_m?",
          "answer": "The three incoming edges to M from A, C, and H_m directly represent the conditional probability distribution p_θ(M|A,C,H_m), which can be modeled by any function f_θ(A,C,H_m) such as a neural network.",
          "visual_anchor": "Three solid green edges pointing to node M from nodes A, C, and H_m",
          "text_evidence": "If M is categorical, p_{θ}(M|A,C,H_{m}) = f_{θ}(A,C,H_{m}), where f_{θ}(A,C,H_{m}) can be any function, e.g. a neural network.",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the dashed green edges between M, L, and R indicate dependencies captured in the model likelihood expression?",
          "answer": "Yes, the dashed edges represent probabilistic dependencies among M, L, and R that are encoded in the joint model likelihood p_θ(A,C,M,L,Y), which captures the complete factorization of the GCM.",
          "visual_anchor": "Dashed green edges connecting nodes M, L, and R in the bottom row",
          "text_evidence": "The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does node L's position between M and R reflect the posterior distribution requiring inputs from both neighbors?",
          "answer": "Yes, node L's central position with connections to M reflects that the posterior distribution p_θ(H_l|...) requires information from neighboring nodes M and L, as indicated by the incomplete expression mentioning H_l conditioning.",
          "visual_anchor": "Node L positioned centrally in bottom row between nodes M and R with bidirectional dashed edges",
          "text_evidence": "The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does node M receive three incoming solid green edges when its distribution depends on A, C, and H_m?",
        "answer": "The three incoming edges to M from A, C, and H_m directly represent the conditional probability distribution p_θ(M|A,C,H_m), which can be modeled by any function f_θ(A,C,H_m) such as a neural network.",
        "visual_anchor": "Three solid green edges pointing to node M from nodes A, C, and H_m",
        "text_evidence": "If M is categorical, p_{θ}(M|A,C,H_{m}) = f_{θ}(A,C,H_{m}), where f_{θ}(A,C,H_{m}) can be any function, e.g. a neural network.",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the dashed green edges between M, L, and R indicate dependencies captured in the model likelihood expression?",
        "answer": "Yes, the dashed edges represent probabilistic dependencies among M, L, and R that are encoded in the joint model likelihood p_θ(A,C,M,L,Y), which captures the complete factorization of the GCM.",
        "visual_anchor": "Dashed green edges connecting nodes M, L, and R in the bottom row",
        "text_evidence": "The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does node L's position between M and R reflect the posterior distribution requiring inputs from both neighbors?",
        "answer": "Yes, node L's central position with connections to M reflects that the posterior distribution p_θ(H_l|...) requires information from neighboring nodes M and L, as indicated by the incomplete expression mentioning H_l conditioning.",
        "visual_anchor": "Node L positioned centrally in bottom row between nodes M and R with bidirectional dashed edges",
        "text_evidence": "The model likelihood p_{θ}(A,C,M,L,Y), and posterior distributions p_{θ}(H_{m}|A,C,M,L) and p_{θ}(H_{l}|",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node M with incoming edges from A, C, and H_m",
      "Node L with incoming edges from M, C, and H_l",
      "Node Y with incoming edge from R",
      "Green solid edges connecting nodes",
      "Dashed green edges between M, L, and R",
      "Nodes A, M, L, R, Y arranged in bottom row",
      "Nodes H_m, C, H_l, H_r arranged in top row"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_12",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg",
    "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue histogram bars for male\", \"red histogram bars for female\", \"bimodal distribution with peaks around -5 and 15\", \"x-axis ranging from -20 to 30\", \"legend indicating male (blue) and female (red)\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue and red histograms show two distinct peaks around -5 and 15 for the housing variable dimension?\",\n      \"answer\": \"The bimodal distribution represents the learned conditional distribution q̃(Hs|A) after training, where A likely encodes gender-specific housing patterns that create separate modes for male and female populations.\",\n      \"visual_anchor\": \"two distinct peaks in both blue (male) and red (female) histograms at approximately -5 and 15\",\n      \"text_evidence\": \"Histograms of $\\\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the taller blue peak near -5 compared to the red peak indicate male dominance in that housing dimension?\",\n      \"answer\": \"Yes, the taller blue histogram bars around -5 suggest a higher density of male samples in that region of the housing variable dimension compared to females.\",\n      \"visual_anchor\": \"blue histogram peak at -5 being taller than the red histogram peak at the same location\",\n      \"text_evidence\": \"Histograms of $\\\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the red histogram's higher peak at 15 relate to the conditional distribution of housing given gender A?\",\n      \"answer\": \"The red (female) histogram's higher peak at 15 indicates that q̃(Hs|A) assigns greater probability density to females in that housing dimension value, showing gender-conditional structure learned during training.\",\n      \"visual_anchor\": \"red histogram peak at x=15 being taller than the blue peak at the same position\",\n      \"text_evidence\": \"Histograms of $\\\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue histogram bars for male",
        "red histogram bars for female",
        "bimodal distribution with peaks around -5 and 15",
        "x-axis ranging from -20 to 30",
        "legend indicating male (blue) and female (red)"
      ],
      "queries": [
        {
          "query": "Why do the blue and red histograms show two distinct peaks around -5 and 15 for the housing variable dimension?",
          "answer": "The bimodal distribution represents the learned conditional distribution q̃(Hs|A) after training, where A likely encodes gender-specific housing patterns that create separate modes for male and female populations.",
          "visual_anchor": "two distinct peaks in both blue (male) and red (female) histograms at approximately -5 and 15",
          "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the taller blue peak near -5 compared to the red peak indicate male dominance in that housing dimension?",
          "answer": "Yes, the taller blue histogram bars around -5 suggest a higher density of male samples in that region of the housing variable dimension compared to females.",
          "visual_anchor": "blue histogram peak at -5 being taller than the red histogram peak at the same location",
          "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the red histogram's higher peak at 15 relate to the conditional distribution of housing given gender A?",
          "answer": "The red (female) histogram's higher peak at 15 indicates that q̃(Hs|A) assigns greater probability density to females in that housing dimension value, showing gender-conditional structure learned during training.",
          "visual_anchor": "red histogram peak at x=15 being taller than the blue peak at the same position",
          "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue and red histograms show two distinct peaks around -5 and 15 for the housing variable dimension?",
        "answer": "The bimodal distribution represents the learned conditional distribution q̃(Hs|A) after training, where A likely encodes gender-specific housing patterns that create separate modes for male and female populations.",
        "visual_anchor": "two distinct peaks in both blue (male) and red (female) histograms at approximately -5 and 15",
        "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the taller blue peak near -5 compared to the red peak indicate male dominance in that housing dimension?",
        "answer": "Yes, the taller blue histogram bars around -5 suggest a higher density of male samples in that region of the housing variable dimension compared to females.",
        "visual_anchor": "blue histogram peak at -5 being taller than the red histogram peak at the same location",
        "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the red histogram's higher peak at 15 relate to the conditional distribution of housing given gender A?",
        "answer": "The red (female) histogram's higher peak at 15 indicates that q̃(Hs|A) assigns greater probability density to females in that housing dimension value, showing gender-conditional structure learned during training.",
        "visual_anchor": "red histogram peak at x=15 being taller than the blue peak at the same position",
        "text_evidence": "Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue histogram bars for male",
      "red histogram bars for female",
      "bimodal distribution with peaks around -5 and 15",
      "x-axis ranging from -20 to 30",
      "legend indicating male (blue) and female (red)"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_15",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg",
    "caption": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Two overlapping histogram distributions (blue for male, red for female)\",\n    \"Bimodal distribution pattern with peaks around -5 and 20\",\n    \"X-axis ranging from -20 to 30\",\n    \"Blue and red outlined rectangles in legend\",\n    \"Left peak taller than right peak\",\n    \"Clear separation between two modes around x=10\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue and red histograms show two distinct peaks near -5 and 20 for the German Credit protected attribute?\",\n      \"answer\": \"The bimodal distribution reflects the variational approximation q̃(Hm|A) learning to separate the latent representation by the protected attribute sex (A) during training on the UCI German Credit dataset with 1,000 loan applicants.\",\n      \"visual_anchor\": \"Two distinct peaks in both blue (male) and red (female) histograms at approximately x=-5 and x=20\",\n      \"text_evidence\": \"The German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans... We assume the GCM in Fig. 4(b), where A corresponds to the protected attribute sex\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the similar height of blue and red peaks around x=20 suggest balanced credit amount and repayment duration across sexes?\",\n      \"answer\": \"Yes, the overlapping right peaks around x=20 indicate that the latent dimension capturing R (credit amount and repayment duration) shows similar distributions for both male and female applicants in the learned representation.\",\n      \"visual_anchor\": \"Right peak centered near x=20 showing overlapping blue and red histograms of similar height\",\n      \"text_evidence\": \"R the duple credit amount and repayment duration\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the taller left peak near -5 relate to the checking account, savings, and housing attributes represented by S?\",\n      \"answer\": \"The dominant left peak likely captures the triple S attributes (checking account, savings, housing) which may be more concentrated or determinative in the latent space than the financial request attributes represented in the right peak.\",\n      \"visual_anchor\": \"Left peak near x=-5 with noticeably greater height than the right peak at x=20\",\n      \"text_evidence\": \"S to the triple status of checking account, savings, and housing\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Two overlapping histogram distributions (blue for male, red for female)",
        "Bimodal distribution pattern with peaks around -5 and 20",
        "X-axis ranging from -20 to 30",
        "Blue and red outlined rectangles in legend",
        "Left peak taller than right peak",
        "Clear separation between two modes around x=10"
      ],
      "queries": [
        {
          "query": "Why do the blue and red histograms show two distinct peaks near -5 and 20 for the German Credit protected attribute?",
          "answer": "The bimodal distribution reflects the variational approximation q̃(Hm|A) learning to separate the latent representation by the protected attribute sex (A) during training on the UCI German Credit dataset with 1,000 loan applicants.",
          "visual_anchor": "Two distinct peaks in both blue (male) and red (female) histograms at approximately x=-5 and x=20",
          "text_evidence": "The German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans... We assume the GCM in Fig. 4(b), where A corresponds to the protected attribute sex",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the similar height of blue and red peaks around x=20 suggest balanced credit amount and repayment duration across sexes?",
          "answer": "Yes, the overlapping right peaks around x=20 indicate that the latent dimension capturing R (credit amount and repayment duration) shows similar distributions for both male and female applicants in the learned representation.",
          "visual_anchor": "Right peak centered near x=20 showing overlapping blue and red histograms of similar height",
          "text_evidence": "R the duple credit amount and repayment duration",
          "query_type": "value_context"
        },
        {
          "query": "How does the taller left peak near -5 relate to the checking account, savings, and housing attributes represented by S?",
          "answer": "The dominant left peak likely captures the triple S attributes (checking account, savings, housing) which may be more concentrated or determinative in the latent space than the financial request attributes represented in the right peak.",
          "visual_anchor": "Left peak near x=-5 with noticeably greater height than the right peak at x=20",
          "text_evidence": "S to the triple status of checking account, savings, and housing",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue and red histograms show two distinct peaks near -5 and 20 for the German Credit protected attribute?",
        "answer": "The bimodal distribution reflects the variational approximation q̃(Hm|A) learning to separate the latent representation by the protected attribute sex (A) during training on the UCI German Credit dataset with 1,000 loan applicants.",
        "visual_anchor": "Two distinct peaks in both blue (male) and red (female) histograms at approximately x=-5 and x=20",
        "text_evidence": "The German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans... We assume the GCM in Fig. 4(b), where A corresponds to the protected attribute sex",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the similar height of blue and red peaks around x=20 suggest balanced credit amount and repayment duration across sexes?",
        "answer": "Yes, the overlapping right peaks around x=20 indicate that the latent dimension capturing R (credit amount and repayment duration) shows similar distributions for both male and female applicants in the learned representation.",
        "visual_anchor": "Right peak centered near x=20 showing overlapping blue and red histograms of similar height",
        "text_evidence": "R the duple credit amount and repayment duration",
        "query_type": "value_context"
      },
      {
        "query": "How does the taller left peak near -5 relate to the checking account, savings, and housing attributes represented by S?",
        "answer": "The dominant left peak likely captures the triple S attributes (checking account, savings, housing) which may be more concentrated or determinative in the latent space than the financial request attributes represented in the right peak.",
        "visual_anchor": "Left peak near x=-5 with noticeably greater height than the right peak at x=20",
        "text_evidence": "S to the triple status of checking account, savings, and housing",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Two overlapping histogram distributions (blue for male, red for female)",
      "Bimodal distribution pattern with peaks around -5 and 20",
      "X-axis ranging from -20 to 30",
      "Blue and red outlined rectangles in legend",
      "Left peak taller than right peak",
      "Clear separation between two modes around x=10"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_17",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
    "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Gray node C\", \"Green path from A to Y\", \"Black directed edges\", \"Red bidirected edge in ADMG\", \"Nodes M, W, A, Y\", \"Two subfigures (a) and (b)\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?\",\n      \"answer\": \"The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).\",\n      \"visual_anchor\": \"Gray node C in subfigure (a) and red bidirected edge in subfigure (b)\",\n      \"text_evidence\": \"An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the green path from A to Y relate to the claim about achieving fair decisions?\",\n      \"answer\": \"The green path from A to Y represents a causal pathway of interest. Fair decisions are achieved by correcting variables that are descendants of the protected attribute A along unfair pathways, which allows retaining fair information while addressing problematic causal influences.\",\n      \"visual_anchor\": \"Green path from node A to node Y\",\n      \"text_evidence\": \"A fair decision is achieved by correcting the variables that are descendants of the protected attribute along unfair pathways, rather than by imposing constraints on the model parameters.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What prevents the causal effect along the green A to Y path from being identified using only M, W, A, Y?\",\n      \"answer\": \"The presence of the unobserved confounder C (shown as gray node) creates hidden common causes that cannot be controlled for using only the observed variables M, W, A, and Y, making the causal effect non-identifiable from observed data alone.\",\n      \"visual_anchor\": \"Green path between A and Y with gray node C present\",\n      \"text_evidence\": \"The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Gray node C",
        "Green path from A to Y",
        "Black directed edges",
        "Red bidirected edge in ADMG",
        "Nodes M, W, A, Y",
        "Two subfigures (a) and (b)"
      ],
      "queries": [
        {
          "query": "Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?",
          "answer": "The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).",
          "visual_anchor": "Gray node C in subfigure (a) and red bidirected edge in subfigure (b)",
          "text_evidence": "An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the green path from A to Y relate to the claim about achieving fair decisions?",
          "answer": "The green path from A to Y represents a causal pathway of interest. Fair decisions are achieved by correcting variables that are descendants of the protected attribute A along unfair pathways, which allows retaining fair information while addressing problematic causal influences.",
          "visual_anchor": "Green path from node A to node Y",
          "text_evidence": "A fair decision is achieved by correcting the variables that are descendants of the protected attribute along unfair pathways, rather than by imposing constraints on the model parameters.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What prevents the causal effect along the green A to Y path from being identified using only M, W, A, Y?",
          "answer": "The presence of the unobserved confounder C (shown as gray node) creates hidden common causes that cannot be controlled for using only the observed variables M, W, A, and Y, making the causal effect non-identifiable from observed data alone.",
          "visual_anchor": "Green path between A and Y with gray node C present",
          "text_evidence": "The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the gray node C in subfigure (a) become a red bidirected link in the ADMG representation?",
        "answer": "The gray node C represents an unobserved confounder. In ADMG notation, unobserved common causes are indicated by red bidirected links between the affected variables, which is why C's influence is shown as a red bidirected edge in subfigure (b).",
        "visual_anchor": "Gray node C in subfigure (a) and red bidirected edge in subfigure (b)",
        "text_evidence": "An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the green path from A to Y relate to the claim about achieving fair decisions?",
        "answer": "The green path from A to Y represents a causal pathway of interest. Fair decisions are achieved by correcting variables that are descendants of the protected attribute A along unfair pathways, which allows retaining fair information while addressing problematic causal influences.",
        "visual_anchor": "Green path from node A to node Y",
        "text_evidence": "A fair decision is achieved by correcting the variables that are descendants of the protected attribute along unfair pathways, rather than by imposing constraints on the model parameters.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What prevents the causal effect along the green A to Y path from being identified using only M, W, A, Y?",
        "answer": "The presence of the unobserved confounder C (shown as gray node) creates hidden common causes that cannot be controlled for using only the observed variables M, W, A, and Y, making the causal effect non-identifiable from observed data alone.",
        "visual_anchor": "Green path between A and Y with gray node C present",
        "text_evidence": "The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Gray node C",
      "Green path from A to Y",
      "Black directed edges",
      "Red bidirected edge in ADMG",
      "Nodes M, W, A, Y",
      "Two subfigures (a) and (b)"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_36",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg",
    "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"bell-shaped histogram curve in blue\", \"x-axis ranging from -8 to 6\", \"symmetric distribution centered near 0\", \"peak around x=0\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the bell-shaped histogram centered near zero match the two-dimensional Gaussian with diagonal covariance used for the variational posterior?\",\n      \"answer\": \"Yes, the symmetric bell-shaped distribution is consistent with a two-dimensional Gaussian with diagonal covariance, as specified for the variational posterior distribution q.\",\n      \"visual_anchor\": \"bell-shaped histogram curve centered near x=0\",\n      \"text_evidence\": \"As the variational posterior distribution $q$ we used a two-dimensional Gaussian with diagonal covariance, with means and log variances obtained as the outputs of a neural network\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does the histogram span from approximately -8 to 6 when the prior uses ten two-dimensional Gaussian mixture components?\",\n      \"answer\": \"The mixture of ten two-dimensional Gaussians creates a broader support range than a single Gaussian, allowing the histogram to span the wider range from -8 to 6.\",\n      \"visual_anchor\": \"x-axis range from -8 to 6\",\n      \"text_evidence\": \"as the prior distribution $p$ for each latent variable we used a mixture of two-dimensional Gaussians with ten mixture components and diagonal covariances\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Is the smooth unimodal shape at 5000 training steps consistent with histograms shown for posterior distributions in latent space?\",\n      \"answer\": \"Yes, the smooth unimodal histogram is consistent with the posterior distributions visualized after 5,000 training steps in the latent space analysis.\",\n      \"visual_anchor\": \"smooth bell-shaped curve with single peak\",\n      \"text_evidence\": \"Histograms of $\\\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "bell-shaped histogram curve in blue",
        "x-axis ranging from -8 to 6",
        "symmetric distribution centered near 0",
        "peak around x=0"
      ],
      "queries": [
        {
          "query": "Does the bell-shaped histogram centered near zero match the two-dimensional Gaussian with diagonal covariance used for the variational posterior?",
          "answer": "Yes, the symmetric bell-shaped distribution is consistent with a two-dimensional Gaussian with diagonal covariance, as specified for the variational posterior distribution q.",
          "visual_anchor": "bell-shaped histogram curve centered near x=0",
          "text_evidence": "As the variational posterior distribution $q$ we used a two-dimensional Gaussian with diagonal covariance, with means and log variances obtained as the outputs of a neural network",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does the histogram span from approximately -8 to 6 when the prior uses ten two-dimensional Gaussian mixture components?",
          "answer": "The mixture of ten two-dimensional Gaussians creates a broader support range than a single Gaussian, allowing the histogram to span the wider range from -8 to 6.",
          "visual_anchor": "x-axis range from -8 to 6",
          "text_evidence": "as the prior distribution $p$ for each latent variable we used a mixture of two-dimensional Gaussians with ten mixture components and diagonal covariances",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Is the smooth unimodal shape at 5000 training steps consistent with histograms shown for posterior distributions in latent space?",
          "answer": "Yes, the smooth unimodal histogram is consistent with the posterior distributions visualized after 5,000 training steps in the latent space analysis.",
          "visual_anchor": "smooth bell-shaped curve with single peak",
          "text_evidence": "Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the bell-shaped histogram centered near zero match the two-dimensional Gaussian with diagonal covariance used for the variational posterior?",
        "answer": "Yes, the symmetric bell-shaped distribution is consistent with a two-dimensional Gaussian with diagonal covariance, as specified for the variational posterior distribution q.",
        "visual_anchor": "bell-shaped histogram curve centered near x=0",
        "text_evidence": "As the variational posterior distribution $q$ we used a two-dimensional Gaussian with diagonal covariance, with means and log variances obtained as the outputs of a neural network",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does the histogram span from approximately -8 to 6 when the prior uses ten two-dimensional Gaussian mixture components?",
        "answer": "The mixture of ten two-dimensional Gaussians creates a broader support range than a single Gaussian, allowing the histogram to span the wider range from -8 to 6.",
        "visual_anchor": "x-axis range from -8 to 6",
        "text_evidence": "as the prior distribution $p$ for each latent variable we used a mixture of two-dimensional Gaussians with ten mixture components and diagonal covariances",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Is the smooth unimodal shape at 5000 training steps consistent with histograms shown for posterior distributions in latent space?",
        "answer": "Yes, the smooth unimodal histogram is consistent with the posterior distributions visualized after 5,000 training steps in the latent space analysis.",
        "visual_anchor": "smooth bell-shaped curve with single peak",
        "text_evidence": "Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "bell-shaped histogram curve in blue",
      "x-axis ranging from -8 to 6",
      "symmetric distribution centered near 0",
      "peak around x=0"
    ]
  },
  {
    "doc_id": "1802.08139",
    "figure_id": "1802.08139_fig_46",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg",
    "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"overlapping red and blue histograms\", \"x-axis range from -10 to 15\", \"y-axis showing frequency\", \"bimodal distribution pattern\", \"peak heights varying across the range\"],\n  \"queries\": [\n    {\n      \"query\": \"How many dimensions does the savings attribute span in the histograms shown for 2,000 training steps?\",\n      \"answer\": \"The savings attribute spans two dimensions in the histograms, as indicated by the bimodal distribution pattern visible in the plot.\",\n      \"visual_anchor\": \"middle histogram with overlapping red and blue bars showing bimodal pattern\",\n      \"text_evidence\": \"From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the rightmost histogram show a single distribution peak while the middle one displays two distinct modes?\",\n      \"answer\": \"The rightmost histogram represents housing with one dimension, producing a single peak, while the middle histogram shows savings with two dimensions, creating the bimodal pattern.\",\n      \"visual_anchor\": \"rightmost histogram with single peak versus middle histogram with two distinct modes\",\n      \"text_evidence\": \"status of checking account (two dimensions), savings (two dimensions), and housing (one dimension)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the overlapping red and blue bars represent the posterior distributions in latent space after training?\",\n      \"answer\": \"Yes, the overlapping red and blue bars represent posterior distributions q̃(Hs|A) in the latent space, visualized as histograms after training steps.\",\n      \"visual_anchor\": \"overlapping red and blue histogram bars across x-axis range -10 to 15\",\n      \"text_evidence\": \"In Fig. 9 we show histograms for posterior distributions in the latent space.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "overlapping red and blue histograms",
        "x-axis range from -10 to 15",
        "y-axis showing frequency",
        "bimodal distribution pattern",
        "peak heights varying across the range"
      ],
      "queries": [
        {
          "query": "How many dimensions does the savings attribute span in the histograms shown for 2,000 training steps?",
          "answer": "The savings attribute spans two dimensions in the histograms, as indicated by the bimodal distribution pattern visible in the plot.",
          "visual_anchor": "middle histogram with overlapping red and blue bars showing bimodal pattern",
          "text_evidence": "From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).",
          "query_type": "value_context"
        },
        {
          "query": "Why does the rightmost histogram show a single distribution peak while the middle one displays two distinct modes?",
          "answer": "The rightmost histogram represents housing with one dimension, producing a single peak, while the middle histogram shows savings with two dimensions, creating the bimodal pattern.",
          "visual_anchor": "rightmost histogram with single peak versus middle histogram with two distinct modes",
          "text_evidence": "status of checking account (two dimensions), savings (two dimensions), and housing (one dimension)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the overlapping red and blue bars represent the posterior distributions in latent space after training?",
          "answer": "Yes, the overlapping red and blue bars represent posterior distributions q̃(Hs|A) in the latent space, visualized as histograms after training steps.",
          "visual_anchor": "overlapping red and blue histogram bars across x-axis range -10 to 15",
          "text_evidence": "In Fig. 9 we show histograms for posterior distributions in the latent space.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "How many dimensions does the savings attribute span in the histograms shown for 2,000 training steps?",
        "answer": "The savings attribute spans two dimensions in the histograms, as indicated by the bimodal distribution pattern visible in the plot.",
        "visual_anchor": "middle histogram with overlapping red and blue bars showing bimodal pattern",
        "text_evidence": "From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).",
        "query_type": "value_context"
      },
      {
        "query": "Why does the rightmost histogram show a single distribution peak while the middle one displays two distinct modes?",
        "answer": "The rightmost histogram represents housing with one dimension, producing a single peak, while the middle histogram shows savings with two dimensions, creating the bimodal pattern.",
        "visual_anchor": "rightmost histogram with single peak versus middle histogram with two distinct modes",
        "text_evidence": "status of checking account (two dimensions), savings (two dimensions), and housing (one dimension)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the overlapping red and blue bars represent the posterior distributions in latent space after training?",
        "answer": "Yes, the overlapping red and blue bars represent posterior distributions q̃(Hs|A) in the latent space, visualized as histograms after training steps.",
        "visual_anchor": "overlapping red and blue histogram bars across x-axis range -10 to 15",
        "text_evidence": "In Fig. 9 we show histograms for posterior distributions in the latent space.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "overlapping red and blue histograms",
      "x-axis range from -10 to 15",
      "y-axis showing frequency",
      "bimodal distribution pattern",
      "peak heights varying across the range"
    ]
  },
  {
    "doc_id": "1803.04383",
    "figure_id": "1803.04383_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg",
    "caption": "Figure 1: The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Horizontal axis labeled 'Selection Rate' from 0 to 1\",\n    \"Vertical axis labeled 'Δμ' with 0 marked\",\n    \"Yellow dotted region in lower left quadrant\",\n    \"Cyan/light blue rectangular region spanning most of the area\",\n    \"Curved boundary between yellow and cyan regions\",\n    \"Dashed boundary lines at top and right edges\",\n    \"Label '(b)' below the diagram\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the cyan rectangular region spanning most of selection rates represent the no harm regime in the outcome curve?\",\n      \"answer\": \"Yes, the cyan region represents the no harm regime. The caption explicitly states that colors indicate regions of active harm, relative harm, and no harm, with the large cyan area depicting the no harm outcome regime.\",\n      \"visual_anchor\": \"Cyan rectangular region occupying the majority of the plot area from selection rate 0 to 1\",\n      \"text_evidence\": \"The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does the yellow dotted region curve from zero selection rate upward before transitioning to cyan at higher rates?\",\n      \"answer\": \"The yellow region represents a group with much potential for gain, as stated in the caption for panel (b). The curved boundary shows how mean score change varies with selection rate for populations that can benefit significantly from the policy.\",\n      \"visual_anchor\": \"Yellow dotted curved region in the lower left, bounded by a curved line transitioning to cyan\",\n      \"text_evidence\": \"In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the larger cyan region relate to the estimated MaxUtil policy underloan condition mentioned for relative improvement?\",\n      \"answer\": \"The larger cyan region corresponds to more regimes under which fairness criteria can yield favorable outcomes. When MaxUtil underloans, the region for relative improvement in the outcome curve expands, creating more opportunities for fairness-constrained policies to improve outcomes.\",\n      \"visual_anchor\": \"Large cyan region spanning from approximately selection rate 0.2 to 1.0\",\n      \"text_evidence\": \"Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Horizontal axis labeled 'Selection Rate' from 0 to 1",
        "Vertical axis labeled 'Δμ' with 0 marked",
        "Yellow dotted region in lower left quadrant",
        "Cyan/light blue rectangular region spanning most of the area",
        "Curved boundary between yellow and cyan regions",
        "Dashed boundary lines at top and right edges",
        "Label '(b)' below the diagram"
      ],
      "queries": [
        {
          "query": "Does the cyan rectangular region spanning most of selection rates represent the no harm regime in the outcome curve?",
          "answer": "Yes, the cyan region represents the no harm regime. The caption explicitly states that colors indicate regions of active harm, relative harm, and no harm, with the large cyan area depicting the no harm outcome regime.",
          "visual_anchor": "Cyan rectangular region occupying the majority of the plot area from selection rate 0 to 1",
          "text_evidence": "The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm.",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does the yellow dotted region curve from zero selection rate upward before transitioning to cyan at higher rates?",
          "answer": "The yellow region represents a group with much potential for gain, as stated in the caption for panel (b). The curved boundary shows how mean score change varies with selection rate for populations that can benefit significantly from the policy.",
          "visual_anchor": "Yellow dotted curved region in the lower left, bounded by a curved line transitioning to cyan",
          "text_evidence": "In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the larger cyan region relate to the estimated MaxUtil policy underloan condition mentioned for relative improvement?",
          "answer": "The larger cyan region corresponds to more regimes under which fairness criteria can yield favorable outcomes. When MaxUtil underloans, the region for relative improvement in the outcome curve expands, creating more opportunities for fairness-constrained policies to improve outcomes.",
          "visual_anchor": "Large cyan region spanning from approximately selection rate 0.2 to 1.0",
          "text_evidence": "Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the cyan rectangular region spanning most of selection rates represent the no harm regime in the outcome curve?",
        "answer": "Yes, the cyan region represents the no harm regime. The caption explicitly states that colors indicate regions of active harm, relative harm, and no harm, with the large cyan area depicting the no harm outcome regime.",
        "visual_anchor": "Cyan rectangular region occupying the majority of the plot area from selection rate 0 to 1",
        "text_evidence": "The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm.",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does the yellow dotted region curve from zero selection rate upward before transitioning to cyan at higher rates?",
        "answer": "The yellow region represents a group with much potential for gain, as stated in the caption for panel (b). The curved boundary shows how mean score change varies with selection rate for populations that can benefit significantly from the policy.",
        "visual_anchor": "Yellow dotted curved region in the lower left, bounded by a curved line transitioning to cyan",
        "text_evidence": "In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the larger cyan region relate to the estimated MaxUtil policy underloan condition mentioned for relative improvement?",
        "answer": "The larger cyan region corresponds to more regimes under which fairness criteria can yield favorable outcomes. When MaxUtil underloans, the region for relative improvement in the outcome curve expands, creating more opportunities for fairness-constrained policies to improve outcomes.",
        "visual_anchor": "Large cyan region spanning from approximately selection rate 0.2 to 1.0",
        "text_evidence": "Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Horizontal axis labeled 'Selection Rate' from 0 to 1",
      "Vertical axis labeled 'Δμ' with 0 marked",
      "Yellow dotted region in lower left quadrant",
      "Cyan/light blue rectangular region spanning most of the area",
      "Curved boundary between yellow and cyan regions",
      "Dashed boundary lines at top and right edges",
      "Label '(b)' below the diagram"
    ]
  },
  {
    "doc_id": "1803.04383",
    "figure_id": "1803.04383_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg",
    "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top panel: Δμ curve peaking early then declining to zero at β=1\",\n    \"Bottom panel: Three utility curves (MU-orange solid, DP-purple dashed, EQ-teal dashed)\",\n    \"Vertical markers: β^MaxUtil, β^EqOpt, β^DemParity\",\n    \"MU curve (orange solid) peaks earliest, leftmost among the three\",\n    \"DP curve (purple dashed) peaks in middle region\",\n    \"EQ curve (teal dashed) peaks rightmost, extends furthest\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the orange MU curve peak at β^MaxUtil, the leftmost position among all three utility curves?\",\n      \"answer\": \"The institution's individual utility function is more stringent than expected score changes (u(x)>0 implies Δ(x)>0), causing MaxUtil to optimize at a lower, more selective threshold than fairness-constrained rules.\",\n      \"visual_anchor\": \"Orange solid MU curve peaking at β^MaxUtil marker, leftmost vertical marker\",\n      \"text_evidence\": \"The institution's individual utility function is more stringent than the expected score changes, u(x)>0 ⟹ Δ(x)>0\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the three distinct peak positions of the utility curves in the lower panel support difficulty comparing DemParity and EqOpt?\",\n      \"answer\": \"Yes, the different peak locations reflect that comparing DemParity and EqOpt requires knowing full distributions πA, πB to compute the transfer function, and settings exist where each causes different outcomes.\",\n      \"visual_anchor\": \"Three utility curve maxima at different horizontal positions (β^MaxUtil, β^EqOpt, β^DemParity)\",\n      \"text_evidence\": \"it is difficult to compare DemParity and EqOpt without knowing the full distributions πA, πB, which is necessary to compute the transfer function G^(A→B). In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemP\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do derivatives with respect to horizontal axis selection rate βA recover the concave utility curves shown in the lower panel?\",\n      \"answer\": \"Projecting the EO and DP constraint curves from the top panel to the horizontal axis, then taking derivatives with respect to βA, transforms the constraints into the concave utility curves displayed below.\",\n      \"visual_anchor\": \"Lower panel showing three concave utility curves (MU, DP, EQ) plotted against Selection Rate horizontal axis\",\n      \"text_evidence\": \"The derivatives considered throughout Section 6 are taken with respect to the selection rate βA (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top panel: Δμ curve peaking early then declining to zero at β=1",
        "Bottom panel: Three utility curves (MU-orange solid, DP-purple dashed, EQ-teal dashed)",
        "Vertical markers: β^MaxUtil, β^EqOpt, β^DemParity",
        "MU curve (orange solid) peaks earliest, leftmost among the three",
        "DP curve (purple dashed) peaks in middle region",
        "EQ curve (teal dashed) peaks rightmost, extends furthest"
      ],
      "queries": [
        {
          "query": "Why does the orange MU curve peak at β^MaxUtil, the leftmost position among all three utility curves?",
          "answer": "The institution's individual utility function is more stringent than expected score changes (u(x)>0 implies Δ(x)>0), causing MaxUtil to optimize at a lower, more selective threshold than fairness-constrained rules.",
          "visual_anchor": "Orange solid MU curve peaking at β^MaxUtil marker, leftmost vertical marker",
          "text_evidence": "The institution's individual utility function is more stringent than the expected score changes, u(x)>0 ⟹ Δ(x)>0",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the three distinct peak positions of the utility curves in the lower panel support difficulty comparing DemParity and EqOpt?",
          "answer": "Yes, the different peak locations reflect that comparing DemParity and EqOpt requires knowing full distributions πA, πB to compute the transfer function, and settings exist where each causes different outcomes.",
          "visual_anchor": "Three utility curve maxima at different horizontal positions (β^MaxUtil, β^EqOpt, β^DemParity)",
          "text_evidence": "it is difficult to compare DemParity and EqOpt without knowing the full distributions πA, πB, which is necessary to compute the transfer function G^(A→B). In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemP",
          "query_type": "value_context"
        },
        {
          "query": "How do derivatives with respect to horizontal axis selection rate βA recover the concave utility curves shown in the lower panel?",
          "answer": "Projecting the EO and DP constraint curves from the top panel to the horizontal axis, then taking derivatives with respect to βA, transforms the constraints into the concave utility curves displayed below.",
          "visual_anchor": "Lower panel showing three concave utility curves (MU, DP, EQ) plotted against Selection Rate horizontal axis",
          "text_evidence": "The derivatives considered throughout Section 6 are taken with respect to the selection rate βA (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the orange MU curve peak at β^MaxUtil, the leftmost position among all three utility curves?",
        "answer": "The institution's individual utility function is more stringent than expected score changes (u(x)>0 implies Δ(x)>0), causing MaxUtil to optimize at a lower, more selective threshold than fairness-constrained rules.",
        "visual_anchor": "Orange solid MU curve peaking at β^MaxUtil marker, leftmost vertical marker",
        "text_evidence": "The institution's individual utility function is more stringent than the expected score changes, u(x)>0 ⟹ Δ(x)>0",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the three distinct peak positions of the utility curves in the lower panel support difficulty comparing DemParity and EqOpt?",
        "answer": "Yes, the different peak locations reflect that comparing DemParity and EqOpt requires knowing full distributions πA, πB to compute the transfer function, and settings exist where each causes different outcomes.",
        "visual_anchor": "Three utility curve maxima at different horizontal positions (β^MaxUtil, β^EqOpt, β^DemParity)",
        "text_evidence": "it is difficult to compare DemParity and EqOpt without knowing the full distributions πA, πB, which is necessary to compute the transfer function G^(A→B). In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemP",
        "query_type": "value_context"
      },
      {
        "query": "How do derivatives with respect to horizontal axis selection rate βA recover the concave utility curves shown in the lower panel?",
        "answer": "Projecting the EO and DP constraint curves from the top panel to the horizontal axis, then taking derivatives with respect to βA, transforms the constraints into the concave utility curves displayed below.",
        "visual_anchor": "Lower panel showing three concave utility curves (MU, DP, EQ) plotted against Selection Rate horizontal axis",
        "text_evidence": "The derivatives considered throughout Section 6 are taken with respect to the selection rate βA (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top panel: Δμ curve peaking early then declining to zero at β=1",
      "Bottom panel: Three utility curves (MU-orange solid, DP-purple dashed, EQ-teal dashed)",
      "Vertical markers: β^MaxUtil, β^EqOpt, β^DemParity",
      "MU curve (orange solid) peaks earliest, leftmost among the three",
      "DP curve (purple dashed) peaks in middle region",
      "EQ curve (teal dashed) peaks rightmost, extends furthest"
    ]
  },
  {
    "doc_id": "1803.04383",
    "figure_id": "1803.04383_fig_6",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg",
    "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Magenta curve labeled EO starting at origin, passing through point at approximately (0.2, 0.67)\",\n    \"Teal straight line labeled DP with slope 1, passing through point at approximately (0.55, 0.48)\",\n    \"Orange point labeled MU at approximately (0.18, 0.73)\",\n    \"Dashed concentric contour lines in background\",\n    \"Gray background with axes from 0.0 to 1.0\",\n    \"Horizontal axis labeled 'group A selection rate'\",\n    \"Vertical axis labeled 'group B selection rate'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the teal DP line have slope 1 while the magenta EO curve is nonlinear?\",\n      \"answer\": \"The DemParity constraint requires equal selection rates across groups (slope 1 straight line), while the EqOpt constraint follows the graph of G^(A→B), which is a curved function of selection rates.\",\n      \"visual_anchor\": \"Teal straight line labeled DP with slope 1 versus magenta curved line labeled EO\",\n      \"text_evidence\": \"The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\\\mathsf { A } \\\\to \\\\mathsf { B } ) }$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do derivatives along the magenta EO curve relate to the horizontal axis representing selection rate β_A?\",\n      \"answer\": \"Derivatives are taken with respect to the selection rate β_A on the horizontal axis. Projecting the EO constraint curve to this axis recovers concave utility curves shown in other figures.\",\n      \"visual_anchor\": \"Horizontal axis labeled 'group A selection rate' and magenta EO curve\",\n      \"text_evidence\": \"The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\\\beta _ { \\\\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the orange MU point lying off the magenta EO curve illustrate that MaxUtil is unconstrained by EqOpt?\",\n      \"answer\": \"Yes. The MU point represents the MaxUtil optimal solution without fairness constraints, while the magenta EO curve restricts optimization to solutions satisfying the EqOpt fairness constraint, which MU does not satisfy.\",\n      \"visual_anchor\": \"Orange point labeled MU at approximately (0.18, 0.73) positioned away from magenta EO curve\",\n      \"text_evidence\": \"fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\\\mathsf { A } \\\\to \\\\mathsf { B } ) }$\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Magenta curve labeled EO starting at origin, passing through point at approximately (0.2, 0.67)",
        "Teal straight line labeled DP with slope 1, passing through point at approximately (0.55, 0.48)",
        "Orange point labeled MU at approximately (0.18, 0.73)",
        "Dashed concentric contour lines in background",
        "Gray background with axes from 0.0 to 1.0",
        "Horizontal axis labeled 'group A selection rate'",
        "Vertical axis labeled 'group B selection rate'"
      ],
      "queries": [
        {
          "query": "Why does the teal DP line have slope 1 while the magenta EO curve is nonlinear?",
          "answer": "The DemParity constraint requires equal selection rates across groups (slope 1 straight line), while the EqOpt constraint follows the graph of G^(A→B), which is a curved function of selection rates.",
          "visual_anchor": "Teal straight line labeled DP with slope 1 versus magenta curved line labeled EO",
          "text_evidence": "The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do derivatives along the magenta EO curve relate to the horizontal axis representing selection rate β_A?",
          "answer": "Derivatives are taken with respect to the selection rate β_A on the horizontal axis. Projecting the EO constraint curve to this axis recovers concave utility curves shown in other figures.",
          "visual_anchor": "Horizontal axis labeled 'group A selection rate' and magenta EO curve",
          "text_evidence": "The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves",
          "query_type": "value_context"
        },
        {
          "query": "Does the orange MU point lying off the magenta EO curve illustrate that MaxUtil is unconstrained by EqOpt?",
          "answer": "Yes. The MU point represents the MaxUtil optimal solution without fairness constraints, while the magenta EO curve restricts optimization to solutions satisfying the EqOpt fairness constraint, which MU does not satisfy.",
          "visual_anchor": "Orange point labeled MU at approximately (0.18, 0.73) positioned away from magenta EO curve",
          "text_evidence": "fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the teal DP line have slope 1 while the magenta EO curve is nonlinear?",
        "answer": "The DemParity constraint requires equal selection rates across groups (slope 1 straight line), while the EqOpt constraint follows the graph of G^(A→B), which is a curved function of selection rates.",
        "visual_anchor": "Teal straight line labeled DP with slope 1 versus magenta curved line labeled EO",
        "text_evidence": "The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do derivatives along the magenta EO curve relate to the horizontal axis representing selection rate β_A?",
        "answer": "Derivatives are taken with respect to the selection rate β_A on the horizontal axis. Projecting the EO constraint curve to this axis recovers concave utility curves shown in other figures.",
        "visual_anchor": "Horizontal axis labeled 'group A selection rate' and magenta EO curve",
        "text_evidence": "The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves",
        "query_type": "value_context"
      },
      {
        "query": "Does the orange MU point lying off the magenta EO curve illustrate that MaxUtil is unconstrained by EqOpt?",
        "answer": "Yes. The MU point represents the MaxUtil optimal solution without fairness constraints, while the magenta EO curve restricts optimization to solutions satisfying the EqOpt fairness constraint, which MU does not satisfy.",
        "visual_anchor": "Orange point labeled MU at approximately (0.18, 0.73) positioned away from magenta EO curve",
        "text_evidence": "fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Magenta curve labeled EO starting at origin, passing through point at approximately (0.2, 0.67)",
      "Teal straight line labeled DP with slope 1, passing through point at approximately (0.55, 0.48)",
      "Orange point labeled MU at approximately (0.18, 0.73)",
      "Dashed concentric contour lines in background",
      "Gray background with axes from 0.0 to 1.0",
      "Horizontal axis labeled 'group A selection rate'",
      "Vertical axis labeled 'group B selection rate'"
    ]
  },
  {
    "doc_id": "1803.04383",
    "figure_id": "1803.04383_fig_10",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg",
    "caption": "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"solid black curve\", \"dashed gray curve\", \"x-axis labeled CDF ranging 0.0 to 1.0\", \"y-axis ranging 0.0 to 1.0\", \"curves starting near origin\", \"gray curve reaching ~1.0 around CDF 0.8\", \"black curve reaching ~1.0 at CDF 1.0\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dashed gray curve plateau near 1.0 before CDF reaches 0.8 while the solid black curve continues rising?\",\n      \"answer\": \"The dashed gray curve represents a group with higher repay probability at lower credit scores, showing that this group achieves near-maximum payback rates earlier in the credit score distribution compared to the solid black curve group.\",\n      \"visual_anchor\": \"dashed gray curve plateauing at ~1.0 around CDF 0.8\",\n      \"text_evidence\": \"we use this data to estimate the success probability given score, $\\\\rho _ { \\\\mathrm { j } } ( x )$ , which we allow to vary by group to match the empiric\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the solid black curve remaining below 0.2 until CDF 0.5 reflect the 90-day default prediction window used?\",\n      \"answer\": \"Yes, the solid black curve's low payback rates (below 0.2) until CDF 0.5 reflect that individuals with lower TransRisk scores are more likely to default within the 90-day period used for labeling credit risk.\",\n      \"visual_anchor\": \"solid black curve staying below 0.2 until CDF 0.5\",\n      \"text_evidence\": \"Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the two curves at CDF 0.4 demonstrate group-specific differences in TransRisk score predictive power?\",\n      \"answer\": \"At CDF 0.4, the dashed gray curve shows approximately 0.95 payback rate while the solid black curve shows only about 0.15, demonstrating that identical CDF positions correspond to vastly different repay probabilities between the two race groups in the TransUnion dataset.\",\n      \"visual_anchor\": \"separation between curves at CDF 0.4 (gray ~0.95, black ~0.15)\",\n      \"text_evidence\": \"These scores, corresponding to $x$ in our model, range from 300 to 850 and are meant to predict credit risk\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid black curve",
        "dashed gray curve",
        "x-axis labeled CDF ranging 0.0 to 1.0",
        "y-axis ranging 0.0 to 1.0",
        "curves starting near origin",
        "gray curve reaching ~1.0 around CDF 0.8",
        "black curve reaching ~1.0 at CDF 1.0"
      ],
      "queries": [
        {
          "query": "Why does the dashed gray curve plateau near 1.0 before CDF reaches 0.8 while the solid black curve continues rising?",
          "answer": "The dashed gray curve represents a group with higher repay probability at lower credit scores, showing that this group achieves near-maximum payback rates earlier in the credit score distribution compared to the solid black curve group.",
          "visual_anchor": "dashed gray curve plateauing at ~1.0 around CDF 0.8",
          "text_evidence": "we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empiric",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the solid black curve remaining below 0.2 until CDF 0.5 reflect the 90-day default prediction window used?",
          "answer": "Yes, the solid black curve's low payback rates (below 0.2) until CDF 0.5 reflect that individuals with lower TransRisk scores are more likely to default within the 90-day period used for labeling credit risk.",
          "visual_anchor": "solid black curve staying below 0.2 until CDF 0.5",
          "text_evidence": "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period",
          "query_type": "value_context"
        },
        {
          "query": "How do the two curves at CDF 0.4 demonstrate group-specific differences in TransRisk score predictive power?",
          "answer": "At CDF 0.4, the dashed gray curve shows approximately 0.95 payback rate while the solid black curve shows only about 0.15, demonstrating that identical CDF positions correspond to vastly different repay probabilities between the two race groups in the TransUnion dataset.",
          "visual_anchor": "separation between curves at CDF 0.4 (gray ~0.95, black ~0.15)",
          "text_evidence": "These scores, corresponding to $x$ in our model, range from 300 to 850 and are meant to predict credit risk",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dashed gray curve plateau near 1.0 before CDF reaches 0.8 while the solid black curve continues rising?",
        "answer": "The dashed gray curve represents a group with higher repay probability at lower credit scores, showing that this group achieves near-maximum payback rates earlier in the credit score distribution compared to the solid black curve group.",
        "visual_anchor": "dashed gray curve plateauing at ~1.0 around CDF 0.8",
        "text_evidence": "we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empiric",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the solid black curve remaining below 0.2 until CDF 0.5 reflect the 90-day default prediction window used?",
        "answer": "Yes, the solid black curve's low payback rates (below 0.2) until CDF 0.5 reflect that individuals with lower TransRisk scores are more likely to default within the 90-day period used for labeling credit risk.",
        "visual_anchor": "solid black curve staying below 0.2 until CDF 0.5",
        "text_evidence": "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period",
        "query_type": "value_context"
      },
      {
        "query": "How do the two curves at CDF 0.4 demonstrate group-specific differences in TransRisk score predictive power?",
        "answer": "At CDF 0.4, the dashed gray curve shows approximately 0.95 payback rate while the solid black curve shows only about 0.15, demonstrating that identical CDF positions correspond to vastly different repay probabilities between the two race groups in the TransUnion dataset.",
        "visual_anchor": "separation between curves at CDF 0.4 (gray ~0.95, black ~0.15)",
        "text_evidence": "These scores, corresponding to $x$ in our model, range from 300 to 850 and are meant to predict credit risk",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid black curve",
      "dashed gray curve",
      "x-axis labeled CDF ranging 0.0 to 1.0",
      "y-axis ranging 0.0 to 1.0",
      "curves starting near origin",
      "gray curve reaching ~1.0 around CDF 0.8",
      "black curve reaching ~1.0 at CDF 1.0"
    ]
  },
  {
    "doc_id": "1803.04383",
    "figure_id": "1803.04383_fig_12",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg",
    "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Black solid curve (empirical CDF)\",\n    \"White dashed curve (empirical CDF)\",\n    \"Red dotted horizontal line at 0.42 (harm threshold)\",\n    \"Orange solid horizontal line at 0.06 (MU threshold)\",\n    \"Teal dashed horizontal line at 0.27 (DP threshold)\",\n    \"Purple dash-dot horizontal line at 0.23 (EO threshold)\",\n    \"Score axis ranging from 300 to 800\",\n    \"Vertical lines at approximately score 500 and 600\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the teal DemParity line at 0.27 intersect the black curve above the red harm line at 0.42?\",\n      \"answer\": \"When the profit/loss ratio is 1/10, DemParity causes active harm to the Black group by setting a threshold that exceeds the harm criterion, as shown by the teal line crossing above the red dotted harm threshold.\",\n      \"visual_anchor\": \"teal dashed horizontal line at 0.27 positioned above red dotted line at 0.42\",\n      \"text_evidence\": \"The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How do the orange MaxUtil and purple EqOpt thresholds at 0.06 and 0.23 compare to the harm threshold at 0.42?\",\n      \"answer\": \"Both MaxUtil (orange, 0.06) and EqOpt (purple, 0.23) remain below the harm threshold (red, 0.42), confirming that these strategies never cause active harm regardless of utility settings.\",\n      \"visual_anchor\": \"orange solid line at 0.06 and purple dash-dot line at 0.23, both below red dotted line at 0.42\",\n      \"text_evidence\": \"EqOpt and MaxUtil never cause active harm.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the steeper slope of the black CDF curve between scores 400-600 compared to the white curve?\",\n      \"answer\": \"The empirical CDFs show different distributions between Black and White groups. The steeper Black curve indicates a higher concentration of scores in the 400-600 range, affecting how different loaning strategies impact each group differently.\",\n      \"visual_anchor\": \"black solid curve with steep descent from 1.0 to 0.2 between scores 400-600, versus gradual white dashed curve\",\n      \"text_evidence\": \"The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black solid curve (empirical CDF)",
        "White dashed curve (empirical CDF)",
        "Red dotted horizontal line at 0.42 (harm threshold)",
        "Orange solid horizontal line at 0.06 (MU threshold)",
        "Teal dashed horizontal line at 0.27 (DP threshold)",
        "Purple dash-dot horizontal line at 0.23 (EO threshold)",
        "Score axis ranging from 300 to 800",
        "Vertical lines at approximately score 500 and 600"
      ],
      "queries": [
        {
          "query": "Why does the teal DemParity line at 0.27 intersect the black curve above the red harm line at 0.42?",
          "answer": "When the profit/loss ratio is 1/10, DemParity causes active harm to the Black group by setting a threshold that exceeds the harm criterion, as shown by the teal line crossing above the red dotted harm threshold.",
          "visual_anchor": "teal dashed horizontal line at 0.27 positioned above red dotted line at 0.42",
          "text_evidence": "The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How do the orange MaxUtil and purple EqOpt thresholds at 0.06 and 0.23 compare to the harm threshold at 0.42?",
          "answer": "Both MaxUtil (orange, 0.06) and EqOpt (purple, 0.23) remain below the harm threshold (red, 0.42), confirming that these strategies never cause active harm regardless of utility settings.",
          "visual_anchor": "orange solid line at 0.06 and purple dash-dot line at 0.23, both below red dotted line at 0.42",
          "text_evidence": "EqOpt and MaxUtil never cause active harm.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the steeper slope of the black CDF curve between scores 400-600 compared to the white curve?",
          "answer": "The empirical CDFs show different distributions between Black and White groups. The steeper Black curve indicates a higher concentration of scores in the 400-600 range, affecting how different loaning strategies impact each group differently.",
          "visual_anchor": "black solid curve with steep descent from 1.0 to 0.2 between scores 400-600, versus gradual white dashed curve",
          "text_evidence": "The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the teal DemParity line at 0.27 intersect the black curve above the red harm line at 0.42?",
        "answer": "When the profit/loss ratio is 1/10, DemParity causes active harm to the Black group by setting a threshold that exceeds the harm criterion, as shown by the teal line crossing above the red dotted harm threshold.",
        "visual_anchor": "teal dashed horizontal line at 0.27 positioned above red dotted line at 0.42",
        "text_evidence": "The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How do the orange MaxUtil and purple EqOpt thresholds at 0.06 and 0.23 compare to the harm threshold at 0.42?",
        "answer": "Both MaxUtil (orange, 0.06) and EqOpt (purple, 0.23) remain below the harm threshold (red, 0.42), confirming that these strategies never cause active harm regardless of utility settings.",
        "visual_anchor": "orange solid line at 0.06 and purple dash-dot line at 0.23, both below red dotted line at 0.42",
        "text_evidence": "EqOpt and MaxUtil never cause active harm.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the steeper slope of the black CDF curve between scores 400-600 compared to the white curve?",
        "answer": "The empirical CDFs show different distributions between Black and White groups. The steeper Black curve indicates a higher concentration of scores in the 400-600 range, affecting how different loaning strategies impact each group differently.",
        "visual_anchor": "black solid curve with steep descent from 1.0 to 0.2 between scores 400-600, versus gradual white dashed curve",
        "text_evidence": "The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black solid curve (empirical CDF)",
      "White dashed curve (empirical CDF)",
      "Red dotted horizontal line at 0.42 (harm threshold)",
      "Orange solid horizontal line at 0.06 (MU threshold)",
      "Teal dashed horizontal line at 0.27 (DP threshold)",
      "Purple dash-dot horizontal line at 0.23 (EO threshold)",
      "Score axis ranging from 300 to 800",
      "Vertical lines at approximately score 500 and 600"
    ]
  },
  {
    "doc_id": "1803.04383",
    "figure_id": "1803.04383_fig_16",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg",
    "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Orange solid curve peaking around 0.6 on x-axis at y≈0.55\",\n    \"Teal dashed curve (D) with maximum around y≈0.25\",\n    \"Purple dot-dashed curve (EC) with maximum around y≈0.35\",\n    \"All three curves crossing y=0 between x=0.8 and x=1.0\",\n    \"Y-axis ranging from -1.00 to 1.00\",\n    \"X-axis ranging from 0.0 to 1.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the purple dot-dashed utility curve's maximum at approximately 0.35 relate to determining decision rule thresholds?\",\n      \"answer\": \"The relative positions of the utility maxima determine the position of the decision rule thresholds, as stated with the fixed ratio u−/u+ = −4. The purple curve's peak indicates the optimal selection rate for that group.\",\n      \"visual_anchor\": \"Purple dot-dashed curve (EC) with maximum around y≈0.35\",\n      \"text_evidence\": \"The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the teal dashed and purple curves both drop below zero after x=0.8, given the examination of normalized outcome curves?\",\n      \"answer\": \"These curves represent utility curves for white and black groups respectively. The negative values after x=0.8 indicate that high selection rates produce negative utility for both groups, reflecting the outcome-utility relationship examined in the figure.\",\n      \"visual_anchor\": \"Teal dashed and purple dot-dashed curves crossing y=0 between x=0.8 and x=1.0\",\n      \"text_evidence\": \"These results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the teal dashed curve's lower maximum compared to purple explain different loan rates between groups?\",\n      \"answer\": \"Yes, the different heights of utility maxima directly determine different loan (selection) rates for each group. The teal curve's lower peak at approximately 0.25 versus purple's 0.35 results in different optimal selection rates.\",\n      \"visual_anchor\": \"Teal dashed curve maximum at y≈0.25 versus purple dot-dashed curve maximum at y≈0.35\",\n      \"text_evidence\": \"Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Orange solid curve peaking around 0.6 on x-axis at y≈0.55",
        "Teal dashed curve (D) with maximum around y≈0.25",
        "Purple dot-dashed curve (EC) with maximum around y≈0.35",
        "All three curves crossing y=0 between x=0.8 and x=1.0",
        "Y-axis ranging from -1.00 to 1.00",
        "X-axis ranging from 0.0 to 1.0"
      ],
      "queries": [
        {
          "query": "How does the purple dot-dashed utility curve's maximum at approximately 0.35 relate to determining decision rule thresholds?",
          "answer": "The relative positions of the utility maxima determine the position of the decision rule thresholds, as stated with the fixed ratio u−/u+ = −4. The purple curve's peak indicates the optimal selection rate for that group.",
          "visual_anchor": "Purple dot-dashed curve (EC) with maximum around y≈0.35",
          "text_evidence": "The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
          "query_type": "value_context"
        },
        {
          "query": "Why do the teal dashed and purple curves both drop below zero after x=0.8, given the examination of normalized outcome curves?",
          "answer": "These curves represent utility curves for white and black groups respectively. The negative values after x=0.8 indicate that high selection rates produce negative utility for both groups, reflecting the outcome-utility relationship examined in the figure.",
          "visual_anchor": "Teal dashed and purple dot-dashed curves crossing y=0 between x=0.8 and x=1.0",
          "text_evidence": "These results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the teal dashed curve's lower maximum compared to purple explain different loan rates between groups?",
          "answer": "Yes, the different heights of utility maxima directly determine different loan (selection) rates for each group. The teal curve's lower peak at approximately 0.25 versus purple's 0.35 results in different optimal selection rates.",
          "visual_anchor": "Teal dashed curve maximum at y≈0.25 versus purple dot-dashed curve maximum at y≈0.35",
          "text_evidence": "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the purple dot-dashed utility curve's maximum at approximately 0.35 relate to determining decision rule thresholds?",
        "answer": "The relative positions of the utility maxima determine the position of the decision rule thresholds, as stated with the fixed ratio u−/u+ = −4. The purple curve's peak indicates the optimal selection rate for that group.",
        "visual_anchor": "Purple dot-dashed curve (EC) with maximum around y≈0.35",
        "text_evidence": "The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
        "query_type": "value_context"
      },
      {
        "query": "Why do the teal dashed and purple curves both drop below zero after x=0.8, given the examination of normalized outcome curves?",
        "answer": "These curves represent utility curves for white and black groups respectively. The negative values after x=0.8 indicate that high selection rates produce negative utility for both groups, reflecting the outcome-utility relationship examined in the figure.",
        "visual_anchor": "Teal dashed and purple dot-dashed curves crossing y=0 between x=0.8 and x=1.0",
        "text_evidence": "These results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the teal dashed curve's lower maximum compared to purple explain different loan rates between groups?",
        "answer": "Yes, the different heights of utility maxima directly determine different loan (selection) rates for each group. The teal curve's lower peak at approximately 0.25 versus purple's 0.35 results in different optimal selection rates.",
        "visual_anchor": "Teal dashed curve maximum at y≈0.25 versus purple dot-dashed curve maximum at y≈0.35",
        "text_evidence": "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Orange solid curve peaking around 0.6 on x-axis at y≈0.55",
      "Teal dashed curve (D) with maximum around y≈0.25",
      "Purple dot-dashed curve (EC) with maximum around y≈0.35",
      "All three curves crossing y=0 between x=0.8 and x=1.0",
      "Y-axis ranging from -1.00 to 1.00",
      "X-axis ranging from 0.0 to 1.0"
    ]
  },
  {
    "doc_id": "1804.06876",
    "figure_id": "1804.06876_fig_3",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg",
    "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"solid blue boxes marking 'The physician' and 'him'\",\n    \"dashed orange boxes marking 'the secretary' and 'her'\",\n    \"solid purple line connecting pro-stereotypical coreference\",\n    \"dashed purple line connecting anti-stereotypical coreference\",\n    \"two parallel sentence pairs with gender-swapped pronouns\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why must systems correctly resolve both the solid and dashed purple coreference lines to pass the test?\",\n      \"answer\": \"Systems must perform equally well in pro-stereotypical scenarios (solid purple) and anti-stereotypical scenarios (dashed purple) to demonstrate they are not biased by gender stereotypes based on occupations.\",\n      \"visual_anchor\": \"solid purple line and dashed purple line showing pro-stereotypical and anti-stereotypical coreferences\",\n      \"text_evidence\": \"Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the dashed orange markings on 'the secretary' and 'her' relate to US labor statistics?\",\n      \"answer\": \"The dashed orange markings indicate female entities in occupations that are stereotypically female according to US Department of Labor statistics, such as secretary where the majority of workers are women.\",\n      \"visual_anchor\": \"dashed orange boxes marking 'the secretary' and 'her'\",\n      \"text_evidence\": \"Importantly, stereotypical occupations are considered based on US Department of Labor statistics.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What makes the gender swap between solid blue 'him' and dashed orange 'her' irrelevant for coreference decisions?\",\n      \"answer\": \"The gender of the pronominal reference is explicitly designed to be irrelevant because the coreference decision should be based on contextual clues rather than stereotypical gender-occupation associations.\",\n      \"visual_anchor\": \"solid blue 'him' in top sentence versus dashed orange 'her' in bottom sentence\",\n      \"text_evidence\": \"For each example, the gender of the pronominal reference is irrelevant for the co-reference decision.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "solid blue boxes marking 'The physician' and 'him'",
        "dashed orange boxes marking 'the secretary' and 'her'",
        "solid purple line connecting pro-stereotypical coreference",
        "dashed purple line connecting anti-stereotypical coreference",
        "two parallel sentence pairs with gender-swapped pronouns"
      ],
      "queries": [
        {
          "query": "Why must systems correctly resolve both the solid and dashed purple coreference lines to pass the test?",
          "answer": "Systems must perform equally well in pro-stereotypical scenarios (solid purple) and anti-stereotypical scenarios (dashed purple) to demonstrate they are not biased by gender stereotypes based on occupations.",
          "visual_anchor": "solid purple line and dashed purple line showing pro-stereotypical and anti-stereotypical coreferences",
          "text_evidence": "Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the dashed orange markings on 'the secretary' and 'her' relate to US labor statistics?",
          "answer": "The dashed orange markings indicate female entities in occupations that are stereotypically female according to US Department of Labor statistics, such as secretary where the majority of workers are women.",
          "visual_anchor": "dashed orange boxes marking 'the secretary' and 'her'",
          "text_evidence": "Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
          "query_type": "value_context"
        },
        {
          "query": "What makes the gender swap between solid blue 'him' and dashed orange 'her' irrelevant for coreference decisions?",
          "answer": "The gender of the pronominal reference is explicitly designed to be irrelevant because the coreference decision should be based on contextual clues rather than stereotypical gender-occupation associations.",
          "visual_anchor": "solid blue 'him' in top sentence versus dashed orange 'her' in bottom sentence",
          "text_evidence": "For each example, the gender of the pronominal reference is irrelevant for the co-reference decision.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why must systems correctly resolve both the solid and dashed purple coreference lines to pass the test?",
        "answer": "Systems must perform equally well in pro-stereotypical scenarios (solid purple) and anti-stereotypical scenarios (dashed purple) to demonstrate they are not biased by gender stereotypes based on occupations.",
        "visual_anchor": "solid purple line and dashed purple line showing pro-stereotypical and anti-stereotypical coreferences",
        "text_evidence": "Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the dashed orange markings on 'the secretary' and 'her' relate to US labor statistics?",
        "answer": "The dashed orange markings indicate female entities in occupations that are stereotypically female according to US Department of Labor statistics, such as secretary where the majority of workers are women.",
        "visual_anchor": "dashed orange boxes marking 'the secretary' and 'her'",
        "text_evidence": "Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
        "query_type": "value_context"
      },
      {
        "query": "What makes the gender swap between solid blue 'him' and dashed orange 'her' irrelevant for coreference decisions?",
        "answer": "The gender of the pronominal reference is explicitly designed to be irrelevant because the coreference decision should be based on contextual clues rather than stereotypical gender-occupation associations.",
        "visual_anchor": "solid blue 'him' in top sentence versus dashed orange 'her' in bottom sentence",
        "text_evidence": "For each example, the gender of the pronominal reference is irrelevant for the co-reference decision.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "solid blue boxes marking 'The physician' and 'him'",
      "dashed orange boxes marking 'the secretary' and 'her'",
      "solid purple line connecting pro-stereotypical coreference",
      "dashed purple line connecting anti-stereotypical coreference",
      "two parallel sentence pairs with gender-swapped pronouns"
    ]
  },
  {
    "doc_id": "1804.09301",
    "figure_id": "1804.09301_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg",
    "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Three example sentences with pronouns 'his', 'their', and 'her'\",\n    \"Yellow highlighted 'Mention' labels\",\n    \"Dashed 'coref' arrows connecting pronouns to 'The surgeon'\",\n    \"First example: coref arrows connect 'his' pronouns\",\n    \"Second example: coref arrows connect 'their' pronouns\",\n    \"Third example: no coref arrows for 'her' pronouns\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the coref arrows appear for male and neutral pronouns but disappear for the female pronoun in the third example?\",\n      \"answer\": \"The Stanford CoreNLP rule-based system exhibits systematic gender bias, resolving male and neutral pronouns as coreferent with \\\"The surgeon\\\" but failing to do so for the corresponding female pronoun.\",\n      \"visual_anchor\": \"Absence of dashed coref arrows in the third example with 'her' pronouns, contrasted with present coref arrows in first and second examples\",\n      \"text_evidence\": \"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the presence of coref arrows only for male pronouns in the first example confirm the systematic gender bias claim?\",\n      \"answer\": \"Yes, the coref arrows connecting 'his' to 'The surgeon' in the first example, while absent for 'her' in the third example, directly demonstrates the systematic gender bias confirmed by evaluating three publicly-available coreference resolution systems.\",\n      \"visual_anchor\": \"Dashed coref arrows connecting 'his' mentions to 'The surgeon' in the first example row\",\n      \"text_evidence\": \"With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the differential coreference resolution across the three pronoun examples illustrate the riddle about occupation stereotypes?\",\n      \"answer\": \"The figure shows that coreference systems are more likely to link male pronouns to 'surgeon' than female pronouns, mirroring how the classic riddle exploits gender stereotypes about occupations. This reveals how coreference resolution in English is tightly bound with questions of gender.\",\n      \"visual_anchor\": \"Comparison of coref arrow patterns across all three examples showing 'his', 'their', and 'her' pronouns\",\n      \"text_evidence\": \"There is a classic riddle: A man and his son get into a terrible ca... As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Three example sentences with pronouns 'his', 'their', and 'her'",
        "Yellow highlighted 'Mention' labels",
        "Dashed 'coref' arrows connecting pronouns to 'The surgeon'",
        "First example: coref arrows connect 'his' pronouns",
        "Second example: coref arrows connect 'their' pronouns",
        "Third example: no coref arrows for 'her' pronouns"
      ],
      "queries": [
        {
          "query": "Why do the coref arrows appear for male and neutral pronouns but disappear for the female pronoun in the third example?",
          "answer": "The Stanford CoreNLP rule-based system exhibits systematic gender bias, resolving male and neutral pronouns as coreferent with \"The surgeon\" but failing to do so for the corresponding female pronoun.",
          "visual_anchor": "Absence of dashed coref arrows in the third example with 'her' pronouns, contrasted with present coref arrows in first and second examples",
          "text_evidence": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the presence of coref arrows only for male pronouns in the first example confirm the systematic gender bias claim?",
          "answer": "Yes, the coref arrows connecting 'his' to 'The surgeon' in the first example, while absent for 'her' in the third example, directly demonstrates the systematic gender bias confirmed by evaluating three publicly-available coreference resolution systems.",
          "visual_anchor": "Dashed coref arrows connecting 'his' mentions to 'The surgeon' in the first example row",
          "text_evidence": "With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",
          "query_type": "value_context"
        },
        {
          "query": "How does the differential coreference resolution across the three pronoun examples illustrate the riddle about occupation stereotypes?",
          "answer": "The figure shows that coreference systems are more likely to link male pronouns to 'surgeon' than female pronouns, mirroring how the classic riddle exploits gender stereotypes about occupations. This reveals how coreference resolution in English is tightly bound with questions of gender.",
          "visual_anchor": "Comparison of coref arrow patterns across all three examples showing 'his', 'their', and 'her' pronouns",
          "text_evidence": "There is a classic riddle: A man and his son get into a terrible ca... As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the coref arrows appear for male and neutral pronouns but disappear for the female pronoun in the third example?",
        "answer": "The Stanford CoreNLP rule-based system exhibits systematic gender bias, resolving male and neutral pronouns as coreferent with \"The surgeon\" but failing to do so for the corresponding female pronoun.",
        "visual_anchor": "Absence of dashed coref arrows in the third example with 'her' pronouns, contrasted with present coref arrows in first and second examples",
        "text_evidence": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the presence of coref arrows only for male pronouns in the first example confirm the systematic gender bias claim?",
        "answer": "Yes, the coref arrows connecting 'his' to 'The surgeon' in the first example, while absent for 'her' in the third example, directly demonstrates the systematic gender bias confirmed by evaluating three publicly-available coreference resolution systems.",
        "visual_anchor": "Dashed coref arrows connecting 'his' mentions to 'The surgeon' in the first example row",
        "text_evidence": "With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",
        "query_type": "value_context"
      },
      {
        "query": "How does the differential coreference resolution across the three pronoun examples illustrate the riddle about occupation stereotypes?",
        "answer": "The figure shows that coreference systems are more likely to link male pronouns to 'surgeon' than female pronouns, mirroring how the classic riddle exploits gender stereotypes about occupations. This reveals how coreference resolution in English is tightly bound with questions of gender.",
        "visual_anchor": "Comparison of coref arrow patterns across all three examples showing 'his', 'their', and 'her' pronouns",
        "text_evidence": "There is a classic riddle: A man and his son get into a terrible ca... As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Three example sentences with pronouns 'his', 'their', and 'her'",
      "Yellow highlighted 'Mention' labels",
      "Dashed 'coref' arrows connecting pronouns to 'The surgeon'",
      "First example: coref arrows connect 'his' pronouns",
      "Second example: coref arrows connect 'their' pronouns",
      "Third example: no coref arrows for 'her' pronouns"
    ]
  },
  {
    "doc_id": "1804.09301",
    "figure_id": "1804.09301_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg",
    "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Sentence 1a with 'the passenger' highlighted in blue\",\n    \"Sentence 2a with 'the passenger' highlighted in blue\",\n    \"Sentence 1b with 'someone' highlighted in blue\",\n    \"Sentence 2b with 'someone' highlighted in blue\",\n    \"Green highlighting on 'The paramedic' in all sentences\",\n    \"Bold text indicating correct answers\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do sentences 1a and 1b both use 'the passenger' and 'someone' when rule-based systems like Stanford multi-pass sieve lack large-scale training data?\",\n      \"answer\": \"The schema tests whether systems can resolve pronouns correctly using different participant types. Early rule-based systems relied on expert knowledge rather than data-driven patterns to handle such coreference variations.\",\n      \"visual_anchor\": \"sentences 1a and 1b showing 'the passenger' (blue) versus 'someone' (blue) as different participant types\",\n      \"text_evidence\": \"In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the bold correct answers in sentences 2a and 2b relate to the claim that all systems perform worse on gotcha sentences?\",\n      \"answer\": \"Sentences 2a and 2b represent cases where pronoun resolution is tested against majority gender expectations. The bold answers show the ground truth that systems struggle with when pronoun gender mismatches the occupation's typical gender distribution.\",\n      \"visual_anchor\": \"bold text indicating correct answers in sentences 2a ('was/were already dead') and 2b ('was/were already dead')\",\n      \"text_evidence\": \"We also identify so-called \\\"gotcha\\\" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these \\\"gotchas.\\\"\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the green highlighting of 'The paramedic' across all four sentences reflect the occupation data source from Bureau of Labor Statistics?\",\n      \"answer\": \"Yes, the consistent green highlighting marks the occupation term that is paired with gender statistics. Each occupation's schema is constructed using Bureau of Labor Statistics data to determine majority gender for evaluation purposes.\",\n      \"visual_anchor\": \"green highlighting on 'The paramedic' appearing identically in sentences 1a, 2a, 1b, and 2b\",\n      \"text_evidence\": \"Bureau of Labor Statistics. For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Sentence 1a with 'the passenger' highlighted in blue",
        "Sentence 2a with 'the passenger' highlighted in blue",
        "Sentence 1b with 'someone' highlighted in blue",
        "Sentence 2b with 'someone' highlighted in blue",
        "Green highlighting on 'The paramedic' in all sentences",
        "Bold text indicating correct answers"
      ],
      "queries": [
        {
          "query": "Why do sentences 1a and 1b both use 'the passenger' and 'someone' when rule-based systems like Stanford multi-pass sieve lack large-scale training data?",
          "answer": "The schema tests whether systems can resolve pronouns correctly using different participant types. Early rule-based systems relied on expert knowledge rather than data-driven patterns to handle such coreference variations.",
          "visual_anchor": "sentences 1a and 1b showing 'the passenger' (blue) versus 'someone' (blue) as different participant types",
          "text_evidence": "In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the bold correct answers in sentences 2a and 2b relate to the claim that all systems perform worse on gotcha sentences?",
          "answer": "Sentences 2a and 2b represent cases where pronoun resolution is tested against majority gender expectations. The bold answers show the ground truth that systems struggle with when pronoun gender mismatches the occupation's typical gender distribution.",
          "visual_anchor": "bold text indicating correct answers in sentences 2a ('was/were already dead') and 2b ('was/were already dead')",
          "text_evidence": "We also identify so-called \"gotcha\" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these \"gotchas.\"",
          "query_type": "value_context"
        },
        {
          "query": "Does the green highlighting of 'The paramedic' across all four sentences reflect the occupation data source from Bureau of Labor Statistics?",
          "answer": "Yes, the consistent green highlighting marks the occupation term that is paired with gender statistics. Each occupation's schema is constructed using Bureau of Labor Statistics data to determine majority gender for evaluation purposes.",
          "visual_anchor": "green highlighting on 'The paramedic' appearing identically in sentences 1a, 2a, 1b, and 2b",
          "text_evidence": "Bureau of Labor Statistics. For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do sentences 1a and 1b both use 'the passenger' and 'someone' when rule-based systems like Stanford multi-pass sieve lack large-scale training data?",
        "answer": "The schema tests whether systems can resolve pronouns correctly using different participant types. Early rule-based systems relied on expert knowledge rather than data-driven patterns to handle such coreference variations.",
        "visual_anchor": "sentences 1a and 1b showing 'the passenger' (blue) versus 'someone' (blue) as different participant types",
        "text_evidence": "In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the bold correct answers in sentences 2a and 2b relate to the claim that all systems perform worse on gotcha sentences?",
        "answer": "Sentences 2a and 2b represent cases where pronoun resolution is tested against majority gender expectations. The bold answers show the ground truth that systems struggle with when pronoun gender mismatches the occupation's typical gender distribution.",
        "visual_anchor": "bold text indicating correct answers in sentences 2a ('was/were already dead') and 2b ('was/were already dead')",
        "text_evidence": "We also identify so-called \"gotcha\" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these \"gotchas.\"",
        "query_type": "value_context"
      },
      {
        "query": "Does the green highlighting of 'The paramedic' across all four sentences reflect the occupation data source from Bureau of Labor Statistics?",
        "answer": "Yes, the consistent green highlighting marks the occupation term that is paired with gender statistics. Each occupation's schema is constructed using Bureau of Labor Statistics data to determine majority gender for evaluation purposes.",
        "visual_anchor": "green highlighting on 'The paramedic' appearing identically in sentences 1a, 2a, 1b, and 2b",
        "text_evidence": "Bureau of Labor Statistics. For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Sentence 1a with 'the passenger' highlighted in blue",
      "Sentence 2a with 'the passenger' highlighted in blue",
      "Sentence 1b with 'someone' highlighted in blue",
      "Sentence 2b with 'someone' highlighted in blue",
      "Green highlighting on 'The paramedic' in all sentences",
      "Bold text indicating correct answers"
    ]
  },
  {
    "doc_id": "1804.09301",
    "figure_id": "1804.09301_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg",
    "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"x-axis: % Female by Occupation (Bureau of Labor Stats, 2015-16)\", \"y-axis: % Female by Occupation in Text (Bergsma and Lin, 2006)\", \"dotted red 45-degree diagonal line\", \"solid blue regression line with 95% confidence interval\", \"scattered blue data points mostly below diagonal\", \"Pearson Γ = 0.67\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do most blue data points lie below the dotted diagonal when Pearson correlation equals 0.67?\",\n      \"answer\": \"The Bergsma and Lin (2006) gender statistics are systematically male-skewed compared to Bureau of Labor Statistics, causing most points to fall below the 45-degree line despite positive correlation.\",\n      \"visual_anchor\": \"blue data points positioned below the dotted red 45-degree diagonal line\",\n      \"text_evidence\": \"although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3)\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the blue regression line's slope below the diagonal support claims about sequential amplification in NLP pipelines?\",\n      \"answer\": \"Yes, the systematically lower slope indicates male-skewed data from Bergsma and Lin, which demonstrates how biased statistics can propagate and amplify through data-driven NLP systems.\",\n      \"visual_anchor\": \"solid blue regression line with slope less than 1, positioned below diagonal\",\n      \"text_evidence\": \"datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the 95% confidence interval around the blue regression line relate to evaluating coreference system architectures?\",\n      \"answer\": \"The confidence interval quantifies uncertainty in the biased gender statistics used by systems like Lee et al. (2011) RULE, Durrett and Klein (2013) STAT, and Clark and Manning (2016a) NEURAL that are being evaluated for gender bias.\",\n      \"visual_anchor\": \"shaded blue 95% confidence interval region around the regression line\",\n      \"text_evidence\": \"We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL)\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "x-axis: % Female by Occupation (Bureau of Labor Stats, 2015-16)",
        "y-axis: % Female by Occupation in Text (Bergsma and Lin, 2006)",
        "dotted red 45-degree diagonal line",
        "solid blue regression line with 95% confidence interval",
        "scattered blue data points mostly below diagonal",
        "Pearson Γ = 0.67"
      ],
      "queries": [
        {
          "query": "Why do most blue data points lie below the dotted diagonal when Pearson correlation equals 0.67?",
          "answer": "The Bergsma and Lin (2006) gender statistics are systematically male-skewed compared to Bureau of Labor Statistics, causing most points to fall below the 45-degree line despite positive correlation.",
          "visual_anchor": "blue data points positioned below the dotted red 45-degree diagonal line",
          "text_evidence": "although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3)",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the blue regression line's slope below the diagonal support claims about sequential amplification in NLP pipelines?",
          "answer": "Yes, the systematically lower slope indicates male-skewed data from Bergsma and Lin, which demonstrates how biased statistics can propagate and amplify through data-driven NLP systems.",
          "visual_anchor": "solid blue regression line with slope less than 1, positioned below diagonal",
          "text_evidence": "datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the 95% confidence interval around the blue regression line relate to evaluating coreference system architectures?",
          "answer": "The confidence interval quantifies uncertainty in the biased gender statistics used by systems like Lee et al. (2011) RULE, Durrett and Klein (2013) STAT, and Clark and Manning (2016a) NEURAL that are being evaluated for gender bias.",
          "visual_anchor": "shaded blue 95% confidence interval region around the regression line",
          "text_evidence": "We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL)",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do most blue data points lie below the dotted diagonal when Pearson correlation equals 0.67?",
        "answer": "The Bergsma and Lin (2006) gender statistics are systematically male-skewed compared to Bureau of Labor Statistics, causing most points to fall below the 45-degree line despite positive correlation.",
        "visual_anchor": "blue data points positioned below the dotted red 45-degree diagonal line",
        "text_evidence": "although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3)",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the blue regression line's slope below the diagonal support claims about sequential amplification in NLP pipelines?",
        "answer": "Yes, the systematically lower slope indicates male-skewed data from Bergsma and Lin, which demonstrates how biased statistics can propagate and amplify through data-driven NLP systems.",
        "visual_anchor": "solid blue regression line with slope less than 1, positioned below diagonal",
        "text_evidence": "datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the 95% confidence interval around the blue regression line relate to evaluating coreference system architectures?",
        "answer": "The confidence interval quantifies uncertainty in the biased gender statistics used by systems like Lee et al. (2011) RULE, Durrett and Klein (2013) STAT, and Clark and Manning (2016a) NEURAL that are being evaluated for gender bias.",
        "visual_anchor": "shaded blue 95% confidence interval region around the regression line",
        "text_evidence": "We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL)",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "x-axis: % Female by Occupation (Bureau of Labor Stats, 2015-16)",
      "y-axis: % Female by Occupation in Text (Bergsma and Lin, 2006)",
      "dotted red 45-degree diagonal line",
      "solid blue regression line with 95% confidence interval",
      "scattered blue data points mostly below diagonal",
      "Pearson Γ = 0.67"
    ]
  },
  {
    "doc_id": "1804.09301",
    "figure_id": "1804.09301_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg",
    "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue dots and regression line representing STAT system\",\n    \"Green triangles and regression line representing RULE system\",\n    \"Red diamonds and regression line representing NEURAL system\",\n    \"Horizontal black dashed line at y=0\",\n    \"X-axis showing % Female by Occupation (0-100)\",\n    \"Y-axis showing Gendered Pronoun Resolution (-100 to 100)\",\n    \"95% confidence interval bands around regression lines\",\n    \"Data points scattered across occupations\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green RULE regression line cross y=0 near 50% female, given the evaluation set is gender-balanced?\",\n      \"answer\": \"The horizontal dotted black line at y=0 represents a hypothetical system with 100% accuracy on the gender-balanced Winogender set. The green RULE line crossing y=0 at 50% female indicates that RULE only achieves unbiased resolution when occupations have equal gender representation in labor statistics.\",\n      \"visual_anchor\": \"green regression line crossing horizontal black dashed line at y=0 around x=50\",\n      \"text_evidence\": \"Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line (y=0) in both plots represents a hypothetical system with 100% accuracy.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the three regression lines with 95% confidence intervals reveal varying degrees of gender bias across system architectures?\",\n      \"answer\": \"Yes, the three regression lines (blue STAT, green RULE, red NEURAL) show different slopes and intercepts, with RULE showing the steepest positive slope and NEURAL showing the flattest. This visual pattern directly confirms that all three systems exhibit varying degrees of gender bias as measured by the Winogender schemas.\",\n      \"visual_anchor\": \"three colored regression lines with different slopes and 95% confidence interval bands\",\n      \"text_evidence\": \"By multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do several occupations cluster at -100 on the y-axis despite testing the deep reinforcement neural system?\",\n      \"answer\": \"The clustering at -100 (maximum male bias) indicates that for certain occupations, the Clark and Manning (2016a) deep reinforcement NEURAL system always resolved male pronouns to those occupations and never female pronouns, showing complete gender bias despite using modern neural architecture.\",\n      \"visual_anchor\": \"multiple green triangles and other markers clustered at y=-100\",\n      \"text_evidence\": \"We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue dots and regression line representing STAT system",
        "Green triangles and regression line representing RULE system",
        "Red diamonds and regression line representing NEURAL system",
        "Horizontal black dashed line at y=0",
        "X-axis showing % Female by Occupation (0-100)",
        "Y-axis showing Gendered Pronoun Resolution (-100 to 100)",
        "95% confidence interval bands around regression lines",
        "Data points scattered across occupations"
      ],
      "queries": [
        {
          "query": "Why does the green RULE regression line cross y=0 near 50% female, given the evaluation set is gender-balanced?",
          "answer": "The horizontal dotted black line at y=0 represents a hypothetical system with 100% accuracy on the gender-balanced Winogender set. The green RULE line crossing y=0 at 50% female indicates that RULE only achieves unbiased resolution when occupations have equal gender representation in labor statistics.",
          "visual_anchor": "green regression line crossing horizontal black dashed line at y=0 around x=50",
          "text_evidence": "Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line (y=0) in both plots represents a hypothetical system with 100% accuracy.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the three regression lines with 95% confidence intervals reveal varying degrees of gender bias across system architectures?",
          "answer": "Yes, the three regression lines (blue STAT, green RULE, red NEURAL) show different slopes and intercepts, with RULE showing the steepest positive slope and NEURAL showing the flattest. This visual pattern directly confirms that all three systems exhibit varying degrees of gender bias as measured by the Winogender schemas.",
          "visual_anchor": "three colored regression lines with different slopes and 95% confidence interval bands",
          "text_evidence": "By multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems.",
          "query_type": "value_context"
        },
        {
          "query": "Why do several occupations cluster at -100 on the y-axis despite testing the deep reinforcement neural system?",
          "answer": "The clustering at -100 (maximum male bias) indicates that for certain occupations, the Clark and Manning (2016a) deep reinforcement NEURAL system always resolved male pronouns to those occupations and never female pronouns, showing complete gender bias despite using modern neural architecture.",
          "visual_anchor": "multiple green triangles and other markers clustered at y=-100",
          "text_evidence": "We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green RULE regression line cross y=0 near 50% female, given the evaluation set is gender-balanced?",
        "answer": "The horizontal dotted black line at y=0 represents a hypothetical system with 100% accuracy on the gender-balanced Winogender set. The green RULE line crossing y=0 at 50% female indicates that RULE only achieves unbiased resolution when occupations have equal gender representation in labor statistics.",
        "visual_anchor": "green regression line crossing horizontal black dashed line at y=0 around x=50",
        "text_evidence": "Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line (y=0) in both plots represents a hypothetical system with 100% accuracy.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the three regression lines with 95% confidence intervals reveal varying degrees of gender bias across system architectures?",
        "answer": "Yes, the three regression lines (blue STAT, green RULE, red NEURAL) show different slopes and intercepts, with RULE showing the steepest positive slope and NEURAL showing the flattest. This visual pattern directly confirms that all three systems exhibit varying degrees of gender bias as measured by the Winogender schemas.",
        "visual_anchor": "three colored regression lines with different slopes and 95% confidence interval bands",
        "text_evidence": "By multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems.",
        "query_type": "value_context"
      },
      {
        "query": "Why do several occupations cluster at -100 on the y-axis despite testing the deep reinforcement neural system?",
        "answer": "The clustering at -100 (maximum male bias) indicates that for certain occupations, the Clark and Manning (2016a) deep reinforcement NEURAL system always resolved male pronouns to those occupations and never female pronouns, showing complete gender bias despite using modern neural architecture.",
        "visual_anchor": "multiple green triangles and other markers clustered at y=-100",
        "text_evidence": "We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue dots and regression line representing STAT system",
      "Green triangles and regression line representing RULE system",
      "Red diamonds and regression line representing NEURAL system",
      "Horizontal black dashed line at y=0",
      "X-axis showing % Female by Occupation (0-100)",
      "Y-axis showing Gendered Pronoun Resolution (-100 to 100)",
      "95% confidence interval bands around regression lines",
      "Data points scattered across occupations"
    ]
  },
  {
    "doc_id": "1805.03094",
    "figure_id": "1805.03094_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg",
    "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Multiple colored curves (blue, purple, pink, red) showing declining trends\",\n    \"Horizontal dashed lines at different probability levels\",\n    \"X-axis labeled 'Answer Position' ranging from 0 to 20\",\n    \"Y-axis labeled 'Acceptance Probability' ranging from 0.1 to 0.6\",\n    \"Error bars shown on data points\",\n    \"Different marker shapes (triangles, circles, squares, diamonds) for different curves\",\n    \"Red curve at top showing highest acceptance probability around 0.45-0.55\",\n    \"Blue curve at bottom showing lowest acceptance probability around 0.2-0.25\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red topmost curve maintain acceptance probability near 0.45 while blue drops below 0.25 across answer positions?\",\n      \"answer\": \"The horizontal bands correspond to different subgroups conditioned on total number of answers the user has written, showing performance variation across experience levels as answer position increases.\",\n      \"visual_anchor\": \"red topmost curve at ~0.45 probability versus blue bottom curve at ~0.25 probability\",\n      \"text_evidence\": \"the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the declining trends from position 0 to 20 in all colored curves support disaggregation by answer count?\",\n      \"answer\": \"Yes, the visualization shows disaggregated data with each curve representing a different subgroup based on number of answers written, revealing distinct declining performance patterns.\",\n      \"visual_anchor\": \"declining trends in all curves from answer position 0 to 20\",\n      \"text_evidence\": \"Figure 1 visualizes the data, disaggregated on the number of answers\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the 95% confidence interval error bars increase at higher answer positions around 15-20?\",\n      \"answer\": \"The outcome becomes noisy when there are few samples at higher answer positions, causing larger confidence intervals despite the stated 95% level.\",\n      \"visual_anchor\": \"error bars increasing in size at answer positions 15-20\",\n      \"text_evidence\": \"Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show 95% confidence interval\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Multiple colored curves (blue, purple, pink, red) showing declining trends",
        "Horizontal dashed lines at different probability levels",
        "X-axis labeled 'Answer Position' ranging from 0 to 20",
        "Y-axis labeled 'Acceptance Probability' ranging from 0.1 to 0.6",
        "Error bars shown on data points",
        "Different marker shapes (triangles, circles, squares, diamonds) for different curves",
        "Red curve at top showing highest acceptance probability around 0.45-0.55",
        "Blue curve at bottom showing lowest acceptance probability around 0.2-0.25"
      ],
      "queries": [
        {
          "query": "Why does the red topmost curve maintain acceptance probability near 0.45 while blue drops below 0.25 across answer positions?",
          "answer": "The horizontal bands correspond to different subgroups conditioned on total number of answers the user has written, showing performance variation across experience levels as answer position increases.",
          "visual_anchor": "red topmost curve at ~0.45 probability versus blue bottom curve at ~0.25 probability",
          "text_evidence": "the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the declining trends from position 0 to 20 in all colored curves support disaggregation by answer count?",
          "answer": "Yes, the visualization shows disaggregated data with each curve representing a different subgroup based on number of answers written, revealing distinct declining performance patterns.",
          "visual_anchor": "declining trends in all curves from answer position 0 to 20",
          "text_evidence": "Figure 1 visualizes the data, disaggregated on the number of answers",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the 95% confidence interval error bars increase at higher answer positions around 15-20?",
          "answer": "The outcome becomes noisy when there are few samples at higher answer positions, causing larger confidence intervals despite the stated 95% level.",
          "visual_anchor": "error bars increasing in size at answer positions 15-20",
          "text_evidence": "Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show 95% confidence interval",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red topmost curve maintain acceptance probability near 0.45 while blue drops below 0.25 across answer positions?",
        "answer": "The horizontal bands correspond to different subgroups conditioned on total number of answers the user has written, showing performance variation across experience levels as answer position increases.",
        "visual_anchor": "red topmost curve at ~0.45 probability versus blue bottom curve at ~0.25 probability",
        "text_evidence": "the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the declining trends from position 0 to 20 in all colored curves support disaggregation by answer count?",
        "answer": "Yes, the visualization shows disaggregated data with each curve representing a different subgroup based on number of answers written, revealing distinct declining performance patterns.",
        "visual_anchor": "declining trends in all curves from answer position 0 to 20",
        "text_evidence": "Figure 1 visualizes the data, disaggregated on the number of answers",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the 95% confidence interval error bars increase at higher answer positions around 15-20?",
        "answer": "The outcome becomes noisy when there are few samples at higher answer positions, causing larger confidence intervals despite the stated 95% level.",
        "visual_anchor": "error bars increasing in size at answer positions 15-20",
        "text_evidence": "Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show 95% confidence interval",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Multiple colored curves (blue, purple, pink, red) showing declining trends",
      "Horizontal dashed lines at different probability levels",
      "X-axis labeled 'Answer Position' ranging from 0 to 20",
      "Y-axis labeled 'Acceptance Probability' ranging from 0.1 to 0.6",
      "Error bars shown on data points",
      "Different marker shapes (triangles, circles, squares, diamonds) for different curves",
      "Red curve at top showing highest acceptance probability around 0.45-0.55",
      "Blue curve at bottom showing lowest acceptance probability around 0.2-0.25"
    ]
  },
  {
    "doc_id": "1805.03094",
    "figure_id": "1805.03094_fig_6",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg",
    "caption": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Multiple colored trend lines (red, purple, blue) with confidence intervals\",\n    \"Y-axis showing Acceptance Probability from 0.1 to 0.6\",\n    \"X-axis showing Answer Position from 0 to 20\",\n    \"Dashed trend lines overlaying data points\",\n    \"Red line at highest acceptance probability around 0.5\",\n    \"Blue line at lowest acceptance probability around 0.2\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue line remain near 0.2 acceptance probability across all answer positions when user reputation categories exist?\",\n      \"answer\": \"The blue line represents users with the lowest reputation category. Lower reputation users experience consistently lower acceptance probabilities regardless of answer position, showing reputation-based stratification in the disaggregated Stack Exchange data.\",\n      \"visual_anchor\": \"Blue line with data points consistently around 0.2 acceptance probability\",\n      \"text_evidence\": \"Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red line's maintenance near 0.5 acceptance probability through position 20 align with high-reputation user behavior patterns?\",\n      \"answer\": \"Yes, the red line represents the highest reputation users who maintain approximately 0.5 acceptance probability across all answer positions, demonstrating that high reputation provides consistent acceptance advantages independent of answer timing.\",\n      \"visual_anchor\": \"Red line with error bars maintaining around 0.5 acceptance probability from position 0 to 20\",\n      \"text_evidence\": \"The heat map shows acceptance probability as a function of its answer position within a session. The trends in (c) disaggregated data and (d) aggregate data.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the sparse confidence intervals at later answer positions despite the platform's gamified learning environment?\",\n      \"answer\": \"The sparse confidence intervals reflect fewer data samples in those bins, as noted in the caption. This occurs because fewer answers reach later positions in a session, making outcomes noisier regardless of Duolingo's gamification features.\",\n      \"visual_anchor\": \"Wider error bars and vertical lines extending from data points at answer positions 15-20\",\n      \"text_evidence\": \"Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Multiple colored trend lines (red, purple, blue) with confidence intervals",
        "Y-axis showing Acceptance Probability from 0.1 to 0.6",
        "X-axis showing Answer Position from 0 to 20",
        "Dashed trend lines overlaying data points",
        "Red line at highest acceptance probability around 0.5",
        "Blue line at lowest acceptance probability around 0.2"
      ],
      "queries": [
        {
          "query": "Why does the blue line remain near 0.2 acceptance probability across all answer positions when user reputation categories exist?",
          "answer": "The blue line represents users with the lowest reputation category. Lower reputation users experience consistently lower acceptance probabilities regardless of answer position, showing reputation-based stratification in the disaggregated Stack Exchange data.",
          "visual_anchor": "Blue line with data points consistently around 0.2 acceptance probability",
          "text_evidence": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red line's maintenance near 0.5 acceptance probability through position 20 align with high-reputation user behavior patterns?",
          "answer": "Yes, the red line represents the highest reputation users who maintain approximately 0.5 acceptance probability across all answer positions, demonstrating that high reputation provides consistent acceptance advantages independent of answer timing.",
          "visual_anchor": "Red line with error bars maintaining around 0.5 acceptance probability from position 0 to 20",
          "text_evidence": "The heat map shows acceptance probability as a function of its answer position within a session. The trends in (c) disaggregated data and (d) aggregate data.",
          "query_type": "value_context"
        },
        {
          "query": "What explains the sparse confidence intervals at later answer positions despite the platform's gamified learning environment?",
          "answer": "The sparse confidence intervals reflect fewer data samples in those bins, as noted in the caption. This occurs because fewer answers reach later positions in a session, making outcomes noisier regardless of Duolingo's gamification features.",
          "visual_anchor": "Wider error bars and vertical lines extending from data points at answer positions 15-20",
          "text_evidence": "Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue line remain near 0.2 acceptance probability across all answer positions when user reputation categories exist?",
        "answer": "The blue line represents users with the lowest reputation category. Lower reputation users experience consistently lower acceptance probabilities regardless of answer position, showing reputation-based stratification in the disaggregated Stack Exchange data.",
        "visual_anchor": "Blue line with data points consistently around 0.2 acceptance probability",
        "text_evidence": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red line's maintenance near 0.5 acceptance probability through position 20 align with high-reputation user behavior patterns?",
        "answer": "Yes, the red line represents the highest reputation users who maintain approximately 0.5 acceptance probability across all answer positions, demonstrating that high reputation provides consistent acceptance advantages independent of answer timing.",
        "visual_anchor": "Red line with error bars maintaining around 0.5 acceptance probability from position 0 to 20",
        "text_evidence": "The heat map shows acceptance probability as a function of its answer position within a session. The trends in (c) disaggregated data and (d) aggregate data.",
        "query_type": "value_context"
      },
      {
        "query": "What explains the sparse confidence intervals at later answer positions despite the platform's gamified learning environment?",
        "answer": "The sparse confidence intervals reflect fewer data samples in those bins, as noted in the caption. This occurs because fewer answers reach later positions in a session, making outcomes noisier regardless of Duolingo's gamification features.",
        "visual_anchor": "Wider error bars and vertical lines extending from data points at answer positions 15-20",
        "text_evidence": "Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Multiple colored trend lines (red, purple, blue) with confidence intervals",
      "Y-axis showing Acceptance Probability from 0.1 to 0.6",
      "X-axis showing Answer Position from 0 to 20",
      "Dashed trend lines overlaying data points",
      "Red line at highest acceptance probability around 0.5",
      "Blue line at lowest acceptance probability around 0.2"
    ]
  },
  {
    "doc_id": "1805.03094",
    "figure_id": "1805.03094_fig_13",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig12.jpg",
    "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"heatmap with color gradient from blue (0.2) to red (1.0)\", \"x-axis labeled 'Month' (0-12)\", \"y-axis labeled 'Five first attempt' (0-5)\", \"color bar showing performance scale\", \"varying intensity patterns across months and attempt numbers\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the red region at month 5 for five first attempt 5 align with lesson index as a performance feature?\",\n      \"answer\": \"Yes, the high performance (red, ~0.9-1.0) at month 5 for five first attempt 5 corresponds with the use of lesson index among all lessons for this user as one of the more than two dozen features describing performance.\",\n      \"visual_anchor\": \"red region at month 5, five first attempt value 5\",\n      \"text_evidence\": \"These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does performance shift from blue at month 6 to lighter colors at later months when tracking first attempts?\",\n      \"answer\": \"The shift from blue (low performance, ~0.2) at month 6 to lighter colors reflects temporal learning patterns captured by features like time to next lesson and time since previous lesson, which track user engagement over time.\",\n      \"visual_anchor\": \"blue region at month 6 transitioning to pink/red at months 8-12\",\n      \"text_evidence\": \"lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the consistently high red performance for five first attempts 4-5 despite session position variability?\",\n      \"answer\": \"The consistently high performance (red, 0.8-1.0) for five first attempts 4-5 suggests that completing more initial lessons, tracked as a user-related feature, strongly predicts success regardless of within-session positioning.\",\n      \"visual_anchor\": \"red horizontal bands at y-values 4 and 5 across most months\",\n      \"text_evidence\": \"User-related features include the number of five first lessons\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "heatmap with color gradient from blue (0.2) to red (1.0)",
        "x-axis labeled 'Month' (0-12)",
        "y-axis labeled 'Five first attempt' (0-5)",
        "color bar showing performance scale",
        "varying intensity patterns across months and attempt numbers"
      ],
      "queries": [
        {
          "query": "Does the red region at month 5 for five first attempt 5 align with lesson index as a performance feature?",
          "answer": "Yes, the high performance (red, ~0.9-1.0) at month 5 for five first attempt 5 corresponds with the use of lesson index among all lessons for this user as one of the more than two dozen features describing performance.",
          "visual_anchor": "red region at month 5, five first attempt value 5",
          "text_evidence": "These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user",
          "query_type": "value_context"
        },
        {
          "query": "Why does performance shift from blue at month 6 to lighter colors at later months when tracking first attempts?",
          "answer": "The shift from blue (low performance, ~0.2) at month 6 to lighter colors reflects temporal learning patterns captured by features like time to next lesson and time since previous lesson, which track user engagement over time.",
          "visual_anchor": "blue region at month 6 transitioning to pink/red at months 8-12",
          "text_evidence": "lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the consistently high red performance for five first attempts 4-5 despite session position variability?",
          "answer": "The consistently high performance (red, 0.8-1.0) for five first attempts 4-5 suggests that completing more initial lessons, tracked as a user-related feature, strongly predicts success regardless of within-session positioning.",
          "visual_anchor": "red horizontal bands at y-values 4 and 5 across most months",
          "text_evidence": "User-related features include the number of five first lessons",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the red region at month 5 for five first attempt 5 align with lesson index as a performance feature?",
        "answer": "Yes, the high performance (red, ~0.9-1.0) at month 5 for five first attempt 5 corresponds with the use of lesson index among all lessons for this user as one of the more than two dozen features describing performance.",
        "visual_anchor": "red region at month 5, five first attempt value 5",
        "text_evidence": "These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user",
        "query_type": "value_context"
      },
      {
        "query": "Why does performance shift from blue at month 6 to lighter colors at later months when tracking first attempts?",
        "answer": "The shift from blue (low performance, ~0.2) at month 6 to lighter colors reflects temporal learning patterns captured by features like time to next lesson and time since previous lesson, which track user engagement over time.",
        "visual_anchor": "blue region at month 6 transitioning to pink/red at months 8-12",
        "text_evidence": "lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the consistently high red performance for five first attempts 4-5 despite session position variability?",
        "answer": "The consistently high performance (red, 0.8-1.0) for five first attempts 4-5 suggests that completing more initial lessons, tracked as a user-related feature, strongly predicts success regardless of within-session positioning.",
        "visual_anchor": "red horizontal bands at y-values 4 and 5 across most months",
        "text_evidence": "User-related features include the number of five first lessons",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "heatmap with color gradient from blue (0.2) to red (1.0)",
      "x-axis labeled 'Month' (0-12)",
      "y-axis labeled 'Five first attempt' (0-5)",
      "color bar showing performance scale",
      "varying intensity patterns across months and attempt numbers"
    ]
  },
  {
    "doc_id": "1805.03094",
    "figure_id": "1805.03094_fig_14",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg",
    "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"pink dots around 0.8 performance\", \"purple dots around 0.6 performance\", \"blue dots around 0.3-0.4 performance\", \"x-axis labeled 'Month' from 0-13\", \"y-axis labeled 'Performance' from 0.2-0.8\", \"horizontal dashed lines separating three groups\", \"error bars on data points\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the pink dots remain stable around 0.8 across all months despite lessons introducing more distinct words?\",\n      \"answer\": \"Users are conditioned on five first attempts, selecting only those who succeeded initially. However, as lessons introduce more words (after 20 new words), even these selected users can no longer maintain perfect performance.\",\n      \"visual_anchor\": \"pink dots with error bars at approximately 0.8 performance level\",\n      \"text_evidence\": \"After 20 new words are shown in a lesson, users can no longer answer all the words correctly.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the vertical separation between pink, purple, and blue performance clusters relate to lesson difficulty patterns?\",\n      \"answer\": \"The three distinct performance levels (0.8, 0.6, 0.3-0.4) show stratification based on lesson difficulty. The heat map diagonal shows perfect lessons, but as more distinct words are introduced, maintaining perfect performance becomes impossible.\",\n      \"visual_anchor\": \"three horizontally separated groups of dots at different y-axis values with dashed lines between them\",\n      \"text_evidence\": \"The trends along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the blue dots at months 6-7 dropping to 0.2 performance align with the Khan Academy disaggregation methodology?\",\n      \"answer\": \"Yes, the drop represents a specific subgroup in the disaggregated data conditioned on five first attempts. The disaggregation reveals month-dependent performance variations that would be masked in aggregated analysis.\",\n      \"visual_anchor\": \"blue dots at months 6-7 showing lowest performance around 0.2-0.3\",\n      \"text_evidence\": \"Figure 4 shows the disaggregation corresponding to the\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "pink dots around 0.8 performance",
        "purple dots around 0.6 performance",
        "blue dots around 0.3-0.4 performance",
        "x-axis labeled 'Month' from 0-13",
        "y-axis labeled 'Performance' from 0.2-0.8",
        "horizontal dashed lines separating three groups",
        "error bars on data points"
      ],
      "queries": [
        {
          "query": "Why do the pink dots remain stable around 0.8 across all months despite lessons introducing more distinct words?",
          "answer": "Users are conditioned on five first attempts, selecting only those who succeeded initially. However, as lessons introduce more words (after 20 new words), even these selected users can no longer maintain perfect performance.",
          "visual_anchor": "pink dots with error bars at approximately 0.8 performance level",
          "text_evidence": "After 20 new words are shown in a lesson, users can no longer answer all the words correctly.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the vertical separation between pink, purple, and blue performance clusters relate to lesson difficulty patterns?",
          "answer": "The three distinct performance levels (0.8, 0.6, 0.3-0.4) show stratification based on lesson difficulty. The heat map diagonal shows perfect lessons, but as more distinct words are introduced, maintaining perfect performance becomes impossible.",
          "visual_anchor": "three horizontally separated groups of dots at different y-axis values with dashed lines between them",
          "text_evidence": "The trends along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the blue dots at months 6-7 dropping to 0.2 performance align with the Khan Academy disaggregation methodology?",
          "answer": "Yes, the drop represents a specific subgroup in the disaggregated data conditioned on five first attempts. The disaggregation reveals month-dependent performance variations that would be masked in aggregated analysis.",
          "visual_anchor": "blue dots at months 6-7 showing lowest performance around 0.2-0.3",
          "text_evidence": "Figure 4 shows the disaggregation corresponding to the",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the pink dots remain stable around 0.8 across all months despite lessons introducing more distinct words?",
        "answer": "Users are conditioned on five first attempts, selecting only those who succeeded initially. However, as lessons introduce more words (after 20 new words), even these selected users can no longer maintain perfect performance.",
        "visual_anchor": "pink dots with error bars at approximately 0.8 performance level",
        "text_evidence": "After 20 new words are shown in a lesson, users can no longer answer all the words correctly.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the vertical separation between pink, purple, and blue performance clusters relate to lesson difficulty patterns?",
        "answer": "The three distinct performance levels (0.8, 0.6, 0.3-0.4) show stratification based on lesson difficulty. The heat map diagonal shows perfect lessons, but as more distinct words are introduced, maintaining perfect performance becomes impossible.",
        "visual_anchor": "three horizontally separated groups of dots at different y-axis values with dashed lines between them",
        "text_evidence": "The trends along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the blue dots at months 6-7 dropping to 0.2 performance align with the Khan Academy disaggregation methodology?",
        "answer": "Yes, the drop represents a specific subgroup in the disaggregated data conditioned on five first attempts. The disaggregation reveals month-dependent performance variations that would be masked in aggregated analysis.",
        "visual_anchor": "blue dots at months 6-7 showing lowest performance around 0.2-0.3",
        "text_evidence": "Figure 4 shows the disaggregation corresponding to the",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "pink dots around 0.8 performance",
      "purple dots around 0.6 performance",
      "blue dots around 0.3-0.4 performance",
      "x-axis labeled 'Month' from 0-13",
      "y-axis labeled 'Performance' from 0.2-0.8",
      "horizontal dashed lines separating three groups",
      "error bars on data points"
    ]
  },
  {
    "doc_id": "1805.03094",
    "figure_id": "1805.03094_fig_21",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg",
    "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"heat map with blue and red coloring\", \"x-axis: Lesson Correct\", \"y-axis: Distinct Words\", \"color scale from 0.0 (blue) to 0.7 (red)\", \"performance degradation pattern from blue to red\", \"diagonal region showing transition from blue to red\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue region transition to red near 20 distinct words, despite the diagonal showing perfect lessons?\",\n      \"answer\": \"After 20 new words are shown in a lesson, users can no longer answer all words correctly, indicating that lesson difficulty increases with more distinct words introduced.\",\n      \"visual_anchor\": \"transition from blue to red coloring around 20 distinct words on the y-axis\",\n      \"text_evidence\": \"After 20 new words are shown in a lesson, users can no longer answer all the words correctly.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the diagonal blue region with high performance values support that users answered all shown words correctly?\",\n      \"answer\": \"Yes, the diagonal blue region with performance values near 0.6-0.7 corresponds to perfect lessons where users answered all words correctly.\",\n      \"visual_anchor\": \"diagonal blue region with performance values 0.6-0.7 on the color scale\",\n      \"text_evidence\": \"along the diagonal show perfect lessons, where users answered all words they were shown correctly.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the red coloring at high distinct word counts relate to the stated impact of experience?\",\n      \"answer\": \"The red coloring indicates declining performance as distinct words increase, demonstrating that experience with more lessons (x-axis) impacts how users handle vocabulary complexity.\",\n      \"visual_anchor\": \"red coloring at high distinct word values (upper portion of heat map)\",\n      \"text_evidence\": \"Figure 5 examines the impact of experience on performance.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "heat map with blue and red coloring",
        "x-axis: Lesson Correct",
        "y-axis: Distinct Words",
        "color scale from 0.0 (blue) to 0.7 (red)",
        "performance degradation pattern from blue to red",
        "diagonal region showing transition from blue to red"
      ],
      "queries": [
        {
          "query": "Why does the blue region transition to red near 20 distinct words, despite the diagonal showing perfect lessons?",
          "answer": "After 20 new words are shown in a lesson, users can no longer answer all words correctly, indicating that lesson difficulty increases with more distinct words introduced.",
          "visual_anchor": "transition from blue to red coloring around 20 distinct words on the y-axis",
          "text_evidence": "After 20 new words are shown in a lesson, users can no longer answer all the words correctly.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the diagonal blue region with high performance values support that users answered all shown words correctly?",
          "answer": "Yes, the diagonal blue region with performance values near 0.6-0.7 corresponds to perfect lessons where users answered all words correctly.",
          "visual_anchor": "diagonal blue region with performance values 0.6-0.7 on the color scale",
          "text_evidence": "along the diagonal show perfect lessons, where users answered all words they were shown correctly.",
          "query_type": "value_context"
        },
        {
          "query": "How does the red coloring at high distinct word counts relate to the stated impact of experience?",
          "answer": "The red coloring indicates declining performance as distinct words increase, demonstrating that experience with more lessons (x-axis) impacts how users handle vocabulary complexity.",
          "visual_anchor": "red coloring at high distinct word values (upper portion of heat map)",
          "text_evidence": "Figure 5 examines the impact of experience on performance.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue region transition to red near 20 distinct words, despite the diagonal showing perfect lessons?",
        "answer": "After 20 new words are shown in a lesson, users can no longer answer all words correctly, indicating that lesson difficulty increases with more distinct words introduced.",
        "visual_anchor": "transition from blue to red coloring around 20 distinct words on the y-axis",
        "text_evidence": "After 20 new words are shown in a lesson, users can no longer answer all the words correctly.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the diagonal blue region with high performance values support that users answered all shown words correctly?",
        "answer": "Yes, the diagonal blue region with performance values near 0.6-0.7 corresponds to perfect lessons where users answered all words correctly.",
        "visual_anchor": "diagonal blue region with performance values 0.6-0.7 on the color scale",
        "text_evidence": "along the diagonal show perfect lessons, where users answered all words they were shown correctly.",
        "query_type": "value_context"
      },
      {
        "query": "How does the red coloring at high distinct word counts relate to the stated impact of experience?",
        "answer": "The red coloring indicates declining performance as distinct words increase, demonstrating that experience with more lessons (x-axis) impacts how users handle vocabulary complexity.",
        "visual_anchor": "red coloring at high distinct word values (upper portion of heat map)",
        "text_evidence": "Figure 5 examines the impact of experience on performance.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "heat map with blue and red coloring",
      "x-axis: Lesson Correct",
      "y-axis: Distinct Words",
      "color scale from 0.0 (blue) to 0.7 (red)",
      "performance degradation pattern from blue to red",
      "diagonal region showing transition from blue to red"
    ]
  },
  {
    "doc_id": "1805.03094",
    "figure_id": "1805.03094_fig_22",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
    "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"heatmap with green gradient showing performance\", \"x-axis: Lesson Correct (10^0 to 10^2)\", \"y-axis: Distinct Words (10^0 to 10^1)\", \"color scale from yellow (~10^1) to dark green (~10^4)\", \"diagonal bright green band from lower-left to upper-right\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the dark green diagonal band in the heatmap reflect users who answer all words correctly?\",\n      \"answer\": \"Yes, the dark green diagonal band represents high performance, specifically the probability to answer all words correctly in a lesson, as shown by the count scale reaching 10^4.\",\n      \"visual_anchor\": \"dark green diagonal band from lower-left to upper-right with count values ~10^3 to 10^4\",\n      \"text_evidence\": \"The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does performance remain high along the diagonal green band despite increasing distinct words from 10^0 to 10^1?\",\n      \"answer\": \"The diagonal pattern suggests that when lesson correct count scales proportionally with distinct words, users maintain high probability of answering all words correctly, indicating balanced difficulty progression.\",\n      \"visual_anchor\": \"diagonal green band maintaining dark green color (~10^3 count) as both axes increase proportionally\",\n      \"text_evidence\": \"Another disaggregation of DL data is shown in Figure 6.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the yellow region at high Lesson Correct values support that initial performance differentiates user subgroups?\",\n      \"answer\": \"Yes, the yellow low-count region at high Lesson Correct values indicates fewer users achieve very high correctness, which aligns with initial performance separating high from low performers over time.\",\n      \"visual_anchor\": \"yellow region (~10^1 count) at Lesson Correct > 10^1.5\",\n      \"text_evidence\": \"Those users who were initially high performers appear to be different from the low performers, especially when looking at how their performance changes over time.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "heatmap with green gradient showing performance",
        "x-axis: Lesson Correct (10^0 to 10^2)",
        "y-axis: Distinct Words (10^0 to 10^1)",
        "color scale from yellow (~10^1) to dark green (~10^4)",
        "diagonal bright green band from lower-left to upper-right"
      ],
      "queries": [
        {
          "query": "Does the dark green diagonal band in the heatmap reflect users who answer all words correctly?",
          "answer": "Yes, the dark green diagonal band represents high performance, specifically the probability to answer all words correctly in a lesson, as shown by the count scale reaching 10^4.",
          "visual_anchor": "dark green diagonal band from lower-left to upper-right with count values ~10^3 to 10^4",
          "text_evidence": "The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson.",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does performance remain high along the diagonal green band despite increasing distinct words from 10^0 to 10^1?",
          "answer": "The diagonal pattern suggests that when lesson correct count scales proportionally with distinct words, users maintain high probability of answering all words correctly, indicating balanced difficulty progression.",
          "visual_anchor": "diagonal green band maintaining dark green color (~10^3 count) as both axes increase proportionally",
          "text_evidence": "Another disaggregation of DL data is shown in Figure 6.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the yellow region at high Lesson Correct values support that initial performance differentiates user subgroups?",
          "answer": "Yes, the yellow low-count region at high Lesson Correct values indicates fewer users achieve very high correctness, which aligns with initial performance separating high from low performers over time.",
          "visual_anchor": "yellow region (~10^1 count) at Lesson Correct > 10^1.5",
          "text_evidence": "Those users who were initially high performers appear to be different from the low performers, especially when looking at how their performance changes over time.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the dark green diagonal band in the heatmap reflect users who answer all words correctly?",
        "answer": "Yes, the dark green diagonal band represents high performance, specifically the probability to answer all words correctly in a lesson, as shown by the count scale reaching 10^4.",
        "visual_anchor": "dark green diagonal band from lower-left to upper-right with count values ~10^3 to 10^4",
        "text_evidence": "The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson.",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does performance remain high along the diagonal green band despite increasing distinct words from 10^0 to 10^1?",
        "answer": "The diagonal pattern suggests that when lesson correct count scales proportionally with distinct words, users maintain high probability of answering all words correctly, indicating balanced difficulty progression.",
        "visual_anchor": "diagonal green band maintaining dark green color (~10^3 count) as both axes increase proportionally",
        "text_evidence": "Another disaggregation of DL data is shown in Figure 6.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the yellow region at high Lesson Correct values support that initial performance differentiates user subgroups?",
        "answer": "Yes, the yellow low-count region at high Lesson Correct values indicates fewer users achieve very high correctness, which aligns with initial performance separating high from low performers over time.",
        "visual_anchor": "yellow region (~10^1 count) at Lesson Correct > 10^1.5",
        "text_evidence": "Those users who were initially high performers appear to be different from the low performers, especially when looking at how their performance changes over time.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "heatmap with green gradient showing performance",
      "x-axis: Lesson Correct (10^0 to 10^2)",
      "y-axis: Distinct Words (10^0 to 10^1)",
      "color scale from yellow (~10^1) to dark green (~10^4)",
      "diagonal bright green band from lower-left to upper-right"
    ]
  },
  {
    "doc_id": "1805.03677",
    "figure_id": "1805.03677_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg",
    "caption": "Figure 1.​ Model Development Pipeline",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Dataset icon with atomic orbital design on left\",\n    \"Dataset Preprocessing stage with wrench and screwdriver tools\",\n    \"Red arrow pointing from Dataset to Dataset Preprocessing with label 'Interrogating data quality & generating nutrition label'\",\n    \"Model Development stage showing neural network structure\",\n    \"Deployment stage with cloud and chip icon\",\n    \"Curved feedback arrow from Model Development back to Dataset\",\n    \"Data Pipeline label at bottom\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red arrow emphasize preprocessing when the pipeline begins with a question or goal?\",\n      \"answer\": \"The red arrow highlights the Dataset Nutrition Label's role in lowering barriers to standardized data analysis before model development, addressing the costly and non-standardized nature of current data analysis methods.\",\n      \"visual_anchor\": \"red arrow pointing from Dataset to Dataset Preprocessing with text 'Interrogating data quality & generating nutrition label'\",\n      \"text_evidence\": \"Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label (the Label) is a diagnostic framework that1 lowers the barrier to standardized data analysis\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the curved arrow returning to Dataset address the timing issue of models coming under scrutiny?\",\n      \"answer\": \"Yes, the feedback arrow enables iterative data refinement throughout the pipeline, allowing scrutiny to occur before deployment rather than only after models are built, trained, and deployed.\",\n      \"visual_anchor\": \"curved feedback arrow from Model Development stage back to Dataset icon\",\n      \"text_evidence\": \"Models often come under scrutiny only after they are built, trained, and deployed.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the wrench and screwdriver icons in Dataset Preprocessing relate to the Label's extensible design?\",\n      \"answer\": \"The tools symbolize the modular construction approach of the Label, which is designed in an extensible fashion with multiple distinct components referred to as modules.\",\n      \"visual_anchor\": \"wrench and screwdriver icons in the Dataset Preprocessing stage box\",\n      \"text_evidence\": \"The Label is designed in an extensible fashion with multiple distinct components that we refer to as \\\"modules\\\"\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Dataset icon with atomic orbital design on left",
        "Dataset Preprocessing stage with wrench and screwdriver tools",
        "Red arrow pointing from Dataset to Dataset Preprocessing with label 'Interrogating data quality & generating nutrition label'",
        "Model Development stage showing neural network structure",
        "Deployment stage with cloud and chip icon",
        "Curved feedback arrow from Model Development back to Dataset",
        "Data Pipeline label at bottom"
      ],
      "queries": [
        {
          "query": "Why does the red arrow emphasize preprocessing when the pipeline begins with a question or goal?",
          "answer": "The red arrow highlights the Dataset Nutrition Label's role in lowering barriers to standardized data analysis before model development, addressing the costly and non-standardized nature of current data analysis methods.",
          "visual_anchor": "red arrow pointing from Dataset to Dataset Preprocessing with text 'Interrogating data quality & generating nutrition label'",
          "text_evidence": "Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label (the Label) is a diagnostic framework that1 lowers the barrier to standardized data analysis",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the curved arrow returning to Dataset address the timing issue of models coming under scrutiny?",
          "answer": "Yes, the feedback arrow enables iterative data refinement throughout the pipeline, allowing scrutiny to occur before deployment rather than only after models are built, trained, and deployed.",
          "visual_anchor": "curved feedback arrow from Model Development stage back to Dataset icon",
          "text_evidence": "Models often come under scrutiny only after they are built, trained, and deployed.",
          "query_type": "value_context"
        },
        {
          "query": "How do the wrench and screwdriver icons in Dataset Preprocessing relate to the Label's extensible design?",
          "answer": "The tools symbolize the modular construction approach of the Label, which is designed in an extensible fashion with multiple distinct components referred to as modules.",
          "visual_anchor": "wrench and screwdriver icons in the Dataset Preprocessing stage box",
          "text_evidence": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as \"modules\"",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red arrow emphasize preprocessing when the pipeline begins with a question or goal?",
        "answer": "The red arrow highlights the Dataset Nutrition Label's role in lowering barriers to standardized data analysis before model development, addressing the costly and non-standardized nature of current data analysis methods.",
        "visual_anchor": "red arrow pointing from Dataset to Dataset Preprocessing with text 'Interrogating data quality & generating nutrition label'",
        "text_evidence": "Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label (the Label) is a diagnostic framework that1 lowers the barrier to standardized data analysis",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the curved arrow returning to Dataset address the timing issue of models coming under scrutiny?",
        "answer": "Yes, the feedback arrow enables iterative data refinement throughout the pipeline, allowing scrutiny to occur before deployment rather than only after models are built, trained, and deployed.",
        "visual_anchor": "curved feedback arrow from Model Development stage back to Dataset icon",
        "text_evidence": "Models often come under scrutiny only after they are built, trained, and deployed.",
        "query_type": "value_context"
      },
      {
        "query": "How do the wrench and screwdriver icons in Dataset Preprocessing relate to the Label's extensible design?",
        "answer": "The tools symbolize the modular construction approach of the Label, which is designed in an extensible fashion with multiple distinct components referred to as modules.",
        "visual_anchor": "wrench and screwdriver icons in the Dataset Preprocessing stage box",
        "text_evidence": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as \"modules\"",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Dataset icon with atomic orbital design on left",
      "Dataset Preprocessing stage with wrench and screwdriver tools",
      "Red arrow pointing from Dataset to Dataset Preprocessing with label 'Interrogating data quality & generating nutrition label'",
      "Model Development stage showing neural network structure",
      "Deployment stage with cloud and chip icon",
      "Curved feedback arrow from Model Development back to Dataset",
      "Data Pipeline label at bottom"
    ]
  },
  {
    "doc_id": "1805.03677",
    "figure_id": "1805.03677_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg",
    "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue segment showing 44.1% (15 respondents) for 'No'\",\n    \"Red segment showing 26.5% (9 respondents) for 'Yes'\",\n    \"Orange segment showing 14.7% (5 respondents) for 'Shared Practices'\",\n    \"Green segment showing 8.8% (3 respondents) for 'In Development'\",\n    \"Purple segment showing 5.9% (2 respondents) for 'Depends'\",\n    \"Center text indicating n=34 total respondents\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the 44.1% majority lacking best practices support the claim that specialists need quicker viability assessment?\",\n      \"answer\": \"Yes, the survey showing 44.1% (15 respondents) answered 'No' to having best practices demonstrates the problem that data specialists require better tools to assess AI system viability more quickly.\",\n      \"visual_anchor\": \"Blue segment at 44.1% (15 respondents) representing 'No'\",\n      \"text_evidence\": \"To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the combined 44.1% with no practices and 8.8% in development exceed those with established practices?\",\n      \"answer\": \"The survey results showing 52.9% either lacking or developing best practices provide credence to the problem that organizations struggle with data interrogation practices before building AI models.\",\n      \"visual_anchor\": \"Blue segment (44.1%) and green segment (8.8%) compared to red segment (26.5%)\",\n      \"text_evidence\": \"We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the 26.5% affirmative response relate to the prototype's need for diverse information modules?\",\n      \"answer\": \"The 26.5% with existing practices suggests organizations have varied approaches, which aligns with developing prototype modules that highlight potential diversity of information to accommodate different organizational needs.\",\n      \"visual_anchor\": \"Red segment showing 26.5% (9 respondents) for 'Yes'\",\n      \"text_evidence\": \"Variability of attributes across prototype modules highlights the potential diversity of information included in a Label\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue segment showing 44.1% (15 respondents) for 'No'",
        "Red segment showing 26.5% (9 respondents) for 'Yes'",
        "Orange segment showing 14.7% (5 respondents) for 'Shared Practices'",
        "Green segment showing 8.8% (3 respondents) for 'In Development'",
        "Purple segment showing 5.9% (2 respondents) for 'Depends'",
        "Center text indicating n=34 total respondents"
      ],
      "queries": [
        {
          "query": "Does the 44.1% majority lacking best practices support the claim that specialists need quicker viability assessment?",
          "answer": "Yes, the survey showing 44.1% (15 respondents) answered 'No' to having best practices demonstrates the problem that data specialists require better tools to assess AI system viability more quickly.",
          "visual_anchor": "Blue segment at 44.1% (15 respondents) representing 'No'",
          "text_evidence": "To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability",
          "query_type": "value_context"
        },
        {
          "query": "Why do the combined 44.1% with no practices and 8.8% in development exceed those with established practices?",
          "answer": "The survey results showing 52.9% either lacking or developing best practices provide credence to the problem that organizations struggle with data interrogation practices before building AI models.",
          "visual_anchor": "Blue segment (44.1%) and green segment (8.8%) compared to red segment (26.5%)",
          "text_evidence": "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the 26.5% affirmative response relate to the prototype's need for diverse information modules?",
          "answer": "The 26.5% with existing practices suggests organizations have varied approaches, which aligns with developing prototype modules that highlight potential diversity of information to accommodate different organizational needs.",
          "visual_anchor": "Red segment showing 26.5% (9 respondents) for 'Yes'",
          "text_evidence": "Variability of attributes across prototype modules highlights the potential diversity of information included in a Label",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 44.1% majority lacking best practices support the claim that specialists need quicker viability assessment?",
        "answer": "Yes, the survey showing 44.1% (15 respondents) answered 'No' to having best practices demonstrates the problem that data specialists require better tools to assess AI system viability more quickly.",
        "visual_anchor": "Blue segment at 44.1% (15 respondents) representing 'No'",
        "text_evidence": "To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability",
        "query_type": "value_context"
      },
      {
        "query": "Why do the combined 44.1% with no practices and 8.8% in development exceed those with established practices?",
        "answer": "The survey results showing 52.9% either lacking or developing best practices provide credence to the problem that organizations struggle with data interrogation practices before building AI models.",
        "visual_anchor": "Blue segment (44.1%) and green segment (8.8%) compared to red segment (26.5%)",
        "text_evidence": "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the 26.5% affirmative response relate to the prototype's need for diverse information modules?",
        "answer": "The 26.5% with existing practices suggests organizations have varied approaches, which aligns with developing prototype modules that highlight potential diversity of information to accommodate different organizational needs.",
        "visual_anchor": "Red segment showing 26.5% (9 respondents) for 'Yes'",
        "text_evidence": "Variability of attributes across prototype modules highlights the potential diversity of information included in a Label",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue segment showing 44.1% (15 respondents) for 'No'",
      "Red segment showing 26.5% (9 respondents) for 'Yes'",
      "Orange segment showing 14.7% (5 respondents) for 'Shared Practices'",
      "Green segment showing 8.8% (3 respondents) for 'In Development'",
      "Purple segment showing 5.9% (2 respondents) for 'Depends'",
      "Center text indicating n=34 total respondents"
    ]
  },
  {
    "doc_id": "1805.03677",
    "figure_id": "1805.03677_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg",
    "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"architecture\",\n  \"visual_elements\": [\n    \"label generator with gear icon\",\n    \"label viewer with eye icon\",\n    \"label repository (cloud)\",\n    \"data host with database icon\",\n    \"data (spreadsheet)\",\n    \"blue arrows indicating label generation flow\",\n    \"red arrows indicating label retrieval flow\",\n    \"link to label component with chain icon\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the blue arrow path from data spreadsheet to label generator support processing tabular datasets under 100K rows?\",\n      \"answer\": \"The blue arrow shows data flowing directly to the label generator, which performs simple statistical analyses in the browser for small tabular datasets under 100K rows without requiring server-side processing.\",\n      \"visual_anchor\": \"blue arrow from data spreadsheet to label generator\",\n      \"text_evidence\": \"Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\\\mathrm { K }$ rows.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the architecture show both the label generator and label viewer as distinct components with separate icons?\",\n      \"answer\": \"The ecosystem comprises two main components designed separately: a label maker (generator with gear icon) and a label viewer (with eye icon), reflecting their different functional roles in creating versus displaying labels.\",\n      \"visual_anchor\": \"label generator (gear icon) and label viewer (eye icon) as separate browser windows\",\n      \"text_evidence\": \"The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​).\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How do the red arrows from label repository enable the extensible module design mentioned for different dataset types?\",\n      \"answer\": \"The red retrieval arrows allow flexible access to stored labels composed of stand-alone modules, enabling different module arrangements to be retrieved and viewed based on specific dataset requirements and available information.\",\n      \"visual_anchor\": \"red arrows showing label retrieval flow from cloud repository to viewer\",\n      \"text_evidence\": \"The Label is designed in an extensible fashion with multiple distinct components that we refer to as \\\"modules\\\" (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "architecture",
      "visual_elements": [
        "label generator with gear icon",
        "label viewer with eye icon",
        "label repository (cloud)",
        "data host with database icon",
        "data (spreadsheet)",
        "blue arrows indicating label generation flow",
        "red arrows indicating label retrieval flow",
        "link to label component with chain icon"
      ],
      "queries": [
        {
          "query": "How does the blue arrow path from data spreadsheet to label generator support processing tabular datasets under 100K rows?",
          "answer": "The blue arrow shows data flowing directly to the label generator, which performs simple statistical analyses in the browser for small tabular datasets under 100K rows without requiring server-side processing.",
          "visual_anchor": "blue arrow from data spreadsheet to label generator",
          "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the architecture show both the label generator and label viewer as distinct components with separate icons?",
          "answer": "The ecosystem comprises two main components designed separately: a label maker (generator with gear icon) and a label viewer (with eye icon), reflecting their different functional roles in creating versus displaying labels.",
          "visual_anchor": "label generator (gear icon) and label viewer (eye icon) as separate browser windows",
          "text_evidence": "The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​).",
          "query_type": "visual_definition"
        },
        {
          "query": "How do the red arrows from label repository enable the extensible module design mentioned for different dataset types?",
          "answer": "The red retrieval arrows allow flexible access to stored labels composed of stand-alone modules, enabling different module arrangements to be retrieved and viewed based on specific dataset requirements and available information.",
          "visual_anchor": "red arrows showing label retrieval flow from cloud repository to viewer",
          "text_evidence": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as \"modules\" (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the blue arrow path from data spreadsheet to label generator support processing tabular datasets under 100K rows?",
        "answer": "The blue arrow shows data flowing directly to the label generator, which performs simple statistical analyses in the browser for small tabular datasets under 100K rows without requiring server-side processing.",
        "visual_anchor": "blue arrow from data spreadsheet to label generator",
        "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the architecture show both the label generator and label viewer as distinct components with separate icons?",
        "answer": "The ecosystem comprises two main components designed separately: a label maker (generator with gear icon) and a label viewer (with eye icon), reflecting their different functional roles in creating versus displaying labels.",
        "visual_anchor": "label generator (gear icon) and label viewer (eye icon) as separate browser windows",
        "text_evidence": "The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​).",
        "query_type": "visual_definition"
      },
      {
        "query": "How do the red arrows from label repository enable the extensible module design mentioned for different dataset types?",
        "answer": "The red retrieval arrows allow flexible access to stored labels composed of stand-alone modules, enabling different module arrangements to be retrieved and viewed based on specific dataset requirements and available information.",
        "visual_anchor": "red arrows showing label retrieval flow from cloud repository to viewer",
        "text_evidence": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as \"modules\" (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "architecture",
    "visual_elements": [
      "label generator with gear icon",
      "label viewer with eye icon",
      "label repository (cloud)",
      "data host with database icon",
      "data (spreadsheet)",
      "blue arrows indicating label generation flow",
      "red arrows indicating label retrieval flow",
      "link to label component with chain icon"
    ]
  },
  {
    "doc_id": "1805.03677",
    "figure_id": "1805.03677_fig_5",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg",
    "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three panel layout with bar chart (top left), heatmap (middle left), histogram (bottom right)\",\n    \"Dropdown menus labeled 'Variable A: product_name' and 'Variable B: recipient_state'\",\n    \"Bar chart showing product names with Dicamba at ~200 and Karate at ~140\",\n    \"Heatmap showing state codes (OK, MT, IN, DE, AZ, WV, NM, VA, WA, TX, PA, AR, MS, OH, FL, RI, LA, CO, CA, GA) vs product names\",\n    \"Histogram showing recipient state distribution with CA showing ~55 peak\",\n    \"Grayscale intensity scale from 0 to 30 on right side\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why can the Pair Plot module's dropdown menus select variables for datasets under 100K rows without server-side processing?\",\n      \"answer\": \"Simple statistical analyses including histograms, distributions, and linear correlations are executed directly in the browser for datasets smaller than 100K rows, reserving server-side processing only for specialized analyses requiring additional computational power.\",\n      \"visual_anchor\": \"Dropdown menus at bottom showing 'Variable A: product_name' and 'Variable B: recipient_state'\",\n      \"text_evidence\": \"Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\\\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the CA state bar at ~55 in the histogram demonstrate viewer choice enabled by the interactive dropdown?\",\n      \"answer\": \"The Pair Plot module introduces interactivity where viewers can choose which variable pairs to compare. The histogram displays the distribution for the selected recipient_state variable, with CA showing the highest frequency around 55.\",\n      \"visual_anchor\": \"Histogram in bottom right panel showing CA bar reaching approximately 55\",\n      \"text_evidence\": \"The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What processing approach supports the grayscale heatmap intensity scale ranging from 0 to 30 for state-product correlations?\",\n      \"answer\": \"Browser-based processing handles linear correlations and distribution information for datasets under 100K rows, enabling the heatmap visualization. More sophisticated analyses requiring additional computational power would utilize server-side processing with multiple backends.\",\n      \"visual_anchor\": \"Grayscale intensity scale on right side of heatmap ranging from 0 to 30\",\n      \"text_evidence\": \"Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\\\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three panel layout with bar chart (top left), heatmap (middle left), histogram (bottom right)",
        "Dropdown menus labeled 'Variable A: product_name' and 'Variable B: recipient_state'",
        "Bar chart showing product names with Dicamba at ~200 and Karate at ~140",
        "Heatmap showing state codes (OK, MT, IN, DE, AZ, WV, NM, VA, WA, TX, PA, AR, MS, OH, FL, RI, LA, CO, CA, GA) vs product names",
        "Histogram showing recipient state distribution with CA showing ~55 peak",
        "Grayscale intensity scale from 0 to 30 on right side"
      ],
      "queries": [
        {
          "query": "Why can the Pair Plot module's dropdown menus select variables for datasets under 100K rows without server-side processing?",
          "answer": "Simple statistical analyses including histograms, distributions, and linear correlations are executed directly in the browser for datasets smaller than 100K rows, reserving server-side processing only for specialized analyses requiring additional computational power.",
          "visual_anchor": "Dropdown menus at bottom showing 'Variable A: product_name' and 'Variable B: recipient_state'",
          "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power.",
          "query_type": "value_context"
        },
        {
          "query": "How does the CA state bar at ~55 in the histogram demonstrate viewer choice enabled by the interactive dropdown?",
          "answer": "The Pair Plot module introduces interactivity where viewers can choose which variable pairs to compare. The histogram displays the distribution for the selected recipient_state variable, with CA showing the highest frequency around 55.",
          "visual_anchor": "Histogram in bottom right panel showing CA bar reaching approximately 55",
          "text_evidence": "The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What processing approach supports the grayscale heatmap intensity scale ranging from 0 to 30 for state-product correlations?",
          "answer": "Browser-based processing handles linear correlations and distribution information for datasets under 100K rows, enabling the heatmap visualization. More sophisticated analyses requiring additional computational power would utilize server-side processing with multiple backends.",
          "visual_anchor": "Grayscale intensity scale on right side of heatmap ranging from 0 to 30",
          "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why can the Pair Plot module's dropdown menus select variables for datasets under 100K rows without server-side processing?",
        "answer": "Simple statistical analyses including histograms, distributions, and linear correlations are executed directly in the browser for datasets smaller than 100K rows, reserving server-side processing only for specialized analyses requiring additional computational power.",
        "visual_anchor": "Dropdown menus at bottom showing 'Variable A: product_name' and 'Variable B: recipient_state'",
        "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power.",
        "query_type": "value_context"
      },
      {
        "query": "How does the CA state bar at ~55 in the histogram demonstrate viewer choice enabled by the interactive dropdown?",
        "answer": "The Pair Plot module introduces interactivity where viewers can choose which variable pairs to compare. The histogram displays the distribution for the selected recipient_state variable, with CA showing the highest frequency around 55.",
        "visual_anchor": "Histogram in bottom right panel showing CA bar reaching approximately 55",
        "text_evidence": "The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What processing approach supports the grayscale heatmap intensity scale ranging from 0 to 30 for state-product correlations?",
        "answer": "Browser-based processing handles linear correlations and distribution information for datasets under 100K rows, enabling the heatmap visualization. More sophisticated analyses requiring additional computational power would utilize server-side processing with multiple backends.",
        "visual_anchor": "Grayscale intensity scale on right side of heatmap ranging from 0 to 30",
        "text_evidence": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three panel layout with bar chart (top left), heatmap (middle left), histogram (bottom right)",
      "Dropdown menus labeled 'Variable A: product_name' and 'Variable B: recipient_state'",
      "Bar chart showing product names with Dicamba at ~200 and Karate at ~140",
      "Heatmap showing state codes (OK, MT, IN, DE, AZ, WV, NM, VA, WA, TX, PA, AR, MS, OH, FL, RI, LA, CO, CA, GA) vs product names",
      "Histogram showing recipient state distribution with CA showing ~55 peak",
      "Grayscale intensity scale from 0 to 30 on right side"
    ]
  },
  {
    "doc_id": "1805.03677",
    "figure_id": "1805.03677_fig_6",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg",
    "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bar chart with descending heights from left to right\",\n    \"Y-axis scale from 0 to 0.1\",\n    \"State abbreviations on X-axis (CT, NJ, TX, FL, PA, etc.)\",\n    \"CT bar at approximately 0.1, tallest bar\",\n    \"Dropdown menu showing 'eliquis' as selected variable\",\n    \"Bars progressively decrease in height toward SD on the right\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the CT bar reaching 0.1 represent synthetic data generated by the BayesDB backend?\",\n      \"answer\": \"Yes, the Probabilistic Model module attempts to generate synthetic data by utilizing the BayesDB backend, and the CT bar at 0.1 represents the highest probability in this generated distribution for Eliquis payments.\",\n      \"visual_anchor\": \"CT bar at approximately 0.1, tallest bar in the chart\",\n      \"text_evidence\": \"While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the bar height decrease from CT at 0.1 to SD near zero in this hypothetical distribution?\",\n      \"answer\": \"The descending pattern represents a hypothetical probability distribution for Eliquis payments across states, with CT having the highest probability and states like SD having minimal probability in the synthetic data model.\",\n      \"visual_anchor\": \"Descending bar heights from CT (0.1) to SD (near 0)\",\n      \"text_evidence\": \"Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the dropdown selection of eliquis determine which drug's payment distribution appears in the bars?\",\n      \"answer\": \"The dropdown menu allows users to select which drug to analyze, with 'eliquis' currently selected, causing the bars to display the hypothetical distribution specifically for Eliquis payments across different states.\",\n      \"visual_anchor\": \"Dropdown menu displaying 'eliquis' above the bar chart\",\n      \"text_evidence\": \"showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bar chart with descending heights from left to right",
        "Y-axis scale from 0 to 0.1",
        "State abbreviations on X-axis (CT, NJ, TX, FL, PA, etc.)",
        "CT bar at approximately 0.1, tallest bar",
        "Dropdown menu showing 'eliquis' as selected variable",
        "Bars progressively decrease in height toward SD on the right"
      ],
      "queries": [
        {
          "query": "Does the CT bar reaching 0.1 represent synthetic data generated by the BayesDB backend?",
          "answer": "Yes, the Probabilistic Model module attempts to generate synthetic data by utilizing the BayesDB backend, and the CT bar at 0.1 represents the highest probability in this generated distribution for Eliquis payments.",
          "visual_anchor": "CT bar at approximately 0.1, tallest bar in the chart",
          "text_evidence": "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the bar height decrease from CT at 0.1 to SD near zero in this hypothetical distribution?",
          "answer": "The descending pattern represents a hypothetical probability distribution for Eliquis payments across states, with CT having the highest probability and states like SD having minimal probability in the synthetic data model.",
          "visual_anchor": "Descending bar heights from CT (0.1) to SD (near 0)",
          "text_evidence": "Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the dropdown selection of eliquis determine which drug's payment distribution appears in the bars?",
          "answer": "The dropdown menu allows users to select which drug to analyze, with 'eliquis' currently selected, causing the bars to display the hypothetical distribution specifically for Eliquis payments across different states.",
          "visual_anchor": "Dropdown menu displaying 'eliquis' above the bar chart",
          "text_evidence": "showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the CT bar reaching 0.1 represent synthetic data generated by the BayesDB backend?",
        "answer": "Yes, the Probabilistic Model module attempts to generate synthetic data by utilizing the BayesDB backend, and the CT bar at 0.1 represents the highest probability in this generated distribution for Eliquis payments.",
        "visual_anchor": "CT bar at approximately 0.1, tallest bar in the chart",
        "text_evidence": "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the bar height decrease from CT at 0.1 to SD near zero in this hypothetical distribution?",
        "answer": "The descending pattern represents a hypothetical probability distribution for Eliquis payments across states, with CT having the highest probability and states like SD having minimal probability in the synthetic data model.",
        "visual_anchor": "Descending bar heights from CT (0.1) to SD (near 0)",
        "text_evidence": "Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the dropdown selection of eliquis determine which drug's payment distribution appears in the bars?",
        "answer": "The dropdown menu allows users to select which drug to analyze, with 'eliquis' currently selected, causing the bars to display the hypothetical distribution specifically for Eliquis payments across different states.",
        "visual_anchor": "Dropdown menu displaying 'eliquis' above the bar chart",
        "text_evidence": "showcasing a hypothetical distribution for payments made towards the drug 'Eliquis' across different states",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bar chart with descending heights from left to right",
      "Y-axis scale from 0 to 0.1",
      "State abbreviations on X-axis (CT, NJ, TX, FL, PA, etc.)",
      "CT bar at approximately 0.1, tallest bar",
      "Dropdown menu showing 'eliquis' as selected variable",
      "Bars progressively decrease in height toward SD on the right"
    ]
  },
  {
    "doc_id": "1805.03677",
    "figure_id": "1805.03677_fig_7",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg",
    "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"negative correlation heatmap with grayscale gradient from 0 to -1\",\n    \"positive correlation heatmap with grayscale gradient from 0 to 1\",\n    \"total_amount_of_payment_usdollars variable row\",\n    \"spend_per_person variable row\",\n    \"demographic variables on x-axis including Small_population, White_alone, Black_or_African_*, Asian_alone, etc.\",\n    \"Rural_fraction showing darkest correlation in bottom heatmap\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the darkest cell in the spend_per_person heatmap at Rural_fraction warrant further analysis as suggested for likely relationships?\",\n      \"answer\": \"Yes, the Ground Truth Correlation module provides initial evidence of a strong positive correlation between spend_per_person and Rural_fraction, indicating this relationship is likely and warrants further analysis.\",\n      \"visual_anchor\": \"darkest cell (correlation value ~1) in bottom heatmap at Rural_fraction column\",\n      \"text_evidence\": \"The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the module show mostly light gray cells for total_amount_of_payment_usdollars when it prompts critical questions during preprocessing?\",\n      \"answer\": \"The light gray cells indicate weak or near-zero correlations between total_amount_of_payment_usdollars and demographic variables, prompting critical questions about whether meaningful demographic relationships exist during the preprocessing phase.\",\n      \"visual_anchor\": \"light gray cells in top heatmap for total_amount_of_payment_usdollars row\",\n      \"text_evidence\": \"Overall, it prompts critical questions and interrogation in the preprocessing phase of model development.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does visualizing both positive and negative correlation heatmaps expedite decision making without sacrificing quality in model development?\",\n      \"answer\": \"The dual heatmap visualization allows data specialists to quickly identify strong correlations (dark cells) versus weak ones (light cells) across all demographic variables simultaneously, enabling faster pattern recognition while maintaining thorough data interrogation.\",\n      \"visual_anchor\": \"two separate heatmaps with grayscale gradients showing correlation strength\",\n      \"text_evidence\": \"It also expedites decision making, which saves time in the overall model development phase without sacrificing the quality or thoroughness of the data interrogation itself\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "negative correlation heatmap with grayscale gradient from 0 to -1",
        "positive correlation heatmap with grayscale gradient from 0 to 1",
        "total_amount_of_payment_usdollars variable row",
        "spend_per_person variable row",
        "demographic variables on x-axis including Small_population, White_alone, Black_or_African_*, Asian_alone, etc.",
        "Rural_fraction showing darkest correlation in bottom heatmap"
      ],
      "queries": [
        {
          "query": "Does the darkest cell in the spend_per_person heatmap at Rural_fraction warrant further analysis as suggested for likely relationships?",
          "answer": "Yes, the Ground Truth Correlation module provides initial evidence of a strong positive correlation between spend_per_person and Rural_fraction, indicating this relationship is likely and warrants further analysis.",
          "visual_anchor": "darkest cell (correlation value ~1) in bottom heatmap at Rural_fraction column",
          "text_evidence": "The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the module show mostly light gray cells for total_amount_of_payment_usdollars when it prompts critical questions during preprocessing?",
          "answer": "The light gray cells indicate weak or near-zero correlations between total_amount_of_payment_usdollars and demographic variables, prompting critical questions about whether meaningful demographic relationships exist during the preprocessing phase.",
          "visual_anchor": "light gray cells in top heatmap for total_amount_of_payment_usdollars row",
          "text_evidence": "Overall, it prompts critical questions and interrogation in the preprocessing phase of model development.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does visualizing both positive and negative correlation heatmaps expedite decision making without sacrificing quality in model development?",
          "answer": "The dual heatmap visualization allows data specialists to quickly identify strong correlations (dark cells) versus weak ones (light cells) across all demographic variables simultaneously, enabling faster pattern recognition while maintaining thorough data interrogation.",
          "visual_anchor": "two separate heatmaps with grayscale gradients showing correlation strength",
          "text_evidence": "It also expedites decision making, which saves time in the overall model development phase without sacrificing the quality or thoroughness of the data interrogation itself",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the darkest cell in the spend_per_person heatmap at Rural_fraction warrant further analysis as suggested for likely relationships?",
        "answer": "Yes, the Ground Truth Correlation module provides initial evidence of a strong positive correlation between spend_per_person and Rural_fraction, indicating this relationship is likely and warrants further analysis.",
        "visual_anchor": "darkest cell (correlation value ~1) in bottom heatmap at Rural_fraction column",
        "text_evidence": "The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the module show mostly light gray cells for total_amount_of_payment_usdollars when it prompts critical questions during preprocessing?",
        "answer": "The light gray cells indicate weak or near-zero correlations between total_amount_of_payment_usdollars and demographic variables, prompting critical questions about whether meaningful demographic relationships exist during the preprocessing phase.",
        "visual_anchor": "light gray cells in top heatmap for total_amount_of_payment_usdollars row",
        "text_evidence": "Overall, it prompts critical questions and interrogation in the preprocessing phase of model development.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does visualizing both positive and negative correlation heatmaps expedite decision making without sacrificing quality in model development?",
        "answer": "The dual heatmap visualization allows data specialists to quickly identify strong correlations (dark cells) versus weak ones (light cells) across all demographic variables simultaneously, enabling faster pattern recognition while maintaining thorough data interrogation.",
        "visual_anchor": "two separate heatmaps with grayscale gradients showing correlation strength",
        "text_evidence": "It also expedites decision making, which saves time in the overall model development phase without sacrificing the quality or thoroughness of the data interrogation itself",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "negative correlation heatmap with grayscale gradient from 0 to -1",
      "positive correlation heatmap with grayscale gradient from 0 to 1",
      "total_amount_of_payment_usdollars variable row",
      "spend_per_person variable row",
      "demographic variables on x-axis including Small_population, White_alone, Black_or_African_*, Asian_alone, etc.",
      "Rural_fraction showing darkest correlation in bottom heatmap"
    ]
  },
  {
    "doc_id": "1805.05859",
    "figure_id": "1805.05859_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg",
    "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Node labeled A with incoming arrow from U_A\",\n    \"Node labeled Z with incoming arrow from U_Z\",\n    \"Node labeled Y with incoming arrow from U_Y\",\n    \"Arrow from U_Z to Z\",\n    \"Arrow from A to Z\",\n    \"Arrow from A to Y\",\n    \"Arrow from Z to Y\",\n    \"Label (a) in top-left corner\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the arrow from A to Y in the graph represent a structural equation that gets overridden during intervention?\",\n      \"answer\": \"Yes, a perfect intervention on variable A at value v corresponds to overriding the structural equation f_i with V_i = v, which would affect the arrow from A to Y in the causal graph.\",\n      \"visual_anchor\": \"Arrow from node A to node Y\",\n      \"text_evidence\": \"This notion says that a perfect intervention on a variable V_i, at value v, corresponds to overriding f_i(·) with the equation V_i = v.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does node Z receive arrows from both U_Z and A if interventions modify remaining variable distributions?\",\n      \"answer\": \"After intervening on A, the joint distribution of remaining variables V\\\\i (including Z) is determined by the causal model. Z retains its incoming arrow from U_Z while A's influence is overridden by the intervention value.\",\n      \"visual_anchor\": \"Node Z with incoming arrows from U_Z and A\",\n      \"text_evidence\": \"Once this is done, the joint distribution of the remaining variables V_{\\\\i} ≡ V \\\\ {V_i} is given by the causal model.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Can the structural equation represented by the arrow from U_A to A be tested for correctness observationally?\",\n      \"answer\": \"No, unless structural equations depend on observed variables only, they cannot be tested for correctness without imposing other untestable assumptions. Since U_A is an unobserved background variable, this equation cannot be verified.\",\n      \"visual_anchor\": \"Arrow from U_A to node A\",\n      \"text_evidence\": \"Unless structural equations depend on observed variables only, they cannot be tested for correctness (unless other untestable assumptions are imposed).\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node labeled A with incoming arrow from U_A",
        "Node labeled Z with incoming arrow from U_Z",
        "Node labeled Y with incoming arrow from U_Y",
        "Arrow from U_Z to Z",
        "Arrow from A to Z",
        "Arrow from A to Y",
        "Arrow from Z to Y",
        "Label (a) in top-left corner"
      ],
      "queries": [
        {
          "query": "Does the arrow from A to Y in the graph represent a structural equation that gets overridden during intervention?",
          "answer": "Yes, a perfect intervention on variable A at value v corresponds to overriding the structural equation f_i with V_i = v, which would affect the arrow from A to Y in the causal graph.",
          "visual_anchor": "Arrow from node A to node Y",
          "text_evidence": "This notion says that a perfect intervention on a variable V_i, at value v, corresponds to overriding f_i(·) with the equation V_i = v.",
          "query_type": "value_context"
        },
        {
          "query": "Why does node Z receive arrows from both U_Z and A if interventions modify remaining variable distributions?",
          "answer": "After intervening on A, the joint distribution of remaining variables V\\i (including Z) is determined by the causal model. Z retains its incoming arrow from U_Z while A's influence is overridden by the intervention value.",
          "visual_anchor": "Node Z with incoming arrows from U_Z and A",
          "text_evidence": "Once this is done, the joint distribution of the remaining variables V_{\\i} ≡ V \\ {V_i} is given by the causal model.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Can the structural equation represented by the arrow from U_A to A be tested for correctness observationally?",
          "answer": "No, unless structural equations depend on observed variables only, they cannot be tested for correctness without imposing other untestable assumptions. Since U_A is an unobserved background variable, this equation cannot be verified.",
          "visual_anchor": "Arrow from U_A to node A",
          "text_evidence": "Unless structural equations depend on observed variables only, they cannot be tested for correctness (unless other untestable assumptions are imposed).",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the arrow from A to Y in the graph represent a structural equation that gets overridden during intervention?",
        "answer": "Yes, a perfect intervention on variable A at value v corresponds to overriding the structural equation f_i with V_i = v, which would affect the arrow from A to Y in the causal graph.",
        "visual_anchor": "Arrow from node A to node Y",
        "text_evidence": "This notion says that a perfect intervention on a variable V_i, at value v, corresponds to overriding f_i(·) with the equation V_i = v.",
        "query_type": "value_context"
      },
      {
        "query": "Why does node Z receive arrows from both U_Z and A if interventions modify remaining variable distributions?",
        "answer": "After intervening on A, the joint distribution of remaining variables V\\i (including Z) is determined by the causal model. Z retains its incoming arrow from U_Z while A's influence is overridden by the intervention value.",
        "visual_anchor": "Node Z with incoming arrows from U_Z and A",
        "text_evidence": "Once this is done, the joint distribution of the remaining variables V_{\\i} ≡ V \\ {V_i} is given by the causal model.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Can the structural equation represented by the arrow from U_A to A be tested for correctness observationally?",
        "answer": "No, unless structural equations depend on observed variables only, they cannot be tested for correctness without imposing other untestable assumptions. Since U_A is an unobserved background variable, this equation cannot be verified.",
        "visual_anchor": "Arrow from U_A to node A",
        "text_evidence": "Unless structural equations depend on observed variables only, they cannot be tested for correctness (unless other untestable assumptions are imposed).",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node labeled A with incoming arrow from U_A",
      "Node labeled Z with incoming arrow from U_Z",
      "Node labeled Y with incoming arrow from U_Y",
      "Arrow from U_Z to Z",
      "Arrow from A to Z",
      "Arrow from A to Y",
      "Arrow from Z to Y",
      "Label (a) in top-left corner"
    ]
  },
  {
    "doc_id": "1805.05859",
    "figure_id": "1805.05859_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg",
    "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A (protected attribute)\", \"Node X2\", \"Node X1\", \"Node Ŷ (predictor)\", \"Directed edges from A to X2, A to X1, X2 to X1, X2 to Ŷ, X1 to Ŷ\", \"Crossing edges between X2 and X1 paths\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do only some edges from A carry counterfactual values when the goal is minimizing untestable assumptions?\",\n      \"answer\": \"When minimizing untestable assumptions, explicit structural equations are avoided but directed edges and independence constraints among counterfactuals are allowed. This selective propagation prevents violations of path-specific counterfactual fairness constraints.\",\n      \"visual_anchor\": \"Subset of directed edges from node A to X1, X2, and through to Ŷ\",\n      \"text_evidence\": \"When the goal is to minimise the number of untestable assumptions, one direction is to avoid making any explicit assumptions about the structural equations of the causal model. In this line of work, assumptions about the directed edges in the causal graph and a parametric formulation for the observed distributions are allowed, as well as independence constraints among counterfactuals.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the edge from X1 to Ŷ relate to the constraint preventing predictor dependence on certain counterfactuals?\",\n      \"answer\": \"The edge from X1 to Ŷ must respect constraints where inputs to Ŷ depend only on factuals that won't violate path-specific fairness. For instance, setting Ŷ ≡ X1 would violate the constraint because it would create improper counterfactual dependencies.\",\n      \"visual_anchor\": \"Direct arrow from node X1 to node Ŷ\",\n      \"text_evidence\": \"To enforce the constraint in (9), the input to $\\\\hat { Y }$ can be made to depend only on the factuals which will not violate the above. For instance, $\\\\hat { Y } \\\\equiv X _ { 1 }$ will violate the above, as $\\\\dot { Y } ( a , X _ { 1 } ( a , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = X _ { 1 } ( a , X _ { 2 } ( a ) ) = x _ { 1 }$ while\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What role do the three variables A, X1, X2 play in demonstrating path-specific counterfactual propagation to predictor Ŷ?\",\n      \"answer\": \"The three variables A, X1, X2 form a causal structure that illustrates how counterfactual values of protected attribute A are selectively propagated through specific paths to the predictor Ŷ, representing path-specific counterfactual fairness constraints.\",\n      \"visual_anchor\": \"Nodes A, X1, X2 connected to Ŷ through multiple directed paths\",\n      \"text_evidence\": \"Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\\\hat { Y }$ .\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A (protected attribute)",
        "Node X2",
        "Node X1",
        "Node Ŷ (predictor)",
        "Directed edges from A to X2, A to X1, X2 to X1, X2 to Ŷ, X1 to Ŷ",
        "Crossing edges between X2 and X1 paths"
      ],
      "queries": [
        {
          "query": "Why do only some edges from A carry counterfactual values when the goal is minimizing untestable assumptions?",
          "answer": "When minimizing untestable assumptions, explicit structural equations are avoided but directed edges and independence constraints among counterfactuals are allowed. This selective propagation prevents violations of path-specific counterfactual fairness constraints.",
          "visual_anchor": "Subset of directed edges from node A to X1, X2, and through to Ŷ",
          "text_evidence": "When the goal is to minimise the number of untestable assumptions, one direction is to avoid making any explicit assumptions about the structural equations of the causal model. In this line of work, assumptions about the directed edges in the causal graph and a parametric formulation for the observed distributions are allowed, as well as independence constraints among counterfactuals.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the edge from X1 to Ŷ relate to the constraint preventing predictor dependence on certain counterfactuals?",
          "answer": "The edge from X1 to Ŷ must respect constraints where inputs to Ŷ depend only on factuals that won't violate path-specific fairness. For instance, setting Ŷ ≡ X1 would violate the constraint because it would create improper counterfactual dependencies.",
          "visual_anchor": "Direct arrow from node X1 to node Ŷ",
          "text_evidence": "To enforce the constraint in (9), the input to $\\hat { Y }$ can be made to depend only on the factuals which will not violate the above. For instance, $\\hat { Y } \\equiv X _ { 1 }$ will violate the above, as $\\dot { Y } ( a , X _ { 1 } ( a , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = X _ { 1 } ( a , X _ { 2 } ( a ) ) = x _ { 1 }$ while",
          "query_type": "value_context"
        },
        {
          "query": "What role do the three variables A, X1, X2 play in demonstrating path-specific counterfactual propagation to predictor Ŷ?",
          "answer": "The three variables A, X1, X2 form a causal structure that illustrates how counterfactual values of protected attribute A are selectively propagated through specific paths to the predictor Ŷ, representing path-specific counterfactual fairness constraints.",
          "visual_anchor": "Nodes A, X1, X2 connected to Ŷ through multiple directed paths",
          "text_evidence": "Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ .",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do only some edges from A carry counterfactual values when the goal is minimizing untestable assumptions?",
        "answer": "When minimizing untestable assumptions, explicit structural equations are avoided but directed edges and independence constraints among counterfactuals are allowed. This selective propagation prevents violations of path-specific counterfactual fairness constraints.",
        "visual_anchor": "Subset of directed edges from node A to X1, X2, and through to Ŷ",
        "text_evidence": "When the goal is to minimise the number of untestable assumptions, one direction is to avoid making any explicit assumptions about the structural equations of the causal model. In this line of work, assumptions about the directed edges in the causal graph and a parametric formulation for the observed distributions are allowed, as well as independence constraints among counterfactuals.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the edge from X1 to Ŷ relate to the constraint preventing predictor dependence on certain counterfactuals?",
        "answer": "The edge from X1 to Ŷ must respect constraints where inputs to Ŷ depend only on factuals that won't violate path-specific fairness. For instance, setting Ŷ ≡ X1 would violate the constraint because it would create improper counterfactual dependencies.",
        "visual_anchor": "Direct arrow from node X1 to node Ŷ",
        "text_evidence": "To enforce the constraint in (9), the input to $\\hat { Y }$ can be made to depend only on the factuals which will not violate the above. For instance, $\\hat { Y } \\equiv X _ { 1 }$ will violate the above, as $\\dot { Y } ( a , X _ { 1 } ( a , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = X _ { 1 } ( a , X _ { 2 } ( a ) ) = x _ { 1 }$ while",
        "query_type": "value_context"
      },
      {
        "query": "What role do the three variables A, X1, X2 play in demonstrating path-specific counterfactual propagation to predictor Ŷ?",
        "answer": "The three variables A, X1, X2 form a causal structure that illustrates how counterfactual values of protected attribute A are selectively propagated through specific paths to the predictor Ŷ, representing path-specific counterfactual fairness constraints.",
        "visual_anchor": "Nodes A, X1, X2 connected to Ŷ through multiple directed paths",
        "text_evidence": "Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ .",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A (protected attribute)",
      "Node X2",
      "Node X1",
      "Node Ŷ (predictor)",
      "Directed edges from A to X2, A to X1, X2 to X1, X2 to Ŷ, X1 to Ŷ",
      "Crossing edges between X2 and X1 paths"
    ]
  },
  {
    "doc_id": "1805.09458",
    "figure_id": "1805.09458_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg",
    "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three groups of bars labeled VFAE, Xie, and Proposed\",\n    \"Four colored bars per group: salmon/red (0 layers), olive/yellow-green (1 layer), cyan (2 layers), and purple (3 layers)\",\n    \"Y-axis labeled 'adversarial Loss' ranging from 0.0 to 1.0\",\n    \"Legend showing n_layers: 0, 1, 2, 3 with corresponding colors\",\n    \"VFAE group shows salmon bar at ~0.75, olive at ~0.90\",\n    \"Xie group shows salmon bar at ~0.92, olive at ~0.95\",\n    \"Proposed group shows salmon bar at ~0.72, olive at ~0.78\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the VFAE salmon bar at 0.75 adversarial loss indicate the encoder-decoder pair uses Gaussian reparameterization?\",\n      \"answer\": \"Yes, VFAE's modified loss (Eq. 18) requires learning an encoder-decoder pair q(z|x) and p(x|z,c) where the encoder q(z|x) uses the Gaussian reparameterization trick, as shown by the 0-layer result.\",\n      \"visual_anchor\": \"VFAE group salmon/red bar at approximately 0.75 adversarial loss\",\n      \"text_evidence\": \"We have two modified VAE loss (Eq. 18) and modified VIB loss (Eq. 31). In both we have to learn an encoder and decoder pair $q ( z | x )$ and $p ( x | z , c )$ . We use feed forward networks to approximate these functions. For $q ( z | x )$ we use the Gaussian reparameterization trick\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does Xie's olive bar reach 0.95 while VFAE remains at 0.90, given both methods have comparable predictive accuracy?\",\n      \"answer\": \"The methods are roughly equivalent in performance. All methods achieve comparable predictive accuracy despite differences in adversarial loss, suggesting that higher adversarial loss does not necessarily compromise prediction quality.\",\n      \"visual_anchor\": \"Xie olive bar at ~0.95 compared to VFAE olive bar at ~0.90\",\n      \"text_evidence\": \"For the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the Proposed method's purple bar at 0.78 reflect the predictor branch concatenating c onto z as features?\",\n      \"answer\": \"Yes, the Proposed method's modified VIB includes a predictor branch p(y|z) implemented with feed forward networks. The decoder p(x|z,c) concatenates c onto z as extra input features, contributing to the adversarial loss shown.\",\n      \"visual_anchor\": \"Proposed group purple bar (3 layers) at approximately 0.78 adversarial loss\",\n      \"text_evidence\": \"In the modified VIB we also have a predictor branch $p ( y | z )$ , which we also use a feed forward network to p\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three groups of bars labeled VFAE, Xie, and Proposed",
        "Four colored bars per group: salmon/red (0 layers), olive/yellow-green (1 layer), cyan (2 layers), and purple (3 layers)",
        "Y-axis labeled 'adversarial Loss' ranging from 0.0 to 1.0",
        "Legend showing n_layers: 0, 1, 2, 3 with corresponding colors",
        "VFAE group shows salmon bar at ~0.75, olive at ~0.90",
        "Xie group shows salmon bar at ~0.92, olive at ~0.95",
        "Proposed group shows salmon bar at ~0.72, olive at ~0.78"
      ],
      "queries": [
        {
          "query": "Does the VFAE salmon bar at 0.75 adversarial loss indicate the encoder-decoder pair uses Gaussian reparameterization?",
          "answer": "Yes, VFAE's modified loss (Eq. 18) requires learning an encoder-decoder pair q(z|x) and p(x|z,c) where the encoder q(z|x) uses the Gaussian reparameterization trick, as shown by the 0-layer result.",
          "visual_anchor": "VFAE group salmon/red bar at approximately 0.75 adversarial loss",
          "text_evidence": "We have two modified VAE loss (Eq. 18) and modified VIB loss (Eq. 31). In both we have to learn an encoder and decoder pair $q ( z | x )$ and $p ( x | z , c )$ . We use feed forward networks to approximate these functions. For $q ( z | x )$ we use the Gaussian reparameterization trick",
          "query_type": "value_context"
        },
        {
          "query": "Why does Xie's olive bar reach 0.95 while VFAE remains at 0.90, given both methods have comparable predictive accuracy?",
          "answer": "The methods are roughly equivalent in performance. All methods achieve comparable predictive accuracy despite differences in adversarial loss, suggesting that higher adversarial loss does not necessarily compromise prediction quality.",
          "visual_anchor": "Xie olive bar at ~0.95 compared to VFAE olive bar at ~0.90",
          "text_evidence": "For the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the Proposed method's purple bar at 0.78 reflect the predictor branch concatenating c onto z as features?",
          "answer": "Yes, the Proposed method's modified VIB includes a predictor branch p(y|z) implemented with feed forward networks. The decoder p(x|z,c) concatenates c onto z as extra input features, contributing to the adversarial loss shown.",
          "visual_anchor": "Proposed group purple bar (3 layers) at approximately 0.78 adversarial loss",
          "text_evidence": "In the modified VIB we also have a predictor branch $p ( y | z )$ , which we also use a feed forward network to p",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the VFAE salmon bar at 0.75 adversarial loss indicate the encoder-decoder pair uses Gaussian reparameterization?",
        "answer": "Yes, VFAE's modified loss (Eq. 18) requires learning an encoder-decoder pair q(z|x) and p(x|z,c) where the encoder q(z|x) uses the Gaussian reparameterization trick, as shown by the 0-layer result.",
        "visual_anchor": "VFAE group salmon/red bar at approximately 0.75 adversarial loss",
        "text_evidence": "We have two modified VAE loss (Eq. 18) and modified VIB loss (Eq. 31). In both we have to learn an encoder and decoder pair $q ( z | x )$ and $p ( x | z , c )$ . We use feed forward networks to approximate these functions. For $q ( z | x )$ we use the Gaussian reparameterization trick",
        "query_type": "value_context"
      },
      {
        "query": "Why does Xie's olive bar reach 0.95 while VFAE remains at 0.90, given both methods have comparable predictive accuracy?",
        "answer": "The methods are roughly equivalent in performance. All methods achieve comparable predictive accuracy despite differences in adversarial loss, suggesting that higher adversarial loss does not necessarily compromise prediction quality.",
        "visual_anchor": "Xie olive bar at ~0.95 compared to VFAE olive bar at ~0.90",
        "text_evidence": "For the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the Proposed method's purple bar at 0.78 reflect the predictor branch concatenating c onto z as features?",
        "answer": "Yes, the Proposed method's modified VIB includes a predictor branch p(y|z) implemented with feed forward networks. The decoder p(x|z,c) concatenates c onto z as extra input features, contributing to the adversarial loss shown.",
        "visual_anchor": "Proposed group purple bar (3 layers) at approximately 0.78 adversarial loss",
        "text_evidence": "In the modified VIB we also have a predictor branch $p ( y | z )$ , which we also use a feed forward network to p",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three groups of bars labeled VFAE, Xie, and Proposed",
      "Four colored bars per group: salmon/red (0 layers), olive/yellow-green (1 layer), cyan (2 layers), and purple (3 layers)",
      "Y-axis labeled 'adversarial Loss' ranging from 0.0 to 1.0",
      "Legend showing n_layers: 0, 1, 2, 3 with corresponding colors",
      "VFAE group shows salmon bar at ~0.75, olive at ~0.90",
      "Xie group shows salmon bar at ~0.92, olive at ~0.95",
      "Proposed group shows salmon bar at ~0.72, olive at ~0.78"
    ]
  },
  {
    "doc_id": "1805.09458",
    "figure_id": "1805.09458_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg",
    "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"t-SNE scatter plot with two clusters\",\n    \"Red points (majority class)\",\n    \"Cyan/turquoise triangular points\",\n    \"X1 axis ranging from -50 to 50\",\n    \"X2 axis ranging from -20 to 20\",\n    \"Title: VFAE Baseline\",\n    \"Overlapping distribution of red and cyan points\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the red and cyan points overlap extensively in VFAE Baseline despite covariate class separation goals?\",\n      \"answer\": \"The VFAE Baseline fails to effectively remove covariate information from the latent encodings, resulting in poor separation between the majority class (red) and minority class (cyan) in the t-SNE visualization.\",\n      \"visual_anchor\": \"overlapping red and cyan point clusters throughout the plot space\",\n      \"text_evidence\": \"t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the VFAE Baseline red majority class distribution relate to pushing stylistic information into latent space?\",\n      \"answer\": \"The VFAE Baseline shows poor separation of the red majority class from other points, suggesting it fails to effectively push non-class stylistic information into the latent space while removing covariate information as intended.\",\n      \"visual_anchor\": \"red points (majority class) mixed throughout the scatter plot\",\n      \"text_evidence\": \"which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the cyan triangular cluster distribution at X1=50 support successful unsupervised manipulation of the decoder?\",\n      \"answer\": \"No, the cyan points' scattered distribution, including the cluster at X1=50, indicates poor latent separation in VFAE Baseline, which would hinder the decoder's ability to manipulate outputs based on style as demonstrated in the unsupervised learning approach.\",\n      \"visual_anchor\": \"cyan triangular points scattered across plot, including cluster near X1=50\",\n      \"text_evidence\": \"We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "t-SNE scatter plot with two clusters",
        "Red points (majority class)",
        "Cyan/turquoise triangular points",
        "X1 axis ranging from -50 to 50",
        "X2 axis ranging from -20 to 20",
        "Title: VFAE Baseline",
        "Overlapping distribution of red and cyan points"
      ],
      "queries": [
        {
          "query": "Why do the red and cyan points overlap extensively in VFAE Baseline despite covariate class separation goals?",
          "answer": "The VFAE Baseline fails to effectively remove covariate information from the latent encodings, resulting in poor separation between the majority class (red) and minority class (cyan) in the t-SNE visualization.",
          "visual_anchor": "overlapping red and cyan point clusters throughout the plot space",
          "text_evidence": "t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the VFAE Baseline red majority class distribution relate to pushing stylistic information into latent space?",
          "answer": "The VFAE Baseline shows poor separation of the red majority class from other points, suggesting it fails to effectively push non-class stylistic information into the latent space while removing covariate information as intended.",
          "visual_anchor": "red points (majority class) mixed throughout the scatter plot",
          "text_evidence": "which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the cyan triangular cluster distribution at X1=50 support successful unsupervised manipulation of the decoder?",
          "answer": "No, the cyan points' scattered distribution, including the cluster at X1=50, indicates poor latent separation in VFAE Baseline, which would hinder the decoder's ability to manipulate outputs based on style as demonstrated in the unsupervised learning approach.",
          "visual_anchor": "cyan triangular points scattered across plot, including cluster near X1=50",
          "text_evidence": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the red and cyan points overlap extensively in VFAE Baseline despite covariate class separation goals?",
        "answer": "The VFAE Baseline fails to effectively remove covariate information from the latent encodings, resulting in poor separation between the majority class (red) and minority class (cyan) in the t-SNE visualization.",
        "visual_anchor": "overlapping red and cyan point clusters throughout the plot space",
        "text_evidence": "t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the VFAE Baseline red majority class distribution relate to pushing stylistic information into latent space?",
        "answer": "The VFAE Baseline shows poor separation of the red majority class from other points, suggesting it fails to effectively push non-class stylistic information into the latent space while removing covariate information as intended.",
        "visual_anchor": "red points (majority class) mixed throughout the scatter plot",
        "text_evidence": "which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the cyan triangular cluster distribution at X1=50 support successful unsupervised manipulation of the decoder?",
        "answer": "No, the cyan points' scattered distribution, including the cluster at X1=50, indicates poor latent separation in VFAE Baseline, which would hinder the decoder's ability to manipulate outputs based on style as demonstrated in the unsupervised learning approach.",
        "visual_anchor": "cyan triangular points scattered across plot, including cluster near X1=50",
        "text_evidence": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "t-SNE scatter plot with two clusters",
      "Red points (majority class)",
      "Cyan/turquoise triangular points",
      "X1 axis ranging from -50 to 50",
      "X2 axis ranging from -20 to 20",
      "Title: VFAE Baseline",
      "Overlapping distribution of red and cyan points"
    ]
  },
  {
    "doc_id": "1805.09458",
    "figure_id": "1805.09458_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
    "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\"Left column labeled 'Real' showing handwritten digits 9,6,6,5,4,0,7,4,0,1\", \"Right section labeled 'Generated' showing 10 rows of digits 0-9\", \"Bold row 6 showing generated digits 0123456789\", \"Handwritten style variations across generated rows\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the generated digits in row 6 maintain the bold stroke style of the real digit 0 in that row?\",\n      \"answer\": \"The encoder maps the real digit into latent space z that is invariant to digit label but captures stylistic information like stroke boldness. The decoder then generates different digits (0-9) while preserving the extracted bold stroke style from the original digit.\",\n      \"visual_anchor\": \"Bold row 6 in the figure with thick strokes in both real (left) and generated (right) digits\",\n      \"text_evidence\": \"The left column is mapped into z that is invariant to its digit label c. We then can generate an image using z and any other specified digit, c′, as show on the right.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?\",\n      \"answer\": \"Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.\",\n      \"visual_anchor\": \"Top row showing real digit 9 on left and generated sequence 0123456789 on right\",\n      \"text_evidence\": \"We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"What allows the real digit 1 in the bottom row to produce generated digits with its slanted handwriting style?\",\n      \"answer\": \"The system performs unsupervised image manipulation by separating style from class information. The encoder captures the slanted writing style in z while being invariant to the digit identity, enabling the decoder to apply this style to any target digit during generation.\",\n      \"visual_anchor\": \"Bottom row showing real slanted digit 1 on left and corresponding slanted generated digits on right\",\n      \"text_evidence\": \"We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Left column labeled 'Real' showing handwritten digits 9,6,6,5,4,0,7,4,0,1",
        "Right section labeled 'Generated' showing 10 rows of digits 0-9",
        "Bold row 6 showing generated digits 0123456789",
        "Handwritten style variations across generated rows"
      ],
      "queries": [
        {
          "query": "Why do the generated digits in row 6 maintain the bold stroke style of the real digit 0 in that row?",
          "answer": "The encoder maps the real digit into latent space z that is invariant to digit label but captures stylistic information like stroke boldness. The decoder then generates different digits (0-9) while preserving the extracted bold stroke style from the original digit.",
          "visual_anchor": "Bold row 6 in the figure with thick strokes in both real (left) and generated (right) digits",
          "text_evidence": "The left column is mapped into z that is invariant to its digit label c. We then can generate an image using z and any other specified digit, c′, as show on the right.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
          "answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
          "visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
          "text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
          "query_type": "visual_definition"
        },
        {
          "query": "What allows the real digit 1 in the bottom row to produce generated digits with its slanted handwriting style?",
          "answer": "The system performs unsupervised image manipulation by separating style from class information. The encoder captures the slanted writing style in z while being invariant to the digit identity, enabling the decoder to apply this style to any target digit during generation.",
          "visual_anchor": "Bottom row showing real slanted digit 1 on left and corresponding slanted generated digits on right",
          "text_evidence": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the generated digits in row 6 maintain the bold stroke style of the real digit 0 in that row?",
        "answer": "The encoder maps the real digit into latent space z that is invariant to digit label but captures stylistic information like stroke boldness. The decoder then generates different digits (0-9) while preserving the extracted bold stroke style from the original digit.",
        "visual_anchor": "Bold row 6 in the figure with thick strokes in both real (left) and generated (right) digits",
        "text_evidence": "The left column is mapped into z that is invariant to its digit label c. We then can generate an image using z and any other specified digit, c′, as show on the right.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does using digit label as covariate enable the leftmost real 9 to generate all ten digits rightward?",
        "answer": "Using digit label as covariate class c pushes all non-class stylistic information into latent space while removing information about the exact digit. This separation allows the decoder to recombine the extracted style with any specified digit label c′ to produce all ten digits.",
        "visual_anchor": "Top row showing real digit 9 on left and generated sequence 0123456789 on right",
        "text_evidence": "We use the digit label as the covariate class c, which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written.",
        "query_type": "visual_definition"
      },
      {
        "query": "What allows the real digit 1 in the bottom row to produce generated digits with its slanted handwriting style?",
        "answer": "The system performs unsupervised image manipulation by separating style from class information. The encoder captures the slanted writing style in z while being invariant to the digit identity, enabling the decoder to apply this style to any target digit during generation.",
        "visual_anchor": "Bottom row showing real slanted digit 1 on left and corresponding slanted generated digits on right",
        "text_evidence": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Left column labeled 'Real' showing handwritten digits 9,6,6,5,4,0,7,4,0,1",
      "Right section labeled 'Generated' showing 10 rows of digits 0-9",
      "Bold row 6 showing generated digits 0123456789",
      "Handwritten style variations across generated rows"
    ]
  },
  {
    "doc_id": "1805.11202",
    "figure_id": "1805.11202_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg",
    "caption": "Figure 1: Illustration of generative adversarial networks",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"P_Z Noise box with arrow to G Generator\",\n    \"G Generator producing G(z) output\",\n    \"P_data Real dataset box with x output\",\n    \"D Discriminator receiving inputs from both G(z) and x\",\n    \"Output labels 0: G(z) and 1: x\",\n    \"Circular feedback element with minus sign\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the discriminator receive input from both the real dataset box and generator output G(z)?\",\n      \"answer\": \"The discriminator must distinguish between real data (labeled 1) and generated synthetic data (labeled 0) to train the GAN adversarially, as it evaluates whether samples come from the real distribution P_data or the generator.\",\n      \"visual_anchor\": \"D Discriminator box with two input arrows and output labels 0: G(z) and 1: x\",\n      \"text_evidence\": \"In fairness-aware learning, the literature has studied notions of group fairness on dataset and classification\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the P_Z noise input feeding into generator G relate to the synthetic datasets SYN1-GAN through SYN4-FairGAN?\",\n      \"answer\": \"Yes, the noise P_Z is the random input that the generator G transforms into synthetic data, which corresponds to the synthetic datasets like SYN1-GAN and SYN4-FairGAN that are generated and compared against real data in the table.\",\n      \"visual_anchor\": \"P_Z Noise box connected to G Generator box\",\n      \"text_evidence\": \"Table 1: Risk differences of real and synthetic datasets with Real Data, SYN1-GAN, SYN2-NFGANI, SYN3-NFGANII, SYN4-FairGAN columns showing disk(D) values\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the discriminator's output label 1: x connect to the statistical parity fairness definition?\",\n      \"answer\": \"The label 1: x indicates real data from P_data, which is the dataset D where statistical parity is evaluated through the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0) to measure discrimination with respect to protected attributes.\",\n      \"visual_anchor\": \"Output label 1: x from discriminator\",\n      \"text_evidence\": \"The discrimination in a labeled dataset w.r.t the protected attribute s is evaluated by the risk difference: disc(D) = P(y = 1|s = 1) - P(y = 1|s = 0)\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "P_Z Noise box with arrow to G Generator",
        "G Generator producing G(z) output",
        "P_data Real dataset box with x output",
        "D Discriminator receiving inputs from both G(z) and x",
        "Output labels 0: G(z) and 1: x",
        "Circular feedback element with minus sign"
      ],
      "queries": [
        {
          "query": "Why does the discriminator receive input from both the real dataset box and generator output G(z)?",
          "answer": "The discriminator must distinguish between real data (labeled 1) and generated synthetic data (labeled 0) to train the GAN adversarially, as it evaluates whether samples come from the real distribution P_data or the generator.",
          "visual_anchor": "D Discriminator box with two input arrows and output labels 0: G(z) and 1: x",
          "text_evidence": "In fairness-aware learning, the literature has studied notions of group fairness on dataset and classification",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the P_Z noise input feeding into generator G relate to the synthetic datasets SYN1-GAN through SYN4-FairGAN?",
          "answer": "Yes, the noise P_Z is the random input that the generator G transforms into synthetic data, which corresponds to the synthetic datasets like SYN1-GAN and SYN4-FairGAN that are generated and compared against real data in the table.",
          "visual_anchor": "P_Z Noise box connected to G Generator box",
          "text_evidence": "Table 1: Risk differences of real and synthetic datasets with Real Data, SYN1-GAN, SYN2-NFGANI, SYN3-NFGANII, SYN4-FairGAN columns showing disk(D) values",
          "query_type": "value_context"
        },
        {
          "query": "How does the discriminator's output label 1: x connect to the statistical parity fairness definition?",
          "answer": "The label 1: x indicates real data from P_data, which is the dataset D where statistical parity is evaluated through the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0) to measure discrimination with respect to protected attributes.",
          "visual_anchor": "Output label 1: x from discriminator",
          "text_evidence": "The discrimination in a labeled dataset w.r.t the protected attribute s is evaluated by the risk difference: disc(D) = P(y = 1|s = 1) - P(y = 1|s = 0)",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the discriminator receive input from both the real dataset box and generator output G(z)?",
        "answer": "The discriminator must distinguish between real data (labeled 1) and generated synthetic data (labeled 0) to train the GAN adversarially, as it evaluates whether samples come from the real distribution P_data or the generator.",
        "visual_anchor": "D Discriminator box with two input arrows and output labels 0: G(z) and 1: x",
        "text_evidence": "In fairness-aware learning, the literature has studied notions of group fairness on dataset and classification",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the P_Z noise input feeding into generator G relate to the synthetic datasets SYN1-GAN through SYN4-FairGAN?",
        "answer": "Yes, the noise P_Z is the random input that the generator G transforms into synthetic data, which corresponds to the synthetic datasets like SYN1-GAN and SYN4-FairGAN that are generated and compared against real data in the table.",
        "visual_anchor": "P_Z Noise box connected to G Generator box",
        "text_evidence": "Table 1: Risk differences of real and synthetic datasets with Real Data, SYN1-GAN, SYN2-NFGANI, SYN3-NFGANII, SYN4-FairGAN columns showing disk(D) values",
        "query_type": "value_context"
      },
      {
        "query": "How does the discriminator's output label 1: x connect to the statistical parity fairness definition?",
        "answer": "The label 1: x indicates real data from P_data, which is the dataset D where statistical parity is evaluated through the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0) to measure discrimination with respect to protected attributes.",
        "visual_anchor": "Output label 1: x from discriminator",
        "text_evidence": "The discrimination in a labeled dataset w.r.t the protected attribute s is evaluated by the risk difference: disc(D) = P(y = 1|s = 1) - P(y = 1|s = 0)",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "P_Z Noise box with arrow to G Generator",
      "G Generator producing G(z) output",
      "P_data Real dataset box with x output",
      "D Discriminator receiving inputs from both G(z) and x",
      "Output labels 0: G(z) and 1: x",
      "Circular feedback element with minus sign"
    ]
  },
  {
    "doc_id": "1805.11202",
    "figure_id": "1805.11202_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg",
    "caption": "Figure 2: The Structure of FairGAN",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"architecture\",\n  \"visual_elements\": [\n    \"Two discriminators D1 and D2 at the top\",\n    \"Generator G_Dec at the bottom center\",\n    \"Protected attribute Ps box on bottom left\",\n    \"Noise Pz box on bottom right\",\n    \"P_data(x,y|s) feeding into D1\",\n    \"P_G(x,y|s) feeding into D1 and Generator\",\n    \"real: (x,y,s) and fake:(x̂,ŷ,ŝ) labels for D1\",\n    \"(x̂,ŷ|ŝ=0) and (x̂,ŷ|ŝ=1) labels for D2\",\n    \"Arrows showing data flow from Ps and Pz through Generator to discriminators\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does discriminator D1 receive both P_data and P_G inputs when its objective function uses D(·) probability?\",\n      \"answer\": \"D1 must distinguish real data from generated fake data by outputting probabilities, so it receives both P_data(x,y|s) and P_G(x,y|s) to learn this discrimination. The objective function D(·) outputs the probability that input is from real data rather than generated fake data.\",\n      \"visual_anchor\": \"D1 discriminator box with two input arrows from P_data(x,y|s) and P_G(x,y|s)\",\n      \"text_evidence\": \"where $D ( \\\\cdot )$ outputs the probability that $\\\\cdot$ is from the real data rather Dthan the generated fake data\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the Generator G_Dec fooling discriminators relate to minimizing Equation 2 for the objective function of G?\",\n      \"answer\": \"The generator G_Dec is trained to fool the discriminator by making generated data indistinguishable from real data. Minimization of Equation 2 ensures that the discriminator is fooled by G(z), achieving the objective function of G.\",\n      \"visual_anchor\": \"G_Dec Generator box with arrows pointing upward to both discriminators D1 and D2\",\n      \"text_evidence\": \"In order to make the generative distribution $P _ { G }$ close to the real data distribution $P _ { \\\\mathrm { d a t a } }$ , $G$ is trained by PG P Gfooling the discriminator unable to distinguish the generated data from the real data. Thus, the objective function of $G$ is defined as: Minimization of Equation 2 ensures that the discriminator is fooled by $G ( \\\\mathbf { z } )$ an\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the structure show G_Dec receiving hidden representations when x'=Dec(Enc(x)) captures salient input information?\",\n      \"answer\": \"The generator G_Dec consists of a generator G that produces salient representations and a decoder Dec that reconstructs data. Because the hidden representation x'=Dec(Enc(x)) can reconstruct the original input, it captures the salient information needed for generation.\",\n      \"visual_anchor\": \"G_Dec Generator box at bottom center receiving inputs from Ps and Pz\",\n      \"text_evidence\": \"where $\\\\mathbf { x } ^ { \\\\prime } = D e c ( E n c ( \\\\mathbf { x } ) )$ . Because the hidden representation can Dec Encbe used to reconstruct the original input, it captures the salient information of the input. To generate the dataset which contains discrete attributes, the generator $G _ { D e c }$ in medGAN consists of two components, the generator $G$ GDecand the decoder\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "architecture",
      "visual_elements": [
        "Two discriminators D1 and D2 at the top",
        "Generator G_Dec at the bottom center",
        "Protected attribute Ps box on bottom left",
        "Noise Pz box on bottom right",
        "P_data(x,y|s) feeding into D1",
        "P_G(x,y|s) feeding into D1 and Generator",
        "real: (x,y,s) and fake:(x̂,ŷ,ŝ) labels for D1",
        "(x̂,ŷ|ŝ=0) and (x̂,ŷ|ŝ=1) labels for D2",
        "Arrows showing data flow from Ps and Pz through Generator to discriminators"
      ],
      "queries": [
        {
          "query": "Why does discriminator D1 receive both P_data and P_G inputs when its objective function uses D(·) probability?",
          "answer": "D1 must distinguish real data from generated fake data by outputting probabilities, so it receives both P_data(x,y|s) and P_G(x,y|s) to learn this discrimination. The objective function D(·) outputs the probability that input is from real data rather than generated fake data.",
          "visual_anchor": "D1 discriminator box with two input arrows from P_data(x,y|s) and P_G(x,y|s)",
          "text_evidence": "where $D ( \\cdot )$ outputs the probability that $\\cdot$ is from the real data rather Dthan the generated fake data",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the Generator G_Dec fooling discriminators relate to minimizing Equation 2 for the objective function of G?",
          "answer": "The generator G_Dec is trained to fool the discriminator by making generated data indistinguishable from real data. Minimization of Equation 2 ensures that the discriminator is fooled by G(z), achieving the objective function of G.",
          "visual_anchor": "G_Dec Generator box with arrows pointing upward to both discriminators D1 and D2",
          "text_evidence": "In order to make the generative distribution $P _ { G }$ close to the real data distribution $P _ { \\mathrm { d a t a } }$ , $G$ is trained by PG P Gfooling the discriminator unable to distinguish the generated data from the real data. Thus, the objective function of $G$ is defined as: Minimization of Equation 2 ensures that the discriminator is fooled by $G ( \\mathbf { z } )$ an",
          "query_type": "value_context"
        },
        {
          "query": "Why does the structure show G_Dec receiving hidden representations when x'=Dec(Enc(x)) captures salient input information?",
          "answer": "The generator G_Dec consists of a generator G that produces salient representations and a decoder Dec that reconstructs data. Because the hidden representation x'=Dec(Enc(x)) can reconstruct the original input, it captures the salient information needed for generation.",
          "visual_anchor": "G_Dec Generator box at bottom center receiving inputs from Ps and Pz",
          "text_evidence": "where $\\mathbf { x } ^ { \\prime } = D e c ( E n c ( \\mathbf { x } ) )$ . Because the hidden representation can Dec Encbe used to reconstruct the original input, it captures the salient information of the input. To generate the dataset which contains discrete attributes, the generator $G _ { D e c }$ in medGAN consists of two components, the generator $G$ GDecand the decoder",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does discriminator D1 receive both P_data and P_G inputs when its objective function uses D(·) probability?",
        "answer": "D1 must distinguish real data from generated fake data by outputting probabilities, so it receives both P_data(x,y|s) and P_G(x,y|s) to learn this discrimination. The objective function D(·) outputs the probability that input is from real data rather than generated fake data.",
        "visual_anchor": "D1 discriminator box with two input arrows from P_data(x,y|s) and P_G(x,y|s)",
        "text_evidence": "where $D ( \\cdot )$ outputs the probability that $\\cdot$ is from the real data rather Dthan the generated fake data",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the Generator G_Dec fooling discriminators relate to minimizing Equation 2 for the objective function of G?",
        "answer": "The generator G_Dec is trained to fool the discriminator by making generated data indistinguishable from real data. Minimization of Equation 2 ensures that the discriminator is fooled by G(z), achieving the objective function of G.",
        "visual_anchor": "G_Dec Generator box with arrows pointing upward to both discriminators D1 and D2",
        "text_evidence": "In order to make the generative distribution $P _ { G }$ close to the real data distribution $P _ { \\mathrm { d a t a } }$ , $G$ is trained by PG P Gfooling the discriminator unable to distinguish the generated data from the real data. Thus, the objective function of $G$ is defined as: Minimization of Equation 2 ensures that the discriminator is fooled by $G ( \\mathbf { z } )$ an",
        "query_type": "value_context"
      },
      {
        "query": "Why does the structure show G_Dec receiving hidden representations when x'=Dec(Enc(x)) captures salient input information?",
        "answer": "The generator G_Dec consists of a generator G that produces salient representations and a decoder Dec that reconstructs data. Because the hidden representation x'=Dec(Enc(x)) can reconstruct the original input, it captures the salient information needed for generation.",
        "visual_anchor": "G_Dec Generator box at bottom center receiving inputs from Ps and Pz",
        "text_evidence": "where $\\mathbf { x } ^ { \\prime } = D e c ( E n c ( \\mathbf { x } ) )$ . Because the hidden representation can Dec Encbe used to reconstruct the original input, it captures the salient information of the input. To generate the dataset which contains discrete attributes, the generator $G _ { D e c }$ in medGAN consists of two components, the generator $G$ GDecand the decoder",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "architecture",
    "visual_elements": [
      "Two discriminators D1 and D2 at the top",
      "Generator G_Dec at the bottom center",
      "Protected attribute Ps box on bottom left",
      "Noise Pz box on bottom right",
      "P_data(x,y|s) feeding into D1",
      "P_G(x,y|s) feeding into D1 and Generator",
      "real: (x,y,s) and fake:(x̂,ŷ,ŝ) labels for D1",
      "(x̂,ŷ|ŝ=0) and (x̂,ŷ|ŝ=1) labels for D2",
      "Arrows showing data flow from Ps and Pz through Generator to discriminators"
    ]
  },
  {
    "doc_id": "1805.11202",
    "figure_id": "1805.11202_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
    "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Black solid curve showing P_data(x) with two peaks around x=1 and x=3.5\",\n    \"Green dashed curve showing P_data(x|s=1) with lower amplitude peaks\",\n    \"Red dashed curve showing P_data(x|s=0) with similar shape to green\",\n    \"Y-axis labeled P ranging from 0.00 to 0.10\",\n    \"X-axis ranging from -1 to 6\",\n    \"Legend in top right showing three distribution types\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?\",\n      \"answer\": \"The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.\",\n      \"visual_anchor\": \"Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05\",\n      \"text_evidence\": \"shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the green and red conditional distributions exhibit similar bimodal patterns as required for evaluating FairGAN versus NaïveFairGAN variants?\",\n      \"answer\": \"Yes, both the green (s=1) and red (s=0) conditional distributions show bimodal patterns with peaks around x=1 and x=3.5, mirroring the overall distribution structure. This toy dataset structure enables comparison of FairGAN, NaïveFairGAN-I and NaïveFairGAN-II performance.\",\n      \"visual_anchor\": \"Green and red dashed curves both showing two peaks at similar x-locations\",\n      \"text_evidence\": \"Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the red and green curve amplitudes relate to the experimental setup testing classifiers on real versus synthetic datasets?\",\n      \"answer\": \"The red (s=0) and green (s=1) curves show the real data's conditional distributions that FairGAN aims to replicate in synthetic data. The experimental setup evaluates whether classifiers trained on synthetic datasets (REAL2REAL setting) can match performance on these real conditional distributions.\",\n      \"visual_anchor\": \"Red and green dashed curves with amplitudes around 0.02-0.05\",\n      \"text_evidence\": \"We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black solid curve showing P_data(x) with two peaks around x=1 and x=3.5",
        "Green dashed curve showing P_data(x|s=1) with lower amplitude peaks",
        "Red dashed curve showing P_data(x|s=0) with similar shape to green",
        "Y-axis labeled P ranging from 0.00 to 0.10",
        "X-axis ranging from -1 to 6",
        "Legend in top right showing three distribution types"
      ],
      "queries": [
        {
          "query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?",
          "answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.",
          "visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05",
          "text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the green and red conditional distributions exhibit similar bimodal patterns as required for evaluating FairGAN versus NaïveFairGAN variants?",
          "answer": "Yes, both the green (s=1) and red (s=0) conditional distributions show bimodal patterns with peaks around x=1 and x=3.5, mirroring the overall distribution structure. This toy dataset structure enables comparison of FairGAN, NaïveFairGAN-I and NaïveFairGAN-II performance.",
          "visual_anchor": "Green and red dashed curves both showing two peaks at similar x-locations",
          "text_evidence": "Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset",
          "query_type": "value_context"
        },
        {
          "query": "How do the red and green curve amplitudes relate to the experimental setup testing classifiers on real versus synthetic datasets?",
          "answer": "The red (s=0) and green (s=1) curves show the real data's conditional distributions that FairGAN aims to replicate in synthetic data. The experimental setup evaluates whether classifiers trained on synthetic datasets (REAL2REAL setting) can match performance on these real conditional distributions.",
          "visual_anchor": "Red and green dashed curves with amplitudes around 0.02-0.05",
          "text_evidence": "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black curve reach approximately 0.10 at x≈3.5 while both conditional distributions peak below 0.05?",
        "answer": "The black curve represents the overall data distribution P_data(x), which combines both conditional distributions P_data(x|s=1) and P_data(x|s=0), resulting in higher probability density than either individual conditional distribution alone.",
        "visual_anchor": "Black solid curve peak at 0.10 versus green and red dashed curves peaking below 0.05",
        "text_evidence": "shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the green and red conditional distributions exhibit similar bimodal patterns as required for evaluating FairGAN versus NaïveFairGAN variants?",
        "answer": "Yes, both the green (s=1) and red (s=0) conditional distributions show bimodal patterns with peaks around x=1 and x=3.5, mirroring the overall distribution structure. This toy dataset structure enables comparison of FairGAN, NaïveFairGAN-I and NaïveFairGAN-II performance.",
        "visual_anchor": "Green and red dashed curves both showing two peaks at similar x-locations",
        "text_evidence": "Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset",
        "query_type": "value_context"
      },
      {
        "query": "How do the red and green curve amplitudes relate to the experimental setup testing classifiers on real versus synthetic datasets?",
        "answer": "The red (s=0) and green (s=1) curves show the real data's conditional distributions that FairGAN aims to replicate in synthetic data. The experimental setup evaluates whether classifiers trained on synthetic datasets (REAL2REAL setting) can match performance on these real conditional distributions.",
        "visual_anchor": "Red and green dashed curves with amplitudes around 0.02-0.05",
        "text_evidence": "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black solid curve showing P_data(x) with two peaks around x=1 and x=3.5",
      "Green dashed curve showing P_data(x|s=1) with lower amplitude peaks",
      "Red dashed curve showing P_data(x|s=0) with similar shape to green",
      "Y-axis labeled P ranging from 0.00 to 0.10",
      "X-axis ranging from -1 to 6",
      "Legend in top right showing three distribution types"
    ]
  },
  {
    "doc_id": "1805.11202",
    "figure_id": "1805.11202_fig_10",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg",
    "caption": "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"diagonal line\", \"scattered dots\", \"x-axis labeled P(x,y|s=1)\", \"y-axis labeled P(x,y|s=0)\", \"dots clustered near diagonal\", \"axis range 0.0 to 1.0\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the dots clustering along the diagonal line indicate that FairGAN achieves identical conditional probability distributions for fairness?\",\n      \"answer\": \"Yes, the dots closely follow the diagonal line, which represents the ideal fairness where data have identical conditional probability distributions given different sensitive attribute values s.\",\n      \"visual_anchor\": \"diagonal line and dots clustered along it\",\n      \"text_evidence\": \"The diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does each dot in the scatter plot represent one attribute when evaluating FairGAN on fair data generation?\",\n      \"answer\": \"Each dot represents the dimension-wise conditional probability for one attribute, allowing comparison between P(x,y|s=1) and P(x,y|s=0) to assess whether FairGAN generates fair data.\",\n      \"visual_anchor\": \"individual dots in scatter plot\",\n      \"text_evidence\": \"We evaluate FairGAN on data generation from two perspectives, fairness and utility. Fairness is to check whether FairGAN can generate fair data\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the scatter pattern along the diagonal support the claim that risk difference disc measures fairness performance?\",\n      \"answer\": \"The dots aligned along the diagonal show that P(y=1|s=1) and P(y=1|s=0) are nearly equal, minimizing the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0), which is the fairness metric used.\",\n      \"visual_anchor\": \"dots aligned along diagonal line from (0,0) to (1,1)\",\n      \"text_evidence\": \"We adopt the risk difference in a labeled dataset $( d i s c ( \\\\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "diagonal line",
        "scattered dots",
        "x-axis labeled P(x,y|s=1)",
        "y-axis labeled P(x,y|s=0)",
        "dots clustered near diagonal",
        "axis range 0.0 to 1.0"
      ],
      "queries": [
        {
          "query": "Do the dots clustering along the diagonal line indicate that FairGAN achieves identical conditional probability distributions for fairness?",
          "answer": "Yes, the dots closely follow the diagonal line, which represents the ideal fairness where data have identical conditional probability distributions given different sensitive attribute values s.",
          "visual_anchor": "diagonal line and dots clustered along it",
          "text_evidence": "The diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does each dot in the scatter plot represent one attribute when evaluating FairGAN on fair data generation?",
          "answer": "Each dot represents the dimension-wise conditional probability for one attribute, allowing comparison between P(x,y|s=1) and P(x,y|s=0) to assess whether FairGAN generates fair data.",
          "visual_anchor": "individual dots in scatter plot",
          "text_evidence": "We evaluate FairGAN on data generation from two perspectives, fairness and utility. Fairness is to check whether FairGAN can generate fair data",
          "query_type": "value_context"
        },
        {
          "query": "How does the scatter pattern along the diagonal support the claim that risk difference disc measures fairness performance?",
          "answer": "The dots aligned along the diagonal show that P(y=1|s=1) and P(y=1|s=0) are nearly equal, minimizing the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0), which is the fairness metric used.",
          "visual_anchor": "dots aligned along diagonal line from (0,0) to (1,1)",
          "text_evidence": "We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the dots clustering along the diagonal line indicate that FairGAN achieves identical conditional probability distributions for fairness?",
        "answer": "Yes, the dots closely follow the diagonal line, which represents the ideal fairness where data have identical conditional probability distributions given different sensitive attribute values s.",
        "visual_anchor": "diagonal line and dots clustered along it",
        "text_evidence": "The diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does each dot in the scatter plot represent one attribute when evaluating FairGAN on fair data generation?",
        "answer": "Each dot represents the dimension-wise conditional probability for one attribute, allowing comparison between P(x,y|s=1) and P(x,y|s=0) to assess whether FairGAN generates fair data.",
        "visual_anchor": "individual dots in scatter plot",
        "text_evidence": "We evaluate FairGAN on data generation from two perspectives, fairness and utility. Fairness is to check whether FairGAN can generate fair data",
        "query_type": "value_context"
      },
      {
        "query": "How does the scatter pattern along the diagonal support the claim that risk difference disc measures fairness performance?",
        "answer": "The dots aligned along the diagonal show that P(y=1|s=1) and P(y=1|s=0) are nearly equal, minimizing the risk difference disc(D) = P(y=1|s=1) - P(y=1|s=0), which is the fairness metric used.",
        "visual_anchor": "dots aligned along diagonal line from (0,0) to (1,1)",
        "text_evidence": "We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "diagonal line",
      "scattered dots",
      "x-axis labeled P(x,y|s=1)",
      "y-axis labeled P(x,y|s=0)",
      "dots clustered near diagonal",
      "axis range 0.0 to 1.0"
    ]
  },
  {
    "doc_id": "1805.11202",
    "figure_id": "1805.11202_fig_22",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg",
    "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Scatter plot with dots representing attributes\",\n    \"Diagonal dashed line from (0.0, 0.0) to (1.0, 1.0)\",\n    \"X-axis labeled P_real(x_d) ranging from 0.0 to 1.0\",\n    \"Y-axis labeled P_syn(y_d) ranging from 0.0 to 1.0\",\n    \"Blue dots clustered near the diagonal line\",\n    \"Red dots also present along the diagonal\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the clustering of dots along the diagonal line demonstrate the ideal case where real and synthetic data show identical quality?\",\n      \"answer\": \"Yes, the diagonal line indicates the ideal case where real and synthetic datasets have identical quality. The close clustering of dots along this line suggests the synthetic data successfully replicates the probability distributions of the real dataset.\",\n      \"visual_anchor\": \"diagonal dashed line and dots clustered near it\",\n      \"text_evidence\": \"The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the Bernoulli success probabilities on the x-axis relate to the evaluation of classifiers trained in REAL2REAL setting?\",\n      \"answer\": \"The x-axis shows Bernoulli success probabilities from the real dataset, which is used both for training and testing in the REAL2REAL setting. This establishes the baseline probability distribution against which synthetic data generation quality is measured.\",\n      \"visual_anchor\": \"x-axis labeled P_real(x_d) with values from 0.0 to 1.0\",\n      \"text_evidence\": \"We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why must each dot's y-axis position match its x-axis value for fair classification training to succeed in subsection 5.3?\",\n      \"answer\": \"For fair classification, the synthetic dataset must preserve the same attribute probability distributions as the real data. Each dot represents one attribute, and alignment along the diagonal ensures the synthetic probability (y-axis) matches the real probability (x-axis), maintaining fairness properties.\",\n      \"visual_anchor\": \"dots positioned along diagonal with matching x and y coordinates\",\n      \"text_evidence\": \"In this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Scatter plot with dots representing attributes",
        "Diagonal dashed line from (0.0, 0.0) to (1.0, 1.0)",
        "X-axis labeled P_real(x_d) ranging from 0.0 to 1.0",
        "Y-axis labeled P_syn(y_d) ranging from 0.0 to 1.0",
        "Blue dots clustered near the diagonal line",
        "Red dots also present along the diagonal"
      ],
      "queries": [
        {
          "query": "Does the clustering of dots along the diagonal line demonstrate the ideal case where real and synthetic data show identical quality?",
          "answer": "Yes, the diagonal line indicates the ideal case where real and synthetic datasets have identical quality. The close clustering of dots along this line suggests the synthetic data successfully replicates the probability distributions of the real dataset.",
          "visual_anchor": "diagonal dashed line and dots clustered near it",
          "text_evidence": "The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.",
          "query_type": "value_context"
        },
        {
          "query": "How do the Bernoulli success probabilities on the x-axis relate to the evaluation of classifiers trained in REAL2REAL setting?",
          "answer": "The x-axis shows Bernoulli success probabilities from the real dataset, which is used both for training and testing in the REAL2REAL setting. This establishes the baseline probability distribution against which synthetic data generation quality is measured.",
          "visual_anchor": "x-axis labeled P_real(x_d) with values from 0.0 to 1.0",
          "text_evidence": "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why must each dot's y-axis position match its x-axis value for fair classification training to succeed in subsection 5.3?",
          "answer": "For fair classification, the synthetic dataset must preserve the same attribute probability distributions as the real data. Each dot represents one attribute, and alignment along the diagonal ensures the synthetic probability (y-axis) matches the real probability (x-axis), maintaining fairness properties.",
          "visual_anchor": "dots positioned along diagonal with matching x and y coordinates",
          "text_evidence": "In this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the clustering of dots along the diagonal line demonstrate the ideal case where real and synthetic data show identical quality?",
        "answer": "Yes, the diagonal line indicates the ideal case where real and synthetic datasets have identical quality. The close clustering of dots along this line suggests the synthetic data successfully replicates the probability distributions of the real dataset.",
        "visual_anchor": "diagonal dashed line and dots clustered near it",
        "text_evidence": "The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.",
        "query_type": "value_context"
      },
      {
        "query": "How do the Bernoulli success probabilities on the x-axis relate to the evaluation of classifiers trained in REAL2REAL setting?",
        "answer": "The x-axis shows Bernoulli success probabilities from the real dataset, which is used both for training and testing in the REAL2REAL setting. This establishes the baseline probability distribution against which synthetic data generation quality is measured.",
        "visual_anchor": "x-axis labeled P_real(x_d) with values from 0.0 to 1.0",
        "text_evidence": "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why must each dot's y-axis position match its x-axis value for fair classification training to succeed in subsection 5.3?",
        "answer": "For fair classification, the synthetic dataset must preserve the same attribute probability distributions as the real data. Each dot represents one attribute, and alignment along the diagonal ensures the synthetic probability (y-axis) matches the real probability (x-axis), maintaining fairness properties.",
        "visual_anchor": "dots positioned along diagonal with matching x and y coordinates",
        "text_evidence": "In this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Scatter plot with dots representing attributes",
      "Diagonal dashed line from (0.0, 0.0) to (1.0, 1.0)",
      "X-axis labeled P_real(x_d) ranging from 0.0 to 1.0",
      "Y-axis labeled P_syn(y_d) ranging from 0.0 to 1.0",
      "Blue dots clustered near the diagonal line",
      "Red dots also present along the diagonal"
    ]
  },
  {
    "doc_id": "1805.11202",
    "figure_id": "1805.11202_fig_25",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg",
    "caption": "Figure 6: The sensitivity analysis of FairGAN with various",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue line with square markers labeled 'risk difference'\", \"orange line with circle markers labeled 'Euclidean distance'\", \"x-axis showing lambda from 0.0 to 2.0\", \"y-axis ranging from 0.00 to 0.18\", \"error bars on blue line\", \"downward trend in blue curve\", \"relatively flat orange curve near 0.02\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue risk difference curve decline from 0.18 to near 0.03 as lambda increases to 2.0?\",\n      \"answer\": \"FairGAN is designed to generate fair data free from disparate treatment and disparate impact. Increasing lambda strengthens fairness constraints, reducing risk difference while maintaining data utility measured by Euclidean distance.\",\n      \"visual_anchor\": \"blue line with square markers showing decline from 0.18 at lambda=0.0 to approximately 0.03 at lambda=2.0\",\n      \"text_evidence\": \"we have developed FairGAN to generate fair data, which is free from disparate treatment and disparate impact in terms of the real protected attribute, while retaining high data utility\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the flat orange Euclidean distance curve near 0.02 confirm that data utility is retained during fairness optimization?\",\n      \"answer\": \"Yes, the orange curve remains stable around 0.02 across all lambda values, demonstrating that FairGAN retains high data utility while the risk difference decreases, confirming the dual objective is achieved.\",\n      \"visual_anchor\": \"orange line with circle markers remaining flat near 0.02 across lambda values from 0.0 to 2.0\",\n      \"text_evidence\": \"We evaluate how the  in FairGAN affects the synthetic datasets λfor fair data generation and fair classification.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes the large error bars on the blue curve at lambda values 0.6 to 1.8 compared to earlier points?\",\n      \"answer\": \"The larger error bars at intermediate lambda values suggest increased variability in fairness metrics during training. This occurs as FairGAN balances between the generator and two discriminators, where training stability varies with different fairness constraint strengths.\",\n      \"visual_anchor\": \"error bars on blue risk difference line, notably larger at lambda 0.6-1.8 compared to lambda 0.0-0.4\",\n      \"text_evidence\": \"FairGAN consists of one generator and two discriminators. In particular, the generator generates fake samples conditioned on\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue line with square markers labeled 'risk difference'",
        "orange line with circle markers labeled 'Euclidean distance'",
        "x-axis showing lambda from 0.0 to 2.0",
        "y-axis ranging from 0.00 to 0.18",
        "error bars on blue line",
        "downward trend in blue curve",
        "relatively flat orange curve near 0.02"
      ],
      "queries": [
        {
          "query": "Why does the blue risk difference curve decline from 0.18 to near 0.03 as lambda increases to 2.0?",
          "answer": "FairGAN is designed to generate fair data free from disparate treatment and disparate impact. Increasing lambda strengthens fairness constraints, reducing risk difference while maintaining data utility measured by Euclidean distance.",
          "visual_anchor": "blue line with square markers showing decline from 0.18 at lambda=0.0 to approximately 0.03 at lambda=2.0",
          "text_evidence": "we have developed FairGAN to generate fair data, which is free from disparate treatment and disparate impact in terms of the real protected attribute, while retaining high data utility",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the flat orange Euclidean distance curve near 0.02 confirm that data utility is retained during fairness optimization?",
          "answer": "Yes, the orange curve remains stable around 0.02 across all lambda values, demonstrating that FairGAN retains high data utility while the risk difference decreases, confirming the dual objective is achieved.",
          "visual_anchor": "orange line with circle markers remaining flat near 0.02 across lambda values from 0.0 to 2.0",
          "text_evidence": "We evaluate how the  in FairGAN affects the synthetic datasets λfor fair data generation and fair classification.",
          "query_type": "value_context"
        },
        {
          "query": "What causes the large error bars on the blue curve at lambda values 0.6 to 1.8 compared to earlier points?",
          "answer": "The larger error bars at intermediate lambda values suggest increased variability in fairness metrics during training. This occurs as FairGAN balances between the generator and two discriminators, where training stability varies with different fairness constraint strengths.",
          "visual_anchor": "error bars on blue risk difference line, notably larger at lambda 0.6-1.8 compared to lambda 0.0-0.4",
          "text_evidence": "FairGAN consists of one generator and two discriminators. In particular, the generator generates fake samples conditioned on",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue risk difference curve decline from 0.18 to near 0.03 as lambda increases to 2.0?",
        "answer": "FairGAN is designed to generate fair data free from disparate treatment and disparate impact. Increasing lambda strengthens fairness constraints, reducing risk difference while maintaining data utility measured by Euclidean distance.",
        "visual_anchor": "blue line with square markers showing decline from 0.18 at lambda=0.0 to approximately 0.03 at lambda=2.0",
        "text_evidence": "we have developed FairGAN to generate fair data, which is free from disparate treatment and disparate impact in terms of the real protected attribute, while retaining high data utility",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the flat orange Euclidean distance curve near 0.02 confirm that data utility is retained during fairness optimization?",
        "answer": "Yes, the orange curve remains stable around 0.02 across all lambda values, demonstrating that FairGAN retains high data utility while the risk difference decreases, confirming the dual objective is achieved.",
        "visual_anchor": "orange line with circle markers remaining flat near 0.02 across lambda values from 0.0 to 2.0",
        "text_evidence": "We evaluate how the  in FairGAN affects the synthetic datasets λfor fair data generation and fair classification.",
        "query_type": "value_context"
      },
      {
        "query": "What causes the large error bars on the blue curve at lambda values 0.6 to 1.8 compared to earlier points?",
        "answer": "The larger error bars at intermediate lambda values suggest increased variability in fairness metrics during training. This occurs as FairGAN balances between the generator and two discriminators, where training stability varies with different fairness constraint strengths.",
        "visual_anchor": "error bars on blue risk difference line, notably larger at lambda 0.6-1.8 compared to lambda 0.0-0.4",
        "text_evidence": "FairGAN consists of one generator and two discriminators. In particular, the generator generates fake samples conditioned on",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue line with square markers labeled 'risk difference'",
      "orange line with circle markers labeled 'Euclidean distance'",
      "x-axis showing lambda from 0.0 to 2.0",
      "y-axis ranging from 0.00 to 0.18",
      "error bars on blue line",
      "downward trend in blue curve",
      "relatively flat orange curve near 0.02"
    ]
  },
  {
    "doc_id": "1808.08166",
    "figure_id": "1808.08166_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg",
    "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Blue dots representing blue individuals\", \"Green dots representing green individuals\", \"Brown circles marking accepted individuals\", \"Four quadrants divided by Male/Female and Blue/Green\", \"Distribution patterns showing acceptance across subgroups\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the brown-circled accepted individuals in the left Male quadrants support the claim about bridging statistical and individual fairness?\",\n      \"answer\": \"Yes, the brown circles show selected individuals across both blue and green male subgroups, illustrating how rich subgroup fairness applies statistical constraints (equalizing false positive rates) to exponentially large collections of subgroups defined by bounded VC dimension classes.\",\n      \"visual_anchor\": \"Brown circles marking accepted individuals in the left Male column, distributed across both Blue and Green rows\",\n      \"text_evidence\": \"Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are more green dots than blue dots circled in the right Female quadrants, given the two sensitive attributes described?\",\n      \"answer\": \"The asymmetric acceptance pattern across Female quadrants demonstrates the fairness gerrymandering problem, where individuals have two sensitive attributes (race as blue/green and gender as male/female), allowing for complex subgroup definitions that can manipulate fairness constraints.\",\n      \"visual_anchor\": \"Female column showing four brown circles among green dots versus two circles among blue dots\",\n      \"text_evidence\": \"Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the circled blue individuals in the Male-Blue quadrant relate to the linear threshold function mentioned for this heuristic?\",\n      \"answer\": \"The five brown-circled blue dots in the Male-Blue quadrant represent individuals selected by the linear threshold function heuristic, demonstrating how the algorithm applies decision boundaries to achieve rich subgroup fairness across the combinatorially defined subgroups.\",\n      \"visual_anchor\": \"Five brown circles among blue dots in the top-left Male-Blue quadrant\",\n      \"text_evidence\": \"The heuristic finds a linear threshold function as follows:\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Blue dots representing blue individuals",
        "Green dots representing green individuals",
        "Brown circles marking accepted individuals",
        "Four quadrants divided by Male/Female and Blue/Green",
        "Distribution patterns showing acceptance across subgroups"
      ],
      "queries": [
        {
          "query": "Do the brown-circled accepted individuals in the left Male quadrants support the claim about bridging statistical and individual fairness?",
          "answer": "Yes, the brown circles show selected individuals across both blue and green male subgroups, illustrating how rich subgroup fairness applies statistical constraints (equalizing false positive rates) to exponentially large collections of subgroups defined by bounded VC dimension classes.",
          "visual_anchor": "Brown circles marking accepted individuals in the left Male column, distributed across both Blue and Green rows",
          "text_evidence": "Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension.",
          "query_type": "value_context"
        },
        {
          "query": "Why are more green dots than blue dots circled in the right Female quadrants, given the two sensitive attributes described?",
          "answer": "The asymmetric acceptance pattern across Female quadrants demonstrates the fairness gerrymandering problem, where individuals have two sensitive attributes (race as blue/green and gender as male/female), allowing for complex subgroup definitions that can manipulate fairness constraints.",
          "visual_anchor": "Female column showing four brown circles among green dots versus two circles among blue dots",
          "text_evidence": "Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the circled blue individuals in the Male-Blue quadrant relate to the linear threshold function mentioned for this heuristic?",
          "answer": "The five brown-circled blue dots in the Male-Blue quadrant represent individuals selected by the linear threshold function heuristic, demonstrating how the algorithm applies decision boundaries to achieve rich subgroup fairness across the combinatorially defined subgroups.",
          "visual_anchor": "Five brown circles among blue dots in the top-left Male-Blue quadrant",
          "text_evidence": "The heuristic finds a linear threshold function as follows:",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the brown-circled accepted individuals in the left Male quadrants support the claim about bridging statistical and individual fairness?",
        "answer": "Yes, the brown circles show selected individuals across both blue and green male subgroups, illustrating how rich subgroup fairness applies statistical constraints (equalizing false positive rates) to exponentially large collections of subgroups defined by bounded VC dimension classes.",
        "visual_anchor": "Brown circles marking accepted individuals in the left Male column, distributed across both Blue and Green rows",
        "text_evidence": "Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension.",
        "query_type": "value_context"
      },
      {
        "query": "Why are more green dots than blue dots circled in the right Female quadrants, given the two sensitive attributes described?",
        "answer": "The asymmetric acceptance pattern across Female quadrants demonstrates the fairness gerrymandering problem, where individuals have two sensitive attributes (race as blue/green and gender as male/female), allowing for complex subgroup definitions that can manipulate fairness constraints.",
        "visual_anchor": "Female column showing four brown circles among green dots versus two circles among blue dots",
        "text_evidence": "Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the circled blue individuals in the Male-Blue quadrant relate to the linear threshold function mentioned for this heuristic?",
        "answer": "The five brown-circled blue dots in the Male-Blue quadrant represent individuals selected by the linear threshold function heuristic, demonstrating how the algorithm applies decision boundaries to achieve rich subgroup fairness across the combinatorially defined subgroups.",
        "visual_anchor": "Five brown circles among blue dots in the top-left Male-Blue quadrant",
        "text_evidence": "The heuristic finds a linear threshold function as follows:",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Blue dots representing blue individuals",
      "Green dots representing green individuals",
      "Brown circles marking accepted individuals",
      "Four quadrants divided by Male/Female and Blue/Green",
      "Distribution patterns showing acceptance across subgroups"
    ]
  },
  {
    "doc_id": "1808.08166",
    "figure_id": "1808.08166_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg",
    "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Multiple colored curves showing gamma_t values converging over iterations\",\n    \"Dashed horizontal lines at varying heights corresponding to input gamma values\",\n    \"Y-axis labeled 'gamma_t' ranging from 0 to 0.06\",\n    \"X-axis labeled 'iteration' ranging from 0 to 14000\",\n    \"Title 'Auditor: gamma_t vs. t'\",\n    \"Dense collection of curves with different colors (red, blue, green, yellow, etc.)\",\n    \"Convergence pattern where curves stabilize around horizontal dashed lines\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the dashed horizontal lines representing varying input gamma values align with where the colored curves stabilize?\",\n      \"answer\": \"Yes, the dashed horizontal lines correspond to the input gamma values where the colored curves converge, demonstrating that the algorithm achieves the specified fairness violation targets as iterations progress.\",\n      \"visual_anchor\": \"Dashed horizontal lines at multiple y-values between 0 and 0.03\",\n      \"text_evidence\": \"Dashed horizontal lines on $\\\\gamma _ { t }$ plots correspond to varying values of $\\\\gamma$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do most colored curves show rapid descent from high gamma values before iteration 2000 for input gamma ranging 0 to 0.03?\",\n      \"answer\": \"The rapid initial descent demonstrates the quick convergence properties of the SUBGROUP algorithm, consistent with findings that the algorithm converges quickly while providing an appealing trade-off between error and fairness.\",\n      \"visual_anchor\": \"Steep decline in colored curves between iteration 0 and 2000\",\n      \"text_evidence\": \"Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\\\gamma$ provides an appealing trade-off between error and fairness\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the stabilization pattern of curves after iteration 4000 support plotting epsilon-gamma pairs across all iterations and input values?\",\n      \"answer\": \"Yes, the stabilization of curves after iteration 4000 provides meaningful epsilon-gamma pairs across iterations and input values, which naturally enables computation of the undominated Pareto frontier of these pairs.\",\n      \"visual_anchor\": \"Relatively flat curves after iteration 4000 converging to dashed horizontal lines\",\n      \"text_evidence\": \"Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\\\langle \\\\varepsilon _ { t } , \\\\gamma _ { t } \\\\rangle$ pairs across all $t$ and all input $\\\\gamma$ , and compute the undominated or Pareto frontier of these pairs\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Multiple colored curves showing gamma_t values converging over iterations",
        "Dashed horizontal lines at varying heights corresponding to input gamma values",
        "Y-axis labeled 'gamma_t' ranging from 0 to 0.06",
        "X-axis labeled 'iteration' ranging from 0 to 14000",
        "Title 'Auditor: gamma_t vs. t'",
        "Dense collection of curves with different colors (red, blue, green, yellow, etc.)",
        "Convergence pattern where curves stabilize around horizontal dashed lines"
      ],
      "queries": [
        {
          "query": "Do the dashed horizontal lines representing varying input gamma values align with where the colored curves stabilize?",
          "answer": "Yes, the dashed horizontal lines correspond to the input gamma values where the colored curves converge, demonstrating that the algorithm achieves the specified fairness violation targets as iterations progress.",
          "visual_anchor": "Dashed horizontal lines at multiple y-values between 0 and 0.03",
          "text_evidence": "Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do most colored curves show rapid descent from high gamma values before iteration 2000 for input gamma ranging 0 to 0.03?",
          "answer": "The rapid initial descent demonstrates the quick convergence properties of the SUBGROUP algorithm, consistent with findings that the algorithm converges quickly while providing an appealing trade-off between error and fairness.",
          "visual_anchor": "Steep decline in colored curves between iteration 0 and 2000",
          "text_evidence": "Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\gamma$ provides an appealing trade-off between error and fairness",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the stabilization pattern of curves after iteration 4000 support plotting epsilon-gamma pairs across all iterations and input values?",
          "answer": "Yes, the stabilization of curves after iteration 4000 provides meaningful epsilon-gamma pairs across iterations and input values, which naturally enables computation of the undominated Pareto frontier of these pairs.",
          "visual_anchor": "Relatively flat curves after iteration 4000 converging to dashed horizontal lines",
          "text_evidence": "Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all $t$ and all input $\\gamma$ , and compute the undominated or Pareto frontier of these pairs",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the dashed horizontal lines representing varying input gamma values align with where the colored curves stabilize?",
        "answer": "Yes, the dashed horizontal lines correspond to the input gamma values where the colored curves converge, demonstrating that the algorithm achieves the specified fairness violation targets as iterations progress.",
        "visual_anchor": "Dashed horizontal lines at multiple y-values between 0 and 0.03",
        "text_evidence": "Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do most colored curves show rapid descent from high gamma values before iteration 2000 for input gamma ranging 0 to 0.03?",
        "answer": "The rapid initial descent demonstrates the quick convergence properties of the SUBGROUP algorithm, consistent with findings that the algorithm converges quickly while providing an appealing trade-off between error and fairness.",
        "visual_anchor": "Steep decline in colored curves between iteration 0 and 2000",
        "text_evidence": "Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\gamma$ provides an appealing trade-off between error and fairness",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the stabilization pattern of curves after iteration 4000 support plotting epsilon-gamma pairs across all iterations and input values?",
        "answer": "Yes, the stabilization of curves after iteration 4000 provides meaningful epsilon-gamma pairs across iterations and input values, which naturally enables computation of the undominated Pareto frontier of these pairs.",
        "visual_anchor": "Relatively flat curves after iteration 4000 converging to dashed horizontal lines",
        "text_evidence": "Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all $t$ and all input $\\gamma$ , and compute the undominated or Pareto frontier of these pairs",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Multiple colored curves showing gamma_t values converging over iterations",
      "Dashed horizontal lines at varying heights corresponding to input gamma values",
      "Y-axis labeled 'gamma_t' ranging from 0 to 0.06",
      "X-axis labeled 'iteration' ranging from 0 to 14000",
      "Title 'Auditor: gamma_t vs. t'",
      "Dense collection of curves with different colors (red, blue, green, yellow, etc.)",
      "Convergence pattern where curves stabilize around horizontal dashed lines"
    ]
  },
  {
    "doc_id": "1808.08166",
    "figure_id": "1808.08166_fig_11",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
    "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue points forming downward trend\", \"x-axis: error range ~0.16 to ~0.23\", \"y-axis: marginal fairness range ~0 to 0.25\", \"dense clustering of points at lower error values\", \"sparse distribution at higher error values\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?\",\n      \"answer\": \"Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.\",\n      \"visual_anchor\": \"blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)\",\n      \"text_evidence\": \"The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are blue points shown in the right column when the SUBGROUP algorithm's Pareto frontier is red in the left column?\",\n      \"answer\": \"The right column specifically displays the MARGINAL algorithm's performance on marginal fairness, while the left column compares SUBGROUP (red) versus MARGINAL (blue) on subgroup fairness.\",\n      \"visual_anchor\": \"blue points in right column plot versus red/blue distinction mentioned for left column\",\n      \"text_evidence\": \"In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the clustering of blue points near error 0.18 reflect the heuristic Auditor's detection limitations for subgroup unfairness?\",\n      \"answer\": \"The clustering reflects the MARGINAL algorithm's performance trade-offs, not the Auditor's limitations. The Auditor's potential failure to detect larger discrimination is a separate concern about measurement accuracy.\",\n      \"visual_anchor\": \"dense clustering of blue points around x=0.18 on error axis\",\n      \"text_evidence\": \"This means that while any detected unfairness is a lower bound on the true subgroup unfairness, it could be the case that the heuristic Auditor is simply failing to detect a larger dis\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue points forming downward trend",
        "x-axis: error range ~0.16 to ~0.23",
        "y-axis: marginal fairness range ~0 to 0.25",
        "dense clustering of points at lower error values",
        "sparse distribution at higher error values"
      ],
      "queries": [
        {
          "query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
          "answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
          "visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
          "text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
          "query_type": "value_context"
        },
        {
          "query": "Why are blue points shown in the right column when the SUBGROUP algorithm's Pareto frontier is red in the left column?",
          "answer": "The right column specifically displays the MARGINAL algorithm's performance on marginal fairness, while the left column compares SUBGROUP (red) versus MARGINAL (blue) on subgroup fairness.",
          "visual_anchor": "blue points in right column plot versus red/blue distinction mentioned for left column",
          "text_evidence": "In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the clustering of blue points near error 0.18 reflect the heuristic Auditor's detection limitations for subgroup unfairness?",
          "answer": "The clustering reflects the MARGINAL algorithm's performance trade-offs, not the Auditor's limitations. The Auditor's potential failure to detect larger discrimination is a separate concern about measurement accuracy.",
          "visual_anchor": "dense clustering of blue points around x=0.18 on error axis",
          "text_evidence": "This means that while any detected unfairness is a lower bound on the true subgroup unfairness, it could be the case that the heuristic Auditor is simply failing to detect a larger dis",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the blue points' downward trend from y=0.25 to y=0 demonstrate the MARGINAL algorithm's error-fairness pairs?",
        "answer": "Yes, the blue points show the error and marginal fairness violation pairs achieved by the MARGINAL algorithm, displaying a clear trade-off as error increases from left to right.",
        "visual_anchor": "blue points forming downward curve from upper-left (error ~0.16, fairness ~0.25) to lower-right (error ~0.23, fairness ~0)",
        "text_evidence": "The error and marginal fairness violation for the MARGINAL algorithm across all four data sets.",
        "query_type": "value_context"
      },
      {
        "query": "Why are blue points shown in the right column when the SUBGROUP algorithm's Pareto frontier is red in the left column?",
        "answer": "The right column specifically displays the MARGINAL algorithm's performance on marginal fairness, while the left column compares SUBGROUP (red) versus MARGINAL (blue) on subgroup fairness.",
        "visual_anchor": "blue points in right column plot versus red/blue distinction mentioned for left column",
        "text_evidence": "In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the clustering of blue points near error 0.18 reflect the heuristic Auditor's detection limitations for subgroup unfairness?",
        "answer": "The clustering reflects the MARGINAL algorithm's performance trade-offs, not the Auditor's limitations. The Auditor's potential failure to detect larger discrimination is a separate concern about measurement accuracy.",
        "visual_anchor": "dense clustering of blue points around x=0.18 on error axis",
        "text_evidence": "This means that while any detected unfairness is a lower bound on the true subgroup unfairness, it could be the case that the heuristic Auditor is simply failing to detect a larger dis",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue points forming downward trend",
      "x-axis: error range ~0.16 to ~0.23",
      "y-axis: marginal fairness range ~0 to 0.25",
      "dense clustering of points at lower error values",
      "sparse distribution at higher error values"
    ]
  },
  {
    "doc_id": "1808.08166",
    "figure_id": "1808.08166_fig_19",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg",
    "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"3D surface plot with yellow peak and blue valley\",\n    \"axes labeled 'white coefficient', 'black coefficient', and 'aggregate disparity'\",\n    \"time stamp t=31 at top\",\n    \"color gradient from blue (negative) through green to yellow (positive)\",\n    \"z-axis range approximately -0.03 to 0.03\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the yellow peak at t=31 represent positive false positive discrepancy for a specific subgroup over two protected attributes?\",\n      \"answer\": \"Yes, each point in the plane corresponds to a different subgroup over two protected attributes, and the z-value represents the current false positive discrepancy for that subgroup, with yellow indicating positive values.\",\n      \"visual_anchor\": \"yellow peak in the surface plot with positive z-values around 0.02-0.03\",\n      \"text_evidence\": \"Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the discrimination surface at t=31 show both yellow peaks and blue valleys across the coefficient plane?\",\n      \"answer\": \"The surface visualizes the evolution of the SUBGROUP algorithm with input γ=0, where different subgroups defined by white and black coefficient combinations exhibit varying false positive discrepancies, creating peaks and valleys.\",\n      \"visual_anchor\": \"contrasting yellow peaks and blue valleys distributed across the white coefficient and black coefficient plane\",\n      \"text_evidence\": \"we show a sequence of \\\"discrimination surfaces\\\" for the SUBGROUP algorithm over the 2 protected features, with input $\\\\gamma = 0$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What temporal range does the snapshot at t=31 belong to in the discrimination surface evolution sequence?\",\n      \"answer\": \"The snapshot at t=31 is an early frame in the evolution sequence that runs from t=1 to t=1301, capturing the algorithm's discrimination surface during its initial iterations.\",\n      \"visual_anchor\": \"time stamp t=31 displayed at the top of the 3D surface plot\",\n      \"text_evidence\": \"Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\\\ldots 1 3 0 1$\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "3D surface plot with yellow peak and blue valley",
        "axes labeled 'white coefficient', 'black coefficient', and 'aggregate disparity'",
        "time stamp t=31 at top",
        "color gradient from blue (negative) through green to yellow (positive)",
        "z-axis range approximately -0.03 to 0.03"
      ],
      "queries": [
        {
          "query": "Does the yellow peak at t=31 represent positive false positive discrepancy for a specific subgroup over two protected attributes?",
          "answer": "Yes, each point in the plane corresponds to a different subgroup over two protected attributes, and the z-value represents the current false positive discrepancy for that subgroup, with yellow indicating positive values.",
          "visual_anchor": "yellow peak in the surface plot with positive z-values around 0.02-0.03",
          "text_evidence": "Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the discrimination surface at t=31 show both yellow peaks and blue valleys across the coefficient plane?",
          "answer": "The surface visualizes the evolution of the SUBGROUP algorithm with input γ=0, where different subgroups defined by white and black coefficient combinations exhibit varying false positive discrepancies, creating peaks and valleys.",
          "visual_anchor": "contrasting yellow peaks and blue valleys distributed across the white coefficient and black coefficient plane",
          "text_evidence": "we show a sequence of \"discrimination surfaces\" for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What temporal range does the snapshot at t=31 belong to in the discrimination surface evolution sequence?",
          "answer": "The snapshot at t=31 is an early frame in the evolution sequence that runs from t=1 to t=1301, capturing the algorithm's discrimination surface during its initial iterations.",
          "visual_anchor": "time stamp t=31 displayed at the top of the 3D surface plot",
          "text_evidence": "Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the yellow peak at t=31 represent positive false positive discrepancy for a specific subgroup over two protected attributes?",
        "answer": "Yes, each point in the plane corresponds to a different subgroup over two protected attributes, and the z-value represents the current false positive discrepancy for that subgroup, with yellow indicating positive values.",
        "visual_anchor": "yellow peak in the surface plot with positive z-values around 0.02-0.03",
        "text_evidence": "Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the discrimination surface at t=31 show both yellow peaks and blue valleys across the coefficient plane?",
        "answer": "The surface visualizes the evolution of the SUBGROUP algorithm with input γ=0, where different subgroups defined by white and black coefficient combinations exhibit varying false positive discrepancies, creating peaks and valleys.",
        "visual_anchor": "contrasting yellow peaks and blue valleys distributed across the white coefficient and black coefficient plane",
        "text_evidence": "we show a sequence of \"discrimination surfaces\" for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What temporal range does the snapshot at t=31 belong to in the discrimination surface evolution sequence?",
        "answer": "The snapshot at t=31 is an early frame in the evolution sequence that runs from t=1 to t=1301, capturing the algorithm's discrimination surface during its initial iterations.",
        "visual_anchor": "time stamp t=31 displayed at the top of the 3D surface plot",
        "text_evidence": "Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "3D surface plot with yellow peak and blue valley",
      "axes labeled 'white coefficient', 'black coefficient', and 'aggregate disparity'",
      "time stamp t=31 at top",
      "color gradient from blue (negative) through green to yellow (positive)",
      "z-axis range approximately -0.03 to 0.03"
    ]
  },
  {
    "doc_id": "1808.08166",
    "figure_id": "1808.08166_fig_23",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg",
    "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Trajectory starting at approximately (0.1, 0.027) moving rightward and upward\",\n    \"Colored markers (red to yellow gradient) near coordinates (0.2, 0.006)\",\n    \"Horizontal dashed line at gamma = 0.005\",\n    \"Peak point at approximately (0.35, 0.047)\",\n    \"Multiple data points clustered between error 0.3 and 0.4\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the trajectory starting at error 0.1 move upward past the dashed line at gamma=0.005 for Communities and Crime?\",\n      \"answer\": \"The trajectory explores different gamma values from the set {0.001, 0.005, 0.009, 0.022}, moving upward as gamma increases beyond 0.005 to test subgroup fairness constraints on the Communities and Crime dataset.\",\n      \"visual_anchor\": \"trajectory starting at (0.1, 0.027) crossing above the horizontal dashed line at gamma=0.005\",\n      \"text_evidence\": \"$\\\\langle \\\\varepsilon _ { t } , \\\\gamma _ { t } \\\\rangle$ trajectories for Communities and Crime, for $\\\\gamma \\\\in \\\\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\\\}$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the colored markers near error 0.2 and gamma 0.006 demonstrate the empirical efficacy of rich subgroup fairness on Communities and Crime?\",\n      \"answer\": \"Yes, the colored markers show the algorithm's trajectory through the error-fairness tradeoff space, empirically demonstrating how subgroup fairness constraints are enforced during training on this fairness-sensitive dataset.\",\n      \"visual_anchor\": \"red to yellow gradient markers clustered near coordinates (0.2, 0.006)\",\n      \"text_evidence\": \"In this work we have established the empirical efficacy of the notion of rich subgroup fairness and the algorithm of Kearns et al. [2018] on four fairness-sensitive datasets\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes the peak at error 0.35 and gamma 0.047 despite testing only four discrete gamma values?\",\n      \"answer\": \"The peak represents an intermediate state in the iterative algorithm trajectory where the Auditor identifies subgroups with high unfairness, forcing gamma_t to temporarily spike before the Learner adjusts to satisfy the subgroup fairness constraint.\",\n      \"visual_anchor\": \"peak point at approximately (0.35, 0.047)\",\n      \"text_evidence\": \"the necessity of explicitly enforcing subgroup (as opposed to only marginal) fairness\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Trajectory starting at approximately (0.1, 0.027) moving rightward and upward",
        "Colored markers (red to yellow gradient) near coordinates (0.2, 0.006)",
        "Horizontal dashed line at gamma = 0.005",
        "Peak point at approximately (0.35, 0.047)",
        "Multiple data points clustered between error 0.3 and 0.4"
      ],
      "queries": [
        {
          "query": "Why does the trajectory starting at error 0.1 move upward past the dashed line at gamma=0.005 for Communities and Crime?",
          "answer": "The trajectory explores different gamma values from the set {0.001, 0.005, 0.009, 0.022}, moving upward as gamma increases beyond 0.005 to test subgroup fairness constraints on the Communities and Crime dataset.",
          "visual_anchor": "trajectory starting at (0.1, 0.027) crossing above the horizontal dashed line at gamma=0.005",
          "text_evidence": "$\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the colored markers near error 0.2 and gamma 0.006 demonstrate the empirical efficacy of rich subgroup fairness on Communities and Crime?",
          "answer": "Yes, the colored markers show the algorithm's trajectory through the error-fairness tradeoff space, empirically demonstrating how subgroup fairness constraints are enforced during training on this fairness-sensitive dataset.",
          "visual_anchor": "red to yellow gradient markers clustered near coordinates (0.2, 0.006)",
          "text_evidence": "In this work we have established the empirical efficacy of the notion of rich subgroup fairness and the algorithm of Kearns et al. [2018] on four fairness-sensitive datasets",
          "query_type": "value_context"
        },
        {
          "query": "What causes the peak at error 0.35 and gamma 0.047 despite testing only four discrete gamma values?",
          "answer": "The peak represents an intermediate state in the iterative algorithm trajectory where the Auditor identifies subgroups with high unfairness, forcing gamma_t to temporarily spike before the Learner adjusts to satisfy the subgroup fairness constraint.",
          "visual_anchor": "peak point at approximately (0.35, 0.047)",
          "text_evidence": "the necessity of explicitly enforcing subgroup (as opposed to only marginal) fairness",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the trajectory starting at error 0.1 move upward past the dashed line at gamma=0.005 for Communities and Crime?",
        "answer": "The trajectory explores different gamma values from the set {0.001, 0.005, 0.009, 0.022}, moving upward as gamma increases beyond 0.005 to test subgroup fairness constraints on the Communities and Crime dataset.",
        "visual_anchor": "trajectory starting at (0.1, 0.027) crossing above the horizontal dashed line at gamma=0.005",
        "text_evidence": "$\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the colored markers near error 0.2 and gamma 0.006 demonstrate the empirical efficacy of rich subgroup fairness on Communities and Crime?",
        "answer": "Yes, the colored markers show the algorithm's trajectory through the error-fairness tradeoff space, empirically demonstrating how subgroup fairness constraints are enforced during training on this fairness-sensitive dataset.",
        "visual_anchor": "red to yellow gradient markers clustered near coordinates (0.2, 0.006)",
        "text_evidence": "In this work we have established the empirical efficacy of the notion of rich subgroup fairness and the algorithm of Kearns et al. [2018] on four fairness-sensitive datasets",
        "query_type": "value_context"
      },
      {
        "query": "What causes the peak at error 0.35 and gamma 0.047 despite testing only four discrete gamma values?",
        "answer": "The peak represents an intermediate state in the iterative algorithm trajectory where the Auditor identifies subgroups with high unfairness, forcing gamma_t to temporarily spike before the Learner adjusts to satisfy the subgroup fairness constraint.",
        "visual_anchor": "peak point at approximately (0.35, 0.047)",
        "text_evidence": "the necessity of explicitly enforcing subgroup (as opposed to only marginal) fairness",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Trajectory starting at approximately (0.1, 0.027) moving rightward and upward",
      "Colored markers (red to yellow gradient) near coordinates (0.2, 0.006)",
      "Horizontal dashed line at gamma = 0.005",
      "Peak point at approximately (0.35, 0.047)",
      "Multiple data points clustered between error 0.3 and 0.4"
    ]
  },
  {
    "doc_id": "1809.01496",
    "figure_id": "1809.01496_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg",
    "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"x-axis from -2.0 to 2.0\", \"y-axis from -2.0 to 2.0\", \"blue x markers for neutral words\", \"red triangles for female-biased words on left\", \"green triangles for male-biased words on right\", \"gender-neutral word labels (colonel, captain, archbishop, etc.)\", \"gender-specific word labels (waitress, nun, maid on left; waiter, salesman, policeman on right)\"],\n  \"queries\": [\n    {\n      \"query\": \"Why are waitress and nun positioned at negative x-values while waiter and salesman appear at positive x-values?\",\n      \"answer\": \"The plot shows cosine similarity with the gender direction, where negative values represent bias towards female and positive values towards male, as defined by the gender direction vector.\",\n      \"visual_anchor\": \"red triangles (waitress, nun) at x < 0 and green triangles (waiter, salesman) at x > 0\",\n      \"text_evidence\": \"where $\\\\Omega ^ { \\\\prime }$ is a set of predefined gender word pairs\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the clustering of blue x markers near the origin support the fixed vector assumption during gradient descent?\",\n      \"answer\": \"Yes, the concentration of gender-neutral words near zero cosine similarity indicates reduced gender bias, consistent with assuming v_g is fixed during training to optimize the embedding space efficiently.\",\n      \"visual_anchor\": \"dense cluster of blue x markers centered around (0,0)\",\n      \"text_evidence\": \"To reduce the computational complexity in training the wording embedding, we assume $v _ { g }$ is a fixed vector (i.e., we do not derive gradient w.r.t $v _ { g }$ in updating $w ^ { ( a ) } , \\\\forall w \\\\in \\\\Omega ^ { \\\\prime } )$ and estimate $v _ { g }$ only at the beginning of each epoch\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the positions of colonel and captain demonstrate the method's generalization beyond binary gender pronouns?\",\n      \"answer\": \"Colonel and captain appear near the center with low gender bias, showing that the method successfully neutralizes gender in occupation words even though the approach can extend beyond binary gender as noted in future directions.\",\n      \"visual_anchor\": \"colonel and captain labels positioned near x=0.5-1.0, y=0 to -1.0\",\n      \"text_evidence\": \"Future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "x-axis from -2.0 to 2.0",
        "y-axis from -2.0 to 2.0",
        "blue x markers for neutral words",
        "red triangles for female-biased words on left",
        "green triangles for male-biased words on right",
        "gender-neutral word labels (colonel, captain, archbishop, etc.)",
        "gender-specific word labels (waitress, nun, maid on left; waiter, salesman, policeman on right)"
      ],
      "queries": [
        {
          "query": "Why are waitress and nun positioned at negative x-values while waiter and salesman appear at positive x-values?",
          "answer": "The plot shows cosine similarity with the gender direction, where negative values represent bias towards female and positive values towards male, as defined by the gender direction vector.",
          "visual_anchor": "red triangles (waitress, nun) at x < 0 and green triangles (waiter, salesman) at x > 0",
          "text_evidence": "where $\\Omega ^ { \\prime }$ is a set of predefined gender word pairs",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the clustering of blue x markers near the origin support the fixed vector assumption during gradient descent?",
          "answer": "Yes, the concentration of gender-neutral words near zero cosine similarity indicates reduced gender bias, consistent with assuming v_g is fixed during training to optimize the embedding space efficiently.",
          "visual_anchor": "dense cluster of blue x markers centered around (0,0)",
          "text_evidence": "To reduce the computational complexity in training the wording embedding, we assume $v _ { g }$ is a fixed vector (i.e., we do not derive gradient w.r.t $v _ { g }$ in updating $w ^ { ( a ) } , \\forall w \\in \\Omega ^ { \\prime } )$ and estimate $v _ { g }$ only at the beginning of each epoch",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the positions of colonel and captain demonstrate the method's generalization beyond binary gender pronouns?",
          "answer": "Colonel and captain appear near the center with low gender bias, showing that the method successfully neutralizes gender in occupation words even though the approach can extend beyond binary gender as noted in future directions.",
          "visual_anchor": "colonel and captain labels positioned near x=0.5-1.0, y=0 to -1.0",
          "text_evidence": "Future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why are waitress and nun positioned at negative x-values while waiter and salesman appear at positive x-values?",
        "answer": "The plot shows cosine similarity with the gender direction, where negative values represent bias towards female and positive values towards male, as defined by the gender direction vector.",
        "visual_anchor": "red triangles (waitress, nun) at x < 0 and green triangles (waiter, salesman) at x > 0",
        "text_evidence": "where $\\Omega ^ { \\prime }$ is a set of predefined gender word pairs",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the clustering of blue x markers near the origin support the fixed vector assumption during gradient descent?",
        "answer": "Yes, the concentration of gender-neutral words near zero cosine similarity indicates reduced gender bias, consistent with assuming v_g is fixed during training to optimize the embedding space efficiently.",
        "visual_anchor": "dense cluster of blue x markers centered around (0,0)",
        "text_evidence": "To reduce the computational complexity in training the wording embedding, we assume $v _ { g }$ is a fixed vector (i.e., we do not derive gradient w.r.t $v _ { g }$ in updating $w ^ { ( a ) } , \\forall w \\in \\Omega ^ { \\prime } )$ and estimate $v _ { g }$ only at the beginning of each epoch",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the positions of colonel and captain demonstrate the method's generalization beyond binary gender pronouns?",
        "answer": "Colonel and captain appear near the center with low gender bias, showing that the method successfully neutralizes gender in occupation words even though the approach can extend beyond binary gender as noted in future directions.",
        "visual_anchor": "colonel and captain labels positioned near x=0.5-1.0, y=0 to -1.0",
        "text_evidence": "Future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "x-axis from -2.0 to 2.0",
      "y-axis from -2.0 to 2.0",
      "blue x markers for neutral words",
      "red triangles for female-biased words on left",
      "green triangles for male-biased words on right",
      "gender-neutral word labels (colonel, captain, archbishop, etc.)",
      "gender-specific word labels (waitress, nun, maid on left; waiter, salesman, policeman on right)"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg",
    "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Hungarian source text on left panel\",\n    \"English translations on right panel\",\n    \"Translation pairs showing 'nurse' with 'she's'\",\n    \"Translation pairs showing 'engineer' with 'he is'\",\n    \"Translation pairs showing 'CEO' with 'he's'\",\n    \"Translation pairs showing 'baker' with 'she's'\",\n    \"Translation pairs showing 'wedding organizer' with 'She is'\",\n    \"Google Translate interface\",\n    \"Character count 110/5000\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does translating 'ő egy ápoló' yield 'she's a nurse' while 'ő egy tudós' produces 'he is a scientist'?\",\n      \"answer\": \"The translation tool reflects gender bias by associating stereotypical female roles like nurse with female pronouns and male-dominated fields like scientist with male pronouns, consistent with societal inequality.\",\n      \"visual_anchor\": \"First line 'ő egy ápoló' translating to 'she's a nurse' versus second line 'ő egy tudós' translating to 'he is a scientist'\",\n      \"text_evidence\": \"In this paper we assume that a statistical translation tool should reflect at most the inequality existent in society – it is only logical that a translation tool will poll from examples that society produced and, as such, will inevitably retain some of that bias.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the male pronoun 'he is' appearing for engineer support claims about bias in male-dominated field translations?\",\n      \"answer\": \"Yes, the translation of 'ő egy mérnök' as 'he is an engineer' demonstrates that occupations from traditionally male-dominated fields are interpreted as male by the statistical translation model.\",\n      \"visual_anchor\": \"Third line showing 'ő egy mérnök' translated as 'he is an engineer'\",\n      \"text_evidence\": \"This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the female pronoun for wedding organizer versus male for CEO illustrate the hypothesis about stereotypical role translation?\",\n      \"answer\": \"The contrast between 'She is a wedding organizer' and 'he's a CEO' demonstrates high probability translation of sentences about stereotypical gender roles, with traditional female roles receiving female pronouns and leadership positions receiving male pronouns.\",\n      \"visual_anchor\": \"Sixth line 'She is a wedding organizer' contrasted with seventh line 'he's a CEO'\",\n      \"text_evidence\": \"this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and CEO are translated with male ones.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Hungarian source text on left panel",
        "English translations on right panel",
        "Translation pairs showing 'nurse' with 'she's'",
        "Translation pairs showing 'engineer' with 'he is'",
        "Translation pairs showing 'CEO' with 'he's'",
        "Translation pairs showing 'baker' with 'she's'",
        "Translation pairs showing 'wedding organizer' with 'She is'",
        "Google Translate interface",
        "Character count 110/5000"
      ],
      "queries": [
        {
          "query": "Why does translating 'ő egy ápoló' yield 'she's a nurse' while 'ő egy tudós' produces 'he is a scientist'?",
          "answer": "The translation tool reflects gender bias by associating stereotypical female roles like nurse with female pronouns and male-dominated fields like scientist with male pronouns, consistent with societal inequality.",
          "visual_anchor": "First line 'ő egy ápoló' translating to 'she's a nurse' versus second line 'ő egy tudós' translating to 'he is a scientist'",
          "text_evidence": "In this paper we assume that a statistical translation tool should reflect at most the inequality existent in society – it is only logical that a translation tool will poll from examples that society produced and, as such, will inevitably retain some of that bias.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the male pronoun 'he is' appearing for engineer support claims about bias in male-dominated field translations?",
          "answer": "Yes, the translation of 'ő egy mérnök' as 'he is an engineer' demonstrates that occupations from traditionally male-dominated fields are interpreted as male by the statistical translation model.",
          "visual_anchor": "Third line showing 'ő egy mérnök' translated as 'he is an engineer'",
          "text_evidence": "This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
          "query_type": "value_context"
        },
        {
          "query": "How does the female pronoun for wedding organizer versus male for CEO illustrate the hypothesis about stereotypical role translation?",
          "answer": "The contrast between 'She is a wedding organizer' and 'he's a CEO' demonstrates high probability translation of sentences about stereotypical gender roles, with traditional female roles receiving female pronouns and leadership positions receiving male pronouns.",
          "visual_anchor": "Sixth line 'She is a wedding organizer' contrasted with seventh line 'he's a CEO'",
          "text_evidence": "this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and CEO are translated with male ones.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does translating 'ő egy ápoló' yield 'she's a nurse' while 'ő egy tudós' produces 'he is a scientist'?",
        "answer": "The translation tool reflects gender bias by associating stereotypical female roles like nurse with female pronouns and male-dominated fields like scientist with male pronouns, consistent with societal inequality.",
        "visual_anchor": "First line 'ő egy ápoló' translating to 'she's a nurse' versus second line 'ő egy tudós' translating to 'he is a scientist'",
        "text_evidence": "In this paper we assume that a statistical translation tool should reflect at most the inequality existent in society – it is only logical that a translation tool will poll from examples that society produced and, as such, will inevitably retain some of that bias.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the male pronoun 'he is' appearing for engineer support claims about bias in male-dominated field translations?",
        "answer": "Yes, the translation of 'ő egy mérnök' as 'he is an engineer' demonstrates that occupations from traditionally male-dominated fields are interpreted as male by the statistical translation model.",
        "visual_anchor": "Third line showing 'ő egy mérnök' translated as 'he is an engineer'",
        "text_evidence": "This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
        "query_type": "value_context"
      },
      {
        "query": "How does the female pronoun for wedding organizer versus male for CEO illustrate the hypothesis about stereotypical role translation?",
        "answer": "The contrast between 'She is a wedding organizer' and 'he's a CEO' demonstrates high probability translation of sentences about stereotypical gender roles, with traditional female roles receiving female pronouns and leadership positions receiving male pronouns.",
        "visual_anchor": "Sixth line 'She is a wedding organizer' contrasted with seventh line 'he's a CEO'",
        "text_evidence": "this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and CEO are translated with male ones.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Hungarian source text on left panel",
      "English translations on right panel",
      "Translation pairs showing 'nurse' with 'she's'",
      "Translation pairs showing 'engineer' with 'he is'",
      "Translation pairs showing 'CEO' with 'he's'",
      "Translation pairs showing 'baker' with 'she's'",
      "Translation pairs showing 'wedding organizer' with 'She is'",
      "Google Translate interface",
      "Character count 110/5000"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg",
    "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Stacked horizontal bar chart with X=0 showing tallest bar (~680 occupations)\",\n    \"Service category (cyan) dominates the leftmost bar at X=0\",\n    \"STEM category (olive/yellow-green) concentrated at X=0\",\n    \"Production (turquoise) and Healthcare (purple) show wider spread across X-axis\",\n    \"Y-axis shows Occupations (0-800), X-axis shows # Translated Female Pronouns (0-12)\",\n    \"Color legend with 10 categories including Service, STEM, Farming/Fishing/Forestry, Corporate, Healthcare, Legal, Arts/Entertainment, Education, Production, Construction/Extraction\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the tallest bar at X=0 reach approximately 680 occupations when STEM fields are nearly exclusively concentrated there?\",\n      \"answer\": \"The concentration of STEM fields at X=0 indicates minimal translation to female pronouns, contributing significantly to the tall bar representing occupations with zero female pronoun translations across languages.\",\n      \"visual_anchor\": \"Tallest stacked bar at X=0 reaching approximately 680 occupations with STEM (olive) visible in upper portion\",\n      \"text_evidence\": \"STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the wider horizontal spread of Production turquoise bars compared to STEM olive bars support an inverse distribution claim?\",\n      \"answer\": \"Yes, Production occupations extend across multiple X values (0-5) while STEM remains concentrated at X=0, demonstrating the inverse distribution where some categories have broader female pronoun translation while others have minimal representation.\",\n      \"visual_anchor\": \"Production category (turquoise) bars visible from X=0 to approximately X=5, contrasted with STEM (olive) concentrated at X=0\",\n      \"text_evidence\": \"The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does Healthcare purple segment visibility across multiple X positions relate to the 73.0% female participation in Education?\",\n      \"answer\": \"Healthcare's distribution across X values suggests more balanced female pronoun translation, similar to Education's high female participation rate, indicating that occupations with higher female representation show less concentrated distribution at X=0.\",\n      \"visual_anchor\": \"Healthcare (purple) segments visible across X=0 to X=4 positions in stacked bars\",\n      \"text_evidence\": \"Education, training, and library\\tEducation\\t22\\t73.0%\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Stacked horizontal bar chart with X=0 showing tallest bar (~680 occupations)",
        "Service category (cyan) dominates the leftmost bar at X=0",
        "STEM category (olive/yellow-green) concentrated at X=0",
        "Production (turquoise) and Healthcare (purple) show wider spread across X-axis",
        "Y-axis shows Occupations (0-800), X-axis shows # Translated Female Pronouns (0-12)",
        "Color legend with 10 categories including Service, STEM, Farming/Fishing/Forestry, Corporate, Healthcare, Legal, Arts/Entertainment, Education, Production, Construction/Extraction"
      ],
      "queries": [
        {
          "query": "Why does the tallest bar at X=0 reach approximately 680 occupations when STEM fields are nearly exclusively concentrated there?",
          "answer": "The concentration of STEM fields at X=0 indicates minimal translation to female pronouns, contributing significantly to the tall bar representing occupations with zero female pronoun translations across languages.",
          "visual_anchor": "Tallest stacked bar at X=0 reaching approximately 680 occupations with STEM (olive) visible in upper portion",
          "text_evidence": "STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the wider horizontal spread of Production turquoise bars compared to STEM olive bars support an inverse distribution claim?",
          "answer": "Yes, Production occupations extend across multiple X values (0-5) while STEM remains concentrated at X=0, demonstrating the inverse distribution where some categories have broader female pronoun translation while others have minimal representation.",
          "visual_anchor": "Production category (turquoise) bars visible from X=0 to approximately X=5, contrasted with STEM (olive) concentrated at X=0",
          "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does Healthcare purple segment visibility across multiple X positions relate to the 73.0% female participation in Education?",
          "answer": "Healthcare's distribution across X values suggests more balanced female pronoun translation, similar to Education's high female participation rate, indicating that occupations with higher female representation show less concentrated distribution at X=0.",
          "visual_anchor": "Healthcare (purple) segments visible across X=0 to X=4 positions in stacked bars",
          "text_evidence": "Education, training, and library\tEducation\t22\t73.0%",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the tallest bar at X=0 reach approximately 680 occupations when STEM fields are nearly exclusively concentrated there?",
        "answer": "The concentration of STEM fields at X=0 indicates minimal translation to female pronouns, contributing significantly to the tall bar representing occupations with zero female pronoun translations across languages.",
        "visual_anchor": "Tallest stacked bar at X=0 reaching approximately 680 occupations with STEM (olive) visible in upper portion",
        "text_evidence": "STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the wider horizontal spread of Production turquoise bars compared to STEM olive bars support an inverse distribution claim?",
        "answer": "Yes, Production occupations extend across multiple X values (0-5) while STEM remains concentrated at X=0, demonstrating the inverse distribution where some categories have broader female pronoun translation while others have minimal representation.",
        "visual_anchor": "Production category (turquoise) bars visible from X=0 to approximately X=5, contrasted with STEM (olive) concentrated at X=0",
        "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does Healthcare purple segment visibility across multiple X positions relate to the 73.0% female participation in Education?",
        "answer": "Healthcare's distribution across X values suggests more balanced female pronoun translation, similar to Education's high female participation rate, indicating that occupations with higher female representation show less concentrated distribution at X=0.",
        "visual_anchor": "Healthcare (purple) segments visible across X=0 to X=4 positions in stacked bars",
        "text_evidence": "Education, training, and library\tEducation\t22\t73.0%",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Stacked horizontal bar chart with X=0 showing tallest bar (~680 occupations)",
      "Service category (cyan) dominates the leftmost bar at X=0",
      "STEM category (olive/yellow-green) concentrated at X=0",
      "Production (turquoise) and Healthcare (purple) show wider spread across X-axis",
      "Y-axis shows Occupations (0-800), X-axis shows # Translated Female Pronouns (0-12)",
      "Color legend with 10 categories including Service, STEM, Farming/Fishing/Forestry, Corporate, Healthcare, Legal, Arts/Entertainment, Education, Production, Construction/Extraction"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg",
    "caption": "Figure 3: In contrast to Figure   ",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"stacked bar chart with occupation counts on y-axis\", \"x-axis showing number of translated male pronouns grouped by language\", \"color-coded categories including Service (cyan), STEM (yellow-green), Corporate (teal), Healthcare (purple)\", \"peak distribution around x=6\", \"STEM fields concentrated to the right (x≥6)\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do STEM fields shown in yellow-green concentrate mainly at x≥6 in the distribution?\",\n      \"answer\": \"STEM fields concentrate to the right (x≥6) because male pronouns in these occupations show a skew normal distribution with a peak at x=6, indicating higher male pronoun usage in STEM translations across languages.\",\n      \"visual_anchor\": \"yellow-green STEM bars concentrated to the right at x≥6\",\n      \"text_evidence\": \"One can see how STEM fields concentrate mainly to the right ( $X \\\\geq 6$ ).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the peak at x=6 in the stacked bars support the skew normal distribution claim?\",\n      \"answer\": \"Yes, the visible peak around x=6 in the stacked bar heights supports the skew normal distribution pattern described for male pronouns, with occupations clustering at this value before tapering off.\",\n      \"visual_anchor\": \"peak in total bar height at x=6\",\n      \"text_evidence\": \"male pronouns are seemingly skew normally distributed, with a peak at $X = 6$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the male pronoun distribution pattern extending rightward contrast with the female pronoun pattern mentioned?\",\n      \"answer\": \"The male pronoun distribution shows a skew normal pattern extending to higher values (x≥6), while female pronouns follow an inversely distributed pattern, suggesting opposite gender bias trends in translation across occupations.\",\n      \"visual_anchor\": \"distribution extending to x=10 with substantial bars at higher values\",\n      \"text_evidence\": \"The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "stacked bar chart with occupation counts on y-axis",
        "x-axis showing number of translated male pronouns grouped by language",
        "color-coded categories including Service (cyan), STEM (yellow-green), Corporate (teal), Healthcare (purple)",
        "peak distribution around x=6",
        "STEM fields concentrated to the right (x≥6)"
      ],
      "queries": [
        {
          "query": "Why do STEM fields shown in yellow-green concentrate mainly at x≥6 in the distribution?",
          "answer": "STEM fields concentrate to the right (x≥6) because male pronouns in these occupations show a skew normal distribution with a peak at x=6, indicating higher male pronoun usage in STEM translations across languages.",
          "visual_anchor": "yellow-green STEM bars concentrated to the right at x≥6",
          "text_evidence": "One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the peak at x=6 in the stacked bars support the skew normal distribution claim?",
          "answer": "Yes, the visible peak around x=6 in the stacked bar heights supports the skew normal distribution pattern described for male pronouns, with occupations clustering at this value before tapering off.",
          "visual_anchor": "peak in total bar height at x=6",
          "text_evidence": "male pronouns are seemingly skew normally distributed, with a peak at $X = 6$",
          "query_type": "value_context"
        },
        {
          "query": "How does the male pronoun distribution pattern extending rightward contrast with the female pronoun pattern mentioned?",
          "answer": "The male pronoun distribution shows a skew normal pattern extending to higher values (x≥6), while female pronouns follow an inversely distributed pattern, suggesting opposite gender bias trends in translation across occupations.",
          "visual_anchor": "distribution extending to x=10 with substantial bars at higher values",
          "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do STEM fields shown in yellow-green concentrate mainly at x≥6 in the distribution?",
        "answer": "STEM fields concentrate to the right (x≥6) because male pronouns in these occupations show a skew normal distribution with a peak at x=6, indicating higher male pronoun usage in STEM translations across languages.",
        "visual_anchor": "yellow-green STEM bars concentrated to the right at x≥6",
        "text_evidence": "One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the peak at x=6 in the stacked bars support the skew normal distribution claim?",
        "answer": "Yes, the visible peak around x=6 in the stacked bar heights supports the skew normal distribution pattern described for male pronouns, with occupations clustering at this value before tapering off.",
        "visual_anchor": "peak in total bar height at x=6",
        "text_evidence": "male pronouns are seemingly skew normally distributed, with a peak at $X = 6$",
        "query_type": "value_context"
      },
      {
        "query": "How does the male pronoun distribution pattern extending rightward contrast with the female pronoun pattern mentioned?",
        "answer": "The male pronoun distribution shows a skew normal pattern extending to higher values (x≥6), while female pronouns follow an inversely distributed pattern, suggesting opposite gender bias trends in translation across occupations.",
        "visual_anchor": "distribution extending to x=10 with substantial bars at higher values",
        "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "stacked bar chart with occupation counts on y-axis",
      "x-axis showing number of translated male pronouns grouped by language",
      "color-coded categories including Service (cyan), STEM (yellow-green), Corporate (teal), Healthcare (purple)",
      "peak distribution around x=6",
      "STEM fields concentrated to the right (x≥6)"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg",
    "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Stacked horizontal bar chart\", \"X-axis showing # Translated Neutral Pronouns (0-12)\", \"Y-axis showing Occupations\", \"Color-coded categories including Service (cyan), STEM (yellow-green), Farming/Fishing/Forestry (pink), Corporate (teal), Healthcare (purple), Legal (red), Arts Entertainment (gray), Education (green), Production (turquoise), Construction Extraction (magenta)\", \"Tallest bar at X=0 reaching ~465 occupations\", \"Decreasing bar heights as X increases\", \"STEM category prominently visible in leftmost bars\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the tallest bar at X=0 reaching approximately 465 occupations validate that STEM fields predominantly concentrate at X=0?\",\n      \"answer\": \"The histogram shows the highest concentration of occupations at X=0, and STEM (yellow-green) is visibly prominent in this tallest bar, directly supporting the observation that STEM fields have minimal gender-neutral pronoun usage.\",\n      \"visual_anchor\": \"Tallest bar at X=0 reaching approximately 465 occupations with visible STEM (yellow-green) segment\",\n      \"text_evidence\": \"The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the rapid decrease from 465 occupations at X=0 to under 50 at X=4 illustrate the inverse distribution compared to male pronouns?\",\n      \"answer\": \"The steep decline in occupation counts as X increases demonstrates an inverse pattern, where gender-neutral pronouns are scarce (concentrated at low X values), contrasting with male pronouns which peak at X=6 in a skew normal distribution.\",\n      \"visual_anchor\": \"Decreasing bar heights from ~465 at X=0 to approximately 50 at X=4\",\n      \"text_evidence\": \"The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the absence of visible bars beyond X=5 contrast with male pronouns extending to higher values as described?\",\n      \"answer\": \"Yes, the histogram shows virtually no occupations with neutral pronoun counts beyond X=5, which sharply contrasts with the stated pattern for male pronouns that extend to higher X values, reflecting fundamental gender-neutral pronoun scarcity.\",\n      \"visual_anchor\": \"Minimal to no visible bars beyond X=5 on the horizontal axis\",\n      \"text_evidence\": \"In contrast to Figure   \\n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\\\geq 6$ )\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Stacked horizontal bar chart",
        "X-axis showing # Translated Neutral Pronouns (0-12)",
        "Y-axis showing Occupations",
        "Color-coded categories including Service (cyan), STEM (yellow-green), Farming/Fishing/Forestry (pink), Corporate (teal), Healthcare (purple), Legal (red), Arts Entertainment (gray), Education (green), Production (turquoise), Construction Extraction (magenta)",
        "Tallest bar at X=0 reaching ~465 occupations",
        "Decreasing bar heights as X increases",
        "STEM category prominently visible in leftmost bars"
      ],
      "queries": [
        {
          "query": "Why does the tallest bar at X=0 reaching approximately 465 occupations validate that STEM fields predominantly concentrate at X=0?",
          "answer": "The histogram shows the highest concentration of occupations at X=0, and STEM (yellow-green) is visibly prominent in this tallest bar, directly supporting the observation that STEM fields have minimal gender-neutral pronoun usage.",
          "visual_anchor": "Tallest bar at X=0 reaching approximately 465 occupations with visible STEM (yellow-green) segment",
          "text_evidence": "The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$",
          "query_type": "value_context"
        },
        {
          "query": "How does the rapid decrease from 465 occupations at X=0 to under 50 at X=4 illustrate the inverse distribution compared to male pronouns?",
          "answer": "The steep decline in occupation counts as X increases demonstrates an inverse pattern, where gender-neutral pronouns are scarce (concentrated at low X values), contrasting with male pronouns which peak at X=6 in a skew normal distribution.",
          "visual_anchor": "Decreasing bar heights from ~465 at X=0 to approximately 50 at X=4",
          "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the absence of visible bars beyond X=5 contrast with male pronouns extending to higher values as described?",
          "answer": "Yes, the histogram shows virtually no occupations with neutral pronoun counts beyond X=5, which sharply contrasts with the stated pattern for male pronouns that extend to higher X values, reflecting fundamental gender-neutral pronoun scarcity.",
          "visual_anchor": "Minimal to no visible bars beyond X=5 on the horizontal axis",
          "text_evidence": "In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ )",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the tallest bar at X=0 reaching approximately 465 occupations validate that STEM fields predominantly concentrate at X=0?",
        "answer": "The histogram shows the highest concentration of occupations at X=0, and STEM (yellow-green) is visibly prominent in this tallest bar, directly supporting the observation that STEM fields have minimal gender-neutral pronoun usage.",
        "visual_anchor": "Tallest bar at X=0 reaching approximately 465 occupations with visible STEM (yellow-green) segment",
        "text_evidence": "The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$",
        "query_type": "value_context"
      },
      {
        "query": "How does the rapid decrease from 465 occupations at X=0 to under 50 at X=4 illustrate the inverse distribution compared to male pronouns?",
        "answer": "The steep decline in occupation counts as X increases demonstrates an inverse pattern, where gender-neutral pronouns are scarce (concentrated at low X values), contrasting with male pronouns which peak at X=6 in a skew normal distribution.",
        "visual_anchor": "Decreasing bar heights from ~465 at X=0 to approximately 50 at X=4",
        "text_evidence": "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the absence of visible bars beyond X=5 contrast with male pronouns extending to higher values as described?",
        "answer": "Yes, the histogram shows virtually no occupations with neutral pronoun counts beyond X=5, which sharply contrasts with the stated pattern for male pronouns that extend to higher X values, reflecting fundamental gender-neutral pronoun scarcity.",
        "visual_anchor": "Minimal to no visible bars beyond X=5 on the horizontal axis",
        "text_evidence": "In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ )",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Stacked horizontal bar chart",
      "X-axis showing # Translated Neutral Pronouns (0-12)",
      "Y-axis showing Occupations",
      "Color-coded categories including Service (cyan), STEM (yellow-green), Farming/Fishing/Forestry (pink), Corporate (teal), Healthcare (purple), Legal (red), Arts Entertainment (gray), Education (green), Production (turquoise), Construction Extraction (magenta)",
      "Tallest bar at X=0 reaching ~465 occupations",
      "Decreasing bar heights as X increases",
      "STEM category prominently visible in leftmost bars"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_5",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg",
    "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Cyan bar at x=0 reaching y≈75\",\n    \"Pink bar at x=0 reaching y≈57\",\n    \"Olive/yellow bars distributed across x=2 to x=11\",\n    \"Pink bar at x=2 reaching y≈16\",\n    \"Olive bar at x=7 reaching y≈23\",\n    \"Legend showing Female (cyan), Male (olive), Neutral (pink)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the cyan bar at x=0 reach 75 occupations when male defaults are second-most prominent in STEM?\",\n      \"answer\": \"The histogram shows translated pronouns grouped among languages, not individual language translations. The high female count at x=0 indicates many STEM occupations where no languages produced female pronouns, consistent with male defaults being prominent.\",\n      \"visual_anchor\": \"Cyan bar at x=0 reaching y≈75\",\n      \"text_evidence\": \"job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the pink bar height of 57 at x=0 compare to the olive distribution, given Legal has more male defaults?\",\n      \"answer\": \"The pink neutral bar at 57 is lower than the cyan female bar at 75, while olive male bars are distributed across higher x-values. This suggests STEM has fewer neutral translations but still substantial gender-neutral defaults, though less male-biased than Legal.\",\n      \"visual_anchor\": \"Pink bar at x=0 reaching y≈57\",\n      \"text_evidence\": \"in which male defaults are the second-to-most prominent (after Legal)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the olive bar peak at x=7 with height 23 align with the claim about translated male pronouns totaled among languages?\",\n      \"answer\": \"Yes, the olive bar at x=7 shows 23 occupations received male pronouns in 7 languages. This distribution pattern confirms the histogram represents pronouns totaled among languages, with most male translations clustering in the 6-9 language range.\",\n      \"visual_anchor\": \"Olive bar at x=7 reaching y≈23\",\n      \"text_evidence\": \"Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Cyan bar at x=0 reaching y≈75",
        "Pink bar at x=0 reaching y≈57",
        "Olive/yellow bars distributed across x=2 to x=11",
        "Pink bar at x=2 reaching y≈16",
        "Olive bar at x=7 reaching y≈23",
        "Legend showing Female (cyan), Male (olive), Neutral (pink)"
      ],
      "queries": [
        {
          "query": "Why does the cyan bar at x=0 reach 75 occupations when male defaults are second-most prominent in STEM?",
          "answer": "The histogram shows translated pronouns grouped among languages, not individual language translations. The high female count at x=0 indicates many STEM occupations where no languages produced female pronouns, consistent with male defaults being prominent.",
          "visual_anchor": "Cyan bar at x=0 reaching y≈75",
          "text_evidence": "job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the pink bar height of 57 at x=0 compare to the olive distribution, given Legal has more male defaults?",
          "answer": "The pink neutral bar at 57 is lower than the cyan female bar at 75, while olive male bars are distributed across higher x-values. This suggests STEM has fewer neutral translations but still substantial gender-neutral defaults, though less male-biased than Legal.",
          "visual_anchor": "Pink bar at x=0 reaching y≈57",
          "text_evidence": "in which male defaults are the second-to-most prominent (after Legal)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the olive bar peak at x=7 with height 23 align with the claim about translated male pronouns totaled among languages?",
          "answer": "Yes, the olive bar at x=7 shows 23 occupations received male pronouns in 7 languages. This distribution pattern confirms the histogram represents pronouns totaled among languages, with most male translations clustering in the 6-9 language range.",
          "visual_anchor": "Olive bar at x=7 reaching y≈23",
          "text_evidence": "Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the cyan bar at x=0 reach 75 occupations when male defaults are second-most prominent in STEM?",
        "answer": "The histogram shows translated pronouns grouped among languages, not individual language translations. The high female count at x=0 indicates many STEM occupations where no languages produced female pronouns, consistent with male defaults being prominent.",
        "visual_anchor": "Cyan bar at x=0 reaching y≈75",
        "text_evidence": "job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the pink bar height of 57 at x=0 compare to the olive distribution, given Legal has more male defaults?",
        "answer": "The pink neutral bar at 57 is lower than the cyan female bar at 75, while olive male bars are distributed across higher x-values. This suggests STEM has fewer neutral translations but still substantial gender-neutral defaults, though less male-biased than Legal.",
        "visual_anchor": "Pink bar at x=0 reaching y≈57",
        "text_evidence": "in which male defaults are the second-to-most prominent (after Legal)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the olive bar peak at x=7 with height 23 align with the claim about translated male pronouns totaled among languages?",
        "answer": "Yes, the olive bar at x=7 shows 23 occupations received male pronouns in 7 languages. This distribution pattern confirms the histogram represents pronouns totaled among languages, with most male translations clustering in the 6-9 language range.",
        "visual_anchor": "Olive bar at x=7 reaching y≈23",
        "text_evidence": "Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Cyan bar at x=0 reaching y≈75",
      "Pink bar at x=0 reaching y≈57",
      "Olive/yellow bars distributed across x=2 to x=11",
      "Pink bar at x=2 reaching y≈16",
      "Olive bar at x=7 reaching y≈23",
      "Legend showing Female (cyan), Male (olive), Neutral (pink)"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_6",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg",
    "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"pink bars showing neutral pronoun distribution\", \"blue bars showing female pronoun distribution\", \"yellow bars showing male pronoun distribution\", \"x-axis showing number of translated pronouns grouped among languages (0-12)\", \"y-axis showing occupations count (0-30)\", \"peak pink bar at x=1 reaching ~28 occupations\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the pink neutral bar peak at 28 occupations near x=1 in Healthcare where male defaults are least prominent?\",\n      \"answer\": \"In Healthcare, where male defaults are least prominent, most occupations translate to gender-neutral pronouns across languages rather than defaulting to male pronouns, resulting in the highest concentration of neutral translations.\",\n      \"visual_anchor\": \"pink bar at x=1 reaching approximately 28 occupations\",\n      \"text_evidence\": \"Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the yellow male pronoun distribution extending to x=13 relate to broader occupation category patterns summarized in tables?\",\n      \"answer\": \"The male pronoun distribution spans the widest range to x=13, indicating that even in Healthcare with least male defaults, some occupations still translate to male pronouns across many languages. This variation is captured in the coalesced broader occupation groups.\",\n      \"visual_anchor\": \"yellow bars extending from x=3 to x=13\",\n      \"text_evidence\": \"Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the blue female bar reaching only 19 at x=1 versus pink at 28 support detailed occupation-level data summarization?\",\n      \"answer\": \"Yes, the lower female pronoun peak (19) compared to neutral (28) at x=1 shows gender-specific translation patterns vary by occupation within Healthcare, justifying the detailed summarization approach for individual occupations.\",\n      \"visual_anchor\": \"blue bar at x=1 reaching approximately 19 occupations compared to pink bar at 28\",\n      \"text_evidence\": \"Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "pink bars showing neutral pronoun distribution",
        "blue bars showing female pronoun distribution",
        "yellow bars showing male pronoun distribution",
        "x-axis showing number of translated pronouns grouped among languages (0-12)",
        "y-axis showing occupations count (0-30)",
        "peak pink bar at x=1 reaching ~28 occupations"
      ],
      "queries": [
        {
          "query": "Why does the pink neutral bar peak at 28 occupations near x=1 in Healthcare where male defaults are least prominent?",
          "answer": "In Healthcare, where male defaults are least prominent, most occupations translate to gender-neutral pronouns across languages rather than defaulting to male pronouns, resulting in the highest concentration of neutral translations.",
          "visual_anchor": "pink bar at x=1 reaching approximately 28 occupations",
          "text_evidence": "Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the yellow male pronoun distribution extending to x=13 relate to broader occupation category patterns summarized in tables?",
          "answer": "The male pronoun distribution spans the widest range to x=13, indicating that even in Healthcare with least male defaults, some occupations still translate to male pronouns across many languages. This variation is captured in the coalesced broader occupation groups.",
          "visual_anchor": "yellow bars extending from x=3 to x=13",
          "text_evidence": "Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
          "query_type": "value_context"
        },
        {
          "query": "Does the blue female bar reaching only 19 at x=1 versus pink at 28 support detailed occupation-level data summarization?",
          "answer": "Yes, the lower female pronoun peak (19) compared to neutral (28) at x=1 shows gender-specific translation patterns vary by occupation within Healthcare, justifying the detailed summarization approach for individual occupations.",
          "visual_anchor": "blue bar at x=1 reaching approximately 19 occupations compared to pink bar at 28",
          "text_evidence": "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the pink neutral bar peak at 28 occupations near x=1 in Healthcare where male defaults are least prominent?",
        "answer": "In Healthcare, where male defaults are least prominent, most occupations translate to gender-neutral pronouns across languages rather than defaulting to male pronouns, resulting in the highest concentration of neutral translations.",
        "visual_anchor": "pink bar at x=1 reaching approximately 28 occupations",
        "text_evidence": "Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the yellow male pronoun distribution extending to x=13 relate to broader occupation category patterns summarized in tables?",
        "answer": "The male pronoun distribution spans the widest range to x=13, indicating that even in Healthcare with least male defaults, some occupations still translate to male pronouns across many languages. This variation is captured in the coalesced broader occupation groups.",
        "visual_anchor": "yellow bars extending from x=3 to x=13",
        "text_evidence": "Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "query_type": "value_context"
      },
      {
        "query": "Does the blue female bar reaching only 19 at x=1 versus pink at 28 support detailed occupation-level data summarization?",
        "answer": "Yes, the lower female pronoun peak (19) compared to neutral (28) at x=1 shows gender-specific translation patterns vary by occupation within Healthcare, justifying the detailed summarization approach for individual occupations.",
        "visual_anchor": "blue bar at x=1 reaching approximately 19 occupations compared to pink bar at 28",
        "text_evidence": "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "pink bars showing neutral pronoun distribution",
      "blue bars showing female pronoun distribution",
      "yellow bars showing male pronoun distribution",
      "x-axis showing number of translated pronouns grouped among languages (0-12)",
      "y-axis showing occupations count (0-30)",
      "peak pink bar at x=1 reaching ~28 occupations"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_7",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg",
    "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Stacked bar chart with pink (Male), yellow (Female), and cyan (Neutral) segments\",\n    \"Ten category bars: Healthcare, Production, Education, Farming Fishing Forestry, Service, Construction Extraction, Corporate, Arts Entertainment, STEM, Legal\",\n    \"Y-axis showing percentage from 0 to 100\",\n    \"Legal bar showing largest cyan (Neutral) segment around 25-30%\",\n    \"Healthcare and Education bars showing larger yellow (Female) segments compared to other categories\",\n    \"Male (pink) segments dominating most bars, especially Legal and STEM\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do Legal and STEM bars show taller pink segments than Healthcare's pink segment, confirming predominance patterns?\",\n      \"answer\": \"Legal and STEM fields exhibit a predominance of male defaults, while Healthcare shows a larger proportion of female and neutral pronouns, resulting in shorter pink (male) segments for Healthcare compared to Legal and STEM.\",\n      \"visual_anchor\": \"Pink (Male) segment height comparison between Legal/STEM bars versus Healthcare bar\",\n      \"text_evidence\": \"Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the bar heights reaching near 100% validate the observation that categories were sorted by male, female, neutral proportions?\",\n      \"answer\": \"The bars do not reach exactly 100% because there is a fair amount of translated sentences for which no gender pronoun could be obtained. Categories are indeed sorted with respect to the proportions of male, female and neutral translated pronouns respectively.\",\n      \"visual_anchor\": \"Bar heights stopping below the 100% gridline across all categories\",\n      \"text_evidence\": \"Note that in general the bars do not add up to $1 0 0 \\\\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How do the yellow and cyan segments in Education's bar help visualize the broader occupation groupings mentioned?\",\n      \"answer\": \"The bar plots visualize how much of each occupation category's distribution is composed of female (yellow), male (pink), and gender-neutral (cyan) pronouns. Education's larger yellow and cyan segments illustrate the coalesced groupings that ease interpretation of gender pronoun distributions.\",\n      \"visual_anchor\": \"Yellow (Female) and cyan (Neutral) segment sizes in the Education category bar\",\n      \"text_evidence\": \"The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Stacked bar chart with pink (Male), yellow (Female), and cyan (Neutral) segments",
        "Ten category bars: Healthcare, Production, Education, Farming Fishing Forestry, Service, Construction Extraction, Corporate, Arts Entertainment, STEM, Legal",
        "Y-axis showing percentage from 0 to 100",
        "Legal bar showing largest cyan (Neutral) segment around 25-30%",
        "Healthcare and Education bars showing larger yellow (Female) segments compared to other categories",
        "Male (pink) segments dominating most bars, especially Legal and STEM"
      ],
      "queries": [
        {
          "query": "Why do Legal and STEM bars show taller pink segments than Healthcare's pink segment, confirming predominance patterns?",
          "answer": "Legal and STEM fields exhibit a predominance of male defaults, while Healthcare shows a larger proportion of female and neutral pronouns, resulting in shorter pink (male) segments for Healthcare compared to Legal and STEM.",
          "visual_anchor": "Pink (Male) segment height comparison between Legal/STEM bars versus Healthcare bar",
          "text_evidence": "Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the bar heights reaching near 100% validate the observation that categories were sorted by male, female, neutral proportions?",
          "answer": "The bars do not reach exactly 100% because there is a fair amount of translated sentences for which no gender pronoun could be obtained. Categories are indeed sorted with respect to the proportions of male, female and neutral translated pronouns respectively.",
          "visual_anchor": "Bar heights stopping below the 100% gridline across all categories",
          "text_evidence": "Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How do the yellow and cyan segments in Education's bar help visualize the broader occupation groupings mentioned?",
          "answer": "The bar plots visualize how much of each occupation category's distribution is composed of female (yellow), male (pink), and gender-neutral (cyan) pronouns. Education's larger yellow and cyan segments illustrate the coalesced groupings that ease interpretation of gender pronoun distributions.",
          "visual_anchor": "Yellow (Female) and cyan (Neutral) segment sizes in the Education category bar",
          "text_evidence": "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do Legal and STEM bars show taller pink segments than Healthcare's pink segment, confirming predominance patterns?",
        "answer": "Legal and STEM fields exhibit a predominance of male defaults, while Healthcare shows a larger proportion of female and neutral pronouns, resulting in shorter pink (male) segments for Healthcare compared to Legal and STEM.",
        "visual_anchor": "Pink (Male) segment height comparison between Legal/STEM bars versus Healthcare bar",
        "text_evidence": "Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the bar heights reaching near 100% validate the observation that categories were sorted by male, female, neutral proportions?",
        "answer": "The bars do not reach exactly 100% because there is a fair amount of translated sentences for which no gender pronoun could be obtained. Categories are indeed sorted with respect to the proportions of male, female and neutral translated pronouns respectively.",
        "visual_anchor": "Bar heights stopping below the 100% gridline across all categories",
        "text_evidence": "Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How do the yellow and cyan segments in Education's bar help visualize the broader occupation groupings mentioned?",
        "answer": "The bar plots visualize how much of each occupation category's distribution is composed of female (yellow), male (pink), and gender-neutral (cyan) pronouns. Education's larger yellow and cyan segments illustrate the coalesced groupings that ease interpretation of gender pronoun distributions.",
        "visual_anchor": "Yellow (Female) and cyan (Neutral) segment sizes in the Education category bar",
        "text_evidence": "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Stacked bar chart with pink (Male), yellow (Female), and cyan (Neutral) segments",
      "Ten category bars: Healthcare, Production, Education, Farming Fishing Forestry, Service, Construction Extraction, Corporate, Arts Entertainment, STEM, Legal",
      "Y-axis showing percentage from 0 to 100",
      "Legal bar showing largest cyan (Neutral) segment around 25-30%",
      "Healthcare and Education bars showing larger yellow (Female) segments compared to other categories",
      "Male (pink) segments dominating most bars, especially Legal and STEM"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_8",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg",
    "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"heatmap with color scale from blue (0%) to red (100%)\", \"12 languages on y-axis\", \"8 occupation categories on x-axis\", \"probability legend showing 0-80 range\", \"bottom right corner concentration of darker colors\", \"Finnish-Healthcare cell showing darkest red (~80%)\", \"Japanese-STEM cell showing dark blue (~0%)\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the bottom right corner show darker colors despite probabilities being sorted to concentrate there?\",\n      \"answer\": \"Both axes are sorted to concentrate higher female pronoun translation probabilities toward the bottom right corner, creating a gradient pattern from blue (low) in the top left to darker colors in the bottom right.\",\n      \"visual_anchor\": \"bottom right corner with darker red/orange colors (40-80% probability range)\",\n      \"text_evidence\": \"both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the blue color in Japanese-STEM cell indicate rejection of the complementary null hypothesis for that pair?\",\n      \"answer\": \"Yes, darker shades of blue indicate language-category pairs where the complementary null hypothesis is rejected, and Japanese-STEM shows a dark blue color near 0% probability.\",\n      \"visual_anchor\": \"Japanese-STEM cell in top-left region showing dark blue color (~0% probability)\",\n      \"text_evidence\": \"Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Which language-category pairs reach the red end of the scale showing close to one hundred percent female pronoun translation?\",\n      \"answer\": \"Finnish-Healthcare and Estonian-Healthcare show the darkest red colors, indicating probabilities near 100% for female pronoun translation in those language-occupation pairs.\",\n      \"visual_anchor\": \"red cells in bottom right corner, particularly Finnish-Healthcare and Estonian-Healthcare (~60-80% range)\",\n      \"text_evidence\": \"Probabilities range from 0% (blue) to $1 0 0 \\\\%$ (red)\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "heatmap with color scale from blue (0%) to red (100%)",
        "12 languages on y-axis",
        "8 occupation categories on x-axis",
        "probability legend showing 0-80 range",
        "bottom right corner concentration of darker colors",
        "Finnish-Healthcare cell showing darkest red (~80%)",
        "Japanese-STEM cell showing dark blue (~0%)"
      ],
      "queries": [
        {
          "query": "Why does the bottom right corner show darker colors despite probabilities being sorted to concentrate there?",
          "answer": "Both axes are sorted to concentrate higher female pronoun translation probabilities toward the bottom right corner, creating a gradient pattern from blue (low) in the top left to darker colors in the bottom right.",
          "visual_anchor": "bottom right corner with darker red/orange colors (40-80% probability range)",
          "text_evidence": "both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the blue color in Japanese-STEM cell indicate rejection of the complementary null hypothesis for that pair?",
          "answer": "Yes, darker shades of blue indicate language-category pairs where the complementary null hypothesis is rejected, and Japanese-STEM shows a dark blue color near 0% probability.",
          "visual_anchor": "Japanese-STEM cell in top-left region showing dark blue color (~0% probability)",
          "text_evidence": "Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue",
          "query_type": "value_context"
        },
        {
          "query": "Which language-category pairs reach the red end of the scale showing close to one hundred percent female pronoun translation?",
          "answer": "Finnish-Healthcare and Estonian-Healthcare show the darkest red colors, indicating probabilities near 100% for female pronoun translation in those language-occupation pairs.",
          "visual_anchor": "red cells in bottom right corner, particularly Finnish-Healthcare and Estonian-Healthcare (~60-80% range)",
          "text_evidence": "Probabilities range from 0% (blue) to $1 0 0 \\%$ (red)",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the bottom right corner show darker colors despite probabilities being sorted to concentrate there?",
        "answer": "Both axes are sorted to concentrate higher female pronoun translation probabilities toward the bottom right corner, creating a gradient pattern from blue (low) in the top left to darker colors in the bottom right.",
        "visual_anchor": "bottom right corner with darker red/orange colors (40-80% probability range)",
        "text_evidence": "both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the blue color in Japanese-STEM cell indicate rejection of the complementary null hypothesis for that pair?",
        "answer": "Yes, darker shades of blue indicate language-category pairs where the complementary null hypothesis is rejected, and Japanese-STEM shows a dark blue color near 0% probability.",
        "visual_anchor": "Japanese-STEM cell in top-left region showing dark blue color (~0% probability)",
        "text_evidence": "Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue",
        "query_type": "value_context"
      },
      {
        "query": "Which language-category pairs reach the red end of the scale showing close to one hundred percent female pronoun translation?",
        "answer": "Finnish-Healthcare and Estonian-Healthcare show the darkest red colors, indicating probabilities near 100% for female pronoun translation in those language-occupation pairs.",
        "visual_anchor": "red cells in bottom right corner, particularly Finnish-Healthcare and Estonian-Healthcare (~60-80% range)",
        "text_evidence": "Probabilities range from 0% (blue) to $1 0 0 \\%$ (red)",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "heatmap with color scale from blue (0%) to red (100%)",
      "12 languages on y-axis",
      "8 occupation categories on x-axis",
      "probability legend showing 0-80 range",
      "bottom right corner concentration of darker colors",
      "Finnish-Healthcare cell showing darkest red (~80%)",
      "Japanese-STEM cell showing dark blue (~0%)"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_9",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig8.jpg",
    "caption": "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Heatmap with color gradient from blue (0%) to red (100%)\",\n    \"12 languages on y-axis (Basque to Malay)\",\n    \"10 occupation categories on x-axis (Healthcare to Legal)\",\n    \"Dark red concentration in bottom right corner\",\n    \"Light blue values in top left corner for Basque-Healthcare\",\n    \"Deep red cell for Chinese-Legal intersection\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Malay show deep red across most categories if probabilities concentrate on the bottom right corner?\",\n      \"answer\": \"Malay is positioned at the bottom of the sorted y-axis, which is arranged to concentrate higher male pronoun translation probabilities toward the bottom right, explaining its consistently dark red coloration across categories.\",\n      \"visual_anchor\": \"Malay row showing deep red coloration across Healthcare, Production, Education, Service, and Legal categories\",\n      \"text_evidence\": \"both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the light blue cell for Basque-Healthcare align with the sorting strategy for translation probability ranges?\",\n      \"answer\": \"Yes, the light blue indicates near 0% male pronoun translation probability for Basque in Healthcare, consistent with the sorting that places lower probabilities away from the bottom right corner concentration area.\",\n      \"visual_anchor\": \"Light blue cell at Basque-Healthcare intersection in top left region\",\n      \"text_evidence\": \"Probabilities range from 0% (blue) to $1 0 0 \\\\%$ (red)\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the deep red Chinese-Legal cell relate to the male pronoun translation probability heatmap design?\",\n      \"answer\": \"The deep red indicates close to 100% male pronoun translation probability for Chinese in Legal occupations, positioned in the bottom right corner where the highest probabilities are concentrated by design.\",\n      \"visual_anchor\": \"Deep red cell at Chinese-Legal intersection in bottom right corner\",\n      \"text_evidence\": \"Heatmap for the translation probability into male pronouns for each pair of language and occupation category\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Heatmap with color gradient from blue (0%) to red (100%)",
        "12 languages on y-axis (Basque to Malay)",
        "10 occupation categories on x-axis (Healthcare to Legal)",
        "Dark red concentration in bottom right corner",
        "Light blue values in top left corner for Basque-Healthcare",
        "Deep red cell for Chinese-Legal intersection"
      ],
      "queries": [
        {
          "query": "Why does Malay show deep red across most categories if probabilities concentrate on the bottom right corner?",
          "answer": "Malay is positioned at the bottom of the sorted y-axis, which is arranged to concentrate higher male pronoun translation probabilities toward the bottom right, explaining its consistently dark red coloration across categories.",
          "visual_anchor": "Malay row showing deep red coloration across Healthcare, Production, Education, Service, and Legal categories",
          "text_evidence": "both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the light blue cell for Basque-Healthcare align with the sorting strategy for translation probability ranges?",
          "answer": "Yes, the light blue indicates near 0% male pronoun translation probability for Basque in Healthcare, consistent with the sorting that places lower probabilities away from the bottom right corner concentration area.",
          "visual_anchor": "Light blue cell at Basque-Healthcare intersection in top left region",
          "text_evidence": "Probabilities range from 0% (blue) to $1 0 0 \\%$ (red)",
          "query_type": "value_context"
        },
        {
          "query": "How does the deep red Chinese-Legal cell relate to the male pronoun translation probability heatmap design?",
          "answer": "The deep red indicates close to 100% male pronoun translation probability for Chinese in Legal occupations, positioned in the bottom right corner where the highest probabilities are concentrated by design.",
          "visual_anchor": "Deep red cell at Chinese-Legal intersection in bottom right corner",
          "text_evidence": "Heatmap for the translation probability into male pronouns for each pair of language and occupation category",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Malay show deep red across most categories if probabilities concentrate on the bottom right corner?",
        "answer": "Malay is positioned at the bottom of the sorted y-axis, which is arranged to concentrate higher male pronoun translation probabilities toward the bottom right, explaining its consistently dark red coloration across categories.",
        "visual_anchor": "Malay row showing deep red coloration across Healthcare, Production, Education, Service, and Legal categories",
        "text_evidence": "both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the light blue cell for Basque-Healthcare align with the sorting strategy for translation probability ranges?",
        "answer": "Yes, the light blue indicates near 0% male pronoun translation probability for Basque in Healthcare, consistent with the sorting that places lower probabilities away from the bottom right corner concentration area.",
        "visual_anchor": "Light blue cell at Basque-Healthcare intersection in top left region",
        "text_evidence": "Probabilities range from 0% (blue) to $1 0 0 \\%$ (red)",
        "query_type": "value_context"
      },
      {
        "query": "How does the deep red Chinese-Legal cell relate to the male pronoun translation probability heatmap design?",
        "answer": "The deep red indicates close to 100% male pronoun translation probability for Chinese in Legal occupations, positioned in the bottom right corner where the highest probabilities are concentrated by design.",
        "visual_anchor": "Deep red cell at Chinese-Legal intersection in bottom right corner",
        "text_evidence": "Heatmap for the translation probability into male pronouns for each pair of language and occupation category",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Heatmap with color gradient from blue (0%) to red (100%)",
      "12 languages on y-axis (Basque to Malay)",
      "10 occupation categories on x-axis (Healthcare to Legal)",
      "Dark red concentration in bottom right corner",
      "Light blue values in top left corner for Basque-Healthcare",
      "Deep red cell for Chinese-Legal intersection"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_10",
    "figure_number": 10,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg",
    "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Heatmap with blue-to-red color gradient (0% to 100%)\",\n    \"13 languages on y-axis (Malay to Basque)\",\n    \"10 occupation categories on x-axis (Education to Production)\",\n    \"Basque row showing red/dark red coloring across most categories\",\n    \"Blue concentration in upper-left corner (Malay through Armenian)\",\n    \"Turkish and Japanese rows showing mixed blue-teal coloring\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Basque show red coloring across Entertainment through Production categories when other languages display blue?\",\n      \"answer\": \"The heatmap is sorted to concentrate higher probabilities in the bottom right corner, so Basque's red coloring (near 100%) reflects its tendency to translate into gender neutral pronouns across those occupation categories, contrasting with other languages' low probabilities (blue, near 0%).\",\n      \"visual_anchor\": \"Basque row with red/dark red cells from Entertainment to Production versus blue cells in upper rows\",\n      \"text_evidence\": \"Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the blue concentration in Malay through Armenian rows support experimenting with a fair amount of different gender neutral languages?\",\n      \"answer\": \"Yes, the predominantly blue coloring in those language rows indicates very low probabilities of gender neutral pronoun translation, demonstrating variation across the multiple gender neutral languages tested in the experiment.\",\n      \"visual_anchor\": \"Blue-dominated rows for Malay, Finnish, Hungarian, Swahili, Estonian, and Armenian\",\n      \"text_evidence\": \"We have taken the care of experimenting with a fair amount of different gender neutral languages.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does Turkish show teal-to-light blue gradient from Education to Production if data is coalesced by language groups?\",\n      \"answer\": \"The gradient indicates varying translation probabilities across occupation categories for Turkish. Coalescing data by language groups helps visualize the effect of different cultures on gender bias genesis, explaining why Turkish shows moderate probabilities (teal/light blue) rather than uniform values.\",\n      \"visual_anchor\": \"Turkish row displaying gradient from darker teal on left to lighter blue/gray on right\",\n      \"text_evidence\": \"another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Heatmap with blue-to-red color gradient (0% to 100%)",
        "13 languages on y-axis (Malay to Basque)",
        "10 occupation categories on x-axis (Education to Production)",
        "Basque row showing red/dark red coloring across most categories",
        "Blue concentration in upper-left corner (Malay through Armenian)",
        "Turkish and Japanese rows showing mixed blue-teal coloring"
      ],
      "queries": [
        {
          "query": "Why does Basque show red coloring across Entertainment through Production categories when other languages display blue?",
          "answer": "The heatmap is sorted to concentrate higher probabilities in the bottom right corner, so Basque's red coloring (near 100%) reflects its tendency to translate into gender neutral pronouns across those occupation categories, contrasting with other languages' low probabilities (blue, near 0%).",
          "visual_anchor": "Basque row with red/dark red cells from Entertainment to Production versus blue cells in upper rows",
          "text_evidence": "Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the blue concentration in Malay through Armenian rows support experimenting with a fair amount of different gender neutral languages?",
          "answer": "Yes, the predominantly blue coloring in those language rows indicates very low probabilities of gender neutral pronoun translation, demonstrating variation across the multiple gender neutral languages tested in the experiment.",
          "visual_anchor": "Blue-dominated rows for Malay, Finnish, Hungarian, Swahili, Estonian, and Armenian",
          "text_evidence": "We have taken the care of experimenting with a fair amount of different gender neutral languages.",
          "query_type": "value_context"
        },
        {
          "query": "Why does Turkish show teal-to-light blue gradient from Education to Production if data is coalesced by language groups?",
          "answer": "The gradient indicates varying translation probabilities across occupation categories for Turkish. Coalescing data by language groups helps visualize the effect of different cultures on gender bias genesis, explaining why Turkish shows moderate probabilities (teal/light blue) rather than uniform values.",
          "visual_anchor": "Turkish row displaying gradient from darker teal on left to lighter blue/gray on right",
          "text_evidence": "another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Basque show red coloring across Entertainment through Production categories when other languages display blue?",
        "answer": "The heatmap is sorted to concentrate higher probabilities in the bottom right corner, so Basque's red coloring (near 100%) reflects its tendency to translate into gender neutral pronouns across those occupation categories, contrasting with other languages' low probabilities (blue, near 0%).",
        "visual_anchor": "Basque row with red/dark red cells from Entertainment to Production versus blue cells in upper rows",
        "text_evidence": "Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the blue concentration in Malay through Armenian rows support experimenting with a fair amount of different gender neutral languages?",
        "answer": "Yes, the predominantly blue coloring in those language rows indicates very low probabilities of gender neutral pronoun translation, demonstrating variation across the multiple gender neutral languages tested in the experiment.",
        "visual_anchor": "Blue-dominated rows for Malay, Finnish, Hungarian, Swahili, Estonian, and Armenian",
        "text_evidence": "We have taken the care of experimenting with a fair amount of different gender neutral languages.",
        "query_type": "value_context"
      },
      {
        "query": "Why does Turkish show teal-to-light blue gradient from Education to Production if data is coalesced by language groups?",
        "answer": "The gradient indicates varying translation probabilities across occupation categories for Turkish. Coalescing data by language groups helps visualize the effect of different cultures on gender bias genesis, explaining why Turkish shows moderate probabilities (teal/light blue) rather than uniform values.",
        "visual_anchor": "Turkish row displaying gradient from darker teal on left to lighter blue/gray on right",
        "text_evidence": "another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Heatmap with blue-to-red color gradient (0% to 100%)",
      "13 languages on y-axis (Malay to Basque)",
      "10 occupation categories on x-axis (Education to Production)",
      "Basque row showing red/dark red coloring across most categories",
      "Blue concentration in upper-left corner (Malay through Armenian)",
      "Turkish and Japanese rows showing mixed blue-teal coloring"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_11",
    "figure_number": 11,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg",
    "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Stacked bar chart with three colors: cyan (Neutral), yellow (Female), pink (Male)\",\n    \"Japanese bar showing ~0.2% female (yellow), ~65% male (pink), ~35% neutral (cyan)\",\n    \"Chinese bar showing ~2% female (yellow), ~50% male (pink), ~48% neutral (cyan)\",\n    \"Basque bar showing ~8% male (pink), ~56% neutral (cyan), ~36% female (yellow)\",\n    \"Bengali, Yoruba, and Basque bars with higher neutral proportions\",\n    \"Languages sorted left to right: Basque, Bengali, Yoruba, Chinese, Finnish, Hungarian, Turkish, Japanese, Estonian, Swahili, Armenian, Malay\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Japanese show only ~0.2% yellow female pronouns when female pronouns reach as low as 0.196% for this language?\",\n      \"answer\": \"The yellow segment in the Japanese bar visually confirms the extremely low female pronoun proportion of 0.196% mentioned, demonstrating the severe male default bias in Japanese translation outputs.\",\n      \"visual_anchor\": \"Yellow (Female) segment in Japanese bar, approximately 0.2% of total height\",\n      \"text_evidence\": \"The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does Basque's larger cyan neutral bar compared to pink male bar support its status as yielding more gender neutral than male pronouns?\",\n      \"answer\": \"Yes, Basque's cyan neutral segment (~56%) visually exceeds its pink male segment (~8%), directly confirming that it was the only tested language to yield more gender neutral than male pronouns.\",\n      \"visual_anchor\": \"Basque bar showing cyan (Neutral) segment larger than pink (Male) segment\",\n      \"text_evidence\": \"Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do bars not reach exactly 100% height, particularly for Basque's shorter bar compared to Japanese or Malay?\",\n      \"answer\": \"Not all bars add up to 100% because there are translated sentences for which no gender pronoun could be obtained, particularly in Basque. This extraction difficulty varies by language, making some bars shorter than others.\",\n      \"visual_anchor\": \"Basque bar height significantly below 100% gridline, shorter than Japanese and Malay bars\",\n      \"text_evidence\": \"Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Stacked bar chart with three colors: cyan (Neutral), yellow (Female), pink (Male)",
        "Japanese bar showing ~0.2% female (yellow), ~65% male (pink), ~35% neutral (cyan)",
        "Chinese bar showing ~2% female (yellow), ~50% male (pink), ~48% neutral (cyan)",
        "Basque bar showing ~8% male (pink), ~56% neutral (cyan), ~36% female (yellow)",
        "Bengali, Yoruba, and Basque bars with higher neutral proportions",
        "Languages sorted left to right: Basque, Bengali, Yoruba, Chinese, Finnish, Hungarian, Turkish, Japanese, Estonian, Swahili, Armenian, Malay"
      ],
      "queries": [
        {
          "query": "Why does Japanese show only ~0.2% yellow female pronouns when female pronouns reach as low as 0.196% for this language?",
          "answer": "The yellow segment in the Japanese bar visually confirms the extremely low female pronoun proportion of 0.196% mentioned, demonstrating the severe male default bias in Japanese translation outputs.",
          "visual_anchor": "Yellow (Female) segment in Japanese bar, approximately 0.2% of total height",
          "text_evidence": "The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.",
          "query_type": "value_context"
        },
        {
          "query": "Does Basque's larger cyan neutral bar compared to pink male bar support its status as yielding more gender neutral than male pronouns?",
          "answer": "Yes, Basque's cyan neutral segment (~56%) visually exceeds its pink male segment (~8%), directly confirming that it was the only tested language to yield more gender neutral than male pronouns.",
          "visual_anchor": "Basque bar showing cyan (Neutral) segment larger than pink (Male) segment",
          "text_evidence": "Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do bars not reach exactly 100% height, particularly for Basque's shorter bar compared to Japanese or Malay?",
          "answer": "Not all bars add up to 100% because there are translated sentences for which no gender pronoun could be obtained, particularly in Basque. This extraction difficulty varies by language, making some bars shorter than others.",
          "visual_anchor": "Basque bar height significantly below 100% gridline, shorter than Japanese and Malay bars",
          "text_evidence": "Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Japanese show only ~0.2% yellow female pronouns when female pronouns reach as low as 0.196% for this language?",
        "answer": "The yellow segment in the Japanese bar visually confirms the extremely low female pronoun proportion of 0.196% mentioned, demonstrating the severe male default bias in Japanese translation outputs.",
        "visual_anchor": "Yellow (Female) segment in Japanese bar, approximately 0.2% of total height",
        "text_evidence": "The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.",
        "query_type": "value_context"
      },
      {
        "query": "Does Basque's larger cyan neutral bar compared to pink male bar support its status as yielding more gender neutral than male pronouns?",
        "answer": "Yes, Basque's cyan neutral segment (~56%) visually exceeds its pink male segment (~8%), directly confirming that it was the only tested language to yield more gender neutral than male pronouns.",
        "visual_anchor": "Basque bar showing cyan (Neutral) segment larger than pink (Male) segment",
        "text_evidence": "Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do bars not reach exactly 100% height, particularly for Basque's shorter bar compared to Japanese or Malay?",
        "answer": "Not all bars add up to 100% because there are translated sentences for which no gender pronoun could be obtained, particularly in Basque. This extraction difficulty varies by language, making some bars shorter than others.",
        "visual_anchor": "Basque bar height significantly below 100% gridline, shorter than Japanese and Malay bars",
        "text_evidence": "Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Stacked bar chart with three colors: cyan (Neutral), yellow (Female), pink (Male)",
      "Japanese bar showing ~0.2% female (yellow), ~65% male (pink), ~35% neutral (cyan)",
      "Chinese bar showing ~2% female (yellow), ~50% male (pink), ~48% neutral (cyan)",
      "Basque bar showing ~8% male (pink), ~56% neutral (cyan), ~36% female (yellow)",
      "Bengali, Yoruba, and Basque bars with higher neutral proportions",
      "Languages sorted left to right: Basque, Bengali, Yoruba, Chinese, Finnish, Hungarian, Turkish, Japanese, Estonian, Swahili, Armenian, Malay"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_12",
    "figure_number": 12,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg",
    "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Pink bars representing Male gender pronouns\",\n    \"Yellow/olive bars representing Female gender pronouns\",\n    \"Cyan bars representing Neutral gender pronouns\",\n    \"Adjectives on x-axis: Happy, Shy, Desirable, Sad, Dumb, Mature, Smart, Polite, Sympathetic, Loving, Modest, Wrong, Afraid, Innocent, Strong, Successful, Right, Brave, Cruel, Guilty, Proud\",\n    \"Percentage scale on y-axis from 0 to 100\",\n    \"Stacked bar chart showing distribution of three gender categories\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do Shy and Desirable show over 45% yellow bars while Proud shows over 85% pink?\",\n      \"answer\": \"Adjectives such as Shy and Desirable amass at the female side of the spectrum, contrasting with Proud which is almost exclusively translated with male pronouns. This demonstrates how stereotypical gender roles influence automatic translation of simple adjectives.\",\n      \"visual_anchor\": \"Yellow bars (Female) dominating Shy and Desirable vs pink bars (Male) dominating Proud\",\n      \"text_evidence\": \"adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the cyan bars for Successful and Right exceeding 90% confirm filtering for human-applicable adjectives?\",\n      \"answer\": \"Yes, the high neutral pronoun percentages for these adjectives are consistent with the filtering process. The researchers filtered adjectives from the COCA corpus that would fit templates and make sense for describing a human being.\",\n      \"visual_anchor\": \"Cyan bars (Neutral) at approximately 90% for Successful and Right adjectives\",\n      \"text_evidence\": \"we queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus, but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Could the pink bars exceeding 80% for Brave and Guilty reflect male-dominated field demographics rather than bias?\",\n      \"answer\": \"This possibility is explicitly addressed as a sensible objection. The researchers acknowledge they must account for the possibility that gender pronoun statistics in Google Translate outputs merely reflect demographics of male-dominated fields.\",\n      \"visual_anchor\": \"Pink bars (Male) at approximately 80-85% for Brave and Guilty adjectives\",\n      \"text_evidence\": \"A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Pink bars representing Male gender pronouns",
        "Yellow/olive bars representing Female gender pronouns",
        "Cyan bars representing Neutral gender pronouns",
        "Adjectives on x-axis: Happy, Shy, Desirable, Sad, Dumb, Mature, Smart, Polite, Sympathetic, Loving, Modest, Wrong, Afraid, Innocent, Strong, Successful, Right, Brave, Cruel, Guilty, Proud",
        "Percentage scale on y-axis from 0 to 100",
        "Stacked bar chart showing distribution of three gender categories"
      ],
      "queries": [
        {
          "query": "Why do Shy and Desirable show over 45% yellow bars while Proud shows over 85% pink?",
          "answer": "Adjectives such as Shy and Desirable amass at the female side of the spectrum, contrasting with Proud which is almost exclusively translated with male pronouns. This demonstrates how stereotypical gender roles influence automatic translation of simple adjectives.",
          "visual_anchor": "Yellow bars (Female) dominating Shy and Desirable vs pink bars (Male) dominating Proud",
          "text_evidence": "adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the cyan bars for Successful and Right exceeding 90% confirm filtering for human-applicable adjectives?",
          "answer": "Yes, the high neutral pronoun percentages for these adjectives are consistent with the filtering process. The researchers filtered adjectives from the COCA corpus that would fit templates and make sense for describing a human being.",
          "visual_anchor": "Cyan bars (Neutral) at approximately 90% for Successful and Right adjectives",
          "text_evidence": "we queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus, but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being",
          "query_type": "value_context"
        },
        {
          "query": "Could the pink bars exceeding 80% for Brave and Guilty reflect male-dominated field demographics rather than bias?",
          "answer": "This possibility is explicitly addressed as a sensible objection. The researchers acknowledge they must account for the possibility that gender pronoun statistics in Google Translate outputs merely reflect demographics of male-dominated fields.",
          "visual_anchor": "Pink bars (Male) at approximately 80-85% for Brave and Guilty adjectives",
          "text_evidence": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do Shy and Desirable show over 45% yellow bars while Proud shows over 85% pink?",
        "answer": "Adjectives such as Shy and Desirable amass at the female side of the spectrum, contrasting with Proud which is almost exclusively translated with male pronouns. This demonstrates how stereotypical gender roles influence automatic translation of simple adjectives.",
        "visual_anchor": "Yellow bars (Female) dominating Shy and Desirable vs pink bars (Male) dominating Proud",
        "text_evidence": "adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the cyan bars for Successful and Right exceeding 90% confirm filtering for human-applicable adjectives?",
        "answer": "Yes, the high neutral pronoun percentages for these adjectives are consistent with the filtering process. The researchers filtered adjectives from the COCA corpus that would fit templates and make sense for describing a human being.",
        "visual_anchor": "Cyan bars (Neutral) at approximately 90% for Successful and Right adjectives",
        "text_evidence": "we queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus, but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being",
        "query_type": "value_context"
      },
      {
        "query": "Could the pink bars exceeding 80% for Brave and Guilty reflect male-dominated field demographics rather than bias?",
        "answer": "This possibility is explicitly addressed as a sensible objection. The researchers acknowledge they must account for the possibility that gender pronoun statistics in Google Translate outputs merely reflect demographics of male-dominated fields.",
        "visual_anchor": "Pink bars (Male) at approximately 80-85% for Brave and Guilty adjectives",
        "text_evidence": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Pink bars representing Male gender pronouns",
      "Yellow/olive bars representing Female gender pronouns",
      "Cyan bars representing Neutral gender pronouns",
      "Adjectives on x-axis: Happy, Shy, Desirable, Sad, Dumb, Mature, Smart, Polite, Sympathetic, Loving, Modest, Wrong, Afraid, Innocent, Strong, Successful, Right, Brave, Cruel, Guilty, Proud",
      "Percentage scale on y-axis from 0 to 100",
      "Stacked bar chart showing distribution of three gender categories"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_13",
    "figure_number": 13,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg",
    "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue bars showing Google Translate Female %\",\n    \"Yellow bars showing BLS Female Participation %\",\n    \"12 quantiles on x-axis\",\n    \"Frequency (%) on y-axis up to 40%\",\n    \"Peak blue bar at quantile 1 (~38%)\",\n    \"Higher yellow bars in later quantiles (3, 4)\",\n    \"Very low blue bars in quantiles 7-12\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the higher yellow bars in quantiles 3-4 correspond to increased blue bar heights, challenging the demographics hypothesis?\",\n      \"answer\": \"No, the blue bars in quantiles 3-4 show approximately 19% and 8.5% respectively, which do not increase proportionally with the yellow bars showing 10% and 23%. This lack of correlation does not support the hypothesis that Google Translate bias reflects female worker demographics.\",\n      \"visual_anchor\": \"Yellow bars at quantiles 3-4 (~10% and ~23%) compared to blue bars (~19% and ~8.5%)\",\n      \"text_evidence\": \"A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do quantiles 9-12 show near-zero blue bars despite yellow bars indicating 5-6% female participation rates?\",\n      \"answer\": \"The blue bars in quantiles 9-12 are nearly zero (less than 1%) while yellow bars show 5-6% female participation, demonstrating that Google Translate's female pronoun frequency does not respond to actual female worker proportions. This contradicts the hypothesis that translation outputs merely reflect job demographics.\",\n      \"visual_anchor\": \"Blue bars at quantiles 9-12 (near 0%) versus yellow bars (5-6%)\",\n      \"text_evidence\": \"the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the 38% blue peak at quantile 1 versus 18% yellow bar validate that male-dominated field statistics explain translation outputs?\",\n      \"answer\": \"No, the blue bar at quantile 1 reaches approximately 38% while the yellow bar shows only 18% female participation, indicating Google Translate over-represents female pronouns in this quantile. This inverse relationship contradicts the hypothesis that translation statistics merely reflect male-dominated field demographics.\",\n      \"visual_anchor\": \"Quantile 1 showing blue bar at ~38% and yellow bar at ~18%\",\n      \"text_evidence\": \"We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue bars showing Google Translate Female %",
        "Yellow bars showing BLS Female Participation %",
        "12 quantiles on x-axis",
        "Frequency (%) on y-axis up to 40%",
        "Peak blue bar at quantile 1 (~38%)",
        "Higher yellow bars in later quantiles (3, 4)",
        "Very low blue bars in quantiles 7-12"
      ],
      "queries": [
        {
          "query": "Do the higher yellow bars in quantiles 3-4 correspond to increased blue bar heights, challenging the demographics hypothesis?",
          "answer": "No, the blue bars in quantiles 3-4 show approximately 19% and 8.5% respectively, which do not increase proportionally with the yellow bars showing 10% and 23%. This lack of correlation does not support the hypothesis that Google Translate bias reflects female worker demographics.",
          "visual_anchor": "Yellow bars at quantiles 3-4 (~10% and ~23%) compared to blue bars (~19% and ~8.5%)",
          "text_evidence": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do quantiles 9-12 show near-zero blue bars despite yellow bars indicating 5-6% female participation rates?",
          "answer": "The blue bars in quantiles 9-12 are nearly zero (less than 1%) while yellow bars show 5-6% female participation, demonstrating that Google Translate's female pronoun frequency does not respond to actual female worker proportions. This contradicts the hypothesis that translation outputs merely reflect job demographics.",
          "visual_anchor": "Blue bars at quantiles 9-12 (near 0%) versus yellow bars (5-6%)",
          "text_evidence": "the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the 38% blue peak at quantile 1 versus 18% yellow bar validate that male-dominated field statistics explain translation outputs?",
          "answer": "No, the blue bar at quantile 1 reaches approximately 38% while the yellow bar shows only 18% female participation, indicating Google Translate over-represents female pronouns in this quantile. This inverse relationship contradicts the hypothesis that translation statistics merely reflect male-dominated field demographics.",
          "visual_anchor": "Quantile 1 showing blue bar at ~38% and yellow bar at ~18%",
          "text_evidence": "We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the higher yellow bars in quantiles 3-4 correspond to increased blue bar heights, challenging the demographics hypothesis?",
        "answer": "No, the blue bars in quantiles 3-4 show approximately 19% and 8.5% respectively, which do not increase proportionally with the yellow bars showing 10% and 23%. This lack of correlation does not support the hypothesis that Google Translate bias reflects female worker demographics.",
        "visual_anchor": "Yellow bars at quantiles 3-4 (~10% and ~23%) compared to blue bars (~19% and ~8.5%)",
        "text_evidence": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do quantiles 9-12 show near-zero blue bars despite yellow bars indicating 5-6% female participation rates?",
        "answer": "The blue bars in quantiles 9-12 are nearly zero (less than 1%) while yellow bars show 5-6% female participation, demonstrating that Google Translate's female pronoun frequency does not respond to actual female worker proportions. This contradicts the hypothesis that translation outputs merely reflect job demographics.",
        "visual_anchor": "Blue bars at quantiles 9-12 (near 0%) versus yellow bars (5-6%)",
        "text_evidence": "the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the 38% blue peak at quantile 1 versus 18% yellow bar validate that male-dominated field statistics explain translation outputs?",
        "answer": "No, the blue bar at quantile 1 reaches approximately 38% while the yellow bar shows only 18% female participation, indicating Google Translate over-represents female pronouns in this quantile. This inverse relationship contradicts the hypothesis that translation statistics merely reflect male-dominated field demographics.",
        "visual_anchor": "Quantile 1 showing blue bar at ~38% and yellow bar at ~18%",
        "text_evidence": "We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue bars showing Google Translate Female %",
      "Yellow bars showing BLS Female Participation %",
      "12 quantiles on x-axis",
      "Frequency (%) on y-axis up to 40%",
      "Peak blue bar at quantile 1 (~38%)",
      "Higher yellow bars in later quantiles (3, 4)",
      "Very low blue bars in quantiles 7-12"
    ]
  },
  {
    "doc_id": "1809.02208",
    "figure_id": "1809.02208_fig_14",
    "figure_number": 14,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg",
    "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Left interface labeled 'Before' showing single translation 'he is a doctor' in blue box\",\n    \"Right interface labeled 'After' (not fully shown in image)\",\n    \"Turkish input text 'o bir doktor'\",\n    \"Language selector showing 'TURKISH' and 'ENGLISH'\",\n    \"Blue translation result box with verification checkmark\",\n    \"Star icon and speaker icon in interface\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the left interface showing 'he is a doctor' represent the version analyzed in August 2018 experiments?\",\n      \"answer\": \"The experiments reported in the study were carried out in August 2018, capturing a snapshot of Google Translate at that time when it offered only one official translation for each input word.\",\n      \"visual_anchor\": \"Left interface labeled 'Before' with single translation 'he is a doctor' in blue box\",\n      \"text_evidence\": \"In this context, all experiments reported here offer an analysis of a \\\"screenshot\\\" of that tool as of August 2018, the moment they were carried out.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the single translation 'he is a doctor' in the left interface demonstrate the male default bias?\",\n      \"answer\": \"The interface shows only the masculine pronoun 'he' for the gender-neutral Turkish input, exemplifying the strong tendency towards male defaults that the study provided evidence for in statistical translation tools.\",\n      \"visual_anchor\": \"Blue box displaying 'he is a doctor' as the sole translation result\",\n      \"text_evidence\": \"In this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What policy change occurred on December 6, 2018 that would alter the left interface display?\",\n      \"answer\": \"Google released a new feature presenting users with both feminine and masculine official translations, moving away from the single translation shown in the 'Before' interface to reduce gender bias.\",\n      \"visual_anchor\": \"Left interface labeled 'Before' showing only one translation option\",\n      \"text_evidence\": \"On December 6, 2018 the company's policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Left interface labeled 'Before' showing single translation 'he is a doctor' in blue box",
        "Right interface labeled 'After' (not fully shown in image)",
        "Turkish input text 'o bir doktor'",
        "Language selector showing 'TURKISH' and 'ENGLISH'",
        "Blue translation result box with verification checkmark",
        "Star icon and speaker icon in interface"
      ],
      "queries": [
        {
          "query": "Why does the left interface showing 'he is a doctor' represent the version analyzed in August 2018 experiments?",
          "answer": "The experiments reported in the study were carried out in August 2018, capturing a snapshot of Google Translate at that time when it offered only one official translation for each input word.",
          "visual_anchor": "Left interface labeled 'Before' with single translation 'he is a doctor' in blue box",
          "text_evidence": "In this context, all experiments reported here offer an analysis of a \"screenshot\" of that tool as of August 2018, the moment they were carried out.",
          "query_type": "value_context"
        },
        {
          "query": "How does the single translation 'he is a doctor' in the left interface demonstrate the male default bias?",
          "answer": "The interface shows only the masculine pronoun 'he' for the gender-neutral Turkish input, exemplifying the strong tendency towards male defaults that the study provided evidence for in statistical translation tools.",
          "visual_anchor": "Blue box displaying 'he is a doctor' as the sole translation result",
          "text_evidence": "In this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What policy change occurred on December 6, 2018 that would alter the left interface display?",
          "answer": "Google released a new feature presenting users with both feminine and masculine official translations, moving away from the single translation shown in the 'Before' interface to reduce gender bias.",
          "visual_anchor": "Left interface labeled 'Before' showing only one translation option",
          "text_evidence": "On December 6, 2018 the company's policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the left interface showing 'he is a doctor' represent the version analyzed in August 2018 experiments?",
        "answer": "The experiments reported in the study were carried out in August 2018, capturing a snapshot of Google Translate at that time when it offered only one official translation for each input word.",
        "visual_anchor": "Left interface labeled 'Before' with single translation 'he is a doctor' in blue box",
        "text_evidence": "In this context, all experiments reported here offer an analysis of a \"screenshot\" of that tool as of August 2018, the moment they were carried out.",
        "query_type": "value_context"
      },
      {
        "query": "How does the single translation 'he is a doctor' in the left interface demonstrate the male default bias?",
        "answer": "The interface shows only the masculine pronoun 'he' for the gender-neutral Turkish input, exemplifying the strong tendency towards male defaults that the study provided evidence for in statistical translation tools.",
        "visual_anchor": "Blue box displaying 'he is a doctor' as the sole translation result",
        "text_evidence": "In this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What policy change occurred on December 6, 2018 that would alter the left interface display?",
        "answer": "Google released a new feature presenting users with both feminine and masculine official translations, moving away from the single translation shown in the 'Before' interface to reduce gender bias.",
        "visual_anchor": "Left interface labeled 'Before' showing only one translation option",
        "text_evidence": "On December 6, 2018 the company's policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Left interface labeled 'Before' showing single translation 'he is a doctor' in blue box",
      "Right interface labeled 'After' (not fully shown in image)",
      "Turkish input text 'o bir doktor'",
      "Language selector showing 'TURKISH' and 'ENGLISH'",
      "Blue translation result box with verification checkmark",
      "Star icon and speaker icon in interface"
    ]
  },
  {
    "doc_id": "1809.02244",
    "figure_id": "1809.02244_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg",
    "caption": "Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node X with outgoing arrows to M and A\", \"Node M with arrow to Y\", \"Node A with arrows to both M and Y\", \"Blue directed edges forming a causal DAG\", \"Four nodes labeled X, M, A, Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the arrow from X to M enable identification of the distribution sum over x,m in the response formula?\",\n      \"answer\": \"The arrow from X to M represents the conditional probability p(M|a=f_A(x), X=x), which is one of the three multiplicative terms in the identified distribution formula that sums over x and m to compute Y(a=f_A(X)).\",\n      \"visual_anchor\": \"Blue arrow from node X to node M\",\n      \"text_evidence\": \"its distribution is identified as $\\\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the direct arrow from A to Y allow defining Y(a=f_A(X)) without including the mediator M in the counterfactual?\",\n      \"answer\": \"No, despite the direct arrow from A to Y, the counterfactual Y(a=f_A(X)) must be defined as Y(a=f_A(X), M(a=f_A(X), X), X), explicitly including the mediator M because M lies on a causal path from A to Y.\",\n      \"visual_anchor\": \"Blue arrow from node A directly to node Y\",\n      \"text_evidence\": \"$Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does node M's position between A and Y relate to the difficulty of validating optimal policy sets?\",\n      \"answer\": \"Node M as a mediator between A and Y makes Y(f_A) a counterfactual quantity involving unobserved pathways through M, which creates difficulty in validating the optimal policy f_A* that maximizes E[Y(f_A)] using only observed data.\",\n      \"visual_anchor\": \"Node M positioned between nodes A and Y with incoming and outgoing blue arrows\",\n      \"text_evidence\": \"Since $Y(f_{A})$ is a counterfactual quantity, validating the found set of policies is difficult given only\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node X with outgoing arrows to M and A",
        "Node M with arrow to Y",
        "Node A with arrows to both M and Y",
        "Blue directed edges forming a causal DAG",
        "Four nodes labeled X, M, A, Y"
      ],
      "queries": [
        {
          "query": "Why does the arrow from X to M enable identification of the distribution sum over x,m in the response formula?",
          "answer": "The arrow from X to M represents the conditional probability p(M|a=f_A(x), X=x), which is one of the three multiplicative terms in the identified distribution formula that sums over x and m to compute Y(a=f_A(X)).",
          "visual_anchor": "Blue arrow from node X to node M",
          "text_evidence": "its distribution is identified as $\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the direct arrow from A to Y allow defining Y(a=f_A(X)) without including the mediator M in the counterfactual?",
          "answer": "No, despite the direct arrow from A to Y, the counterfactual Y(a=f_A(X)) must be defined as Y(a=f_A(X), M(a=f_A(X), X), X), explicitly including the mediator M because M lies on a causal path from A to Y.",
          "visual_anchor": "Blue arrow from node A directly to node Y",
          "text_evidence": "$Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does node M's position between A and Y relate to the difficulty of validating optimal policy sets?",
          "answer": "Node M as a mediator between A and Y makes Y(f_A) a counterfactual quantity involving unobserved pathways through M, which creates difficulty in validating the optimal policy f_A* that maximizes E[Y(f_A)] using only observed data.",
          "visual_anchor": "Node M positioned between nodes A and Y with incoming and outgoing blue arrows",
          "text_evidence": "Since $Y(f_{A})$ is a counterfactual quantity, validating the found set of policies is difficult given only",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the arrow from X to M enable identification of the distribution sum over x,m in the response formula?",
        "answer": "The arrow from X to M represents the conditional probability p(M|a=f_A(x), X=x), which is one of the three multiplicative terms in the identified distribution formula that sums over x and m to compute Y(a=f_A(X)).",
        "visual_anchor": "Blue arrow from node X to node M",
        "text_evidence": "its distribution is identified as $\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the direct arrow from A to Y allow defining Y(a=f_A(X)) without including the mediator M in the counterfactual?",
        "answer": "No, despite the direct arrow from A to Y, the counterfactual Y(a=f_A(X)) must be defined as Y(a=f_A(X), M(a=f_A(X), X), X), explicitly including the mediator M because M lies on a causal path from A to Y.",
        "visual_anchor": "Blue arrow from node A directly to node Y",
        "text_evidence": "$Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does node M's position between A and Y relate to the difficulty of validating optimal policy sets?",
        "answer": "Node M as a mediator between A and Y makes Y(f_A) a counterfactual quantity involving unobserved pathways through M, which creates difficulty in validating the optimal policy f_A* that maximizes E[Y(f_A)] using only observed data.",
        "visual_anchor": "Node M positioned between nodes A and Y with incoming and outgoing blue arrows",
        "text_evidence": "Since $Y(f_{A})$ is a counterfactual quantity, validating the found set of policies is difficult given only",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node X with outgoing arrows to M and A",
      "Node M with arrow to Y",
      "Node A with arrows to both M and Y",
      "Blue directed edges forming a causal DAG",
      "Four nodes labeled X, M, A, Y"
    ]
  },
  {
    "doc_id": "1809.02244",
    "figure_id": "1809.02244_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg",
    "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue solid line for African-American incarceration rate\",\n    \"Red solid line for Caucasian incarceration rate\",\n    \"Dashed lines for optimal unrestricted policy\",\n    \"Dotted lines for optimal fair policy\",\n    \"Observed policy shown as horizontal dotted lines\",\n    \"Vertical dashed lines at utility parameter values ~2.5 and ~2.8\",\n    \"Y-axis: Incarceration Rate (0 to 1.0)\",\n    \"X-axis: Utility Parameter (1 to 4)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue solid curve remain near 0.8 across all utility parameters despite the dataset size of 5,000?\",\n      \"answer\": \"The synthetic data was generated with sensitive variable S as an informative covariate in estimating Y, which maintains the African-American incarceration rate at elevated levels regardless of the utility parameter value.\",\n      \"visual_anchor\": \"Blue solid line staying approximately constant at 0.8 from utility parameter 1 to 4\",\n      \"text_evidence\": \"We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red solid line's rise from 0.0 to 0.3 between utility parameters 2.5 and 3.5 align with the two-stage decision structure?\",\n      \"answer\": \"Yes, the rise reflects the causal model in Fig. 1(c) with K=2 stages, where the utility parameter θ influences the decision policy, causing changes in Caucasian incarceration rates as the policy optimizes the continuous response utility Y.\",\n      \"visual_anchor\": \"Red solid line increasing from approximately 0.0 to 0.3 between utility parameter 2.5 and 3.5\",\n      \"text_evidence\": \"We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$, where all variables are binary except for the continuous response utility $Y \\\\equiv Y_{2}$.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the dashed and dotted blue curves demonstrate the formalization extended from Nabi and Shpitser 2018 for utility parameter above 2.5?\",\n      \"answer\": \"The gap between the dashed (optimal unrestricted) and dotted (optimal fair) blue curves shows the tradeoff between unconstrained policy optimization and fairness-constrained policy learning, illustrating how fairness constraints modify decision-making to induce more equitable outcomes.\",\n      \"visual_anchor\": \"Separation between blue dashed line (~1.0) and blue dotted line (~0.8) for utility parameter values above 2.5\",\n      \"text_evidence\": \"We have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \\\"unfair world\\\" induces high-quality outcomes while satisfying the specified fairness constraints\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue solid line for African-American incarceration rate",
        "Red solid line for Caucasian incarceration rate",
        "Dashed lines for optimal unrestricted policy",
        "Dotted lines for optimal fair policy",
        "Observed policy shown as horizontal dotted lines",
        "Vertical dashed lines at utility parameter values ~2.5 and ~2.8",
        "Y-axis: Incarceration Rate (0 to 1.0)",
        "X-axis: Utility Parameter (1 to 4)"
      ],
      "queries": [
        {
          "query": "Why does the blue solid curve remain near 0.8 across all utility parameters despite the dataset size of 5,000?",
          "answer": "The synthetic data was generated with sensitive variable S as an informative covariate in estimating Y, which maintains the African-American incarceration rate at elevated levels regardless of the utility parameter value.",
          "visual_anchor": "Blue solid line staying approximately constant at 0.8 from utility parameter 1 to 4",
          "text_evidence": "We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red solid line's rise from 0.0 to 0.3 between utility parameters 2.5 and 3.5 align with the two-stage decision structure?",
          "answer": "Yes, the rise reflects the causal model in Fig. 1(c) with K=2 stages, where the utility parameter θ influences the decision policy, causing changes in Caucasian incarceration rates as the policy optimizes the continuous response utility Y.",
          "visual_anchor": "Red solid line increasing from approximately 0.0 to 0.3 between utility parameter 2.5 and 3.5",
          "text_evidence": "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$, where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$.",
          "query_type": "value_context"
        },
        {
          "query": "How do the dashed and dotted blue curves demonstrate the formalization extended from Nabi and Shpitser 2018 for utility parameter above 2.5?",
          "answer": "The gap between the dashed (optimal unrestricted) and dotted (optimal fair) blue curves shows the tradeoff between unconstrained policy optimization and fairness-constrained policy learning, illustrating how fairness constraints modify decision-making to induce more equitable outcomes.",
          "visual_anchor": "Separation between blue dashed line (~1.0) and blue dotted line (~0.8) for utility parameter values above 2.5",
          "text_evidence": "We have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue solid curve remain near 0.8 across all utility parameters despite the dataset size of 5,000?",
        "answer": "The synthetic data was generated with sensitive variable S as an informative covariate in estimating Y, which maintains the African-American incarceration rate at elevated levels regardless of the utility parameter value.",
        "visual_anchor": "Blue solid line staying approximately constant at 0.8 from utility parameter 1 to 4",
        "text_evidence": "We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red solid line's rise from 0.0 to 0.3 between utility parameters 2.5 and 3.5 align with the two-stage decision structure?",
        "answer": "Yes, the rise reflects the causal model in Fig. 1(c) with K=2 stages, where the utility parameter θ influences the decision policy, causing changes in Caucasian incarceration rates as the policy optimizes the continuous response utility Y.",
        "visual_anchor": "Red solid line increasing from approximately 0.0 to 0.3 between utility parameter 2.5 and 3.5",
        "text_evidence": "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$, where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$.",
        "query_type": "value_context"
      },
      {
        "query": "How do the dashed and dotted blue curves demonstrate the formalization extended from Nabi and Shpitser 2018 for utility parameter above 2.5?",
        "answer": "The gap between the dashed (optimal unrestricted) and dotted (optimal fair) blue curves shows the tradeoff between unconstrained policy optimization and fairness-constrained policy learning, illustrating how fairness constraints modify decision-making to induce more equitable outcomes.",
        "visual_anchor": "Separation between blue dashed line (~1.0) and blue dotted line (~0.8) for utility parameter values above 2.5",
        "text_evidence": "We have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue solid line for African-American incarceration rate",
      "Red solid line for Caucasian incarceration rate",
      "Dashed lines for optimal unrestricted policy",
      "Dotted lines for optimal fair policy",
      "Observed policy shown as horizontal dotted lines",
      "Vertical dashed lines at utility parameter values ~2.5 and ~2.8",
      "Y-axis: Incarceration Rate (0 to 1.0)",
      "X-axis: Utility Parameter (1 to 4)"
    ]
  },
  {
    "doc_id": "1809.02244",
    "figure_id": "1809.02244_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg",
    "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid line representing Optimal unfair policy\",\n    \"Dashed line representing Optimal Fair policy\",\n    \"Dotted horizontal line representing Observed policy at approximately 0.28\",\n    \"X-axis showing Utility Parameter from 1 to 4\",\n    \"Y-axis showing Incarceration Rate from 0.0 to 0.6\",\n    \"Sharp increase in solid line starting around utility parameter 2.5\",\n    \"Dashed line dropping to near 0 around utility parameter 2.5, then rising sharply after 3.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dashed Fair policy line drop to near zero around utility parameter 2.5 when PSE constraints lie between negative 0.05 and 0.05?\",\n      \"answer\": \"The PSE constraints between -0.05 and 0.05 enforce fairness by limiting predictive score differences between groups, which at this utility parameter value requires minimizing incarceration rates to satisfy the fairness bounds.\",\n      \"visual_anchor\": \"Dashed line dropping to approximately 0.0 at utility parameter 2.5\",\n      \"text_evidence\": \"We constrained the PSEs to lie between $-0.05$ and 0.05 and 0.95 and 1.05, respectively.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the solid unfair policy line behavior starting at utility parameter 2.5 relate to the IPW estimators used for PSE estimation?\",\n      \"answer\": \"The solid unfair policy line shows an unconstrained optimization that sharply increases incarceration rates starting at utility parameter 2.5, reflecting utility maximization without the PSE fairness constraints that the IPW estimators would enforce.\",\n      \"visual_anchor\": \"Solid line showing sharp increase from approximately 0.0 to 0.2 starting at utility parameter 2.5\",\n      \"text_evidence\": \"For estimating the PSEs which we constrain, we used the same IPW estimators described in the main paper and reproduced in the theorem below.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the dotted observed policy line remaining constant at 0.28 validate the regression model specifications mentioned for COMPAS data analysis?\",\n      \"answer\": \"Yes, the constant observed policy line at 0.28 represents the baseline empirical incarceration rate from the actual COMPAS data, serving as a reference point against which the regression models evaluate optimal fair and unfair policies.\",\n      \"visual_anchor\": \"Dotted horizontal line at incarceration rate approximately 0.28\",\n      \"text_evidence\": \"The regression models we used in the COMPAS data analysis were specified as follows:\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid line representing Optimal unfair policy",
        "Dashed line representing Optimal Fair policy",
        "Dotted horizontal line representing Observed policy at approximately 0.28",
        "X-axis showing Utility Parameter from 1 to 4",
        "Y-axis showing Incarceration Rate from 0.0 to 0.6",
        "Sharp increase in solid line starting around utility parameter 2.5",
        "Dashed line dropping to near 0 around utility parameter 2.5, then rising sharply after 3.0"
      ],
      "queries": [
        {
          "query": "Why does the dashed Fair policy line drop to near zero around utility parameter 2.5 when PSE constraints lie between negative 0.05 and 0.05?",
          "answer": "The PSE constraints between -0.05 and 0.05 enforce fairness by limiting predictive score differences between groups, which at this utility parameter value requires minimizing incarceration rates to satisfy the fairness bounds.",
          "visual_anchor": "Dashed line dropping to approximately 0.0 at utility parameter 2.5",
          "text_evidence": "We constrained the PSEs to lie between $-0.05$ and 0.05 and 0.95 and 1.05, respectively.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the solid unfair policy line behavior starting at utility parameter 2.5 relate to the IPW estimators used for PSE estimation?",
          "answer": "The solid unfair policy line shows an unconstrained optimization that sharply increases incarceration rates starting at utility parameter 2.5, reflecting utility maximization without the PSE fairness constraints that the IPW estimators would enforce.",
          "visual_anchor": "Solid line showing sharp increase from approximately 0.0 to 0.2 starting at utility parameter 2.5",
          "text_evidence": "For estimating the PSEs which we constrain, we used the same IPW estimators described in the main paper and reproduced in the theorem below.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the dotted observed policy line remaining constant at 0.28 validate the regression model specifications mentioned for COMPAS data analysis?",
          "answer": "Yes, the constant observed policy line at 0.28 represents the baseline empirical incarceration rate from the actual COMPAS data, serving as a reference point against which the regression models evaluate optimal fair and unfair policies.",
          "visual_anchor": "Dotted horizontal line at incarceration rate approximately 0.28",
          "text_evidence": "The regression models we used in the COMPAS data analysis were specified as follows:",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dashed Fair policy line drop to near zero around utility parameter 2.5 when PSE constraints lie between negative 0.05 and 0.05?",
        "answer": "The PSE constraints between -0.05 and 0.05 enforce fairness by limiting predictive score differences between groups, which at this utility parameter value requires minimizing incarceration rates to satisfy the fairness bounds.",
        "visual_anchor": "Dashed line dropping to approximately 0.0 at utility parameter 2.5",
        "text_evidence": "We constrained the PSEs to lie between $-0.05$ and 0.05 and 0.95 and 1.05, respectively.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the solid unfair policy line behavior starting at utility parameter 2.5 relate to the IPW estimators used for PSE estimation?",
        "answer": "The solid unfair policy line shows an unconstrained optimization that sharply increases incarceration rates starting at utility parameter 2.5, reflecting utility maximization without the PSE fairness constraints that the IPW estimators would enforce.",
        "visual_anchor": "Solid line showing sharp increase from approximately 0.0 to 0.2 starting at utility parameter 2.5",
        "text_evidence": "For estimating the PSEs which we constrain, we used the same IPW estimators described in the main paper and reproduced in the theorem below.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the dotted observed policy line remaining constant at 0.28 validate the regression model specifications mentioned for COMPAS data analysis?",
        "answer": "Yes, the constant observed policy line at 0.28 represents the baseline empirical incarceration rate from the actual COMPAS data, serving as a reference point against which the regression models evaluate optimal fair and unfair policies.",
        "visual_anchor": "Dotted horizontal line at incarceration rate approximately 0.28",
        "text_evidence": "The regression models we used in the COMPAS data analysis were specified as follows:",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid line representing Optimal unfair policy",
      "Dashed line representing Optimal Fair policy",
      "Dotted horizontal line representing Observed policy at approximately 0.28",
      "X-axis showing Utility Parameter from 1 to 4",
      "Y-axis showing Incarceration Rate from 0.0 to 0.6",
      "Sharp increase in solid line starting around utility parameter 2.5",
      "Dashed line dropping to near 0 around utility parameter 2.5, then rising sharply after 3.0"
    ]
  },
  {
    "doc_id": "1809.02244",
    "figure_id": "1809.02244_fig_6",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg",
    "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid line labeled 'Optimal unfair policy'\",\n    \"dashed line labeled 'Optimal Fair policy'\",\n    \"dotted line labeled 'Observed policy'\",\n    \"y-axis showing Expected Utility from 0.2 to -0.8\",\n    \"x-axis showing Utility Parameter from 1 to 4\",\n    \"all three lines descending from left to right\",\n    \"lines converging around utility parameter 2.5-3\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid line remain above both other curves across all utility parameter values despite all three descending?\",\n      \"answer\": \"The solid line represents the optimal unfair policy, which maximizes expected utility without fairness constraints, allowing it to achieve higher utility values than the constrained fair policy or the observed policy across all parameter settings.\",\n      \"visual_anchor\": \"solid line (Optimal unfair policy) positioned above dashed and dotted lines from utility parameter 1 to 4\",\n      \"text_evidence\": \"The relative utility of policies for the COMPAS data as a function of the utility parameter θ\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the dashed and dotted lines converging near utility parameter 3 support that fairness constraints reduce optimality gaps?\",\n      \"answer\": \"Yes, the convergence of the optimal fair policy (dashed) and observed policy (dotted) around utility parameter 3 suggests that at higher parameter values, fairness constraints become less costly, narrowing the gap between optimal and observed fair policies.\",\n      \"visual_anchor\": \"dashed line (Optimal Fair policy) and dotted line (Observed policy) converging around utility parameter 3\",\n      \"text_evidence\": \"Assume S is binary. Under the causal model above, the following are consistent estimators of PSE^{sy} and PSE^{sak_k}, assuming all models are correctly specified\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the steeper decline of all three curves between utility parameter 1 and 2 compared to later regions?\",\n      \"answer\": \"The steeper initial decline reflects higher sensitivity of expected utility to the utility parameter θ at lower values, where policy differences have greater impact. As θ increases beyond 2, the rate of utility decrease moderates across all policies.\",\n      \"visual_anchor\": \"all three curves showing steeper negative slope between utility parameter 1 and 2 on x-axis\",\n      \"text_evidence\": \"The latent projection (Verma and Pearl, 1990) of any K stage DAG onto X, S, M, A, Y suffices to identify and estimate the two path-specific effects in question\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid line labeled 'Optimal unfair policy'",
        "dashed line labeled 'Optimal Fair policy'",
        "dotted line labeled 'Observed policy'",
        "y-axis showing Expected Utility from 0.2 to -0.8",
        "x-axis showing Utility Parameter from 1 to 4",
        "all three lines descending from left to right",
        "lines converging around utility parameter 2.5-3"
      ],
      "queries": [
        {
          "query": "Why does the solid line remain above both other curves across all utility parameter values despite all three descending?",
          "answer": "The solid line represents the optimal unfair policy, which maximizes expected utility without fairness constraints, allowing it to achieve higher utility values than the constrained fair policy or the observed policy across all parameter settings.",
          "visual_anchor": "solid line (Optimal unfair policy) positioned above dashed and dotted lines from utility parameter 1 to 4",
          "text_evidence": "The relative utility of policies for the COMPAS data as a function of the utility parameter θ",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the dashed and dotted lines converging near utility parameter 3 support that fairness constraints reduce optimality gaps?",
          "answer": "Yes, the convergence of the optimal fair policy (dashed) and observed policy (dotted) around utility parameter 3 suggests that at higher parameter values, fairness constraints become less costly, narrowing the gap between optimal and observed fair policies.",
          "visual_anchor": "dashed line (Optimal Fair policy) and dotted line (Observed policy) converging around utility parameter 3",
          "text_evidence": "Assume S is binary. Under the causal model above, the following are consistent estimators of PSE^{sy} and PSE^{sak_k}, assuming all models are correctly specified",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the steeper decline of all three curves between utility parameter 1 and 2 compared to later regions?",
          "answer": "The steeper initial decline reflects higher sensitivity of expected utility to the utility parameter θ at lower values, where policy differences have greater impact. As θ increases beyond 2, the rate of utility decrease moderates across all policies.",
          "visual_anchor": "all three curves showing steeper negative slope between utility parameter 1 and 2 on x-axis",
          "text_evidence": "The latent projection (Verma and Pearl, 1990) of any K stage DAG onto X, S, M, A, Y suffices to identify and estimate the two path-specific effects in question",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid line remain above both other curves across all utility parameter values despite all three descending?",
        "answer": "The solid line represents the optimal unfair policy, which maximizes expected utility without fairness constraints, allowing it to achieve higher utility values than the constrained fair policy or the observed policy across all parameter settings.",
        "visual_anchor": "solid line (Optimal unfair policy) positioned above dashed and dotted lines from utility parameter 1 to 4",
        "text_evidence": "The relative utility of policies for the COMPAS data as a function of the utility parameter θ",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the dashed and dotted lines converging near utility parameter 3 support that fairness constraints reduce optimality gaps?",
        "answer": "Yes, the convergence of the optimal fair policy (dashed) and observed policy (dotted) around utility parameter 3 suggests that at higher parameter values, fairness constraints become less costly, narrowing the gap between optimal and observed fair policies.",
        "visual_anchor": "dashed line (Optimal Fair policy) and dotted line (Observed policy) converging around utility parameter 3",
        "text_evidence": "Assume S is binary. Under the causal model above, the following are consistent estimators of PSE^{sy} and PSE^{sak_k}, assuming all models are correctly specified",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the steeper decline of all three curves between utility parameter 1 and 2 compared to later regions?",
        "answer": "The steeper initial decline reflects higher sensitivity of expected utility to the utility parameter θ at lower values, where policy differences have greater impact. As θ increases beyond 2, the rate of utility decrease moderates across all policies.",
        "visual_anchor": "all three curves showing steeper negative slope between utility parameter 1 and 2 on x-axis",
        "text_evidence": "The latent projection (Verma and Pearl, 1990) of any K stage DAG onto X, S, M, A, Y suffices to identify and estimate the two path-specific effects in question",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid line labeled 'Optimal unfair policy'",
      "dashed line labeled 'Optimal Fair policy'",
      "dotted line labeled 'Observed policy'",
      "y-axis showing Expected Utility from 0.2 to -0.8",
      "x-axis showing Utility Parameter from 1 to 4",
      "all three lines descending from left to right",
      "lines converging around utility parameter 2.5-3"
    ]
  },
  {
    "doc_id": "1809.04737",
    "figure_id": "1809.04737_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
    "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid black line labeled 0-1\",\n    \"dotted black line labeled κ=hinge\",\n    \"dashed red line labeled κ=logistic\",\n    \"dash-dot green line labeled κ=square\",\n    \"solid red line labeled δ=logistic\",\n    \"dashed red line labeled δ=logistic\",\n    \"dash-dot green line labeled δ=square\",\n    \"vertical axis from -1.5 to 2.5\",\n    \"horizontal axis from -2.0 to 2.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the dotted black κ=hinge and dash-dot green κ=square curves diverge after x=0.5 despite both being surrogate functions?\",\n      \"answer\": \"Different surrogate functions approximate the indicator function differently. The hinge function grows linearly while the square function grows quadratically in the positive region, leading to divergent behavior after x=0.5.\",\n      \"visual_anchor\": \"dotted black κ=hinge and dash-dot green κ=square curves diverging after x=0.5\",\n      \"text_evidence\": \"Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\\\mathbb { R D } ( f ) \\\\leq \\\\tau$ and $- \\\\mathbb { R } \\\\mathbb { D } ( f ) \\\\leq \\\\tau$ are opposite to each other.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the solid red δ=logistic curve's plateau near y=1.0 for x>0.5 relate to fairness constraint satisfaction?\",\n      \"answer\": \"The plateau demonstrates the logistic surrogate's bounded growth, which helps in satisfying fairness constraints without extreme penalty values when optimizing the fairness-aware classification objective.\",\n      \"visual_anchor\": \"solid red δ=logistic curve plateauing near y=1.0 for x>0.5\",\n      \"text_evidence\": \"The fairness-aware classification aims to find a classifier that minimizes the empirical loss while satisfying certain fairness constraints. Several fairness notions or definitions are proposed in the literature\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes the dashed red κ=logistic curve to reach only y≈0.5 at x=2.0 while dotted κ=hinge exceeds y=2.0?\",\n      \"answer\": \"The logistic function has a gentler growth rate due to its sigmoid-based nature, while the hinge function grows linearly without bound, resulting in the dramatic difference at x=2.0.\",\n      \"visual_anchor\": \"dashed red κ=logistic at y≈0.5 versus dotted black κ=hinge above y=2.0 at x=2.0\",\n      \"text_evidence\": \"For simplicity, we may want to denote $P ( S = s ^ { + } | \\\\mathbf { x } )$ by $\\\\eta ( \\\\mathbf { x } )$ and $P ( \\\\bar { S } = s ^ { + } )$ by $p$\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid black line labeled 0-1",
        "dotted black line labeled κ=hinge",
        "dashed red line labeled κ=logistic",
        "dash-dot green line labeled κ=square",
        "solid red line labeled δ=logistic",
        "dashed red line labeled δ=logistic",
        "dash-dot green line labeled δ=square",
        "vertical axis from -1.5 to 2.5",
        "horizontal axis from -2.0 to 2.0"
      ],
      "queries": [
        {
          "query": "Why do the dotted black κ=hinge and dash-dot green κ=square curves diverge after x=0.5 despite both being surrogate functions?",
          "answer": "Different surrogate functions approximate the indicator function differently. The hinge function grows linearly while the square function grows quadratically in the positive region, leading to divergent behavior after x=0.5.",
          "visual_anchor": "dotted black κ=hinge and dash-dot green κ=square curves diverging after x=0.5",
          "text_evidence": "Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the solid red δ=logistic curve's plateau near y=1.0 for x>0.5 relate to fairness constraint satisfaction?",
          "answer": "The plateau demonstrates the logistic surrogate's bounded growth, which helps in satisfying fairness constraints without extreme penalty values when optimizing the fairness-aware classification objective.",
          "visual_anchor": "solid red δ=logistic curve plateauing near y=1.0 for x>0.5",
          "text_evidence": "The fairness-aware classification aims to find a classifier that minimizes the empirical loss while satisfying certain fairness constraints. Several fairness notions or definitions are proposed in the literature",
          "query_type": "value_context"
        },
        {
          "query": "What causes the dashed red κ=logistic curve to reach only y≈0.5 at x=2.0 while dotted κ=hinge exceeds y=2.0?",
          "answer": "The logistic function has a gentler growth rate due to its sigmoid-based nature, while the hinge function grows linearly without bound, resulting in the dramatic difference at x=2.0.",
          "visual_anchor": "dashed red κ=logistic at y≈0.5 versus dotted black κ=hinge above y=2.0 at x=2.0",
          "text_evidence": "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the dotted black κ=hinge and dash-dot green κ=square curves diverge after x=0.5 despite both being surrogate functions?",
        "answer": "Different surrogate functions approximate the indicator function differently. The hinge function grows linearly while the square function grows quadratically in the positive region, leading to divergent behavior after x=0.5.",
        "visual_anchor": "dotted black κ=hinge and dash-dot green κ=square curves diverging after x=0.5",
        "text_evidence": "Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the solid red δ=logistic curve's plateau near y=1.0 for x>0.5 relate to fairness constraint satisfaction?",
        "answer": "The plateau demonstrates the logistic surrogate's bounded growth, which helps in satisfying fairness constraints without extreme penalty values when optimizing the fairness-aware classification objective.",
        "visual_anchor": "solid red δ=logistic curve plateauing near y=1.0 for x>0.5",
        "text_evidence": "The fairness-aware classification aims to find a classifier that minimizes the empirical loss while satisfying certain fairness constraints. Several fairness notions or definitions are proposed in the literature",
        "query_type": "value_context"
      },
      {
        "query": "What causes the dashed red κ=logistic curve to reach only y≈0.5 at x=2.0 while dotted κ=hinge exceeds y=2.0?",
        "answer": "The logistic function has a gentler growth rate due to its sigmoid-based nature, while the hinge function grows linearly without bound, resulting in the dramatic difference at x=2.0.",
        "visual_anchor": "dashed red κ=logistic at y≈0.5 versus dotted black κ=hinge above y=2.0 at x=2.0",
        "text_evidence": "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid black line labeled 0-1",
      "dotted black line labeled κ=hinge",
      "dashed red line labeled κ=logistic",
      "dash-dot green line labeled κ=square",
      "solid red line labeled δ=logistic",
      "dashed red line labeled δ=logistic",
      "dash-dot green line labeled δ=square",
      "vertical axis from -1.5 to 2.5",
      "horizontal axis from -2.0 to 2.0"
    ]
  },
  {
    "doc_id": "1809.04737",
    "figure_id": "1809.04737_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg",
    "caption": "Figure 2: Two classifiers and their predictions.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"yellow circles at y=1.5 and y=-1.5\",\n    \"blue circles at y=1.5 and y=-0.5\",\n    \"horizontal gridlines at y=1.0, 0.5, -0.5, -1.0\",\n    \"legend showing male (yellow) and female (blue)\",\n    \"two distinct prediction patterns for two classifiers\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the yellow circles at y=1.5 and blue circles at y=-0.5 illustrate maximal conditional risk difference H⁺(η)?\",\n      \"answer\": \"Yes, the yellow and blue circles at different y-values demonstrate two classifiers making predictions with different conditional risks. This visualization supports the definition of maximal conditional risk difference H⁺(η) between gender groups.\",\n      \"visual_anchor\": \"yellow circles at y=1.5 and blue circles at y=-0.5\",\n      \"text_evidence\": \"We similarly define $H ^ { + } ( \\\\eta )$ the maximal conditional risk difference, $H _ { \\\\delta } ^ { + } ( \\\\eta )$ the maximal conditional $\\\\delta$ -risk difference, $\\\\mathbb { R } \\\\mathbb { D } _ { \\\\delta } ^ { + }$ the maximal $\\\\delta$ -risk difference\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why do female predictions shift from y=1.5 to y=-0.5 while male predictions remain constant across the two classifiers?\",\n      \"answer\": \"The shift demonstrates how different classifiers can produce varying risk differences between gender groups. This pattern is relevant to the constrained optimization problem that seeks fair classifiers satisfying -c₂ ≤ RD(f) ≤ c₁.\",\n      \"visual_anchor\": \"blue circles transitioning from y=1.5 to y=-0.5 across classifiers\",\n      \"text_evidence\": \"A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\\\leq \\\\mathbb { R D } ( f ) \\\\leq c _ { 1 }$ can be obtained by solving the following constrained optimization\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the constant male prediction at y=1.5 relate to the constraint RDκ(h) ≤ ψκ(c₁ - RD⁻) + RDκ⁻?\",\n      \"answer\": \"The constant male predictions suggest one group maintains fixed risk while the other varies, which is captured by the RHS constraint being constant for a given dataset. This allows the constrained optimization to remain convex despite varying predictions.\",\n      \"visual_anchor\": \"yellow circles consistently at y=1.5 across both classifiers\",\n      \"text_evidence\": \"subject to $\\\\mathbb { R } \\\\mathbb { D } _ { \\\\kappa } ( h ) \\\\leq \\\\psi _ { \\\\kappa } \\\\big ( c _ { 1 } - \\\\mathbb { R } \\\\mathbb { D } ^ { - } \\\\big ) + \\\\mathbb { R } \\\\mathbb { D } _ { \\\\kappa } ^ { - } ,$ Note that the RHS of above two inequalities are constants for a given dataset. Therefore, the constrained optimization problem is still convex.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "yellow circles at y=1.5 and y=-1.5",
        "blue circles at y=1.5 and y=-0.5",
        "horizontal gridlines at y=1.0, 0.5, -0.5, -1.0",
        "legend showing male (yellow) and female (blue)",
        "two distinct prediction patterns for two classifiers"
      ],
      "queries": [
        {
          "query": "Do the yellow circles at y=1.5 and blue circles at y=-0.5 illustrate maximal conditional risk difference H⁺(η)?",
          "answer": "Yes, the yellow and blue circles at different y-values demonstrate two classifiers making predictions with different conditional risks. This visualization supports the definition of maximal conditional risk difference H⁺(η) between gender groups.",
          "visual_anchor": "yellow circles at y=1.5 and blue circles at y=-0.5",
          "text_evidence": "We similarly define $H ^ { + } ( \\eta )$ the maximal conditional risk difference, $H _ { \\delta } ^ { + } ( \\eta )$ the maximal conditional $\\delta$ -risk difference, $\\mathbb { R } \\mathbb { D } _ { \\delta } ^ { + }$ the maximal $\\delta$ -risk difference",
          "query_type": "visual_definition"
        },
        {
          "query": "Why do female predictions shift from y=1.5 to y=-0.5 while male predictions remain constant across the two classifiers?",
          "answer": "The shift demonstrates how different classifiers can produce varying risk differences between gender groups. This pattern is relevant to the constrained optimization problem that seeks fair classifiers satisfying -c₂ ≤ RD(f) ≤ c₁.",
          "visual_anchor": "blue circles transitioning from y=1.5 to y=-0.5 across classifiers",
          "text_evidence": "A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the constant male prediction at y=1.5 relate to the constraint RDκ(h) ≤ ψκ(c₁ - RD⁻) + RDκ⁻?",
          "answer": "The constant male predictions suggest one group maintains fixed risk while the other varies, which is captured by the RHS constraint being constant for a given dataset. This allows the constrained optimization to remain convex despite varying predictions.",
          "visual_anchor": "yellow circles consistently at y=1.5 across both classifiers",
          "text_evidence": "subject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq \\psi _ { \\kappa } \\big ( c _ { 1 } - \\mathbb { R } \\mathbb { D } ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$ Note that the RHS of above two inequalities are constants for a given dataset. Therefore, the constrained optimization problem is still convex.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the yellow circles at y=1.5 and blue circles at y=-0.5 illustrate maximal conditional risk difference H⁺(η)?",
        "answer": "Yes, the yellow and blue circles at different y-values demonstrate two classifiers making predictions with different conditional risks. This visualization supports the definition of maximal conditional risk difference H⁺(η) between gender groups.",
        "visual_anchor": "yellow circles at y=1.5 and blue circles at y=-0.5",
        "text_evidence": "We similarly define $H ^ { + } ( \\eta )$ the maximal conditional risk difference, $H _ { \\delta } ^ { + } ( \\eta )$ the maximal conditional $\\delta$ -risk difference, $\\mathbb { R } \\mathbb { D } _ { \\delta } ^ { + }$ the maximal $\\delta$ -risk difference",
        "query_type": "visual_definition"
      },
      {
        "query": "Why do female predictions shift from y=1.5 to y=-0.5 while male predictions remain constant across the two classifiers?",
        "answer": "The shift demonstrates how different classifiers can produce varying risk differences between gender groups. This pattern is relevant to the constrained optimization problem that seeks fair classifiers satisfying -c₂ ≤ RD(f) ≤ c₁.",
        "visual_anchor": "blue circles transitioning from y=1.5 to y=-0.5 across classifiers",
        "text_evidence": "A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the constant male prediction at y=1.5 relate to the constraint RDκ(h) ≤ ψκ(c₁ - RD⁻) + RDκ⁻?",
        "answer": "The constant male predictions suggest one group maintains fixed risk while the other varies, which is captured by the RHS constraint being constant for a given dataset. This allows the constrained optimization to remain convex despite varying predictions.",
        "visual_anchor": "yellow circles consistently at y=1.5 across both classifiers",
        "text_evidence": "subject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq \\psi _ { \\kappa } \\big ( c _ { 1 } - \\mathbb { R } \\mathbb { D } ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$ Note that the RHS of above two inequalities are constants for a given dataset. Therefore, the constrained optimization problem is still convex.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "yellow circles at y=1.5 and y=-1.5",
      "blue circles at y=1.5 and y=-0.5",
      "horizontal gridlines at y=1.0, 0.5, -0.5, -1.0",
      "legend showing male (yellow) and female (blue)",
      "two distinct prediction patterns for two classifiers"
    ]
  },
  {
    "doc_id": "1809.04737",
    "figure_id": "1809.04737_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg",
    "caption": "Figure 3: Comparison of fair classifiers.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid blue line labeled Zafar-1\",\n    \"Dashed green line labeled Zafar-2\",\n    \"Dotted red line labeled Our Method\",\n    \"Blue X marker labeled Unconstrained\",\n    \"X-axis: Risk Difference (range -0.1 to 0.4)\",\n    \"Y-axis: Empirical Loss (range 0.18 to 0.30)\",\n    \"Red dotted line starts at approximately 0.295 loss and -0.1 risk difference\",\n    \"All three methods converge near 0.195 loss at 0.3 risk difference\",\n    \"Unconstrained marker at approximately 0.193 loss and 0.37 risk difference\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red dotted Our Method curve start at higher empirical loss near 0.295 compared to other methods?\",\n      \"answer\": \"Our Method begins at the minimal risk difference point (RD^-), which corresponds to the maximal/minimal risk difference classifiers measured for the datasets. This represents the lower bound of achievable risk differences with corresponding empirical loss.\",\n      \"visual_anchor\": \"Red dotted line starting at approximately 0.295 empirical loss at -0.1 risk difference\",\n      \"text_evidence\": \"we build the maximal/minimal risk difference classifiers $f _ { \\\\mathrm { m i n } }$ , $f _ { \\\\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\\\mathbb { R D } ^ { - } , \\\\mathbb { R D } ^ { + }$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the converging trajectories of all three methods near 0.195 loss support the claim about large maximal risk differences?\",\n      \"answer\": \"Yes, all methods converge near 0.3 risk difference at approximately 0.195 empirical loss, demonstrating the large span between minimal and maximal risk differences. This confirms the measured RD^+ values shown in Table 3.\",\n      \"visual_anchor\": \"Convergence point of solid blue, dashed green, and dotted red lines at approximately 0.195 loss and 0.3 risk difference\",\n      \"text_evidence\": \"The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the blue X Unconstrained marker at 0.193 loss relate to fairness-aware classifier construction methods?\",\n      \"answer\": \"The Unconstrained marker represents a baseline without fairness constraints, achieving slightly lower loss but much higher risk difference (0.37). This illustrates why methods for constructing fairness-aware classifiers are needed to balance accuracy and fairness.\",\n      \"visual_anchor\": \"Blue X marker labeled Unconstrained at approximately 0.193 empirical loss and 0.37 risk difference\",\n      \"text_evidence\": \"Many methods have been proposed for constructing fairnessaware classifiers, which can be broadly classified into pre/post-processing and in-processing methods.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid blue line labeled Zafar-1",
        "Dashed green line labeled Zafar-2",
        "Dotted red line labeled Our Method",
        "Blue X marker labeled Unconstrained",
        "X-axis: Risk Difference (range -0.1 to 0.4)",
        "Y-axis: Empirical Loss (range 0.18 to 0.30)",
        "Red dotted line starts at approximately 0.295 loss and -0.1 risk difference",
        "All three methods converge near 0.195 loss at 0.3 risk difference",
        "Unconstrained marker at approximately 0.193 loss and 0.37 risk difference"
      ],
      "queries": [
        {
          "query": "Why does the red dotted Our Method curve start at higher empirical loss near 0.295 compared to other methods?",
          "answer": "Our Method begins at the minimal risk difference point (RD^-), which corresponds to the maximal/minimal risk difference classifiers measured for the datasets. This represents the lower bound of achievable risk differences with corresponding empirical loss.",
          "visual_anchor": "Red dotted line starting at approximately 0.295 empirical loss at -0.1 risk difference",
          "text_evidence": "we build the maximal/minimal risk difference classifiers $f _ { \\mathrm { m i n } }$ , $f _ { \\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\mathbb { R D } ^ { - } , \\mathbb { R D } ^ { + }$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the converging trajectories of all three methods near 0.195 loss support the claim about large maximal risk differences?",
          "answer": "Yes, all methods converge near 0.3 risk difference at approximately 0.195 empirical loss, demonstrating the large span between minimal and maximal risk differences. This confirms the measured RD^+ values shown in Table 3.",
          "visual_anchor": "Convergence point of solid blue, dashed green, and dotted red lines at approximately 0.195 loss and 0.3 risk difference",
          "text_evidence": "The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences.",
          "query_type": "value_context"
        },
        {
          "query": "How does the blue X Unconstrained marker at 0.193 loss relate to fairness-aware classifier construction methods?",
          "answer": "The Unconstrained marker represents a baseline without fairness constraints, achieving slightly lower loss but much higher risk difference (0.37). This illustrates why methods for constructing fairness-aware classifiers are needed to balance accuracy and fairness.",
          "visual_anchor": "Blue X marker labeled Unconstrained at approximately 0.193 empirical loss and 0.37 risk difference",
          "text_evidence": "Many methods have been proposed for constructing fairnessaware classifiers, which can be broadly classified into pre/post-processing and in-processing methods.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red dotted Our Method curve start at higher empirical loss near 0.295 compared to other methods?",
        "answer": "Our Method begins at the minimal risk difference point (RD^-), which corresponds to the maximal/minimal risk difference classifiers measured for the datasets. This represents the lower bound of achievable risk differences with corresponding empirical loss.",
        "visual_anchor": "Red dotted line starting at approximately 0.295 empirical loss at -0.1 risk difference",
        "text_evidence": "we build the maximal/minimal risk difference classifiers $f _ { \\mathrm { m i n } }$ , $f _ { \\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\mathbb { R D } ^ { - } , \\mathbb { R D } ^ { + }$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the converging trajectories of all three methods near 0.195 loss support the claim about large maximal risk differences?",
        "answer": "Yes, all methods converge near 0.3 risk difference at approximately 0.195 empirical loss, demonstrating the large span between minimal and maximal risk differences. This confirms the measured RD^+ values shown in Table 3.",
        "visual_anchor": "Convergence point of solid blue, dashed green, and dotted red lines at approximately 0.195 loss and 0.3 risk difference",
        "text_evidence": "The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences.",
        "query_type": "value_context"
      },
      {
        "query": "How does the blue X Unconstrained marker at 0.193 loss relate to fairness-aware classifier construction methods?",
        "answer": "The Unconstrained marker represents a baseline without fairness constraints, achieving slightly lower loss but much higher risk difference (0.37). This illustrates why methods for constructing fairness-aware classifiers are needed to balance accuracy and fairness.",
        "visual_anchor": "Blue X marker labeled Unconstrained at approximately 0.193 empirical loss and 0.37 risk difference",
        "text_evidence": "Many methods have been proposed for constructing fairnessaware classifiers, which can be broadly classified into pre/post-processing and in-processing methods.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid blue line labeled Zafar-1",
      "Dashed green line labeled Zafar-2",
      "Dotted red line labeled Our Method",
      "Blue X marker labeled Unconstrained",
      "X-axis: Risk Difference (range -0.1 to 0.4)",
      "Y-axis: Empirical Loss (range 0.18 to 0.30)",
      "Red dotted line starts at approximately 0.295 loss and -0.1 risk difference",
      "All three methods converge near 0.195 loss at 0.3 risk difference",
      "Unconstrained marker at approximately 0.193 loss and 0.37 risk difference"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg",
    "caption": "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Input node x (black circle)\",\n    \"Encoder block Enc (green rectangle)\",\n    \"Split representation e1 and e2 (gray circles)\",\n    \"Disentangle connection (dashed line)\",\n    \"Function ψ block (green rectangle)\",\n    \"Predictor Pred block (green rectangle)\",\n    \"Decoder Dec block (green rectangle)\",\n    \"Output nodes y and x' (white circles)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dashed disentangle line connect e2 to ψ if Dec tries to extract all information into e2?\",\n      \"answer\": \"The disentanglement term enforces separation between e1 and e2, while ψ makes ẽ1 unreliable for reconstruction, forcing Dec to rely on e2 for extracting information about x.\",\n      \"visual_anchor\": \"dashed line labeled 'disentangle' between e2 and ψ block\",\n      \"text_evidence\": \"This is made possible by $\\\\psi$ , which makes $\\\\tilde { e } _ { 1 }$ an unreliable source of information for reconstructing $x$ . Moreover, a version of this framework without $\\\\psi$ can converge to a degenerate solution where $e _ { 1 }$ contains all the information about $x$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the green Pred block receiving e1 support the claim that Pred pulls y-relevant information into e1?\",\n      \"answer\": \"The predictor Pred directly receives e1 as input to predict y, creating competitive pressure to pull information relevant to y into e1 while Dec competes to extract all x information into e2.\",\n      \"visual_anchor\": \"green Pred block with arrow from e1 circle\",\n      \"text_evidence\": \"The predictor and the decoder are designed to enter into a competition, where P red tries to pull information relevant to $y$ into $e _ { 1 }$ while $D e c$ tries to extract all the information about $x$ into $e _ { 2 }$\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"What role does the split representation e=[e1 e2] shown as gray circles play in achieving unsupervised invariance induction?\",\n      \"answer\": \"The split representation disentangles information required for predicting y (in e1) from other unrelated information (in e2), enabling invariance to nuisance factors through competing reconstruction and prediction tasks.\",\n      \"visual_anchor\": \"two gray circles labeled e1 and e2\",\n      \"text_evidence\": \"This is achieved by learning a split representation of data as $\\\\boldsymbol { e } = \\\\left[ e _ { 1 } e _ { 2 } \\\\right]$ , su\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Input node x (black circle)",
        "Encoder block Enc (green rectangle)",
        "Split representation e1 and e2 (gray circles)",
        "Disentangle connection (dashed line)",
        "Function ψ block (green rectangle)",
        "Predictor Pred block (green rectangle)",
        "Decoder Dec block (green rectangle)",
        "Output nodes y and x' (white circles)"
      ],
      "queries": [
        {
          "query": "Why does the dashed disentangle line connect e2 to ψ if Dec tries to extract all information into e2?",
          "answer": "The disentanglement term enforces separation between e1 and e2, while ψ makes ẽ1 unreliable for reconstruction, forcing Dec to rely on e2 for extracting information about x.",
          "visual_anchor": "dashed line labeled 'disentangle' between e2 and ψ block",
          "text_evidence": "This is made possible by $\\psi$ , which makes $\\tilde { e } _ { 1 }$ an unreliable source of information for reconstructing $x$ . Moreover, a version of this framework without $\\psi$ can converge to a degenerate solution where $e _ { 1 }$ contains all the information about $x$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the green Pred block receiving e1 support the claim that Pred pulls y-relevant information into e1?",
          "answer": "The predictor Pred directly receives e1 as input to predict y, creating competitive pressure to pull information relevant to y into e1 while Dec competes to extract all x information into e2.",
          "visual_anchor": "green Pred block with arrow from e1 circle",
          "text_evidence": "The predictor and the decoder are designed to enter into a competition, where P red tries to pull information relevant to $y$ into $e _ { 1 }$ while $D e c$ tries to extract all the information about $x$ into $e _ { 2 }$",
          "query_type": "visual_definition"
        },
        {
          "query": "What role does the split representation e=[e1 e2] shown as gray circles play in achieving unsupervised invariance induction?",
          "answer": "The split representation disentangles information required for predicting y (in e1) from other unrelated information (in e2), enabling invariance to nuisance factors through competing reconstruction and prediction tasks.",
          "visual_anchor": "two gray circles labeled e1 and e2",
          "text_evidence": "This is achieved by learning a split representation of data as $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , su",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dashed disentangle line connect e2 to ψ if Dec tries to extract all information into e2?",
        "answer": "The disentanglement term enforces separation between e1 and e2, while ψ makes ẽ1 unreliable for reconstruction, forcing Dec to rely on e2 for extracting information about x.",
        "visual_anchor": "dashed line labeled 'disentangle' between e2 and ψ block",
        "text_evidence": "This is made possible by $\\psi$ , which makes $\\tilde { e } _ { 1 }$ an unreliable source of information for reconstructing $x$ . Moreover, a version of this framework without $\\psi$ can converge to a degenerate solution where $e _ { 1 }$ contains all the information about $x$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the green Pred block receiving e1 support the claim that Pred pulls y-relevant information into e1?",
        "answer": "The predictor Pred directly receives e1 as input to predict y, creating competitive pressure to pull information relevant to y into e1 while Dec competes to extract all x information into e2.",
        "visual_anchor": "green Pred block with arrow from e1 circle",
        "text_evidence": "The predictor and the decoder are designed to enter into a competition, where P red tries to pull information relevant to $y$ into $e _ { 1 }$ while $D e c$ tries to extract all the information about $x$ into $e _ { 2 }$",
        "query_type": "visual_definition"
      },
      {
        "query": "What role does the split representation e=[e1 e2] shown as gray circles play in achieving unsupervised invariance induction?",
        "answer": "The split representation disentangles information required for predicting y (in e1) from other unrelated information (in e2), enabling invariance to nuisance factors through competing reconstruction and prediction tasks.",
        "visual_anchor": "two gray circles labeled e1 and e2",
        "text_evidence": "This is achieved by learning a split representation of data as $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , su",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Input node x (black circle)",
      "Encoder block Enc (green rectangle)",
      "Split representation e1 and e2 (gray circles)",
      "Disentangle connection (dashed line)",
      "Function ψ block (green rectangle)",
      "Predictor Pred block (green rectangle)",
      "Decoder Dec block (green rectangle)",
      "Output nodes y and x' (white circles)"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_5",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg",
    "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"scattered colored dots with various shapes (circles, triangles, plus signs)\",\n    \"legend showing five categories: upper-left, lower-right, front, upper-right, lower-left\",\n    \"blue circles marking 'front' category\",\n    \"orange plus signs marking 'lower-right' category\",\n    \"cyan stars marking 'upper-right' category\",\n    \"lime circles marking 'lower-left' category\",\n    \"pink triangles marking 'upper-left' category\",\n    \"t-SNE 2D projection layout\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue circles representing front lighting remain clustered together in this t-SNE projection?\",\n      \"answer\": \"This is a t-SNE visualization of embedding e₁ labeled by lighting condition, showing that the model successfully groups images with the same front lighting into a coherent cluster in the embedding space.\",\n      \"visual_anchor\": \"blue circles (front lighting category) forming a distinct cluster\",\n      \"text_evidence\": \"Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the separation between orange lower-right markers and cyan upper-right markers confirm that embedding e₁ captures lighting variations?\",\n      \"answer\": \"Yes, the clear spatial separation between different lighting conditions (lower-right vs upper-right) in the t-SNE visualization demonstrates that embedding e₁ successfully disentangles and represents lighting condition as a distinct factor.\",\n      \"visual_anchor\": \"orange plus signs (lower-right) and cyan stars (upper-right) spatially separated\",\n      \"text_evidence\": \"Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the clustering pattern of lime lower-left circles compare to the model's 0.74 accuracy on Chairs?\",\n      \"answer\": \"The tight clustering of lower-left lighting conditions demonstrates successful disentanglement in the Yale-B dataset, which is consistent with the model achieving improved performance (Ay=0.74) on the separate Chairs dataset.\",\n      \"visual_anchor\": \"lime circles (lower-left category) showing clustering pattern\",\n      \"text_evidence\": \"Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired. Metric CAI Ours Ay 0.68 0.74 Az 0.69 0.34\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "scattered colored dots with various shapes (circles, triangles, plus signs)",
        "legend showing five categories: upper-left, lower-right, front, upper-right, lower-left",
        "blue circles marking 'front' category",
        "orange plus signs marking 'lower-right' category",
        "cyan stars marking 'upper-right' category",
        "lime circles marking 'lower-left' category",
        "pink triangles marking 'upper-left' category",
        "t-SNE 2D projection layout"
      ],
      "queries": [
        {
          "query": "Why do the blue circles representing front lighting remain clustered together in this t-SNE projection?",
          "answer": "This is a t-SNE visualization of embedding e₁ labeled by lighting condition, showing that the model successfully groups images with the same front lighting into a coherent cluster in the embedding space.",
          "visual_anchor": "blue circles (front lighting category) forming a distinct cluster",
          "text_evidence": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the separation between orange lower-right markers and cyan upper-right markers confirm that embedding e₁ captures lighting variations?",
          "answer": "Yes, the clear spatial separation between different lighting conditions (lower-right vs upper-right) in the t-SNE visualization demonstrates that embedding e₁ successfully disentangles and represents lighting condition as a distinct factor.",
          "visual_anchor": "orange plus signs (lower-right) and cyan stars (upper-right) spatially separated",
          "text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the clustering pattern of lime lower-left circles compare to the model's 0.74 accuracy on Chairs?",
          "answer": "The tight clustering of lower-left lighting conditions demonstrates successful disentanglement in the Yale-B dataset, which is consistent with the model achieving improved performance (Ay=0.74) on the separate Chairs dataset.",
          "visual_anchor": "lime circles (lower-left category) showing clustering pattern",
          "text_evidence": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired. Metric CAI Ours Ay 0.68 0.74 Az 0.69 0.34",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue circles representing front lighting remain clustered together in this t-SNE projection?",
        "answer": "This is a t-SNE visualization of embedding e₁ labeled by lighting condition, showing that the model successfully groups images with the same front lighting into a coherent cluster in the embedding space.",
        "visual_anchor": "blue circles (front lighting category) forming a distinct cluster",
        "text_evidence": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the separation between orange lower-right markers and cyan upper-right markers confirm that embedding e₁ captures lighting variations?",
        "answer": "Yes, the clear spatial separation between different lighting conditions (lower-right vs upper-right) in the t-SNE visualization demonstrates that embedding e₁ successfully disentangles and represents lighting condition as a distinct factor.",
        "visual_anchor": "orange plus signs (lower-right) and cyan stars (upper-right) spatially separated",
        "text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the clustering pattern of lime lower-left circles compare to the model's 0.74 accuracy on Chairs?",
        "answer": "The tight clustering of lower-left lighting conditions demonstrates successful disentanglement in the Yale-B dataset, which is consistent with the model achieving improved performance (Ay=0.74) on the separate Chairs dataset.",
        "visual_anchor": "lime circles (lower-left category) showing clustering pattern",
        "text_evidence": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired. Metric CAI Ours Ay 0.68 0.74 Az 0.69 0.34",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "scattered colored dots with various shapes (circles, triangles, plus signs)",
      "legend showing five categories: upper-left, lower-right, front, upper-right, lower-left",
      "blue circles marking 'front' category",
      "orange plus signs marking 'lower-right' category",
      "cyan stars marking 'upper-right' category",
      "lime circles marking 'lower-left' category",
      "pink triangles marking 'upper-left' category",
      "t-SNE 2D projection layout"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_13",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg",
    "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Three columns per row showing real image, reconstruction from e1, and reconstruction from e2\",\n    \"Four rows displaying different chair samples\",\n    \"Left column shows clear chair images with varying viewpoints\",\n    \"Middle column shows blurred reconstructions from e1\",\n    \"Right column shows blurred reconstructions from e2\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the middle column reconstructions from e1 appear blurred compared to the sharp left column originals?\",\n      \"answer\": \"The reconstructions from e1 demonstrate the model's ability to handle expected variations that are unobserved or infrequently seen during training, which is achieved through the unsupervised adversarial approach rather than requiring supervised rotation angle information.\",\n      \"visual_anchor\": \"middle column showing blurred chair reconstructions from e1\",\n      \"text_evidence\": \"A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the rightmost column e2 reconstructions support claims that unsupervised adversarial models outperform baseline ablation versions?\",\n      \"answer\": \"Yes, the e2 reconstructions demonstrate the model's learned representation capabilities. The results show that the unsupervised adversarial model performs better than baseline ablations and even outperforms CAI which uses supervised information.\",\n      \"visual_anchor\": \"rightmost column showing e2 reconstructions with preserved chair structure\",\n      \"text_evidence\": \"our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the blurred middle and right columns relate to synthetic generation methods for handling rotation variations?\",\n      \"answer\": \"The blurred reconstructions from e1 and e2 represent learned latent representations that capture rotation and viewpoint variations. These demonstrate synthetic generation capabilities ranging from simple operations to more sophisticated approaches for handling expected transformations.\",\n      \"visual_anchor\": \"middle and right columns showing progressively blurred reconstructions\",\n      \"text_evidence\": \"data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Three columns per row showing real image, reconstruction from e1, and reconstruction from e2",
        "Four rows displaying different chair samples",
        "Left column shows clear chair images with varying viewpoints",
        "Middle column shows blurred reconstructions from e1",
        "Right column shows blurred reconstructions from e2"
      ],
      "queries": [
        {
          "query": "Why do the middle column reconstructions from e1 appear blurred compared to the sharp left column originals?",
          "answer": "The reconstructions from e1 demonstrate the model's ability to handle expected variations that are unobserved or infrequently seen during training, which is achieved through the unsupervised adversarial approach rather than requiring supervised rotation angle information.",
          "visual_anchor": "middle column showing blurred chair reconstructions from e1",
          "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the rightmost column e2 reconstructions support claims that unsupervised adversarial models outperform baseline ablation versions?",
          "answer": "Yes, the e2 reconstructions demonstrate the model's learned representation capabilities. The results show that the unsupervised adversarial model performs better than baseline ablations and even outperforms CAI which uses supervised information.",
          "visual_anchor": "rightmost column showing e2 reconstructions with preserved chair structure",
          "text_evidence": "our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle",
          "query_type": "value_context"
        },
        {
          "query": "How do the blurred middle and right columns relate to synthetic generation methods for handling rotation variations?",
          "answer": "The blurred reconstructions from e1 and e2 represent learned latent representations that capture rotation and viewpoint variations. These demonstrate synthetic generation capabilities ranging from simple operations to more sophisticated approaches for handling expected transformations.",
          "visual_anchor": "middle and right columns showing progressively blurred reconstructions",
          "text_evidence": "data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the middle column reconstructions from e1 appear blurred compared to the sharp left column originals?",
        "answer": "The reconstructions from e1 demonstrate the model's ability to handle expected variations that are unobserved or infrequently seen during training, which is achieved through the unsupervised adversarial approach rather than requiring supervised rotation angle information.",
        "visual_anchor": "middle column showing blurred chair reconstructions from e1",
        "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the rightmost column e2 reconstructions support claims that unsupervised adversarial models outperform baseline ablation versions?",
        "answer": "Yes, the e2 reconstructions demonstrate the model's learned representation capabilities. The results show that the unsupervised adversarial model performs better than baseline ablations and even outperforms CAI which uses supervised information.",
        "visual_anchor": "rightmost column showing e2 reconstructions with preserved chair structure",
        "text_evidence": "our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle",
        "query_type": "value_context"
      },
      {
        "query": "How do the blurred middle and right columns relate to synthetic generation methods for handling rotation variations?",
        "answer": "The blurred reconstructions from e1 and e2 represent learned latent representations that capture rotation and viewpoint variations. These demonstrate synthetic generation capabilities ranging from simple operations to more sophisticated approaches for handling expected transformations.",
        "visual_anchor": "middle and right columns showing progressively blurred reconstructions",
        "text_evidence": "data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Three columns per row showing real image, reconstruction from e1, and reconstruction from e2",
      "Four rows displaying different chair samples",
      "Left column shows clear chair images with varying viewpoints",
      "Middle column shows blurred reconstructions from e1",
      "Right column shows blurred reconstructions from e2"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_16",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg",
    "caption": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue cluster on the left side centered around x=-45.0\",\n    \"Green cluster in the center around x=0.0\",\n    \"Red cluster on the right side around x=45.0\",\n    \"t-SNE 2D scatter plot with three distinct color-coded groups\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue cluster at x=-45.0 show tighter grouping than the red cluster at x=45.0 in this embedding?\",\n      \"answer\": \"The visualization shows e₁ labeled by subject-ID with numerical markers. The tighter blue clustering likely reflects better subject-ID separation in this region of the embedding space compared to the more dispersed red region.\",\n      \"visual_anchor\": \"Blue cluster at x=-45.0 versus red cluster at x=45.0\",\n      \"text_evidence\": \"Figure 2 shows t-SNE [15] visualization of raw data and embeddings e₁ and e₂ for our model.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the three-cluster separation at positions -45.0, 0.0, and 45.0 align with the desired high Ay metric of 0.74?\",\n      \"answer\": \"Yes, the clear spatial separation of the three colored clusters along the horizontal axis demonstrates effective disentanglement of the y-axis rotation factor, which directly corresponds to achieving the high Ay accuracy of 0.74.\",\n      \"visual_anchor\": \"Three distinct clusters positioned at -45.0 (blue), 0.0 (green), and 45.0 (red)\",\n      \"text_evidence\": \"Table 2: Results on Chairs. High Ay and low Az are desired. Ay: 0.68 (CAI), 0.74 (Ours). Az: 0.69 (CAI), 0.34 (Ours).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the green cluster's central position at x=0.0 relate to lighting condition versus subject-ID labeling distinctions?\",\n      \"answer\": \"The green cluster represents an intermediate embedding state. Since e₂ is labeled by lighting condition and e₁ by subject-ID, the central positioning at x=0.0 suggests this group captures a distinct categorical feature in the disentangled representation.\",\n      \"visual_anchor\": \"Green cluster centered at x=0.0\",\n      \"text_evidence\": \"Extended Yale-B – t-SNE visualization of (a) raw data, (b) e₂ labeled by lighting condition, (c) e₁ labeled by lighting condition, and (d) e₁ labeled by subject-ID (numerical markers, not colors).\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue cluster on the left side centered around x=-45.0",
        "Green cluster in the center around x=0.0",
        "Red cluster on the right side around x=45.0",
        "t-SNE 2D scatter plot with three distinct color-coded groups"
      ],
      "queries": [
        {
          "query": "Why does the blue cluster at x=-45.0 show tighter grouping than the red cluster at x=45.0 in this embedding?",
          "answer": "The visualization shows e₁ labeled by subject-ID with numerical markers. The tighter blue clustering likely reflects better subject-ID separation in this region of the embedding space compared to the more dispersed red region.",
          "visual_anchor": "Blue cluster at x=-45.0 versus red cluster at x=45.0",
          "text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings e₁ and e₂ for our model.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the three-cluster separation at positions -45.0, 0.0, and 45.0 align with the desired high Ay metric of 0.74?",
          "answer": "Yes, the clear spatial separation of the three colored clusters along the horizontal axis demonstrates effective disentanglement of the y-axis rotation factor, which directly corresponds to achieving the high Ay accuracy of 0.74.",
          "visual_anchor": "Three distinct clusters positioned at -45.0 (blue), 0.0 (green), and 45.0 (red)",
          "text_evidence": "Table 2: Results on Chairs. High Ay and low Az are desired. Ay: 0.68 (CAI), 0.74 (Ours). Az: 0.69 (CAI), 0.34 (Ours).",
          "query_type": "value_context"
        },
        {
          "query": "How does the green cluster's central position at x=0.0 relate to lighting condition versus subject-ID labeling distinctions?",
          "answer": "The green cluster represents an intermediate embedding state. Since e₂ is labeled by lighting condition and e₁ by subject-ID, the central positioning at x=0.0 suggests this group captures a distinct categorical feature in the disentangled representation.",
          "visual_anchor": "Green cluster centered at x=0.0",
          "text_evidence": "Extended Yale-B – t-SNE visualization of (a) raw data, (b) e₂ labeled by lighting condition, (c) e₁ labeled by lighting condition, and (d) e₁ labeled by subject-ID (numerical markers, not colors).",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue cluster at x=-45.0 show tighter grouping than the red cluster at x=45.0 in this embedding?",
        "answer": "The visualization shows e₁ labeled by subject-ID with numerical markers. The tighter blue clustering likely reflects better subject-ID separation in this region of the embedding space compared to the more dispersed red region.",
        "visual_anchor": "Blue cluster at x=-45.0 versus red cluster at x=45.0",
        "text_evidence": "Figure 2 shows t-SNE [15] visualization of raw data and embeddings e₁ and e₂ for our model.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the three-cluster separation at positions -45.0, 0.0, and 45.0 align with the desired high Ay metric of 0.74?",
        "answer": "Yes, the clear spatial separation of the three colored clusters along the horizontal axis demonstrates effective disentanglement of the y-axis rotation factor, which directly corresponds to achieving the high Ay accuracy of 0.74.",
        "visual_anchor": "Three distinct clusters positioned at -45.0 (blue), 0.0 (green), and 45.0 (red)",
        "text_evidence": "Table 2: Results on Chairs. High Ay and low Az are desired. Ay: 0.68 (CAI), 0.74 (Ours). Az: 0.69 (CAI), 0.34 (Ours).",
        "query_type": "value_context"
      },
      {
        "query": "How does the green cluster's central position at x=0.0 relate to lighting condition versus subject-ID labeling distinctions?",
        "answer": "The green cluster represents an intermediate embedding state. Since e₂ is labeled by lighting condition and e₁ by subject-ID, the central positioning at x=0.0 suggests this group captures a distinct categorical feature in the disentangled representation.",
        "visual_anchor": "Green cluster centered at x=0.0",
        "text_evidence": "Extended Yale-B – t-SNE visualization of (a) raw data, (b) e₂ labeled by lighting condition, (c) e₁ labeled by lighting condition, and (d) e₁ labeled by subject-ID (numerical markers, not colors).",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue cluster on the left side centered around x=-45.0",
      "Green cluster in the center around x=0.0",
      "Red cluster on the right side around x=45.0",
      "t-SNE 2D scatter plot with three distinct color-coded groups"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_18",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg",
    "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"t-SNE scatter plot with 10 distinct colored clusters (0-9)\", \"cluster separation and overlap patterns\", \"color-coded legend showing digits 0-9\", \"spatial distribution of data points\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the well-separated clusters in the visualization of e₁ demonstrate improved representation learning compared to raw data?\",\n      \"answer\": \"Yes, the e₁ visualization shows clearer cluster separation compared to the raw data visualization, demonstrating that the model learned more discriminative features for the MNIST-ROT dataset.\",\n      \"visual_anchor\": \"comparison between overlapping clusters in raw data versus distinct separated clusters in e₁ visualization\",\n      \"text_evidence\": \"Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the 0.977 accuracy achieved by the model relate to the cluster quality visible in the e₁ t-SNE plot?\",\n      \"answer\": \"The 0.977 accuracy at Θ angle reflects the well-separated and distinct clusters visible in the e₁ t-SNE visualization, indicating the learned representation effectively captures digit identity despite rotation.\",\n      \"visual_anchor\": \"well-separated colored clusters in e₁ visualization corresponding to digits 0-9\",\n      \"text_evidence\": \"Metric Angle CAI Ours B0 B1 Ay Θ 0.958 0.977 0.974 0.972 ±55° 0.826 0.856 0.826 0.829 ±65° 0.662\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do certain digit clusters like red and purple remain partially overlapped in the e₁ visualization despite rotation invariance training?\",\n      \"answer\": \"Some digits share similar structural features even after learning rotation-invariant representations, leading to partial overlap. This reflects inherent visual similarities between certain digit classes that the model cannot fully disambiguate.\",\n      \"visual_anchor\": \"overlapping red (digit 2) and purple (digit 3) clusters in the central region of e₁ plot\",\n      \"text_evidence\": \"Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "t-SNE scatter plot with 10 distinct colored clusters (0-9)",
        "cluster separation and overlap patterns",
        "color-coded legend showing digits 0-9",
        "spatial distribution of data points"
      ],
      "queries": [
        {
          "query": "Do the well-separated clusters in the visualization of e₁ demonstrate improved representation learning compared to raw data?",
          "answer": "Yes, the e₁ visualization shows clearer cluster separation compared to the raw data visualization, demonstrating that the model learned more discriminative features for the MNIST-ROT dataset.",
          "visual_anchor": "comparison between overlapping clusters in raw data versus distinct separated clusters in e₁ visualization",
          "text_evidence": "Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the 0.977 accuracy achieved by the model relate to the cluster quality visible in the e₁ t-SNE plot?",
          "answer": "The 0.977 accuracy at Θ angle reflects the well-separated and distinct clusters visible in the e₁ t-SNE visualization, indicating the learned representation effectively captures digit identity despite rotation.",
          "visual_anchor": "well-separated colored clusters in e₁ visualization corresponding to digits 0-9",
          "text_evidence": "Metric Angle CAI Ours B0 B1 Ay Θ 0.958 0.977 0.974 0.972 ±55° 0.826 0.856 0.826 0.829 ±65° 0.662",
          "query_type": "value_context"
        },
        {
          "query": "Why do certain digit clusters like red and purple remain partially overlapped in the e₁ visualization despite rotation invariance training?",
          "answer": "Some digits share similar structural features even after learning rotation-invariant representations, leading to partial overlap. This reflects inherent visual similarities between certain digit classes that the model cannot fully disambiguate.",
          "visual_anchor": "overlapping red (digit 2) and purple (digit 3) clusters in the central region of e₁ plot",
          "text_evidence": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the well-separated clusters in the visualization of e₁ demonstrate improved representation learning compared to raw data?",
        "answer": "Yes, the e₁ visualization shows clearer cluster separation compared to the raw data visualization, demonstrating that the model learned more discriminative features for the MNIST-ROT dataset.",
        "visual_anchor": "comparison between overlapping clusters in raw data versus distinct separated clusters in e₁ visualization",
        "text_evidence": "Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the 0.977 accuracy achieved by the model relate to the cluster quality visible in the e₁ t-SNE plot?",
        "answer": "The 0.977 accuracy at Θ angle reflects the well-separated and distinct clusters visible in the e₁ t-SNE visualization, indicating the learned representation effectively captures digit identity despite rotation.",
        "visual_anchor": "well-separated colored clusters in e₁ visualization corresponding to digits 0-9",
        "text_evidence": "Metric Angle CAI Ours B0 B1 Ay Θ 0.958 0.977 0.974 0.972 ±55° 0.826 0.856 0.826 0.829 ±65° 0.662",
        "query_type": "value_context"
      },
      {
        "query": "Why do certain digit clusters like red and purple remain partially overlapped in the e₁ visualization despite rotation invariance training?",
        "answer": "Some digits share similar structural features even after learning rotation-invariant representations, leading to partial overlap. This reflects inherent visual similarities between certain digit classes that the model cannot fully disambiguate.",
        "visual_anchor": "overlapping red (digit 2) and purple (digit 3) clusters in the central region of e₁ plot",
        "text_evidence": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "t-SNE scatter plot with 10 distinct colored clusters (0-9)",
      "cluster separation and overlap patterns",
      "color-coded legend showing digits 0-9",
      "spatial distribution of data points"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_19",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg",
    "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"t-SNE scatter plot with 10 colored clusters (0-9)\",\n    \"Green cluster (label 0) in bottom-left region\",\n    \"Blue cluster (label 1) in top-center region\",\n    \"Orange/yellow cluster (label 4) in top-center region\",\n    \"Purple cluster (label 5) in center region\",\n    \"Red cluster (label 7) in center-right region\",\n    \"Cyan cluster (label 8) in bottom-right region\",\n    \"Black cluster (label 9) in far right region\",\n    \"Maroon cluster (label 3) in top-right region\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the purple and orange clusters overlap in the top-center despite training on rotations from -45 to +45 degrees?\",\n      \"answer\": \"The proposed Unsupervised Adversarial Invariance model was trained on Θ = {0, ±22.5, ±45} but visualized on unseen rotations Θ = {±55}, testing generalization to variations not present in training data.\",\n      \"visual_anchor\": \"overlapping purple (label 5) and orange (label 4) clusters in top-center region\",\n      \"text_evidence\": \"Models trained on Θ = {0, ±22.5, ±45}. Visualization generated for Θ = {±55}.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the distinct separation of the green cluster in the bottom-left support learning invariance to unobserved rotation angles?\",\n      \"answer\": \"Yes, the clear separation demonstrates that the model effectively uses synthetic data augmentation to learn robustness to expected yet unobserved variations, as the visualization shows generalization to ±55 degrees despite training only up to ±45 degrees.\",\n      \"visual_anchor\": \"green cluster (label 0) distinctly separated in bottom-left region\",\n      \"text_evidence\": \"A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the black cluster's tight grouping in the far right compare to baseline model performance on rotated digits?\",\n      \"answer\": \"The tight black cluster (label 9) in the proposed model's embedding suggests superior invariance learning compared to baseline model B₀, demonstrating effective adversarial training for rotation robustness on the MNIST-ROT dataset.\",\n      \"visual_anchor\": \"tightly grouped black cluster (label 9) in far right region\",\n      \"text_evidence\": \"t-SNE visualization of MNIST-ROT e₁ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model B₀ (b) & (d)\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "t-SNE scatter plot with 10 colored clusters (0-9)",
        "Green cluster (label 0) in bottom-left region",
        "Blue cluster (label 1) in top-center region",
        "Orange/yellow cluster (label 4) in top-center region",
        "Purple cluster (label 5) in center region",
        "Red cluster (label 7) in center-right region",
        "Cyan cluster (label 8) in bottom-right region",
        "Black cluster (label 9) in far right region",
        "Maroon cluster (label 3) in top-right region"
      ],
      "queries": [
        {
          "query": "Why do the purple and orange clusters overlap in the top-center despite training on rotations from -45 to +45 degrees?",
          "answer": "The proposed Unsupervised Adversarial Invariance model was trained on Θ = {0, ±22.5, ±45} but visualized on unseen rotations Θ = {±55}, testing generalization to variations not present in training data.",
          "visual_anchor": "overlapping purple (label 5) and orange (label 4) clusters in top-center region",
          "text_evidence": "Models trained on Θ = {0, ±22.5, ±45}. Visualization generated for Θ = {±55}.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the distinct separation of the green cluster in the bottom-left support learning invariance to unobserved rotation angles?",
          "answer": "Yes, the clear separation demonstrates that the model effectively uses synthetic data augmentation to learn robustness to expected yet unobserved variations, as the visualization shows generalization to ±55 degrees despite training only up to ±45 degrees.",
          "visual_anchor": "green cluster (label 0) distinctly separated in bottom-left region",
          "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation",
          "query_type": "value_context"
        },
        {
          "query": "How does the black cluster's tight grouping in the far right compare to baseline model performance on rotated digits?",
          "answer": "The tight black cluster (label 9) in the proposed model's embedding suggests superior invariance learning compared to baseline model B₀, demonstrating effective adversarial training for rotation robustness on the MNIST-ROT dataset.",
          "visual_anchor": "tightly grouped black cluster (label 9) in far right region",
          "text_evidence": "t-SNE visualization of MNIST-ROT e₁ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model B₀ (b) & (d)",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the purple and orange clusters overlap in the top-center despite training on rotations from -45 to +45 degrees?",
        "answer": "The proposed Unsupervised Adversarial Invariance model was trained on Θ = {0, ±22.5, ±45} but visualized on unseen rotations Θ = {±55}, testing generalization to variations not present in training data.",
        "visual_anchor": "overlapping purple (label 5) and orange (label 4) clusters in top-center region",
        "text_evidence": "Models trained on Θ = {0, ±22.5, ±45}. Visualization generated for Θ = {±55}.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the distinct separation of the green cluster in the bottom-left support learning invariance to unobserved rotation angles?",
        "answer": "Yes, the clear separation demonstrates that the model effectively uses synthetic data augmentation to learn robustness to expected yet unobserved variations, as the visualization shows generalization to ±55 degrees despite training only up to ±45 degrees.",
        "visual_anchor": "green cluster (label 0) distinctly separated in bottom-left region",
        "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation",
        "query_type": "value_context"
      },
      {
        "query": "How does the black cluster's tight grouping in the far right compare to baseline model performance on rotated digits?",
        "answer": "The tight black cluster (label 9) in the proposed model's embedding suggests superior invariance learning compared to baseline model B₀, demonstrating effective adversarial training for rotation robustness on the MNIST-ROT dataset.",
        "visual_anchor": "tightly grouped black cluster (label 9) in far right region",
        "text_evidence": "t-SNE visualization of MNIST-ROT e₁ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model B₀ (b) & (d)",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "t-SNE scatter plot with 10 colored clusters (0-9)",
      "Green cluster (label 0) in bottom-left region",
      "Blue cluster (label 1) in top-center region",
      "Orange/yellow cluster (label 4) in top-center region",
      "Purple cluster (label 5) in center region",
      "Red cluster (label 7) in center-right region",
      "Cyan cluster (label 8) in bottom-right region",
      "Black cluster (label 9) in far right region",
      "Maroon cluster (label 3) in top-right region"
    ]
  },
  {
    "doc_id": "1809.10083",
    "figure_id": "1809.10083_fig_22",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg",
    "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Three columns per block showing real, reconstruction from e₁, and reconstruction from e₂\",\n    \"Ten blocks showing digits 0-9 with rotational variations\",\n    \"Left column: original rotated MNIST digits\",\n    \"Middle column: reconstructions from encoder e₁\",\n    \"Right column: reconstructions from encoder e₂\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the middle column reconstructions preserve digit identity better than the right column reconstructions for rotated inputs?\",\n      \"answer\": \"The middle column shows reconstructions from e₁, which likely captures class-relevant features invariant to rotation. The right column from e₂ appears to encode nuisance factors like rotation, making digit identity less clear.\",\n      \"visual_anchor\": \"middle column reconstructions showing clear digit shapes versus right column showing degraded representations\",\n      \"text_evidence\": \"Data is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the left column rotated originals demonstrate the type of synthetic variation needed when training data lacks rotational diversity?\",\n      \"answer\": \"Yes, the left column displays rotated MNIST digits that exemplify synthetic augmentation through rotation operations. This addresses the problem of learning robustness to expected but unobserved variations during training.\",\n      \"visual_anchor\": \"left column showing real rotated digit samples across all ten blocks\",\n      \"text_evidence\": \"A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the three-column reconstruction structure relate to inducing invariance by separating domain information from prediction tasks?\",\n      \"answer\": \"The structure separates class-relevant features (e₁, middle column) from nuisance factors (e₂, right column), similar to domain adaptation where prediction must be invariant to domain. This disentanglement enables learning invariant representations.\",\n      \"visual_anchor\": \"three-column organization showing real, e₁ reconstruction, and e₂ reconstruction across all digit blocks\",\n      \"text_evidence\": \"Domain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the \\\"domain\\\" information.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Three columns per block showing real, reconstruction from e₁, and reconstruction from e₂",
        "Ten blocks showing digits 0-9 with rotational variations",
        "Left column: original rotated MNIST digits",
        "Middle column: reconstructions from encoder e₁",
        "Right column: reconstructions from encoder e₂"
      ],
      "queries": [
        {
          "query": "Why do the middle column reconstructions preserve digit identity better than the right column reconstructions for rotated inputs?",
          "answer": "The middle column shows reconstructions from e₁, which likely captures class-relevant features invariant to rotation. The right column from e₂ appears to encode nuisance factors like rotation, making digit identity less clear.",
          "visual_anchor": "middle column reconstructions showing clear digit shapes versus right column showing degraded representations",
          "text_evidence": "Data is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the left column rotated originals demonstrate the type of synthetic variation needed when training data lacks rotational diversity?",
          "answer": "Yes, the left column displays rotated MNIST digits that exemplify synthetic augmentation through rotation operations. This addresses the problem of learning robustness to expected but unobserved variations during training.",
          "visual_anchor": "left column showing real rotated digit samples across all ten blocks",
          "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks",
          "query_type": "value_context"
        },
        {
          "query": "How does the three-column reconstruction structure relate to inducing invariance by separating domain information from prediction tasks?",
          "answer": "The structure separates class-relevant features (e₁, middle column) from nuisance factors (e₂, right column), similar to domain adaptation where prediction must be invariant to domain. This disentanglement enables learning invariant representations.",
          "visual_anchor": "three-column organization showing real, e₁ reconstruction, and e₂ reconstruction across all digit blocks",
          "text_evidence": "Domain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the \"domain\" information.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the middle column reconstructions preserve digit identity better than the right column reconstructions for rotated inputs?",
        "answer": "The middle column shows reconstructions from e₁, which likely captures class-relevant features invariant to rotation. The right column from e₂ appears to encode nuisance factors like rotation, making digit identity less clear.",
        "visual_anchor": "middle column reconstructions showing clear digit shapes versus right column showing degraded representations",
        "text_evidence": "Data is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the left column rotated originals demonstrate the type of synthetic variation needed when training data lacks rotational diversity?",
        "answer": "Yes, the left column displays rotated MNIST digits that exemplify synthetic augmentation through rotation operations. This addresses the problem of learning robustness to expected but unobserved variations during training.",
        "visual_anchor": "left column showing real rotated digit samples across all ten blocks",
        "text_evidence": "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks",
        "query_type": "value_context"
      },
      {
        "query": "How does the three-column reconstruction structure relate to inducing invariance by separating domain information from prediction tasks?",
        "answer": "The structure separates class-relevant features (e₁, middle column) from nuisance factors (e₂, right column), similar to domain adaptation where prediction must be invariant to domain. This disentanglement enables learning invariant representations.",
        "visual_anchor": "three-column organization showing real, e₁ reconstruction, and e₂ reconstruction across all digit blocks",
        "text_evidence": "Domain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the \"domain\" information.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Three columns per block showing real, reconstruction from e₁, and reconstruction from e₂",
      "Ten blocks showing digits 0-9 with rotational variations",
      "Left column: original rotated MNIST digits",
      "Middle column: reconstructions from encoder e₁",
      "Right column: reconstructions from encoder e₂"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg",
    "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Raw Data cylinder on left\",\n    \"Original Dataset box with training/validation/testing subdivisions\",\n    \"fair pre-processor trapezoid in lower left\",\n    \"Transformed Dataset box in lower center\",\n    \"fair classifier trapezoid in center\",\n    \"classifier triangles (two instances)\",\n    \"Predicted Dataset box in upper right\",\n    \"Fair Predicted Dataset box in lower right\",\n    \"fair post-processor trapezoid in right side\",\n    \"arrows labeled 'learn', 'apply', 'convert to CSV', 'data pre-processing and loading'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the fair pre-processor trapezoid receive input from training, validation, and testing boxes simultaneously?\",\n      \"answer\": \"The fair pre-processor learns from all three dataset splits (training, validation, testing) to transform the original dataset into a fairer version, as fairness transformations require understanding patterns across all data partitions.\",\n      \"visual_anchor\": \"fair pre-processor trapezoid with three incoming arrows from training, validation, and testing boxes\",\n      \"text_evidence\": \"An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the Transformed Dataset box enable metrics calculation as stated for transformed datasets?\",\n      \"answer\": \"The Transformed Dataset box outputs to a classifier that learns and applies predictions, creating data points where metrics can be calculated on the transformed data itself and compared between transformed and predicted datasets.\",\n      \"visual_anchor\": \"Transformed Dataset box with training and testing subdivisions, positioned in lower center\",\n      \"text_evidence\": \"Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the fair classifier trapezoid placement between Transformed Dataset and Fair Predicted Dataset support in-processing bias mitigation?\",\n      \"answer\": \"Yes, the fair classifier trapezoid positioned in the center receives 'learn' arrows from multiple dataset boxes and applies fairness constraints during model training, which aligns with in-processing techniques that operate during the learning phase.\",\n      \"visual_anchor\": \"fair classifier trapezoid labeled 'fair in-processing' in center of diagram\",\n      \"text_evidence\": \"BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Raw Data cylinder on left",
        "Original Dataset box with training/validation/testing subdivisions",
        "fair pre-processor trapezoid in lower left",
        "Transformed Dataset box in lower center",
        "fair classifier trapezoid in center",
        "classifier triangles (two instances)",
        "Predicted Dataset box in upper right",
        "Fair Predicted Dataset box in lower right",
        "fair post-processor trapezoid in right side",
        "arrows labeled 'learn', 'apply', 'convert to CSV', 'data pre-processing and loading'"
      ],
      "queries": [
        {
          "query": "Why does the fair pre-processor trapezoid receive input from training, validation, and testing boxes simultaneously?",
          "answer": "The fair pre-processor learns from all three dataset splits (training, validation, testing) to transform the original dataset into a fairer version, as fairness transformations require understanding patterns across all data partitions.",
          "visual_anchor": "fair pre-processor trapezoid with three incoming arrows from training, validation, and testing boxes",
          "text_evidence": "An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the Transformed Dataset box enable metrics calculation as stated for transformed datasets?",
          "answer": "The Transformed Dataset box outputs to a classifier that learns and applies predictions, creating data points where metrics can be calculated on the transformed data itself and compared between transformed and predicted datasets.",
          "visual_anchor": "Transformed Dataset box with training and testing subdivisions, positioned in lower center",
          "text_evidence": "Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets.",
          "query_type": "value_context"
        },
        {
          "query": "Does the fair classifier trapezoid placement between Transformed Dataset and Fair Predicted Dataset support in-processing bias mitigation?",
          "answer": "Yes, the fair classifier trapezoid positioned in the center receives 'learn' arrows from multiple dataset boxes and applies fairness constraints during model training, which aligns with in-processing techniques that operate during the learning phase.",
          "visual_anchor": "fair classifier trapezoid labeled 'fair in-processing' in center of diagram",
          "text_evidence": "BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the fair pre-processor trapezoid receive input from training, validation, and testing boxes simultaneously?",
        "answer": "The fair pre-processor learns from all three dataset splits (training, validation, testing) to transform the original dataset into a fairer version, as fairness transformations require understanding patterns across all data partitions.",
        "visual_anchor": "fair pre-processor trapezoid with three incoming arrows from training, validation, and testing boxes",
        "text_evidence": "An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the Transformed Dataset box enable metrics calculation as stated for transformed datasets?",
        "answer": "The Transformed Dataset box outputs to a classifier that learns and applies predictions, creating data points where metrics can be calculated on the transformed data itself and compared between transformed and predicted datasets.",
        "visual_anchor": "Transformed Dataset box with training and testing subdivisions, positioned in lower center",
        "text_evidence": "Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets.",
        "query_type": "value_context"
      },
      {
        "query": "Does the fair classifier trapezoid placement between Transformed Dataset and Fair Predicted Dataset support in-processing bias mitigation?",
        "answer": "Yes, the fair classifier trapezoid positioned in the center receives 'learn' arrows from multiple dataset boxes and applies fairness constraints during model training, which aligns with in-processing techniques that operate during the learning phase.",
        "visual_anchor": "fair classifier trapezoid labeled 'fair in-processing' in center of diagram",
        "text_evidence": "BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Raw Data cylinder on left",
      "Original Dataset box with training/validation/testing subdivisions",
      "fair pre-processor trapezoid in lower left",
      "Transformed Dataset box in lower center",
      "fair classifier trapezoid in center",
      "classifier triangles (two instances)",
      "Predicted Dataset box in upper right",
      "Fair Predicted Dataset box in lower right",
      "fair post-processor trapezoid in right side",
      "arrows labeled 'learn', 'apply', 'convert to CSV', 'data pre-processing and loading'"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg",
    "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.",
    "original_quality_score": 0.9,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"X-axis showing age ranges: 17-27, 22-27, 27-32, 32-37\",\n    \"Y-axis showing ratio of favorable to total ranging from 0 to 0.25\",\n    \"Four blue bars of increasing height\",\n    \"Horizontal line at approximately 0.14\",\n    \"Tallest bar at age 32-37 reaching approximately 0.25\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the bar at 32-37 exceed 0.20 when the localization approach targets the 43-58 range in older groups?\",\n      \"answer\": \"The figure shows the younger (unprivileged) group where the 17-27 range is localized. The 43-58 range applies to the separate older (privileged) group analysis, not shown in this panel.\",\n      \"visual_anchor\": \"tallest blue bar at age 32-37 reaching approximately 0.25\",\n      \"text_evidence\": \"The 17–27 range in the younger group and the 43–58 range in the older group would be localized by the approach.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the horizontal line at 0.14 represent the threshold separating privileged from unprivileged age groups in German Credit?\",\n      \"answer\": \"No, the horizontal line likely represents a reference threshold for fairness metrics. The caption indicates this panel shows the younger (unprivileged) group specifically, while privileged refers to the separate older group.\",\n      \"visual_anchor\": \"horizontal line intersecting at approximately 0.14 on the y-axis\",\n      \"text_evidence\": \"Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the increasing trend from 0.05 at 22-27 to 0.25 at 32-37 relate to dataset fairness checks?\",\n      \"answer\": \"The increasing ratio of favorable outcomes across age ranges demonstrates bias patterns that the DatasetMetric class computes to check for bias in datasets and models, as part of the fairness analysis framework.\",\n      \"visual_anchor\": \"blue bars increasing from approximately 0.05 at age 22-27 to 0.25 at age 32-37\",\n      \"text_evidence\": \"The Metric class and its subclasses compute various individual and group fairness metrics to check for bias in datasets and models.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "X-axis showing age ranges: 17-27, 22-27, 27-32, 32-37",
        "Y-axis showing ratio of favorable to total ranging from 0 to 0.25",
        "Four blue bars of increasing height",
        "Horizontal line at approximately 0.14",
        "Tallest bar at age 32-37 reaching approximately 0.25"
      ],
      "queries": [
        {
          "query": "Why does the bar at 32-37 exceed 0.20 when the localization approach targets the 43-58 range in older groups?",
          "answer": "The figure shows the younger (unprivileged) group where the 17-27 range is localized. The 43-58 range applies to the separate older (privileged) group analysis, not shown in this panel.",
          "visual_anchor": "tallest blue bar at age 32-37 reaching approximately 0.25",
          "text_evidence": "The 17–27 range in the younger group and the 43–58 range in the older group would be localized by the approach.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the horizontal line at 0.14 represent the threshold separating privileged from unprivileged age groups in German Credit?",
          "answer": "No, the horizontal line likely represents a reference threshold for fairness metrics. The caption indicates this panel shows the younger (unprivileged) group specifically, while privileged refers to the separate older group.",
          "visual_anchor": "horizontal line intersecting at approximately 0.14 on the y-axis",
          "text_evidence": "Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the increasing trend from 0.05 at 22-27 to 0.25 at 32-37 relate to dataset fairness checks?",
          "answer": "The increasing ratio of favorable outcomes across age ranges demonstrates bias patterns that the DatasetMetric class computes to check for bias in datasets and models, as part of the fairness analysis framework.",
          "visual_anchor": "blue bars increasing from approximately 0.05 at age 22-27 to 0.25 at age 32-37",
          "text_evidence": "The Metric class and its subclasses compute various individual and group fairness metrics to check for bias in datasets and models.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the bar at 32-37 exceed 0.20 when the localization approach targets the 43-58 range in older groups?",
        "answer": "The figure shows the younger (unprivileged) group where the 17-27 range is localized. The 43-58 range applies to the separate older (privileged) group analysis, not shown in this panel.",
        "visual_anchor": "tallest blue bar at age 32-37 reaching approximately 0.25",
        "text_evidence": "The 17–27 range in the younger group and the 43–58 range in the older group would be localized by the approach.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the horizontal line at 0.14 represent the threshold separating privileged from unprivileged age groups in German Credit?",
        "answer": "No, the horizontal line likely represents a reference threshold for fairness metrics. The caption indicates this panel shows the younger (unprivileged) group specifically, while privileged refers to the separate older group.",
        "visual_anchor": "horizontal line intersecting at approximately 0.14 on the y-axis",
        "text_evidence": "Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the increasing trend from 0.05 at 22-27 to 0.25 at 32-37 relate to dataset fairness checks?",
        "answer": "The increasing ratio of favorable outcomes across age ranges demonstrates bias patterns that the DatasetMetric class computes to check for bias in datasets and models, as part of the fairness analysis framework.",
        "visual_anchor": "blue bars increasing from approximately 0.05 at age 22-27 to 0.25 at age 32-37",
        "text_evidence": "The Metric class and its subclasses compute various individual and group fairness metrics to check for bias in datasets and models.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "X-axis showing age ranges: 17-27, 22-27, 27-32, 32-37",
      "Y-axis showing ratio of favorable to total ranging from 0 to 0.25",
      "Four blue bars of increasing height",
      "Horizontal line at approximately 0.14",
      "Tallest bar at age 32-37 reaching approximately 0.25"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg",
    "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three bar groups per county showing white_search_rate (blue), black_search_rate (orange), hispanic_search_rate (green)\",\n    \"Hartford County with notably high green bar (~0.043) and orange bar (~0.042)\",\n    \"Middlesex County with very low blue bar (~0.006) and orange bar (~0.021)\",\n    \"Y-axis labeled 'Search Rate' ranging from 0.000 to 0.040\",\n    \"X-axis showing eight counties: Fairfield, Hartford, Litchfield, Middlesex, New Haven, New London, Tolland, Windham\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Hartford County's green bar at 0.043 make its DI ratio higher than Middlesex County's?\",\n      \"answer\": \"Hartford's Hispanic search rate (0.043) is much closer to its white rate (0.013), creating a smaller disparity ratio. Middlesex's Hispanic rate (0.012) compared to its white rate (0.006) shows different proportions, affecting the DI fairness metric differently.\",\n      \"visual_anchor\": \"Hartford County green bar at 0.043 versus Middlesex County green bar at 0.012\",\n      \"text_evidence\": \"In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does Hartford's orange bar at 0.042 compared to its blue bar support localization as a biased feature?\",\n      \"answer\": \"Yes, Hartford's black search rate (0.042) is significantly higher than its white rate (0.013), demonstrating substantial disparity. This disparity pattern makes county_name a feature where bias can be localized, as Hartford exhibits different fairness characteristics.\",\n      \"visual_anchor\": \"Hartford County orange bar at 0.042 versus blue bar at 0.013\",\n      \"text_evidence\": \"Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. The approach would localize Hartford County.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do Middlesex's three bars at 0.006, 0.021, and 0.012 relate to unprivileged group search rates?\",\n      \"answer\": \"Middlesex shows black (0.021) and Hispanic (0.012) search rates that are both higher than white rates (0.006), representing the unprivileged groups. These rates demonstrate the disparity between privileged and unprivileged groups that the DI fairness metric captures.\",\n      \"visual_anchor\": \"Middlesex County three bars showing blue at 0.006, orange at 0.021, green at 0.012\",\n      \"text_evidence\": \"the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric)\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three bar groups per county showing white_search_rate (blue), black_search_rate (orange), hispanic_search_rate (green)",
        "Hartford County with notably high green bar (~0.043) and orange bar (~0.042)",
        "Middlesex County with very low blue bar (~0.006) and orange bar (~0.021)",
        "Y-axis labeled 'Search Rate' ranging from 0.000 to 0.040",
        "X-axis showing eight counties: Fairfield, Hartford, Litchfield, Middlesex, New Haven, New London, Tolland, Windham"
      ],
      "queries": [
        {
          "query": "Why does Hartford County's green bar at 0.043 make its DI ratio higher than Middlesex County's?",
          "answer": "Hartford's Hispanic search rate (0.043) is much closer to its white rate (0.013), creating a smaller disparity ratio. Middlesex's Hispanic rate (0.012) compared to its white rate (0.006) shows different proportions, affecting the DI fairness metric differently.",
          "visual_anchor": "Hartford County green bar at 0.043 versus Middlesex County green bar at 0.012",
          "text_evidence": "In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does Hartford's orange bar at 0.042 compared to its blue bar support localization as a biased feature?",
          "answer": "Yes, Hartford's black search rate (0.042) is significantly higher than its white rate (0.013), demonstrating substantial disparity. This disparity pattern makes county_name a feature where bias can be localized, as Hartford exhibits different fairness characteristics.",
          "visual_anchor": "Hartford County orange bar at 0.042 versus blue bar at 0.013",
          "text_evidence": "Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. The approach would localize Hartford County.",
          "query_type": "value_context"
        },
        {
          "query": "How do Middlesex's three bars at 0.006, 0.021, and 0.012 relate to unprivileged group search rates?",
          "answer": "Middlesex shows black (0.021) and Hispanic (0.012) search rates that are both higher than white rates (0.006), representing the unprivileged groups. These rates demonstrate the disparity between privileged and unprivileged groups that the DI fairness metric captures.",
          "visual_anchor": "Middlesex County three bars showing blue at 0.006, orange at 0.021, green at 0.012",
          "text_evidence": "the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric)",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Hartford County's green bar at 0.043 make its DI ratio higher than Middlesex County's?",
        "answer": "Hartford's Hispanic search rate (0.043) is much closer to its white rate (0.013), creating a smaller disparity ratio. Middlesex's Hispanic rate (0.012) compared to its white rate (0.006) shows different proportions, affecting the DI fairness metric differently.",
        "visual_anchor": "Hartford County green bar at 0.043 versus Middlesex County green bar at 0.012",
        "text_evidence": "In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does Hartford's orange bar at 0.042 compared to its blue bar support localization as a biased feature?",
        "answer": "Yes, Hartford's black search rate (0.042) is significantly higher than its white rate (0.013), demonstrating substantial disparity. This disparity pattern makes county_name a feature where bias can be localized, as Hartford exhibits different fairness characteristics.",
        "visual_anchor": "Hartford County orange bar at 0.042 versus blue bar at 0.013",
        "text_evidence": "Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. The approach would localize Hartford County.",
        "query_type": "value_context"
      },
      {
        "query": "How do Middlesex's three bars at 0.006, 0.021, and 0.012 relate to unprivileged group search rates?",
        "answer": "Middlesex shows black (0.021) and Hispanic (0.012) search rates that are both higher than white rates (0.006), representing the unprivileged groups. These rates demonstrate the disparity between privileged and unprivileged groups that the DI fairness metric captures.",
        "visual_anchor": "Middlesex County three bars showing blue at 0.006, orange at 0.021, green at 0.012",
        "text_evidence": "the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric)",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three bar groups per county showing white_search_rate (blue), black_search_rate (orange), hispanic_search_rate (green)",
      "Hartford County with notably high green bar (~0.043) and orange bar (~0.042)",
      "Middlesex County with very low blue bar (~0.006) and orange bar (~0.021)",
      "Y-axis labeled 'Search Rate' ranging from 0.000 to 0.040",
      "X-axis showing eight counties: Fairfield, Hartford, Litchfield, Middlesex, New Haven, New London, Tolland, Windham"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_7",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg",
    "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue bars showing metrics before pre-processing\",\n    \"Orange bars showing metrics after pre-processing\",\n    \"Dark gray bars indicating ±1 standard deviation\",\n    \"Three datasets: compas, german, adult\",\n    \"Two metrics: SPD (Statistical Parity Difference) and DI (Disparate Impact)\",\n    \"Horizontal bar chart with values from 0 to 1\",\n    \"Protected attributes: race, sex, age\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the orange bars showing post-transformation values support that the original dataset itself gets transformed?\",\n      \"answer\": \"Yes, the orange bars consistently show values closer to the ideal fair values (SPD=0, DI=1) compared to blue bars, demonstrating that pre-processing algorithms transform the original dataset to improve fairness metrics.\",\n      \"visual_anchor\": \"Orange bars positioned after blue bars for each dataset-attribute combination\",\n      \"text_evidence\": \"For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the dark gray bars extending from orange bars remain small despite the ideal SPD value being 0?\",\n      \"answer\": \"The small dark gray bars indicate low variance (±1 standard deviation) in the fairness metrics after pre-processing, suggesting consistent algorithm performance across runs even when achieving near-ideal SPD values close to 0.\",\n      \"visual_anchor\": \"Dark gray bars showing standard deviation extent on orange bars\",\n      \"text_evidence\": \"The dark gray bars indicate the extent of $\\\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the blue bar values before transformation relate to computing fairness metrics for pre-processing algorithms?\",\n      \"answer\": \"The blue bars represent baseline fairness metrics computed on the original untransformed data, providing the comparison point to evaluate improvement after applying pre-processing algorithms shown by the orange bars.\",\n      \"visual_anchor\": \"Blue bars showing initial metric values ranging from approximately 0.2 to 0.8\",\n      \"text_evidence\": \"we compute fairness metrics before and after this transformation and present results in Figure 4\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue bars showing metrics before pre-processing",
        "Orange bars showing metrics after pre-processing",
        "Dark gray bars indicating ±1 standard deviation",
        "Three datasets: compas, german, adult",
        "Two metrics: SPD (Statistical Parity Difference) and DI (Disparate Impact)",
        "Horizontal bar chart with values from 0 to 1",
        "Protected attributes: race, sex, age"
      ],
      "queries": [
        {
          "query": "Do the orange bars showing post-transformation values support that the original dataset itself gets transformed?",
          "answer": "Yes, the orange bars consistently show values closer to the ideal fair values (SPD=0, DI=1) compared to blue bars, demonstrating that pre-processing algorithms transform the original dataset to improve fairness metrics.",
          "visual_anchor": "Orange bars positioned after blue bars for each dataset-attribute combination",
          "text_evidence": "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation",
          "query_type": "value_context"
        },
        {
          "query": "Why do the dark gray bars extending from orange bars remain small despite the ideal SPD value being 0?",
          "answer": "The small dark gray bars indicate low variance (±1 standard deviation) in the fairness metrics after pre-processing, suggesting consistent algorithm performance across runs even when achieving near-ideal SPD values close to 0.",
          "visual_anchor": "Dark gray bars showing standard deviation extent on orange bars",
          "text_evidence": "The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the blue bar values before transformation relate to computing fairness metrics for pre-processing algorithms?",
          "answer": "The blue bars represent baseline fairness metrics computed on the original untransformed data, providing the comparison point to evaluate improvement after applying pre-processing algorithms shown by the orange bars.",
          "visual_anchor": "Blue bars showing initial metric values ranging from approximately 0.2 to 0.8",
          "text_evidence": "we compute fairness metrics before and after this transformation and present results in Figure 4",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the orange bars showing post-transformation values support that the original dataset itself gets transformed?",
        "answer": "Yes, the orange bars consistently show values closer to the ideal fair values (SPD=0, DI=1) compared to blue bars, demonstrating that pre-processing algorithms transform the original dataset to improve fairness metrics.",
        "visual_anchor": "Orange bars positioned after blue bars for each dataset-attribute combination",
        "text_evidence": "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation",
        "query_type": "value_context"
      },
      {
        "query": "Why do the dark gray bars extending from orange bars remain small despite the ideal SPD value being 0?",
        "answer": "The small dark gray bars indicate low variance (±1 standard deviation) in the fairness metrics after pre-processing, suggesting consistent algorithm performance across runs even when achieving near-ideal SPD values close to 0.",
        "visual_anchor": "Dark gray bars showing standard deviation extent on orange bars",
        "text_evidence": "The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the blue bar values before transformation relate to computing fairness metrics for pre-processing algorithms?",
        "answer": "The blue bars represent baseline fairness metrics computed on the original untransformed data, providing the comparison point to evaluate improvement after applying pre-processing algorithms shown by the orange bars.",
        "visual_anchor": "Blue bars showing initial metric values ranging from approximately 0.2 to 0.8",
        "text_evidence": "we compute fairness metrics before and after this transformation and present results in Figure 4",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue bars showing metrics before pre-processing",
      "Orange bars showing metrics after pre-processing",
      "Dark gray bars indicating ±1 standard deviation",
      "Three datasets: compas, german, adult",
      "Two metrics: SPD (Statistical Parity Difference) and DI (Disparate Impact)",
      "Horizontal bar chart with values from 0 to 1",
      "Protected attributes: race, sex, age"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_10",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
    "caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top panel with disparate impact on x-axis (0.4-1.2) and metric values on y-axis (0.6-0.8)\",\n    \"Bottom panel with disparate impact on x-axis (0.4-1.2) and metric values on y-axis (0.6-0.8)\",\n    \"Circles representing mean values\",\n    \"Horizontal bars representing ±1 standard deviation\",\n    \"Color-coded legend: orange (LR,Optimized pre-processing), light orange (RF,Optimized pre-processing), dark red (LR,Equal odds postprocessing), pink (RF,Equal odds postprocessing)\",\n    \"Green circles at disparate impact ~0.8 in top panel\",\n    \"Clustered points around disparate impact 0.7-0.9 in bottom panel\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the pink circles cluster near disparate impact 0.7 in the bottom panel when race is the protected attribute?\",\n      \"answer\": \"The bottom panel shows results after applying bias mitigation algorithms to the Adult dataset with race as the protected attribute. The pink circles represent RF with Equal odds postprocessing, which achieves disparate impact values around 0.7 after mitigation.\",\n      \"visual_anchor\": \"pink circles at disparate impact ~0.7 in bottom panel\",\n      \"text_evidence\": \"Dataset: Adult, Protected attribute: race.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?\",\n      \"answer\": \"Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.\",\n      \"visual_anchor\": \"green circles at disparate impact ~0.8 in top panel\",\n      \"text_evidence\": \"The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does standard deviation shown by horizontal bars compare between logistic regression and random forest classifiers?\",\n      \"answer\": \"The horizontal bars show that standard deviation varies across methods, with some configurations showing wider bars (greater uncertainty) than others. Both LR and RF classifiers were used to evaluate the extent of variation in fairness metrics.\",\n      \"visual_anchor\": \"horizontal bars representing ±1 standard deviation across different colored circles\",\n      \"text_evidence\": \"In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top panel with disparate impact on x-axis (0.4-1.2) and metric values on y-axis (0.6-0.8)",
        "Bottom panel with disparate impact on x-axis (0.4-1.2) and metric values on y-axis (0.6-0.8)",
        "Circles representing mean values",
        "Horizontal bars representing ±1 standard deviation",
        "Color-coded legend: orange (LR,Optimized pre-processing), light orange (RF,Optimized pre-processing), dark red (LR,Equal odds postprocessing), pink (RF,Equal odds postprocessing)",
        "Green circles at disparate impact ~0.8 in top panel",
        "Clustered points around disparate impact 0.7-0.9 in bottom panel"
      ],
      "queries": [
        {
          "query": "Why do the pink circles cluster near disparate impact 0.7 in the bottom panel when race is the protected attribute?",
          "answer": "The bottom panel shows results after applying bias mitigation algorithms to the Adult dataset with race as the protected attribute. The pink circles represent RF with Equal odds postprocessing, which achieves disparate impact values around 0.7 after mitigation.",
          "visual_anchor": "pink circles at disparate impact ~0.7 in bottom panel",
          "text_evidence": "Dataset: Adult, Protected attribute: race.",
          "query_type": "value_context"
        },
        {
          "query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
          "answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
          "visual_anchor": "green circles at disparate impact ~0.8 in top panel",
          "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does standard deviation shown by horizontal bars compare between logistic regression and random forest classifiers?",
          "answer": "The horizontal bars show that standard deviation varies across methods, with some configurations showing wider bars (greater uncertainty) than others. Both LR and RF classifiers were used to evaluate the extent of variation in fairness metrics.",
          "visual_anchor": "horizontal bars representing ±1 standard deviation across different colored circles",
          "text_evidence": "In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the pink circles cluster near disparate impact 0.7 in the bottom panel when race is the protected attribute?",
        "answer": "The bottom panel shows results after applying bias mitigation algorithms to the Adult dataset with race as the protected attribute. The pink circles represent RF with Equal odds postprocessing, which achieves disparate impact values around 0.7 after mitigation.",
        "visual_anchor": "pink circles at disparate impact ~0.7 in bottom panel",
        "text_evidence": "Dataset: Adult, Protected attribute: race.",
        "query_type": "value_context"
      },
      {
        "query": "Do the green circles at disparate impact 0.8 in the top panel support that optimized pre-processing improves fairness?",
        "answer": "Yes, the green circles (RF,Optimized pre-processing) show disparate impact closest to the ideal value of 1 in the top panel before mitigation, indicating that optimized pre-processing achieves better fairness metrics.",
        "visual_anchor": "green circles at disparate impact ~0.8 in top panel",
        "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does standard deviation shown by horizontal bars compare between logistic regression and random forest classifiers?",
        "answer": "The horizontal bars show that standard deviation varies across methods, with some configurations showing wider bars (greater uncertainty) than others. Both LR and RF classifiers were used to evaluate the extent of variation in fairness metrics.",
        "visual_anchor": "horizontal bars representing ±1 standard deviation across different colored circles",
        "text_evidence": "In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top panel with disparate impact on x-axis (0.4-1.2) and metric values on y-axis (0.6-0.8)",
      "Bottom panel with disparate impact on x-axis (0.4-1.2) and metric values on y-axis (0.6-0.8)",
      "Circles representing mean values",
      "Horizontal bars representing ±1 standard deviation",
      "Color-coded legend: orange (LR,Optimized pre-processing), light orange (RF,Optimized pre-processing), dark red (LR,Equal odds postprocessing), pink (RF,Equal odds postprocessing)",
      "Green circles at disparate impact ~0.8 in top panel",
      "Clustered points around disparate impact 0.7-0.9 in bottom panel"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_14",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg",
    "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.",
    "original_quality_score": 0.7000000000000001,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Gray bar labeled 'original' with value 0.59\",\n    \"Teal bar labeled 'mitigated' with value 0.85\",\n    \"Y-axis scale from 0 to 1.5 with gridlines at 0.5 and 1.5\",\n    \"Label 'Fair' on right side at y=1.0\",\n    \"Title 'Disparate Impact'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the gray bar value of 0.59 improve to 0.85 after the adversarial debiasing mitigation described?\",\n      \"answer\": \"Yes, the gray 'original' bar shows 0.59, while the teal 'mitigated' bar shows 0.85, indicating improvement after adversarial debiasing mitigation.\",\n      \"visual_anchor\": \"Gray bar at 0.59 and teal bar at 0.85\",\n      \"text_evidence\": \"After adversarial debiasing mitigation   \\nFigure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How close does the teal mitigated bar at 0.85 come to the Fair threshold shown on the graph?\",\n      \"answer\": \"The teal mitigated bar at 0.85 is 0.15 units below the Fair threshold marked at 1.0 on the graph.\",\n      \"visual_anchor\": \"Teal bar at 0.85 relative to 'Fair' label at y=1.0\",\n      \"text_evidence\": \"Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the graph display both 0.59 and 0.85 values for the same Disparate Impact metric?\",\n      \"answer\": \"The graph shows both values to compare performance before (0.59, gray) and after (0.85, teal) mitigation, demonstrating the effectiveness of the toolkit's mitigation approach.\",\n      \"visual_anchor\": \"Two bars showing 0.59 (gray) and 0.85 (teal) for same metric\",\n      \"text_evidence\": \"convey toolkit richness while still being approachable. In the final design, a short textual introduction to the content of the site, along with direct links to the API documentation and code repository\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Gray bar labeled 'original' with value 0.59",
        "Teal bar labeled 'mitigated' with value 0.85",
        "Y-axis scale from 0 to 1.5 with gridlines at 0.5 and 1.5",
        "Label 'Fair' on right side at y=1.0",
        "Title 'Disparate Impact'"
      ],
      "queries": [
        {
          "query": "Does the gray bar value of 0.59 improve to 0.85 after the adversarial debiasing mitigation described?",
          "answer": "Yes, the gray 'original' bar shows 0.59, while the teal 'mitigated' bar shows 0.85, indicating improvement after adversarial debiasing mitigation.",
          "visual_anchor": "Gray bar at 0.59 and teal bar at 0.85",
          "text_evidence": "After adversarial debiasing mitigation   \nFigure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How close does the teal mitigated bar at 0.85 come to the Fair threshold shown on the graph?",
          "answer": "The teal mitigated bar at 0.85 is 0.15 units below the Fair threshold marked at 1.0 on the graph.",
          "visual_anchor": "Teal bar at 0.85 relative to 'Fair' label at y=1.0",
          "text_evidence": "Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation",
          "query_type": "value_context"
        },
        {
          "query": "Why does the graph display both 0.59 and 0.85 values for the same Disparate Impact metric?",
          "answer": "The graph shows both values to compare performance before (0.59, gray) and after (0.85, teal) mitigation, demonstrating the effectiveness of the toolkit's mitigation approach.",
          "visual_anchor": "Two bars showing 0.59 (gray) and 0.85 (teal) for same metric",
          "text_evidence": "convey toolkit richness while still being approachable. In the final design, a short textual introduction to the content of the site, along with direct links to the API documentation and code repository",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the gray bar value of 0.59 improve to 0.85 after the adversarial debiasing mitigation described?",
        "answer": "Yes, the gray 'original' bar shows 0.59, while the teal 'mitigated' bar shows 0.85, indicating improvement after adversarial debiasing mitigation.",
        "visual_anchor": "Gray bar at 0.59 and teal bar at 0.85",
        "text_evidence": "After adversarial debiasing mitigation   \nFigure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How close does the teal mitigated bar at 0.85 come to the Fair threshold shown on the graph?",
        "answer": "The teal mitigated bar at 0.85 is 0.15 units below the Fair threshold marked at 1.0 on the graph.",
        "visual_anchor": "Teal bar at 0.85 relative to 'Fair' label at y=1.0",
        "text_evidence": "Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation",
        "query_type": "value_context"
      },
      {
        "query": "Why does the graph display both 0.59 and 0.85 values for the same Disparate Impact metric?",
        "answer": "The graph shows both values to compare performance before (0.59, gray) and after (0.85, teal) mitigation, demonstrating the effectiveness of the toolkit's mitigation approach.",
        "visual_anchor": "Two bars showing 0.59 (gray) and 0.85 (teal) for same metric",
        "text_evidence": "convey toolkit richness while still being approachable. In the final design, a short textual introduction to the content of the site, along with direct links to the API documentation and code repository",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Gray bar labeled 'original' with value 0.59",
      "Teal bar labeled 'mitigated' with value 0.85",
      "Y-axis scale from 0 to 1.5 with gridlines at 0.5 and 1.5",
      "Label 'Fair' on right side at y=1.0",
      "Title 'Disparate Impact'"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_15",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg",
    "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Dataset class at top with inheritance arrow to StructuredDataset and StandardDataset\",\n    \"BinaryLabelDataset with two aggregation arrows labeled '1' and '2' from StructuredDataset and StandardDataset\",\n    \"Transformer class with methods fit(), predict(), transform(), fit_transform(), addmetadata()\",\n    \"GenericPreProcessing, GenericInProcessing, GenericPostProcessing classes as placeholders\",\n    \"Metric class with memoize() method and inheritance arrow to DatasetMetric\",\n    \"BinaryLabelDatasetMetric with methods including disparate_impact() and statistical_parity_difference()\",\n    \"ClassificationMetric with methods including binary_confusion_matrix() and generalized_entropy_index()\",\n    \"Explainer class with MetricTextExplainer and MetricJSONExplainer subclasses\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the BinaryLabelDataset's dual aggregation arrows from StructuredDataset and StandardDataset support the toolbox's transformation workflow?\",\n      \"answer\": \"The dual aggregation enables BinaryLabelDataset to accept inputs from both structured and standard formats, allowing the Transformer class to operate on either type. This supports the common task of loading a dataset, understanding outcome disparity, and transforming it to mitigate disparity.\",\n      \"visual_anchor\": \"Two aggregation arrows labeled '1' and '2' pointing from StructuredDataset and StandardDataset to BinaryLabelDataset\",\n      \"text_evidence\": \"The example involves the user loading a dataset, splitting it into training and testing partitions, understanding the outcome disparity between two demographic groups, and transforming the dataset to mitigate this disparity.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why are GenericPreProcessing, GenericInProcessing, and GenericPostProcessing shown as separate classes when real bias mitigation algorithms exist?\",\n      \"answer\": \"These are placeholder classes representing the actual bias mitigation algorithms implemented in AIF360. They serve to illustrate the three-stage processing architecture without enumerating all specific algorithm implementations.\",\n      \"visual_anchor\": \"Three boxes labeled GenericPreProcessing, GenericInProcessing, and GenericPostProcessing with params... notation\",\n      \"text_evidence\": \"The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the memoize decorator applied to Metric class methods explain why metric computation efficiency matters for metametrics?\",\n      \"answer\": \"Yes, the memoize decorator automatically caches metric results, which is crucial for metametrics like difference() and ratio() that repeatedly call other metrics. This avoids redundant computations when calculating aggregate fairness measures.\",\n      \"visual_anchor\": \"Metric class box showing -memoize() method, connected to DatasetMetric with methods like difference() and ratio()\",\n      \"text_evidence\": \"memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Dataset class at top with inheritance arrow to StructuredDataset and StandardDataset",
        "BinaryLabelDataset with two aggregation arrows labeled '1' and '2' from StructuredDataset and StandardDataset",
        "Transformer class with methods fit(), predict(), transform(), fit_transform(), addmetadata()",
        "GenericPreProcessing, GenericInProcessing, GenericPostProcessing classes as placeholders",
        "Metric class with memoize() method and inheritance arrow to DatasetMetric",
        "BinaryLabelDatasetMetric with methods including disparate_impact() and statistical_parity_difference()",
        "ClassificationMetric with methods including binary_confusion_matrix() and generalized_entropy_index()",
        "Explainer class with MetricTextExplainer and MetricJSONExplainer subclasses"
      ],
      "queries": [
        {
          "query": "How does the BinaryLabelDataset's dual aggregation arrows from StructuredDataset and StandardDataset support the toolbox's transformation workflow?",
          "answer": "The dual aggregation enables BinaryLabelDataset to accept inputs from both structured and standard formats, allowing the Transformer class to operate on either type. This supports the common task of loading a dataset, understanding outcome disparity, and transforming it to mitigate disparity.",
          "visual_anchor": "Two aggregation arrows labeled '1' and '2' pointing from StructuredDataset and StandardDataset to BinaryLabelDataset",
          "text_evidence": "The example involves the user loading a dataset, splitting it into training and testing partitions, understanding the outcome disparity between two demographic groups, and transforming the dataset to mitigate this disparity.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why are GenericPreProcessing, GenericInProcessing, and GenericPostProcessing shown as separate classes when real bias mitigation algorithms exist?",
          "answer": "These are placeholder classes representing the actual bias mitigation algorithms implemented in AIF360. They serve to illustrate the three-stage processing architecture without enumerating all specific algorithm implementations.",
          "visual_anchor": "Three boxes labeled GenericPreProcessing, GenericInProcessing, and GenericPostProcessing with params... notation",
          "text_evidence": "The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented.",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the memoize decorator applied to Metric class methods explain why metric computation efficiency matters for metametrics?",
          "answer": "Yes, the memoize decorator automatically caches metric results, which is crucial for metametrics like difference() and ratio() that repeatedly call other metrics. This avoids redundant computations when calculating aggregate fairness measures.",
          "visual_anchor": "Metric class box showing -memoize() method, connected to DatasetMetric with methods like difference() and ratio()",
          "text_evidence": "memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the BinaryLabelDataset's dual aggregation arrows from StructuredDataset and StandardDataset support the toolbox's transformation workflow?",
        "answer": "The dual aggregation enables BinaryLabelDataset to accept inputs from both structured and standard formats, allowing the Transformer class to operate on either type. This supports the common task of loading a dataset, understanding outcome disparity, and transforming it to mitigate disparity.",
        "visual_anchor": "Two aggregation arrows labeled '1' and '2' pointing from StructuredDataset and StandardDataset to BinaryLabelDataset",
        "text_evidence": "The example involves the user loading a dataset, splitting it into training and testing partitions, understanding the outcome disparity between two demographic groups, and transforming the dataset to mitigate this disparity.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why are GenericPreProcessing, GenericInProcessing, and GenericPostProcessing shown as separate classes when real bias mitigation algorithms exist?",
        "answer": "These are placeholder classes representing the actual bias mitigation algorithms implemented in AIF360. They serve to illustrate the three-stage processing architecture without enumerating all specific algorithm implementations.",
        "visual_anchor": "Three boxes labeled GenericPreProcessing, GenericInProcessing, and GenericPostProcessing with params... notation",
        "text_evidence": "The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented.",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the memoize decorator applied to Metric class methods explain why metric computation efficiency matters for metametrics?",
        "answer": "Yes, the memoize decorator automatically caches metric results, which is crucial for metametrics like difference() and ratio() that repeatedly call other metrics. This avoids redundant computations when calculating aggregate fairness measures.",
        "visual_anchor": "Metric class box showing -memoize() method, connected to DatasetMetric with methods like difference() and ratio()",
        "text_evidence": "memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Dataset class at top with inheritance arrow to StructuredDataset and StandardDataset",
      "BinaryLabelDataset with two aggregation arrows labeled '1' and '2' from StructuredDataset and StandardDataset",
      "Transformer class with methods fit(), predict(), transform(), fit_transform(), addmetadata()",
      "GenericPreProcessing, GenericInProcessing, GenericPostProcessing classes as placeholders",
      "Metric class with memoize() method and inheritance arrow to DatasetMetric",
      "BinaryLabelDatasetMetric with methods including disparate_impact() and statistical_parity_difference()",
      "ClassificationMetric with methods including binary_confusion_matrix() and generalized_entropy_index()",
      "Explainer class with MetricTextExplainer and MetricJSONExplainer subclasses"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_16",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg",
    "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top panel showing four scatter plots with circles and error bars\",\n    \"Bottom panel showing four scatter plots with circles and error bars\",\n    \"Color-coded methods: blue (LR,Re-weighing), orange (LR,Optimized pre-processing), green (LR,Disparate impact remover), purple (LR,Calibrated equal odds postprocessing), brown (LR,Reject option classification)\",\n    \"X-axis labels: Statistical parity difference, Disparate impact, Average odds difference, Equal opportunity difference\",\n    \"Y-axis: Balanced accuracy ranging from 0.55 to 0.8\",\n    \"Horizontal and vertical error bars indicating ±1 standard deviation\",\n    \"Gray dots representing NN,Adversarial debiasing and LR,Prejudice remover\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the green circles at disparate impact 0.75 in both panels maintain balanced accuracy above 0.73 despite the ideal fair value being 1?\",\n      \"answer\": \"The green circles represent the Disparate impact remover method, which achieves high balanced accuracy while approaching the ideal fair value of 1 for disparate impact. The method balances fairness and accuracy trade-offs effectively.\",\n      \"visual_anchor\": \"Green circles positioned at disparate impact ~0.75 in both top and bottom panels, with balanced accuracy above 0.73\",\n      \"text_evidence\": \"The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the brown circles near balanced accuracy 0.6 in all bottom panel plots support higher benefit for privileged or unprivileged groups?\",\n      \"answer\": \"The brown circles represent Reject option classification, showing balanced accuracy around 0.6. The positive values in equal opportunity difference suggest higher benefit for the unprivileged group, as values greater than 0 indicate this pattern.\",\n      \"visual_anchor\": \"Brown circles positioned at balanced accuracy ~0.6 across all four bottom panel plots\",\n      \"text_evidence\": \"A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the purple circle shift from statistical parity difference -0.35 to near 0.3 between top and bottom panels?\",\n      \"answer\": \"The purple circle represents Calibrated equal odds postprocessing applied to the classifier. The shift reflects the bias mitigation algorithm's effect on statistical parity, moving the metric closer to the ideal fair value of 0 after applying the postprocessing method.\",\n      \"visual_anchor\": \"Purple circles in the leftmost plots, positioned at statistical parity difference ~-0.35 (top) and ~0.3 (bottom)\",\n      \"text_evidence\": \"Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top panel showing four scatter plots with circles and error bars",
        "Bottom panel showing four scatter plots with circles and error bars",
        "Color-coded methods: blue (LR,Re-weighing), orange (LR,Optimized pre-processing), green (LR,Disparate impact remover), purple (LR,Calibrated equal odds postprocessing), brown (LR,Reject option classification)",
        "X-axis labels: Statistical parity difference, Disparate impact, Average odds difference, Equal opportunity difference",
        "Y-axis: Balanced accuracy ranging from 0.55 to 0.8",
        "Horizontal and vertical error bars indicating ±1 standard deviation",
        "Gray dots representing NN,Adversarial debiasing and LR,Prejudice remover"
      ],
      "queries": [
        {
          "query": "Why do the green circles at disparate impact 0.75 in both panels maintain balanced accuracy above 0.73 despite the ideal fair value being 1?",
          "answer": "The green circles represent the Disparate impact remover method, which achieves high balanced accuracy while approaching the ideal fair value of 1 for disparate impact. The method balances fairness and accuracy trade-offs effectively.",
          "visual_anchor": "Green circles positioned at disparate impact ~0.75 in both top and bottom panels, with balanced accuracy above 0.73",
          "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the brown circles near balanced accuracy 0.6 in all bottom panel plots support higher benefit for privileged or unprivileged groups?",
          "answer": "The brown circles represent Reject option classification, showing balanced accuracy around 0.6. The positive values in equal opportunity difference suggest higher benefit for the unprivileged group, as values greater than 0 indicate this pattern.",
          "visual_anchor": "Brown circles positioned at balanced accuracy ~0.6 across all four bottom panel plots",
          "text_evidence": "A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the purple circle shift from statistical parity difference -0.35 to near 0.3 between top and bottom panels?",
          "answer": "The purple circle represents Calibrated equal odds postprocessing applied to the classifier. The shift reflects the bias mitigation algorithm's effect on statistical parity, moving the metric closer to the ideal fair value of 0 after applying the postprocessing method.",
          "visual_anchor": "Purple circles in the leftmost plots, positioned at statistical parity difference ~-0.35 (top) and ~0.3 (bottom)",
          "text_evidence": "Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the green circles at disparate impact 0.75 in both panels maintain balanced accuracy above 0.73 despite the ideal fair value being 1?",
        "answer": "The green circles represent the Disparate impact remover method, which achieves high balanced accuracy while approaching the ideal fair value of 1 for disparate impact. The method balances fairness and accuracy trade-offs effectively.",
        "visual_anchor": "Green circles positioned at disparate impact ~0.75 in both top and bottom panels, with balanced accuracy above 0.73",
        "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the brown circles near balanced accuracy 0.6 in all bottom panel plots support higher benefit for privileged or unprivileged groups?",
        "answer": "The brown circles represent Reject option classification, showing balanced accuracy around 0.6. The positive values in equal opportunity difference suggest higher benefit for the unprivileged group, as values greater than 0 indicate this pattern.",
        "visual_anchor": "Brown circles positioned at balanced accuracy ~0.6 across all four bottom panel plots",
        "text_evidence": "A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the purple circle shift from statistical parity difference -0.35 to near 0.3 between top and bottom panels?",
        "answer": "The purple circle represents Calibrated equal odds postprocessing applied to the classifier. The shift reflects the bias mitigation algorithm's effect on statistical parity, moving the metric closer to the ideal fair value of 0 after applying the postprocessing method.",
        "visual_anchor": "Purple circles in the leftmost plots, positioned at statistical parity difference ~-0.35 (top) and ~0.3 (bottom)",
        "text_evidence": "Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top panel showing four scatter plots with circles and error bars",
      "Bottom panel showing four scatter plots with circles and error bars",
      "Color-coded methods: blue (LR,Re-weighing), orange (LR,Optimized pre-processing), green (LR,Disparate impact remover), purple (LR,Calibrated equal odds postprocessing), brown (LR,Reject option classification)",
      "X-axis labels: Statistical parity difference, Disparate impact, Average odds difference, Equal opportunity difference",
      "Y-axis: Balanced accuracy ranging from 0.55 to 0.8",
      "Horizontal and vertical error bars indicating ±1 standard deviation",
      "Gray dots representing NN,Adversarial debiasing and LR,Prejudice remover"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_18",
    "figure_number": 10,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg",
    "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top panel showing balanced accuracy ~0.63-0.65 for most methods\",\n    \"Bottom panel showing balanced accuracy ~0.60 for most methods after mitigation\",\n    \"Green circles representing Disparate impact remover methods\",\n    \"Error bars showing ±1 standard deviation\",\n    \"Four columns showing different fairness metrics\",\n    \"Horizontal lines at ideal fair values\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green Disparate impact remover achieve balanced accuracy near 0.57 in the bottom panel for the german dataset with age as protected attribute?\",\n      \"answer\": \"After applying bias mitigation, the Disparate impact remover trades off accuracy for fairness, reducing from ~0.58 in the top panel to ~0.57 in the bottom panel. This demonstrates the fairness-accuracy tradeoff when mitigating bias on the german dataset with age as the protected attribute.\",\n      \"visual_anchor\": \"Green circles in bottom panel at balanced accuracy ~0.57\",\n      \"text_evidence\": \"Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Data set: german, Protected attribute: age.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the brown circles at balanced accuracy 0.60 in the bottom right panel support that Reject option classification maintains accuracy after mitigation?\",\n      \"answer\": \"Yes, the brown circles representing Reject option classification remain at approximately 0.60 balanced accuracy in the bottom panel, similar to their position in the top panel. This indicates the method maintains relatively stable accuracy after bias mitigation is applied.\",\n      \"visual_anchor\": \"Brown circles at balanced accuracy ~0.60 in bottom right panel\",\n      \"text_evidence\": \"The circles indicate the mean value and bars indicate the extent of ±1 standard deviation.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do most methods cluster near disparate impact 0.8 in the top panel when the ideal fair value is 1?\",\n      \"answer\": \"Most methods show disparate impact around 0.8 before mitigation, deviating from the ideal value of 1, indicating initial bias. The clustering demonstrates that without bias mitigation, these algorithms exhibit similar levels of unfairness on this fairness metric.\",\n      \"visual_anchor\": \"Cluster of circles near disparate impact 0.8 in top panel, second column\",\n      \"text_evidence\": \"The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top panel showing balanced accuracy ~0.63-0.65 for most methods",
        "Bottom panel showing balanced accuracy ~0.60 for most methods after mitigation",
        "Green circles representing Disparate impact remover methods",
        "Error bars showing ±1 standard deviation",
        "Four columns showing different fairness metrics",
        "Horizontal lines at ideal fair values"
      ],
      "queries": [
        {
          "query": "Why does the green Disparate impact remover achieve balanced accuracy near 0.57 in the bottom panel for the german dataset with age as protected attribute?",
          "answer": "After applying bias mitigation, the Disparate impact remover trades off accuracy for fairness, reducing from ~0.58 in the top panel to ~0.57 in the bottom panel. This demonstrates the fairness-accuracy tradeoff when mitigating bias on the german dataset with age as the protected attribute.",
          "visual_anchor": "Green circles in bottom panel at balanced accuracy ~0.57",
          "text_evidence": "Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Data set: german, Protected attribute: age.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the brown circles at balanced accuracy 0.60 in the bottom right panel support that Reject option classification maintains accuracy after mitigation?",
          "answer": "Yes, the brown circles representing Reject option classification remain at approximately 0.60 balanced accuracy in the bottom panel, similar to their position in the top panel. This indicates the method maintains relatively stable accuracy after bias mitigation is applied.",
          "visual_anchor": "Brown circles at balanced accuracy ~0.60 in bottom right panel",
          "text_evidence": "The circles indicate the mean value and bars indicate the extent of ±1 standard deviation.",
          "query_type": "value_context"
        },
        {
          "query": "Why do most methods cluster near disparate impact 0.8 in the top panel when the ideal fair value is 1?",
          "answer": "Most methods show disparate impact around 0.8 before mitigation, deviating from the ideal value of 1, indicating initial bias. The clustering demonstrates that without bias mitigation, these algorithms exhibit similar levels of unfairness on this fairness metric.",
          "visual_anchor": "Cluster of circles near disparate impact 0.8 in top panel, second column",
          "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green Disparate impact remover achieve balanced accuracy near 0.57 in the bottom panel for the german dataset with age as protected attribute?",
        "answer": "After applying bias mitigation, the Disparate impact remover trades off accuracy for fairness, reducing from ~0.58 in the top panel to ~0.57 in the bottom panel. This demonstrates the fairness-accuracy tradeoff when mitigating bias on the german dataset with age as the protected attribute.",
        "visual_anchor": "Green circles in bottom panel at balanced accuracy ~0.57",
        "text_evidence": "Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Data set: german, Protected attribute: age.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the brown circles at balanced accuracy 0.60 in the bottom right panel support that Reject option classification maintains accuracy after mitigation?",
        "answer": "Yes, the brown circles representing Reject option classification remain at approximately 0.60 balanced accuracy in the bottom panel, similar to their position in the top panel. This indicates the method maintains relatively stable accuracy after bias mitigation is applied.",
        "visual_anchor": "Brown circles at balanced accuracy ~0.60 in bottom right panel",
        "text_evidence": "The circles indicate the mean value and bars indicate the extent of ±1 standard deviation.",
        "query_type": "value_context"
      },
      {
        "query": "Why do most methods cluster near disparate impact 0.8 in the top panel when the ideal fair value is 1?",
        "answer": "Most methods show disparate impact around 0.8 before mitigation, deviating from the ideal value of 1, indicating initial bias. The clustering demonstrates that without bias mitigation, these algorithms exhibit similar levels of unfairness on this fairness metric.",
        "visual_anchor": "Cluster of circles near disparate impact 0.8 in top panel, second column",
        "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top panel showing balanced accuracy ~0.63-0.65 for most methods",
      "Bottom panel showing balanced accuracy ~0.60 for most methods after mitigation",
      "Green circles representing Disparate impact remover methods",
      "Error bars showing ±1 standard deviation",
      "Four columns showing different fairness metrics",
      "Horizontal lines at ideal fair values"
    ]
  },
  {
    "doc_id": "1810.01943",
    "figure_id": "1810.01943_fig_20",
    "figure_number": 12,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg",
    "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top panel showing balanced accuracy around 0.65-0.68 before mitigation\",\n    \"Bottom panel showing balanced accuracy around 0.65-0.68 after mitigation\",\n    \"Green circles for Disparate impact remover methods\",\n    \"Blue circles for LR Re-weighing and RF Re-weighing\",\n    \"Pink/red circles for Equal odds postprocessing methods\",\n    \"Purple circles for Calibrated equal odds postprocessing methods\",\n    \"Brown circles for Reject option classification methods\",\n    \"Gray circles for Adversarial debiasing and Prejudice remover\",\n    \"Orange circles for Optimized pre-processing methods\",\n    \"Four columns showing different fairness metrics: Statistical parity difference, Disparate impact, Average odds difference, Equal opportunity difference\",\n    \"Horizontal and vertical error bars indicating ±1 standard deviation\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the green Disparate impact remover circles at balanced accuracy 0.67-0.68 align with reducing bias to acceptable levels for race?\",\n      \"answer\": \"Yes, the green circles show high balanced accuracy (0.67-0.68) across metrics, consistent with the claim that bias against the unprivileged group was reduced to acceptable levels for 1 of 2 previously biased metrics when Optimized Pre-processing was applied to the race attribute.\",\n      \"visual_anchor\": \"Green circles (Disparate impact remover methods) positioned at balanced accuracy 0.67-0.68 in top panel\",\n      \"text_evidence\": \"Bias against unprivileged group was reduced to acceptable levels*for1 of 2 previously biased metrics\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do bottom panel methods cluster around balanced accuracy 0.65 despite the accuracy drop from 82% to 74% after mitigation?\",\n      \"answer\": \"The bottom panel shows post-mitigation methods maintain similar balanced accuracy (0.65) because balanced accuracy accounts for both privileged and unprivileged group performance, while the overall accuracy drop reflects the trade-off between fairness and raw prediction performance.\",\n      \"visual_anchor\": \"Bottom panel clustering of multiple colored circles around balanced accuracy 0.65\",\n      \"text_evidence\": \"Accuracy after mitigation changed from $8 2 \\\\%$ to $7 4 \\\\%$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the four fairness metrics with ideal values at disparate impact 1 and others at 0 explain the horizontal positioning differences?\",\n      \"answer\": \"Yes, the horizontal positioning reflects different fairness metric scales: disparate impact centers around 1 (ideal fairness) with values 0.6-1.2, while statistical parity, average odds, and equal opportunity center around 0 (ideal) with ranges approximately -0.4 to 0.1.\",\n      \"visual_anchor\": \"Four columns with different x-axis scales: disparate impact at 0.6-1.2, others at -0.4 to 0.1\",\n      \"text_evidence\": \"The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top panel showing balanced accuracy around 0.65-0.68 before mitigation",
        "Bottom panel showing balanced accuracy around 0.65-0.68 after mitigation",
        "Green circles for Disparate impact remover methods",
        "Blue circles for LR Re-weighing and RF Re-weighing",
        "Pink/red circles for Equal odds postprocessing methods",
        "Purple circles for Calibrated equal odds postprocessing methods",
        "Brown circles for Reject option classification methods",
        "Gray circles for Adversarial debiasing and Prejudice remover",
        "Orange circles for Optimized pre-processing methods",
        "Four columns showing different fairness metrics: Statistical parity difference, Disparate impact, Average odds difference, Equal opportunity difference",
        "Horizontal and vertical error bars indicating ±1 standard deviation"
      ],
      "queries": [
        {
          "query": "Do the green Disparate impact remover circles at balanced accuracy 0.67-0.68 align with reducing bias to acceptable levels for race?",
          "answer": "Yes, the green circles show high balanced accuracy (0.67-0.68) across metrics, consistent with the claim that bias against the unprivileged group was reduced to acceptable levels for 1 of 2 previously biased metrics when Optimized Pre-processing was applied to the race attribute.",
          "visual_anchor": "Green circles (Disparate impact remover methods) positioned at balanced accuracy 0.67-0.68 in top panel",
          "text_evidence": "Bias against unprivileged group was reduced to acceptable levels*for1 of 2 previously biased metrics",
          "query_type": "value_context"
        },
        {
          "query": "Why do bottom panel methods cluster around balanced accuracy 0.65 despite the accuracy drop from 82% to 74% after mitigation?",
          "answer": "The bottom panel shows post-mitigation methods maintain similar balanced accuracy (0.65) because balanced accuracy accounts for both privileged and unprivileged group performance, while the overall accuracy drop reflects the trade-off between fairness and raw prediction performance.",
          "visual_anchor": "Bottom panel clustering of multiple colored circles around balanced accuracy 0.65",
          "text_evidence": "Accuracy after mitigation changed from $8 2 \\%$ to $7 4 \\%$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the four fairness metrics with ideal values at disparate impact 1 and others at 0 explain the horizontal positioning differences?",
          "answer": "Yes, the horizontal positioning reflects different fairness metric scales: disparate impact centers around 1 (ideal fairness) with values 0.6-1.2, while statistical parity, average odds, and equal opportunity center around 0 (ideal) with ranges approximately -0.4 to 0.1.",
          "visual_anchor": "Four columns with different x-axis scales: disparate impact at 0.6-1.2, others at -0.4 to 0.1",
          "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the green Disparate impact remover circles at balanced accuracy 0.67-0.68 align with reducing bias to acceptable levels for race?",
        "answer": "Yes, the green circles show high balanced accuracy (0.67-0.68) across metrics, consistent with the claim that bias against the unprivileged group was reduced to acceptable levels for 1 of 2 previously biased metrics when Optimized Pre-processing was applied to the race attribute.",
        "visual_anchor": "Green circles (Disparate impact remover methods) positioned at balanced accuracy 0.67-0.68 in top panel",
        "text_evidence": "Bias against unprivileged group was reduced to acceptable levels*for1 of 2 previously biased metrics",
        "query_type": "value_context"
      },
      {
        "query": "Why do bottom panel methods cluster around balanced accuracy 0.65 despite the accuracy drop from 82% to 74% after mitigation?",
        "answer": "The bottom panel shows post-mitigation methods maintain similar balanced accuracy (0.65) because balanced accuracy accounts for both privileged and unprivileged group performance, while the overall accuracy drop reflects the trade-off between fairness and raw prediction performance.",
        "visual_anchor": "Bottom panel clustering of multiple colored circles around balanced accuracy 0.65",
        "text_evidence": "Accuracy after mitigation changed from $8 2 \\%$ to $7 4 \\%$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the four fairness metrics with ideal values at disparate impact 1 and others at 0 explain the horizontal positioning differences?",
        "answer": "Yes, the horizontal positioning reflects different fairness metric scales: disparate impact centers around 1 (ideal fairness) with values 0.6-1.2, while statistical parity, average odds, and equal opportunity center around 0 (ideal) with ranges approximately -0.4 to 0.1.",
        "visual_anchor": "Four columns with different x-axis scales: disparate impact at 0.6-1.2, others at -0.4 to 0.1",
        "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top panel showing balanced accuracy around 0.65-0.68 before mitigation",
      "Bottom panel showing balanced accuracy around 0.65-0.68 after mitigation",
      "Green circles for Disparate impact remover methods",
      "Blue circles for LR Re-weighing and RF Re-weighing",
      "Pink/red circles for Equal odds postprocessing methods",
      "Purple circles for Calibrated equal odds postprocessing methods",
      "Brown circles for Reject option classification methods",
      "Gray circles for Adversarial debiasing and Prejudice remover",
      "Orange circles for Optimized pre-processing methods",
      "Four columns showing different fairness metrics: Statistical parity difference, Disparate impact, Average odds difference, Equal opportunity difference",
      "Horizontal and vertical error bars indicating ±1 standard deviation"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg",
    "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Histogram bars showing distribution centered near 0.0% differential bias\",\n    \"Peak at approximately 0.0% differential bias reaching 10^6 documents\",\n    \"Long tails extending to -0.6% and +0.4% on x-axis\",\n    \"Logarithmic y-axis from 10^0 to 10^6\",\n    \"Text box showing N: 1412846, μ: 0.00001, σ: 0.00430\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the peak at 0.0% contain 10^6 documents when targeted perturbation sets use documents from histogram tails?\",\n      \"answer\": \"The targeted (increase, decrease) perturbation sets are constructed from documents whose removals cause the greatest differential bias, located in the tails. The peak at 0.0% represents documents with minimal bias impact, which are not selected for perturbation.\",\n      \"visual_anchor\": \"Peak at 0.0% reaching 10^6 documents and histogram tails at -0.6% and +0.4%\",\n      \"text_evidence\": \"The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the standard deviation σ: 0.00430 relate to the visible spread of bars between -0.2% and +0.2%?\",\n      \"answer\": \"The σ: 0.00430 (0.430%) indicates most documents fall within approximately ±0.4% differential bias. The histogram confirms this, with the majority of bars concentrated between -0.2% and +0.2%, representing roughly one standard deviation from the mean μ: 0.00001.\",\n      \"visual_anchor\": \"Text box showing σ: 0.00430 and histogram bars concentrated between -0.2% and +0.2%\",\n      \"text_evidence\": \"Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the near-zero mean μ: 0.00001 align with the corpus using 75-dimensional vectors for semantic meaning?\",\n      \"answer\": \"Yes, the near-zero mean indicates that most document removals have minimal impact on bias in the NYT corpus. The 75-dimensional GloVe vectors are sufficient to capture syntactic and semantic meaning, allowing the method to identify which specific documents in the tails actually influence bias.\",\n      \"visual_anchor\": \"Text box showing μ: 0.00001 and symmetric distribution centered at 0.0%\",\n      \"text_evidence\": \"This first setup consists of a corpus constructed from a Simple English Wikipedia dump (2017- 11-03) (Wikimedia, 2018) using 75-dimensional word vectors. These dimensions are small by the standards of a typical word embedding, but sufficient to start capturing syntactic and semantic meaning.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Histogram bars showing distribution centered near 0.0% differential bias",
        "Peak at approximately 0.0% differential bias reaching 10^6 documents",
        "Long tails extending to -0.6% and +0.4% on x-axis",
        "Logarithmic y-axis from 10^0 to 10^6",
        "Text box showing N: 1412846, μ: 0.00001, σ: 0.00430"
      ],
      "queries": [
        {
          "query": "Why does the peak at 0.0% contain 10^6 documents when targeted perturbation sets use documents from histogram tails?",
          "answer": "The targeted (increase, decrease) perturbation sets are constructed from documents whose removals cause the greatest differential bias, located in the tails. The peak at 0.0% represents documents with minimal bias impact, which are not selected for perturbation.",
          "visual_anchor": "Peak at 0.0% reaching 10^6 documents and histogram tails at -0.6% and +0.4%",
          "text_evidence": "The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the standard deviation σ: 0.00430 relate to the visible spread of bars between -0.2% and +0.2%?",
          "answer": "The σ: 0.00430 (0.430%) indicates most documents fall within approximately ±0.4% differential bias. The histogram confirms this, with the majority of bars concentrated between -0.2% and +0.2%, representing roughly one standard deviation from the mean μ: 0.00001.",
          "visual_anchor": "Text box showing σ: 0.00430 and histogram bars concentrated between -0.2% and +0.2%",
          "text_evidence": "Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.",
          "query_type": "value_context"
        },
        {
          "query": "Does the near-zero mean μ: 0.00001 align with the corpus using 75-dimensional vectors for semantic meaning?",
          "answer": "Yes, the near-zero mean indicates that most document removals have minimal impact on bias in the NYT corpus. The 75-dimensional GloVe vectors are sufficient to capture syntactic and semantic meaning, allowing the method to identify which specific documents in the tails actually influence bias.",
          "visual_anchor": "Text box showing μ: 0.00001 and symmetric distribution centered at 0.0%",
          "text_evidence": "This first setup consists of a corpus constructed from a Simple English Wikipedia dump (2017- 11-03) (Wikimedia, 2018) using 75-dimensional word vectors. These dimensions are small by the standards of a typical word embedding, but sufficient to start capturing syntactic and semantic meaning.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the peak at 0.0% contain 10^6 documents when targeted perturbation sets use documents from histogram tails?",
        "answer": "The targeted (increase, decrease) perturbation sets are constructed from documents whose removals cause the greatest differential bias, located in the tails. The peak at 0.0% represents documents with minimal bias impact, which are not selected for perturbation.",
        "visual_anchor": "Peak at 0.0% reaching 10^6 documents and histogram tails at -0.6% and +0.4%",
        "text_evidence": "The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the standard deviation σ: 0.00430 relate to the visible spread of bars between -0.2% and +0.2%?",
        "answer": "The σ: 0.00430 (0.430%) indicates most documents fall within approximately ±0.4% differential bias. The histogram confirms this, with the majority of bars concentrated between -0.2% and +0.2%, representing roughly one standard deviation from the mean μ: 0.00001.",
        "visual_anchor": "Text box showing σ: 0.00430 and histogram bars concentrated between -0.2% and +0.2%",
        "text_evidence": "Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.",
        "query_type": "value_context"
      },
      {
        "query": "Does the near-zero mean μ: 0.00001 align with the corpus using 75-dimensional vectors for semantic meaning?",
        "answer": "Yes, the near-zero mean indicates that most document removals have minimal impact on bias in the NYT corpus. The 75-dimensional GloVe vectors are sufficient to capture syntactic and semantic meaning, allowing the method to identify which specific documents in the tails actually influence bias.",
        "visual_anchor": "Text box showing μ: 0.00001 and symmetric distribution centered at 0.0%",
        "text_evidence": "This first setup consists of a corpus constructed from a Simple English Wikipedia dump (2017- 11-03) (Wikimedia, 2018) using 75-dimensional word vectors. These dimensions are small by the standards of a typical word embedding, but sufficient to start capturing syntactic and semantic meaning.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Histogram bars showing distribution centered near 0.0% differential bias",
      "Peak at approximately 0.0% differential bias reaching 10^6 documents",
      "Long tails extending to -0.6% and +0.4% on x-axis",
      "Logarithmic y-axis from 10^0 to 10^6",
      "Text box showing N: 1412846, μ: 0.00001, σ: 0.00430"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg",
    "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red dashed diagonal trend line\",\n    \"Blue data points with error bars\",\n    \"Vertical dotted line at x≈1.3\",\n    \"X-axis: Validated Effect Size (-1.5 to 2.0)\",\n    \"Y-axis: Approximated Effect Size (-1.5 to 2.0)\",\n    \"Points ranging from bottom-left to top-right\",\n    \"Error bars showing standard deviation\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the vertical dotted line at x≈1.3 represent the baseline unperturbed mean for WEAT1 bias?\",\n      \"answer\": \"The vertical dotted line marks the baseline (unperturbed) mean effect size before any perturbation sets were removed from the NYT corpus, serving as the reference point for measuring bias changes.\",\n      \"visual_anchor\": \"vertical dotted line at x≈1.3\",\n      \"text_evidence\": \"the baseline (unperturbed) mean is shown with a vertical dotted line\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?\",\n      \"answer\": \"The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.\",\n      \"visual_anchor\": \"blue data points aligned with red dashed diagonal line\",\n      \"text_evidence\": \"We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the error bars varying in length across points from bottom-left to top-right on the plot?\",\n      \"answer\": \"Each point describes the mean effect size of one perturbation set, and error bars depict one standard deviation, indicating that different perturbation sets have varying levels of consistency in their bias impact across multiple measurements.\",\n      \"visual_anchor\": \"error bars of varying lengths on blue data points\",\n      \"text_evidence\": \"Each point describes the mean effect size of one set; error bars depict one standard deviation\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red dashed diagonal trend line",
        "Blue data points with error bars",
        "Vertical dotted line at x≈1.3",
        "X-axis: Validated Effect Size (-1.5 to 2.0)",
        "Y-axis: Approximated Effect Size (-1.5 to 2.0)",
        "Points ranging from bottom-left to top-right",
        "Error bars showing standard deviation"
      ],
      "queries": [
        {
          "query": "Why does the vertical dotted line at x≈1.3 represent the baseline unperturbed mean for WEAT1 bias?",
          "answer": "The vertical dotted line marks the baseline (unperturbed) mean effect size before any perturbation sets were removed from the NYT corpus, serving as the reference point for measuring bias changes.",
          "visual_anchor": "vertical dotted line at x≈1.3",
          "text_evidence": "the baseline (unperturbed) mean is shown with a vertical dotted line",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?",
          "answer": "The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.",
          "visual_anchor": "blue data points aligned with red dashed diagonal line",
          "text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the error bars varying in length across points from bottom-left to top-right on the plot?",
          "answer": "Each point describes the mean effect size of one perturbation set, and error bars depict one standard deviation, indicating that different perturbation sets have varying levels of consistency in their bias impact across multiple measurements.",
          "visual_anchor": "error bars of varying lengths on blue data points",
          "text_evidence": "Each point describes the mean effect size of one set; error bars depict one standard deviation",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the vertical dotted line at x≈1.3 represent the baseline unperturbed mean for WEAT1 bias?",
        "answer": "The vertical dotted line marks the baseline (unperturbed) mean effect size before any perturbation sets were removed from the NYT corpus, serving as the reference point for measuring bias changes.",
        "visual_anchor": "vertical dotted line at x≈1.3",
        "text_evidence": "the baseline (unperturbed) mean is shown with a vertical dotted line",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the strong alignment of blue points along the red dashed diagonal support that approximation accuracy exceeds PPMI baseline?",
        "answer": "The close alignment of data points along the diagonal trend line demonstrates high correlation between approximated and validated effect sizes, confirming the method's accuracy. This visual validation directly relates to the comparison with the PPMI baseline discussed in the subsequent section.",
        "visual_anchor": "blue data points aligned with red dashed diagonal line",
        "text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the error bars varying in length across points from bottom-left to top-right on the plot?",
        "answer": "Each point describes the mean effect size of one perturbation set, and error bars depict one standard deviation, indicating that different perturbation sets have varying levels of consistency in their bias impact across multiple measurements.",
        "visual_anchor": "error bars of varying lengths on blue data points",
        "text_evidence": "Each point describes the mean effect size of one set; error bars depict one standard deviation",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red dashed diagonal trend line",
      "Blue data points with error bars",
      "Vertical dotted line at x≈1.3",
      "X-axis: Validated Effect Size (-1.5 to 2.0)",
      "Y-axis: Approximated Effect Size (-1.5 to 2.0)",
      "Points ranging from bottom-left to top-right",
      "Error bars showing standard deviation"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg",
    "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue diamond markers labeled 'approx. mean'\",\n    \"Red diamond markers labeled 'gnd. truth mean'\",\n    \"Vertical dotted line at x=0\",\n    \"Y-axis labels showing perturbation sets (e.g., increase-10000, decrease-1000)\",\n    \"X-axis showing WEAT effect size from -2.0 to 2.0\",\n    \"Blue dots for approximation values\",\n    \"Red dots for ground truth values\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the red and blue diamond means at decrease-1000 align so closely despite the method being an approximation?\",\n      \"answer\": \"The method accurately approximates the impact of removing bias-influencing documents. The close alignment demonstrates that the approximation method successfully identifies and predicts the effect of document removal on WEAT effect size.\",\n      \"visual_anchor\": \"Red and blue diamond markers at decrease-1000 row, both positioned near x=-0.5\",\n      \"text_evidence\": \"We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the reversal of WEAT effect size from positive to negative at decrease-1000 demonstrate the document removal impact?\",\n      \"answer\": \"Yes, the decrease-1000 perturbation shows mean values shifting from baseline-0 (near x=1.0) to negative values (near x=-0.5), confirming that removing a small percentage of articles can reverse the WEAT effect size as stated.\",\n      \"visual_anchor\": \"Blue and red diamond markers at decrease-1000 positioned at negative x-values, contrasting with baseline-0 at x≈1.0\",\n      \"text_evidence\": \"0 7 % of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Do increase perturbation sets above baseline-0 show larger WEAT effect sizes than the baseline near the vertical dotted line?\",\n      \"answer\": \"Yes, all increase perturbation sets (increase-100 through increase-10000) show mean values positioned to the right of baseline-0, indicating larger positive WEAT effect sizes ranging from approximately 1.5 to 2.0.\",\n      \"visual_anchor\": \"Blue and red diamond markers for increase-100, increase-300, increase-1000, increase-3000, and increase-10000, all positioned at x>1.5, right of baseline-0\",\n      \"text_evidence\": \"Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue diamond markers labeled 'approx. mean'",
        "Red diamond markers labeled 'gnd. truth mean'",
        "Vertical dotted line at x=0",
        "Y-axis labels showing perturbation sets (e.g., increase-10000, decrease-1000)",
        "X-axis showing WEAT effect size from -2.0 to 2.0",
        "Blue dots for approximation values",
        "Red dots for ground truth values"
      ],
      "queries": [
        {
          "query": "Why do the red and blue diamond means at decrease-1000 align so closely despite the method being an approximation?",
          "answer": "The method accurately approximates the impact of removing bias-influencing documents. The close alignment demonstrates that the approximation method successfully identifies and predicts the effect of document removal on WEAT effect size.",
          "visual_anchor": "Red and blue diamond markers at decrease-1000 row, both positioned near x=-0.5",
          "text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the reversal of WEAT effect size from positive to negative at decrease-1000 demonstrate the document removal impact?",
          "answer": "Yes, the decrease-1000 perturbation shows mean values shifting from baseline-0 (near x=1.0) to negative values (near x=-0.5), confirming that removing a small percentage of articles can reverse the WEAT effect size as stated.",
          "visual_anchor": "Blue and red diamond markers at decrease-1000 positioned at negative x-values, contrasting with baseline-0 at x≈1.0",
          "text_evidence": "0 7 % of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000",
          "query_type": "value_context"
        },
        {
          "query": "Do increase perturbation sets above baseline-0 show larger WEAT effect sizes than the baseline near the vertical dotted line?",
          "answer": "Yes, all increase perturbation sets (increase-100 through increase-10000) show mean values positioned to the right of baseline-0, indicating larger positive WEAT effect sizes ranging from approximately 1.5 to 2.0.",
          "visual_anchor": "Blue and red diamond markers for increase-100, increase-300, increase-1000, increase-3000, and increase-10000, all positioned at x>1.5, right of baseline-0",
          "text_evidence": "Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the red and blue diamond means at decrease-1000 align so closely despite the method being an approximation?",
        "answer": "The method accurately approximates the impact of removing bias-influencing documents. The close alignment demonstrates that the approximation method successfully identifies and predicts the effect of document removal on WEAT effect size.",
        "visual_anchor": "Red and blue diamond markers at decrease-1000 row, both positioned near x=-0.5",
        "text_evidence": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the reversal of WEAT effect size from positive to negative at decrease-1000 demonstrate the document removal impact?",
        "answer": "Yes, the decrease-1000 perturbation shows mean values shifting from baseline-0 (near x=1.0) to negative values (near x=-0.5), confirming that removing a small percentage of articles can reverse the WEAT effect size as stated.",
        "visual_anchor": "Blue and red diamond markers at decrease-1000 positioned at negative x-values, contrasting with baseline-0 at x≈1.0",
        "text_evidence": "0 7 % of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000",
        "query_type": "value_context"
      },
      {
        "query": "Do increase perturbation sets above baseline-0 show larger WEAT effect sizes than the baseline near the vertical dotted line?",
        "answer": "Yes, all increase perturbation sets (increase-100 through increase-10000) show mean values positioned to the right of baseline-0, indicating larger positive WEAT effect sizes ranging from approximately 1.5 to 2.0.",
        "visual_anchor": "Blue and red diamond markers for increase-100, increase-300, increase-1000, increase-3000, and increase-10000, all positioned at x>1.5, right of baseline-0",
        "text_evidence": "Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue diamond markers labeled 'approx. mean'",
      "Red diamond markers labeled 'gnd. truth mean'",
      "Vertical dotted line at x=0",
      "Y-axis labels showing perturbation sets (e.g., increase-10000, decrease-1000)",
      "X-axis showing WEAT effect size from -2.0 to 2.0",
      "Blue dots for approximation values",
      "Red dots for ground truth values"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg",
    "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue diamonds representing PPMI representation\",\n    \"Black circles with line representing GloVe embeddings\",\n    \"Red line representing word2vec embeddings\",\n    \"Y-axis labels showing perturbation sets from decrease-10000 to increase-10000\",\n    \"X-axis showing WEAT effect size from -2.0 to 2.0\",\n    \"Error bars representing one standard deviation\",\n    \"Baseline-0 reference line\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the black circles showing GloVe effects confirm the strong impact claimed for documents identified by the method?\",\n      \"answer\": \"Yes, the black circles show substantial changes in WEAT effect size across perturbation sets, ranging from approximately -1.0 to 2.0, demonstrating the strong impact of identified influential documents on GloVe embeddings.\",\n      \"visual_anchor\": \"black circles with connecting line spanning from approximately -1.0 to 2.0 WEAT effect size\",\n      \"text_evidence\": \"The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do blue diamonds for PPMI and black circles for GloVe show similar trends when testing identical perturbation sets?\",\n      \"answer\": \"Both representations respond to the same influential documents identified by the methodology, showing that these documents affect bias across different embedding types. The similar patterns validate that the identified documents have consistent impact on word association bias.\",\n      \"visual_anchor\": \"parallel trends of blue diamonds (PPMI) and black circles (GloVe) across all perturbation levels\",\n      \"text_evidence\": \"Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the red line for word2vec at baseline-0 align with comparable hyperparameters used in training these embeddings?\",\n      \"answer\": \"Yes, the red line shows word2vec results at baseline-0 near zero WEAT effect, consistent with training using comparable hyperparameters. This provides a controlled comparison point for evaluating how the same perturbation sets affect different embedding methods.\",\n      \"visual_anchor\": \"red line at baseline-0 position showing WEAT effect size near zero\",\n      \"text_evidence\": \"We start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue diamonds representing PPMI representation",
        "Black circles with line representing GloVe embeddings",
        "Red line representing word2vec embeddings",
        "Y-axis labels showing perturbation sets from decrease-10000 to increase-10000",
        "X-axis showing WEAT effect size from -2.0 to 2.0",
        "Error bars representing one standard deviation",
        "Baseline-0 reference line"
      ],
      "queries": [
        {
          "query": "Do the black circles showing GloVe effects confirm the strong impact claimed for documents identified by the method?",
          "answer": "Yes, the black circles show substantial changes in WEAT effect size across perturbation sets, ranging from approximately -1.0 to 2.0, demonstrating the strong impact of identified influential documents on GloVe embeddings.",
          "visual_anchor": "black circles with connecting line spanning from approximately -1.0 to 2.0 WEAT effect size",
          "text_evidence": "The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings.",
          "query_type": "value_context"
        },
        {
          "query": "Why do blue diamonds for PPMI and black circles for GloVe show similar trends when testing identical perturbation sets?",
          "answer": "Both representations respond to the same influential documents identified by the methodology, showing that these documents affect bias across different embedding types. The similar patterns validate that the identified documents have consistent impact on word association bias.",
          "visual_anchor": "parallel trends of blue diamonds (PPMI) and black circles (GloVe) across all perturbation levels",
          "text_evidence": "Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the red line for word2vec at baseline-0 align with comparable hyperparameters used in training these embeddings?",
          "answer": "Yes, the red line shows word2vec results at baseline-0 near zero WEAT effect, consistent with training using comparable hyperparameters. This provides a controlled comparison point for evaluating how the same perturbation sets affect different embedding methods.",
          "visual_anchor": "red line at baseline-0 position showing WEAT effect size near zero",
          "text_evidence": "We start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the black circles showing GloVe effects confirm the strong impact claimed for documents identified by the method?",
        "answer": "Yes, the black circles show substantial changes in WEAT effect size across perturbation sets, ranging from approximately -1.0 to 2.0, demonstrating the strong impact of identified influential documents on GloVe embeddings.",
        "visual_anchor": "black circles with connecting line spanning from approximately -1.0 to 2.0 WEAT effect size",
        "text_evidence": "The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings.",
        "query_type": "value_context"
      },
      {
        "query": "Why do blue diamonds for PPMI and black circles for GloVe show similar trends when testing identical perturbation sets?",
        "answer": "Both representations respond to the same influential documents identified by the methodology, showing that these documents affect bias across different embedding types. The similar patterns validate that the identified documents have consistent impact on word association bias.",
        "visual_anchor": "parallel trends of blue diamonds (PPMI) and black circles (GloVe) across all perturbation levels",
        "text_evidence": "Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the red line for word2vec at baseline-0 align with comparable hyperparameters used in training these embeddings?",
        "answer": "Yes, the red line shows word2vec results at baseline-0 near zero WEAT effect, consistent with training using comparable hyperparameters. This provides a controlled comparison point for evaluating how the same perturbation sets affect different embedding methods.",
        "visual_anchor": "red line at baseline-0 position showing WEAT effect size near zero",
        "text_evidence": "We start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue diamonds representing PPMI representation",
      "Black circles with line representing GloVe embeddings",
      "Red line representing word2vec embeddings",
      "Y-axis labels showing perturbation sets from decrease-10000 to increase-10000",
      "X-axis showing WEAT effect size from -2.0 to 2.0",
      "Error bars representing one standard deviation",
      "Baseline-0 reference line"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_5",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg",
    "caption": "Figure 5. The effect of removing the 10k most bias increasing and bias decreasing documents as identified by our method on the projection of the target words onto the gender axis vs. unperturbed corpus (base); error bars show one std dev; corpus word frequency noted in parentheses. (NYT - WEAT1)",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Gender axis ranging from male (-0.2) to female (0.4)\",\n    \"Three markers per category: base (black square), (-) inc. (green inverted triangle), (-) dec. (red triangle)\",\n    \"Two sections: ARTS (top 9 categories) and SCIENCE (bottom 7 categories)\",\n    \"Error bars showing one standard deviation\",\n    \"Vertical dotted line at x=0\",\n    \"Word frequencies in parentheses (e.g., drama 46k, technology 158k)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the red triangle for astronomy at -0.2 support that removing bias-decreasing documents increases male-associated bias?\",\n      \"answer\": \"Yes, the red triangle (bias decreasing removal) shifts astronomy leftward toward the male end compared to baseline, demonstrating that removing these documents increases male association as predicted by the methodology.\",\n      \"visual_anchor\": \"red triangle marker for astronomy positioned at approximately -0.2 on the gender axis\",\n      \"text_evidence\": \"removing the most influential documents identified by our methodology significantly impacts the WEAT, a metric that has been shown to correlate with known human biases\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do green markers for poetry and art shift rightward beyond 0.2 while their red counterparts remain near baseline?\",\n      \"answer\": \"Removing the 10k most bias-increasing documents (green markers) shifts these arts terms more toward female association, while removing bias-decreasing documents (red) keeps them near baseline, showing opposite directional effects of the two document sets.\",\n      \"visual_anchor\": \"green inverted triangle markers for poetry and art positioned beyond 0.2, compared to red triangles near 0.0\",\n      \"text_evidence\": \"In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the contrasting green marker positions for dance at 0.15 versus experiment at 0.4 despite both removing bias-increasing documents?\",\n      \"answer\": \"The methodology traces bias origins by measuring change when removing documents. Dance and experiment have different baseline biases and corpus frequencies (136k vs 22k), so removing bias-increasing documents produces different magnitudes of shift toward female association.\",\n      \"visual_anchor\": \"green inverted triangle at 0.15 for dance versus 0.4 for experiment\",\n      \"text_evidence\": \"We conceptualize the problem as measuring the resulting change in bias when we remove a training document (or small subset of the training corpus), and interpret this as the amount of bias contributed by the document to the overall embedding bias\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Gender axis ranging from male (-0.2) to female (0.4)",
        "Three markers per category: base (black square), (-) inc. (green inverted triangle), (-) dec. (red triangle)",
        "Two sections: ARTS (top 9 categories) and SCIENCE (bottom 7 categories)",
        "Error bars showing one standard deviation",
        "Vertical dotted line at x=0",
        "Word frequencies in parentheses (e.g., drama 46k, technology 158k)"
      ],
      "queries": [
        {
          "query": "Does the red triangle for astronomy at -0.2 support that removing bias-decreasing documents increases male-associated bias?",
          "answer": "Yes, the red triangle (bias decreasing removal) shifts astronomy leftward toward the male end compared to baseline, demonstrating that removing these documents increases male association as predicted by the methodology.",
          "visual_anchor": "red triangle marker for astronomy positioned at approximately -0.2 on the gender axis",
          "text_evidence": "removing the most influential documents identified by our methodology significantly impacts the WEAT, a metric that has been shown to correlate with known human biases",
          "query_type": "value_context"
        },
        {
          "query": "Why do green markers for poetry and art shift rightward beyond 0.2 while their red counterparts remain near baseline?",
          "answer": "Removing the 10k most bias-increasing documents (green markers) shifts these arts terms more toward female association, while removing bias-decreasing documents (red) keeps them near baseline, showing opposite directional effects of the two document sets.",
          "visual_anchor": "green inverted triangle markers for poetry and art positioned beyond 0.2, compared to red triangles near 0.0",
          "text_evidence": "In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the contrasting green marker positions for dance at 0.15 versus experiment at 0.4 despite both removing bias-increasing documents?",
          "answer": "The methodology traces bias origins by measuring change when removing documents. Dance and experiment have different baseline biases and corpus frequencies (136k vs 22k), so removing bias-increasing documents produces different magnitudes of shift toward female association.",
          "visual_anchor": "green inverted triangle at 0.15 for dance versus 0.4 for experiment",
          "text_evidence": "We conceptualize the problem as measuring the resulting change in bias when we remove a training document (or small subset of the training corpus), and interpret this as the amount of bias contributed by the document to the overall embedding bias",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the red triangle for astronomy at -0.2 support that removing bias-decreasing documents increases male-associated bias?",
        "answer": "Yes, the red triangle (bias decreasing removal) shifts astronomy leftward toward the male end compared to baseline, demonstrating that removing these documents increases male association as predicted by the methodology.",
        "visual_anchor": "red triangle marker for astronomy positioned at approximately -0.2 on the gender axis",
        "text_evidence": "removing the most influential documents identified by our methodology significantly impacts the WEAT, a metric that has been shown to correlate with known human biases",
        "query_type": "value_context"
      },
      {
        "query": "Why do green markers for poetry and art shift rightward beyond 0.2 while their red counterparts remain near baseline?",
        "answer": "Removing the 10k most bias-increasing documents (green markers) shifts these arts terms more toward female association, while removing bias-decreasing documents (red) keeps them near baseline, showing opposite directional effects of the two document sets.",
        "visual_anchor": "green inverted triangle markers for poetry and art positioned beyond 0.2, compared to red triangles near 0.0",
        "text_evidence": "In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the contrasting green marker positions for dance at 0.15 versus experiment at 0.4 despite both removing bias-increasing documents?",
        "answer": "The methodology traces bias origins by measuring change when removing documents. Dance and experiment have different baseline biases and corpus frequencies (136k vs 22k), so removing bias-increasing documents produces different magnitudes of shift toward female association.",
        "visual_anchor": "green inverted triangle at 0.15 for dance versus 0.4 for experiment",
        "text_evidence": "We conceptualize the problem as measuring the resulting change in bias when we remove a training document (or small subset of the training corpus), and interpret this as the amount of bias contributed by the document to the overall embedding bias",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Gender axis ranging from male (-0.2) to female (0.4)",
      "Three markers per category: base (black square), (-) inc. (green inverted triangle), (-) dec. (red triangle)",
      "Two sections: ARTS (top 9 categories) and SCIENCE (bottom 7 categories)",
      "Error bars showing one standard deviation",
      "Vertical dotted line at x=0",
      "Word frequencies in parentheses (e.g., drama 46k, technology 158k)"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_7",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg",
    "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"histogram bars\", \"N: 29344 label\", \"μ: -0.00236 label\", \"σ: 0.96661 label\", \"x-axis: Differential Bias of Removal (%)\", \"y-axis: Number of Documents (logarithmic scale)\", \"peak near 0%\", \"distribution range from -30% to 30%\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the peak near 0% and μ value of -0.00236 confirm that most documents have minimal differential bias of removal?\",\n      \"answer\": \"Yes, the histogram shows the highest concentration of documents clustered around 0% differential bias, with the mean μ of -0.00236 indicating the distribution is centered very close to zero, confirming minimal average bias impact per document.\",\n      \"visual_anchor\": \"peak near 0% on x-axis and μ: -0.00236 label in the legend box\",\n      \"text_evidence\": \"Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the σ value of 0.96661 indicate low variance despite the histogram extending from -30% to 30%?\",\n      \"answer\": \"The σ of 0.96661 represents standard deviation in the original scale, not percentage points. The histogram's spread to ±30% represents extreme outliers, while most documents cluster tightly near zero, resulting in the relatively small standard deviation.\",\n      \"visual_anchor\": \"σ: 0.96661 label and histogram bars extending to -30% and 30% on x-axis\",\n      \"text_evidence\": \"measured in percent change from the corresponding mean baseline bias\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the logarithmic y-axis scaling reveal the distribution pattern of the 29344 documents across differential bias ranges?\",\n      \"answer\": \"The logarithmic scale shows that while thousands of documents concentrate near 0% bias (peak reaching ~10^4), only single-digit or tens of documents exist at extreme values (±20-30%), revealing a highly concentrated central distribution with rare outliers.\",\n      \"visual_anchor\": \"y-axis labeled 'Number of Documents' with logarithmic scale from 10^0 to 10^4\",\n      \"text_evidence\": \"Histogram of the approximated differential bias of removal for every document in our Wiki setup\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "histogram bars",
        "N: 29344 label",
        "μ: -0.00236 label",
        "σ: 0.96661 label",
        "x-axis: Differential Bias of Removal (%)",
        "y-axis: Number of Documents (logarithmic scale)",
        "peak near 0%",
        "distribution range from -30% to 30%"
      ],
      "queries": [
        {
          "query": "Does the peak near 0% and μ value of -0.00236 confirm that most documents have minimal differential bias of removal?",
          "answer": "Yes, the histogram shows the highest concentration of documents clustered around 0% differential bias, with the mean μ of -0.00236 indicating the distribution is centered very close to zero, confirming minimal average bias impact per document.",
          "visual_anchor": "peak near 0% on x-axis and μ: -0.00236 label in the legend box",
          "text_evidence": "Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the σ value of 0.96661 indicate low variance despite the histogram extending from -30% to 30%?",
          "answer": "The σ of 0.96661 represents standard deviation in the original scale, not percentage points. The histogram's spread to ±30% represents extreme outliers, while most documents cluster tightly near zero, resulting in the relatively small standard deviation.",
          "visual_anchor": "σ: 0.96661 label and histogram bars extending to -30% and 30% on x-axis",
          "text_evidence": "measured in percent change from the corresponding mean baseline bias",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the logarithmic y-axis scaling reveal the distribution pattern of the 29344 documents across differential bias ranges?",
          "answer": "The logarithmic scale shows that while thousands of documents concentrate near 0% bias (peak reaching ~10^4), only single-digit or tens of documents exist at extreme values (±20-30%), revealing a highly concentrated central distribution with rare outliers.",
          "visual_anchor": "y-axis labeled 'Number of Documents' with logarithmic scale from 10^0 to 10^4",
          "text_evidence": "Histogram of the approximated differential bias of removal for every document in our Wiki setup",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the peak near 0% and μ value of -0.00236 confirm that most documents have minimal differential bias of removal?",
        "answer": "Yes, the histogram shows the highest concentration of documents clustered around 0% differential bias, with the mean μ of -0.00236 indicating the distribution is centered very close to zero, confirming minimal average bias impact per document.",
        "visual_anchor": "peak near 0% on x-axis and μ: -0.00236 label in the legend box",
        "text_evidence": "Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the σ value of 0.96661 indicate low variance despite the histogram extending from -30% to 30%?",
        "answer": "The σ of 0.96661 represents standard deviation in the original scale, not percentage points. The histogram's spread to ±30% represents extreme outliers, while most documents cluster tightly near zero, resulting in the relatively small standard deviation.",
        "visual_anchor": "σ: 0.96661 label and histogram bars extending to -30% and 30% on x-axis",
        "text_evidence": "measured in percent change from the corresponding mean baseline bias",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the logarithmic y-axis scaling reveal the distribution pattern of the 29344 documents across differential bias ranges?",
        "answer": "The logarithmic scale shows that while thousands of documents concentrate near 0% bias (peak reaching ~10^4), only single-digit or tens of documents exist at extreme values (±20-30%), revealing a highly concentrated central distribution with rare outliers.",
        "visual_anchor": "y-axis labeled 'Number of Documents' with logarithmic scale from 10^0 to 10^4",
        "text_evidence": "Histogram of the approximated differential bias of removal for every document in our Wiki setup",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "histogram bars",
      "N: 29344 label",
      "μ: -0.00236 label",
      "σ: 0.96661 label",
      "x-axis: Differential Bias of Removal (%)",
      "y-axis: Number of Documents (logarithmic scale)",
      "peak near 0%",
      "distribution range from -30% to 30%"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_13",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg",
    "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"blue circular points with error bars\", \"red dashed least squares line\", \"vertical dotted baseline line at x≈1.5\", \"x-axis: Validated Effect Size\", \"y-axis: Approximated Effect Size\", \"points ranging from (-0.5, -0.5) to (1.8, 1.8)\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the positive correlation shown by the red dashed line validate that approximated effect sizes predict ground truth WEAT bias changes?\",\n      \"answer\": \"Yes, the strong positive correlation indicated by the red dashed line confirms that approximated effect sizes reliably predict ground truth WEAT bias effect sizes due to perturbation set removal.\",\n      \"visual_anchor\": \"red dashed least squares line showing positive correlation\",\n      \"text_evidence\": \"Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the rightmost blue points at x≈1.5-1.8 show increased vertical spread compared to the baseline vertical dotted line position?\",\n      \"answer\": \"The increased spread at higher validated effect sizes indicates greater variability in approximation accuracy when perturbation removal has larger ground truth impact, as shown by the error bars depicting one standard deviation.\",\n      \"visual_anchor\": \"rightmost blue points with larger error bars near vertical dotted baseline line\",\n      \"text_evidence\": \"points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the leftmost blue point at approximately (-0.5, -0.5) relate to WEAT1 and WEAT2 measurements in both Wiki and NYT setups?\",\n      \"answer\": \"This point represents one of the non-random perturbation sets tested across both Wiki and NYT setups for either WEAT1 or WEAT2, showing negative bias effect sizes in both approximated and validated measurements.\",\n      \"visual_anchor\": \"leftmost blue point at coordinates approximately (-0.5, -0.5)\",\n      \"text_evidence\": \"WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right)\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "blue circular points with error bars",
        "red dashed least squares line",
        "vertical dotted baseline line at x≈1.5",
        "x-axis: Validated Effect Size",
        "y-axis: Approximated Effect Size",
        "points ranging from (-0.5, -0.5) to (1.8, 1.8)"
      ],
      "queries": [
        {
          "query": "Does the positive correlation shown by the red dashed line validate that approximated effect sizes predict ground truth WEAT bias changes?",
          "answer": "Yes, the strong positive correlation indicated by the red dashed line confirms that approximated effect sizes reliably predict ground truth WEAT bias effect sizes due to perturbation set removal.",
          "visual_anchor": "red dashed least squares line showing positive correlation",
          "text_evidence": "Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the rightmost blue points at x≈1.5-1.8 show increased vertical spread compared to the baseline vertical dotted line position?",
          "answer": "The increased spread at higher validated effect sizes indicates greater variability in approximation accuracy when perturbation removal has larger ground truth impact, as shown by the error bars depicting one standard deviation.",
          "visual_anchor": "rightmost blue points with larger error bars near vertical dotted baseline line",
          "text_evidence": "points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the leftmost blue point at approximately (-0.5, -0.5) relate to WEAT1 and WEAT2 measurements in both Wiki and NYT setups?",
          "answer": "This point represents one of the non-random perturbation sets tested across both Wiki and NYT setups for either WEAT1 or WEAT2, showing negative bias effect sizes in both approximated and validated measurements.",
          "visual_anchor": "leftmost blue point at coordinates approximately (-0.5, -0.5)",
          "text_evidence": "WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right)",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the positive correlation shown by the red dashed line validate that approximated effect sizes predict ground truth WEAT bias changes?",
        "answer": "Yes, the strong positive correlation indicated by the red dashed line confirms that approximated effect sizes reliably predict ground truth WEAT bias effect sizes due to perturbation set removal.",
        "visual_anchor": "red dashed least squares line showing positive correlation",
        "text_evidence": "Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the rightmost blue points at x≈1.5-1.8 show increased vertical spread compared to the baseline vertical dotted line position?",
        "answer": "The increased spread at higher validated effect sizes indicates greater variability in approximation accuracy when perturbation removal has larger ground truth impact, as shown by the error bars depicting one standard deviation.",
        "visual_anchor": "rightmost blue points with larger error bars near vertical dotted baseline line",
        "text_evidence": "points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the leftmost blue point at approximately (-0.5, -0.5) relate to WEAT1 and WEAT2 measurements in both Wiki and NYT setups?",
        "answer": "This point represents one of the non-random perturbation sets tested across both Wiki and NYT setups for either WEAT1 or WEAT2, showing negative bias effect sizes in both approximated and validated measurements.",
        "visual_anchor": "leftmost blue point at coordinates approximately (-0.5, -0.5)",
        "text_evidence": "WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right)",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "blue circular points with error bars",
      "red dashed least squares line",
      "vertical dotted baseline line at x≈1.5",
      "x-axis: Validated Effect Size",
      "y-axis: Approximated Effect Size",
      "points ranging from (-0.5, -0.5) to (1.8, 1.8)"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_15",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg",
    "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue dots representing approximation values\",\n    \"Red dots representing ground truth values\",\n    \"Blue diamonds showing approximation means\",\n    \"Red diamonds showing ground truth means\",\n    \"Vertical dotted line at x≈0 marking baseline mean\",\n    \"Y-axis labels from decrease-1000 to increase-1000\",\n    \"X-axis showing WEAT effect size from -2.0 to 2.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the blue and red diamonds cluster near the vertical dotted baseline for every non-random perturbation set?\",\n      \"answer\": \"No, the approximation means (blue diamonds) and ground truth means (red diamonds) span the entire range from decrease-1000 to increase-1000, showing substantial deviation from the baseline dotted line at x≈0 across different perturbation sets.\",\n      \"visual_anchor\": \"vertical dotted line at x≈0 and colored diamonds spanning all y-axis levels\",\n      \"text_evidence\": \"Approximated and ground truth differential bias of removal for every (non-random) perturbation set\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do red ground truth dots scatter more widely than blue approximation dots at the increase-1000 level?\",\n      \"answer\": \"At the increase-1000 level, the red ground truth points extend from approximately x=1.0 to x=2.0, showing greater horizontal dispersion than the more tightly clustered blue approximation points, suggesting approximation methods may underestimate variance in extreme perturbation scenarios.\",\n      \"visual_anchor\": \"red and blue dots at increase-1000 y-axis level\",\n      \"text_evidence\": \"considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the concentration of both blue and red points near x=0 at baseline-0 validate the WEAT effect size measurement?\",\n      \"answer\": \"Yes, at the baseline-0 level, both approximation (blue) and ground truth (red) points cluster tightly around x=0, aligning with the vertical dotted baseline mean and confirming minimal bias change when no perturbation is applied.\",\n      \"visual_anchor\": \"clustering of blue and red dots at baseline-0 row near x=0\",\n      \"text_evidence\": \"Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue dots representing approximation values",
        "Red dots representing ground truth values",
        "Blue diamonds showing approximation means",
        "Red diamonds showing ground truth means",
        "Vertical dotted line at x≈0 marking baseline mean",
        "Y-axis labels from decrease-1000 to increase-1000",
        "X-axis showing WEAT effect size from -2.0 to 2.0"
      ],
      "queries": [
        {
          "query": "Do the blue and red diamonds cluster near the vertical dotted baseline for every non-random perturbation set?",
          "answer": "No, the approximation means (blue diamonds) and ground truth means (red diamonds) span the entire range from decrease-1000 to increase-1000, showing substantial deviation from the baseline dotted line at x≈0 across different perturbation sets.",
          "visual_anchor": "vertical dotted line at x≈0 and colored diamonds spanning all y-axis levels",
          "text_evidence": "Approximated and ground truth differential bias of removal for every (non-random) perturbation set",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do red ground truth dots scatter more widely than blue approximation dots at the increase-1000 level?",
          "answer": "At the increase-1000 level, the red ground truth points extend from approximately x=1.0 to x=2.0, showing greater horizontal dispersion than the more tightly clustered blue approximation points, suggesting approximation methods may underestimate variance in extreme perturbation scenarios.",
          "visual_anchor": "red and blue dots at increase-1000 y-axis level",
          "text_evidence": "considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the concentration of both blue and red points near x=0 at baseline-0 validate the WEAT effect size measurement?",
          "answer": "Yes, at the baseline-0 level, both approximation (blue) and ground truth (red) points cluster tightly around x=0, aligning with the vertical dotted baseline mean and confirming minimal bias change when no perturbation is applied.",
          "visual_anchor": "clustering of blue and red dots at baseline-0 row near x=0",
          "text_evidence": "Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the blue and red diamonds cluster near the vertical dotted baseline for every non-random perturbation set?",
        "answer": "No, the approximation means (blue diamonds) and ground truth means (red diamonds) span the entire range from decrease-1000 to increase-1000, showing substantial deviation from the baseline dotted line at x≈0 across different perturbation sets.",
        "visual_anchor": "vertical dotted line at x≈0 and colored diamonds spanning all y-axis levels",
        "text_evidence": "Approximated and ground truth differential bias of removal for every (non-random) perturbation set",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do red ground truth dots scatter more widely than blue approximation dots at the increase-1000 level?",
        "answer": "At the increase-1000 level, the red ground truth points extend from approximately x=1.0 to x=2.0, showing greater horizontal dispersion than the more tightly clustered blue approximation points, suggesting approximation methods may underestimate variance in extreme perturbation scenarios.",
        "visual_anchor": "red and blue dots at increase-1000 y-axis level",
        "text_evidence": "considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the concentration of both blue and red points near x=0 at baseline-0 validate the WEAT effect size measurement?",
        "answer": "Yes, at the baseline-0 level, both approximation (blue) and ground truth (red) points cluster tightly around x=0, aligning with the vertical dotted baseline mean and confirming minimal bias change when no perturbation is applied.",
        "visual_anchor": "clustering of blue and red dots at baseline-0 row near x=0",
        "text_evidence": "Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue dots representing approximation values",
      "Red dots representing ground truth values",
      "Blue diamonds showing approximation means",
      "Red diamonds showing ground truth means",
      "Vertical dotted line at x≈0 marking baseline mean",
      "Y-axis labels from decrease-1000 to increase-1000",
      "X-axis showing WEAT effect size from -2.0 to 2.0"
    ]
  },
  {
    "doc_id": "1810.03611",
    "figure_id": "1810.03611_fig_18",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg",
    "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Scatter plot with blue dots showing positive correlation\",\n    \"X-axis labeled 'GloVe WEAT' ranging from -2.0 to 2.0\",\n    \"Y-axis labeled 'PPM WEAT' ranging from -2.0 to 2.0\",\n    \"Dense cluster of points along diagonal from bottom-left to top-right\",\n    \"r² value of 0.725 for correlation strength\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the diagonal scatter pattern from bottom-left to top-right with r²=0.725 validate WEAT measurement across NYT GloVe embeddings?\",\n      \"answer\": \"Yes, the strong positive correlation (r²=0.725) demonstrates that WEAT bias measured in NYT GloVe embeddings correlates consistently with the corpus' PPMI representation across 2000 randomly generated word sets.\",\n      \"visual_anchor\": \"diagonal scatter pattern with r²=0.725\",\n      \"text_evidence\": \"The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus' PPMI representation in 2000 randomly generated word sets, r²=0.725\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why would the scatter correlation at r²=0.725 be considered weaker than word2vec's comparable measurement?\",\n      \"answer\": \"The r²=0.725 correlation is weaker than word2vec's r²=0.803 when measured with comparable hyper-parameters, suggesting that word2vec embeddings show more consistent bias patterns with the underlying PPMI representation than GloVe embeddings.\",\n      \"visual_anchor\": \"r²=0.725 correlation coefficient\",\n      \"text_evidence\": \"versus when measured in word2vec embeddings with comparable hyper-parameters, r²=0.803\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the positive correlation in the scatter plot relate to identifying bias-influencing documents dated January 1987 to June 2007?\",\n      \"answer\": \"The positive correlation validates that WEAT can reliably measure bias in embeddings trained on the corpus, enabling the identification of the 50 most WEAT1 bias-influencing documents from the NYT dataset spanning those dates.\",\n      \"visual_anchor\": \"positive correlation scatter pattern\",\n      \"text_evidence\": \"The below documents were identified to be the 50 most WEAT1 bias influencing documents in our NYT setup. Publication dates range from January 1, 1987 to June 19, 2007.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Scatter plot with blue dots showing positive correlation",
        "X-axis labeled 'GloVe WEAT' ranging from -2.0 to 2.0",
        "Y-axis labeled 'PPM WEAT' ranging from -2.0 to 2.0",
        "Dense cluster of points along diagonal from bottom-left to top-right",
        "r² value of 0.725 for correlation strength"
      ],
      "queries": [
        {
          "query": "Does the diagonal scatter pattern from bottom-left to top-right with r²=0.725 validate WEAT measurement across NYT GloVe embeddings?",
          "answer": "Yes, the strong positive correlation (r²=0.725) demonstrates that WEAT bias measured in NYT GloVe embeddings correlates consistently with the corpus' PPMI representation across 2000 randomly generated word sets.",
          "visual_anchor": "diagonal scatter pattern with r²=0.725",
          "text_evidence": "The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus' PPMI representation in 2000 randomly generated word sets, r²=0.725",
          "query_type": "value_context"
        },
        {
          "query": "Why would the scatter correlation at r²=0.725 be considered weaker than word2vec's comparable measurement?",
          "answer": "The r²=0.725 correlation is weaker than word2vec's r²=0.803 when measured with comparable hyper-parameters, suggesting that word2vec embeddings show more consistent bias patterns with the underlying PPMI representation than GloVe embeddings.",
          "visual_anchor": "r²=0.725 correlation coefficient",
          "text_evidence": "versus when measured in word2vec embeddings with comparable hyper-parameters, r²=0.803",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the positive correlation in the scatter plot relate to identifying bias-influencing documents dated January 1987 to June 2007?",
          "answer": "The positive correlation validates that WEAT can reliably measure bias in embeddings trained on the corpus, enabling the identification of the 50 most WEAT1 bias-influencing documents from the NYT dataset spanning those dates.",
          "visual_anchor": "positive correlation scatter pattern",
          "text_evidence": "The below documents were identified to be the 50 most WEAT1 bias influencing documents in our NYT setup. Publication dates range from January 1, 1987 to June 19, 2007.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the diagonal scatter pattern from bottom-left to top-right with r²=0.725 validate WEAT measurement across NYT GloVe embeddings?",
        "answer": "Yes, the strong positive correlation (r²=0.725) demonstrates that WEAT bias measured in NYT GloVe embeddings correlates consistently with the corpus' PPMI representation across 2000 randomly generated word sets.",
        "visual_anchor": "diagonal scatter pattern with r²=0.725",
        "text_evidence": "The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus' PPMI representation in 2000 randomly generated word sets, r²=0.725",
        "query_type": "value_context"
      },
      {
        "query": "Why would the scatter correlation at r²=0.725 be considered weaker than word2vec's comparable measurement?",
        "answer": "The r²=0.725 correlation is weaker than word2vec's r²=0.803 when measured with comparable hyper-parameters, suggesting that word2vec embeddings show more consistent bias patterns with the underlying PPMI representation than GloVe embeddings.",
        "visual_anchor": "r²=0.725 correlation coefficient",
        "text_evidence": "versus when measured in word2vec embeddings with comparable hyper-parameters, r²=0.803",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the positive correlation in the scatter plot relate to identifying bias-influencing documents dated January 1987 to June 2007?",
        "answer": "The positive correlation validates that WEAT can reliably measure bias in embeddings trained on the corpus, enabling the identification of the 50 most WEAT1 bias-influencing documents from the NYT dataset spanning those dates.",
        "visual_anchor": "positive correlation scatter pattern",
        "text_evidence": "The below documents were identified to be the 50 most WEAT1 bias influencing documents in our NYT setup. Publication dates range from January 1, 1987 to June 19, 2007.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Scatter plot with blue dots showing positive correlation",
      "X-axis labeled 'GloVe WEAT' ranging from -2.0 to 2.0",
      "Y-axis labeled 'PPM WEAT' ranging from -2.0 to 2.0",
      "Dense cluster of points along diagonal from bottom-left to top-right",
      "r² value of 0.725 for correlation strength"
    ]
  },
  {
    "doc_id": "1810.03993",
    "figure_id": "1810.03993_fig_1",
    "figure_number": null,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg",
    "caption": "",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Horizontal error bars showing confidence intervals\",\n    \"Green dots indicating false positive rates\",\n    \"Y-axis labels: old-male, old-female, young-female, young-male, old, young, male, female, all\",\n    \"X-axis: False Positive Rate @ 0.5 ranging from 0.00 to 0.14\",\n    \"old-male group shows highest FPR around 0.08\",\n    \"all group shows lowest FPR around 0.06\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does old-male reach the highest false positive rate near 0.08 when analyzing public figures only?\",\n      \"answer\": \"The dataset is based on faces and annotations of public figures (celebrities), which may introduce demographic biases. The old-male group shows the highest FPR, suggesting potential bias in how the model performs across different demographic segments.\",\n      \"visual_anchor\": \"Green dot at old-male row positioned at approximately 0.08 on x-axis\",\n      \"text_evidence\": \"Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the 0.02 difference between old-male and young-female false positive rates indicate demographic performance variation?\",\n      \"answer\": \"Yes, the approximately 0.02 difference between old-male (around 0.08) and young-female (around 0.06) suggests notable demographic performance variation, which is an important ethical consideration when using celebrity-based annotations.\",\n      \"visual_anchor\": \"Comparison between old-male green dot at 0.08 and young-female green dot at 0.06\",\n      \"text_evidence\": \"Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the overlapping confidence intervals for young and all groups relate to ethical annotation practices?\",\n      \"answer\": \"The overlapping confidence intervals between young (around 0.06) and all (around 0.06) groups suggest similar performance levels. This relates to ethical practices because no new information is inferred beyond the original celebrity annotations.\",\n      \"visual_anchor\": \"Horizontal error bars for young and all rows showing overlap near 0.06\",\n      \"text_evidence\": \"No new information is inferred or annotated.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Horizontal error bars showing confidence intervals",
        "Green dots indicating false positive rates",
        "Y-axis labels: old-male, old-female, young-female, young-male, old, young, male, female, all",
        "X-axis: False Positive Rate @ 0.5 ranging from 0.00 to 0.14",
        "old-male group shows highest FPR around 0.08",
        "all group shows lowest FPR around 0.06"
      ],
      "queries": [
        {
          "query": "Why does old-male reach the highest false positive rate near 0.08 when analyzing public figures only?",
          "answer": "The dataset is based on faces and annotations of public figures (celebrities), which may introduce demographic biases. The old-male group shows the highest FPR, suggesting potential bias in how the model performs across different demographic segments.",
          "visual_anchor": "Green dot at old-male row positioned at approximately 0.08 on x-axis",
          "text_evidence": "Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the 0.02 difference between old-male and young-female false positive rates indicate demographic performance variation?",
          "answer": "Yes, the approximately 0.02 difference between old-male (around 0.08) and young-female (around 0.06) suggests notable demographic performance variation, which is an important ethical consideration when using celebrity-based annotations.",
          "visual_anchor": "Comparison between old-male green dot at 0.08 and young-female green dot at 0.06",
          "text_evidence": "Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the overlapping confidence intervals for young and all groups relate to ethical annotation practices?",
          "answer": "The overlapping confidence intervals between young (around 0.06) and all (around 0.06) groups suggest similar performance levels. This relates to ethical practices because no new information is inferred beyond the original celebrity annotations.",
          "visual_anchor": "Horizontal error bars for young and all rows showing overlap near 0.06",
          "text_evidence": "No new information is inferred or annotated.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does old-male reach the highest false positive rate near 0.08 when analyzing public figures only?",
        "answer": "The dataset is based on faces and annotations of public figures (celebrities), which may introduce demographic biases. The old-male group shows the highest FPR, suggesting potential bias in how the model performs across different demographic segments.",
        "visual_anchor": "Green dot at old-male row positioned at approximately 0.08 on x-axis",
        "text_evidence": "Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the 0.02 difference between old-male and young-female false positive rates indicate demographic performance variation?",
        "answer": "Yes, the approximately 0.02 difference between old-male (around 0.08) and young-female (around 0.06) suggests notable demographic performance variation, which is an important ethical consideration when using celebrity-based annotations.",
        "visual_anchor": "Comparison between old-male green dot at 0.08 and young-female green dot at 0.06",
        "text_evidence": "Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the overlapping confidence intervals for young and all groups relate to ethical annotation practices?",
        "answer": "The overlapping confidence intervals between young (around 0.06) and all (around 0.06) groups suggest similar performance levels. This relates to ethical practices because no new information is inferred beyond the original celebrity annotations.",
        "visual_anchor": "Horizontal error bars for young and all rows showing overlap near 0.06",
        "text_evidence": "No new information is inferred or annotated.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Horizontal error bars showing confidence intervals",
      "Green dots indicating false positive rates",
      "Y-axis labels: old-male, old-female, young-female, young-male, old, young, male, female, all",
      "X-axis: False Positive Rate @ 0.5 ranging from 0.00 to 0.14",
      "old-male group shows highest FPR around 0.08",
      "all group shows lowest FPR around 0.06"
    ]
  },
  {
    "doc_id": "1810.03993",
    "figure_id": "1810.03993_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg",
    "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Error bars showing confidence intervals for each demographic group\",\n    \"False negative rate values on x-axis from 0.00 to 0.14\",\n    \"Threshold value of 0.5 in title\",\n    \"Nine demographic categories on y-axis: old-male, old-female, young-female, young-male, old, young, male, female, all\",\n    \"Circle markers indicating point estimates for each group\",\n    \"Horizontal error bars extending from each circle marker\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the error bars for old-male extend nearly to 0.12 despite the model card goal of highlighting disproportionate errors?\",\n      \"answer\": \"The wide error bars for old-male demonstrate metric variation that the model card uses to highlight errors falling disproportionately on some groups, consistent with mathematical fairness notions.\",\n      \"visual_anchor\": \"old-male error bars extending from approximately 0.09 to 0.12\",\n      \"text_evidence\": \"Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the absence of race-specific categories in the y-axis labels reflect the evaluation dataset limitation about skin type annotations?\",\n      \"answer\": \"Yes, the model card explicitly notes it does not capture race or skin type, which has been reported as a source of disproportionate errors, indicating a limitation in the evaluation approach.\",\n      \"visual_anchor\": \"y-axis showing only gender and age categories without race or skin type\",\n      \"text_evidence\": \"Does not capture race or skin type, which has been reported as a source of disproportionate errors [5].\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the binary male-female categorization visible in the demographic groups relate to the stated evaluation spectrum concern?\",\n      \"answer\": \"The binary gender classes shown as male/female reflect a limitation, as further work is needed to evaluate across a spectrum of genders beyond this binary representation.\",\n      \"visual_anchor\": \"demographic labels showing male, female, old-male, old-female, young-male, young-female categories\",\n      \"text_evidence\": \"Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Error bars showing confidence intervals for each demographic group",
        "False negative rate values on x-axis from 0.00 to 0.14",
        "Threshold value of 0.5 in title",
        "Nine demographic categories on y-axis: old-male, old-female, young-female, young-male, old, young, male, female, all",
        "Circle markers indicating point estimates for each group",
        "Horizontal error bars extending from each circle marker"
      ],
      "queries": [
        {
          "query": "Why do the error bars for old-male extend nearly to 0.12 despite the model card goal of highlighting disproportionate errors?",
          "answer": "The wide error bars for old-male demonstrate metric variation that the model card uses to highlight errors falling disproportionately on some groups, consistent with mathematical fairness notions.",
          "visual_anchor": "old-male error bars extending from approximately 0.09 to 0.12",
          "text_evidence": "Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the absence of race-specific categories in the y-axis labels reflect the evaluation dataset limitation about skin type annotations?",
          "answer": "Yes, the model card explicitly notes it does not capture race or skin type, which has been reported as a source of disproportionate errors, indicating a limitation in the evaluation approach.",
          "visual_anchor": "y-axis showing only gender and age categories without race or skin type",
          "text_evidence": "Does not capture race or skin type, which has been reported as a source of disproportionate errors [5].",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the binary male-female categorization visible in the demographic groups relate to the stated evaluation spectrum concern?",
          "answer": "The binary gender classes shown as male/female reflect a limitation, as further work is needed to evaluate across a spectrum of genders beyond this binary representation.",
          "visual_anchor": "demographic labels showing male, female, old-male, old-female, young-male, young-female categories",
          "text_evidence": "Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the error bars for old-male extend nearly to 0.12 despite the model card goal of highlighting disproportionate errors?",
        "answer": "The wide error bars for old-male demonstrate metric variation that the model card uses to highlight errors falling disproportionately on some groups, consistent with mathematical fairness notions.",
        "visual_anchor": "old-male error bars extending from approximately 0.09 to 0.12",
        "text_evidence": "Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the absence of race-specific categories in the y-axis labels reflect the evaluation dataset limitation about skin type annotations?",
        "answer": "Yes, the model card explicitly notes it does not capture race or skin type, which has been reported as a source of disproportionate errors, indicating a limitation in the evaluation approach.",
        "visual_anchor": "y-axis showing only gender and age categories without race or skin type",
        "text_evidence": "Does not capture race or skin type, which has been reported as a source of disproportionate errors [5].",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the binary male-female categorization visible in the demographic groups relate to the stated evaluation spectrum concern?",
        "answer": "The binary gender classes shown as male/female reflect a limitation, as further work is needed to evaluate across a spectrum of genders beyond this binary representation.",
        "visual_anchor": "demographic labels showing male, female, old-male, old-female, young-male, young-female categories",
        "text_evidence": "Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Error bars showing confidence intervals for each demographic group",
      "False negative rate values on x-axis from 0.00 to 0.14",
      "Threshold value of 0.5 in title",
      "Nine demographic categories on y-axis: old-male, old-female, young-female, young-male, old, young, male, female, all",
      "Circle markers indicating point estimates for each group",
      "Horizontal error bars extending from each circle marker"
    ]
  },
  {
    "doc_id": "1810.03993",
    "figure_id": "1810.03993_fig_6",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg",
    "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three groups of light blue bars with black borders\",\n    \"Y-axis ranging from 0.82 to 1.00 showing Pinned AUC\",\n    \"X-axis labels: lesbian, gay, queer, straight in first group; bisexual, homosexual, heterosexual in second group; cis, trans, nonbinary in third group; black, white in fourth group\",\n    \"All bars approximately between 0.95 and 0.98 height\",\n    \"Title: Pinned AUC by Unitary Groups (Version 5)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the bisexual, homosexual, and heterosexual bars all reaching approximately 0.97 AUC demonstrate Perspective API's toxicity detection performance?\",\n      \"answer\": \"Yes, the bars show Version 5 of Perspective API's TOXICITY classifier achieves consistently high AUC scores across different sexual orientation identity groups, indicating relatively balanced performance for toxicity detection across these demographic categories.\",\n      \"visual_anchor\": \"Middle group of three bars (bisexual, homosexual, heterosexual) all at approximately 0.97 AUC\",\n      \"text_evidence\": \"Our second example provides a model card for Perspective API's TOXICITY classifier built to detect 'toxicity' in text [32], and is presented in Figure 3.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the lesbian, gay, queer, and straight bars show nearly identical heights around 0.97 despite representing different identity groups?\",\n      \"answer\": \"The uniform performance across these identity groups suggests Version 5 of the toxicity detector was designed or trained to maintain consistent detection capabilities across different sexual orientations, reflecting the model card's documentation of performance across demographic slices.\",\n      \"visual_anchor\": \"First group of four bars (lesbian, gay, queer, straight) all at approximately 0.97 height\",\n      \"text_evidence\": \"We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the black bar reaching approximately 0.97 while white reaches 0.96 indicate racial performance parity in toxicity classification?\",\n      \"answer\": \"The minimal difference between the black and white bars suggests near-parity in the toxicity detector's performance across these racial groups, demonstrating relatively balanced model behavior as documented in the model card example.\",\n      \"visual_anchor\": \"Fourth group showing black bar at ~0.97 and white bar at ~0.96\",\n      \"text_evidence\": \"Example Model Card for two versions of Perspective API's toxicity detector.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three groups of light blue bars with black borders",
        "Y-axis ranging from 0.82 to 1.00 showing Pinned AUC",
        "X-axis labels: lesbian, gay, queer, straight in first group; bisexual, homosexual, heterosexual in second group; cis, trans, nonbinary in third group; black, white in fourth group",
        "All bars approximately between 0.95 and 0.98 height",
        "Title: Pinned AUC by Unitary Groups (Version 5)"
      ],
      "queries": [
        {
          "query": "Do the bisexual, homosexual, and heterosexual bars all reaching approximately 0.97 AUC demonstrate Perspective API's toxicity detection performance?",
          "answer": "Yes, the bars show Version 5 of Perspective API's TOXICITY classifier achieves consistently high AUC scores across different sexual orientation identity groups, indicating relatively balanced performance for toxicity detection across these demographic categories.",
          "visual_anchor": "Middle group of three bars (bisexual, homosexual, heterosexual) all at approximately 0.97 AUC",
          "text_evidence": "Our second example provides a model card for Perspective API's TOXICITY classifier built to detect 'toxicity' in text [32], and is presented in Figure 3.",
          "query_type": "value_context"
        },
        {
          "query": "Why do the lesbian, gay, queer, and straight bars show nearly identical heights around 0.97 despite representing different identity groups?",
          "answer": "The uniform performance across these identity groups suggests Version 5 of the toxicity detector was designed or trained to maintain consistent detection capabilities across different sexual orientations, reflecting the model card's documentation of performance across demographic slices.",
          "visual_anchor": "First group of four bars (lesbian, gay, queer, straight) all at approximately 0.97 height",
          "text_evidence": "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the black bar reaching approximately 0.97 while white reaches 0.96 indicate racial performance parity in toxicity classification?",
          "answer": "The minimal difference between the black and white bars suggests near-parity in the toxicity detector's performance across these racial groups, demonstrating relatively balanced model behavior as documented in the model card example.",
          "visual_anchor": "Fourth group showing black bar at ~0.97 and white bar at ~0.96",
          "text_evidence": "Example Model Card for two versions of Perspective API's toxicity detector.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the bisexual, homosexual, and heterosexual bars all reaching approximately 0.97 AUC demonstrate Perspective API's toxicity detection performance?",
        "answer": "Yes, the bars show Version 5 of Perspective API's TOXICITY classifier achieves consistently high AUC scores across different sexual orientation identity groups, indicating relatively balanced performance for toxicity detection across these demographic categories.",
        "visual_anchor": "Middle group of three bars (bisexual, homosexual, heterosexual) all at approximately 0.97 AUC",
        "text_evidence": "Our second example provides a model card for Perspective API's TOXICITY classifier built to detect 'toxicity' in text [32], and is presented in Figure 3.",
        "query_type": "value_context"
      },
      {
        "query": "Why do the lesbian, gay, queer, and straight bars show nearly identical heights around 0.97 despite representing different identity groups?",
        "answer": "The uniform performance across these identity groups suggests Version 5 of the toxicity detector was designed or trained to maintain consistent detection capabilities across different sexual orientations, reflecting the model card's documentation of performance across demographic slices.",
        "visual_anchor": "First group of four bars (lesbian, gay, queer, straight) all at approximately 0.97 height",
        "text_evidence": "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the black bar reaching approximately 0.97 while white reaches 0.96 indicate racial performance parity in toxicity classification?",
        "answer": "The minimal difference between the black and white bars suggests near-parity in the toxicity detector's performance across these racial groups, demonstrating relatively balanced model behavior as documented in the model card example.",
        "visual_anchor": "Fourth group showing black bar at ~0.97 and white bar at ~0.96",
        "text_evidence": "Example Model Card for two versions of Perspective API's toxicity detector.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three groups of light blue bars with black borders",
      "Y-axis ranging from 0.82 to 1.00 showing Pinned AUC",
      "X-axis labels: lesbian, gay, queer, straight in first group; bisexual, homosexual, heterosexual in second group; cis, trans, nonbinary in third group; black, white in fourth group",
      "All bars approximately between 0.95 and 0.98 height",
      "Title: Pinned AUC by Unitary Groups (Version 5)"
    ]
  },
  {
    "doc_id": "1810.08683",
    "figure_id": "1810.08683_fig_1",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg",
    "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Blue square markers labeled STL/ITL\", \"Red circle markers labeled MTL\", \"Normalized EOI on y-axis (0-1)\", \"Averaged Group Error on x-axis (0-1)\", \"Data point labels with 4-digit codes (0000, 0001, 0010, etc.)\", \"Dashed diagonal grid lines\", \"Legend showing STL/ITL and MTL\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do MTL red circles cluster near zero Normalized EOI despite the Adult dataset containing 45222 instances?\",\n      \"answer\": \"The MTL approach appears to achieve lower normalized EOI values across different configurations, suggesting multi-task learning may reduce bias in the income prediction task across the large Adult dataset population.\",\n      \"visual_anchor\": \"Red circle markers (MTL) positioned at bottom of plot with Normalized EOI values near 0-0.2\",\n      \"text_evidence\": \"The Adult dataset contains 14 features concerning demographic characteristics of 45222 instances (32561 for training and 12661 for testing), 2 features, Gender (G) and Race (R), can be considered sensitive.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the blue square at coordinates approximately (0.05, 0.73) support that single-task learning handles Gender sensitivity differently?\",\n      \"answer\": \"Yes, the high Normalized EOI value of approximately 0.73 for this STL/ITL configuration suggests single-task approaches may exhibit greater bias when Gender is the sensitive feature being analyzed.\",\n      \"visual_anchor\": \"Blue square marker (STL/ITL) at top-left region with Normalized EOI ~0.73 and Averaged Group Error ~0.05\",\n      \"text_evidence\": \"2 features, Gender (G) and Race (R), can be considered sensitive. The task is to predict if a person has an income per year that is more (or less) than 50 000$.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Which approach shows lower Averaged Group Error at high Normalized EOI, given that text labels indicate P, D, F, S configurations?\",\n      \"answer\": \"STL/ITL (blue squares) shows lower Averaged Group Error values (0.05-0.2) at high Normalized EOI (0.7-1.0) compared to MTL, suggesting single-task methods achieve better group accuracy when normalized EOI is high.\",\n      \"visual_anchor\": \"Blue squares in upper-left region (low x-axis, high y-axis) versus red circles distributed across higher x-axis values\",\n      \"text_evidence\": \"complete results set for Gender (text close to the symbols in plot are P, D, F, and S)\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue square markers labeled STL/ITL",
        "Red circle markers labeled MTL",
        "Normalized EOI on y-axis (0-1)",
        "Averaged Group Error on x-axis (0-1)",
        "Data point labels with 4-digit codes (0000, 0001, 0010, etc.)",
        "Dashed diagonal grid lines",
        "Legend showing STL/ITL and MTL"
      ],
      "queries": [
        {
          "query": "Why do MTL red circles cluster near zero Normalized EOI despite the Adult dataset containing 45222 instances?",
          "answer": "The MTL approach appears to achieve lower normalized EOI values across different configurations, suggesting multi-task learning may reduce bias in the income prediction task across the large Adult dataset population.",
          "visual_anchor": "Red circle markers (MTL) positioned at bottom of plot with Normalized EOI values near 0-0.2",
          "text_evidence": "The Adult dataset contains 14 features concerning demographic characteristics of 45222 instances (32561 for training and 12661 for testing), 2 features, Gender (G) and Race (R), can be considered sensitive.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the blue square at coordinates approximately (0.05, 0.73) support that single-task learning handles Gender sensitivity differently?",
          "answer": "Yes, the high Normalized EOI value of approximately 0.73 for this STL/ITL configuration suggests single-task approaches may exhibit greater bias when Gender is the sensitive feature being analyzed.",
          "visual_anchor": "Blue square marker (STL/ITL) at top-left region with Normalized EOI ~0.73 and Averaged Group Error ~0.05",
          "text_evidence": "2 features, Gender (G) and Race (R), can be considered sensitive. The task is to predict if a person has an income per year that is more (or less) than 50 000$.",
          "query_type": "value_context"
        },
        {
          "query": "Which approach shows lower Averaged Group Error at high Normalized EOI, given that text labels indicate P, D, F, S configurations?",
          "answer": "STL/ITL (blue squares) shows lower Averaged Group Error values (0.05-0.2) at high Normalized EOI (0.7-1.0) compared to MTL, suggesting single-task methods achieve better group accuracy when normalized EOI is high.",
          "visual_anchor": "Blue squares in upper-left region (low x-axis, high y-axis) versus red circles distributed across higher x-axis values",
          "text_evidence": "complete results set for Gender (text close to the symbols in plot are P, D, F, and S)",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do MTL red circles cluster near zero Normalized EOI despite the Adult dataset containing 45222 instances?",
        "answer": "The MTL approach appears to achieve lower normalized EOI values across different configurations, suggesting multi-task learning may reduce bias in the income prediction task across the large Adult dataset population.",
        "visual_anchor": "Red circle markers (MTL) positioned at bottom of plot with Normalized EOI values near 0-0.2",
        "text_evidence": "The Adult dataset contains 14 features concerning demographic characteristics of 45222 instances (32561 for training and 12661 for testing), 2 features, Gender (G) and Race (R), can be considered sensitive.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the blue square at coordinates approximately (0.05, 0.73) support that single-task learning handles Gender sensitivity differently?",
        "answer": "Yes, the high Normalized EOI value of approximately 0.73 for this STL/ITL configuration suggests single-task approaches may exhibit greater bias when Gender is the sensitive feature being analyzed.",
        "visual_anchor": "Blue square marker (STL/ITL) at top-left region with Normalized EOI ~0.73 and Averaged Group Error ~0.05",
        "text_evidence": "2 features, Gender (G) and Race (R), can be considered sensitive. The task is to predict if a person has an income per year that is more (or less) than 50 000$.",
        "query_type": "value_context"
      },
      {
        "query": "Which approach shows lower Averaged Group Error at high Normalized EOI, given that text labels indicate P, D, F, S configurations?",
        "answer": "STL/ITL (blue squares) shows lower Averaged Group Error values (0.05-0.2) at high Normalized EOI (0.7-1.0) compared to MTL, suggesting single-task methods achieve better group accuracy when normalized EOI is high.",
        "visual_anchor": "Blue squares in upper-left region (low x-axis, high y-axis) versus red circles distributed across higher x-axis values",
        "text_evidence": "complete results set for Gender (text close to the symbols in plot are P, D, F, and S)",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue square markers labeled STL/ITL",
      "Red circle markers labeled MTL",
      "Normalized EOI on y-axis (0-1)",
      "Averaged Group Error on x-axis (0-1)",
      "Data point labels with 4-digit codes (0000, 0001, 0010, etc.)",
      "Dashed diagonal grid lines",
      "Legend showing STL/ITL and MTL"
    ]
  },
  {
    "doc_id": "1810.08683",
    "figure_id": "1810.08683_fig_2",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig1.jpg",
    "caption": "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Red circles labeled MTL\", \"Blue squares labeled STL/ITL\", \"Normalized EOd on y-axis\", \"Averaged Group Error on x-axis\", \"Binary labels like 0100, 0001, 1001, 0111, 1011, 0010, 1010, 0011, 1000, 0000, 1001, 0101, 1100, 1101, 1000, 0110, 1110, 1011, 0001, 1101, 1011, 0010, 1010\"],\n  \"queries\": [\n    {\n      \"query\": \"Do the red MTL circles at labels 0001 and 0000 confirm the confusion matrices computed on the test set?\",\n      \"answer\": \"Yes, the red MTL circles at 0001 (Normalized EOd ~0.9) and 0000 (Normalized EOd ~0.7) represent specific race prediction configurations that align with the confusion matrix results reported for the test set.\",\n      \"visual_anchor\": \"Red circles labeled MTL at positions 0001 (Normalized EOd ~0.9, Averaged Group Error ~0.65) and 0000 (Normalized EOd ~0.7, Averaged Group Error ~0.8)\",\n      \"text_evidence\": \"Table 3 and Table 4 report the confusion matrices computed on the test set.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do blue STL/ITL squares at 0111 cluster lower in Normalized EOd than red MTL circles at 1001?\",\n      \"answer\": \"The blue STL/ITL squares at 0111 (Normalized EOd ~0.4) show better fairness metrics than red MTL circles at 1001 (Normalized EOd ~0.55), likely because Random Forests predict Race differently depending on the combination of sensitive features P, D, F, and S.\",\n      \"visual_anchor\": \"Blue squares labeled STL/ITL at 0111 (Normalized EOd ~0.4) versus red circles labeled MTL at 1001 (Normalized EOd ~0.55)\",\n      \"text_evidence\": \"Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the high Normalized EOd of red circle 0001 given Random Forests predict Race from nonsensitive features?\",\n      \"answer\": \"The red MTL circle at 0001 shows high Normalized EOd (~0.9) because this specific combination of protected attributes (P, D, F, S) leads to higher equalized odds disparity when Random Forests predict Race from other nonsensitive features in the Adult dataset.\",\n      \"visual_anchor\": \"Red circle labeled MTL at 0001 with Normalized EOd ~0.9 and Averaged Group Error ~0.65\",\n      \"text_evidence\": \"confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red circles labeled MTL",
        "Blue squares labeled STL/ITL",
        "Normalized EOd on y-axis",
        "Averaged Group Error on x-axis",
        "Binary labels like 0100, 0001, 1001, 0111, 1011, 0010, 1010, 0011, 1000, 0000, 1001, 0101, 1100, 1101, 1000, 0110, 1110, 1011, 0001, 1101, 1011, 0010, 1010"
      ],
      "queries": [
        {
          "query": "Do the red MTL circles at labels 0001 and 0000 confirm the confusion matrices computed on the test set?",
          "answer": "Yes, the red MTL circles at 0001 (Normalized EOd ~0.9) and 0000 (Normalized EOd ~0.7) represent specific race prediction configurations that align with the confusion matrix results reported for the test set.",
          "visual_anchor": "Red circles labeled MTL at positions 0001 (Normalized EOd ~0.9, Averaged Group Error ~0.65) and 0000 (Normalized EOd ~0.7, Averaged Group Error ~0.8)",
          "text_evidence": "Table 3 and Table 4 report the confusion matrices computed on the test set.",
          "query_type": "value_context"
        },
        {
          "query": "Why do blue STL/ITL squares at 0111 cluster lower in Normalized EOd than red MTL circles at 1001?",
          "answer": "The blue STL/ITL squares at 0111 (Normalized EOd ~0.4) show better fairness metrics than red MTL circles at 1001 (Normalized EOd ~0.55), likely because Random Forests predict Race differently depending on the combination of sensitive features P, D, F, and S.",
          "visual_anchor": "Blue squares labeled STL/ITL at 0111 (Normalized EOd ~0.4) versus red circles labeled MTL at 1001 (Normalized EOd ~0.55)",
          "text_evidence": "Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the high Normalized EOd of red circle 0001 given Random Forests predict Race from nonsensitive features?",
          "answer": "The red MTL circle at 0001 shows high Normalized EOd (~0.9) because this specific combination of protected attributes (P, D, F, S) leads to higher equalized odds disparity when Random Forests predict Race from other nonsensitive features in the Adult dataset.",
          "visual_anchor": "Red circle labeled MTL at 0001 with Normalized EOd ~0.9 and Averaged Group Error ~0.65",
          "text_evidence": "confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the red MTL circles at labels 0001 and 0000 confirm the confusion matrices computed on the test set?",
        "answer": "Yes, the red MTL circles at 0001 (Normalized EOd ~0.9) and 0000 (Normalized EOd ~0.7) represent specific race prediction configurations that align with the confusion matrix results reported for the test set.",
        "visual_anchor": "Red circles labeled MTL at positions 0001 (Normalized EOd ~0.9, Averaged Group Error ~0.65) and 0000 (Normalized EOd ~0.7, Averaged Group Error ~0.8)",
        "text_evidence": "Table 3 and Table 4 report the confusion matrices computed on the test set.",
        "query_type": "value_context"
      },
      {
        "query": "Why do blue STL/ITL squares at 0111 cluster lower in Normalized EOd than red MTL circles at 1001?",
        "answer": "The blue STL/ITL squares at 0111 (Normalized EOd ~0.4) show better fairness metrics than red MTL circles at 1001 (Normalized EOd ~0.55), likely because Random Forests predict Race differently depending on the combination of sensitive features P, D, F, and S.",
        "visual_anchor": "Blue squares labeled STL/ITL at 0111 (Normalized EOd ~0.4) versus red circles labeled MTL at 1001 (Normalized EOd ~0.55)",
        "text_evidence": "Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the high Normalized EOd of red circle 0001 given Random Forests predict Race from nonsensitive features?",
        "answer": "The red MTL circle at 0001 shows high Normalized EOd (~0.9) because this specific combination of protected attributes (P, D, F, S) leads to higher equalized odds disparity when Random Forests predict Race from other nonsensitive features in the Adult dataset.",
        "visual_anchor": "Red circle labeled MTL at 0001 with Normalized EOd ~0.9 and Averaged Group Error ~0.65",
        "text_evidence": "confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red circles labeled MTL",
      "Blue squares labeled STL/ITL",
      "Normalized EOd on y-axis",
      "Averaged Group Error on x-axis",
      "Binary labels like 0100, 0001, 1001, 0111, 1011, 0010, 1010, 0011, 1000, 0000, 1001, 0101, 1100, 1101, 1000, 0110, 1110, 1011, 0001, 1101, 1011, 0010, 1010"
    ]
  },
  {
    "doc_id": "1810.08683",
    "figure_id": "1810.08683_fig_3",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig2.jpg",
    "caption": "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red circles labeled MTL\",\n    \"Blue squares labeled STL/ITL\",\n    \"Normalized EOd on y-axis (0 to 1.0)\",\n    \"Averaged Group Error on x-axis (0 to 1.0)\",\n    \"Binary labels near symbols (e.g., 0101, 1000, 1001)\",\n    \"Diagonal dashed contour lines\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the red MTL circles at coordinates near 0.8 averaged error support the Random Forests method used for predicting from nonsensitive features?\",\n      \"answer\": \"The red MTL circles at high averaged error (around 0.8-1.0) reflect poor group fairness, consistent with using Random Forests to predict Gender and Race from other features, as shown in the confusion matrices.\",\n      \"visual_anchor\": \"Red MTL circles at x≈0.8-1.0\",\n      \"text_evidence\": \"confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do blue STL/ITL squares cluster at lower normalized EOd values than red MTL circles for the same binary label patterns?\",\n      \"answer\": \"Blue STL/ITL squares achieve lower normalized EOd (better fairness) than red MTL circles, suggesting single-task or independent learning approaches yield more equitable outcomes across protected groups than multi-task learning for Gender+Race combinations.\",\n      \"visual_anchor\": \"Blue squares at lower y-values compared to red circles with similar x-coordinates\",\n      \"text_evidence\": \"complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the red circle at 1011 having near-zero normalized EOd despite the test set confusion matrices showing prediction errors?\",\n      \"answer\": \"The 1011 red circle at normalized EOd≈0 indicates equal error rates across groups for that specific Gender+Race combination, even though overall prediction accuracy may be poor as evidenced by the confusion matrices on the test set.\",\n      \"visual_anchor\": \"Red circle labeled 1011 at y≈0\",\n      \"text_evidence\": \"Table 3 and Table 4 report the confusion matrices computed on the test set\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red circles labeled MTL",
        "Blue squares labeled STL/ITL",
        "Normalized EOd on y-axis (0 to 1.0)",
        "Averaged Group Error on x-axis (0 to 1.0)",
        "Binary labels near symbols (e.g., 0101, 1000, 1001)",
        "Diagonal dashed contour lines"
      ],
      "queries": [
        {
          "query": "Do the red MTL circles at coordinates near 0.8 averaged error support the Random Forests method used for predicting from nonsensitive features?",
          "answer": "The red MTL circles at high averaged error (around 0.8-1.0) reflect poor group fairness, consistent with using Random Forests to predict Gender and Race from other features, as shown in the confusion matrices.",
          "visual_anchor": "Red MTL circles at x≈0.8-1.0",
          "text_evidence": "confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests",
          "query_type": "value_context"
        },
        {
          "query": "Why do blue STL/ITL squares cluster at lower normalized EOd values than red MTL circles for the same binary label patterns?",
          "answer": "Blue STL/ITL squares achieve lower normalized EOd (better fairness) than red MTL circles, suggesting single-task or independent learning approaches yield more equitable outcomes across protected groups than multi-task learning for Gender+Race combinations.",
          "visual_anchor": "Blue squares at lower y-values compared to red circles with similar x-coordinates",
          "text_evidence": "complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the red circle at 1011 having near-zero normalized EOd despite the test set confusion matrices showing prediction errors?",
          "answer": "The 1011 red circle at normalized EOd≈0 indicates equal error rates across groups for that specific Gender+Race combination, even though overall prediction accuracy may be poor as evidenced by the confusion matrices on the test set.",
          "visual_anchor": "Red circle labeled 1011 at y≈0",
          "text_evidence": "Table 3 and Table 4 report the confusion matrices computed on the test set",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the red MTL circles at coordinates near 0.8 averaged error support the Random Forests method used for predicting from nonsensitive features?",
        "answer": "The red MTL circles at high averaged error (around 0.8-1.0) reflect poor group fairness, consistent with using Random Forests to predict Gender and Race from other features, as shown in the confusion matrices.",
        "visual_anchor": "Red MTL circles at x≈0.8-1.0",
        "text_evidence": "confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests",
        "query_type": "value_context"
      },
      {
        "query": "Why do blue STL/ITL squares cluster at lower normalized EOd values than red MTL circles for the same binary label patterns?",
        "answer": "Blue STL/ITL squares achieve lower normalized EOd (better fairness) than red MTL circles, suggesting single-task or independent learning approaches yield more equitable outcomes across protected groups than multi-task learning for Gender+Race combinations.",
        "visual_anchor": "Blue squares at lower y-values compared to red circles with similar x-coordinates",
        "text_evidence": "complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the red circle at 1011 having near-zero normalized EOd despite the test set confusion matrices showing prediction errors?",
        "answer": "The 1011 red circle at normalized EOd≈0 indicates equal error rates across groups for that specific Gender+Race combination, even though overall prediction accuracy may be poor as evidenced by the confusion matrices on the test set.",
        "visual_anchor": "Red circle labeled 1011 at y≈0",
        "text_evidence": "Table 3 and Table 4 report the confusion matrices computed on the test set",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red circles labeled MTL",
      "Blue squares labeled STL/ITL",
      "Normalized EOd on y-axis (0 to 1.0)",
      "Averaged Group Error on x-axis (0 to 1.0)",
      "Binary labels near symbols (e.g., 0101, 1000, 1001)",
      "Diagonal dashed contour lines"
    ]
  },
  {
    "doc_id": "1810.08683",
    "figure_id": "1810.08683_fig_4",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
    "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top panel: blue line (ACC) peaks around 85 at λ=1e-4, orange line (EOd) remains relatively flat around 0.07\",\n    \"Bottom panel: blue line (ACC) peaks around 91 at λ=2e-10 then declines to ~67 at λ=1, orange line (EOd) rises from ~0.05 to ~0.13\",\n    \"X-axis shows λ values from 0 to 1 with logarithmic spacing\",\n    \"Two y-axes: left (blue) for ACC ranging 65-95, right (orange) for EOd ranging 0-0.2\",\n    \"Top panel labeled D=0, bottom panel labeled D=1\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does ACC peak at 91 near λ=2e-10 in the bottom panel when θ and ρ are fixed?\",\n      \"answer\": \"The figure shows results when θ and ρ are fixed to best validation values while varying λ with P=0, F=1, S=0. The peak at λ=2e-10 represents the optimal balance for D=1 under these specific parameter constraints.\",\n      \"visual_anchor\": \"blue line peak at approximately 91 ACC at λ=2e-10 in bottom panel (D=1)\",\n      \"text_evidence\": \"we fix $\\\\theta$ and $\\\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?\",\n      \"answer\": \"Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.\",\n      \"visual_anchor\": \"opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel\",\n      \"text_evidence\": \"This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the top panel show stable EOd around 0.07 while bottom panel EOd varies from 0.05 to 0.13?\",\n      \"answer\": \"The difference between D=0 (top) and D=1 (bottom) conditions creates distinct fairness behavior. D=1 allows the sensitive feature prediction to affect fairness metrics more dynamically across λ values, while D=0 maintains more stable EOd.\",\n      \"visual_anchor\": \"flat orange EOd line (~0.07) in top panel versus curved orange EOd line (0.05 to 0.13) in bottom panel\",\n      \"text_evidence\": \"to first predict the sensitive feature based on the non-sensitive features, and then use the predicted value in the functional form of a model, allowing to treat people belonging to different groups, but having similar non-se\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top panel: blue line (ACC) peaks around 85 at λ=1e-4, orange line (EOd) remains relatively flat around 0.07",
        "Bottom panel: blue line (ACC) peaks around 91 at λ=2e-10 then declines to ~67 at λ=1, orange line (EOd) rises from ~0.05 to ~0.13",
        "X-axis shows λ values from 0 to 1 with logarithmic spacing",
        "Two y-axes: left (blue) for ACC ranging 65-95, right (orange) for EOd ranging 0-0.2",
        "Top panel labeled D=0, bottom panel labeled D=1"
      ],
      "queries": [
        {
          "query": "Why does ACC peak at 91 near λ=2e-10 in the bottom panel when θ and ρ are fixed?",
          "answer": "The figure shows results when θ and ρ are fixed to best validation values while varying λ with P=0, F=1, S=0. The peak at λ=2e-10 represents the optimal balance for D=1 under these specific parameter constraints.",
          "visual_anchor": "blue line peak at approximately 91 ACC at λ=2e-10 in bottom panel (D=1)",
          "text_evidence": "we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$",
          "query_type": "value_context"
        },
        {
          "query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
          "answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
          "visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
          "text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the top panel show stable EOd around 0.07 while bottom panel EOd varies from 0.05 to 0.13?",
          "answer": "The difference between D=0 (top) and D=1 (bottom) conditions creates distinct fairness behavior. D=1 allows the sensitive feature prediction to affect fairness metrics more dynamically across λ values, while D=0 maintains more stable EOd.",
          "visual_anchor": "flat orange EOd line (~0.07) in top panel versus curved orange EOd line (0.05 to 0.13) in bottom panel",
          "text_evidence": "to first predict the sensitive feature based on the non-sensitive features, and then use the predicted value in the functional form of a model, allowing to treat people belonging to different groups, but having similar non-se",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does ACC peak at 91 near λ=2e-10 in the bottom panel when θ and ρ are fixed?",
        "answer": "The figure shows results when θ and ρ are fixed to best validation values while varying λ with P=0, F=1, S=0. The peak at λ=2e-10 represents the optimal balance for D=1 under these specific parameter constraints.",
        "visual_anchor": "blue line peak at approximately 91 ACC at λ=2e-10 in bottom panel (D=1)",
        "text_evidence": "we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$",
        "query_type": "value_context"
      },
      {
        "query": "Does the inverse relationship between blue ACC and orange EOd curves in the bottom panel support varying λ conclusions?",
        "answer": "Yes, as λ increases from 2e-10 to 1, ACC drops from ~91 to ~67 while EOd rises from ~0.05 to ~0.13, demonstrating the accuracy-fairness trade-off when varying λ as described in the analysis.",
        "visual_anchor": "opposing trends of blue ACC (declining) and orange EOd (rising) curves from left to right in bottom panel",
        "text_evidence": "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the top panel show stable EOd around 0.07 while bottom panel EOd varies from 0.05 to 0.13?",
        "answer": "The difference between D=0 (top) and D=1 (bottom) conditions creates distinct fairness behavior. D=1 allows the sensitive feature prediction to affect fairness metrics more dynamically across λ values, while D=0 maintains more stable EOd.",
        "visual_anchor": "flat orange EOd line (~0.07) in top panel versus curved orange EOd line (0.05 to 0.13) in bottom panel",
        "text_evidence": "to first predict the sensitive feature based on the non-sensitive features, and then use the predicted value in the functional form of a model, allowing to treat people belonging to different groups, but having similar non-se",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top panel: blue line (ACC) peaks around 85 at λ=1e-4, orange line (EOd) remains relatively flat around 0.07",
      "Bottom panel: blue line (ACC) peaks around 91 at λ=2e-10 then declines to ~67 at λ=1, orange line (EOd) rises from ~0.05 to ~0.13",
      "X-axis shows λ values from 0 to 1 with logarithmic spacing",
      "Two y-axes: left (blue) for ACC ranging 65-95, right (orange) for EOd ranging 0-0.2",
      "Top panel labeled D=0, bottom panel labeled D=1"
    ]
  },
  {
    "doc_id": "1811.00103",
    "figure_id": "1811.00103_fig_1",
    "figure_number": null,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg",
    "caption": "",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"purple line labeled Male RE PCA\", \"blue line labeled Female RE PCA\", \"x-axis showing Number of features from 1 to 20\", \"y-axis showing Reconstruction error from 10 to 35\", \"both curves declining from left to right\", \"initial gap between curves at feature count 1\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the purple Male curve starting at 35 reconstruction error illustrate higher error on population A versus B?\",\n      \"answer\": \"Yes, the purple Male curve consistently shows higher reconstruction error than the blue Female curve across all feature counts, demonstrating that PCA has higher reconstruction error on one population than the other.\",\n      \"visual_anchor\": \"purple line labeled Male RE PCA starting at reconstruction error 35\",\n      \"text_evidence\": \"We show on several real-world data sets, PCA has higher reconstruction error on population $A$ than on $B$ (for example, women versus men or lower- versus higher-educated individuals).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do both curves converge from their initial 3-point gap to nearly overlapping at 20 features?\",\n      \"answer\": \"As the number of features increases, PCA retains more information for both populations, reducing the reconstruction error disparity. The convergence shows that dimensionality reduction amplifies representation fidelity differences most severely at low feature counts.\",\n      \"visual_anchor\": \"convergence of purple and blue curves from ~3-point gap at 1 feature to near-overlap at 20 features\",\n      \"text_evidence\": \"We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the steeper decline from features 1 to 5 versus 10 to 20 motivate studying dimensionality reduction?\",\n      \"answer\": \"Yes, the rapid error decrease at low feature counts (35 to ~20) followed by gradual decline demonstrates how dimensionality reduction critically impacts reconstruction quality, motivating investigation into population-specific fidelity effects.\",\n      \"visual_anchor\": \"steep decline in both curves from features 1 to 5, followed by gradual decline from 10 to 20\",\n      \"text_evidence\": \"This motivates our study of dimensionality reduc\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "purple line labeled Male RE PCA",
        "blue line labeled Female RE PCA",
        "x-axis showing Number of features from 1 to 20",
        "y-axis showing Reconstruction error from 10 to 35",
        "both curves declining from left to right",
        "initial gap between curves at feature count 1"
      ],
      "queries": [
        {
          "query": "Does the purple Male curve starting at 35 reconstruction error illustrate higher error on population A versus B?",
          "answer": "Yes, the purple Male curve consistently shows higher reconstruction error than the blue Female curve across all feature counts, demonstrating that PCA has higher reconstruction error on one population than the other.",
          "visual_anchor": "purple line labeled Male RE PCA starting at reconstruction error 35",
          "text_evidence": "We show on several real-world data sets, PCA has higher reconstruction error on population $A$ than on $B$ (for example, women versus men or lower- versus higher-educated individuals).",
          "query_type": "value_context"
        },
        {
          "query": "Why do both curves converge from their initial 3-point gap to nearly overlapping at 20 features?",
          "answer": "As the number of features increases, PCA retains more information for both populations, reducing the reconstruction error disparity. The convergence shows that dimensionality reduction amplifies representation fidelity differences most severely at low feature counts.",
          "visual_anchor": "convergence of purple and blue curves from ~3-point gap at 1 feature to near-overlap at 20 features",
          "text_evidence": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the steeper decline from features 1 to 5 versus 10 to 20 motivate studying dimensionality reduction?",
          "answer": "Yes, the rapid error decrease at low feature counts (35 to ~20) followed by gradual decline demonstrates how dimensionality reduction critically impacts reconstruction quality, motivating investigation into population-specific fidelity effects.",
          "visual_anchor": "steep decline in both curves from features 1 to 5, followed by gradual decline from 10 to 20",
          "text_evidence": "This motivates our study of dimensionality reduc",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the purple Male curve starting at 35 reconstruction error illustrate higher error on population A versus B?",
        "answer": "Yes, the purple Male curve consistently shows higher reconstruction error than the blue Female curve across all feature counts, demonstrating that PCA has higher reconstruction error on one population than the other.",
        "visual_anchor": "purple line labeled Male RE PCA starting at reconstruction error 35",
        "text_evidence": "We show on several real-world data sets, PCA has higher reconstruction error on population $A$ than on $B$ (for example, women versus men or lower- versus higher-educated individuals).",
        "query_type": "value_context"
      },
      {
        "query": "Why do both curves converge from their initial 3-point gap to nearly overlapping at 20 features?",
        "answer": "As the number of features increases, PCA retains more information for both populations, reducing the reconstruction error disparity. The convergence shows that dimensionality reduction amplifies representation fidelity differences most severely at low feature counts.",
        "visual_anchor": "convergence of purple and blue curves from ~3-point gap at 1 feature to near-overlap at 20 features",
        "text_evidence": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the steeper decline from features 1 to 5 versus 10 to 20 motivate studying dimensionality reduction?",
        "answer": "Yes, the rapid error decrease at low feature counts (35 to ~20) followed by gradual decline demonstrates how dimensionality reduction critically impacts reconstruction quality, motivating investigation into population-specific fidelity effects.",
        "visual_anchor": "steep decline in both curves from features 1 to 5, followed by gradual decline from 10 to 20",
        "text_evidence": "This motivates our study of dimensionality reduc",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "purple line labeled Male RE PCA",
      "blue line labeled Female RE PCA",
      "x-axis showing Number of features from 1 to 20",
      "y-axis showing Reconstruction error from 10 to 35",
      "both curves declining from left to right",
      "initial gap between curves at feature count 1"
    ]
  },
  {
    "doc_id": "1811.00103",
    "figure_id": "1811.00103_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg",
    "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"purple line labeled Male RE PCA\", \"blue line labeled Female RE PCA\", \"y-axis showing Reconstruction error from 10 to 35\", \"x-axis showing Number of features from 0 to 20\", \"both curves starting around 32-36 and declining\", \"curves converging around 11-12 at 20 features\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue curve start higher at approximately 36 compared to the purple curve at lower dimensions?\",\n      \"answer\": \"At lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set, with women (blue) showing higher reconstruction error initially.\",\n      \"visual_anchor\": \"blue Female RE PCA curve starting at approximately 36 versus purple Male RE PCA curve starting at approximately 32\",\n      \"text_evidence\": \"As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the convergence of both curves to approximately 11 at 20 features indicate fairness in dimensionality reduction?\",\n      \"answer\": \"The convergence suggests reduced disparity at higher dimensions, but the work analyzes when dimensionality reduction introduces unfairness, indicating this is part of examining where unfairness enters the ML pipeline.\",\n      \"visual_anchor\": \"both purple and blue curves converging to approximately 11-12 reconstruction error at 20 features\",\n      \"text_evidence\": \"Our work takes another step in fleshing out this picture by analyzing when dimensi\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What sampling strategy would equalize the purple and blue reconstruction error curves shown in the left panel?\",\n      \"answer\": \"Sampling 1000 faces with men and women equiprobably (mean over 20 samples) would equalize the curves, as shown in the right panel of the figure.\",\n      \"visual_anchor\": \"purple Male RE PCA and blue Female RE PCA curves showing different trajectories\",\n      \"text_evidence\": \"Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "purple line labeled Male RE PCA",
        "blue line labeled Female RE PCA",
        "y-axis showing Reconstruction error from 10 to 35",
        "x-axis showing Number of features from 0 to 20",
        "both curves starting around 32-36 and declining",
        "curves converging around 11-12 at 20 features"
      ],
      "queries": [
        {
          "query": "Why does the blue curve start higher at approximately 36 compared to the purple curve at lower dimensions?",
          "answer": "At lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set, with women (blue) showing higher reconstruction error initially.",
          "visual_anchor": "blue Female RE PCA curve starting at approximately 36 versus purple Male RE PCA curve starting at approximately 32",
          "text_evidence": "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the convergence of both curves to approximately 11 at 20 features indicate fairness in dimensionality reduction?",
          "answer": "The convergence suggests reduced disparity at higher dimensions, but the work analyzes when dimensionality reduction introduces unfairness, indicating this is part of examining where unfairness enters the ML pipeline.",
          "visual_anchor": "both purple and blue curves converging to approximately 11-12 reconstruction error at 20 features",
          "text_evidence": "Our work takes another step in fleshing out this picture by analyzing when dimensi",
          "query_type": "value_context"
        },
        {
          "query": "What sampling strategy would equalize the purple and blue reconstruction error curves shown in the left panel?",
          "answer": "Sampling 1000 faces with men and women equiprobably (mean over 20 samples) would equalize the curves, as shown in the right panel of the figure.",
          "visual_anchor": "purple Male RE PCA and blue Female RE PCA curves showing different trajectories",
          "text_evidence": "Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue curve start higher at approximately 36 compared to the purple curve at lower dimensions?",
        "answer": "At lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set, with women (blue) showing higher reconstruction error initially.",
        "visual_anchor": "blue Female RE PCA curve starting at approximately 36 versus purple Male RE PCA curve starting at approximately 32",
        "text_evidence": "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA's average reconstruction error for men and women on the LFW data set.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the convergence of both curves to approximately 11 at 20 features indicate fairness in dimensionality reduction?",
        "answer": "The convergence suggests reduced disparity at higher dimensions, but the work analyzes when dimensionality reduction introduces unfairness, indicating this is part of examining where unfairness enters the ML pipeline.",
        "visual_anchor": "both purple and blue curves converging to approximately 11-12 reconstruction error at 20 features",
        "text_evidence": "Our work takes another step in fleshing out this picture by analyzing when dimensi",
        "query_type": "value_context"
      },
      {
        "query": "What sampling strategy would equalize the purple and blue reconstruction error curves shown in the left panel?",
        "answer": "Sampling 1000 faces with men and women equiprobably (mean over 20 samples) would equalize the curves, as shown in the right panel of the figure.",
        "visual_anchor": "purple Male RE PCA and blue Female RE PCA curves showing different trajectories",
        "text_evidence": "Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "purple line labeled Male RE PCA",
      "blue line labeled Female RE PCA",
      "y-axis showing Reconstruction error from 10 to 35",
      "x-axis showing Number of features from 0 to 20",
      "both curves starting around 32-36 and declining",
      "curves converging around 11-12 at 20 features"
    ]
  },
  {
    "doc_id": "1811.00103",
    "figure_id": "1811.00103_fig_6",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg",
    "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four lines: purple solid (Male RE PCA), blue solid (Female RE PCA), green dashed (Male RE Fair PCA), red triangular (Female RE Fair PCA)\",\n    \"X-axis: Number of features (0 to 20)\",\n    \"Y-axis: Reconstruction error (10 to 35)\",\n    \"All curves start high (~30-35 at low features) and converge to ~12 at 20 features\",\n    \"Red triangular line (Female RE Fair PCA) starts highest at ~35\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red triangular Female RE Fair PCA line start at approximately 35 when optimal fairness for dimension d requires at most d+k-1 dimensions?\",\n      \"answer\": \"The polynomial-time algorithm achieves optimal fairness objective value for dimension d using at most d+k-1 dimensions, but at very low feature counts (near 1), the reconstruction error is inherently high before Fair PCA's fairness constraints take full effect.\",\n      \"visual_anchor\": \"red triangular line starting at reconstruction error ~35 at lowest feature count\",\n      \"text_evidence\": \"There is a polynomial-time algorithm to find a projection such that it is of dimension at most $d + k - 1$ and achieves the optimal fairness objective value for dimension $d$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the convergence of all four lines to approximately 12 at 20 features support the claim that Fair PCA assigns equal loss across populations?\",\n      \"answer\": \"Yes, the convergence demonstrates that with sufficient features, Fair PCA achieves near-equal reconstruction error across Male and Female groups, consistent with optimal fairness solutions that minimize loss disparity between groups.\",\n      \"visual_anchor\": \"convergence of purple, blue, green, and red lines to ~12 at x=20\",\n      \"text_evidence\": \"Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do all curves show steeper descent between 2.5 and 7.5 features compared to the gradual decline after 10 features, given that k groups exist?\",\n      \"answer\": \"With more than two groups, optimal solutions may not assign identical loss initially, but as features increase beyond k groups, the fairness constraints stabilize and reconstruction error improvements plateau, explaining the transition from steep to gradual decline.\",\n      \"visual_anchor\": \"steep slope of all four lines between x=2.5 and x=7.5 versus shallow slope after x=10\",\n      \"text_evidence\": \"when there are more than two groups in the data, it is possible that all optimal solutions to fair PCA will not assign the same loss to all groups. However, with $k -\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four lines: purple solid (Male RE PCA), blue solid (Female RE PCA), green dashed (Male RE Fair PCA), red triangular (Female RE Fair PCA)",
        "X-axis: Number of features (0 to 20)",
        "Y-axis: Reconstruction error (10 to 35)",
        "All curves start high (~30-35 at low features) and converge to ~12 at 20 features",
        "Red triangular line (Female RE Fair PCA) starts highest at ~35"
      ],
      "queries": [
        {
          "query": "Why does the red triangular Female RE Fair PCA line start at approximately 35 when optimal fairness for dimension d requires at most d+k-1 dimensions?",
          "answer": "The polynomial-time algorithm achieves optimal fairness objective value for dimension d using at most d+k-1 dimensions, but at very low feature counts (near 1), the reconstruction error is inherently high before Fair PCA's fairness constraints take full effect.",
          "visual_anchor": "red triangular line starting at reconstruction error ~35 at lowest feature count",
          "text_evidence": "There is a polynomial-time algorithm to find a projection such that it is of dimension at most $d + k - 1$ and achieves the optimal fairness objective value for dimension $d$",
          "query_type": "value_context"
        },
        {
          "query": "Does the convergence of all four lines to approximately 12 at 20 features support the claim that Fair PCA assigns equal loss across populations?",
          "answer": "Yes, the convergence demonstrates that with sufficient features, Fair PCA achieves near-equal reconstruction error across Male and Female groups, consistent with optimal fairness solutions that minimize loss disparity between groups.",
          "visual_anchor": "convergence of purple, blue, green, and red lines to ~12 at x=20",
          "text_evidence": "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do all curves show steeper descent between 2.5 and 7.5 features compared to the gradual decline after 10 features, given that k groups exist?",
          "answer": "With more than two groups, optimal solutions may not assign identical loss initially, but as features increase beyond k groups, the fairness constraints stabilize and reconstruction error improvements plateau, explaining the transition from steep to gradual decline.",
          "visual_anchor": "steep slope of all four lines between x=2.5 and x=7.5 versus shallow slope after x=10",
          "text_evidence": "when there are more than two groups in the data, it is possible that all optimal solutions to fair PCA will not assign the same loss to all groups. However, with $k -",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red triangular Female RE Fair PCA line start at approximately 35 when optimal fairness for dimension d requires at most d+k-1 dimensions?",
        "answer": "The polynomial-time algorithm achieves optimal fairness objective value for dimension d using at most d+k-1 dimensions, but at very low feature counts (near 1), the reconstruction error is inherently high before Fair PCA's fairness constraints take full effect.",
        "visual_anchor": "red triangular line starting at reconstruction error ~35 at lowest feature count",
        "text_evidence": "There is a polynomial-time algorithm to find a projection such that it is of dimension at most $d + k - 1$ and achieves the optimal fairness objective value for dimension $d$",
        "query_type": "value_context"
      },
      {
        "query": "Does the convergence of all four lines to approximately 12 at 20 features support the claim that Fair PCA assigns equal loss across populations?",
        "answer": "Yes, the convergence demonstrates that with sufficient features, Fair PCA achieves near-equal reconstruction error across Male and Female groups, consistent with optimal fairness solutions that minimize loss disparity between groups.",
        "visual_anchor": "convergence of purple, blue, green, and red lines to ~12 at x=20",
        "text_evidence": "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do all curves show steeper descent between 2.5 and 7.5 features compared to the gradual decline after 10 features, given that k groups exist?",
        "answer": "With more than two groups, optimal solutions may not assign identical loss initially, but as features increase beyond k groups, the fairness constraints stabilize and reconstruction error improvements plateau, explaining the transition from steep to gradual decline.",
        "visual_anchor": "steep slope of all four lines between x=2.5 and x=7.5 versus shallow slope after x=10",
        "text_evidence": "when there are more than two groups in the data, it is possible that all optimal solutions to fair PCA will not assign the same loss to all groups. However, with $k -",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four lines: purple solid (Male RE PCA), blue solid (Female RE PCA), green dashed (Male RE Fair PCA), red triangular (Female RE Fair PCA)",
      "X-axis: Number of features (0 to 20)",
      "Y-axis: Reconstruction error (10 to 35)",
      "All curves start high (~30-35 at low features) and converge to ~12 at 20 features",
      "Red triangular line (Female RE Fair PCA) starts highest at ~35"
    ]
  },
  {
    "doc_id": "1811.00103",
    "figure_id": "1811.00103_fig_8",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg",
    "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"purple line labeled 'Male loss PCA'\", \"blue line labeled 'Female loss PCA'\", \"green line with dots labeled 'Fair loss'\", \"x-axis showing number of features from 0 to 20\", \"y-axis showing loss from 0 to 0.6\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue Female loss PCA curve reach 0.63 at 7 features while eigenvector basis is discussed for optimization?\",\n      \"answer\": \"The peak at 0.63 represents a local maximum in the PCA loss landscape. The eigenvector basis with eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ defines the global minimum structure that the optimization seeks.\",\n      \"visual_anchor\": \"blue line peak at approximately 0.63 loss when number of features equals 7\",\n      \"text_evidence\": \"Let {v₁, ..., vₙ} be an orthonormal basis of eigenvectors of AᵀA with corresponding eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the purple Male loss PCA staying flat near 0.06 across all features support proving local minima equal global minimum?\",\n      \"answer\": \"Yes, the consistently low and flat male loss suggests convergence to a stable minimum. This aligns with proving that local minima values equal the global minimum value at the top d eigenvector subspace.\",\n      \"visual_anchor\": \"purple line remaining approximately constant at 0.06 loss from features 1 to 20\",\n      \"text_evidence\": \"We prove that the value of function gₐ at its local minima is equal to its value at its global minimum, which we know is the subspace spanned by a top d eigenvectors of AᵀA\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes the green Fair loss curve to stabilize around 0.22 after 10 features when analyzing PCA loss on LFW?\",\n      \"answer\": \"The stabilization indicates that adding more features beyond 10 does not significantly reduce the fair loss, suggesting the optimal subspace dimensionality has been reached for balancing fairness and reconstruction on the LFW dataset.\",\n      \"visual_anchor\": \"green line with dots plateauing at approximately 0.22 loss from 10 to 20 features\",\n      \"text_evidence\": \"Loss of PCA/Fair PCA on LFW and the Default Credit data set\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "purple line labeled 'Male loss PCA'",
        "blue line labeled 'Female loss PCA'",
        "green line with dots labeled 'Fair loss'",
        "x-axis showing number of features from 0 to 20",
        "y-axis showing loss from 0 to 0.6"
      ],
      "queries": [
        {
          "query": "Why does the blue Female loss PCA curve reach 0.63 at 7 features while eigenvector basis is discussed for optimization?",
          "answer": "The peak at 0.63 represents a local maximum in the PCA loss landscape. The eigenvector basis with eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ defines the global minimum structure that the optimization seeks.",
          "visual_anchor": "blue line peak at approximately 0.63 loss when number of features equals 7",
          "text_evidence": "Let {v₁, ..., vₙ} be an orthonormal basis of eigenvectors of AᵀA with corresponding eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the purple Male loss PCA staying flat near 0.06 across all features support proving local minima equal global minimum?",
          "answer": "Yes, the consistently low and flat male loss suggests convergence to a stable minimum. This aligns with proving that local minima values equal the global minimum value at the top d eigenvector subspace.",
          "visual_anchor": "purple line remaining approximately constant at 0.06 loss from features 1 to 20",
          "text_evidence": "We prove that the value of function gₐ at its local minima is equal to its value at its global minimum, which we know is the subspace spanned by a top d eigenvectors of AᵀA",
          "query_type": "value_context"
        },
        {
          "query": "What causes the green Fair loss curve to stabilize around 0.22 after 10 features when analyzing PCA loss on LFW?",
          "answer": "The stabilization indicates that adding more features beyond 10 does not significantly reduce the fair loss, suggesting the optimal subspace dimensionality has been reached for balancing fairness and reconstruction on the LFW dataset.",
          "visual_anchor": "green line with dots plateauing at approximately 0.22 loss from 10 to 20 features",
          "text_evidence": "Loss of PCA/Fair PCA on LFW and the Default Credit data set",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue Female loss PCA curve reach 0.63 at 7 features while eigenvector basis is discussed for optimization?",
        "answer": "The peak at 0.63 represents a local maximum in the PCA loss landscape. The eigenvector basis with eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ defines the global minimum structure that the optimization seeks.",
        "visual_anchor": "blue line peak at approximately 0.63 loss when number of features equals 7",
        "text_evidence": "Let {v₁, ..., vₙ} be an orthonormal basis of eigenvectors of AᵀA with corresponding eigenvalues λ₁ ≥ λ₂ ≥ ... ≥ λₙ",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the purple Male loss PCA staying flat near 0.06 across all features support proving local minima equal global minimum?",
        "answer": "Yes, the consistently low and flat male loss suggests convergence to a stable minimum. This aligns with proving that local minima values equal the global minimum value at the top d eigenvector subspace.",
        "visual_anchor": "purple line remaining approximately constant at 0.06 loss from features 1 to 20",
        "text_evidence": "We prove that the value of function gₐ at its local minima is equal to its value at its global minimum, which we know is the subspace spanned by a top d eigenvectors of AᵀA",
        "query_type": "value_context"
      },
      {
        "query": "What causes the green Fair loss curve to stabilize around 0.22 after 10 features when analyzing PCA loss on LFW?",
        "answer": "The stabilization indicates that adding more features beyond 10 does not significantly reduce the fair loss, suggesting the optimal subspace dimensionality has been reached for balancing fairness and reconstruction on the LFW dataset.",
        "visual_anchor": "green line with dots plateauing at approximately 0.22 loss from 10 to 20 features",
        "text_evidence": "Loss of PCA/Fair PCA on LFW and the Default Credit data set",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "purple line labeled 'Male loss PCA'",
      "blue line labeled 'Female loss PCA'",
      "green line with dots labeled 'Fair loss'",
      "x-axis showing number of features from 0 to 20",
      "y-axis showing loss from 0 to 0.6"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg",
    "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four panels showing Treatment 1-4 with three conditions each: All A, Equal, Ratio\",\n    \"Vertical error bars representing 95% confidence intervals\",\n    \"Mean values labeled on each data point (e.g., 5.09, 6.34, 7.03)\",\n    \"Significance brackets with asterisks (*, **, ***) above conditions\",\n    \"Y-axis labeled 'Participants' fairness rating' ranging from 3 to 8\",\n    \"Pink/red colored data points and error bars\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the Ratio condition reach 7.03 in Treatment 1, supporting the hypothesis about proportional loan repayment rate allocation?\",\n      \"answer\": \"The Ratio condition's high rating of 7.03 demonstrates that participants consistently rated dividing the $50,000 in proportion to loan repayment rates as more fair than equal splitting, providing evidence for hypothesis H1A.\",\n      \"visual_anchor\": \"Mean value 7.03 labeled at Ratio condition in Treatment 1 panel\",\n      \"text_evidence\": \"We found evidence in support of H1A in all treatments: participants consistently rated dividing the $50,000 between the two individuals in proportion of their loan repayment rates (the 'Ratio' decision) as more fair than splitting the $50,000 equally (the 'Equal' decision)\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the consistent Ratio superiority marked with asterisks across all four treatments relate to task-specific feature influence?\",\n      \"answer\": \"The statistical significance markers above Ratio conditions in all treatments demonstrate that information on the candidates' loan repayment rates systematically influenced fairness perceptions, supporting the study's motivation to investigate task-specific feature effects.\",\n      \"visual_anchor\": \"Significance brackets with asterisks (**, ***) connecting Ratio to other conditions across all four treatment panels\",\n      \"text_evidence\": \"In this study, our motivation is to investigate how information on an individual task-specific feature (i.e., the candidates' loan repayment rate) influences perceptions of fairness\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the three-level progression from 5.09 to 6.34 to 7.03 in Treatment 1 given the allocation rules tested?\",\n      \"answer\": \"The ascending fairness ratings from All A (5.09) through Equal (6.34) to Ratio (7.03) reflect participants' preferences across the three allocation rules formulated to test qualitative judgments about loan distribution fairness.\",\n      \"visual_anchor\": \"Sequential mean values 5.09, 6.34, and 7.03 in Treatment 1 panel from left to right\",\n      \"text_evidence\": \"We choose three allocation rules, described in the following paragraphs, that allow us to formulate qualitative judgments\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four panels showing Treatment 1-4 with three conditions each: All A, Equal, Ratio",
        "Vertical error bars representing 95% confidence intervals",
        "Mean values labeled on each data point (e.g., 5.09, 6.34, 7.03)",
        "Significance brackets with asterisks (*, **, ***) above conditions",
        "Y-axis labeled 'Participants' fairness rating' ranging from 3 to 8",
        "Pink/red colored data points and error bars"
      ],
      "queries": [
        {
          "query": "Why does the Ratio condition reach 7.03 in Treatment 1, supporting the hypothesis about proportional loan repayment rate allocation?",
          "answer": "The Ratio condition's high rating of 7.03 demonstrates that participants consistently rated dividing the $50,000 in proportion to loan repayment rates as more fair than equal splitting, providing evidence for hypothesis H1A.",
          "visual_anchor": "Mean value 7.03 labeled at Ratio condition in Treatment 1 panel",
          "text_evidence": "We found evidence in support of H1A in all treatments: participants consistently rated dividing the $50,000 between the two individuals in proportion of their loan repayment rates (the 'Ratio' decision) as more fair than splitting the $50,000 equally (the 'Equal' decision)",
          "query_type": "value_context"
        },
        {
          "query": "How does the consistent Ratio superiority marked with asterisks across all four treatments relate to task-specific feature influence?",
          "answer": "The statistical significance markers above Ratio conditions in all treatments demonstrate that information on the candidates' loan repayment rates systematically influenced fairness perceptions, supporting the study's motivation to investigate task-specific feature effects.",
          "visual_anchor": "Significance brackets with asterisks (**, ***) connecting Ratio to other conditions across all four treatment panels",
          "text_evidence": "In this study, our motivation is to investigate how information on an individual task-specific feature (i.e., the candidates' loan repayment rate) influences perceptions of fairness",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the three-level progression from 5.09 to 6.34 to 7.03 in Treatment 1 given the allocation rules tested?",
          "answer": "The ascending fairness ratings from All A (5.09) through Equal (6.34) to Ratio (7.03) reflect participants' preferences across the three allocation rules formulated to test qualitative judgments about loan distribution fairness.",
          "visual_anchor": "Sequential mean values 5.09, 6.34, and 7.03 in Treatment 1 panel from left to right",
          "text_evidence": "We choose three allocation rules, described in the following paragraphs, that allow us to formulate qualitative judgments",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the Ratio condition reach 7.03 in Treatment 1, supporting the hypothesis about proportional loan repayment rate allocation?",
        "answer": "The Ratio condition's high rating of 7.03 demonstrates that participants consistently rated dividing the $50,000 in proportion to loan repayment rates as more fair than equal splitting, providing evidence for hypothesis H1A.",
        "visual_anchor": "Mean value 7.03 labeled at Ratio condition in Treatment 1 panel",
        "text_evidence": "We found evidence in support of H1A in all treatments: participants consistently rated dividing the $50,000 between the two individuals in proportion of their loan repayment rates (the 'Ratio' decision) as more fair than splitting the $50,000 equally (the 'Equal' decision)",
        "query_type": "value_context"
      },
      {
        "query": "How does the consistent Ratio superiority marked with asterisks across all four treatments relate to task-specific feature influence?",
        "answer": "The statistical significance markers above Ratio conditions in all treatments demonstrate that information on the candidates' loan repayment rates systematically influenced fairness perceptions, supporting the study's motivation to investigate task-specific feature effects.",
        "visual_anchor": "Significance brackets with asterisks (**, ***) connecting Ratio to other conditions across all four treatment panels",
        "text_evidence": "In this study, our motivation is to investigate how information on an individual task-specific feature (i.e., the candidates' loan repayment rate) influences perceptions of fairness",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the three-level progression from 5.09 to 6.34 to 7.03 in Treatment 1 given the allocation rules tested?",
        "answer": "The ascending fairness ratings from All A (5.09) through Equal (6.34) to Ratio (7.03) reflect participants' preferences across the three allocation rules formulated to test qualitative judgments about loan distribution fairness.",
        "visual_anchor": "Sequential mean values 5.09, 6.34, and 7.03 in Treatment 1 panel from left to right",
        "text_evidence": "We choose three allocation rules, described in the following paragraphs, that allow us to formulate qualitative judgments",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four panels showing Treatment 1-4 with three conditions each: All A, Equal, Ratio",
      "Vertical error bars representing 95% confidence intervals",
      "Mean values labeled on each data point (e.g., 5.09, 6.34, 7.03)",
      "Significance brackets with asterisks (*, **, ***) above conditions",
      "Y-axis labeled 'Participants' fairness rating' ranging from 3 to 8",
      "Pink/red colored data points and error bars"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg",
    "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four subplots showing Treatment 1-4 with fairness ratings on y-axis\",\n    \"Red/pink error bars with confidence intervals\",\n    \"Three conditions per treatment: All A, Equal, Ratio\",\n    \"Statistical significance markers (*, **, ***)\",\n    \"Specific values labeled: 3.26, 6.74, 6.73 in Treatment 1; 4.00, 5.85, 6.36 in Treatment 2; 4.86, 5.28, 5.98 in Treatment 3; 4.86, 5.28, 5.97 in Treatment 4\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Treatment 2's Equal condition rating of 5.85 exceed All A's 4.00 when the higher-repayment candidate is white?\",\n      \"answer\": \"Participants rated the \\\"Equal\\\" decision as more fair than the \\\"All A\\\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white, as shown by the statistical significance markers.\",\n      \"visual_anchor\": \"Treatment 2 subplot showing Equal at 5.85 and All A at 4.00 with *** and * significance markers\",\n      \"text_evidence\": \"Participants also rated the \\\"Equal\\\" decision as more fair than the \\\"All A\\\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the Ratio condition's highest rating of 6.74 in Treatment 1 support fairness preference when sensitive information includes loan repayment rates?\",\n      \"answer\": \"Yes, participants consistently gave most support to the decision to divide the money between individuals in proportion to their repayment rates, viewing the \\\"Ratio\\\" decision as more fair than both \\\"Equal\\\" and \\\"All A\\\" decisions across all treatments.\",\n      \"visual_anchor\": \"Treatment 1 Ratio value of 6.74 with *** significance compared to Equal and All A conditions\",\n      \"text_evidence\": \"participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does Ratio's 5.98 rating in Treatment 3 compare to Equal's 5.28 given the experimental paradigm presenting two loan applicants?\",\n      \"answer\": \"The Ratio decision (5.98) was viewed as significantly more fair than the Equal decision (5.28) in Treatment 3, consistent with participants viewing proportional allocation based on task-specific features as fairer across all treatments.\",\n      \"visual_anchor\": \"Treatment 3 subplot showing Ratio at 5.98 and Equal at 5.28 with * significance marker\",\n      \"text_evidence\": \"We employed the same experimental paradigm as in Study 1, presenting participants with the scenario of two individuals applying for a loan, and three possible ways of allocating the loan money\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four subplots showing Treatment 1-4 with fairness ratings on y-axis",
        "Red/pink error bars with confidence intervals",
        "Three conditions per treatment: All A, Equal, Ratio",
        "Statistical significance markers (*, **, ***)",
        "Specific values labeled: 3.26, 6.74, 6.73 in Treatment 1; 4.00, 5.85, 6.36 in Treatment 2; 4.86, 5.28, 5.98 in Treatment 3; 4.86, 5.28, 5.97 in Treatment 4"
      ],
      "queries": [
        {
          "query": "Why does Treatment 2's Equal condition rating of 5.85 exceed All A's 4.00 when the higher-repayment candidate is white?",
          "answer": "Participants rated the \"Equal\" decision as more fair than the \"All A\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white, as shown by the statistical significance markers.",
          "visual_anchor": "Treatment 2 subplot showing Equal at 5.85 and All A at 4.00 with *** and * significance markers",
          "text_evidence": "Participants also rated the \"Equal\" decision as more fair than the \"All A\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the Ratio condition's highest rating of 6.74 in Treatment 1 support fairness preference when sensitive information includes loan repayment rates?",
          "answer": "Yes, participants consistently gave most support to the decision to divide the money between individuals in proportion to their repayment rates, viewing the \"Ratio\" decision as more fair than both \"Equal\" and \"All A\" decisions across all treatments.",
          "visual_anchor": "Treatment 1 Ratio value of 6.74 with *** significance compared to Equal and All A conditions",
          "text_evidence": "participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop",
          "query_type": "value_context"
        },
        {
          "query": "How does Ratio's 5.98 rating in Treatment 3 compare to Equal's 5.28 given the experimental paradigm presenting two loan applicants?",
          "answer": "The Ratio decision (5.98) was viewed as significantly more fair than the Equal decision (5.28) in Treatment 3, consistent with participants viewing proportional allocation based on task-specific features as fairer across all treatments.",
          "visual_anchor": "Treatment 3 subplot showing Ratio at 5.98 and Equal at 5.28 with * significance marker",
          "text_evidence": "We employed the same experimental paradigm as in Study 1, presenting participants with the scenario of two individuals applying for a loan, and three possible ways of allocating the loan money",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Treatment 2's Equal condition rating of 5.85 exceed All A's 4.00 when the higher-repayment candidate is white?",
        "answer": "Participants rated the \"Equal\" decision as more fair than the \"All A\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white, as shown by the statistical significance markers.",
        "visual_anchor": "Treatment 2 subplot showing Equal at 5.85 and All A at 4.00 with *** and * significance markers",
        "text_evidence": "Participants also rated the \"Equal\" decision as more fair than the \"All A\" decision in Treatment 2, but only when the candidate with the higher repayment rate was white",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the Ratio condition's highest rating of 6.74 in Treatment 1 support fairness preference when sensitive information includes loan repayment rates?",
        "answer": "Yes, participants consistently gave most support to the decision to divide the money between individuals in proportion to their repayment rates, viewing the \"Ratio\" decision as more fair than both \"Equal\" and \"All A\" decisions across all treatments.",
        "visual_anchor": "Treatment 1 Ratio value of 6.74 with *** significance compared to Equal and All A conditions",
        "text_evidence": "participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop",
        "query_type": "value_context"
      },
      {
        "query": "How does Ratio's 5.98 rating in Treatment 3 compare to Equal's 5.28 given the experimental paradigm presenting two loan applicants?",
        "answer": "The Ratio decision (5.98) was viewed as significantly more fair than the Equal decision (5.28) in Treatment 3, consistent with participants viewing proportional allocation based on task-specific features as fairer across all treatments.",
        "visual_anchor": "Treatment 3 subplot showing Ratio at 5.98 and Equal at 5.28 with * significance marker",
        "text_evidence": "We employed the same experimental paradigm as in Study 1, presenting participants with the scenario of two individuals applying for a loan, and three possible ways of allocating the loan money",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four subplots showing Treatment 1-4 with fairness ratings on y-axis",
      "Red/pink error bars with confidence intervals",
      "Three conditions per treatment: All A, Equal, Ratio",
      "Statistical significance markers (*, **, ***)",
      "Specific values labeled: 3.26, 6.74, 6.73 in Treatment 1; 4.00, 5.85, 6.36 in Treatment 2; 4.86, 5.28, 5.98 in Treatment 3; 4.86, 5.28, 5.97 in Treatment 4"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg",
    "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four panels showing Treatment 1-4 comparisons\",\n    \"Three conditions per treatment: All A, Equal, Ratio\",\n    \"Error bars with 95% CI\",\n    \"Significance markers: *, **, ***\",\n    \"Y-axis: Participants' fairness rating (3-8 scale)\",\n    \"Pink/red data points with numerical labels\",\n    \"Treatment 3 shows values 5.55, 4.83, 6.09\",\n    \"Treatment 4 shows values 5.44, 4.68, 5.98\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the Ratio condition's 6.00 rating in Treatment 2 support that participants consistently favored proportional division?\",\n      \"answer\": \"Yes, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in proportion (Ratio condition), as shown by the highest ratings across all four treatments.\",\n      \"visual_anchor\": \"Ratio condition value of 6.00 with error bars in Treatment 2 upper-right panel\",\n      \"text_evidence\": \"Thus, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why is the All A rating 5.55 significantly higher than Equal's 4.83 in Treatment 3 when both lack proportional allocation?\",\n      \"answer\": \"When the difference between repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision specifically when the candidate with the higher repayment rate was black, reflecting race-influenced fairness perceptions under larger performance disparities.\",\n      \"visual_anchor\": \"Treatment 3 lower-left panel showing All A at 5.55 versus Equal at 4.83 with significant difference markers\",\n      \"text_evidence\": \"When the difference between the two candidates' repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision but only when the candidate with the higher repayment rate was black\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the Ratio values 6.09 and 5.98 in Treatments 3-4 remain highest despite race attributes?\",\n      \"answer\": \"Yes, participants perceived the 'Ratio' decision to be more fair than the other two options regardless of race attribute. These results are not dependent on the race attribute, showing consistent preference for proportional fairness.\",\n      \"visual_anchor\": \"Ratio condition showing highest values (6.09 in Treatment 3, 5.98 in Treatment 4) compared to All A and Equal\",\n      \"text_evidence\": \"Our results show that participants perceived the 'Ratio' decision to be more fair than the other two, hence supporting the results from Study 1 and the related discussion. These results are not dependent on the race attribute.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four panels showing Treatment 1-4 comparisons",
        "Three conditions per treatment: All A, Equal, Ratio",
        "Error bars with 95% CI",
        "Significance markers: *, **, ***",
        "Y-axis: Participants' fairness rating (3-8 scale)",
        "Pink/red data points with numerical labels",
        "Treatment 3 shows values 5.55, 4.83, 6.09",
        "Treatment 4 shows values 5.44, 4.68, 5.98"
      ],
      "queries": [
        {
          "query": "Does the Ratio condition's 6.00 rating in Treatment 2 support that participants consistently favored proportional division?",
          "answer": "Yes, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in proportion (Ratio condition), as shown by the highest ratings across all four treatments.",
          "visual_anchor": "Ratio condition value of 6.00 with error bars in Treatment 2 upper-right panel",
          "text_evidence": "Thus, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop",
          "query_type": "value_context"
        },
        {
          "query": "Why is the All A rating 5.55 significantly higher than Equal's 4.83 in Treatment 3 when both lack proportional allocation?",
          "answer": "When the difference between repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision specifically when the candidate with the higher repayment rate was black, reflecting race-influenced fairness perceptions under larger performance disparities.",
          "visual_anchor": "Treatment 3 lower-left panel showing All A at 5.55 versus Equal at 4.83 with significant difference markers",
          "text_evidence": "When the difference between the two candidates' repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision but only when the candidate with the higher repayment rate was black",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the Ratio values 6.09 and 5.98 in Treatments 3-4 remain highest despite race attributes?",
          "answer": "Yes, participants perceived the 'Ratio' decision to be more fair than the other two options regardless of race attribute. These results are not dependent on the race attribute, showing consistent preference for proportional fairness.",
          "visual_anchor": "Ratio condition showing highest values (6.09 in Treatment 3, 5.98 in Treatment 4) compared to All A and Equal",
          "text_evidence": "Our results show that participants perceived the 'Ratio' decision to be more fair than the other two, hence supporting the results from Study 1 and the related discussion. These results are not dependent on the race attribute.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the Ratio condition's 6.00 rating in Treatment 2 support that participants consistently favored proportional division?",
        "answer": "Yes, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in proportion (Ratio condition), as shown by the highest ratings across all four treatments.",
        "visual_anchor": "Ratio condition value of 6.00 with error bars in Treatment 2 upper-right panel",
        "text_evidence": "Thus, participants in Study 2 consistently gave most support to the decision to divide the $50,000 between the two individuals in prop",
        "query_type": "value_context"
      },
      {
        "query": "Why is the All A rating 5.55 significantly higher than Equal's 4.83 in Treatment 3 when both lack proportional allocation?",
        "answer": "When the difference between repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision specifically when the candidate with the higher repayment rate was black, reflecting race-influenced fairness perceptions under larger performance disparities.",
        "visual_anchor": "Treatment 3 lower-left panel showing All A at 5.55 versus Equal at 4.83 with significant difference markers",
        "text_evidence": "When the difference between the two candidates' repayment rates was larger (Treatments 3 and 4), participants viewed the 'All A' decision as more fair than the 'Equal' decision but only when the candidate with the higher repayment rate was black",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the Ratio values 6.09 and 5.98 in Treatments 3-4 remain highest despite race attributes?",
        "answer": "Yes, participants perceived the 'Ratio' decision to be more fair than the other two options regardless of race attribute. These results are not dependent on the race attribute, showing consistent preference for proportional fairness.",
        "visual_anchor": "Ratio condition showing highest values (6.09 in Treatment 3, 5.98 in Treatment 4) compared to All A and Equal",
        "text_evidence": "Our results show that participants perceived the 'Ratio' decision to be more fair than the other two, hence supporting the results from Study 1 and the related discussion. These results are not dependent on the race attribute.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four panels showing Treatment 1-4 comparisons",
      "Three conditions per treatment: All A, Equal, Ratio",
      "Error bars with 95% CI",
      "Significance markers: *, **, ***",
      "Y-axis: Participants' fairness rating (3-8 scale)",
      "Pink/red data points with numerical labels",
      "Treatment 3 shows values 5.55, 4.83, 6.09",
      "Treatment 4 shows values 5.44, 4.68, 5.98"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_4",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg",
    "caption": "Figure 6: Age distribution of the participants in Study 1.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Pie chart with age ranges\",\n    \"30-39 segment at 30% (largest, cyan)\",\n    \"20-29 segment at 28% (cyan)\",\n    \"40-49 segment at 21% (blue)\",\n    \"50-59 segment at 13% (darker blue)\",\n    \"60-69 segment at 7% (dark blue)\",\n    \"70 and above segment at 2% (darkest blue)\",\n    \"Under 20 segment at 1% (smallest)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the 30% largest cyan segment contradict the eligibility requirement that workers be located in the United States?\",\n      \"answer\": \"No, the 30-39 age group comprising 30% does not contradict the U.S. location requirement, as age distribution is independent of geographic eligibility criteria.\",\n      \"visual_anchor\": \"30-39 segment at 30% (largest cyan portion)\",\n      \"text_evidence\": \"To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the 28% in the 20-29 cyan segment nearly match the 30% segment despite participation restrictions?\",\n      \"answer\": \"The similar percentages (28% vs 30%) reflect natural age distribution among MTurk workers, not participation restrictions, which only prevented workers from taking both studies.\",\n      \"visual_anchor\": \"20-29 segment at 28% compared to 30-39 at 30%\",\n      \"text_evidence\": \"Amazon Mechanical Turk workers ('MTurker') could only participate in on one of the two studies, and not both.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the 1% Under 20 dark blue segment indicate incomplete demographic data collection in the first section?\",\n      \"answer\": \"No, the 1% Under 20 representation does not indicate incomplete data collection; it simply reflects the actual age distribution of participants who completed the demographic questions.\",\n      \"visual_anchor\": \"Under 20 segment at 1% (smallest dark blue slice)\",\n      \"text_evidence\": \"The first section contains all questions the workers were asked in the studies. The second section contains the demographics dat\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Pie chart with age ranges",
        "30-39 segment at 30% (largest, cyan)",
        "20-29 segment at 28% (cyan)",
        "40-49 segment at 21% (blue)",
        "50-59 segment at 13% (darker blue)",
        "60-69 segment at 7% (dark blue)",
        "70 and above segment at 2% (darkest blue)",
        "Under 20 segment at 1% (smallest)"
      ],
      "queries": [
        {
          "query": "Does the 30% largest cyan segment contradict the eligibility requirement that workers be located in the United States?",
          "answer": "No, the 30-39 age group comprising 30% does not contradict the U.S. location requirement, as age distribution is independent of geographic eligibility criteria.",
          "visual_anchor": "30-39 segment at 30% (largest cyan portion)",
          "text_evidence": "To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the 28% in the 20-29 cyan segment nearly match the 30% segment despite participation restrictions?",
          "answer": "The similar percentages (28% vs 30%) reflect natural age distribution among MTurk workers, not participation restrictions, which only prevented workers from taking both studies.",
          "visual_anchor": "20-29 segment at 28% compared to 30-39 at 30%",
          "text_evidence": "Amazon Mechanical Turk workers ('MTurker') could only participate in on one of the two studies, and not both.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the 1% Under 20 dark blue segment indicate incomplete demographic data collection in the first section?",
          "answer": "No, the 1% Under 20 representation does not indicate incomplete data collection; it simply reflects the actual age distribution of participants who completed the demographic questions.",
          "visual_anchor": "Under 20 segment at 1% (smallest dark blue slice)",
          "text_evidence": "The first section contains all questions the workers were asked in the studies. The second section contains the demographics dat",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 30% largest cyan segment contradict the eligibility requirement that workers be located in the United States?",
        "answer": "No, the 30-39 age group comprising 30% does not contradict the U.S. location requirement, as age distribution is independent of geographic eligibility criteria.",
        "visual_anchor": "30-39 segment at 30% (largest cyan portion)",
        "text_evidence": "To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the 28% in the 20-29 cyan segment nearly match the 30% segment despite participation restrictions?",
        "answer": "The similar percentages (28% vs 30%) reflect natural age distribution among MTurk workers, not participation restrictions, which only prevented workers from taking both studies.",
        "visual_anchor": "20-29 segment at 28% compared to 30-39 at 30%",
        "text_evidence": "Amazon Mechanical Turk workers ('MTurker') could only participate in on one of the two studies, and not both.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the 1% Under 20 dark blue segment indicate incomplete demographic data collection in the first section?",
        "answer": "No, the 1% Under 20 representation does not indicate incomplete data collection; it simply reflects the actual age distribution of participants who completed the demographic questions.",
        "visual_anchor": "Under 20 segment at 1% (smallest dark blue slice)",
        "text_evidence": "The first section contains all questions the workers were asked in the studies. The second section contains the demographics dat",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Pie chart with age ranges",
      "30-39 segment at 30% (largest, cyan)",
      "20-29 segment at 28% (cyan)",
      "40-49 segment at 21% (blue)",
      "50-59 segment at 13% (darker blue)",
      "60-69 segment at 7% (dark blue)",
      "70 and above segment at 2% (darkest blue)",
      "Under 20 segment at 1% (smallest)"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_5",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig4.jpg",
    "caption": "Figure 7: Education of the participants in Study 1.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Bachelor's deg. 30% (largest segment in light blue)\", \"Some college 27% (turquoise segment)\", \"Graduate degree 20% (dark blue segment)\", \"Associate's deg. 14% (cyan segment)\", \"High school/GED 9% (small dark blue segment)\", \"No high scl. deg 1% (smallest segment)\"],\n  \"queries\": [\n    {\n      \"query\": \"What percentage of Study 1 participants hold the Bachelor's degree shown as the largest 30% light blue segment?\",\n      \"answer\": \"30% of Study 1 participants hold a Bachelor's degree, representing the largest educational attainment group in the study.\",\n      \"visual_anchor\": \"Bachelor's deg. 30% (largest light blue segment)\",\n      \"text_evidence\": \"Study 1: Demographic information of the participants\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the 27% turquoise Some college segment compare to the 14% cyan Associate's degree segment among participants?\",\n      \"answer\": \"The Some college segment at 27% is nearly double the Associate's degree segment at 14%, indicating more participants attended college without completing a four-year degree than those who earned an Associate's degree.\",\n      \"visual_anchor\": \"Some college 27% (turquoise) vs Associate's deg. 14% (cyan)\",\n      \"text_evidence\": \"Figure 6: Age distribution of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the 20% dark blue Graduate degree segment rank third in size for education levels documented?\",\n      \"answer\": \"The Graduate degree segment at 20% ranks third because Bachelor's degree (30%) and Some college (27%) have higher representation, reflecting that fewer participants pursued advanced degrees beyond undergraduate education.\",\n      \"visual_anchor\": \"Graduate degree 20% (dark blue segment, third largest)\",\n      \"text_evidence\": \"Figure 7: Education of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bachelor's deg. 30% (largest segment in light blue)",
        "Some college 27% (turquoise segment)",
        "Graduate degree 20% (dark blue segment)",
        "Associate's deg. 14% (cyan segment)",
        "High school/GED 9% (small dark blue segment)",
        "No high scl. deg 1% (smallest segment)"
      ],
      "queries": [
        {
          "query": "What percentage of Study 1 participants hold the Bachelor's degree shown as the largest 30% light blue segment?",
          "answer": "30% of Study 1 participants hold a Bachelor's degree, representing the largest educational attainment group in the study.",
          "visual_anchor": "Bachelor's deg. 30% (largest light blue segment)",
          "text_evidence": "Study 1: Demographic information of the participants",
          "query_type": "value_context"
        },
        {
          "query": "How does the 27% turquoise Some college segment compare to the 14% cyan Associate's degree segment among participants?",
          "answer": "The Some college segment at 27% is nearly double the Associate's degree segment at 14%, indicating more participants attended college without completing a four-year degree than those who earned an Associate's degree.",
          "visual_anchor": "Some college 27% (turquoise) vs Associate's deg. 14% (cyan)",
          "text_evidence": "Figure 6: Age distribution of the participants in Study 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the 20% dark blue Graduate degree segment rank third in size for education levels documented?",
          "answer": "The Graduate degree segment at 20% ranks third because Bachelor's degree (30%) and Some college (27%) have higher representation, reflecting that fewer participants pursued advanced degrees beyond undergraduate education.",
          "visual_anchor": "Graduate degree 20% (dark blue segment, third largest)",
          "text_evidence": "Figure 7: Education of the participants in Study 1.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "What percentage of Study 1 participants hold the Bachelor's degree shown as the largest 30% light blue segment?",
        "answer": "30% of Study 1 participants hold a Bachelor's degree, representing the largest educational attainment group in the study.",
        "visual_anchor": "Bachelor's deg. 30% (largest light blue segment)",
        "text_evidence": "Study 1: Demographic information of the participants",
        "query_type": "value_context"
      },
      {
        "query": "How does the 27% turquoise Some college segment compare to the 14% cyan Associate's degree segment among participants?",
        "answer": "The Some college segment at 27% is nearly double the Associate's degree segment at 14%, indicating more participants attended college without completing a four-year degree than those who earned an Associate's degree.",
        "visual_anchor": "Some college 27% (turquoise) vs Associate's deg. 14% (cyan)",
        "text_evidence": "Figure 6: Age distribution of the participants in Study 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the 20% dark blue Graduate degree segment rank third in size for education levels documented?",
        "answer": "The Graduate degree segment at 20% ranks third because Bachelor's degree (30%) and Some college (27%) have higher representation, reflecting that fewer participants pursued advanced degrees beyond undergraduate education.",
        "visual_anchor": "Graduate degree 20% (dark blue segment, third largest)",
        "text_evidence": "Figure 7: Education of the participants in Study 1.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bachelor's deg. 30% (largest segment in light blue)",
      "Some college 27% (turquoise segment)",
      "Graduate degree 20% (dark blue segment)",
      "Associate's deg. 14% (cyan segment)",
      "High school/GED 9% (small dark blue segment)",
      "No high scl. deg 1% (smallest segment)"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_6",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg",
    "caption": "Figure 8: Gender breakdown of the participants in Study 1.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Turquoise/cyan segment labeled Female 56%\", \"Dark blue segment labeled Male 44%\", \"Pie chart showing two segments\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the turquoise segment representing 56% Female align with the stated gender breakdown for Study 1 participants?\",\n      \"answer\": \"Yes, the turquoise segment showing 56% Female directly corresponds to the gender breakdown of participants in Study 1 as stated.\",\n      \"visual_anchor\": \"Turquoise segment labeled Female 56%\",\n      \"text_evidence\": \"Figure 8: Gender breakdown of the participants in Study 1.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the dark blue Male segment occupy 44% while the turquoise Female segment is larger at 56%?\",\n      \"answer\": \"The difference reflects the actual gender composition of Study 1 participants, where females comprised a slight majority over males.\",\n      \"visual_anchor\": \"Dark blue segment labeled Male 44% compared to turquoise segment at 56%\",\n      \"text_evidence\": \"Gender   \\nFigure 8: Gender breakdown of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What do the two contrasting segments in the pie chart reveal about participant demographics in Study 1?\",\n      \"answer\": \"The two segments show a nearly balanced but female-majority gender distribution, with 56% female and 44% male participants in Study 1.\",\n      \"visual_anchor\": \"Two contrasting colored segments (turquoise at 56%, dark blue at 44%)\",\n      \"text_evidence\": \"- Gender   \\nFigure 8: Gender breakdown of the participants in Study 1.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Turquoise/cyan segment labeled Female 56%",
        "Dark blue segment labeled Male 44%",
        "Pie chart showing two segments"
      ],
      "queries": [
        {
          "query": "Does the turquoise segment representing 56% Female align with the stated gender breakdown for Study 1 participants?",
          "answer": "Yes, the turquoise segment showing 56% Female directly corresponds to the gender breakdown of participants in Study 1 as stated.",
          "visual_anchor": "Turquoise segment labeled Female 56%",
          "text_evidence": "Figure 8: Gender breakdown of the participants in Study 1.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the dark blue Male segment occupy 44% while the turquoise Female segment is larger at 56%?",
          "answer": "The difference reflects the actual gender composition of Study 1 participants, where females comprised a slight majority over males.",
          "visual_anchor": "Dark blue segment labeled Male 44% compared to turquoise segment at 56%",
          "text_evidence": "Gender   \nFigure 8: Gender breakdown of the participants in Study 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What do the two contrasting segments in the pie chart reveal about participant demographics in Study 1?",
          "answer": "The two segments show a nearly balanced but female-majority gender distribution, with 56% female and 44% male participants in Study 1.",
          "visual_anchor": "Two contrasting colored segments (turquoise at 56%, dark blue at 44%)",
          "text_evidence": "- Gender   \nFigure 8: Gender breakdown of the participants in Study 1.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the turquoise segment representing 56% Female align with the stated gender breakdown for Study 1 participants?",
        "answer": "Yes, the turquoise segment showing 56% Female directly corresponds to the gender breakdown of participants in Study 1 as stated.",
        "visual_anchor": "Turquoise segment labeled Female 56%",
        "text_evidence": "Figure 8: Gender breakdown of the participants in Study 1.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the dark blue Male segment occupy 44% while the turquoise Female segment is larger at 56%?",
        "answer": "The difference reflects the actual gender composition of Study 1 participants, where females comprised a slight majority over males.",
        "visual_anchor": "Dark blue segment labeled Male 44% compared to turquoise segment at 56%",
        "text_evidence": "Gender   \nFigure 8: Gender breakdown of the participants in Study 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What do the two contrasting segments in the pie chart reveal about participant demographics in Study 1?",
        "answer": "The two segments show a nearly balanced but female-majority gender distribution, with 56% female and 44% male participants in Study 1.",
        "visual_anchor": "Two contrasting colored segments (turquoise at 56%, dark blue at 44%)",
        "text_evidence": "- Gender   \nFigure 8: Gender breakdown of the participants in Study 1.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Turquoise/cyan segment labeled Female 56%",
      "Dark blue segment labeled Male 44%",
      "Pie chart showing two segments"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_7",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig6.jpg",
    "caption": "Figure 9: Political Affiliation of the participants in Study 1.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Democratic Party cyan segment at 34%\", \"Independent blue segment at 33%\", \"Republican Party darker cyan segment at 28%\", \"Green Party 1%\", \"Libertarian Part 4%\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the 34% Democratic Party segment in the pie chart represent the largest group in Study 1?\",\n      \"answer\": \"Yes, the Democratic Party at 34% represents the largest political affiliation group among participants in Study 1, followed closely by Independent at 33%.\",\n      \"visual_anchor\": \"cyan segment labeled Democratic Party 34%\",\n      \"text_evidence\": \"Figure 9: Political Affiliation of the participants in Study 1.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why is the Independent blue segment at 33% nearly equal to the Democratic segment, given participants were from Study 1?\",\n      \"answer\": \"The nearly equal distribution between Independent (33%) and Democratic Party (34%) participants suggests Study 1 recruited a politically balanced sample with minimal difference between these two largest groups.\",\n      \"visual_anchor\": \"blue Independent segment at 33% adjacent to cyan Democratic Party segment at 34%\",\n      \"text_evidence\": \"Gender breakdown of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the Republican Party segment at 28% compare to the combined 5% of minor parties in this political affiliation breakdown?\",\n      \"answer\": \"The Republican Party at 28% represents more than five times the combined percentage of Green Party (1%) and Libertarian Part (4%), showing significantly stronger representation of major parties.\",\n      \"visual_anchor\": \"darker cyan Republican Party segment at 28% versus small purple and dark blue segments totaling 5%\",\n      \"text_evidence\": \"[Section: Political Affiliation]\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Democratic Party cyan segment at 34%",
        "Independent blue segment at 33%",
        "Republican Party darker cyan segment at 28%",
        "Green Party 1%",
        "Libertarian Part 4%"
      ],
      "queries": [
        {
          "query": "Does the 34% Democratic Party segment in the pie chart represent the largest group in Study 1?",
          "answer": "Yes, the Democratic Party at 34% represents the largest political affiliation group among participants in Study 1, followed closely by Independent at 33%.",
          "visual_anchor": "cyan segment labeled Democratic Party 34%",
          "text_evidence": "Figure 9: Political Affiliation of the participants in Study 1.",
          "query_type": "value_context"
        },
        {
          "query": "Why is the Independent blue segment at 33% nearly equal to the Democratic segment, given participants were from Study 1?",
          "answer": "The nearly equal distribution between Independent (33%) and Democratic Party (34%) participants suggests Study 1 recruited a politically balanced sample with minimal difference between these two largest groups.",
          "visual_anchor": "blue Independent segment at 33% adjacent to cyan Democratic Party segment at 34%",
          "text_evidence": "Gender breakdown of the participants in Study 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the Republican Party segment at 28% compare to the combined 5% of minor parties in this political affiliation breakdown?",
          "answer": "The Republican Party at 28% represents more than five times the combined percentage of Green Party (1%) and Libertarian Part (4%), showing significantly stronger representation of major parties.",
          "visual_anchor": "darker cyan Republican Party segment at 28% versus small purple and dark blue segments totaling 5%",
          "text_evidence": "[Section: Political Affiliation]",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 34% Democratic Party segment in the pie chart represent the largest group in Study 1?",
        "answer": "Yes, the Democratic Party at 34% represents the largest political affiliation group among participants in Study 1, followed closely by Independent at 33%.",
        "visual_anchor": "cyan segment labeled Democratic Party 34%",
        "text_evidence": "Figure 9: Political Affiliation of the participants in Study 1.",
        "query_type": "value_context"
      },
      {
        "query": "Why is the Independent blue segment at 33% nearly equal to the Democratic segment, given participants were from Study 1?",
        "answer": "The nearly equal distribution between Independent (33%) and Democratic Party (34%) participants suggests Study 1 recruited a politically balanced sample with minimal difference between these two largest groups.",
        "visual_anchor": "blue Independent segment at 33% adjacent to cyan Democratic Party segment at 34%",
        "text_evidence": "Gender breakdown of the participants in Study 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the Republican Party segment at 28% compare to the combined 5% of minor parties in this political affiliation breakdown?",
        "answer": "The Republican Party at 28% represents more than five times the combined percentage of Green Party (1%) and Libertarian Part (4%), showing significantly stronger representation of major parties.",
        "visual_anchor": "darker cyan Republican Party segment at 28% versus small purple and dark blue segments totaling 5%",
        "text_evidence": "[Section: Political Affiliation]",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Democratic Party cyan segment at 34%",
      "Independent blue segment at 33%",
      "Republican Party darker cyan segment at 28%",
      "Green Party 1%",
      "Libertarian Part 4%"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_8",
    "figure_number": 11,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg",
    "caption": "Figure 11: Breakup by state of the participants in Study 1.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Bar chart with state abbreviations on x-axis\", \"Cyan/turquoise colored bars\", \"PA showing highest bar at approximately 24\", \"FL and TX showing bars around 15\", \"Y-axis ranging from 0 to 25\"],\n  \"queries\": [\n    {\n      \"query\": \"Does PA's bar reaching approximately 24 participants represent the highest state participation in Study 1?\",\n      \"answer\": \"Yes, PA shows the highest bar at approximately 24 participants, significantly exceeding all other states shown in the Study 1 participant distribution.\",\n      \"visual_anchor\": \"PA bar reaching approximately 24, the tallest bar in the chart\",\n      \"text_evidence\": \"Breakup by state of the participants in Study 1.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the participant count in FL around 15 compare to TX which also shows approximately 15 participants?\",\n      \"answer\": \"Both FL and TX show similar participant counts at approximately 15, making them the second-highest contributing states after PA in Study 1.\",\n      \"visual_anchor\": \"FL and TX bars both at approximately 15 height\",\n      \"text_evidence\": \"Figure 11: Breakup by state of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do most state bars remain below 10 participants despite the political affiliation focus of this study section?\",\n      \"answer\": \"The majority of states contributed fewer than 10 participants, with only three states (PA, FL, TX) showing substantial participation above 13, indicating geographic concentration in Study 1 recruitment.\",\n      \"visual_anchor\": \"Majority of bars staying below the 10 mark on y-axis\",\n      \"text_evidence\": \"State breakup   \\nFigure 11: Breakup by state of the participants in Study 1.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bar chart with state abbreviations on x-axis",
        "Cyan/turquoise colored bars",
        "PA showing highest bar at approximately 24",
        "FL and TX showing bars around 15",
        "Y-axis ranging from 0 to 25"
      ],
      "queries": [
        {
          "query": "Does PA's bar reaching approximately 24 participants represent the highest state participation in Study 1?",
          "answer": "Yes, PA shows the highest bar at approximately 24 participants, significantly exceeding all other states shown in the Study 1 participant distribution.",
          "visual_anchor": "PA bar reaching approximately 24, the tallest bar in the chart",
          "text_evidence": "Breakup by state of the participants in Study 1.",
          "query_type": "value_context"
        },
        {
          "query": "How does the participant count in FL around 15 compare to TX which also shows approximately 15 participants?",
          "answer": "Both FL and TX show similar participant counts at approximately 15, making them the second-highest contributing states after PA in Study 1.",
          "visual_anchor": "FL and TX bars both at approximately 15 height",
          "text_evidence": "Figure 11: Breakup by state of the participants in Study 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do most state bars remain below 10 participants despite the political affiliation focus of this study section?",
          "answer": "The majority of states contributed fewer than 10 participants, with only three states (PA, FL, TX) showing substantial participation above 13, indicating geographic concentration in Study 1 recruitment.",
          "visual_anchor": "Majority of bars staying below the 10 mark on y-axis",
          "text_evidence": "State breakup   \nFigure 11: Breakup by state of the participants in Study 1.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does PA's bar reaching approximately 24 participants represent the highest state participation in Study 1?",
        "answer": "Yes, PA shows the highest bar at approximately 24 participants, significantly exceeding all other states shown in the Study 1 participant distribution.",
        "visual_anchor": "PA bar reaching approximately 24, the tallest bar in the chart",
        "text_evidence": "Breakup by state of the participants in Study 1.",
        "query_type": "value_context"
      },
      {
        "query": "How does the participant count in FL around 15 compare to TX which also shows approximately 15 participants?",
        "answer": "Both FL and TX show similar participant counts at approximately 15, making them the second-highest contributing states after PA in Study 1.",
        "visual_anchor": "FL and TX bars both at approximately 15 height",
        "text_evidence": "Figure 11: Breakup by state of the participants in Study 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do most state bars remain below 10 participants despite the political affiliation focus of this study section?",
        "answer": "The majority of states contributed fewer than 10 participants, with only three states (PA, FL, TX) showing substantial participation above 13, indicating geographic concentration in Study 1 recruitment.",
        "visual_anchor": "Majority of bars staying below the 10 mark on y-axis",
        "text_evidence": "State breakup   \nFigure 11: Breakup by state of the participants in Study 1.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bar chart with state abbreviations on x-axis",
      "Cyan/turquoise colored bars",
      "PA showing highest bar at approximately 24",
      "FL and TX showing bars around 15",
      "Y-axis ranging from 0 to 25"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_9",
    "figure_number": 10,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig8.jpg",
    "caption": "Figure 10: Race of the participants in Study 1.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Large blue pie slice labeled 'White 82%'\",\n    \"Small cyan slice labeled 'Asian 5%'\",\n    \"Small dark blue slice labeled 'Black 8%'\",\n    \"Small light cyan slice labeled 'Spanish/Hispanic 2%'\",\n    \"Small purple slice labeled 'Multiple 1%'\",\n    \"Small slice labeled 'Asian-American 1%'\",\n    \"Small slice labeled 'American Indian 0%'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the 82% white slice dominate Study 1's racial composition?\",\n      \"answer\": \"The large blue slice representing White participants at 82% indicates that the overwhelming majority of Study 1 participants identified as White, reflecting potential sampling demographics or recruitment patterns.\",\n      \"visual_anchor\": \"Large blue pie slice labeled 'White 82%'\",\n      \"text_evidence\": \"Race of the participants in Study 1.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the 5% Asian slice compare to the 1% Asian-American slice in Study 1?\",\n      \"answer\": \"The cyan Asian slice at 5% is five times larger than the Asian-American slice at 1%, suggesting participants distinguished between Asian and Asian-American identities in their racial self-identification.\",\n      \"visual_anchor\": \"Cyan slice labeled 'Asian 5%' versus smaller slice labeled 'Asian-American 1%'\",\n      \"text_evidence\": \"Breakup by state of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the 0% American Indian representation in the purple slice?\",\n      \"answer\": \"The purple slice labeled American Indian 0% indicates no participants in Study 1 identified with this racial category, potentially reflecting recruitment limitations or demographic unavailability in the sampled population.\",\n      \"visual_anchor\": \"Small purple slice labeled 'American Indian 0%'\",\n      \"text_evidence\": \"State breakup\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Large blue pie slice labeled 'White 82%'",
        "Small cyan slice labeled 'Asian 5%'",
        "Small dark blue slice labeled 'Black 8%'",
        "Small light cyan slice labeled 'Spanish/Hispanic 2%'",
        "Small purple slice labeled 'Multiple 1%'",
        "Small slice labeled 'Asian-American 1%'",
        "Small slice labeled 'American Indian 0%'"
      ],
      "queries": [
        {
          "query": "Why does the 82% white slice dominate Study 1's racial composition?",
          "answer": "The large blue slice representing White participants at 82% indicates that the overwhelming majority of Study 1 participants identified as White, reflecting potential sampling demographics or recruitment patterns.",
          "visual_anchor": "Large blue pie slice labeled 'White 82%'",
          "text_evidence": "Race of the participants in Study 1.",
          "query_type": "value_context"
        },
        {
          "query": "How does the 5% Asian slice compare to the 1% Asian-American slice in Study 1?",
          "answer": "The cyan Asian slice at 5% is five times larger than the Asian-American slice at 1%, suggesting participants distinguished between Asian and Asian-American identities in their racial self-identification.",
          "visual_anchor": "Cyan slice labeled 'Asian 5%' versus smaller slice labeled 'Asian-American 1%'",
          "text_evidence": "Breakup by state of the participants in Study 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the 0% American Indian representation in the purple slice?",
          "answer": "The purple slice labeled American Indian 0% indicates no participants in Study 1 identified with this racial category, potentially reflecting recruitment limitations or demographic unavailability in the sampled population.",
          "visual_anchor": "Small purple slice labeled 'American Indian 0%'",
          "text_evidence": "State breakup",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the 82% white slice dominate Study 1's racial composition?",
        "answer": "The large blue slice representing White participants at 82% indicates that the overwhelming majority of Study 1 participants identified as White, reflecting potential sampling demographics or recruitment patterns.",
        "visual_anchor": "Large blue pie slice labeled 'White 82%'",
        "text_evidence": "Race of the participants in Study 1.",
        "query_type": "value_context"
      },
      {
        "query": "How does the 5% Asian slice compare to the 1% Asian-American slice in Study 1?",
        "answer": "The cyan Asian slice at 5% is five times larger than the Asian-American slice at 1%, suggesting participants distinguished between Asian and Asian-American identities in their racial self-identification.",
        "visual_anchor": "Cyan slice labeled 'Asian 5%' versus smaller slice labeled 'Asian-American 1%'",
        "text_evidence": "Breakup by state of the participants in Study 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the 0% American Indian representation in the purple slice?",
        "answer": "The purple slice labeled American Indian 0% indicates no participants in Study 1 identified with this racial category, potentially reflecting recruitment limitations or demographic unavailability in the sampled population.",
        "visual_anchor": "Small purple slice labeled 'American Indian 0%'",
        "text_evidence": "State breakup",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Large blue pie slice labeled 'White 82%'",
      "Small cyan slice labeled 'Asian 5%'",
      "Small dark blue slice labeled 'Black 8%'",
      "Small light cyan slice labeled 'Spanish/Hispanic 2%'",
      "Small purple slice labeled 'Multiple 1%'",
      "Small slice labeled 'Asian-American 1%'",
      "Small slice labeled 'American Indian 0%'"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_10",
    "figure_number": 12,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg",
    "caption": "Figure 12: Residential breakdown of the participants in Study 1.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Dark blue segment labeled 'Suburban comm.' at 49%\",\n    \"Cyan segment labeled 'City or urban' at 26%\",\n    \"Blue segment labeled 'Rural community' at 25%\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the 49% suburban community segment represent the largest residential category among Study 1 participants?\",\n      \"answer\": \"Yes, the suburban community segment at 49% is the largest category, nearly double the city/urban and rural segments which are at 26% and 25% respectively.\",\n      \"visual_anchor\": \"Dark blue segment labeled 'Suburban comm.' showing 49%\",\n      \"text_evidence\": \"Figure 12: Residential breakdown of the participants in Study 1.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why is the cyan city or urban segment at 26% only slightly larger than the rural segment?\",\n      \"answer\": \"The city or urban segment at 26% is nearly equal to the rural community segment at 25%, differing by only 1 percentage point, indicating a balanced distribution between these two residential categories in Study 1.\",\n      \"visual_anchor\": \"Cyan segment at 26% compared to blue segment at 25%\",\n      \"text_evidence\": \"Residential breakdown of the participants in Study 1.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the dominance of the 49% suburban segment over the combined 51% of other communities?\",\n      \"answer\": \"The suburban community represents 49% of Study 1 participants, making it the single largest residential category though marginally less than the combined city/urban and rural categories together.\",\n      \"visual_anchor\": \"Dark blue suburban segment occupying nearly half the pie chart at 49%\",\n      \"text_evidence\": \"Residential Community Figure 12: Residential breakdown of the participants in Study 1.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Dark blue segment labeled 'Suburban comm.' at 49%",
        "Cyan segment labeled 'City or urban' at 26%",
        "Blue segment labeled 'Rural community' at 25%"
      ],
      "queries": [
        {
          "query": "Does the 49% suburban community segment represent the largest residential category among Study 1 participants?",
          "answer": "Yes, the suburban community segment at 49% is the largest category, nearly double the city/urban and rural segments which are at 26% and 25% respectively.",
          "visual_anchor": "Dark blue segment labeled 'Suburban comm.' showing 49%",
          "text_evidence": "Figure 12: Residential breakdown of the participants in Study 1.",
          "query_type": "value_context"
        },
        {
          "query": "Why is the cyan city or urban segment at 26% only slightly larger than the rural segment?",
          "answer": "The city or urban segment at 26% is nearly equal to the rural community segment at 25%, differing by only 1 percentage point, indicating a balanced distribution between these two residential categories in Study 1.",
          "visual_anchor": "Cyan segment at 26% compared to blue segment at 25%",
          "text_evidence": "Residential breakdown of the participants in Study 1.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the dominance of the 49% suburban segment over the combined 51% of other communities?",
          "answer": "The suburban community represents 49% of Study 1 participants, making it the single largest residential category though marginally less than the combined city/urban and rural categories together.",
          "visual_anchor": "Dark blue suburban segment occupying nearly half the pie chart at 49%",
          "text_evidence": "Residential Community Figure 12: Residential breakdown of the participants in Study 1.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 49% suburban community segment represent the largest residential category among Study 1 participants?",
        "answer": "Yes, the suburban community segment at 49% is the largest category, nearly double the city/urban and rural segments which are at 26% and 25% respectively.",
        "visual_anchor": "Dark blue segment labeled 'Suburban comm.' showing 49%",
        "text_evidence": "Figure 12: Residential breakdown of the participants in Study 1.",
        "query_type": "value_context"
      },
      {
        "query": "Why is the cyan city or urban segment at 26% only slightly larger than the rural segment?",
        "answer": "The city or urban segment at 26% is nearly equal to the rural community segment at 25%, differing by only 1 percentage point, indicating a balanced distribution between these two residential categories in Study 1.",
        "visual_anchor": "Cyan segment at 26% compared to blue segment at 25%",
        "text_evidence": "Residential breakdown of the participants in Study 1.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the dominance of the 49% suburban segment over the combined 51% of other communities?",
        "answer": "The suburban community represents 49% of Study 1 participants, making it the single largest residential category though marginally less than the combined city/urban and rural categories together.",
        "visual_anchor": "Dark blue suburban segment occupying nearly half the pie chart at 49%",
        "text_evidence": "Residential Community Figure 12: Residential breakdown of the participants in Study 1.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Dark blue segment labeled 'Suburban comm.' at 49%",
      "Cyan segment labeled 'City or urban' at 26%",
      "Blue segment labeled 'Rural community' at 25%"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_12",
    "figure_number": 13,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg",
    "caption": "Figure 13: Age distribution of the participants in Study 2.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bachelor's degree segment showing 37%\",\n    \"Some college segment showing 27%\",\n    \"Graduate degree segment showing 13%\",\n    \"Associate's degree segment showing 12%\",\n    \"High school/GED segment showing 11%\",\n    \"No high school degree segment showing 0%\",\n    \"Pie chart with segments in shades of blue and cyan\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the 37% Bachelor's degree segment align with typical demographic patterns in Study 2 participants?\",\n      \"answer\": \"Yes, the 37% Bachelor's degree representation is the largest educational segment among Study 2 participants, indicating a well-educated sample population.\",\n      \"visual_anchor\": \"Bachelor's degree segment showing 37% (largest segment in cyan)\",\n      \"text_evidence\": \"Study 2: Demographic information of the participants\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why is the combined percentage of Bachelor's and Graduate degrees larger than the Some college segment showing 27%?\",\n      \"answer\": \"Bachelor's degree (37%) and Graduate degree (13%) together represent 50% of participants, indicating Study 2 recruited more individuals with completed degrees than those with partial college education.\",\n      \"visual_anchor\": \"Bachelor's degree (37%), Graduate degree (13%), and Some college (27%) segments\",\n      \"text_evidence\": \"Figure 14: Education of the participants in Study 2.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the 0% representation in the No high school degree category for Study 2's age-related demographics?\",\n      \"answer\": \"The 0% indicates that all Study 2 participants had at least a high school education or equivalent, likely reflecting recruitment criteria targeting educated populations for the age distribution analysis.\",\n      \"visual_anchor\": \"No high school degree segment labeled 0% at the top of the pie chart\",\n      \"text_evidence\": \"Figure 13: Age distribution of the participants in Study 2.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bachelor's degree segment showing 37%",
        "Some college segment showing 27%",
        "Graduate degree segment showing 13%",
        "Associate's degree segment showing 12%",
        "High school/GED segment showing 11%",
        "No high school degree segment showing 0%",
        "Pie chart with segments in shades of blue and cyan"
      ],
      "queries": [
        {
          "query": "Does the 37% Bachelor's degree segment align with typical demographic patterns in Study 2 participants?",
          "answer": "Yes, the 37% Bachelor's degree representation is the largest educational segment among Study 2 participants, indicating a well-educated sample population.",
          "visual_anchor": "Bachelor's degree segment showing 37% (largest segment in cyan)",
          "text_evidence": "Study 2: Demographic information of the participants",
          "query_type": "value_context"
        },
        {
          "query": "Why is the combined percentage of Bachelor's and Graduate degrees larger than the Some college segment showing 27%?",
          "answer": "Bachelor's degree (37%) and Graduate degree (13%) together represent 50% of participants, indicating Study 2 recruited more individuals with completed degrees than those with partial college education.",
          "visual_anchor": "Bachelor's degree (37%), Graduate degree (13%), and Some college (27%) segments",
          "text_evidence": "Figure 14: Education of the participants in Study 2.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the 0% representation in the No high school degree category for Study 2's age-related demographics?",
          "answer": "The 0% indicates that all Study 2 participants had at least a high school education or equivalent, likely reflecting recruitment criteria targeting educated populations for the age distribution analysis.",
          "visual_anchor": "No high school degree segment labeled 0% at the top of the pie chart",
          "text_evidence": "Figure 13: Age distribution of the participants in Study 2.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 37% Bachelor's degree segment align with typical demographic patterns in Study 2 participants?",
        "answer": "Yes, the 37% Bachelor's degree representation is the largest educational segment among Study 2 participants, indicating a well-educated sample population.",
        "visual_anchor": "Bachelor's degree segment showing 37% (largest segment in cyan)",
        "text_evidence": "Study 2: Demographic information of the participants",
        "query_type": "value_context"
      },
      {
        "query": "Why is the combined percentage of Bachelor's and Graduate degrees larger than the Some college segment showing 27%?",
        "answer": "Bachelor's degree (37%) and Graduate degree (13%) together represent 50% of participants, indicating Study 2 recruited more individuals with completed degrees than those with partial college education.",
        "visual_anchor": "Bachelor's degree (37%), Graduate degree (13%), and Some college (27%) segments",
        "text_evidence": "Figure 14: Education of the participants in Study 2.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the 0% representation in the No high school degree category for Study 2's age-related demographics?",
        "answer": "The 0% indicates that all Study 2 participants had at least a high school education or equivalent, likely reflecting recruitment criteria targeting educated populations for the age distribution analysis.",
        "visual_anchor": "No high school degree segment labeled 0% at the top of the pie chart",
        "text_evidence": "Figure 13: Age distribution of the participants in Study 2.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bachelor's degree segment showing 37%",
      "Some college segment showing 27%",
      "Graduate degree segment showing 13%",
      "Associate's degree segment showing 12%",
      "High school/GED segment showing 11%",
      "No high school degree segment showing 0%",
      "Pie chart with segments in shades of blue and cyan"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_13",
    "figure_number": 15,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig12.jpg",
    "caption": "Figure 15: Gender breakdown of the participants in Study 2.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Female segment in turquoise showing 58%\", \"Male segment in blue showing 42%\", \"Non-binary segment at top showing 0%\", \"Pie chart with three segments\"],\n  \"queries\": [\n    {\n      \"query\": \"What does the 58% female representation in turquoise reveal about gender balance in Study 2?\",\n      \"answer\": \"The 58% female representation indicates a slight female majority among participants in Study 2, showing a relatively balanced but female-leaning gender distribution.\",\n      \"visual_anchor\": \"Turquoise segment labeled 'Female 58%'\",\n      \"text_evidence\": \"Gender breakdown of the participants in Study 2.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the 42% male blue segment compare to the 58% female turquoise segment in Study 2?\",\n      \"answer\": \"The male segment at 42% is 16 percentage points lower than the female segment at 58%, indicating females outnumber males by a moderate margin in Study 2.\",\n      \"visual_anchor\": \"Blue segment labeled 'Male 42%' compared to turquoise segment labeled 'Female 58%'\",\n      \"text_evidence\": \"Figure 15: Gender breakdown of the participants in Study 2.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the non-binary segment at the top show 0% despite being included in Study 2?\",\n      \"answer\": \"The 0% indicates that no participants in Study 2 identified as non-binary, though the category was tracked and included in the gender breakdown analysis.\",\n      \"visual_anchor\": \"Non-binary segment at top of pie chart showing 0%\",\n      \"text_evidence\": \"Gender breakdown of the participants in Study 2\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Female segment in turquoise showing 58%",
        "Male segment in blue showing 42%",
        "Non-binary segment at top showing 0%",
        "Pie chart with three segments"
      ],
      "queries": [
        {
          "query": "What does the 58% female representation in turquoise reveal about gender balance in Study 2?",
          "answer": "The 58% female representation indicates a slight female majority among participants in Study 2, showing a relatively balanced but female-leaning gender distribution.",
          "visual_anchor": "Turquoise segment labeled 'Female 58%'",
          "text_evidence": "Gender breakdown of the participants in Study 2.",
          "query_type": "value_context"
        },
        {
          "query": "How does the 42% male blue segment compare to the 58% female turquoise segment in Study 2?",
          "answer": "The male segment at 42% is 16 percentage points lower than the female segment at 58%, indicating females outnumber males by a moderate margin in Study 2.",
          "visual_anchor": "Blue segment labeled 'Male 42%' compared to turquoise segment labeled 'Female 58%'",
          "text_evidence": "Figure 15: Gender breakdown of the participants in Study 2.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the non-binary segment at the top show 0% despite being included in Study 2?",
          "answer": "The 0% indicates that no participants in Study 2 identified as non-binary, though the category was tracked and included in the gender breakdown analysis.",
          "visual_anchor": "Non-binary segment at top of pie chart showing 0%",
          "text_evidence": "Gender breakdown of the participants in Study 2",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "What does the 58% female representation in turquoise reveal about gender balance in Study 2?",
        "answer": "The 58% female representation indicates a slight female majority among participants in Study 2, showing a relatively balanced but female-leaning gender distribution.",
        "visual_anchor": "Turquoise segment labeled 'Female 58%'",
        "text_evidence": "Gender breakdown of the participants in Study 2.",
        "query_type": "value_context"
      },
      {
        "query": "How does the 42% male blue segment compare to the 58% female turquoise segment in Study 2?",
        "answer": "The male segment at 42% is 16 percentage points lower than the female segment at 58%, indicating females outnumber males by a moderate margin in Study 2.",
        "visual_anchor": "Blue segment labeled 'Male 42%' compared to turquoise segment labeled 'Female 58%'",
        "text_evidence": "Figure 15: Gender breakdown of the participants in Study 2.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the non-binary segment at the top show 0% despite being included in Study 2?",
        "answer": "The 0% indicates that no participants in Study 2 identified as non-binary, though the category was tracked and included in the gender breakdown analysis.",
        "visual_anchor": "Non-binary segment at top of pie chart showing 0%",
        "text_evidence": "Gender breakdown of the participants in Study 2",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Female segment in turquoise showing 58%",
      "Male segment in blue showing 42%",
      "Non-binary segment at top showing 0%",
      "Pie chart with three segments"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_14",
    "figure_number": 16,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg",
    "caption": "Figure 16: Political Affiliation of the participants in Study 2.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Democratic Party segment at 43% in turquoise\",\n    \"Republican Party segment at 26% in dark blue\",\n    \"Independent segment at 25% in cyan\",\n    \"Libertarian segment at 4% in medium blue\",\n    \"Green Party segment at 1% in light cyan\",\n    \"Multiple segment at 1% in purple\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the 43% Democratic Party turquoise segment represent the political affiliation distribution in Study 2?\",\n      \"answer\": \"Yes, the 43% turquoise segment represents Democratic Party affiliation in Study 2, as shown in the pie chart displaying political affiliation breakdown of participants.\",\n      \"visual_anchor\": \"turquoise segment labeled Democratic Party at 43%\",\n      \"text_evidence\": \"Figure 16: Political Affiliation of the participants in Study 2.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why is the dark blue Republican segment at 26% only slightly larger than the cyan Independent segment at 25%?\",\n      \"answer\": \"The Republican and Independent segments are nearly equal because Study 2 participants were almost evenly distributed between these two affiliations, with only a 1 percentage point difference.\",\n      \"visual_anchor\": \"dark blue Republican segment at 26% compared to cyan Independent segment at 25%\",\n      \"text_evidence\": \"Political Affiliation   \\nFigure 16: Political Affiliation of the participants in Study 2.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the combined 1% purple Multiple and 1% Green Party segments represent minority affiliations among Study 2 participants?\",\n      \"answer\": \"Yes, the Multiple and Green Party affiliations each represent only 1% of participants, making them the smallest groups compared to the dominant Democratic Party, Republican, and Independent categories.\",\n      \"visual_anchor\": \"purple Multiple segment at 1% and light cyan Green Party segment at 1%\",\n      \"text_evidence\": \"Gender   \\nFigure 15: Gender breakdown of the participants in Study 2.\\nText after: Political Affiliation   \\nFigure 16: Political Affiliation of the participants in Study 2.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Democratic Party segment at 43% in turquoise",
        "Republican Party segment at 26% in dark blue",
        "Independent segment at 25% in cyan",
        "Libertarian segment at 4% in medium blue",
        "Green Party segment at 1% in light cyan",
        "Multiple segment at 1% in purple"
      ],
      "queries": [
        {
          "query": "Does the 43% Democratic Party turquoise segment represent the political affiliation distribution in Study 2?",
          "answer": "Yes, the 43% turquoise segment represents Democratic Party affiliation in Study 2, as shown in the pie chart displaying political affiliation breakdown of participants.",
          "visual_anchor": "turquoise segment labeled Democratic Party at 43%",
          "text_evidence": "Figure 16: Political Affiliation of the participants in Study 2.",
          "query_type": "value_context"
        },
        {
          "query": "Why is the dark blue Republican segment at 26% only slightly larger than the cyan Independent segment at 25%?",
          "answer": "The Republican and Independent segments are nearly equal because Study 2 participants were almost evenly distributed between these two affiliations, with only a 1 percentage point difference.",
          "visual_anchor": "dark blue Republican segment at 26% compared to cyan Independent segment at 25%",
          "text_evidence": "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the combined 1% purple Multiple and 1% Green Party segments represent minority affiliations among Study 2 participants?",
          "answer": "Yes, the Multiple and Green Party affiliations each represent only 1% of participants, making them the smallest groups compared to the dominant Democratic Party, Republican, and Independent categories.",
          "visual_anchor": "purple Multiple segment at 1% and light cyan Green Party segment at 1%",
          "text_evidence": "Gender   \nFigure 15: Gender breakdown of the participants in Study 2.\nText after: Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 43% Democratic Party turquoise segment represent the political affiliation distribution in Study 2?",
        "answer": "Yes, the 43% turquoise segment represents Democratic Party affiliation in Study 2, as shown in the pie chart displaying political affiliation breakdown of participants.",
        "visual_anchor": "turquoise segment labeled Democratic Party at 43%",
        "text_evidence": "Figure 16: Political Affiliation of the participants in Study 2.",
        "query_type": "value_context"
      },
      {
        "query": "Why is the dark blue Republican segment at 26% only slightly larger than the cyan Independent segment at 25%?",
        "answer": "The Republican and Independent segments are nearly equal because Study 2 participants were almost evenly distributed between these two affiliations, with only a 1 percentage point difference.",
        "visual_anchor": "dark blue Republican segment at 26% compared to cyan Independent segment at 25%",
        "text_evidence": "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the combined 1% purple Multiple and 1% Green Party segments represent minority affiliations among Study 2 participants?",
        "answer": "Yes, the Multiple and Green Party affiliations each represent only 1% of participants, making them the smallest groups compared to the dominant Democratic Party, Republican, and Independent categories.",
        "visual_anchor": "purple Multiple segment at 1% and light cyan Green Party segment at 1%",
        "text_evidence": "Gender   \nFigure 15: Gender breakdown of the participants in Study 2.\nText after: Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Democratic Party segment at 43% in turquoise",
      "Republican Party segment at 26% in dark blue",
      "Independent segment at 25% in cyan",
      "Libertarian segment at 4% in medium blue",
      "Green Party segment at 1% in light cyan",
      "Multiple segment at 1% in purple"
    ]
  },
  {
    "doc_id": "1811.03654",
    "figure_id": "1811.03654_fig_15",
    "figure_number": 17,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg",
    "caption": "Figure 17: Race of the participants in Study 2.   ",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Large blue pie segment labeled White 74%\",\n    \"Small cyan segment labeled Black 9%\",\n    \"Light cyan segment labeled Spanish/Hispanic 5%\",\n    \"Small segment labeled Asian 6%\",\n    \"Small segment labeled Asian-American 1%\",\n    \"Small segment labeled Multiple 4%\",\n    \"Segments labeled Native Hawaiian 0% and American Indian 0%\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the 74% White majority in the pie chart align with the racial breakdown described for Study 2 participants?\",\n      \"answer\": \"Yes, the 74% White segment dominates the pie chart and represents the racial composition of Study 2 participants as documented in the study's demographic data.\",\n      \"visual_anchor\": \"Large blue pie segment labeled White 74%\",\n      \"text_evidence\": \"Figure 17: Race of the participants in Study 2.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the combined minority representation of 25% appear smaller than the 74% White segment despite multiple racial categories?\",\n      \"answer\": \"The 74% White majority significantly outnumbers all other racial groups combined (Black 9%, Asian 6%, Spanish/Hispanic 5%, Multiple 4%, Asian-American 1%, and 0% for Native Hawaiian and American Indian), creating the visual disparity in segment sizes.\",\n      \"visual_anchor\": \"Comparison between large blue segment (74%) and all smaller colored segments totaling 25%\",\n      \"text_evidence\": \"[Section: Race]\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the 0% representation for both Native Hawaiian and American Indian segments in the racial composition pie chart?\",\n      \"answer\": \"No participants in Study 2 identified as Native Hawaiian or American Indian, resulting in zero percentage allocation for these demographic categories in the participant pool.\",\n      \"visual_anchor\": \"Segments labeled Native Hawaiian 0% and American Indian 0%\",\n      \"text_evidence\": \"Residential Community   \\nFigure 18: Residential breakdown of the participants in Study 2.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Large blue pie segment labeled White 74%",
        "Small cyan segment labeled Black 9%",
        "Light cyan segment labeled Spanish/Hispanic 5%",
        "Small segment labeled Asian 6%",
        "Small segment labeled Asian-American 1%",
        "Small segment labeled Multiple 4%",
        "Segments labeled Native Hawaiian 0% and American Indian 0%"
      ],
      "queries": [
        {
          "query": "Does the 74% White majority in the pie chart align with the racial breakdown described for Study 2 participants?",
          "answer": "Yes, the 74% White segment dominates the pie chart and represents the racial composition of Study 2 participants as documented in the study's demographic data.",
          "visual_anchor": "Large blue pie segment labeled White 74%",
          "text_evidence": "Figure 17: Race of the participants in Study 2.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the combined minority representation of 25% appear smaller than the 74% White segment despite multiple racial categories?",
          "answer": "The 74% White majority significantly outnumbers all other racial groups combined (Black 9%, Asian 6%, Spanish/Hispanic 5%, Multiple 4%, Asian-American 1%, and 0% for Native Hawaiian and American Indian), creating the visual disparity in segment sizes.",
          "visual_anchor": "Comparison between large blue segment (74%) and all smaller colored segments totaling 25%",
          "text_evidence": "[Section: Race]",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the 0% representation for both Native Hawaiian and American Indian segments in the racial composition pie chart?",
          "answer": "No participants in Study 2 identified as Native Hawaiian or American Indian, resulting in zero percentage allocation for these demographic categories in the participant pool.",
          "visual_anchor": "Segments labeled Native Hawaiian 0% and American Indian 0%",
          "text_evidence": "Residential Community   \nFigure 18: Residential breakdown of the participants in Study 2.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the 74% White majority in the pie chart align with the racial breakdown described for Study 2 participants?",
        "answer": "Yes, the 74% White segment dominates the pie chart and represents the racial composition of Study 2 participants as documented in the study's demographic data.",
        "visual_anchor": "Large blue pie segment labeled White 74%",
        "text_evidence": "Figure 17: Race of the participants in Study 2.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the combined minority representation of 25% appear smaller than the 74% White segment despite multiple racial categories?",
        "answer": "The 74% White majority significantly outnumbers all other racial groups combined (Black 9%, Asian 6%, Spanish/Hispanic 5%, Multiple 4%, Asian-American 1%, and 0% for Native Hawaiian and American Indian), creating the visual disparity in segment sizes.",
        "visual_anchor": "Comparison between large blue segment (74%) and all smaller colored segments totaling 25%",
        "text_evidence": "[Section: Race]",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the 0% representation for both Native Hawaiian and American Indian segments in the racial composition pie chart?",
        "answer": "No participants in Study 2 identified as Native Hawaiian or American Indian, resulting in zero percentage allocation for these demographic categories in the participant pool.",
        "visual_anchor": "Segments labeled Native Hawaiian 0% and American Indian 0%",
        "text_evidence": "Residential Community   \nFigure 18: Residential breakdown of the participants in Study 2.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Large blue pie segment labeled White 74%",
      "Small cyan segment labeled Black 9%",
      "Light cyan segment labeled Spanish/Hispanic 5%",
      "Small segment labeled Asian 6%",
      "Small segment labeled Asian-American 1%",
      "Small segment labeled Multiple 4%",
      "Segments labeled Native Hawaiian 0% and American Indian 0%"
    ]
  },
  {
    "doc_id": "1811.10104",
    "figure_id": "1811.10104_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg",
    "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Two diagonal lines labeled π₂ and π₁\",\n    \"Horizontal axis labeled Test (X)\",\n    \"Vertical axis labeled y*\",\n    \"Marginal distribution curves for π₁ and π₂ on both axes\",\n    \"Shaded regions under distribution curves\",\n    \"Connecting lines between axes forming a geometric projection\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How do the π₁ and π₂ marginal distributions on the vertical axis relate to Cleary's bias definition for different student groups?\",\n      \"answer\": \"The vertical marginal distributions show ground truth score distributions for subgroups π₁ and π₂, which Cleary used to define test bias quantitatively for black and white students by examining differences between these group distributions.\",\n      \"visual_anchor\": \"Marginal distribution curves labeled π₁ and π₂ on the vertical y* axis\",\n      \"text_evidence\": \"Concerned with the fairness of tests for black and white students, T. Anne Cleary defined a quantitative measure of test bias\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the diagonal projection lines connect Test X and y* axes differently for π₁ versus π₂ subgroups?\",\n      \"answer\": \"The different diagonal lines for π₁ and π₂ illustrate that test scores and ground truth scores relate differently across subgroups, demonstrating that available criterion scores may not be perfectly relevant, reliable, and unbiased measures as Thorndike critiqued.\",\n      \"visual_anchor\": \"Two separate diagonal lines labeled π₁ and π₂ connecting horizontal and vertical axes\",\n      \"text_evidence\": \"The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What shift in perspective does the shaded region under π₂ on the horizontal Test axis represent?\",\n      \"answer\": \"The shaded region represents the evolution from 1960s bias analysis to 1970s fairness frameworks, marking the change in framing from unfairness to fairness that parallels recent ML fairness work as the decade transitioned.\",\n      \"visual_anchor\": \"Shaded region under the π₂ curve on the horizontal Test (X) axis\",\n      \"text_evidence\": \"As the 1960s turned to the 1970s, work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Two diagonal lines labeled π₂ and π₁",
        "Horizontal axis labeled Test (X)",
        "Vertical axis labeled y*",
        "Marginal distribution curves for π₁ and π₂ on both axes",
        "Shaded regions under distribution curves",
        "Connecting lines between axes forming a geometric projection"
      ],
      "queries": [
        {
          "query": "How do the π₁ and π₂ marginal distributions on the vertical axis relate to Cleary's bias definition for different student groups?",
          "answer": "The vertical marginal distributions show ground truth score distributions for subgroups π₁ and π₂, which Cleary used to define test bias quantitatively for black and white students by examining differences between these group distributions.",
          "visual_anchor": "Marginal distribution curves labeled π₁ and π₂ on the vertical y* axis",
          "text_evidence": "Concerned with the fairness of tests for black and white students, T. Anne Cleary defined a quantitative measure of test bias",
          "query_type": "value_context"
        },
        {
          "query": "Why do the diagonal projection lines connect Test X and y* axes differently for π₁ versus π₂ subgroups?",
          "answer": "The different diagonal lines for π₁ and π₂ illustrate that test scores and ground truth scores relate differently across subgroups, demonstrating that available criterion scores may not be perfectly relevant, reliable, and unbiased measures as Thorndike critiqued.",
          "visual_anchor": "Two separate diagonal lines labeled π₁ and π₂ connecting horizontal and vertical axes",
          "text_evidence": "The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What shift in perspective does the shaded region under π₂ on the horizontal Test axis represent?",
          "answer": "The shaded region represents the evolution from 1960s bias analysis to 1970s fairness frameworks, marking the change in framing from unfairness to fairness that parallels recent ML fairness work as the decade transitioned.",
          "visual_anchor": "Shaded region under the π₂ curve on the horizontal Test (X) axis",
          "text_evidence": "As the 1960s turned to the 1970s, work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How do the π₁ and π₂ marginal distributions on the vertical axis relate to Cleary's bias definition for different student groups?",
        "answer": "The vertical marginal distributions show ground truth score distributions for subgroups π₁ and π₂, which Cleary used to define test bias quantitatively for black and white students by examining differences between these group distributions.",
        "visual_anchor": "Marginal distribution curves labeled π₁ and π₂ on the vertical y* axis",
        "text_evidence": "Concerned with the fairness of tests for black and white students, T. Anne Cleary defined a quantitative measure of test bias",
        "query_type": "value_context"
      },
      {
        "query": "Why do the diagonal projection lines connect Test X and y* axes differently for π₁ versus π₂ subgroups?",
        "answer": "The different diagonal lines for π₁ and π₂ illustrate that test scores and ground truth scores relate differently across subgroups, demonstrating that available criterion scores may not be perfectly relevant, reliable, and unbiased measures as Thorndike critiqued.",
        "visual_anchor": "Two separate diagonal lines labeled π₁ and π₂ connecting horizontal and vertical axes",
        "text_evidence": "The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What shift in perspective does the shaded region under π₂ on the horizontal Test axis represent?",
        "answer": "The shaded region represents the evolution from 1960s bias analysis to 1970s fairness frameworks, marking the change in framing from unfairness to fairness that parallels recent ML fairness work as the decade transitioned.",
        "visual_anchor": "Shaded region under the π₂ curve on the horizontal Test (X) axis",
        "text_evidence": "As the 1960s turned to the 1970s, work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Two diagonal lines labeled π₂ and π₁",
      "Horizontal axis labeled Test (X)",
      "Vertical axis labeled y*",
      "Marginal distribution curves for π₁ and π₂ on both axes",
      "Shaded regions under distribution curves",
      "Connecting lines between axes forming a geometric projection"
    ]
  },
  {
    "doc_id": "1811.10104",
    "figure_id": "1811.10104_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg",
    "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"curved line labeled 1\", \"horizontal line labeled 2\", \"diagonal line labeled 3\", \"horizontal line labeled 4\", \"y-axis labeled r_CX\", \"x-axis labeled r_XY\", \"origin point labeled 0\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does curve 1 show decreasing r_CX as r_XY increases, given Thorndike's premise about criterion scores?\",\n      \"answer\": \"Thorndike questioned the oversimplified premise that criterion scores are perfectly relevant, reliable, and unbiased, which relates to how culture-test correlations should vary with test-truth correlations under different fairness definitions.\",\n      \"visual_anchor\": \"curved line labeled 1 descending from top-left to bottom-right\",\n      \"text_evidence\": \"Following Thorndike [62], 'The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...'\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do lines 2, 3, and 4 converging at the right support Cole's claim that research failed to indicate fairness unequivocally?\",\n      \"answer\": \"Yes, the convergence of multiple fairness definitions (lines 2-4) at high r_XY values illustrates that different fairness criteria yield different fair values, supporting Cole's observation that research supplied no unequivocal fairness indication.\",\n      \"visual_anchor\": \"lines labeled 2, 3, and 4 converging at the right side of the x-axis\",\n      \"text_evidence\": \"The same Cole who in 1973 proposed equality of TPR, wrote in 2001 that [15]: In short, research over the last 30 or so years has not supplied any analyses to unequivocally indicate fairness or unfairness\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the divergence among curves 1 through 4 at low r_XY values under the fixed 0.2 demographic-target correlation assumption?\",\n      \"answer\": \"Darlington's four different fairness definitions yield distinct fair correlation values when test-truth correlation is low, reflecting fundamentally different conceptions of what constitutes fair testing under the same demographic conditions.\",\n      \"visual_anchor\": \"separation of curves 1, 2, 3, and 4 at the left side where r_XY is low\",\n      \"text_evidence\": \"fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton's notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "curved line labeled 1",
        "horizontal line labeled 2",
        "diagonal line labeled 3",
        "horizontal line labeled 4",
        "y-axis labeled r_CX",
        "x-axis labeled r_XY",
        "origin point labeled 0"
      ],
      "queries": [
        {
          "query": "Why does curve 1 show decreasing r_CX as r_XY increases, given Thorndike's premise about criterion scores?",
          "answer": "Thorndike questioned the oversimplified premise that criterion scores are perfectly relevant, reliable, and unbiased, which relates to how culture-test correlations should vary with test-truth correlations under different fairness definitions.",
          "visual_anchor": "curved line labeled 1 descending from top-left to bottom-right",
          "text_evidence": "Following Thorndike [62], 'The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...'",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do lines 2, 3, and 4 converging at the right support Cole's claim that research failed to indicate fairness unequivocally?",
          "answer": "Yes, the convergence of multiple fairness definitions (lines 2-4) at high r_XY values illustrates that different fairness criteria yield different fair values, supporting Cole's observation that research supplied no unequivocal fairness indication.",
          "visual_anchor": "lines labeled 2, 3, and 4 converging at the right side of the x-axis",
          "text_evidence": "The same Cole who in 1973 proposed equality of TPR, wrote in 2001 that [15]: In short, research over the last 30 or so years has not supplied any analyses to unequivocally indicate fairness or unfairness",
          "query_type": "value_context"
        },
        {
          "query": "What explains the divergence among curves 1 through 4 at low r_XY values under the fixed 0.2 demographic-target correlation assumption?",
          "answer": "Darlington's four different fairness definitions yield distinct fair correlation values when test-truth correlation is low, reflecting fundamentally different conceptions of what constitutes fair testing under the same demographic conditions.",
          "visual_anchor": "separation of curves 1, 2, 3, and 4 at the left side where r_XY is low",
          "text_evidence": "fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton's notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does curve 1 show decreasing r_CX as r_XY increases, given Thorndike's premise about criterion scores?",
        "answer": "Thorndike questioned the oversimplified premise that criterion scores are perfectly relevant, reliable, and unbiased, which relates to how culture-test correlations should vary with test-truth correlations under different fairness definitions.",
        "visual_anchor": "curved line labeled 1 descending from top-left to bottom-right",
        "text_evidence": "Following Thorndike [62], 'The discussion of 'fairness' in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...'",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do lines 2, 3, and 4 converging at the right support Cole's claim that research failed to indicate fairness unequivocally?",
        "answer": "Yes, the convergence of multiple fairness definitions (lines 2-4) at high r_XY values illustrates that different fairness criteria yield different fair values, supporting Cole's observation that research supplied no unequivocal fairness indication.",
        "visual_anchor": "lines labeled 2, 3, and 4 converging at the right side of the x-axis",
        "text_evidence": "The same Cole who in 1973 proposed equality of TPR, wrote in 2001 that [15]: In short, research over the last 30 or so years has not supplied any analyses to unequivocally indicate fairness or unfairness",
        "query_type": "value_context"
      },
      {
        "query": "What explains the divergence among curves 1 through 4 at low r_XY values under the fixed 0.2 demographic-target correlation assumption?",
        "answer": "Darlington's four different fairness definitions yield distinct fair correlation values when test-truth correlation is low, reflecting fundamentally different conceptions of what constitutes fair testing under the same demographic conditions.",
        "visual_anchor": "separation of curves 1, 2, 3, and 4 at the left side where r_XY is low",
        "text_evidence": "fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton's notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "curved line labeled 1",
      "horizontal line labeled 2",
      "diagonal line labeled 3",
      "horizontal line labeled 4",
      "y-axis labeled r_CX",
      "x-axis labeled r_XY",
      "origin point labeled 0"
    ]
  },
  {
    "doc_id": "1811.10104",
    "figure_id": "1811.10104_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg",
    "caption": "Figure 3: Original graph from [22] illustrating DIF.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Filled circles (Males)\", \"Open circles (Females)\", \"SAT-VERBAL SCALED SCORE on x-axis (200-800)\", \"PERCENT CORRECT on y-axis (0-100)\", \"Two distinct curves showing gender performance differences\", \"Males curve consistently above Females curve\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the filled circles remain above open circles across all SAT-VERBAL scores if DIF examines bias causes?\",\n      \"answer\": \"DIF identifies items displaying differential functioning between groups, which test designers examine to identify the cause of bias and possibly remove from tests. The consistent separation between male and female performance curves illustrates such differential item functioning that would warrant further investigation.\",\n      \"visual_anchor\": \"Filled circles (Males) consistently positioned above open circles (Females) across the entire SAT-VERBAL score range from 200 to 800\",\n      \"text_evidence\": \"Items displaying a DIF are ideally examined further to identify the cause of bias, and possibly removed from the test\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the 30-point gap between filled and open circles at SAT-VERBAL score 400 exemplify conditional probability fairness?\",\n      \"answer\": \"Yes, the performance gap at matched SAT-VERBAL scores directly relates to conditional probability fairness, which modern ML fairness calls sufficiency or equalized odds. This criterion examines whether outcomes differ between groups given the same predictor score.\",\n      \"visual_anchor\": \"Vertical distance between filled circles (Males) and open circles (Females) at SAT-VERBAL score 400, showing approximately 30 percentage points difference\",\n      \"text_evidence\": \"Peterson and Novick's \\\"conditional probability and its converse\\\" is equivalent to what in ML fairness is variously called sufficiency [4], equalized odds [32], or conditional procedure accuracy [5], sometimes expressed as the conditional independence A ⊥ D | Y\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the consistent vertical separation between filled and open circles relate to renewed 1980s intelligence debates?\",\n      \"answer\": \"The persistent performance gap between males and females across all score levels illustrates the type of group differences that fueled 1980s public debates about racial differences in general intelligence and fair testing implications. Such visualizations of differential performance became central to discussions about test fairness.\",\n      \"visual_anchor\": \"Consistent vertical separation of approximately 10-20 percentage points between filled circles (Males) and open circles (Females) throughout the entire plot\",\n      \"text_evidence\": \"With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Filled circles (Males)",
        "Open circles (Females)",
        "SAT-VERBAL SCALED SCORE on x-axis (200-800)",
        "PERCENT CORRECT on y-axis (0-100)",
        "Two distinct curves showing gender performance differences",
        "Males curve consistently above Females curve"
      ],
      "queries": [
        {
          "query": "Why do the filled circles remain above open circles across all SAT-VERBAL scores if DIF examines bias causes?",
          "answer": "DIF identifies items displaying differential functioning between groups, which test designers examine to identify the cause of bias and possibly remove from tests. The consistent separation between male and female performance curves illustrates such differential item functioning that would warrant further investigation.",
          "visual_anchor": "Filled circles (Males) consistently positioned above open circles (Females) across the entire SAT-VERBAL score range from 200 to 800",
          "text_evidence": "Items displaying a DIF are ideally examined further to identify the cause of bias, and possibly removed from the test",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the 30-point gap between filled and open circles at SAT-VERBAL score 400 exemplify conditional probability fairness?",
          "answer": "Yes, the performance gap at matched SAT-VERBAL scores directly relates to conditional probability fairness, which modern ML fairness calls sufficiency or equalized odds. This criterion examines whether outcomes differ between groups given the same predictor score.",
          "visual_anchor": "Vertical distance between filled circles (Males) and open circles (Females) at SAT-VERBAL score 400, showing approximately 30 percentage points difference",
          "text_evidence": "Peterson and Novick's \"conditional probability and its converse\" is equivalent to what in ML fairness is variously called sufficiency [4], equalized odds [32], or conditional procedure accuracy [5], sometimes expressed as the conditional independence A ⊥ D | Y",
          "query_type": "value_context"
        },
        {
          "query": "How does the consistent vertical separation between filled and open circles relate to renewed 1980s intelligence debates?",
          "answer": "The persistent performance gap between males and females across all score levels illustrates the type of group differences that fueled 1980s public debates about racial differences in general intelligence and fair testing implications. Such visualizations of differential performance became central to discussions about test fairness.",
          "visual_anchor": "Consistent vertical separation of approximately 10-20 percentage points between filled circles (Males) and open circles (Females) throughout the entire plot",
          "text_evidence": "With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the filled circles remain above open circles across all SAT-VERBAL scores if DIF examines bias causes?",
        "answer": "DIF identifies items displaying differential functioning between groups, which test designers examine to identify the cause of bias and possibly remove from tests. The consistent separation between male and female performance curves illustrates such differential item functioning that would warrant further investigation.",
        "visual_anchor": "Filled circles (Males) consistently positioned above open circles (Females) across the entire SAT-VERBAL score range from 200 to 800",
        "text_evidence": "Items displaying a DIF are ideally examined further to identify the cause of bias, and possibly removed from the test",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the 30-point gap between filled and open circles at SAT-VERBAL score 400 exemplify conditional probability fairness?",
        "answer": "Yes, the performance gap at matched SAT-VERBAL scores directly relates to conditional probability fairness, which modern ML fairness calls sufficiency or equalized odds. This criterion examines whether outcomes differ between groups given the same predictor score.",
        "visual_anchor": "Vertical distance between filled circles (Males) and open circles (Females) at SAT-VERBAL score 400, showing approximately 30 percentage points difference",
        "text_evidence": "Peterson and Novick's \"conditional probability and its converse\" is equivalent to what in ML fairness is variously called sufficiency [4], equalized odds [32], or conditional procedure accuracy [5], sometimes expressed as the conditional independence A ⊥ D | Y",
        "query_type": "value_context"
      },
      {
        "query": "How does the consistent vertical separation between filled and open circles relate to renewed 1980s intelligence debates?",
        "answer": "The persistent performance gap between males and females across all score levels illustrates the type of group differences that fueled 1980s public debates about racial differences in general intelligence and fair testing implications. Such visualizations of differential performance became central to discussions about test fairness.",
        "visual_anchor": "Consistent vertical separation of approximately 10-20 percentage points between filled circles (Males) and open circles (Females) throughout the entire plot",
        "text_evidence": "With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Filled circles (Males)",
      "Open circles (Females)",
      "SAT-VERBAL SCALED SCORE on x-axis (200-800)",
      "PERCENT CORRECT on y-axis (0-100)",
      "Two distinct curves showing gender performance differences",
      "Males curve consistently above Females curve"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg",
    "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"red bounding box around face region\",\n    \"frontal face photo (a)\",\n    \"face with landmark points (b)\",\n    \"rectified face (c)\",\n    \"50x50 pixel minimum region size\",\n    \"30 pixel inter-ocular distance threshold\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red bounding box in stage (a) need minimum 50x50 pixels before landmark extraction?\",\n      \"answer\": \"Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded to ensure sufficient resolution for reliable processing and feature extraction.\",\n      \"visual_anchor\": \"red bounding box in image (a) showing face detection region\",\n      \"text_evidence\": \"Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the Faster-RCNN detector producing the red box relate to the YFCC-100M photo retrieval process?\",\n      \"answer\": \"After downloading photos from active YFCC-100M URLs with Creative Commons licenses, the Faster-RCNN based CNN object detector was applied to detect all depicted faces in each retrieved photo.\",\n      \"visual_anchor\": \"red bounding box from CNN face detection in image (a)\",\n      \"text_evidence\": \"Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why must faces progress from the red box detection to landmark points before reaching the rectified stage?\",\n      \"answer\": \"The affine transformation for rectification requires center points of both eyes, which are obtained from DLIB landmark extraction performed on the detected face region.\",\n      \"visual_anchor\": \"progression from red box (a) through landmark points (b) to rectified face (c)\",\n      \"text_evidence\": \"Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "red bounding box around face region",
        "frontal face photo (a)",
        "face with landmark points (b)",
        "rectified face (c)",
        "50x50 pixel minimum region size",
        "30 pixel inter-ocular distance threshold"
      ],
      "queries": [
        {
          "query": "Why does the red bounding box in stage (a) need minimum 50x50 pixels before landmark extraction?",
          "answer": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded to ensure sufficient resolution for reliable processing and feature extraction.",
          "visual_anchor": "red bounding box in image (a) showing face detection region",
          "text_evidence": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded.",
          "query_type": "value_context"
        },
        {
          "query": "How does the Faster-RCNN detector producing the red box relate to the YFCC-100M photo retrieval process?",
          "answer": "After downloading photos from active YFCC-100M URLs with Creative Commons licenses, the Faster-RCNN based CNN object detector was applied to detect all depicted faces in each retrieved photo.",
          "visual_anchor": "red bounding box from CNN face detection in image (a)",
          "text_evidence": "Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why must faces progress from the red box detection to landmark points before reaching the rectified stage?",
          "answer": "The affine transformation for rectification requires center points of both eyes, which are obtained from DLIB landmark extraction performed on the detected face region.",
          "visual_anchor": "progression from red box (a) through landmark points (b) to rectified face (c)",
          "text_evidence": "Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red bounding box in stage (a) need minimum 50x50 pixels before landmark extraction?",
        "answer": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded to ensure sufficient resolution for reliable processing and feature extraction.",
        "visual_anchor": "red bounding box in image (a) showing face detection region",
        "text_evidence": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded.",
        "query_type": "value_context"
      },
      {
        "query": "How does the Faster-RCNN detector producing the red box relate to the YFCC-100M photo retrieval process?",
        "answer": "After downloading photos from active YFCC-100M URLs with Creative Commons licenses, the Faster-RCNN based CNN object detector was applied to detect all depicted faces in each retrieved photo.",
        "visual_anchor": "red bounding box from CNN face detection in image (a)",
        "text_evidence": "Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why must faces progress from the red box detection to landmark points before reaching the rectified stage?",
        "answer": "The affine transformation for rectification requires center points of both eyes, which are obtained from DLIB landmark extraction performed on the detected face region.",
        "visual_anchor": "progression from red box (a) through landmark points (b) to rectified face (c)",
        "text_evidence": "Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "red bounding box around face region",
      "frontal face photo (a)",
      "face with landmark points (b)",
      "rectified face (c)",
      "50x50 pixel minimum region size",
      "30 pixel inter-ocular distance threshold"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_4",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig3.jpg",
    "caption": "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"68 small dots representing DLIB key-points\",\n    \"19 large yellow dots representing facial landmarks with labels (tn, or, ps, ex, en, pi, n, al, sbal, sn, cph, ch, sto, li, go, gn, zy)\",\n    \"Green small dots forming outline\",\n    \"Red rectangular boundary box\",\n    \"Directional arrows with distance labels (zy-zy, ex-ex, ex-en, en-en, or-pi, ps-pi, n-sto, sn-c, sn-sto, sbal-ls, ls-sto, sto-li, sto-gn, cph-cph, al-al, ch-ch, go-go, tn-n, n-sn, sn-gn, tn-gn)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why are the 68 small dots used as the basis for localizing the 19 large yellow labeled landmarks?\",\n      \"answer\": \"The 68 key-points extracted using DLIB from each face serve as the foundation to map and localize the 19 facial landmarks that were employed for extracting craniofacial measures for coding schemes 1-3.\",\n      \"visual_anchor\": \"68 small dots and 19 large yellow dots with labels like tn, or, ps, ex, en\",\n      \"text_evidence\": \"Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the vertical distance measures labeled n-sto and sto-gn relate to the eight characterizing measures?\",\n      \"answer\": \"These vertical distances are part of the eight measures from coding scheme 1 that characterize all vertical distances between face elements: forehead top, eyes, nose, mouth, and chin.\",\n      \"visual_anchor\": \"Vertical arrows labeled n-sto and sto-gn on the left side of the face\",\n      \"text_evidence\": \"It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why were the landmarks tn and sto marked despite DLIB providing 68 key-points for face detection?\",\n      \"answer\": \"The landmarks tn and sto were required for the craniofacial coding scheme but were not part of the original 68 DLIB key-points set, necessitating additional localization.\",\n      \"visual_anchor\": \"Large yellow dots labeled tn at the top and sto near the mouth\",\n      \"text_evidence\": \"We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "68 small dots representing DLIB key-points",
        "19 large yellow dots representing facial landmarks with labels (tn, or, ps, ex, en, pi, n, al, sbal, sn, cph, ch, sto, li, go, gn, zy)",
        "Green small dots forming outline",
        "Red rectangular boundary box",
        "Directional arrows with distance labels (zy-zy, ex-ex, ex-en, en-en, or-pi, ps-pi, n-sto, sn-c, sn-sto, sbal-ls, ls-sto, sto-li, sto-gn, cph-cph, al-al, ch-ch, go-go, tn-n, n-sn, sn-gn, tn-gn)"
      ],
      "queries": [
        {
          "query": "Why are the 68 small dots used as the basis for localizing the 19 large yellow labeled landmarks?",
          "answer": "The 68 key-points extracted using DLIB from each face serve as the foundation to map and localize the 19 facial landmarks that were employed for extracting craniofacial measures for coding schemes 1-3.",
          "visual_anchor": "68 small dots and 19 large yellow dots with labels like tn, or, ps, ex, en",
          "text_evidence": "Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.",
          "query_type": "value_context"
        },
        {
          "query": "How do the vertical distance measures labeled n-sto and sto-gn relate to the eight characterizing measures?",
          "answer": "These vertical distances are part of the eight measures from coding scheme 1 that characterize all vertical distances between face elements: forehead top, eyes, nose, mouth, and chin.",
          "visual_anchor": "Vertical arrows labeled n-sto and sto-gn on the left side of the face",
          "text_evidence": "It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why were the landmarks tn and sto marked despite DLIB providing 68 key-points for face detection?",
          "answer": "The landmarks tn and sto were required for the craniofacial coding scheme but were not part of the original 68 DLIB key-points set, necessitating additional localization.",
          "visual_anchor": "Large yellow dots labeled tn at the top and sto near the mouth",
          "text_evidence": "We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why are the 68 small dots used as the basis for localizing the 19 large yellow labeled landmarks?",
        "answer": "The 68 key-points extracted using DLIB from each face serve as the foundation to map and localize the 19 facial landmarks that were employed for extracting craniofacial measures for coding schemes 1-3.",
        "visual_anchor": "68 small dots and 19 large yellow dots with labels like tn, or, ps, ex, en",
        "text_evidence": "Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.",
        "query_type": "value_context"
      },
      {
        "query": "How do the vertical distance measures labeled n-sto and sto-gn relate to the eight characterizing measures?",
        "answer": "These vertical distances are part of the eight measures from coding scheme 1 that characterize all vertical distances between face elements: forehead top, eyes, nose, mouth, and chin.",
        "visual_anchor": "Vertical arrows labeled n-sto and sto-gn on the left side of the face",
        "text_evidence": "It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why were the landmarks tn and sto marked despite DLIB providing 68 key-points for face detection?",
        "answer": "The landmarks tn and sto were required for the craniofacial coding scheme but were not part of the original 68 DLIB key-points set, necessitating additional localization.",
        "visual_anchor": "Large yellow dots labeled tn at the top and sto near the mouth",
        "text_evidence": "We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "68 small dots representing DLIB key-points",
      "19 large yellow dots representing facial landmarks with labels (tn, or, ps, ex, en, pi, n, al, sbal, sn, cph, ch, sto, li, go, gn, zy)",
      "Green small dots forming outline",
      "Red rectangular boundary box",
      "Directional arrows with distance labels (zy-zy, ex-ex, ex-en, en-en, or-pi, ps-pi, n-sto, sn-c, sn-sto, sbal-ls, ls-sto, sto-li, sto-gn, cph-cph, al-al, ch-ch, go-go, tn-n, n-sn, sn-gn, tn-gn)"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg",
    "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Face midline (vertical dashed line)\",\n    \"Yellow point C1 (left inner canthus)\",\n    \"Yellow point C2 (right inner canthus)\",\n    \"Yellow point C3 (philtrum)\",\n    \"Blue line segment 'a' connecting C1 and C2\",\n    \"Blue line segment 'b' connecting C3 to midpoint of segment a\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the blue segment 'a' connecting yellow points C1 and C2 relate to craniofacial ratio features?\",\n      \"answer\": \"The craniofacial ratio features used mapped DLIB key-points as facial landmarks, with eight dimensions summarized in Table 8, similar to the inner canthus points shown.\",\n      \"visual_anchor\": \"Blue line segment 'a' connecting yellow points C1 and C2\",\n      \"text_evidence\": \"Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are reference points C1, C2, and C3 marked in yellow before applying the Sobel filter?\",\n      \"answer\": \"These reference points for inner canthus and philtrum are extracted first to establish facial symmetry measures, which psychology and anthropology studies have found to be significant.\",\n      \"visual_anchor\": \"Yellow points C1, C2, and C3 with connecting line segments\",\n      \"text_evidence\": \"Facial symmetry has been found in psychology and anthropology studies to be\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the vertical dashed midline shown support measuring age progression features used for 0 to 18 age groups?\",\n      \"answer\": \"The midline enables facial symmetry measurement, but the age progression features for 0-18 age groups specifically used craniofacial ratios from the third coding scheme, not symmetry.\",\n      \"visual_anchor\": \"Vertical dashed midline dividing the face\",\n      \"text_evidence\": \"The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4].\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Face midline (vertical dashed line)",
        "Yellow point C1 (left inner canthus)",
        "Yellow point C2 (right inner canthus)",
        "Yellow point C3 (philtrum)",
        "Blue line segment 'a' connecting C1 and C2",
        "Blue line segment 'b' connecting C3 to midpoint of segment a"
      ],
      "queries": [
        {
          "query": "How does the blue segment 'a' connecting yellow points C1 and C2 relate to craniofacial ratio features?",
          "answer": "The craniofacial ratio features used mapped DLIB key-points as facial landmarks, with eight dimensions summarized in Table 8, similar to the inner canthus points shown.",
          "visual_anchor": "Blue line segment 'a' connecting yellow points C1 and C2",
          "text_evidence": "Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.",
          "query_type": "value_context"
        },
        {
          "query": "Why are reference points C1, C2, and C3 marked in yellow before applying the Sobel filter?",
          "answer": "These reference points for inner canthus and philtrum are extracted first to establish facial symmetry measures, which psychology and anthropology studies have found to be significant.",
          "visual_anchor": "Yellow points C1, C2, and C3 with connecting line segments",
          "text_evidence": "Facial symmetry has been found in psychology and anthropology studies to be",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the vertical dashed midline shown support measuring age progression features used for 0 to 18 age groups?",
          "answer": "The midline enables facial symmetry measurement, but the age progression features for 0-18 age groups specifically used craniofacial ratios from the third coding scheme, not symmetry.",
          "visual_anchor": "Vertical dashed midline dividing the face",
          "text_evidence": "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4].",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the blue segment 'a' connecting yellow points C1 and C2 relate to craniofacial ratio features?",
        "answer": "The craniofacial ratio features used mapped DLIB key-points as facial landmarks, with eight dimensions summarized in Table 8, similar to the inner canthus points shown.",
        "visual_anchor": "Blue line segment 'a' connecting yellow points C1 and C2",
        "text_evidence": "Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.",
        "query_type": "value_context"
      },
      {
        "query": "Why are reference points C1, C2, and C3 marked in yellow before applying the Sobel filter?",
        "answer": "These reference points for inner canthus and philtrum are extracted first to establish facial symmetry measures, which psychology and anthropology studies have found to be significant.",
        "visual_anchor": "Yellow points C1, C2, and C3 with connecting line segments",
        "text_evidence": "Facial symmetry has been found in psychology and anthropology studies to be",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the vertical dashed midline shown support measuring age progression features used for 0 to 18 age groups?",
        "answer": "The midline enables facial symmetry measurement, but the age progression features for 0-18 age groups specifically used craniofacial ratios from the third coding scheme, not symmetry.",
        "visual_anchor": "Vertical dashed midline dividing the face",
        "text_evidence": "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4].",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Face midline (vertical dashed line)",
      "Yellow point C1 (left inner canthus)",
      "Yellow point C2 (right inner canthus)",
      "Yellow point C3 (philtrum)",
      "Blue line segment 'a' connecting C1 and C2",
      "Blue line segment 'b' connecting C3 to midpoint of segment a"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_8",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg",
    "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"red outer polygons around eyes and mouth\",\n    \"green inner polygons/arrows within the red regions\",\n    \"three distinct facial regions: left eye, right eye, and mouth\",\n    \"directional arrows showing inner-outer measurement zones\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the red outer and green inner regions around the mouth support the cross-cultural age perception findings?\",\n      \"answer\": \"Yes, the contrast measurement between outer (red) and inner (green) mouth regions implements the facial contrast method validated across Chinese, Latin American, and black South African women aged 20-80, showing high-contrast faces appear younger.\",\n      \"visual_anchor\": \"red outer polygon and green inner arrow around the mouth region\",\n      \"text_evidence\": \"An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast face\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why are three separate red-green region pairs shown instead of a single full-face contrast measurement?\",\n      \"answer\": \"The three pairs (eyes, eyebrows, lips) enable computation based on average pixel intensity differences between outer and inner regions for each facial feature separately, rather than a single aggregate measure.\",\n      \"visual_anchor\": \"three distinct red outer and green inner region pairs at left eye, right eye, and mouth\",\n      \"text_evidence\": \"The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the red and green contrast zones relate to the limitation that no universal skin measure exists?\",\n      \"answer\": \"The contrast zones measure relative intensity differences between facial regions rather than absolute skin characteristics, addressing the challenge that there is no universal measure for skin despite its large fraction of facial area.\",\n      \"visual_anchor\": \"red outer boundaries and green inner regions defining contrast measurement zones\",\n      \"text_evidence\": \"Skin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for s\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "red outer polygons around eyes and mouth",
        "green inner polygons/arrows within the red regions",
        "three distinct facial regions: left eye, right eye, and mouth",
        "directional arrows showing inner-outer measurement zones"
      ],
      "queries": [
        {
          "query": "Do the red outer and green inner regions around the mouth support the cross-cultural age perception findings?",
          "answer": "Yes, the contrast measurement between outer (red) and inner (green) mouth regions implements the facial contrast method validated across Chinese, Latin American, and black South African women aged 20-80, showing high-contrast faces appear younger.",
          "visual_anchor": "red outer polygon and green inner arrow around the mouth region",
          "text_evidence": "An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast face",
          "query_type": "value_context"
        },
        {
          "query": "Why are three separate red-green region pairs shown instead of a single full-face contrast measurement?",
          "answer": "The three pairs (eyes, eyebrows, lips) enable computation based on average pixel intensity differences between outer and inner regions for each facial feature separately, rather than a single aggregate measure.",
          "visual_anchor": "three distinct red outer and green inner region pairs at left eye, right eye, and mouth",
          "text_evidence": "The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the red and green contrast zones relate to the limitation that no universal skin measure exists?",
          "answer": "The contrast zones measure relative intensity differences between facial regions rather than absolute skin characteristics, addressing the challenge that there is no universal measure for skin despite its large fraction of facial area.",
          "visual_anchor": "red outer boundaries and green inner regions defining contrast measurement zones",
          "text_evidence": "Skin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for s",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the red outer and green inner regions around the mouth support the cross-cultural age perception findings?",
        "answer": "Yes, the contrast measurement between outer (red) and inner (green) mouth regions implements the facial contrast method validated across Chinese, Latin American, and black South African women aged 20-80, showing high-contrast faces appear younger.",
        "visual_anchor": "red outer polygon and green inner arrow around the mouth region",
        "text_evidence": "An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast face",
        "query_type": "value_context"
      },
      {
        "query": "Why are three separate red-green region pairs shown instead of a single full-face contrast measurement?",
        "answer": "The three pairs (eyes, eyebrows, lips) enable computation based on average pixel intensity differences between outer and inner regions for each facial feature separately, rather than a single aggregate measure.",
        "visual_anchor": "three distinct red outer and green inner region pairs at left eye, right eye, and mouth",
        "text_evidence": "The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the red and green contrast zones relate to the limitation that no universal skin measure exists?",
        "answer": "The contrast zones measure relative intensity differences between facial regions rather than absolute skin characteristics, addressing the challenge that there is no universal measure for skin despite its large fraction of facial area.",
        "visual_anchor": "red outer boundaries and green inner regions defining contrast measurement zones",
        "text_evidence": "Skin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for s",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "red outer polygons around eyes and mouth",
      "green inner polygons/arrows within the red regions",
      "three distinct facial regions: left eye, right eye, and mouth",
      "directional arrows showing inner-outer measurement zones"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_14",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg",
    "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Input face (a) as thermal-style image\",\n    \"Skin map (b) showing segmented regions\",\n    \"L channel (c) showing luminance\",\n    \"a channel (d) showing red-green axis\",\n    \"b channel (e) showing blue-yellow axis\",\n    \"ITA map (f) showing angle-based values\",\n    \"Masked ITA map (g) with skin regions only\",\n    \"ITA histogram (h) showing distribution\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the masked ITA map in step (g) enable extraction of Individual Typology Angle-based skin color?\",\n      \"answer\": \"The masked ITA map (g) isolates skin regions from the full ITA map (f), allowing the histogram (h) to compute ITA values exclusively from facial skin pixels for coding scheme 6.\",\n      \"visual_anchor\": \"masked ITA map labeled (g)\",\n      \"text_evidence\": \"Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA).\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why are channels (d) and (e) necessary before generating the ITA map shown in step (f)?\",\n      \"answer\": \"The a channel (d) and b channel (e) provide the red-green and blue-yellow color axes required to calculate the Individual Typology Angle, which is then visualized in the ITA map (f).\",\n      \"visual_anchor\": \"channels (d) and (e) preceding ITA map (f)\",\n      \"text_evidence\": \"(a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the eight-step pipeline ending in histogram (h) contrast with the CNN approach for predicting age attributes?\",\n      \"answer\": \"Yes, the ITA-based pipeline uses explicit color channel decomposition and geometric angle computation for skin color, while the CNN for age prediction learns feature representations directly from face images without manual feature design.\",\n      \"visual_anchor\": \"histogram (h) as final step\",\n      \"text_evidence\": \"As an alternative to designing specific feature representations for predicting age, for coding scheme 7, we adopt a Convolutional Neural Network (CNN) that is trained from face images to predict age.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Input face (a) as thermal-style image",
        "Skin map (b) showing segmented regions",
        "L channel (c) showing luminance",
        "a channel (d) showing red-green axis",
        "b channel (e) showing blue-yellow axis",
        "ITA map (f) showing angle-based values",
        "Masked ITA map (g) with skin regions only",
        "ITA histogram (h) showing distribution"
      ],
      "queries": [
        {
          "query": "How does the masked ITA map in step (g) enable extraction of Individual Typology Angle-based skin color?",
          "answer": "The masked ITA map (g) isolates skin regions from the full ITA map (f), allowing the histogram (h) to compute ITA values exclusively from facial skin pixels for coding scheme 6.",
          "visual_anchor": "masked ITA map labeled (g)",
          "text_evidence": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA).",
          "query_type": "visual_definition"
        },
        {
          "query": "Why are channels (d) and (e) necessary before generating the ITA map shown in step (f)?",
          "answer": "The a channel (d) and b channel (e) provide the red-green and blue-yellow color axes required to calculate the Individual Typology Angle, which is then visualized in the ITA map (f).",
          "visual_anchor": "channels (d) and (e) preceding ITA map (f)",
          "text_evidence": "(a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the eight-step pipeline ending in histogram (h) contrast with the CNN approach for predicting age attributes?",
          "answer": "Yes, the ITA-based pipeline uses explicit color channel decomposition and geometric angle computation for skin color, while the CNN for age prediction learns feature representations directly from face images without manual feature design.",
          "visual_anchor": "histogram (h) as final step",
          "text_evidence": "As an alternative to designing specific feature representations for predicting age, for coding scheme 7, we adopt a Convolutional Neural Network (CNN) that is trained from face images to predict age.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the masked ITA map in step (g) enable extraction of Individual Typology Angle-based skin color?",
        "answer": "The masked ITA map (g) isolates skin regions from the full ITA map (f), allowing the histogram (h) to compute ITA values exclusively from facial skin pixels for coding scheme 6.",
        "visual_anchor": "masked ITA map labeled (g)",
        "text_evidence": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA).",
        "query_type": "visual_definition"
      },
      {
        "query": "Why are channels (d) and (e) necessary before generating the ITA map shown in step (f)?",
        "answer": "The a channel (d) and b channel (e) provide the red-green and blue-yellow color axes required to calculate the Individual Typology Angle, which is then visualized in the ITA map (f).",
        "visual_anchor": "channels (d) and (e) preceding ITA map (f)",
        "text_evidence": "(a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the eight-step pipeline ending in histogram (h) contrast with the CNN approach for predicting age attributes?",
        "answer": "Yes, the ITA-based pipeline uses explicit color channel decomposition and geometric angle computation for skin color, while the CNN for age prediction learns feature representations directly from face images without manual feature design.",
        "visual_anchor": "histogram (h) as final step",
        "text_evidence": "As an alternative to designing specific feature representations for predicting age, for coding scheme 7, we adopt a Convolutional Neural Network (CNN) that is trained from face images to predict age.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Input face (a) as thermal-style image",
      "Skin map (b) showing segmented regions",
      "L channel (c) showing luminance",
      "a channel (d) showing red-green axis",
      "b channel (e) showing blue-yellow axis",
      "ITA map (f) showing angle-based values",
      "Masked ITA map (g) with skin regions only",
      "ITA histogram (h) showing distribution"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_18",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg",
    "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Horizontal red line at y=1.00\",\n    \"Horizontal blue line at y=1.00\",\n    \"Y-axis labeled 'Evenness: Uniform Distribution'\",\n    \"X-axis labeled '# of Classes' ranging from 2 to 20\",\n    \"Legend showing 'Simpson E' and 'Shannon E'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the horizontal lines at 1.00 for both Simpson E and Shannon E confirm balanced evenness across all class counts?\",\n      \"answer\": \"Yes, the perfect 1.00 evenness values across all class counts demonstrate a uniform distribution where all classes have equal representation, confirming balanced evenness.\",\n      \"visual_anchor\": \"Horizontal red and blue lines both at y=1.00\",\n      \"text_evidence\": \"Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the uniform distribution maintain constant evenness at 1.00 while craniofacial distances achieve only 0.981 Simpson E maximum?\",\n      \"answer\": \"The uniform distribution represents perfect theoretical balance with equal class representation, while real craniofacial distance measurements show slight natural variation, preventing perfect evenness.\",\n      \"visual_anchor\": \"Red Simpson E line at constant 1.00 value\",\n      \"text_evidence\": \"The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the perfect overlap of red and blue lines at 1.00 relate to typical class mapping from Shannon H?\",\n      \"answer\": \"The perfect 1.00 evenness overlap illustrates ideal uniform distribution, contrasting with the craniofacial scheme where Shannon H values of 1.777-1.782 typically map to 6 classes rather than perfect uniformity.\",\n      \"visual_anchor\": \"Overlapping red Simpson E and blue Shannon E lines at y=1.00\",\n      \"text_evidence\": \"Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Horizontal red line at y=1.00",
        "Horizontal blue line at y=1.00",
        "Y-axis labeled 'Evenness: Uniform Distribution'",
        "X-axis labeled '# of Classes' ranging from 2 to 20",
        "Legend showing 'Simpson E' and 'Shannon E'"
      ],
      "queries": [
        {
          "query": "Do the horizontal lines at 1.00 for both Simpson E and Shannon E confirm balanced evenness across all class counts?",
          "answer": "Yes, the perfect 1.00 evenness values across all class counts demonstrate a uniform distribution where all classes have equal representation, confirming balanced evenness.",
          "visual_anchor": "Horizontal red and blue lines both at y=1.00",
          "text_evidence": "Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the uniform distribution maintain constant evenness at 1.00 while craniofacial distances achieve only 0.981 Simpson E maximum?",
          "answer": "The uniform distribution represents perfect theoretical balance with equal class representation, while real craniofacial distance measurements show slight natural variation, preventing perfect evenness.",
          "visual_anchor": "Red Simpson E line at constant 1.00 value",
          "text_evidence": "The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the perfect overlap of red and blue lines at 1.00 relate to typical class mapping from Shannon H?",
          "answer": "The perfect 1.00 evenness overlap illustrates ideal uniform distribution, contrasting with the craniofacial scheme where Shannon H values of 1.777-1.782 typically map to 6 classes rather than perfect uniformity.",
          "visual_anchor": "Overlapping red Simpson E and blue Shannon E lines at y=1.00",
          "text_evidence": "Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the horizontal lines at 1.00 for both Simpson E and Shannon E confirm balanced evenness across all class counts?",
        "answer": "Yes, the perfect 1.00 evenness values across all class counts demonstrate a uniform distribution where all classes have equal representation, confirming balanced evenness.",
        "visual_anchor": "Horizontal red and blue lines both at y=1.00",
        "text_evidence": "Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the uniform distribution maintain constant evenness at 1.00 while craniofacial distances achieve only 0.981 Simpson E maximum?",
        "answer": "The uniform distribution represents perfect theoretical balance with equal class representation, while real craniofacial distance measurements show slight natural variation, preventing perfect evenness.",
        "visual_anchor": "Red Simpson E line at constant 1.00 value",
        "text_evidence": "The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the perfect overlap of red and blue lines at 1.00 relate to typical class mapping from Shannon H?",
        "answer": "The perfect 1.00 evenness overlap illustrates ideal uniform distribution, contrasting with the craniofacial scheme where Shannon H values of 1.777-1.782 typically map to 6 classes rather than perfect uniformity.",
        "visual_anchor": "Overlapping red Simpson E and blue Shannon E lines at y=1.00",
        "text_evidence": "Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Horizontal red line at y=1.00",
      "Horizontal blue line at y=1.00",
      "Y-axis labeled 'Evenness: Uniform Distribution'",
      "X-axis labeled '# of Classes' ranging from 2 to 20",
      "Legend showing 'Simpson E' and 'Shannon E'"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_26",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg",
    "caption": "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bar chart with dark blue bars\",\n    \"Y-axis labeled 'num faces' ranging 0 to 2.5 × 10^5\",\n    \"X-axis labeled 'value' ranging 2 to 10\",\n    \"Highest bar at value 4 reaching approximately 2 × 10^5\",\n    \"Lowest bar at value 8 reaching approximately 1.3 × 10^5\",\n    \"Title 'sto-li' at top\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the bar height of approximately 2×10^5 at value 4 represent one of eight craniofacial distances?\",\n      \"answer\": \"Yes, the bar represents the frequency distribution of one of the 8 craniofacial distances in coding scheme 1, as the figure summarizes feature distribution for all eight dimensions.\",\n      \"visual_anchor\": \"Highest bar at value 4 reaching approximately 2×10^5\",\n      \"text_evidence\": \"Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the bar at value 8 reach only 1.3×10^5 while coding scheme 2 shows similar ranges?\",\n      \"answer\": \"Both coding schemes map to similar value ranges (around 6 classes), suggesting comparable discretization despite different feature types. The bar height reflects frequency distribution within the DiF dataset.\",\n      \"visual_anchor\": \"Bar at value 8 reaching approximately 1.3×10^5\",\n      \"text_evidence\": \"Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the distribution shown by bars from value 4 to 8 relate to the twelve craniofacial area features?\",\n      \"answer\": \"The distribution represents coding scheme 1 (craniofacial distances), not the twelve area features. The area features are summarized separately in coding scheme 2 with their own distinct dimensions.\",\n      \"visual_anchor\": \"Bars spanning values 4 to 8 on x-axis\",\n      \"text_evidence\": \"Table 7 summarizes the twelve dimensions of the craniofacial area features.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bar chart with dark blue bars",
        "Y-axis labeled 'num faces' ranging 0 to 2.5 × 10^5",
        "X-axis labeled 'value' ranging 2 to 10",
        "Highest bar at value 4 reaching approximately 2 × 10^5",
        "Lowest bar at value 8 reaching approximately 1.3 × 10^5",
        "Title 'sto-li' at top"
      ],
      "queries": [
        {
          "query": "Does the bar height of approximately 2×10^5 at value 4 represent one of eight craniofacial distances?",
          "answer": "Yes, the bar represents the frequency distribution of one of the 8 craniofacial distances in coding scheme 1, as the figure summarizes feature distribution for all eight dimensions.",
          "visual_anchor": "Highest bar at value 4 reaching approximately 2×10^5",
          "text_evidence": "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the bar at value 8 reach only 1.3×10^5 while coding scheme 2 shows similar ranges?",
          "answer": "Both coding schemes map to similar value ranges (around 6 classes), suggesting comparable discretization despite different feature types. The bar height reflects frequency distribution within the DiF dataset.",
          "visual_anchor": "Bar at value 8 reaching approximately 1.3×10^5",
          "text_evidence": "Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the distribution shown by bars from value 4 to 8 relate to the twelve craniofacial area features?",
          "answer": "The distribution represents coding scheme 1 (craniofacial distances), not the twelve area features. The area features are summarized separately in coding scheme 2 with their own distinct dimensions.",
          "visual_anchor": "Bars spanning values 4 to 8 on x-axis",
          "text_evidence": "Table 7 summarizes the twelve dimensions of the craniofacial area features.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the bar height of approximately 2×10^5 at value 4 represent one of eight craniofacial distances?",
        "answer": "Yes, the bar represents the frequency distribution of one of the 8 craniofacial distances in coding scheme 1, as the figure summarizes feature distribution for all eight dimensions.",
        "visual_anchor": "Highest bar at value 4 reaching approximately 2×10^5",
        "text_evidence": "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the bar at value 8 reach only 1.3×10^5 while coding scheme 2 shows similar ranges?",
        "answer": "Both coding schemes map to similar value ranges (around 6 classes), suggesting comparable discretization despite different feature types. The bar height reflects frequency distribution within the DiF dataset.",
        "visual_anchor": "Bar at value 8 reaching approximately 1.3×10^5",
        "text_evidence": "Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the distribution shown by bars from value 4 to 8 relate to the twelve craniofacial area features?",
        "answer": "The distribution represents coding scheme 1 (craniofacial distances), not the twelve area features. The area features are summarized separately in coding scheme 2 with their own distinct dimensions.",
        "visual_anchor": "Bars spanning values 4 to 8 on x-axis",
        "text_evidence": "Table 7 summarizes the twelve dimensions of the craniofacial area features.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bar chart with dark blue bars",
      "Y-axis labeled 'num faces' ranging 0 to 2.5 × 10^5",
      "X-axis labeled 'value' ranging 2 to 10",
      "Highest bar at value 4 reaching approximately 2 × 10^5",
      "Lowest bar at value 8 reaching approximately 1.3 × 10^5",
      "Title 'sto-li' at top"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_38",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg",
    "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"bar chart with 6 bars\", \"x-axis labeled 'value' ranging 20-27\", \"y-axis labeled 'num faces' scaled ×10^5\", \"tallest bar at value 20 reaching ~1.9×10^5\", \"shortest bar at value 22 reaching ~1.2×10^5\", \"title 'n-sn'\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the bar at value 20 reaching 1.9×10^5 faces demonstrate the feature distribution for craniofacial areas?\",\n      \"answer\": \"Yes, the bar at value 20 shows the highest frequency in the craniofacial area feature distribution for coding scheme 2 in the DiF dataset.\",\n      \"visual_anchor\": \"tallest bar at value 20 reaching approximately 1.9×10^5\",\n      \"text_evidence\": \"Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the variation between the tallest bar at 1.9×10^5 and shortest at 1.2×10^5 compare to age progression measures?\",\n      \"answer\": \"The variation in bar heights reflects craniofacial ratios similar to those used for estimating age progression from faces in age groups 0 to 18.\",\n      \"visual_anchor\": \"difference between tallest bar (~1.9×10^5 at value 20) and shortest bar (~1.2×10^5 at value 22)\",\n      \"text_evidence\": \"These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do all bars remain between 1.2 and 1.9×10^5 if coding scheme 3 uses eight craniofacial measures?\",\n      \"answer\": \"This figure shows coding scheme 2 with 12 craniofacial areas, not coding scheme 3. Scheme 3 uses different ratio-based measures across the face.\",\n      \"visual_anchor\": \"all six bars constrained between 1.2×10^5 and 1.9×10^5\",\n      \"text_evidence\": \"Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "bar chart with 6 bars",
        "x-axis labeled 'value' ranging 20-27",
        "y-axis labeled 'num faces' scaled ×10^5",
        "tallest bar at value 20 reaching ~1.9×10^5",
        "shortest bar at value 22 reaching ~1.2×10^5",
        "title 'n-sn'"
      ],
      "queries": [
        {
          "query": "Does the bar at value 20 reaching 1.9×10^5 faces demonstrate the feature distribution for craniofacial areas?",
          "answer": "Yes, the bar at value 20 shows the highest frequency in the craniofacial area feature distribution for coding scheme 2 in the DiF dataset.",
          "visual_anchor": "tallest bar at value 20 reaching approximately 1.9×10^5",
          "text_evidence": "Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2.",
          "query_type": "value_context"
        },
        {
          "query": "How does the variation between the tallest bar at 1.9×10^5 and shortest at 1.2×10^5 compare to age progression measures?",
          "answer": "The variation in bar heights reflects craniofacial ratios similar to those used for estimating age progression from faces in age groups 0 to 18.",
          "visual_anchor": "difference between tallest bar (~1.9×10^5 at value 20) and shortest bar (~1.2×10^5 at value 22)",
          "text_evidence": "These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do all bars remain between 1.2 and 1.9×10^5 if coding scheme 3 uses eight craniofacial measures?",
          "answer": "This figure shows coding scheme 2 with 12 craniofacial areas, not coding scheme 3. Scheme 3 uses different ratio-based measures across the face.",
          "visual_anchor": "all six bars constrained between 1.2×10^5 and 1.9×10^5",
          "text_evidence": "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the bar at value 20 reaching 1.9×10^5 faces demonstrate the feature distribution for craniofacial areas?",
        "answer": "Yes, the bar at value 20 shows the highest frequency in the craniofacial area feature distribution for coding scheme 2 in the DiF dataset.",
        "visual_anchor": "tallest bar at value 20 reaching approximately 1.9×10^5",
        "text_evidence": "Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2.",
        "query_type": "value_context"
      },
      {
        "query": "How does the variation between the tallest bar at 1.9×10^5 and shortest at 1.2×10^5 compare to age progression measures?",
        "answer": "The variation in bar heights reflects craniofacial ratios similar to those used for estimating age progression from faces in age groups 0 to 18.",
        "visual_anchor": "difference between tallest bar (~1.9×10^5 at value 20) and shortest bar (~1.2×10^5 at value 22)",
        "text_evidence": "These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do all bars remain between 1.2 and 1.9×10^5 if coding scheme 3 uses eight craniofacial measures?",
        "answer": "This figure shows coding scheme 2 with 12 craniofacial areas, not coding scheme 3. Scheme 3 uses different ratio-based measures across the face.",
        "visual_anchor": "all six bars constrained between 1.2×10^5 and 1.9×10^5",
        "text_evidence": "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "bar chart with 6 bars",
      "x-axis labeled 'value' ranging 20-27",
      "y-axis labeled 'num faces' scaled ×10^5",
      "tallest bar at value 20 reaching ~1.9×10^5",
      "shortest bar at value 22 reaching ~1.2×10^5",
      "title 'n-sn'"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_47",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg",
    "caption": "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bar chart with 5 blue bars\",\n    \"Y-axis labeled 'num:faces' ranging from 0 to 2 × 10^5\",\n    \"X-axis labeled 'value' with positions 0.5, 0.6, 0.7, 0.8, 0.9\",\n    \"Title '(ls-sto)/(sto-li)'\",\n    \"Tallest bars at positions 0.5 and 0.9 reaching approximately 1.9 × 10^5\",\n    \"Shortest bar at position 0.8 reaching approximately 1.3 × 10^5\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the bar at value 0.5 reaching approximately 190,000 faces support the Simpson D value of 5.902 being the largest?\",\n      \"answer\": \"Yes, the high frequency at 0.5 indicates strong class separation, which aligns with the largest Simpson D value of 5.902 mentioned for craniofacial ratios in coding scheme 3.\",\n      \"visual_anchor\": \"leftmost bar at value 0.5 with height approximately 1.9 × 10^5\",\n      \"text_evidence\": \"Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the bar at value 0.8 show the lowest frequency despite Shannon H values mapping to approximately 6 classes?\",\n      \"answer\": \"The low frequency at 0.8 reflects natural variation in craniofacial ratio distributions across the 6 classes, while Shannon H values of approximately 1.783 to 1.781 indicate overall entropy across all ratio bins.\",\n      \"visual_anchor\": \"bar at value 0.8 with height approximately 1.3 × 10^5, the shortest among all bars\",\n      \"text_evidence\": \"Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the distribution pattern with peaks at 0.5 and 0.9 relate to computing facial symmetry measures using density difference?\",\n      \"answer\": \"The bimodal distribution at 0.5 and 0.9 likely reflects the craniofacial ratio (ls-sto)/(sto-li) computed from facial symmetry measures, where extreme values indicate distinct facial structure categories in the left and right 128x64 parts.\",\n      \"visual_anchor\": \"two tallest bars at values 0.5 and 0.9, both reaching approximately 1.9 × 10^5\",\n      \"text_evidence\": \"Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bar chart with 5 blue bars",
        "Y-axis labeled 'num:faces' ranging from 0 to 2 × 10^5",
        "X-axis labeled 'value' with positions 0.5, 0.6, 0.7, 0.8, 0.9",
        "Title '(ls-sto)/(sto-li)'",
        "Tallest bars at positions 0.5 and 0.9 reaching approximately 1.9 × 10^5",
        "Shortest bar at position 0.8 reaching approximately 1.3 × 10^5"
      ],
      "queries": [
        {
          "query": "Does the bar at value 0.5 reaching approximately 190,000 faces support the Simpson D value of 5.902 being the largest?",
          "answer": "Yes, the high frequency at 0.5 indicates strong class separation, which aligns with the largest Simpson D value of 5.902 mentioned for craniofacial ratios in coding scheme 3.",
          "visual_anchor": "leftmost bar at value 0.5 with height approximately 1.9 × 10^5",
          "text_evidence": "Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the bar at value 0.8 show the lowest frequency despite Shannon H values mapping to approximately 6 classes?",
          "answer": "The low frequency at 0.8 reflects natural variation in craniofacial ratio distributions across the 6 classes, while Shannon H values of approximately 1.783 to 1.781 indicate overall entropy across all ratio bins.",
          "visual_anchor": "bar at value 0.8 with height approximately 1.3 × 10^5, the shortest among all bars",
          "text_evidence": "Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the distribution pattern with peaks at 0.5 and 0.9 relate to computing facial symmetry measures using density difference?",
          "answer": "The bimodal distribution at 0.5 and 0.9 likely reflects the craniofacial ratio (ls-sto)/(sto-li) computed from facial symmetry measures, where extreme values indicate distinct facial structure categories in the left and right 128x64 parts.",
          "visual_anchor": "two tallest bars at values 0.5 and 0.9, both reaching approximately 1.9 × 10^5",
          "text_evidence": "Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the bar at value 0.5 reaching approximately 190,000 faces support the Simpson D value of 5.902 being the largest?",
        "answer": "Yes, the high frequency at 0.5 indicates strong class separation, which aligns with the largest Simpson D value of 5.902 mentioned for craniofacial ratios in coding scheme 3.",
        "visual_anchor": "leftmost bar at value 0.5 with height approximately 1.9 × 10^5",
        "text_evidence": "Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the bar at value 0.8 show the lowest frequency despite Shannon H values mapping to approximately 6 classes?",
        "answer": "The low frequency at 0.8 reflects natural variation in craniofacial ratio distributions across the 6 classes, while Shannon H values of approximately 1.783 to 1.781 indicate overall entropy across all ratio bins.",
        "visual_anchor": "bar at value 0.8 with height approximately 1.3 × 10^5, the shortest among all bars",
        "text_evidence": "Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the distribution pattern with peaks at 0.5 and 0.9 relate to computing facial symmetry measures using density difference?",
        "answer": "The bimodal distribution at 0.5 and 0.9 likely reflects the craniofacial ratio (ls-sto)/(sto-li) computed from facial symmetry measures, where extreme values indicate distinct facial structure categories in the left and right 128x64 parts.",
        "visual_anchor": "two tallest bars at values 0.5 and 0.9, both reaching approximately 1.9 × 10^5",
        "text_evidence": "Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bar chart with 5 blue bars",
      "Y-axis labeled 'num:faces' ranging from 0 to 2 × 10^5",
      "X-axis labeled 'value' with positions 0.5, 0.6, 0.7, 0.8, 0.9",
      "Title '(ls-sto)/(sto-li)'",
      "Tallest bars at positions 0.5 and 0.9 reaching approximately 1.9 × 10^5",
      "Shortest bar at position 0.8 reaching approximately 1.3 × 10^5"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_50",
    "figure_number": 10,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg",
    "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bar chart with y-axis labeled 'num faces' ranging from 0 to 2.5 × 10^5\",\n    \"X-axis showing edge orientation similarity values from -0.01 to 0.025\",\n    \"Seven dark blue bars of varying heights\",\n    \"Tallest bar at approximately 0.005 reaching about 2.35 × 10^5\",\n    \"Second tallest bar at approximately 0.01 reaching about 2.15 × 10^5\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the tallest bar at 0.005 reach 2.35×10^5 faces despite facial symmetry having only middle-range diversity?\",\n      \"answer\": \"The middle-range diversity (Simpson D of 5.510 and Shannon H of 1.748) indicates moderate variation in symmetry features, but the peak at 0.005 shows most faces cluster around this particular edge orientation similarity value, creating a concentrated distribution within the moderate diversity range.\",\n      \"visual_anchor\": \"Tallest bar at x=0.005 reaching approximately 2.35×10^5 on y-axis\",\n      \"text_evidence\": \"Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the bar height at 0.01 compare to the peak, given that evenness values are lower for this coding scheme?\",\n      \"answer\": \"The bar at 0.01 reaches approximately 2.15×10^5, only slightly lower than the peak at 2.35×10^5. The lower evenness values explain this near-peak concentration, indicating the distribution is not flat but clustered around specific similarity values.\",\n      \"visual_anchor\": \"Second tallest bar at x=0.01 reaching approximately 2.15×10^5\",\n      \"text_evidence\": \"The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highe\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What facial feature computation produces the edge orientation similarity distribution shown with peak at 0.005?\",\n      \"answer\": \"The edge orientation similarity is computed using pixel intensity at (x,y) for CIE-Lab channels, comparing outer and inner regions around each facial part to measure symmetry. This computation generates the distribution with its characteristic peak at 0.005.\",\n      \"visual_anchor\": \"Distribution peak at x=0.005 in edge orientation similarity axis\",\n      \"text_evidence\": \"The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bar chart with y-axis labeled 'num faces' ranging from 0 to 2.5 × 10^5",
        "X-axis showing edge orientation similarity values from -0.01 to 0.025",
        "Seven dark blue bars of varying heights",
        "Tallest bar at approximately 0.005 reaching about 2.35 × 10^5",
        "Second tallest bar at approximately 0.01 reaching about 2.15 × 10^5"
      ],
      "queries": [
        {
          "query": "Why does the tallest bar at 0.005 reach 2.35×10^5 faces despite facial symmetry having only middle-range diversity?",
          "answer": "The middle-range diversity (Simpson D of 5.510 and Shannon H of 1.748) indicates moderate variation in symmetry features, but the peak at 0.005 shows most faces cluster around this particular edge orientation similarity value, creating a concentrated distribution within the moderate diversity range.",
          "visual_anchor": "Tallest bar at x=0.005 reaching approximately 2.35×10^5 on y-axis",
          "text_evidence": "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the bar height at 0.01 compare to the peak, given that evenness values are lower for this coding scheme?",
          "answer": "The bar at 0.01 reaches approximately 2.15×10^5, only slightly lower than the peak at 2.35×10^5. The lower evenness values explain this near-peak concentration, indicating the distribution is not flat but clustered around specific similarity values.",
          "visual_anchor": "Second tallest bar at x=0.01 reaching approximately 2.15×10^5",
          "text_evidence": "The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highe",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What facial feature computation produces the edge orientation similarity distribution shown with peak at 0.005?",
          "answer": "The edge orientation similarity is computed using pixel intensity at (x,y) for CIE-Lab channels, comparing outer and inner regions around each facial part to measure symmetry. This computation generates the distribution with its characteristic peak at 0.005.",
          "visual_anchor": "Distribution peak at x=0.005 in edge orientation similarity axis",
          "text_evidence": "The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the tallest bar at 0.005 reach 2.35×10^5 faces despite facial symmetry having only middle-range diversity?",
        "answer": "The middle-range diversity (Simpson D of 5.510 and Shannon H of 1.748) indicates moderate variation in symmetry features, but the peak at 0.005 shows most faces cluster around this particular edge orientation similarity value, creating a concentrated distribution within the moderate diversity range.",
        "visual_anchor": "Tallest bar at x=0.005 reaching approximately 2.35×10^5 on y-axis",
        "text_evidence": "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the bar height at 0.01 compare to the peak, given that evenness values are lower for this coding scheme?",
        "answer": "The bar at 0.01 reaches approximately 2.15×10^5, only slightly lower than the peak at 2.35×10^5. The lower evenness values explain this near-peak concentration, indicating the distribution is not flat but clustered around specific similarity values.",
        "visual_anchor": "Second tallest bar at x=0.01 reaching approximately 2.15×10^5",
        "text_evidence": "The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highe",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What facial feature computation produces the edge orientation similarity distribution shown with peak at 0.005?",
        "answer": "The edge orientation similarity is computed using pixel intensity at (x,y) for CIE-Lab channels, comparing outer and inner regions around each facial part to measure symmetry. This computation generates the distribution with its characteristic peak at 0.005.",
        "visual_anchor": "Distribution peak at x=0.005 in edge orientation similarity axis",
        "text_evidence": "The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bar chart with y-axis labeled 'num faces' ranging from 0 to 2.5 × 10^5",
      "X-axis showing edge orientation similarity values from -0.01 to 0.025",
      "Seven dark blue bars of varying heights",
      "Tallest bar at approximately 0.005 reaching about 2.35 × 10^5",
      "Second tallest bar at approximately 0.01 reaching about 2.15 × 10^5"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_58",
    "figure_number": 11,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg",
    "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue vertical bars showing frequency distribution\",\n    \"X-axis labeled 'value' ranging from -0.02 to 0.01\",\n    \"Y-axis labeled 'num faces' with scale x10^5, ranging from 0 to 2\",\n    \"Seven bars at different value positions\",\n    \"Tallest bar at approximately 0.01 reaching nearly 200,000 faces\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the tallest bar at value 0.01 reaching 200,000 faces support the Shannon E evenness factor of 0.979?\",\n      \"answer\": \"No, the prominent peak at 0.01 contradicts perfect evenness. A Shannon E of 0.979 indicates measures are close to evenly distributed, but the histogram shows clear imbalance with one dominant bar.\",\n      \"visual_anchor\": \"Tallest blue bar at x=0.01 reaching approximately 200,000 on y-axis\",\n      \"text_evidence\": \"The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to ev\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the bar at value -0.01 reaching approximately 100,000 faces relate to the highest Simpson D value of 5.872?\",\n      \"answer\": \"The Simpson D value of 5.872 represents diversity across all bins. The bar at -0.01 is the shortest, contributing to overall diversity by showing lower frequency compared to other contrast values.\",\n      \"visual_anchor\": \"Shortest blue bar at x=-0.01 reaching approximately 100,000 faces\",\n      \"text_evidence\": \"The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do the seven bars span from -0.02 to 0.01 when contrast is computed from a single ITA score?\",\n      \"answer\": \"The contrast values represent differences between facial regions, not the ITA score itself. Averaging ITA values per face produces the baseline, while regional contrasts create the distribution shown spanning negative to positive values.\",\n      \"visual_anchor\": \"Seven blue bars distributed across x-axis from -0.02 to 0.01\",\n      \"text_evidence\": \"Average the values to give a single ITA score for each face\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue vertical bars showing frequency distribution",
        "X-axis labeled 'value' ranging from -0.02 to 0.01",
        "Y-axis labeled 'num faces' with scale x10^5, ranging from 0 to 2",
        "Seven bars at different value positions",
        "Tallest bar at approximately 0.01 reaching nearly 200,000 faces"
      ],
      "queries": [
        {
          "query": "Does the tallest bar at value 0.01 reaching 200,000 faces support the Shannon E evenness factor of 0.979?",
          "answer": "No, the prominent peak at 0.01 contradicts perfect evenness. A Shannon E of 0.979 indicates measures are close to evenly distributed, but the histogram shows clear imbalance with one dominant bar.",
          "visual_anchor": "Tallest blue bar at x=0.01 reaching approximately 200,000 on y-axis",
          "text_evidence": "The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to ev",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the bar at value -0.01 reaching approximately 100,000 faces relate to the highest Simpson D value of 5.872?",
          "answer": "The Simpson D value of 5.872 represents diversity across all bins. The bar at -0.01 is the shortest, contributing to overall diversity by showing lower frequency compared to other contrast values.",
          "visual_anchor": "Shortest blue bar at x=-0.01 reaching approximately 100,000 faces",
          "text_evidence": "The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes.",
          "query_type": "value_context"
        },
        {
          "query": "Why do the seven bars span from -0.02 to 0.01 when contrast is computed from a single ITA score?",
          "answer": "The contrast values represent differences between facial regions, not the ITA score itself. Averaging ITA values per face produces the baseline, while regional contrasts create the distribution shown spanning negative to positive values.",
          "visual_anchor": "Seven blue bars distributed across x-axis from -0.02 to 0.01",
          "text_evidence": "Average the values to give a single ITA score for each face",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the tallest bar at value 0.01 reaching 200,000 faces support the Shannon E evenness factor of 0.979?",
        "answer": "No, the prominent peak at 0.01 contradicts perfect evenness. A Shannon E of 0.979 indicates measures are close to evenly distributed, but the histogram shows clear imbalance with one dominant bar.",
        "visual_anchor": "Tallest blue bar at x=0.01 reaching approximately 200,000 on y-axis",
        "text_evidence": "The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to ev",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the bar at value -0.01 reaching approximately 100,000 faces relate to the highest Simpson D value of 5.872?",
        "answer": "The Simpson D value of 5.872 represents diversity across all bins. The bar at -0.01 is the shortest, contributing to overall diversity by showing lower frequency compared to other contrast values.",
        "visual_anchor": "Shortest blue bar at x=-0.01 reaching approximately 100,000 faces",
        "text_evidence": "The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes.",
        "query_type": "value_context"
      },
      {
        "query": "Why do the seven bars span from -0.02 to 0.01 when contrast is computed from a single ITA score?",
        "answer": "The contrast values represent differences between facial regions, not the ITA score itself. Averaging ITA values per face produces the baseline, while regional contrasts create the distribution shown spanning negative to positive values.",
        "visual_anchor": "Seven blue bars distributed across x-axis from -0.02 to 0.01",
        "text_evidence": "Average the values to give a single ITA score for each face",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue vertical bars showing frequency distribution",
      "X-axis labeled 'value' ranging from -0.02 to 0.01",
      "Y-axis labeled 'num faces' with scale x10^5, ranging from 0 to 2",
      "Seven bars at different value positions",
      "Tallest bar at approximately 0.01 reaching nearly 200,000 faces"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_61",
    "figure_number": 13,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg",
    "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"bar chart with 7 age groups on x-axis\", \"y-axis showing num_faces up to 3.5×10^5\", \"tallest dark blue bar at 20-30 age group reaching ~3.4×10^5\", \"shortest bar at >60 group near 0.2×10^5\", \"title 'age prediction'\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the tallest bar at 20-30 reaching 340,000 faces explain why Simpson D equals 4.368 for this distribution?\",\n      \"answer\": \"Yes, the extreme peak at age 20-30 with 340,000 faces creates an uneven distribution, directly resulting in the calculated Simpson D value of 4.368 and Shannon H value of 1.601.\",\n      \"visual_anchor\": \"tallest dark blue bar at 20-30 age group reaching approximately 3.4×10^5\",\n      \"text_evidence\": \"The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the 20-30 bar exceed the 31-45 bar by over 120,000 faces in coding scheme 7?\",\n      \"answer\": \"The DiF dataset has an uneven age distribution concentrated in the 20-30 range, which is reflected in the seven-group binning structure of coding scheme 7 ([0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]).\",\n      \"visual_anchor\": \"20-30 bar at ~3.4×10^5 compared to 31-45 bar at ~2.1×10^5\",\n      \"text_evidence\": \"summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the minimal height of the >60 bar relate to the feature distribution summary for gender prediction?\",\n      \"answer\": \"The >60 bar's minimal height (under 0.3×10^5) shows age imbalance in the DiF dataset, which contrasts with the separate gender prediction analysis also performed on the same DiF dataset using coding scheme 8.\",\n      \"visual_anchor\": \"shortest bar at >60 group with height approximately 0.2×10^5\",\n      \"text_evidence\": \"Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "bar chart with 7 age groups on x-axis",
        "y-axis showing num_faces up to 3.5×10^5",
        "tallest dark blue bar at 20-30 age group reaching ~3.4×10^5",
        "shortest bar at >60 group near 0.2×10^5",
        "title 'age prediction'"
      ],
      "queries": [
        {
          "query": "Does the tallest bar at 20-30 reaching 340,000 faces explain why Simpson D equals 4.368 for this distribution?",
          "answer": "Yes, the extreme peak at age 20-30 with 340,000 faces creates an uneven distribution, directly resulting in the calculated Simpson D value of 4.368 and Shannon H value of 1.601.",
          "visual_anchor": "tallest dark blue bar at 20-30 age group reaching approximately 3.4×10^5",
          "text_evidence": "The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even",
          "query_type": "value_context"
        },
        {
          "query": "Why does the 20-30 bar exceed the 31-45 bar by over 120,000 faces in coding scheme 7?",
          "answer": "The DiF dataset has an uneven age distribution concentrated in the 20-30 range, which is reflected in the seven-group binning structure of coding scheme 7 ([0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]).",
          "visual_anchor": "20-30 bar at ~3.4×10^5 compared to 31-45 bar at ~2.1×10^5",
          "text_evidence": "summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the minimal height of the >60 bar relate to the feature distribution summary for gender prediction?",
          "answer": "The >60 bar's minimal height (under 0.3×10^5) shows age imbalance in the DiF dataset, which contrasts with the separate gender prediction analysis also performed on the same DiF dataset using coding scheme 8.",
          "visual_anchor": "shortest bar at >60 group with height approximately 0.2×10^5",
          "text_evidence": "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the tallest bar at 20-30 reaching 340,000 faces explain why Simpson D equals 4.368 for this distribution?",
        "answer": "Yes, the extreme peak at age 20-30 with 340,000 faces creates an uneven distribution, directly resulting in the calculated Simpson D value of 4.368 and Shannon H value of 1.601.",
        "visual_anchor": "tallest dark blue bar at 20-30 age group reaching approximately 3.4×10^5",
        "text_evidence": "The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even",
        "query_type": "value_context"
      },
      {
        "query": "Why does the 20-30 bar exceed the 31-45 bar by over 120,000 faces in coding scheme 7?",
        "answer": "The DiF dataset has an uneven age distribution concentrated in the 20-30 range, which is reflected in the seven-group binning structure of coding scheme 7 ([0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]).",
        "visual_anchor": "20-30 bar at ~3.4×10^5 compared to 31-45 bar at ~2.1×10^5",
        "text_evidence": "summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the minimal height of the >60 bar relate to the feature distribution summary for gender prediction?",
        "answer": "The >60 bar's minimal height (under 0.3×10^5) shows age imbalance in the DiF dataset, which contrasts with the separate gender prediction analysis also performed on the same DiF dataset using coding scheme 8.",
        "visual_anchor": "shortest bar at >60 group with height approximately 0.2×10^5",
        "text_evidence": "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "bar chart with 7 age groups on x-axis",
      "y-axis showing num_faces up to 3.5×10^5",
      "tallest dark blue bar at 20-30 age group reaching ~3.4×10^5",
      "shortest bar at >60 group near 0.2×10^5",
      "title 'age prediction'"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_64",
    "figure_number": 14,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg",
    "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Two dark blue bars of equal height reaching approximately 4.6×10^5\",\n    \"Y-axis labeled 'numbers' ranging from 0 to 5×10^5\",\n    \"X-axis showing 'Female' and 'Male' categories\",\n    \"Title 'gender annotation'\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the nearly equal bar heights at 4.6×10^5 for Female and Male support higher evenness scores in gender annotations?\",\n      \"answer\": \"Yes, the equal distribution between Female and Male categories in the bar chart reflects balanced representation, which contributes to evenness scores closer to one as mentioned for the coding scheme measurements.\",\n      \"visual_anchor\": \"Two bars at approximately 4.6×10^5 height for Female and Male\",\n      \"text_evidence\": \"Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do both bars reach approximately 460,000 when this represents subjective annotations for the DiF data set?\",\n      \"answer\": \"The equal heights around 460,000 for both Female and Male indicate that the DiF data set contains roughly balanced gender representation in its subjective annotations, showing approximately equal numbers of images annotated for each gender category.\",\n      \"visual_anchor\": \"Both bars at 4.6×10^5 level\",\n      \"text_evidence\": \"Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the uniform height of both gender bars contrast with the diversity scores observed in craniofacial schemes?\",\n      \"answer\": \"The uniform bar heights show limited diversity with only two gender categories, whereas craniofacial schemes demonstrate higher diversity scores relative to gender measures, indicating more varied dimensional distributions across their feature spaces.\",\n      \"visual_anchor\": \"Equal height bars for Female and Male categories\",\n      \"text_evidence\": \"One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Two dark blue bars of equal height reaching approximately 4.6×10^5",
        "Y-axis labeled 'numbers' ranging from 0 to 5×10^5",
        "X-axis showing 'Female' and 'Male' categories",
        "Title 'gender annotation'"
      ],
      "queries": [
        {
          "query": "Do the nearly equal bar heights at 4.6×10^5 for Female and Male support higher evenness scores in gender annotations?",
          "answer": "Yes, the equal distribution between Female and Male categories in the bar chart reflects balanced representation, which contributes to evenness scores closer to one as mentioned for the coding scheme measurements.",
          "visual_anchor": "Two bars at approximately 4.6×10^5 height for Female and Male",
          "text_evidence": "Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do both bars reach approximately 460,000 when this represents subjective annotations for the DiF data set?",
          "answer": "The equal heights around 460,000 for both Female and Male indicate that the DiF data set contains roughly balanced gender representation in its subjective annotations, showing approximately equal numbers of images annotated for each gender category.",
          "visual_anchor": "Both bars at 4.6×10^5 level",
          "text_evidence": "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9.",
          "query_type": "value_context"
        },
        {
          "query": "How does the uniform height of both gender bars contrast with the diversity scores observed in craniofacial schemes?",
          "answer": "The uniform bar heights show limited diversity with only two gender categories, whereas craniofacial schemes demonstrate higher diversity scores relative to gender measures, indicating more varied dimensional distributions across their feature spaces.",
          "visual_anchor": "Equal height bars for Female and Male categories",
          "text_evidence": "One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the nearly equal bar heights at 4.6×10^5 for Female and Male support higher evenness scores in gender annotations?",
        "answer": "Yes, the equal distribution between Female and Male categories in the bar chart reflects balanced representation, which contributes to evenness scores closer to one as mentioned for the coding scheme measurements.",
        "visual_anchor": "Two bars at approximately 4.6×10^5 height for Female and Male",
        "text_evidence": "Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do both bars reach approximately 460,000 when this represents subjective annotations for the DiF data set?",
        "answer": "The equal heights around 460,000 for both Female and Male indicate that the DiF data set contains roughly balanced gender representation in its subjective annotations, showing approximately equal numbers of images annotated for each gender category.",
        "visual_anchor": "Both bars at 4.6×10^5 level",
        "text_evidence": "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9.",
        "query_type": "value_context"
      },
      {
        "query": "How does the uniform height of both gender bars contrast with the diversity scores observed in craniofacial schemes?",
        "answer": "The uniform bar heights show limited diversity with only two gender categories, whereas craniofacial schemes demonstrate higher diversity scores relative to gender measures, indicating more varied dimensional distributions across their feature spaces.",
        "visual_anchor": "Equal height bars for Female and Male categories",
        "text_evidence": "One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Two dark blue bars of equal height reaching approximately 4.6×10^5",
      "Y-axis labeled 'numbers' ranging from 0 to 5×10^5",
      "X-axis showing 'Female' and 'Male' categories",
      "Title 'gender annotation'"
    ]
  },
  {
    "doc_id": "1901.10436",
    "figure_id": "1901.10436_fig_65",
    "figure_number": 15,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg",
    "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three dark blue bars representing pose categories\",\n    \"Y-axis labeled 'num images' ranging from 0 to 9×10⁵\",\n    \"X-axis categories: 'Frontal Tilted Left', 'Frontal', 'Frontal Tilted Right'\",\n    \"Middle bar (Frontal) reaches approximately 9×10⁵\",\n    \"Left and right bars both reach approximately 0.5×10⁵\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the Frontal bar reach 9×10⁵ while craniofacial schemes show higher diversity than pose?\",\n      \"answer\": \"The Frontal pose dominates the dataset with approximately 900,000 images, creating low diversity (Shannon H=0.39). Craniofacial schemes achieve higher diversity scores across their multiple dimensions despite this pose imbalance.\",\n      \"visual_anchor\": \"Middle bar labeled 'Frontal' at approximately 9×10⁵ on y-axis\",\n      \"text_evidence\": \"One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the overwhelming height of the Frontal bar compared to tilted categories support the Shannon H of 0.39?\",\n      \"answer\": \"Yes, the Frontal category contains roughly 18 times more images than either tilted category, creating severe imbalance. This extreme skew directly produces the low Shannon H value of 0.39 for the three-class pose distribution.\",\n      \"visual_anchor\": \"Frontal bar at 9×10⁵ versus Frontal Tilted Left and Right bars at approximately 0.5×10⁵\",\n      \"text_evidence\": \"The three class pose distribution has a Shannon $H$ value of 0.39\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the three pose categories shown relate to the DLIB face detection output dimensions mentioned?\",\n      \"answer\": \"The three bars represent binned pose values derived from DLIB face detection output. The values used are Frontal Tilted Left, Frontal, and Frontal Tilted Right, computed from DLIB's facial landmark detection for the DiF dataset.\",\n      \"visual_anchor\": \"Three x-axis categories: Frontal Tilted Left, Frontal, and Frontal Tilted Right\",\n      \"text_evidence\": \"Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted L\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three dark blue bars representing pose categories",
        "Y-axis labeled 'num images' ranging from 0 to 9×10⁵",
        "X-axis categories: 'Frontal Tilted Left', 'Frontal', 'Frontal Tilted Right'",
        "Middle bar (Frontal) reaches approximately 9×10⁵",
        "Left and right bars both reach approximately 0.5×10⁵"
      ],
      "queries": [
        {
          "query": "Why does the Frontal bar reach 9×10⁵ while craniofacial schemes show higher diversity than pose?",
          "answer": "The Frontal pose dominates the dataset with approximately 900,000 images, creating low diversity (Shannon H=0.39). Craniofacial schemes achieve higher diversity scores across their multiple dimensions despite this pose imbalance.",
          "visual_anchor": "Middle bar labeled 'Frontal' at approximately 9×10⁵ on y-axis",
          "text_evidence": "One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the overwhelming height of the Frontal bar compared to tilted categories support the Shannon H of 0.39?",
          "answer": "Yes, the Frontal category contains roughly 18 times more images than either tilted category, creating severe imbalance. This extreme skew directly produces the low Shannon H value of 0.39 for the three-class pose distribution.",
          "visual_anchor": "Frontal bar at 9×10⁵ versus Frontal Tilted Left and Right bars at approximately 0.5×10⁵",
          "text_evidence": "The three class pose distribution has a Shannon $H$ value of 0.39",
          "query_type": "value_context"
        },
        {
          "query": "How do the three pose categories shown relate to the DLIB face detection output dimensions mentioned?",
          "answer": "The three bars represent binned pose values derived from DLIB face detection output. The values used are Frontal Tilted Left, Frontal, and Frontal Tilted Right, computed from DLIB's facial landmark detection for the DiF dataset.",
          "visual_anchor": "Three x-axis categories: Frontal Tilted Left, Frontal, and Frontal Tilted Right",
          "text_evidence": "Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted L",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the Frontal bar reach 9×10⁵ while craniofacial schemes show higher diversity than pose?",
        "answer": "The Frontal pose dominates the dataset with approximately 900,000 images, creating low diversity (Shannon H=0.39). Craniofacial schemes achieve higher diversity scores across their multiple dimensions despite this pose imbalance.",
        "visual_anchor": "Middle bar labeled 'Frontal' at approximately 9×10⁵ on y-axis",
        "text_evidence": "One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the overwhelming height of the Frontal bar compared to tilted categories support the Shannon H of 0.39?",
        "answer": "Yes, the Frontal category contains roughly 18 times more images than either tilted category, creating severe imbalance. This extreme skew directly produces the low Shannon H value of 0.39 for the three-class pose distribution.",
        "visual_anchor": "Frontal bar at 9×10⁵ versus Frontal Tilted Left and Right bars at approximately 0.5×10⁵",
        "text_evidence": "The three class pose distribution has a Shannon $H$ value of 0.39",
        "query_type": "value_context"
      },
      {
        "query": "How do the three pose categories shown relate to the DLIB face detection output dimensions mentioned?",
        "answer": "The three bars represent binned pose values derived from DLIB face detection output. The values used are Frontal Tilted Left, Frontal, and Frontal Tilted Right, computed from DLIB's facial landmark detection for the DiF dataset.",
        "visual_anchor": "Three x-axis categories: Frontal Tilted Left, Frontal, and Frontal Tilted Right",
        "text_evidence": "Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted L",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three dark blue bars representing pose categories",
      "Y-axis labeled 'num images' ranging from 0 to 9×10⁵",
      "X-axis categories: 'Frontal Tilted Left', 'Frontal', 'Frontal Tilted Right'",
      "Middle bar (Frontal) reaches approximately 9×10⁵",
      "Left and right bars both reach approximately 0.5×10⁵"
    ]
  },
  {
    "doc_id": "1902.03519",
    "figure_id": "1902.03519_fig_4",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg",
    "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"top node with 3 blue circles and 1 red circle\",\n    \"three child nodes at bottom level\",\n    \"left child node with 1 red and 2 blue circles\",\n    \"middle child node with 1 red and 2 blue circles\",\n    \"right child node with 1 red and 2 blue circles\",\n    \"hierarchical tree structure with connecting lines\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the top node containing 3 blue and 1 red point demonstrate the embedding into γ-HST?\",\n      \"answer\": \"The top node represents the root of the γ-HST embedding of input points, which is the first step in the fairlet decomposition algorithm after embedding the point set into a hierarchical tree structure.\",\n      \"visual_anchor\": \"top node with 3 blue circles and 1 red circle\",\n      \"text_evidence\": \"The first step in our algorithm is to embed the input point set into a γ-HST (for a value of γ to be determined later in this section). Once we build a γ-HST embedding T of the input points\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do all three bottom child nodes show balanced 1:2 red-to-blue ratios when the algorithm processes 8 blue and 4 red points?\",\n      \"answer\": \"The three child nodes demonstrate the fairlet decomposition stages where the algorithm creates balanced (1,3)-fairlets, ensuring each fairlet maintains the required r:b ratio of 1 red to 3 blue points as specified by the decomposition constraint.\",\n      \"visual_anchor\": \"three bottom nodes each containing 1 red and 2 blue circles\",\n      \"text_evidence\": \"A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the hierarchical decomposition shown improve cost compared to the baseline, given each node can have γ^d children?\",\n      \"answer\": \"Yes, the empirical results show cost improvements on all instances compared to the baseline algorithm, despite the potential for each node to have up to γ^d children in the tree structure.\",\n      \"visual_anchor\": \"tree structure with parent node branching to three child nodes\",\n      \"text_evidence\": \"Although the number of children of $T ( v )$ can be as large as $\\\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs... Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "top node with 3 blue circles and 1 red circle",
        "three child nodes at bottom level",
        "left child node with 1 red and 2 blue circles",
        "middle child node with 1 red and 2 blue circles",
        "right child node with 1 red and 2 blue circles",
        "hierarchical tree structure with connecting lines"
      ],
      "queries": [
        {
          "query": "How does the top node containing 3 blue and 1 red point demonstrate the embedding into γ-HST?",
          "answer": "The top node represents the root of the γ-HST embedding of input points, which is the first step in the fairlet decomposition algorithm after embedding the point set into a hierarchical tree structure.",
          "visual_anchor": "top node with 3 blue circles and 1 red circle",
          "text_evidence": "The first step in our algorithm is to embed the input point set into a γ-HST (for a value of γ to be determined later in this section). Once we build a γ-HST embedding T of the input points",
          "query_type": "value_context"
        },
        {
          "query": "Why do all three bottom child nodes show balanced 1:2 red-to-blue ratios when the algorithm processes 8 blue and 4 red points?",
          "answer": "The three child nodes demonstrate the fairlet decomposition stages where the algorithm creates balanced (1,3)-fairlets, ensuring each fairlet maintains the required r:b ratio of 1 red to 3 blue points as specified by the decomposition constraint.",
          "visual_anchor": "three bottom nodes each containing 1 red and 2 blue circles",
          "text_evidence": "A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the hierarchical decomposition shown improve cost compared to the baseline, given each node can have γ^d children?",
          "answer": "Yes, the empirical results show cost improvements on all instances compared to the baseline algorithm, despite the potential for each node to have up to γ^d children in the tree structure.",
          "visual_anchor": "tree structure with parent node branching to three child nodes",
          "text_evidence": "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs... Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the top node containing 3 blue and 1 red point demonstrate the embedding into γ-HST?",
        "answer": "The top node represents the root of the γ-HST embedding of input points, which is the first step in the fairlet decomposition algorithm after embedding the point set into a hierarchical tree structure.",
        "visual_anchor": "top node with 3 blue circles and 1 red circle",
        "text_evidence": "The first step in our algorithm is to embed the input point set into a γ-HST (for a value of γ to be determined later in this section). Once we build a γ-HST embedding T of the input points",
        "query_type": "value_context"
      },
      {
        "query": "Why do all three bottom child nodes show balanced 1:2 red-to-blue ratios when the algorithm processes 8 blue and 4 red points?",
        "answer": "The three child nodes demonstrate the fairlet decomposition stages where the algorithm creates balanced (1,3)-fairlets, ensuring each fairlet maintains the required r:b ratio of 1 red to 3 blue points as specified by the decomposition constraint.",
        "visual_anchor": "three bottom nodes each containing 1 red and 2 blue circles",
        "text_evidence": "A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the hierarchical decomposition shown improve cost compared to the baseline, given each node can have γ^d children?",
        "answer": "Yes, the empirical results show cost improvements on all instances compared to the baseline algorithm, despite the potential for each node to have up to γ^d children in the tree structure.",
        "visual_anchor": "tree structure with parent node branching to three child nodes",
        "text_evidence": "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs... Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "top node with 3 blue circles and 1 red circle",
      "three child nodes at bottom level",
      "left child node with 1 red and 2 blue circles",
      "middle child node with 1 red and 2 blue circles",
      "right child node with 1 red and 2 blue circles",
      "hierarchical tree structure with connecting lines"
    ]
  },
  {
    "doc_id": "1902.03519",
    "figure_id": "1902.03519_fig_8",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg",
    "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Runtime curve from 0.01 to 0.23 seconds\", \"X-axis showing number of sub-sampled points from 0 to 5000\", \"Y-axis showing runtime in seconds from 0 to 0.25\", \"Nearly linear upward trend\", \"Bank dataset label in title\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the runtime increase from 0.01 to 0.23 seconds across 5000 points confirm the claimed almost linear scaling?\",\n      \"answer\": \"Yes, the smooth, nearly linear growth from 0.01 to 0.23 seconds as points increase from 0 to 5000 demonstrates the algorithm's almost linear scalability in the number of points.\",\n      \"visual_anchor\": \"Runtime curve rising from 0.01 seconds at 0 points to 0.23 seconds at 5000 points\",\n      \"text_evidence\": \"At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the runtime reach only 0.23 seconds at 5000 points for the Bank dataset's fairlet decomposition?\",\n      \"answer\": \"The fairlet decomposition algorithm for the Bank dataset with (1,2)-fairlet balance achieves efficient performance, with runtime staying below 0.25 seconds even at 5000 points, demonstrating its applicability to massive datasets.\",\n      \"visual_anchor\": \"Maximum runtime value of approximately 0.23 seconds at 5000 sub-sampled points\",\n      \"text_evidence\": \"Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the consistent upward slope without plateaus between 1000 and 4000 points support applicability to massive datasets?\",\n      \"answer\": \"Yes, the steady linear increase without performance degradation between 1000 and 4000 points indicates the algorithm maintains consistent efficiency as data size grows, validating its suitability for massive datasets.\",\n      \"visual_anchor\": \"Smooth, linear upward slope between 1000 points (0.07 seconds) and 4000 points (0.20 seconds)\",\n      \"text_evidence\": \"the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Runtime curve from 0.01 to 0.23 seconds",
        "X-axis showing number of sub-sampled points from 0 to 5000",
        "Y-axis showing runtime in seconds from 0 to 0.25",
        "Nearly linear upward trend",
        "Bank dataset label in title"
      ],
      "queries": [
        {
          "query": "Does the runtime increase from 0.01 to 0.23 seconds across 5000 points confirm the claimed almost linear scaling?",
          "answer": "Yes, the smooth, nearly linear growth from 0.01 to 0.23 seconds as points increase from 0 to 5000 demonstrates the algorithm's almost linear scalability in the number of points.",
          "visual_anchor": "Runtime curve rising from 0.01 seconds at 0 points to 0.23 seconds at 5000 points",
          "text_evidence": "At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the runtime reach only 0.23 seconds at 5000 points for the Bank dataset's fairlet decomposition?",
          "answer": "The fairlet decomposition algorithm for the Bank dataset with (1,2)-fairlet balance achieves efficient performance, with runtime staying below 0.25 seconds even at 5000 points, demonstrating its applicability to massive datasets.",
          "visual_anchor": "Maximum runtime value of approximately 0.23 seconds at 5000 sub-sampled points",
          "text_evidence": "Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
          "query_type": "value_context"
        },
        {
          "query": "Does the consistent upward slope without plateaus between 1000 and 4000 points support applicability to massive datasets?",
          "answer": "Yes, the steady linear increase without performance degradation between 1000 and 4000 points indicates the algorithm maintains consistent efficiency as data size grows, validating its suitability for massive datasets.",
          "visual_anchor": "Smooth, linear upward slope between 1000 points (0.07 seconds) and 4000 points (0.20 seconds)",
          "text_evidence": "the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the runtime increase from 0.01 to 0.23 seconds across 5000 points confirm the claimed almost linear scaling?",
        "answer": "Yes, the smooth, nearly linear growth from 0.01 to 0.23 seconds as points increase from 0 to 5000 demonstrates the algorithm's almost linear scalability in the number of points.",
        "visual_anchor": "Runtime curve rising from 0.01 seconds at 0 points to 0.23 seconds at 5000 points",
        "text_evidence": "At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the runtime reach only 0.23 seconds at 5000 points for the Bank dataset's fairlet decomposition?",
        "answer": "The fairlet decomposition algorithm for the Bank dataset with (1,2)-fairlet balance achieves efficient performance, with runtime staying below 0.25 seconds even at 5000 points, demonstrating its applicability to massive datasets.",
        "visual_anchor": "Maximum runtime value of approximately 0.23 seconds at 5000 sub-sampled points",
        "text_evidence": "Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
        "query_type": "value_context"
      },
      {
        "query": "Does the consistent upward slope without plateaus between 1000 and 4000 points support applicability to massive datasets?",
        "answer": "Yes, the steady linear increase without performance degradation between 1000 and 4000 points indicates the algorithm maintains consistent efficiency as data size grows, validating its suitability for massive datasets.",
        "visual_anchor": "Smooth, linear upward slope between 1000 points (0.07 seconds) and 4000 points (0.20 seconds)",
        "text_evidence": "the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Runtime curve from 0.01 to 0.23 seconds",
      "X-axis showing number of sub-sampled points from 0 to 5000",
      "Y-axis showing runtime in seconds from 0 to 0.25",
      "Nearly linear upward trend",
      "Bank dataset label in title"
    ]
  },
  {
    "doc_id": "1902.07823",
    "figure_id": "1902.07823_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg",
    "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"ZVRG-St solid blue line with diamond markers\",\n    \"KAAS-St dashed orange line with triangle markers\",\n    \"GYR-St dotted green line with circle markers\",\n    \"λ (lambda) on x-axis from 0 to 0.05\",\n    \"stab on y-axis from 30 to 100\",\n    \"All three curves start high (70-100) at λ=0\",\n    \"KAAS-St drops sharply from ~100 to ~40 between λ=0 and λ=0.01\",\n    \"ZVRG-St and GYR-St converge to ~35-40 at λ=0.05\",\n    \"KAAS-St rises to ~55 at λ=0.05\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the orange KAAS-St curve drop sharply to 40 between λ=0 and λ=0.01 for race attribute?\",\n      \"answer\": \"KAAS is formulated as Program (RegFair) and selected as a representative algorithm ensuring statistical parity. The sharp stability drop reflects how the regularization parameter λ impacts the algorithm's convergence behavior when enforcing fairness constraints.\",\n      \"visual_anchor\": \"orange dashed line dropping from ~100 to ~40 between λ=0 and λ=0.01\",\n      \"text_evidence\": \"We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair).\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does ZVRG-St's stability of approximately 35 at λ=0.05 support its reported better fairness than other algorithms?\",\n      \"answer\": \"Yes, ZVRG-St (solid blue line) maintains relatively low and stable values (~35) at λ=0.05, consistent with its formulation in the convex optimization framework. This stability pattern aligns with reports that ZVRG achieves better fairness and comparable accuracy.\",\n      \"visual_anchor\": \"solid blue line with diamond markers ending at ~35 when λ=0.05\",\n      \"text_evidence\": \"We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32].\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the three statistical parity algorithms compare in stability at λ=0.03 on the race attribute?\",\n      \"answer\": \"At λ=0.03, ZVRG-St (blue) and GYR-St (green dotted) both show stability around 40, while KAAS-St (orange dashed) peaks near 60. All three algorithms are designed to ensure statistical parity through convex optimization, but exhibit different stability responses to the regularization parameter.\",\n      \"visual_anchor\": \"three curves at λ=0.03: blue at ~40, green dotted at ~40, orange dashed at ~60\",\n      \"text_evidence\": \"We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair).\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "ZVRG-St solid blue line with diamond markers",
        "KAAS-St dashed orange line with triangle markers",
        "GYR-St dotted green line with circle markers",
        "λ (lambda) on x-axis from 0 to 0.05",
        "stab on y-axis from 30 to 100",
        "All three curves start high (70-100) at λ=0",
        "KAAS-St drops sharply from ~100 to ~40 between λ=0 and λ=0.01",
        "ZVRG-St and GYR-St converge to ~35-40 at λ=0.05",
        "KAAS-St rises to ~55 at λ=0.05"
      ],
      "queries": [
        {
          "query": "Why does the orange KAAS-St curve drop sharply to 40 between λ=0 and λ=0.01 for race attribute?",
          "answer": "KAAS is formulated as Program (RegFair) and selected as a representative algorithm ensuring statistical parity. The sharp stability drop reflects how the regularization parameter λ impacts the algorithm's convergence behavior when enforcing fairness constraints.",
          "visual_anchor": "orange dashed line dropping from ~100 to ~40 between λ=0 and λ=0.01",
          "text_evidence": "We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair).",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does ZVRG-St's stability of approximately 35 at λ=0.05 support its reported better fairness than other algorithms?",
          "answer": "Yes, ZVRG-St (solid blue line) maintains relatively low and stable values (~35) at λ=0.05, consistent with its formulation in the convex optimization framework. This stability pattern aligns with reports that ZVRG achieves better fairness and comparable accuracy.",
          "visual_anchor": "solid blue line with diamond markers ending at ~35 when λ=0.05",
          "text_evidence": "We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32].",
          "query_type": "value_context"
        },
        {
          "query": "How do the three statistical parity algorithms compare in stability at λ=0.03 on the race attribute?",
          "answer": "At λ=0.03, ZVRG-St (blue) and GYR-St (green dotted) both show stability around 40, while KAAS-St (orange dashed) peaks near 60. All three algorithms are designed to ensure statistical parity through convex optimization, but exhibit different stability responses to the regularization parameter.",
          "visual_anchor": "three curves at λ=0.03: blue at ~40, green dotted at ~40, orange dashed at ~60",
          "text_evidence": "We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair).",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the orange KAAS-St curve drop sharply to 40 between λ=0 and λ=0.01 for race attribute?",
        "answer": "KAAS is formulated as Program (RegFair) and selected as a representative algorithm ensuring statistical parity. The sharp stability drop reflects how the regularization parameter λ impacts the algorithm's convergence behavior when enforcing fairness constraints.",
        "visual_anchor": "orange dashed line dropping from ~100 to ~40 between λ=0 and λ=0.01",
        "text_evidence": "We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair).",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does ZVRG-St's stability of approximately 35 at λ=0.05 support its reported better fairness than other algorithms?",
        "answer": "Yes, ZVRG-St (solid blue line) maintains relatively low and stable values (~35) at λ=0.05, consistent with its formulation in the convex optimization framework. This stability pattern aligns with reports that ZVRG achieves better fairness and comparable accuracy.",
        "visual_anchor": "solid blue line with diamond markers ending at ~35 when λ=0.05",
        "text_evidence": "We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32].",
        "query_type": "value_context"
      },
      {
        "query": "How do the three statistical parity algorithms compare in stability at λ=0.03 on the race attribute?",
        "answer": "At λ=0.03, ZVRG-St (blue) and GYR-St (green dotted) both show stability around 40, while KAAS-St (orange dashed) peaks near 60. All three algorithms are designed to ensure statistical parity through convex optimization, but exhibit different stability responses to the regularization parameter.",
        "visual_anchor": "three curves at λ=0.03: blue at ~40, green dotted at ~40, orange dashed at ~60",
        "text_evidence": "We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair).",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "ZVRG-St solid blue line with diamond markers",
      "KAAS-St dashed orange line with triangle markers",
      "GYR-St dotted green line with circle markers",
      "λ (lambda) on x-axis from 0 to 0.05",
      "stab on y-axis from 30 to 100",
      "All three curves start high (70-100) at λ=0",
      "KAAS-St drops sharply from ~100 to ~40 between λ=0 and λ=0.01",
      "ZVRG-St and GYR-St converge to ~35-40 at λ=0.05",
      "KAAS-St rises to ~55 at λ=0.05"
    ]
  },
  {
    "doc_id": "1902.07823",
    "figure_id": "1902.07823_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg",
    "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid blue line labeled ZVRG-St starting at approximately 70 and declining to around 25\",\n    \"dashed orange line labeled KAAS-St starting at approximately 105 and declining to around 45\",\n    \"dotted purple line labeled GYR-St starting at approximately 70 and stabilizing around 35-40\",\n    \"x-axis labeled λ ranging from 0 to 0.05\",\n    \"y-axis labeled stab ranging from approximately 20 to 100\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does KAAS-St's orange dashed line peak at λ=0.03 when the expected number of different predictions should decrease monotonically?\",\n      \"answer\": \"The peak at λ=0.03 (around 65) contradicts typical monotonic behavior. The stab metric measures expected differences between predictions on training sets S and S', which may not decrease uniformly across all λ values for KAAS-St.\",\n      \"visual_anchor\": \"orange dashed line (KAAS-St) peaking at approximately 65 when λ=0.03\",\n      \"text_evidence\": \"$\\\\mathrm { s t a b } _ { T } ( \\\\mathcal { A } )$ indicates the expected number of different predictions of $A _ { S }$ and $\\\\mathbf { \\\\boldsymbol { A } } _ { S ^ { \\\\prime } }$ over the testing set $T$\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the solid blue ZVRG-St line reaching its minimum around 25 at λ=0.03 demonstrate better prediction stability than GYR-St's dotted line?\",\n      \"answer\": \"Yes, ZVRG-St achieves the lowest stab value (approximately 25) at λ=0.03, indicating fewer different predictions between training sets compared to GYR-St which stabilizes around 35-40.\",\n      \"visual_anchor\": \"solid blue line (ZVRG-St) minimum at approximately 25 when λ=0.03\",\n      \"text_evidence\": \"Note that this metric is considered in [32], but is slightly different from prediction stability since $S$ and $S ^ { \\\\prime }$ may differ by more than one training sample\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the convergence pattern of all three methods after λ=0.03 relate to stability measurement for the sex attribute?\",\n      \"answer\": \"After λ=0.03, all three methods show convergence toward similar stab values (30-45 range), suggesting that higher λ values reduce differences in prediction stability across algorithms for the sex attribute.\",\n      \"visual_anchor\": \"convergence of all three lines (blue solid, orange dashed, purple dotted) between λ=0.03 and λ=0.05\",\n      \"text_evidence\": \"Adult dataset, sex attribute   \\nFigure 2: stab vs. $\\\\lambda$ for sex attribute\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid blue line labeled ZVRG-St starting at approximately 70 and declining to around 25",
        "dashed orange line labeled KAAS-St starting at approximately 105 and declining to around 45",
        "dotted purple line labeled GYR-St starting at approximately 70 and stabilizing around 35-40",
        "x-axis labeled λ ranging from 0 to 0.05",
        "y-axis labeled stab ranging from approximately 20 to 100"
      ],
      "queries": [
        {
          "query": "Why does KAAS-St's orange dashed line peak at λ=0.03 when the expected number of different predictions should decrease monotonically?",
          "answer": "The peak at λ=0.03 (around 65) contradicts typical monotonic behavior. The stab metric measures expected differences between predictions on training sets S and S', which may not decrease uniformly across all λ values for KAAS-St.",
          "visual_anchor": "orange dashed line (KAAS-St) peaking at approximately 65 when λ=0.03",
          "text_evidence": "$\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ indicates the expected number of different predictions of $A _ { S }$ and $\\mathbf { \\boldsymbol { A } } _ { S ^ { \\prime } }$ over the testing set $T$",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the solid blue ZVRG-St line reaching its minimum around 25 at λ=0.03 demonstrate better prediction stability than GYR-St's dotted line?",
          "answer": "Yes, ZVRG-St achieves the lowest stab value (approximately 25) at λ=0.03, indicating fewer different predictions between training sets compared to GYR-St which stabilizes around 35-40.",
          "visual_anchor": "solid blue line (ZVRG-St) minimum at approximately 25 when λ=0.03",
          "text_evidence": "Note that this metric is considered in [32], but is slightly different from prediction stability since $S$ and $S ^ { \\prime }$ may differ by more than one training sample",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the convergence pattern of all three methods after λ=0.03 relate to stability measurement for the sex attribute?",
          "answer": "After λ=0.03, all three methods show convergence toward similar stab values (30-45 range), suggesting that higher λ values reduce differences in prediction stability across algorithms for the sex attribute.",
          "visual_anchor": "convergence of all three lines (blue solid, orange dashed, purple dotted) between λ=0.03 and λ=0.05",
          "text_evidence": "Adult dataset, sex attribute   \nFigure 2: stab vs. $\\lambda$ for sex attribute",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does KAAS-St's orange dashed line peak at λ=0.03 when the expected number of different predictions should decrease monotonically?",
        "answer": "The peak at λ=0.03 (around 65) contradicts typical monotonic behavior. The stab metric measures expected differences between predictions on training sets S and S', which may not decrease uniformly across all λ values for KAAS-St.",
        "visual_anchor": "orange dashed line (KAAS-St) peaking at approximately 65 when λ=0.03",
        "text_evidence": "$\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ indicates the expected number of different predictions of $A _ { S }$ and $\\mathbf { \\boldsymbol { A } } _ { S ^ { \\prime } }$ over the testing set $T$",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the solid blue ZVRG-St line reaching its minimum around 25 at λ=0.03 demonstrate better prediction stability than GYR-St's dotted line?",
        "answer": "Yes, ZVRG-St achieves the lowest stab value (approximately 25) at λ=0.03, indicating fewer different predictions between training sets compared to GYR-St which stabilizes around 35-40.",
        "visual_anchor": "solid blue line (ZVRG-St) minimum at approximately 25 when λ=0.03",
        "text_evidence": "Note that this metric is considered in [32], but is slightly different from prediction stability since $S$ and $S ^ { \\prime }$ may differ by more than one training sample",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the convergence pattern of all three methods after λ=0.03 relate to stability measurement for the sex attribute?",
        "answer": "After λ=0.03, all three methods show convergence toward similar stab values (30-45 range), suggesting that higher λ values reduce differences in prediction stability across algorithms for the sex attribute.",
        "visual_anchor": "convergence of all three lines (blue solid, orange dashed, purple dotted) between λ=0.03 and λ=0.05",
        "text_evidence": "Adult dataset, sex attribute   \nFigure 2: stab vs. $\\lambda$ for sex attribute",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid blue line labeled ZVRG-St starting at approximately 70 and declining to around 25",
      "dashed orange line labeled KAAS-St starting at approximately 105 and declining to around 45",
      "dotted purple line labeled GYR-St starting at approximately 70 and stabilizing around 35-40",
      "x-axis labeled λ ranging from 0 to 0.05",
      "y-axis labeled stab ranging from approximately 20 to 100"
    ]
  },
  {
    "doc_id": "1903.08136",
    "figure_id": "1903.08136_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
    "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"gray quarter-circle sector in upper-left\",\n    \"light gray three-quarter circle region\",\n    \"blue triangles with various patterns (solid, striped, horizontal lines)\",\n    \"red circles with various patterns (solid, striped, horizontal lines)\",\n    \"shapes distributed across both regions\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?\",\n      \"answer\": \"The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.\",\n      \"visual_anchor\": \"gray quarter-circle sector with mixed blue triangles and red circles\",\n      \"text_evidence\": \"many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the scattered shapes in the light gray region represent the versatile relevant people ignored by biased methods?\",\n      \"answer\": \"Yes, the scattered low-degree nodes in the light gray region represent versatile relevant people who hold diverse opinions (circle vs triangle) and opinion variations (different textures) but get omitted when excluding lowly-connected users.\",\n      \"visual_anchor\": \"scattered blue triangles and red circles throughout light gray three-quarter region\",\n      \"text_evidence\": \"Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How do the different textures on red circles and blue triangles relate to analysis bias risks?\",\n      \"answer\": \"The different textures represent variations within opinions that provide important information. Analysis focusing only on largest communities risks losing this nuanced information from low-degree users, as shown by what Louvain and CESNA methods omit.\",\n      \"visual_anchor\": \"red circles and blue triangles with striped, solid, and horizontal line textures\",\n      \"text_evidence\": \"Analysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "gray quarter-circle sector in upper-left",
        "light gray three-quarter circle region",
        "blue triangles with various patterns (solid, striped, horizontal lines)",
        "red circles with various patterns (solid, striped, horizontal lines)",
        "shapes distributed across both regions"
      ],
      "queries": [
        {
          "query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
          "answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
          "visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
          "text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the scattered shapes in the light gray region represent the versatile relevant people ignored by biased methods?",
          "answer": "Yes, the scattered low-degree nodes in the light gray region represent versatile relevant people who hold diverse opinions (circle vs triangle) and opinion variations (different textures) but get omitted when excluding lowly-connected users.",
          "visual_anchor": "scattered blue triangles and red circles throughout light gray three-quarter region",
          "text_evidence": "Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches.",
          "query_type": "visual_definition"
        },
        {
          "query": "How do the different textures on red circles and blue triangles relate to analysis bias risks?",
          "answer": "The different textures represent variations within opinions that provide important information. Analysis focusing only on largest communities risks losing this nuanced information from low-degree users, as shown by what Louvain and CESNA methods omit.",
          "visual_anchor": "red circles and blue triangles with striped, solid, and horizontal line textures",
          "text_evidence": "Analysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the gray sector contain only triangles and circles without showing how low-degree users get excluded?",
        "answer": "The gray sector demonstrates a community where both opinion types (shapes) and variations (textures) coexist, but methods like CESNA and Louvain fail to assign low-degree nodes or assign them to trivially small communities that prevent inclusion in analysis.",
        "visual_anchor": "gray quarter-circle sector with mixed blue triangles and red circles",
        "text_evidence": "many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the scattered shapes in the light gray region represent the versatile relevant people ignored by biased methods?",
        "answer": "Yes, the scattered low-degree nodes in the light gray region represent versatile relevant people who hold diverse opinions (circle vs triangle) and opinion variations (different textures) but get omitted when excluding lowly-connected users.",
        "visual_anchor": "scattered blue triangles and red circles throughout light gray three-quarter region",
        "text_evidence": "Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches.",
        "query_type": "visual_definition"
      },
      {
        "query": "How do the different textures on red circles and blue triangles relate to analysis bias risks?",
        "answer": "The different textures represent variations within opinions that provide important information. Analysis focusing only on largest communities risks losing this nuanced information from low-degree users, as shown by what Louvain and CESNA methods omit.",
        "visual_anchor": "red circles and blue triangles with striped, solid, and horizontal line textures",
        "text_evidence": "Analysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "gray quarter-circle sector in upper-left",
      "light gray three-quarter circle region",
      "blue triangles with various patterns (solid, striped, horizontal lines)",
      "red circles with various patterns (solid, striped, horizontal lines)",
      "shapes distributed across both regions"
    ]
  },
  {
    "doc_id": "1903.08136",
    "figure_id": "1903.08136_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg",
    "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Two network visualizations side by side\",\n    \"Left network colored by modularity-based community detection\",\n    \"Right network with green and magenta clusters showing ground truth political party labels\",\n    \"Callout box emphasizing low-degree users on the left network\",\n    \"Scattered colored dots throughout both networks\",\n    \"Dense green cluster in center-right of right network\",\n    \"Dense magenta/pink cluster adjacent to green in right network\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green cluster in the right visualization appear despite containing 10,074 seed users from a million-user dataset?\",\n      \"answer\": \"The dataset was reduced to only seed users to ensure pure ground truth labels obtained independently from network structure and label propagation, bringing the size down from over a million to 10,074 users.\",\n      \"visual_anchor\": \"green cluster in right network visualization\",\n      \"text_evidence\": \"we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label pr\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the difference between the left modularity-colored and right ground-truth-colored networks confirm disagreement between structural and actual political labels?\",\n      \"answer\": \"Yes, the visual disagreement between the left network colored by network structure and the right network colored by ground truth labels confirms that modularity-based communities do not align with actual political party affiliations.\",\n      \"visual_anchor\": \"contrasting color patterns between left and right network visualizations\",\n      \"text_evidence\": \"Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What do the scattered colored dots emphasized in the callout reveal about the 2016 presidential election retweet network structure?\",\n      \"answer\": \"The callout emphasizes low-degree users who are sparsely connected in the network, representing users with fewer retweet connections in the political discussion dataset.\",\n      \"visual_anchor\": \"callout box highlighting scattered colored dots (low-degree users)\",\n      \"text_evidence\": \"This dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Two network visualizations side by side",
        "Left network colored by modularity-based community detection",
        "Right network with green and magenta clusters showing ground truth political party labels",
        "Callout box emphasizing low-degree users on the left network",
        "Scattered colored dots throughout both networks",
        "Dense green cluster in center-right of right network",
        "Dense magenta/pink cluster adjacent to green in right network"
      ],
      "queries": [
        {
          "query": "Why does the green cluster in the right visualization appear despite containing 10,074 seed users from a million-user dataset?",
          "answer": "The dataset was reduced to only seed users to ensure pure ground truth labels obtained independently from network structure and label propagation, bringing the size down from over a million to 10,074 users.",
          "visual_anchor": "green cluster in right network visualization",
          "text_evidence": "we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label pr",
          "query_type": "value_context"
        },
        {
          "query": "Does the difference between the left modularity-colored and right ground-truth-colored networks confirm disagreement between structural and actual political labels?",
          "answer": "Yes, the visual disagreement between the left network colored by network structure and the right network colored by ground truth labels confirms that modularity-based communities do not align with actual political party affiliations.",
          "visual_anchor": "contrasting color patterns between left and right network visualizations",
          "text_evidence": "Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What do the scattered colored dots emphasized in the callout reveal about the 2016 presidential election retweet network structure?",
          "answer": "The callout emphasizes low-degree users who are sparsely connected in the network, representing users with fewer retweet connections in the political discussion dataset.",
          "visual_anchor": "callout box highlighting scattered colored dots (low-degree users)",
          "text_evidence": "This dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green cluster in the right visualization appear despite containing 10,074 seed users from a million-user dataset?",
        "answer": "The dataset was reduced to only seed users to ensure pure ground truth labels obtained independently from network structure and label propagation, bringing the size down from over a million to 10,074 users.",
        "visual_anchor": "green cluster in right network visualization",
        "text_evidence": "we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label pr",
        "query_type": "value_context"
      },
      {
        "query": "Does the difference between the left modularity-colored and right ground-truth-colored networks confirm disagreement between structural and actual political labels?",
        "answer": "Yes, the visual disagreement between the left network colored by network structure and the right network colored by ground truth labels confirms that modularity-based communities do not align with actual political party affiliations.",
        "visual_anchor": "contrasting color patterns between left and right network visualizations",
        "text_evidence": "Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What do the scattered colored dots emphasized in the callout reveal about the 2016 presidential election retweet network structure?",
        "answer": "The callout emphasizes low-degree users who are sparsely connected in the network, representing users with fewer retweet connections in the political discussion dataset.",
        "visual_anchor": "callout box highlighting scattered colored dots (low-degree users)",
        "text_evidence": "This dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Two network visualizations side by side",
      "Left network colored by modularity-based community detection",
      "Right network with green and magenta clusters showing ground truth political party labels",
      "Callout box emphasizing low-degree users on the left network",
      "Scattered colored dots throughout both networks",
      "Dense green cluster in center-right of right network",
      "Dense magenta/pink cluster adjacent to green in right network"
    ]
  },
  {
    "doc_id": "1903.08136",
    "figure_id": "1903.08136_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg",
    "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Left network colored by network structure (purple and green clusters)\",\n    \"Right network colored by ground truth labels (purple and green clusters)\",\n    \"Callout box zooming into one component showing disagreement\",\n    \"Purple nodes representing Gamergate opposers\",\n    \"Green nodes representing Gamergate supporters\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the callout box show disagreement between purple and green node distributions despite using 8,128 labeled users?\",\n      \"answer\": \"The network structure-based communities (left) show very low agreement with the ground truth labels from Mechanical Turk (right), revealing that network attributes alone do not capture the manually-labeled user affiliations.\",\n      \"visual_anchor\": \"Callout box showing mixed purple and green nodes in zoomed component\",\n      \"text_evidence\": \"After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"What explains the purple cluster positions in the left network given that users with single tweets were excluded?\",\n      \"answer\": \"The left network shows communities detected purely by network structure (retweet patterns), while excluding single-tweet users from 21,441 total ensured sufficient data for structural analysis. The purple clusters emerge from connectivity patterns rather than content-based labels.\",\n      \"visual_anchor\": \"Purple clusters in left network visualization\",\n      \"text_evidence\": \"Out of 21,441 total users, we excluded users who had only single tweets and duplicated users with same tweets. We then asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the green node concentrations differ between left and right networks given the Gamergate supporter label definition?\",\n      \"answer\": \"The left network shows green nodes clustered by retweet connectivity patterns, while the right network shows green nodes positioned based on users fighting for ethics in journalism. The spatial disagreement reveals that structural communities do not align with ideological affiliations.\",\n      \"visual_anchor\": \"Green node clusters in left versus right network\",\n      \"text_evidence\": \"Gamergate Supporter: Users fighting for ethics in journalis\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Left network colored by network structure (purple and green clusters)",
        "Right network colored by ground truth labels (purple and green clusters)",
        "Callout box zooming into one component showing disagreement",
        "Purple nodes representing Gamergate opposers",
        "Green nodes representing Gamergate supporters"
      ],
      "queries": [
        {
          "query": "Why does the callout box show disagreement between purple and green node distributions despite using 8,128 labeled users?",
          "answer": "The network structure-based communities (left) show very low agreement with the ground truth labels from Mechanical Turk (right), revealing that network attributes alone do not capture the manually-labeled user affiliations.",
          "visual_anchor": "Callout box showing mixed purple and green nodes in zoomed component",
          "text_evidence": "After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement",
          "query_type": "anomaly_cause"
        },
        {
          "query": "What explains the purple cluster positions in the left network given that users with single tweets were excluded?",
          "answer": "The left network shows communities detected purely by network structure (retweet patterns), while excluding single-tweet users from 21,441 total ensured sufficient data for structural analysis. The purple clusters emerge from connectivity patterns rather than content-based labels.",
          "visual_anchor": "Purple clusters in left network visualization",
          "text_evidence": "Out of 21,441 total users, we excluded users who had only single tweets and duplicated users with same tweets. We then asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the green node concentrations differ between left and right networks given the Gamergate supporter label definition?",
          "answer": "The left network shows green nodes clustered by retweet connectivity patterns, while the right network shows green nodes positioned based on users fighting for ethics in journalism. The spatial disagreement reveals that structural communities do not align with ideological affiliations.",
          "visual_anchor": "Green node clusters in left versus right network",
          "text_evidence": "Gamergate Supporter: Users fighting for ethics in journalis",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the callout box show disagreement between purple and green node distributions despite using 8,128 labeled users?",
        "answer": "The network structure-based communities (left) show very low agreement with the ground truth labels from Mechanical Turk (right), revealing that network attributes alone do not capture the manually-labeled user affiliations.",
        "visual_anchor": "Callout box showing mixed purple and green nodes in zoomed component",
        "text_evidence": "After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement",
        "query_type": "anomaly_cause"
      },
      {
        "query": "What explains the purple cluster positions in the left network given that users with single tweets were excluded?",
        "answer": "The left network shows communities detected purely by network structure (retweet patterns), while excluding single-tweet users from 21,441 total ensured sufficient data for structural analysis. The purple clusters emerge from connectivity patterns rather than content-based labels.",
        "visual_anchor": "Purple clusters in left network visualization",
        "text_evidence": "Out of 21,441 total users, we excluded users who had only single tweets and duplicated users with same tweets. We then asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the green node concentrations differ between left and right networks given the Gamergate supporter label definition?",
        "answer": "The left network shows green nodes clustered by retweet connectivity patterns, while the right network shows green nodes positioned based on users fighting for ethics in journalism. The spatial disagreement reveals that structural communities do not align with ideological affiliations.",
        "visual_anchor": "Green node clusters in left versus right network",
        "text_evidence": "Gamergate Supporter: Users fighting for ethics in journalis",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Left network colored by network structure (purple and green clusters)",
      "Right network colored by ground truth labels (purple and green clusters)",
      "Callout box zooming into one component showing disagreement",
      "Purple nodes representing Gamergate opposers",
      "Green nodes representing Gamergate supporters"
    ]
  },
  {
    "doc_id": "1903.08136",
    "figure_id": "1903.08136_fig_6",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg",
    "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Two distinct network clusters colored by agreement with ground truth\",\n    \"Red and green dots representing correct and incorrect label assignments\",\n    \"Upper and lower cluster formations in the network visualization\",\n    \"Color-coded nodes showing agreement patterns for three methods\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the two distinct red-green clusters in the network support CESNA's ability to label the anti-Gamergate tweet correctly?\",\n      \"answer\": \"The table shows CESNA assigned N/A to the anti-Gamergate tweet, while CLAN correctly labeled it, indicating the visual clusters may represent CLAN's superior performance rather than CESNA's capabilities.\",\n      \"visual_anchor\": \"Two separated red-green network clusters\",\n      \"text_evidence\": \"\\\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\\\" CESNA Label: N/A, CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the upper and lower clusters show similar red-green mixing patterns despite Modularity assigning N/A labels?\",\n      \"answer\": \"Modularity's N/A assignments for both example tweets suggest it failed to classify these cases, so the visual mixing likely represents other methods' performance rather than Modularity's agreement with ground truth.\",\n      \"visual_anchor\": \"Red and green mixed patterns in upper and lower clusters\",\n      \"text_evidence\": \"\\\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\\\" CESNA Label: N/A, Modularity Label: N/A\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the green coloring in the network clusters indicate where CLAN disagreed with the anti-Gamergate ground truth?\",\n      \"answer\": \"No, CLAN correctly labeled the anti-Gamergate tweet, so green coloring represents disagreement by other methods. The visualization shows agreement patterns across all three methods, not just CLAN.\",\n      \"visual_anchor\": \"Green colored nodes distributed throughout both clusters\",\n      \"text_evidence\": \"Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset. CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Two distinct network clusters colored by agreement with ground truth",
        "Red and green dots representing correct and incorrect label assignments",
        "Upper and lower cluster formations in the network visualization",
        "Color-coded nodes showing agreement patterns for three methods"
      ],
      "queries": [
        {
          "query": "Do the two distinct red-green clusters in the network support CESNA's ability to label the anti-Gamergate tweet correctly?",
          "answer": "The table shows CESNA assigned N/A to the anti-Gamergate tweet, while CLAN correctly labeled it, indicating the visual clusters may represent CLAN's superior performance rather than CESNA's capabilities.",
          "visual_anchor": "Two separated red-green network clusters",
          "text_evidence": "\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\" CESNA Label: N/A, CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the upper and lower clusters show similar red-green mixing patterns despite Modularity assigning N/A labels?",
          "answer": "Modularity's N/A assignments for both example tweets suggest it failed to classify these cases, so the visual mixing likely represents other methods' performance rather than Modularity's agreement with ground truth.",
          "visual_anchor": "Red and green mixed patterns in upper and lower clusters",
          "text_evidence": "\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\" CESNA Label: N/A, Modularity Label: N/A",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the green coloring in the network clusters indicate where CLAN disagreed with the anti-Gamergate ground truth?",
          "answer": "No, CLAN correctly labeled the anti-Gamergate tweet, so green coloring represents disagreement by other methods. The visualization shows agreement patterns across all three methods, not just CLAN.",
          "visual_anchor": "Green colored nodes distributed throughout both clusters",
          "text_evidence": "Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset. CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the two distinct red-green clusters in the network support CESNA's ability to label the anti-Gamergate tweet correctly?",
        "answer": "The table shows CESNA assigned N/A to the anti-Gamergate tweet, while CLAN correctly labeled it, indicating the visual clusters may represent CLAN's superior performance rather than CESNA's capabilities.",
        "visual_anchor": "Two separated red-green network clusters",
        "text_evidence": "\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\" CESNA Label: N/A, CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the upper and lower clusters show similar red-green mixing patterns despite Modularity assigning N/A labels?",
        "answer": "Modularity's N/A assignments for both example tweets suggest it failed to classify these cases, so the visual mixing likely represents other methods' performance rather than Modularity's agreement with ground truth.",
        "visual_anchor": "Red and green mixed patterns in upper and lower clusters",
        "text_evidence": "\"#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame\" CESNA Label: N/A, Modularity Label: N/A",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the green coloring in the network clusters indicate where CLAN disagreed with the anti-Gamergate ground truth?",
        "answer": "No, CLAN correctly labeled the anti-Gamergate tweet, so green coloring represents disagreement by other methods. The visualization shows agreement patterns across all three methods, not just CLAN.",
        "visual_anchor": "Green colored nodes distributed throughout both clusters",
        "text_evidence": "Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset. CLAN Label: Anti Gamerge, Ground Truth Label: Anti Gamerge",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Two distinct network clusters colored by agreement with ground truth",
      "Red and green dots representing correct and incorrect label assignments",
      "Upper and lower cluster formations in the network visualization",
      "Color-coded nodes showing agreement patterns for three methods"
    ]
  },
  {
    "doc_id": "1903.08136",
    "figure_id": "1903.08136_fig_9",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg",
    "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Four oval network visualizations with colored node clusters\",\n    \"Green clusters in top-left and top-right networks\",\n    \"Cyan/blue clusters in bottom-left network\",\n    \"Magenta/purple clusters connecting regions\",\n    \"Label 'Slope' at the top center\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How do the green clusters in the top-right network relate to testing CLAN against overlooked introverts?\",\n      \"answer\": \"The top-right shows the original Gamergate distribution used to test whether CLAN's Step 1 community detection would ignore communities with members having low degree, like introverts who form few links.\",\n      \"visual_anchor\": \"green clusters in top-right oval network\",\n      \"text_evidence\": \"One potential issue with community detection is that it could completely ignore entire communities if membership in that community is correlated with degree. For example, a community of introverts is likely to be overlooked because they do not form many links. This presents a challenge to our approach as well, because Step 1 consists of identifying the major communities in the dataset.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the bottom-left network with cyan cluster represent a synthetic distribution with particular slope?\",\n      \"answer\": \"The bottom-left network is generated from a synthetic distribution designed with a specific slope parameter to test CLAN's resilience to skewed data where community membership correlates with degree.\",\n      \"visual_anchor\": \"bottom-left oval network with cyan cluster and label referencing slope\",\n      \"text_evidence\": \"In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the four network ovals with varying cluster colors demonstrate results from both Gamergate and presidential election datasets?\",\n      \"answer\": \"No, all four network visualizations show results exclusively from the Gamergate dataset, comparing the original distribution to synthetic variations. The presidential election dataset results appear separately in Table 5.\",\n      \"visual_anchor\": \"four oval networks with green, magenta, and cyan clusters\",\n      \"text_evidence\": \"Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Four oval network visualizations with colored node clusters",
        "Green clusters in top-left and top-right networks",
        "Cyan/blue clusters in bottom-left network",
        "Magenta/purple clusters connecting regions",
        "Label 'Slope' at the top center"
      ],
      "queries": [
        {
          "query": "How do the green clusters in the top-right network relate to testing CLAN against overlooked introverts?",
          "answer": "The top-right shows the original Gamergate distribution used to test whether CLAN's Step 1 community detection would ignore communities with members having low degree, like introverts who form few links.",
          "visual_anchor": "green clusters in top-right oval network",
          "text_evidence": "One potential issue with community detection is that it could completely ignore entire communities if membership in that community is correlated with degree. For example, a community of introverts is likely to be overlooked because they do not form many links. This presents a challenge to our approach as well, because Step 1 consists of identifying the major communities in the dataset.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the bottom-left network with cyan cluster represent a synthetic distribution with particular slope?",
          "answer": "The bottom-left network is generated from a synthetic distribution designed with a specific slope parameter to test CLAN's resilience to skewed data where community membership correlates with degree.",
          "visual_anchor": "bottom-left oval network with cyan cluster and label referencing slope",
          "text_evidence": "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the four network ovals with varying cluster colors demonstrate results from both Gamergate and presidential election datasets?",
          "answer": "No, all four network visualizations show results exclusively from the Gamergate dataset, comparing the original distribution to synthetic variations. The presidential election dataset results appear separately in Table 5.",
          "visual_anchor": "four oval networks with green, magenta, and cyan clusters",
          "text_evidence": "Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "How do the green clusters in the top-right network relate to testing CLAN against overlooked introverts?",
        "answer": "The top-right shows the original Gamergate distribution used to test whether CLAN's Step 1 community detection would ignore communities with members having low degree, like introverts who form few links.",
        "visual_anchor": "green clusters in top-right oval network",
        "text_evidence": "One potential issue with community detection is that it could completely ignore entire communities if membership in that community is correlated with degree. For example, a community of introverts is likely to be overlooked because they do not form many links. This presents a challenge to our approach as well, because Step 1 consists of identifying the major communities in the dataset.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the bottom-left network with cyan cluster represent a synthetic distribution with particular slope?",
        "answer": "The bottom-left network is generated from a synthetic distribution designed with a specific slope parameter to test CLAN's resilience to skewed data where community membership correlates with degree.",
        "visual_anchor": "bottom-left oval network with cyan cluster and label referencing slope",
        "text_evidence": "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the four network ovals with varying cluster colors demonstrate results from both Gamergate and presidential election datasets?",
        "answer": "No, all four network visualizations show results exclusively from the Gamergate dataset, comparing the original distribution to synthetic variations. The presidential election dataset results appear separately in Table 5.",
        "visual_anchor": "four oval networks with green, magenta, and cyan clusters",
        "text_evidence": "Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Four oval network visualizations with colored node clusters",
      "Green clusters in top-left and top-right networks",
      "Cyan/blue clusters in bottom-left network",
      "Magenta/purple clusters connecting regions",
      "Label 'Slope' at the top center"
    ]
  },
  {
    "doc_id": "1903.08136",
    "figure_id": "1903.08136_fig_14",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg",
    "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Four oval scatter plots arranged horizontally\",\n    \"Label 'Slope' at the top\",\n    \"Color gradients including magenta, cyan, green, and black particles\",\n    \"Label 'Degree' at the bottom right\",\n    \"Distinct particle distribution patterns in each oval\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How do the green particle clusters in each oval relate to lowly-connected nodes that CLAN aims to properly include?\",\n      \"answer\": \"The green clusters represent sparsely-connected nodes that CLAN minimizes bias for by including them into their true communities, unlike existing detection methods that fail to properly account for them.\",\n      \"visual_anchor\": \"green particle clusters visible in all four oval plots\",\n      \"text_evidence\": \"CLAN minimizes such biases by including the lowly-connected nodes into their true communities.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does the rightmost oval show a horizontal distribution pattern compared to the vertical patterns in the middle ovals?\",\n      \"answer\": \"The different distribution patterns correspond to synthetic distributions with particular slope values generated for the 2016 U.S. Presidential Election dataset, where varying slopes produce different spatial arrangements of communities.\",\n      \"visual_anchor\": \"horizontal green-to-cyan distribution in rightmost oval versus vertical distributions in middle ovals\",\n      \"text_evidence\": \"one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What relationship exists between the magenta-cyan clusters in the top portions and the modularity value mentioned for network coloring?\",\n      \"answer\": \"The magenta and cyan colored clusters represent the network colored by modularity value, showing how communities are distinguished in the 2016 U.S. Presidential Election dataset's synthetic distributions.\",\n      \"visual_anchor\": \"magenta and cyan colored particles in upper portions of the ovals\",\n      \"text_evidence\": \"the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Four oval scatter plots arranged horizontally",
        "Label 'Slope' at the top",
        "Color gradients including magenta, cyan, green, and black particles",
        "Label 'Degree' at the bottom right",
        "Distinct particle distribution patterns in each oval"
      ],
      "queries": [
        {
          "query": "How do the green particle clusters in each oval relate to lowly-connected nodes that CLAN aims to properly include?",
          "answer": "The green clusters represent sparsely-connected nodes that CLAN minimizes bias for by including them into their true communities, unlike existing detection methods that fail to properly account for them.",
          "visual_anchor": "green particle clusters visible in all four oval plots",
          "text_evidence": "CLAN minimizes such biases by including the lowly-connected nodes into their true communities.",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does the rightmost oval show a horizontal distribution pattern compared to the vertical patterns in the middle ovals?",
          "answer": "The different distribution patterns correspond to synthetic distributions with particular slope values generated for the 2016 U.S. Presidential Election dataset, where varying slopes produce different spatial arrangements of communities.",
          "visual_anchor": "horizontal green-to-cyan distribution in rightmost oval versus vertical distributions in middle ovals",
          "text_evidence": "one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What relationship exists between the magenta-cyan clusters in the top portions and the modularity value mentioned for network coloring?",
          "answer": "The magenta and cyan colored clusters represent the network colored by modularity value, showing how communities are distinguished in the 2016 U.S. Presidential Election dataset's synthetic distributions.",
          "visual_anchor": "magenta and cyan colored particles in upper portions of the ovals",
          "text_evidence": "the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How do the green particle clusters in each oval relate to lowly-connected nodes that CLAN aims to properly include?",
        "answer": "The green clusters represent sparsely-connected nodes that CLAN minimizes bias for by including them into their true communities, unlike existing detection methods that fail to properly account for them.",
        "visual_anchor": "green particle clusters visible in all four oval plots",
        "text_evidence": "CLAN minimizes such biases by including the lowly-connected nodes into their true communities.",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does the rightmost oval show a horizontal distribution pattern compared to the vertical patterns in the middle ovals?",
        "answer": "The different distribution patterns correspond to synthetic distributions with particular slope values generated for the 2016 U.S. Presidential Election dataset, where varying slopes produce different spatial arrangements of communities.",
        "visual_anchor": "horizontal green-to-cyan distribution in rightmost oval versus vertical distributions in middle ovals",
        "text_evidence": "one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What relationship exists between the magenta-cyan clusters in the top portions and the modularity value mentioned for network coloring?",
        "answer": "The magenta and cyan colored clusters represent the network colored by modularity value, showing how communities are distinguished in the 2016 U.S. Presidential Election dataset's synthetic distributions.",
        "visual_anchor": "magenta and cyan colored particles in upper portions of the ovals",
        "text_evidence": "the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Four oval scatter plots arranged horizontally",
      "Label 'Slope' at the top",
      "Color gradients including magenta, cyan, green, and black particles",
      "Label 'Degree' at the bottom right",
      "Distinct particle distribution patterns in each oval"
    ]
  },
  {
    "doc_id": "1903.10561",
    "figure_id": "1903.10561_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg",
    "caption": "Figure 1: Significance of results for all models and tests.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Heatmap with rows representing different tests (weat9, weat8b, sent-weat9, heilman tests, angry_black_woman_stereotype)\",\n    \"Columns representing different models (cased, bert-base-uncased, bert-large-uncased, bow, elmo-avg, etc.)\",\n    \"Color coding: white/very light blue (insignificant), light blue (significant at 0.01), dark blue (significant at 0.01 after correction)\",\n    \"Legend showing three significance levels\",\n    \"Dense dark blue regions in middle rows (sent-weat tests)\",\n    \"Sparse coloring in bottom rows (heilman and angry_black_woman tests)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do InferSent and GenSen show dark blue cells for sent-weat tests despite using BiLSTM architectures trained on MultiNLI and SNLI?\",\n      \"answer\": \"InferSent is a 4096-dimensional BiLSTM trained on MultiNLI and SNLI, while GenSen is a 2048-dimensional BiLSTM jointly trained on MultiNLI, SNLI, and other tasks. The dark blue cells indicate these encoders significantly reproduce biases at the 0.01 level after correction.\",\n      \"visual_anchor\": \"Dark blue cells in sent-weat rows (middle section) for InferSent and GenSen columns\",\n      \"text_evidence\": \"InferSent: A 4096-dimensional BiLSTM trained on both MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) with max pooling over the hidden states of the sequence\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the light blue coloring for CBoW in angry_black_woman_stereotype tests match its use of 300-dimensional GloVe Common Crawl vectors?\",\n      \"answer\": \"Yes, the light blue indicates significance at the 0.01 level. CBoW uses 300-dimensional GloVe vectors trained on Common Crawl as a simple baseline, encoding sentences as word embedding averages, which still captures stereotypical associations.\",\n      \"visual_anchor\": \"Light blue cell in angry_black_woman_stereotype row at bow (CBoW) column\",\n      \"text_evidence\": \"CBoW: As a simple baseline, we encode sentences as an average of the word embeddings. We use 300-dimensional GloVe vectors trained on the Common Crawl (Pennington et al., 2014).\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the predominantly white cells in heilman_double_bind tests compared to dark blue sent-weat rows for the same encoder models?\",\n      \"answer\": \"The heilman tests measure gender stereotypes using female given names from Sweeney (2013) as targets with specific stereotype-related adjectives as attributes. The white cells suggest these particular stereotype dimensions are less consistently reproduced across encoders than the broader biases captured in sent-weat tests.\",\n      \"visual_anchor\": \"White and very light blue cells in bottom heilman_double_bind rows versus dark blue cells in middle sent-weat rows\",\n      \"text_evidence\": \"To measure sentence encoders' reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Heatmap with rows representing different tests (weat9, weat8b, sent-weat9, heilman tests, angry_black_woman_stereotype)",
        "Columns representing different models (cased, bert-base-uncased, bert-large-uncased, bow, elmo-avg, etc.)",
        "Color coding: white/very light blue (insignificant), light blue (significant at 0.01), dark blue (significant at 0.01 after correction)",
        "Legend showing three significance levels",
        "Dense dark blue regions in middle rows (sent-weat tests)",
        "Sparse coloring in bottom rows (heilman and angry_black_woman tests)"
      ],
      "queries": [
        {
          "query": "Why do InferSent and GenSen show dark blue cells for sent-weat tests despite using BiLSTM architectures trained on MultiNLI and SNLI?",
          "answer": "InferSent is a 4096-dimensional BiLSTM trained on MultiNLI and SNLI, while GenSen is a 2048-dimensional BiLSTM jointly trained on MultiNLI, SNLI, and other tasks. The dark blue cells indicate these encoders significantly reproduce biases at the 0.01 level after correction.",
          "visual_anchor": "Dark blue cells in sent-weat rows (middle section) for InferSent and GenSen columns",
          "text_evidence": "InferSent: A 4096-dimensional BiLSTM trained on both MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) with max pooling over the hidden states of the sequence",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the light blue coloring for CBoW in angry_black_woman_stereotype tests match its use of 300-dimensional GloVe Common Crawl vectors?",
          "answer": "Yes, the light blue indicates significance at the 0.01 level. CBoW uses 300-dimensional GloVe vectors trained on Common Crawl as a simple baseline, encoding sentences as word embedding averages, which still captures stereotypical associations.",
          "visual_anchor": "Light blue cell in angry_black_woman_stereotype row at bow (CBoW) column",
          "text_evidence": "CBoW: As a simple baseline, we encode sentences as an average of the word embeddings. We use 300-dimensional GloVe vectors trained on the Common Crawl (Pennington et al., 2014).",
          "query_type": "value_context"
        },
        {
          "query": "What explains the predominantly white cells in heilman_double_bind tests compared to dark blue sent-weat rows for the same encoder models?",
          "answer": "The heilman tests measure gender stereotypes using female given names from Sweeney (2013) as targets with specific stereotype-related adjectives as attributes. The white cells suggest these particular stereotype dimensions are less consistently reproduced across encoders than the broader biases captured in sent-weat tests.",
          "visual_anchor": "White and very light blue cells in bottom heilman_double_bind rows versus dark blue cells in middle sent-weat rows",
          "text_evidence": "To measure sentence encoders' reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do InferSent and GenSen show dark blue cells for sent-weat tests despite using BiLSTM architectures trained on MultiNLI and SNLI?",
        "answer": "InferSent is a 4096-dimensional BiLSTM trained on MultiNLI and SNLI, while GenSen is a 2048-dimensional BiLSTM jointly trained on MultiNLI, SNLI, and other tasks. The dark blue cells indicate these encoders significantly reproduce biases at the 0.01 level after correction.",
        "visual_anchor": "Dark blue cells in sent-weat rows (middle section) for InferSent and GenSen columns",
        "text_evidence": "InferSent: A 4096-dimensional BiLSTM trained on both MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) with max pooling over the hidden states of the sequence",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the light blue coloring for CBoW in angry_black_woman_stereotype tests match its use of 300-dimensional GloVe Common Crawl vectors?",
        "answer": "Yes, the light blue indicates significance at the 0.01 level. CBoW uses 300-dimensional GloVe vectors trained on Common Crawl as a simple baseline, encoding sentences as word embedding averages, which still captures stereotypical associations.",
        "visual_anchor": "Light blue cell in angry_black_woman_stereotype row at bow (CBoW) column",
        "text_evidence": "CBoW: As a simple baseline, we encode sentences as an average of the word embeddings. We use 300-dimensional GloVe vectors trained on the Common Crawl (Pennington et al., 2014).",
        "query_type": "value_context"
      },
      {
        "query": "What explains the predominantly white cells in heilman_double_bind tests compared to dark blue sent-weat rows for the same encoder models?",
        "answer": "The heilman tests measure gender stereotypes using female given names from Sweeney (2013) as targets with specific stereotype-related adjectives as attributes. The white cells suggest these particular stereotype dimensions are less consistently reproduced across encoders than the broader biases captured in sent-weat tests.",
        "visual_anchor": "White and very light blue cells in bottom heilman_double_bind rows versus dark blue cells in middle sent-weat rows",
        "text_evidence": "To measure sentence encoders' reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Heatmap with rows representing different tests (weat9, weat8b, sent-weat9, heilman tests, angry_black_woman_stereotype)",
      "Columns representing different models (cased, bert-base-uncased, bert-large-uncased, bow, elmo-avg, etc.)",
      "Color coding: white/very light blue (insignificant), light blue (significant at 0.01), dark blue (significant at 0.01 after correction)",
      "Legend showing three significance levels",
      "Dense dark blue regions in middle rows (sent-weat tests)",
      "Sparse coloring in bottom rows (heilman and angry_black_woman tests)"
    ]
  },
  {
    "doc_id": "1903.10598",
    "figure_id": "1903.10598_fig_1",
    "figure_number": null,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg",
    "caption": "",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Accuracy (%) on y-axis ranging from 72 to 84\", \"Discrimination (%) on x-axis ranging from 0.0 to 1.2\", \"Family·MIP shown as black circles\", \"DADT shown as gray triangles\", \"CART shown as gray squares\", \"logistic shown as light gray diamonds\", \"Multiple data points connected by lines for each method\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the black circles representing Family·MIP cluster between 78-86% accuracy while DADT gray triangles span 72-84%?\",\n      \"answer\": \"Family·MIP maintains consistently higher accuracy across discrimination levels because it is optimized using the MIP formulation that minimizes the specified loss function, whereas DADT shows greater variability in its accuracy-discrimination trade-off.\",\n      \"visual_anchor\": \"black circles (Family·MIP) clustered at 78-86% accuracy versus gray triangles (DADT) spanning 72-84%\",\n      \"text_evidence\": \"minimize $\\\\ell _ { \\\\mathrm { c / r } } ( \\\\math\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the gray squares for CART at discrimination 0.6-1.2 support using leaf node notation for categorical feature levels?\",\n      \"answer\": \"Yes, CART's gray squares show a concentrated cluster at 83-85% accuracy across discrimination 0.6-1.2, consistent with decision tree structures that partition data using leaf nodes as described in the formulation.\",\n      \"visual_anchor\": \"gray squares (CART) clustered at 83-85% accuracy in discrimination range 0.6-1.2\",\n      \"text_evidence\": \"For $\\\\nu \\\\in \\\\mathcal V$ , let $\\\\mathcal { L } ^ { \\\\mathrm { r } } ( \\\\nu )$ (resp. $\\\\mathcal { L } ^ { 1 } ( \\\\nu ) )$ denote all the leaf nodes that lie to the right (resp. left) of node $\\\\nu$\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the steep accuracy drop from 78% to 72% in gray triangles as discrimination decreases from 0.4 to 0.8?\",\n      \"answer\": \"The DADT method shows this steep decline because it enforces fairness constraints more aggressively at higher discrimination values, trading off accuracy. This contrasts with the feature-based formulation where the jth feature values are constrained by possible levels in χ_j.\",\n      \"visual_anchor\": \"gray triangles (DADT) dropping from 78% at discrimination 0.4 to 72% at discrimination 0.8\",\n      \"text_evidence\": \"Denote with $\\\\mathbf { x } _ { i , j }$ the value attained by the $j$ th feature of the ith data point and for $j \\\\in \\\\mathcal { F } _ { \\\\mathrm { c } }$ , let $\\\\chi _ { j }$ collect the possible levels attainable by feature $j$\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Accuracy (%) on y-axis ranging from 72 to 84",
        "Discrimination (%) on x-axis ranging from 0.0 to 1.2",
        "Family·MIP shown as black circles",
        "DADT shown as gray triangles",
        "CART shown as gray squares",
        "logistic shown as light gray diamonds",
        "Multiple data points connected by lines for each method"
      ],
      "queries": [
        {
          "query": "Why do the black circles representing Family·MIP cluster between 78-86% accuracy while DADT gray triangles span 72-84%?",
          "answer": "Family·MIP maintains consistently higher accuracy across discrimination levels because it is optimized using the MIP formulation that minimizes the specified loss function, whereas DADT shows greater variability in its accuracy-discrimination trade-off.",
          "visual_anchor": "black circles (Family·MIP) clustered at 78-86% accuracy versus gray triangles (DADT) spanning 72-84%",
          "text_evidence": "minimize $\\ell _ { \\mathrm { c / r } } ( \\math",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the gray squares for CART at discrimination 0.6-1.2 support using leaf node notation for categorical feature levels?",
          "answer": "Yes, CART's gray squares show a concentrated cluster at 83-85% accuracy across discrimination 0.6-1.2, consistent with decision tree structures that partition data using leaf nodes as described in the formulation.",
          "visual_anchor": "gray squares (CART) clustered at 83-85% accuracy in discrimination range 0.6-1.2",
          "text_evidence": "For $\\nu \\in \\mathcal V$ , let $\\mathcal { L } ^ { \\mathrm { r } } ( \\nu )$ (resp. $\\mathcal { L } ^ { 1 } ( \\nu ) )$ denote all the leaf nodes that lie to the right (resp. left) of node $\\nu$",
          "query_type": "value_context"
        },
        {
          "query": "What explains the steep accuracy drop from 78% to 72% in gray triangles as discrimination decreases from 0.4 to 0.8?",
          "answer": "The DADT method shows this steep decline because it enforces fairness constraints more aggressively at higher discrimination values, trading off accuracy. This contrasts with the feature-based formulation where the jth feature values are constrained by possible levels in χ_j.",
          "visual_anchor": "gray triangles (DADT) dropping from 78% at discrimination 0.4 to 72% at discrimination 0.8",
          "text_evidence": "Denote with $\\mathbf { x } _ { i , j }$ the value attained by the $j$ th feature of the ith data point and for $j \\in \\mathcal { F } _ { \\mathrm { c } }$ , let $\\chi _ { j }$ collect the possible levels attainable by feature $j$",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the black circles representing Family·MIP cluster between 78-86% accuracy while DADT gray triangles span 72-84%?",
        "answer": "Family·MIP maintains consistently higher accuracy across discrimination levels because it is optimized using the MIP formulation that minimizes the specified loss function, whereas DADT shows greater variability in its accuracy-discrimination trade-off.",
        "visual_anchor": "black circles (Family·MIP) clustered at 78-86% accuracy versus gray triangles (DADT) spanning 72-84%",
        "text_evidence": "minimize $\\ell _ { \\mathrm { c / r } } ( \\math",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the gray squares for CART at discrimination 0.6-1.2 support using leaf node notation for categorical feature levels?",
        "answer": "Yes, CART's gray squares show a concentrated cluster at 83-85% accuracy across discrimination 0.6-1.2, consistent with decision tree structures that partition data using leaf nodes as described in the formulation.",
        "visual_anchor": "gray squares (CART) clustered at 83-85% accuracy in discrimination range 0.6-1.2",
        "text_evidence": "For $\\nu \\in \\mathcal V$ , let $\\mathcal { L } ^ { \\mathrm { r } } ( \\nu )$ (resp. $\\mathcal { L } ^ { 1 } ( \\nu ) )$ denote all the leaf nodes that lie to the right (resp. left) of node $\\nu$",
        "query_type": "value_context"
      },
      {
        "query": "What explains the steep accuracy drop from 78% to 72% in gray triangles as discrimination decreases from 0.4 to 0.8?",
        "answer": "The DADT method shows this steep decline because it enforces fairness constraints more aggressively at higher discrimination values, trading off accuracy. This contrasts with the feature-based formulation where the jth feature values are constrained by possible levels in χ_j.",
        "visual_anchor": "gray triangles (DADT) dropping from 78% at discrimination 0.4 to 72% at discrimination 0.8",
        "text_evidence": "Denote with $\\mathbf { x } _ { i , j }$ the value attained by the $j$ th feature of the ith data point and for $j \\in \\mathcal { F } _ { \\mathrm { c } }$ , let $\\chi _ { j }$ collect the possible levels attainable by feature $j$",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Accuracy (%) on y-axis ranging from 72 to 84",
      "Discrimination (%) on x-axis ranging from 0.0 to 1.2",
      "Family·MIP shown as black circles",
      "DADT shown as gray triangles",
      "CART shown as gray squares",
      "logistic shown as light gray diamonds",
      "Multiple data points connected by lines for each method"
    ]
  },
  {
    "doc_id": "1903.10598",
    "figure_id": "1903.10598_fig_4",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg",
    "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Triangular markers (CART)\", \"Square markers (Regression)\", \"Circular markers (MIP)\", \"X-axis: Discrimination (%)\", \"Y-axis: MAE (%)\", \"Shaded convex hull areas\", \"Discrimination range 0.000-0.020\", \"MAE range 0.05-0.25\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do triangular markers span discrimination values 0.010 to 0.015 when representing results from 5-fold cross-validation?\",\n      \"answer\": \"Each triangular marker (CART) represents accuracy and discrimination evaluated on a different test set fold. The spread from 0.010 to 0.015 reflects variability across the 5 cross-validation splits.\",\n      \"visual_anchor\": \"Triangular markers (CART) positioned between 0.010 and 0.015 on discrimination axis\",\n      \"text_evidence\": \"Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Do the square markers near discrimination 0.013 achieve lower MAE than triangular markers at discrimination 0.010?\",\n      \"answer\": \"Yes, the square markers (Regression) near 0.013 show MAE around 0.14-0.15, which is lower than the triangular markers (CART) at 0.010 showing MAE around 0.14 or higher, supporting different regression versus classification trade-offs.\",\n      \"visual_anchor\": \"Square markers at discrimination ~0.013 versus triangular markers at discrimination ~0.010\",\n      \"text_evidence\": \"Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Which approach shows the smallest discrimination value among the circular markers clustered near MAE 0.18?\",\n      \"answer\": \"The circular markers (MIP) clustered near MAE 0.18 show discrimination values very close to 0.000, representing the approach with minimal discrimination at that accuracy level on the Crime regression dataset.\",\n      \"visual_anchor\": \"Circular markers (MIP) clustered at MAE ~0.18 and discrimination near 0.000\",\n      \"text_evidence\": \"we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with λ⋆ on the test set and add this as a point in the corresponding graph in Figure 1.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Triangular markers (CART)",
        "Square markers (Regression)",
        "Circular markers (MIP)",
        "X-axis: Discrimination (%)",
        "Y-axis: MAE (%)",
        "Shaded convex hull areas",
        "Discrimination range 0.000-0.020",
        "MAE range 0.05-0.25"
      ],
      "queries": [
        {
          "query": "Why do triangular markers span discrimination values 0.010 to 0.015 when representing results from 5-fold cross-validation?",
          "answer": "Each triangular marker (CART) represents accuracy and discrimination evaluated on a different test set fold. The spread from 0.010 to 0.015 reflects variability across the 5 cross-validation splits.",
          "visual_anchor": "Triangular markers (CART) positioned between 0.010 and 0.015 on discrimination axis",
          "text_evidence": "Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space.",
          "query_type": "visual_definition"
        },
        {
          "query": "Do the square markers near discrimination 0.013 achieve lower MAE than triangular markers at discrimination 0.010?",
          "answer": "Yes, the square markers (Regression) near 0.013 show MAE around 0.14-0.15, which is lower than the triangular markers (CART) at 0.010 showing MAE around 0.14 or higher, supporting different regression versus classification trade-offs.",
          "visual_anchor": "Square markers at discrimination ~0.013 versus triangular markers at discrimination ~0.010",
          "text_evidence": "Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Which approach shows the smallest discrimination value among the circular markers clustered near MAE 0.18?",
          "answer": "The circular markers (MIP) clustered near MAE 0.18 show discrimination values very close to 0.000, representing the approach with minimal discrimination at that accuracy level on the Crime regression dataset.",
          "visual_anchor": "Circular markers (MIP) clustered at MAE ~0.18 and discrimination near 0.000",
          "text_evidence": "we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with λ⋆ on the test set and add this as a point in the corresponding graph in Figure 1.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do triangular markers span discrimination values 0.010 to 0.015 when representing results from 5-fold cross-validation?",
        "answer": "Each triangular marker (CART) represents accuracy and discrimination evaluated on a different test set fold. The spread from 0.010 to 0.015 reflects variability across the 5 cross-validation splits.",
        "visual_anchor": "Triangular markers (CART) positioned between 0.010 and 0.015 on discrimination axis",
        "text_evidence": "Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space.",
        "query_type": "visual_definition"
      },
      {
        "query": "Do the square markers near discrimination 0.013 achieve lower MAE than triangular markers at discrimination 0.010?",
        "answer": "Yes, the square markers (Regression) near 0.013 show MAE around 0.14-0.15, which is lower than the triangular markers (CART) at 0.010 showing MAE around 0.14 or higher, supporting different regression versus classification trade-offs.",
        "visual_anchor": "Square markers at discrimination ~0.013 versus triangular markers at discrimination ~0.010",
        "text_evidence": "Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Which approach shows the smallest discrimination value among the circular markers clustered near MAE 0.18?",
        "answer": "The circular markers (MIP) clustered near MAE 0.18 show discrimination values very close to 0.000, representing the approach with minimal discrimination at that accuracy level on the Crime regression dataset.",
        "visual_anchor": "Circular markers (MIP) clustered at MAE ~0.18 and discrimination near 0.000",
        "text_evidence": "we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with λ⋆ on the test set and add this as a point in the corresponding graph in Figure 1.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Triangular markers (CART)",
      "Square markers (Regression)",
      "Circular markers (MIP)",
      "X-axis: Discrimination (%)",
      "Y-axis: MAE (%)",
      "Shaded convex hull areas",
      "Discrimination range 0.000-0.020",
      "MAE range 0.05-0.25"
    ]
  },
  {
    "doc_id": "1903.10598",
    "figure_id": "1903.10598_fig_6",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg",
    "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid line labeled MIP\",\n    \"Dashed line labeled Training\",\n    \"Solid line with diamond markers labeled CART\",\n    \"Solid line with circle markers labeled MIP\",\n    \"Circle markers for Accuracy\",\n    \"Triangle markers for DTDI\",\n    \"Tree Depth on x-axis (1-6)\",\n    \"DTDI / ACC (%) on y-axis (0-100)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does MIP's DTDI triangle marker stay near zero across all tree depths while CART's rises to 40%?\",\n      \"answer\": \"MIP maintains fairness by ensuring individuals differing only in protected characteristics are twice as likely to receive the same treatment compared to CART, keeping DTDI near zero.\",\n      \"visual_anchor\": \"Triangle markers showing DTDI: MIP near 0%, CART rising to ~40%\",\n      \"text_evidence\": \"the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does MIP's accuracy circle marker at approximately 80% across depths 2-6 validate the fairness metric improvement?\",\n      \"answer\": \"Yes, MIP maintains high accuracy around 80% while achieving DTDI of 0, demonstrating that fairness improvements do not compromise predictive performance.\",\n      \"visual_anchor\": \"Circle markers for MIP Accuracy at ~80% from tree depth 2 to 6\",\n      \"text_evidence\": \"this is in line with our metric – in this experiment, DTDI was $0\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does CART's DTDI triangle marker peak near 40% at depth 3 then decline to 35% at depth 6?\",\n      \"answer\": \"The DTDI peak at depth 3 suggests maximum disparity in protected characteristic treatment at moderate complexity, with slight improvement at higher depths possibly due to overfitting or more granular splits.\",\n      \"visual_anchor\": \"CART's triangle markers peaking at ~40% at tree depth 3, declining to ~35% at depth 6\",\n      \"text_evidence\": \"Accuracy and fairness in dependence of tree depth\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid line labeled MIP",
        "Dashed line labeled Training",
        "Solid line with diamond markers labeled CART",
        "Solid line with circle markers labeled MIP",
        "Circle markers for Accuracy",
        "Triangle markers for DTDI",
        "Tree Depth on x-axis (1-6)",
        "DTDI / ACC (%) on y-axis (0-100)"
      ],
      "queries": [
        {
          "query": "Why does MIP's DTDI triangle marker stay near zero across all tree depths while CART's rises to 40%?",
          "answer": "MIP maintains fairness by ensuring individuals differing only in protected characteristics are twice as likely to receive the same treatment compared to CART, keeping DTDI near zero.",
          "visual_anchor": "Triangle markers showing DTDI: MIP near 0%, CART rising to ~40%",
          "text_evidence": "the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does MIP's accuracy circle marker at approximately 80% across depths 2-6 validate the fairness metric improvement?",
          "answer": "Yes, MIP maintains high accuracy around 80% while achieving DTDI of 0, demonstrating that fairness improvements do not compromise predictive performance.",
          "visual_anchor": "Circle markers for MIP Accuracy at ~80% from tree depth 2 to 6",
          "text_evidence": "this is in line with our metric – in this experiment, DTDI was $0",
          "query_type": "value_context"
        },
        {
          "query": "Why does CART's DTDI triangle marker peak near 40% at depth 3 then decline to 35% at depth 6?",
          "answer": "The DTDI peak at depth 3 suggests maximum disparity in protected characteristic treatment at moderate complexity, with slight improvement at higher depths possibly due to overfitting or more granular splits.",
          "visual_anchor": "CART's triangle markers peaking at ~40% at tree depth 3, declining to ~35% at depth 6",
          "text_evidence": "Accuracy and fairness in dependence of tree depth",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does MIP's DTDI triangle marker stay near zero across all tree depths while CART's rises to 40%?",
        "answer": "MIP maintains fairness by ensuring individuals differing only in protected characteristics are twice as likely to receive the same treatment compared to CART, keeping DTDI near zero.",
        "visual_anchor": "Triangle markers showing DTDI: MIP near 0%, CART rising to ~40%",
        "text_evidence": "the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does MIP's accuracy circle marker at approximately 80% across depths 2-6 validate the fairness metric improvement?",
        "answer": "Yes, MIP maintains high accuracy around 80% while achieving DTDI of 0, demonstrating that fairness improvements do not compromise predictive performance.",
        "visual_anchor": "Circle markers for MIP Accuracy at ~80% from tree depth 2 to 6",
        "text_evidence": "this is in line with our metric – in this experiment, DTDI was $0",
        "query_type": "value_context"
      },
      {
        "query": "Why does CART's DTDI triangle marker peak near 40% at depth 3 then decline to 35% at depth 6?",
        "answer": "The DTDI peak at depth 3 suggests maximum disparity in protected characteristic treatment at moderate complexity, with slight improvement at higher depths possibly due to overfitting or more granular splits.",
        "visual_anchor": "CART's triangle markers peaking at ~40% at tree depth 3, declining to ~35% at depth 6",
        "text_evidence": "Accuracy and fairness in dependence of tree depth",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid line labeled MIP",
      "Dashed line labeled Training",
      "Solid line with diamond markers labeled CART",
      "Solid line with circle markers labeled MIP",
      "Circle markers for Accuracy",
      "Triangle markers for DTDI",
      "Tree Depth on x-axis (1-6)",
      "DTDI / ACC (%) on y-axis (0-100)"
    ]
  },
  {
    "doc_id": "1903.10598",
    "figure_id": "1903.10598_fig_9",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg",
    "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three grouped bar charts labeled Default, Adult Dataset, and COMPAS\",\n    \"Dark bars representing MIP-BT and MIP-BT-A approaches\",\n    \"Medium gray bars for IGCS and IGCS-RLb\",\n    \"Light gray bars for log-ind and log-grp\",\n    \"White bar with error bars labeled N\",\n    \"Y-axis showing Accuracy (%) ranging from 0 to 75\",\n    \"Error bars on several bars particularly visible on white bar in Default group\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the white bar in the Default group show approximately 45% accuracy with large error bars?\",\n      \"answer\": \"The white bar represents the maximally non-discriminative model (N) in the Default approach, showing lower accuracy with high variance because it achieves zero discrimination at the cost of predictive performance.\",\n      \"visual_anchor\": \"white bar with large error bars at approximately 45% in the Default group\",\n      \"text_evidence\": \"Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Do the COMPAS dark bars reaching approximately 90% accuracy support claims about maximally non-discriminative model performance?\",\n      \"answer\": \"Yes, the COMPAS dark bars (MIP-BT approaches) show that maximally non-discriminative models can achieve high accuracy around 90%, demonstrating that zero discrimination does not always require sacrificing predictive performance substantially.\",\n      \"visual_anchor\": \"dark bars in COMPAS group reaching approximately 90% accuracy\",\n      \"text_evidence\": \"The authors gratefully acknowledge support from Schmidt Futures and from the James H. Zumberge Faculty Research and Innovation Fund at the University of Southern California.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do all Adult Dataset bars cluster near 75% accuracy despite representing different fairness approaches?\",\n      \"answer\": \"All Adult Dataset approaches achieve similar accuracy around 75% because they represent maximally non-discriminative models that maintain competitive performance while enforcing zero discrimination constraints across different methodological frameworks.\",\n      \"visual_anchor\": \"all bars in Adult Dataset group clustered between approximately 70-80% accuracy\",\n      \"text_evidence\": \"They thank the 6 anonymous referees whose reviews helped substantially improve the quality of the paper.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three grouped bar charts labeled Default, Adult Dataset, and COMPAS",
        "Dark bars representing MIP-BT and MIP-BT-A approaches",
        "Medium gray bars for IGCS and IGCS-RLb",
        "Light gray bars for log-ind and log-grp",
        "White bar with error bars labeled N",
        "Y-axis showing Accuracy (%) ranging from 0 to 75",
        "Error bars on several bars particularly visible on white bar in Default group"
      ],
      "queries": [
        {
          "query": "Why does the white bar in the Default group show approximately 45% accuracy with large error bars?",
          "answer": "The white bar represents the maximally non-discriminative model (N) in the Default approach, showing lower accuracy with high variance because it achieves zero discrimination at the cost of predictive performance.",
          "visual_anchor": "white bar with large error bars at approximately 45% in the Default group",
          "text_evidence": "Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.",
          "query_type": "value_context"
        },
        {
          "query": "Do the COMPAS dark bars reaching approximately 90% accuracy support claims about maximally non-discriminative model performance?",
          "answer": "Yes, the COMPAS dark bars (MIP-BT approaches) show that maximally non-discriminative models can achieve high accuracy around 90%, demonstrating that zero discrimination does not always require sacrificing predictive performance substantially.",
          "visual_anchor": "dark bars in COMPAS group reaching approximately 90% accuracy",
          "text_evidence": "The authors gratefully acknowledge support from Schmidt Futures and from the James H. Zumberge Faculty Research and Innovation Fund at the University of Southern California.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do all Adult Dataset bars cluster near 75% accuracy despite representing different fairness approaches?",
          "answer": "All Adult Dataset approaches achieve similar accuracy around 75% because they represent maximally non-discriminative models that maintain competitive performance while enforcing zero discrimination constraints across different methodological frameworks.",
          "visual_anchor": "all bars in Adult Dataset group clustered between approximately 70-80% accuracy",
          "text_evidence": "They thank the 6 anonymous referees whose reviews helped substantially improve the quality of the paper.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the white bar in the Default group show approximately 45% accuracy with large error bars?",
        "answer": "The white bar represents the maximally non-discriminative model (N) in the Default approach, showing lower accuracy with high variance because it achieves zero discrimination at the cost of predictive performance.",
        "visual_anchor": "white bar with large error bars at approximately 45% in the Default group",
        "text_evidence": "Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.",
        "query_type": "value_context"
      },
      {
        "query": "Do the COMPAS dark bars reaching approximately 90% accuracy support claims about maximally non-discriminative model performance?",
        "answer": "Yes, the COMPAS dark bars (MIP-BT approaches) show that maximally non-discriminative models can achieve high accuracy around 90%, demonstrating that zero discrimination does not always require sacrificing predictive performance substantially.",
        "visual_anchor": "dark bars in COMPAS group reaching approximately 90% accuracy",
        "text_evidence": "The authors gratefully acknowledge support from Schmidt Futures and from the James H. Zumberge Faculty Research and Innovation Fund at the University of Southern California.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do all Adult Dataset bars cluster near 75% accuracy despite representing different fairness approaches?",
        "answer": "All Adult Dataset approaches achieve similar accuracy around 75% because they represent maximally non-discriminative models that maintain competitive performance while enforcing zero discrimination constraints across different methodological frameworks.",
        "visual_anchor": "all bars in Adult Dataset group clustered between approximately 70-80% accuracy",
        "text_evidence": "They thank the 6 anonymous referees whose reviews helped substantially improve the quality of the paper.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three grouped bar charts labeled Default, Adult Dataset, and COMPAS",
      "Dark bars representing MIP-BT and MIP-BT-A approaches",
      "Medium gray bars for IGCS and IGCS-RLb",
      "Light gray bars for log-ind and log-grp",
      "White bar with error bars labeled N",
      "Y-axis showing Accuracy (%) ranging from 0 to 75",
      "Error bars on several bars particularly visible on white bar in Default group"
    ]
  },
  {
    "doc_id": "1904.03035",
    "figure_id": "1904.03035_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg",
    "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Training Corpus (stacked documents, top left)\",\n    \"Bias in Training Corpus (curve, bottom left)\",\n    \"Training (Word Level Language Model) - vertical bar with text\",\n    \"Cross Entropy Loss (box, top path)\",\n    \"Cross Entropy Loss + λ(N,B) (box, bottom path)\",\n    \"Generated Text (stacked documents, top right)\",\n    \"Bias in Generated Text (curve, top right)\",\n    \"Generated Text with Regularization (stacked documents, middle right)\",\n    \"Bias after Regularization (curve, bottom right)\",\n    \"Arrows showing flow between components\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the bottom path include λ(N,B) term while the top Cross Entropy Loss box lacks regularization parameters?\",\n      \"answer\": \"The bottom path applies a regularization procedure that encourages embeddings to depend minimally on gender, controlled by λ, while the top path uses standard cross entropy loss without debiasing.\",\n      \"visual_anchor\": \"Cross Entropy Loss + λ(N,B) box in bottom path versus plain Cross Entropy Loss box in top path\",\n      \"text_evidence\": \"we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the bias curve shape change from Generated Text to Bias after Regularization in the diagram's right side?\",\n      \"answer\": \"The bias curve becomes lower and flatter after regularization, indicating reduced discriminatory bias in the generated output compared to the unregularized model's generated text.\",\n      \"visual_anchor\": \"Bias in Generated Text curve (top right) compared to Bias after Regularization curve (bottom right)\",\n      \"text_evidence\": \"Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What role does the vertical Training component play between the corpus and the two distinct loss function pathways?\",\n      \"answer\": \"The Training component represents a three-layer LSTM word-level language model that processes the corpus and feeds into both the standard cross entropy loss and the regularized loss with λ(N,B) pathways.\",\n      \"visual_anchor\": \"Training (Word Level Language Model) vertical bar positioned between Training Corpus on left and both loss boxes on right\",\n      \"text_evidence\": \"We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Training Corpus (stacked documents, top left)",
        "Bias in Training Corpus (curve, bottom left)",
        "Training (Word Level Language Model) - vertical bar with text",
        "Cross Entropy Loss (box, top path)",
        "Cross Entropy Loss + λ(N,B) (box, bottom path)",
        "Generated Text (stacked documents, top right)",
        "Bias in Generated Text (curve, top right)",
        "Generated Text with Regularization (stacked documents, middle right)",
        "Bias after Regularization (curve, bottom right)",
        "Arrows showing flow between components"
      ],
      "queries": [
        {
          "query": "Why does the bottom path include λ(N,B) term while the top Cross Entropy Loss box lacks regularization parameters?",
          "answer": "The bottom path applies a regularization procedure that encourages embeddings to depend minimally on gender, controlled by λ, while the top path uses standard cross entropy loss without debiasing.",
          "visual_anchor": "Cross Entropy Loss + λ(N,B) box in bottom path versus plain Cross Entropy Loss box in top path",
          "text_evidence": "we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the bias curve shape change from Generated Text to Bias after Regularization in the diagram's right side?",
          "answer": "The bias curve becomes lower and flatter after regularization, indicating reduced discriminatory bias in the generated output compared to the unregularized model's generated text.",
          "visual_anchor": "Bias in Generated Text curve (top right) compared to Bias after Regularization curve (bottom right)",
          "text_evidence": "Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What role does the vertical Training component play between the corpus and the two distinct loss function pathways?",
          "answer": "The Training component represents a three-layer LSTM word-level language model that processes the corpus and feeds into both the standard cross entropy loss and the regularized loss with λ(N,B) pathways.",
          "visual_anchor": "Training (Word Level Language Model) vertical bar positioned between Training Corpus on left and both loss boxes on right",
          "text_evidence": "We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the bottom path include λ(N,B) term while the top Cross Entropy Loss box lacks regularization parameters?",
        "answer": "The bottom path applies a regularization procedure that encourages embeddings to depend minimally on gender, controlled by λ, while the top path uses standard cross entropy loss without debiasing.",
        "visual_anchor": "Cross Entropy Loss + λ(N,B) box in bottom path versus plain Cross Entropy Loss box in top path",
        "text_evidence": "we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the bias curve shape change from Generated Text to Bias after Regularization in the diagram's right side?",
        "answer": "The bias curve becomes lower and flatter after regularization, indicating reduced discriminatory bias in the generated output compared to the unregularized model's generated text.",
        "visual_anchor": "Bias in Generated Text curve (top right) compared to Bias after Regularization curve (bottom right)",
        "text_evidence": "Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What role does the vertical Training component play between the corpus and the two distinct loss function pathways?",
        "answer": "The Training component represents a three-layer LSTM word-level language model that processes the corpus and feeds into both the standard cross entropy loss and the regularized loss with λ(N,B) pathways.",
        "visual_anchor": "Training (Word Level Language Model) vertical bar positioned between Training Corpus on left and both loss boxes on right",
        "text_evidence": "We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Training Corpus (stacked documents, top left)",
      "Bias in Training Corpus (curve, bottom left)",
      "Training (Word Level Language Model) - vertical bar with text",
      "Cross Entropy Loss (box, top path)",
      "Cross Entropy Loss + λ(N,B) (box, bottom path)",
      "Generated Text (stacked documents, top right)",
      "Bias in Generated Text (curve, top right)",
      "Generated Text with Regularization (stacked documents, middle right)",
      "Bias after Regularization (curve, bottom right)",
      "Arrows showing flow between components"
    ]
  },
  {
    "doc_id": "1904.03310",
    "figure_id": "1904.03310_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg",
    "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Bar chart showing decreasing percentage of explained variance\",\n    \"First bar at position 0 reaching approximately 0.27 (27%)\",\n    \"Second bar at position 1 reaching approximately 0.22 (22%)\",\n    \"Third bar at position 2 reaching approximately 0.09 (9%)\",\n    \"Bars 3-8 showing values below 0.05\",\n    \"Y-axis ranging from 0 to 0.30\",\n    \"X-axis showing positions 0 through 8\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the first two bars at positions 0 and 1 reach 0.27 and 0.22 instead of one dominant component?\",\n      \"answer\": \"ELMo has two principal components for gender, unlike GloVe which has only one. This explains why the first two bars show comparable variance levels rather than a single dominant component.\",\n      \"visual_anchor\": \"First bar at position 0 (0.27) and second bar at position 1 (0.22)\",\n      \"text_evidence\": \"Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the rapid decline from bar 2 to bar 3 dropping below 0.05 support the claim about male-female pronoun co-occurrence patterns?\",\n      \"answer\": \"Yes, the sharp drop after the first two components suggests that gender variance is captured in just two dimensions. This aligns with the corpus analysis showing systematic co-occurrence patterns between gendered pronouns and occupation words.\",\n      \"visual_anchor\": \"Bar at position 2 (0.09) dropping to bar at position 3 (below 0.05)\",\n      \"text_evidence\": \"We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains why bars 3 through 8 all remain below 0.05 when testing ELMo's gender embedding behavior?\",\n      \"answer\": \"The classifier testing ELMo's gender prediction from occupation words reveals that gender information is concentrated in the first two components. The remaining components contribute negligible variance, indicating gender is encoded in a low-dimensional subspace.\",\n      \"visual_anchor\": \"Bars at positions 3-8 all below 0.05\",\n      \"text_evidence\": \"To test how ELMo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Bar chart showing decreasing percentage of explained variance",
        "First bar at position 0 reaching approximately 0.27 (27%)",
        "Second bar at position 1 reaching approximately 0.22 (22%)",
        "Third bar at position 2 reaching approximately 0.09 (9%)",
        "Bars 3-8 showing values below 0.05",
        "Y-axis ranging from 0 to 0.30",
        "X-axis showing positions 0 through 8"
      ],
      "queries": [
        {
          "query": "Why do the first two bars at positions 0 and 1 reach 0.27 and 0.22 instead of one dominant component?",
          "answer": "ELMo has two principal components for gender, unlike GloVe which has only one. This explains why the first two bars show comparable variance levels rather than a single dominant component.",
          "visual_anchor": "First bar at position 0 (0.27) and second bar at position 1 (0.22)",
          "text_evidence": "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the rapid decline from bar 2 to bar 3 dropping below 0.05 support the claim about male-female pronoun co-occurrence patterns?",
          "answer": "Yes, the sharp drop after the first two components suggests that gender variance is captured in just two dimensions. This aligns with the corpus analysis showing systematic co-occurrence patterns between gendered pronouns and occupation words.",
          "visual_anchor": "Bar at position 2 (0.09) dropping to bar at position 3 (below 0.05)",
          "text_evidence": "We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains why bars 3 through 8 all remain below 0.05 when testing ELMo's gender embedding behavior?",
          "answer": "The classifier testing ELMo's gender prediction from occupation words reveals that gender information is concentrated in the first two components. The remaining components contribute negligible variance, indicating gender is encoded in a low-dimensional subspace.",
          "visual_anchor": "Bars at positions 3-8 all below 0.05",
          "text_evidence": "To test how ELMo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the first two bars at positions 0 and 1 reach 0.27 and 0.22 instead of one dominant component?",
        "answer": "ELMo has two principal components for gender, unlike GloVe which has only one. This explains why the first two bars show comparable variance levels rather than a single dominant component.",
        "visual_anchor": "First bar at position 0 (0.27) and second bar at position 1 (0.22)",
        "text_evidence": "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the rapid decline from bar 2 to bar 3 dropping below 0.05 support the claim about male-female pronoun co-occurrence patterns?",
        "answer": "Yes, the sharp drop after the first two components suggests that gender variance is captured in just two dimensions. This aligns with the corpus analysis showing systematic co-occurrence patterns between gendered pronouns and occupation words.",
        "visual_anchor": "Bar at position 2 (0.09) dropping to bar at position 3 (below 0.05)",
        "text_evidence": "We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains why bars 3 through 8 all remain below 0.05 when testing ELMo's gender embedding behavior?",
        "answer": "The classifier testing ELMo's gender prediction from occupation words reveals that gender information is concentrated in the first two components. The remaining components contribute negligible variance, indicating gender is encoded in a low-dimensional subspace.",
        "visual_anchor": "Bars at positions 3-8 all below 0.05",
        "text_evidence": "To test how ELMo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Bar chart showing decreasing percentage of explained variance",
      "First bar at position 0 reaching approximately 0.27 (27%)",
      "Second bar at position 1 reaching approximately 0.22 (22%)",
      "Third bar at position 2 reaching approximately 0.09 (9%)",
      "Bars 3-8 showing values below 0.05",
      "Y-axis ranging from 0 to 0.30",
      "X-axis showing positions 0 through 8"
    ]
  },
  {
    "doc_id": "1905.03674",
    "figure_id": "1905.03674_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
    "caption": "Figure 1: Proportionality Example",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\"black dots representing individuals\", \"red dots representing centers\", \"blue dot representing center\", \"green dot representing center\", \"vertical column of dots\", \"horizontal distribution of dots\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the vertical column contain both the blue and green centers when proportionality requires equal distribution?\",\n      \"answer\": \"The vertical column represents group M, which is entitled to one of the two centers proportionally. The blue center serves M while the green center is positioned at M's boundary, demonstrating the proportional allocation where each group receives k centers proportional to its size.\",\n      \"visual_anchor\": \"blue dot and green dot positioned vertically in the middle column\",\n      \"text_evidence\": \"In Figure 1, $\\\\mathcal N = \\\\mathcal M$, $k = 2$, and there are 12 individuals, represented by the embedded points.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the two red centers support the claim that groups are entitled to proportional shares of centers?\",\n      \"answer\": \"Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.\",\n      \"visual_anchor\": \"two red dots positioned symmetrically on left and right sides\",\n      \"text_evidence\": \"Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the green dot's position relate to the stated impossibility of always achieving proportional solutions?\",\n      \"answer\": \"The green dot sits at the boundary between groups, illustrating the tension in achieving exact proportionality. This positioning demonstrates why proportional solutions may not always exist and why the algorithm can only guarantee approximate proportionality in the worst case.\",\n      \"visual_anchor\": \"green dot at bottom of vertical column near boundary\",\n      \"text_evidence\": \"In Section 2 we show that proportional solutions may not always exist. In fact, one cannot get better than a 2-proportional solution in the worst case.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "black dots representing individuals",
        "red dots representing centers",
        "blue dot representing center",
        "green dot representing center",
        "vertical column of dots",
        "horizontal distribution of dots"
      ],
      "queries": [
        {
          "query": "Why does the vertical column contain both the blue and green centers when proportionality requires equal distribution?",
          "answer": "The vertical column represents group M, which is entitled to one of the two centers proportionally. The blue center serves M while the green center is positioned at M's boundary, demonstrating the proportional allocation where each group receives k centers proportional to its size.",
          "visual_anchor": "blue dot and green dot positioned vertically in the middle column",
          "text_evidence": "In Figure 1, $\\mathcal N = \\mathcal M$, $k = 2$, and there are 12 individuals, represented by the embedded points.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
          "answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
          "visual_anchor": "two red dots positioned symmetrically on left and right sides",
          "text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
          "query_type": "value_context"
        },
        {
          "query": "How does the green dot's position relate to the stated impossibility of always achieving proportional solutions?",
          "answer": "The green dot sits at the boundary between groups, illustrating the tension in achieving exact proportionality. This positioning demonstrates why proportional solutions may not always exist and why the algorithm can only guarantee approximate proportionality in the worst case.",
          "visual_anchor": "green dot at bottom of vertical column near boundary",
          "text_evidence": "In Section 2 we show that proportional solutions may not always exist. In fact, one cannot get better than a 2-proportional solution in the worst case.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the vertical column contain both the blue and green centers when proportionality requires equal distribution?",
        "answer": "The vertical column represents group M, which is entitled to one of the two centers proportionally. The blue center serves M while the green center is positioned at M's boundary, demonstrating the proportional allocation where each group receives k centers proportional to its size.",
        "visual_anchor": "blue dot and green dot positioned vertically in the middle column",
        "text_evidence": "In Figure 1, $\\mathcal N = \\mathcal M$, $k = 2$, and there are 12 individuals, represented by the embedded points.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the two red centers support the claim that groups are entitled to proportional shares of centers?",
        "answer": "Yes, the two red centers are positioned to serve individuals outside the middle column, demonstrating that when groups N and M are equal in size, each receives a proportional share of the k=2 centers available.",
        "visual_anchor": "two red dots positioned symmetrically on left and right sides",
        "text_evidence": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers.",
        "query_type": "value_context"
      },
      {
        "query": "How does the green dot's position relate to the stated impossibility of always achieving proportional solutions?",
        "answer": "The green dot sits at the boundary between groups, illustrating the tension in achieving exact proportionality. This positioning demonstrates why proportional solutions may not always exist and why the algorithm can only guarantee approximate proportionality in the worst case.",
        "visual_anchor": "green dot at bottom of vertical column near boundary",
        "text_evidence": "In Section 2 we show that proportional solutions may not always exist. In fact, one cannot get better than a 2-proportional solution in the worst case.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "black dots representing individuals",
      "red dots representing centers",
      "blue dot representing center",
      "green dot representing center",
      "vertical column of dots",
      "horizontal distribution of dots"
    ]
  },
  {
    "doc_id": "1905.03674",
    "figure_id": "1905.03674_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg",
    "caption": "Figure 2: Diagram for Proof of Theorem 1",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"two dashed circles\", \"point i* at top\", \"point y with distance r_y from i*\", \"point i between the circles\", \"point x on the right circle\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the distance r_y from i* to y appear in the diagram when the algorithm smoothly increases δ?\",\n      \"answer\": \"The diagram illustrates the proof scenario where r_y represents the radius at which point y is added to set X during the greedy algorithm's execution as δ increases smoothly from 0.\",\n      \"visual_anchor\": \"vertical line segment labeled r_y connecting i* to y\",\n      \"text_evidence\": \"Smoothly increase $\\\\delta$ ... while $\\\\exists x \\\\in X$ s.t. $| B ( x , \\\\delta ) \\\\cap N | \\\\geq 1$ do\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does point i's position between the two dashed circles relate to the triangle inequality condition d(x,y) ≤ d(i,x) + d(i,y)?\",\n      \"answer\": \"Point i lies in the intersection region B(x, r_y) ∩ S, allowing the triangle inequality to bound the distance between x and y through their respective distances to i, as shown by the overlapping circles.\",\n      \"visual_anchor\": \"point i positioned in the overlapping region between the two dashed circles\",\n      \"text_evidence\": \"$\\\\exists x \\\\in X$ and $\\\\exists i \\\\in N$ such that $i \\\\in B ( x , r _ { y } ) \\\\cap S$ ... By the triangle inequality, $d ( x , y ) \\\\leq d ( i , x ) + d ( i , y )$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What constraint is violated if point x captures fewer than ⌈n/k⌉ agents from N at the radius shown?\",\n      \"answer\": \"Equation 1 would be violated because the algorithm only adds x to X when its ball B(x, δ) captures at least ⌈n/k⌉ agents from N, ensuring proper coverage constraints are maintained.\",\n      \"visual_anchor\": \"right dashed circle centered at point x\",\n      \"text_evidence\": \"while $\\\\exists x \\\\in ( { \\\\mathcal { M } } \\\\backslash X )$ s.t. $\\\\begin{array} { r } { | B ( x , \\\\delta ) \\\\cap N | \\\\ge \\\\lceil \\\\frac { n } { k } \\\\rceil } \\\\end{array}$ do ... which violates equation 1\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "two dashed circles",
        "point i* at top",
        "point y with distance r_y from i*",
        "point i between the circles",
        "point x on the right circle"
      ],
      "queries": [
        {
          "query": "Why does the distance r_y from i* to y appear in the diagram when the algorithm smoothly increases δ?",
          "answer": "The diagram illustrates the proof scenario where r_y represents the radius at which point y is added to set X during the greedy algorithm's execution as δ increases smoothly from 0.",
          "visual_anchor": "vertical line segment labeled r_y connecting i* to y",
          "text_evidence": "Smoothly increase $\\delta$ ... while $\\exists x \\in X$ s.t. $| B ( x , \\delta ) \\cap N | \\geq 1$ do",
          "query_type": "value_context"
        },
        {
          "query": "How does point i's position between the two dashed circles relate to the triangle inequality condition d(x,y) ≤ d(i,x) + d(i,y)?",
          "answer": "Point i lies in the intersection region B(x, r_y) ∩ S, allowing the triangle inequality to bound the distance between x and y through their respective distances to i, as shown by the overlapping circles.",
          "visual_anchor": "point i positioned in the overlapping region between the two dashed circles",
          "text_evidence": "$\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ ... By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What constraint is violated if point x captures fewer than ⌈n/k⌉ agents from N at the radius shown?",
          "answer": "Equation 1 would be violated because the algorithm only adds x to X when its ball B(x, δ) captures at least ⌈n/k⌉ agents from N, ensuring proper coverage constraints are maintained.",
          "visual_anchor": "right dashed circle centered at point x",
          "text_evidence": "while $\\exists x \\in ( { \\mathcal { M } } \\backslash X )$ s.t. $\\begin{array} { r } { | B ( x , \\delta ) \\cap N | \\ge \\lceil \\frac { n } { k } \\rceil } \\end{array}$ do ... which violates equation 1",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the distance r_y from i* to y appear in the diagram when the algorithm smoothly increases δ?",
        "answer": "The diagram illustrates the proof scenario where r_y represents the radius at which point y is added to set X during the greedy algorithm's execution as δ increases smoothly from 0.",
        "visual_anchor": "vertical line segment labeled r_y connecting i* to y",
        "text_evidence": "Smoothly increase $\\delta$ ... while $\\exists x \\in X$ s.t. $| B ( x , \\delta ) \\cap N | \\geq 1$ do",
        "query_type": "value_context"
      },
      {
        "query": "How does point i's position between the two dashed circles relate to the triangle inequality condition d(x,y) ≤ d(i,x) + d(i,y)?",
        "answer": "Point i lies in the intersection region B(x, r_y) ∩ S, allowing the triangle inequality to bound the distance between x and y through their respective distances to i, as shown by the overlapping circles.",
        "visual_anchor": "point i positioned in the overlapping region between the two dashed circles",
        "text_evidence": "$\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ ... By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What constraint is violated if point x captures fewer than ⌈n/k⌉ agents from N at the radius shown?",
        "answer": "Equation 1 would be violated because the algorithm only adds x to X when its ball B(x, δ) captures at least ⌈n/k⌉ agents from N, ensuring proper coverage constraints are maintained.",
        "visual_anchor": "right dashed circle centered at point x",
        "text_evidence": "while $\\exists x \\in ( { \\mathcal { M } } \\backslash X )$ s.t. $\\begin{array} { r } { | B ( x , \\delta ) \\cap N | \\ge \\lceil \\frac { n } { k } \\rceil } \\end{array}$ do ... which violates equation 1",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "two dashed circles",
      "point i* at top",
      "point y with distance r_y from i*",
      "point i between the circles",
      "point x on the right circle"
    ]
  },
  {
    "doc_id": "1905.03674",
    "figure_id": "1905.03674_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg",
    "caption": "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   ",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue circles representing Local Capture\",\n    \"Red squares representing k-means++\",\n    \"Green triangles representing Greedy Capture\",\n    \"X-axis labeled k ranging from 2 to 10\",\n    \"Y-axis labeled ρ ranging from 1.0 to 1.8\",\n    \"Local Capture and k-means++ curves both near ρ=1.0\",\n    \"Greedy Capture curve showing higher ρ values between 1.2 and 1.7\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the blue circles and red squares both remain near ρ=1.0 across all k values?\",\n      \"answer\": \"Both Local Capture (blue circles) and k-means++ (red squares) consistently achieve near-optimal ρ-proportional solutions close to 1.0, indicating they produce solutions very close to proportional across different values of k.\",\n      \"visual_anchor\": \"Blue circles and red squares both clustered near ρ=1.0 on the y-axis\",\n      \"text_evidence\": \"Minimum $\\\\rho$ such that the solution is $\\\\rho$ -proportional\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the green triangles fluctuating between ρ=1.27 and ρ=1.67 while remaining consistently above 1.2?\",\n      \"answer\": \"Greedy Capture produces solutions that require significantly higher ρ values (1.27 to 1.67) to be ρ-proportional compared to the other methods, indicating it achieves less proportional fairness, particularly at k=2, k=6, and k=8 where peaks occur.\",\n      \"visual_anchor\": \"Green triangles showing variable heights between 1.27 and 1.67 on the ρ axis\",\n      \"text_evidence\": \"Minimum $\\\\rho$ such that the solution is $\\\\rho$ -proportional\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the consistent gap between green triangles and blue circles demonstrate algorithm performance differences on Iris data?\",\n      \"answer\": \"Yes, the substantial gap shows that Greedy Capture requires ρ values 20-67% higher than Local Capture to achieve proportionality, demonstrating significant performance differences between the algorithms on the Iris dataset.\",\n      \"visual_anchor\": \"Vertical separation between green triangles (1.2-1.7) and blue circles (near 1.0)\",\n      \"text_evidence\": \"Minimum $\\\\rho$ such that the solution is $\\\\rho$ -proportional (a) Iris\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue circles representing Local Capture",
        "Red squares representing k-means++",
        "Green triangles representing Greedy Capture",
        "X-axis labeled k ranging from 2 to 10",
        "Y-axis labeled ρ ranging from 1.0 to 1.8",
        "Local Capture and k-means++ curves both near ρ=1.0",
        "Greedy Capture curve showing higher ρ values between 1.2 and 1.7"
      ],
      "queries": [
        {
          "query": "Why do the blue circles and red squares both remain near ρ=1.0 across all k values?",
          "answer": "Both Local Capture (blue circles) and k-means++ (red squares) consistently achieve near-optimal ρ-proportional solutions close to 1.0, indicating they produce solutions very close to proportional across different values of k.",
          "visual_anchor": "Blue circles and red squares both clustered near ρ=1.0 on the y-axis",
          "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the green triangles fluctuating between ρ=1.27 and ρ=1.67 while remaining consistently above 1.2?",
          "answer": "Greedy Capture produces solutions that require significantly higher ρ values (1.27 to 1.67) to be ρ-proportional compared to the other methods, indicating it achieves less proportional fairness, particularly at k=2, k=6, and k=8 where peaks occur.",
          "visual_anchor": "Green triangles showing variable heights between 1.27 and 1.67 on the ρ axis",
          "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the consistent gap between green triangles and blue circles demonstrate algorithm performance differences on Iris data?",
          "answer": "Yes, the substantial gap shows that Greedy Capture requires ρ values 20-67% higher than Local Capture to achieve proportionality, demonstrating significant performance differences between the algorithms on the Iris dataset.",
          "visual_anchor": "Vertical separation between green triangles (1.2-1.7) and blue circles (near 1.0)",
          "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional (a) Iris",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the blue circles and red squares both remain near ρ=1.0 across all k values?",
        "answer": "Both Local Capture (blue circles) and k-means++ (red squares) consistently achieve near-optimal ρ-proportional solutions close to 1.0, indicating they produce solutions very close to proportional across different values of k.",
        "visual_anchor": "Blue circles and red squares both clustered near ρ=1.0 on the y-axis",
        "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the green triangles fluctuating between ρ=1.27 and ρ=1.67 while remaining consistently above 1.2?",
        "answer": "Greedy Capture produces solutions that require significantly higher ρ values (1.27 to 1.67) to be ρ-proportional compared to the other methods, indicating it achieves less proportional fairness, particularly at k=2, k=6, and k=8 where peaks occur.",
        "visual_anchor": "Green triangles showing variable heights between 1.27 and 1.67 on the ρ axis",
        "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the consistent gap between green triangles and blue circles demonstrate algorithm performance differences on Iris data?",
        "answer": "Yes, the substantial gap shows that Greedy Capture requires ρ values 20-67% higher than Local Capture to achieve proportionality, demonstrating significant performance differences between the algorithms on the Iris dataset.",
        "visual_anchor": "Vertical separation between green triangles (1.2-1.7) and blue circles (near 1.0)",
        "text_evidence": "Minimum $\\rho$ such that the solution is $\\rho$ -proportional (a) Iris",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue circles representing Local Capture",
      "Red squares representing k-means++",
      "Green triangles representing Greedy Capture",
      "X-axis labeled k ranging from 2 to 10",
      "Y-axis labeled ρ ranging from 1.0 to 1.8",
      "Local Capture and k-means++ curves both near ρ=1.0",
      "Greedy Capture curve showing higher ρ values between 1.2 and 1.7"
    ]
  },
  {
    "doc_id": "1905.03674",
    "figure_id": "1905.03674_fig_7",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg",
    "caption": "Figure 5: $k$ -means objective",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue circles representing Local Capture\",\n    \"Orange squares representing k-means++\",\n    \"Green triangles representing Greedy Capture\",\n    \"X-axis showing k values from 2 to 10\",\n    \"Y-axis showing k-means objective in millions\",\n    \"Local Capture starting at ~12 million at k=2\",\n    \"k-means++ consistently lowest around 1-3 million\",\n    \"Greedy Capture highest at k=2 (~16 million)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does Local Capture's blue circle objective at k=2 (~12 million) exceed k-means++ despite achieving proportionality?\",\n      \"answer\": \"Local Capture prioritizes proportionality over k-means objective optimization. Taking the union of both solutions with 2k centers would trivially achieve both proportionality and k-means++ objective, but Local Capture uses only k centers.\",\n      \"visual_anchor\": \"Blue circle at k=2 showing ~12 million objective compared to orange square at ~3 million\",\n      \"text_evidence\": \"one can trivially achieve the proportionality of Local Capture and the $k$ -means objective of the $k$ -means $^ { + + }$ algorithm by taking the union of the two solutions\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the green triangle's peak at k=2 (~16 million) illustrate the tradeoff when prioritizing proportionality over objective minimization?\",\n      \"answer\": \"Yes, Greedy Capture shows the highest k-means objective at k=2, demonstrating the cost of approximately proportional solutions. The figure quantifies how many extra centers would be needed to match k-means++ objective while maintaining proportionality.\",\n      \"visual_anchor\": \"Green triangle at k=2 showing highest objective value of ~16 million\",\n      \"text_evidence\": \"Given an approximately proportional solution, how many extra\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why do all three methods converge to similar objectives around 4-5 million at k=10 despite different proportionality guarantees?\",\n      \"answer\": \"As k increases, the tradeoff between proportionality and k-means objective diminishes. With more centers available, even proportionality-focused algorithms can achieve objectives comparable to k-means++, reducing the need for extra centers mentioned in the tradeoff quantification.\",\n      \"visual_anchor\": \"Convergence of blue circles, orange squares, and green triangles near 4-5 million at k=10\",\n      \"text_evidence\": \"Thinking in this way leads to a different way of quantifying the tradeoff between proportionality and the $k$ -means objective\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue circles representing Local Capture",
        "Orange squares representing k-means++",
        "Green triangles representing Greedy Capture",
        "X-axis showing k values from 2 to 10",
        "Y-axis showing k-means objective in millions",
        "Local Capture starting at ~12 million at k=2",
        "k-means++ consistently lowest around 1-3 million",
        "Greedy Capture highest at k=2 (~16 million)"
      ],
      "queries": [
        {
          "query": "Why does Local Capture's blue circle objective at k=2 (~12 million) exceed k-means++ despite achieving proportionality?",
          "answer": "Local Capture prioritizes proportionality over k-means objective optimization. Taking the union of both solutions with 2k centers would trivially achieve both proportionality and k-means++ objective, but Local Capture uses only k centers.",
          "visual_anchor": "Blue circle at k=2 showing ~12 million objective compared to orange square at ~3 million",
          "text_evidence": "one can trivially achieve the proportionality of Local Capture and the $k$ -means objective of the $k$ -means $^ { + + }$ algorithm by taking the union of the two solutions",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the green triangle's peak at k=2 (~16 million) illustrate the tradeoff when prioritizing proportionality over objective minimization?",
          "answer": "Yes, Greedy Capture shows the highest k-means objective at k=2, demonstrating the cost of approximately proportional solutions. The figure quantifies how many extra centers would be needed to match k-means++ objective while maintaining proportionality.",
          "visual_anchor": "Green triangle at k=2 showing highest objective value of ~16 million",
          "text_evidence": "Given an approximately proportional solution, how many extra",
          "query_type": "value_context"
        },
        {
          "query": "Why do all three methods converge to similar objectives around 4-5 million at k=10 despite different proportionality guarantees?",
          "answer": "As k increases, the tradeoff between proportionality and k-means objective diminishes. With more centers available, even proportionality-focused algorithms can achieve objectives comparable to k-means++, reducing the need for extra centers mentioned in the tradeoff quantification.",
          "visual_anchor": "Convergence of blue circles, orange squares, and green triangles near 4-5 million at k=10",
          "text_evidence": "Thinking in this way leads to a different way of quantifying the tradeoff between proportionality and the $k$ -means objective",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does Local Capture's blue circle objective at k=2 (~12 million) exceed k-means++ despite achieving proportionality?",
        "answer": "Local Capture prioritizes proportionality over k-means objective optimization. Taking the union of both solutions with 2k centers would trivially achieve both proportionality and k-means++ objective, but Local Capture uses only k centers.",
        "visual_anchor": "Blue circle at k=2 showing ~12 million objective compared to orange square at ~3 million",
        "text_evidence": "one can trivially achieve the proportionality of Local Capture and the $k$ -means objective of the $k$ -means $^ { + + }$ algorithm by taking the union of the two solutions",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the green triangle's peak at k=2 (~16 million) illustrate the tradeoff when prioritizing proportionality over objective minimization?",
        "answer": "Yes, Greedy Capture shows the highest k-means objective at k=2, demonstrating the cost of approximately proportional solutions. The figure quantifies how many extra centers would be needed to match k-means++ objective while maintaining proportionality.",
        "visual_anchor": "Green triangle at k=2 showing highest objective value of ~16 million",
        "text_evidence": "Given an approximately proportional solution, how many extra",
        "query_type": "value_context"
      },
      {
        "query": "Why do all three methods converge to similar objectives around 4-5 million at k=10 despite different proportionality guarantees?",
        "answer": "As k increases, the tradeoff between proportionality and k-means objective diminishes. With more centers available, even proportionality-focused algorithms can achieve objectives comparable to k-means++, reducing the need for extra centers mentioned in the tradeoff quantification.",
        "visual_anchor": "Convergence of blue circles, orange squares, and green triangles near 4-5 million at k=10",
        "text_evidence": "Thinking in this way leads to a different way of quantifying the tradeoff between proportionality and the $k$ -means objective",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue circles representing Local Capture",
      "Orange squares representing k-means++",
      "Green triangles representing Greedy Capture",
      "X-axis showing k values from 2 to 10",
      "Y-axis showing k-means objective in millions",
      "Local Capture starting at ~12 million at k=2",
      "k-means++ consistently lowest around 1-3 million",
      "Greedy Capture highest at k=2 (~16 million)"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
    "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Input Graph with blue and red nodes connected in a network\",\n    \"Node Embedding represented as blue stacked layers\",\n    \"Filters shown as blue stacked layers with arrows\",\n    \"Three green boxes labeled Gender, Occupation, Age\",\n    \"Filtered Embedding as blue stacked layers\",\n    \"Three pink/red boxes labeled D_Gender, D_Occupation, D_Age\",\n    \"Black arrows showing data flow from Input Graph through components\",\n    \"Prohibition symbol between Filters and Discriminators\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How do the three green attribute boxes relate to the compositional approach described for enforcing fairness constraints?\",\n      \"answer\": \"The three green boxes (Gender, Occupation, Age) represent sensitive attributes that the compositional adversarial framework can filter in different combinations, allowing flexible generation of embeddings invariant to any subset of these attributes.\",\n      \"visual_anchor\": \"three green boxes labeled Gender, Occupation, and Age positioned between Filters and Filtered Embedding\",\n      \"text_evidence\": \"Our approach is compositi[onal] ... allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why do the blue filter layers connect to pink discriminator boxes if the goal is preventing classification of sensitive information?\",\n      \"answer\": \"The filters are trained adversarially against the discriminators (D_Gender, D_Occupation, D_Age) to prevent them from classifying sensitive information, with the prohibition symbol indicating this adversarial relationship that enforces invariance.\",\n      \"visual_anchor\": \"blue stacked layers labeled Filters connecting through prohibition symbol to pink boxes labeled D_Gender, D_Occupation, D_Age\",\n      \"text_evidence\": \"We train a set of 'filters' to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the input graph's blue and red node structure support embedding generation that operates on social networks?\",\n      \"answer\": \"Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.\",\n      \"visual_anchor\": \"Input Graph box showing blue and red nodes connected in a network structure on the left side\",\n      \"text_evidence\": \"Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Input Graph with blue and red nodes connected in a network",
        "Node Embedding represented as blue stacked layers",
        "Filters shown as blue stacked layers with arrows",
        "Three green boxes labeled Gender, Occupation, Age",
        "Filtered Embedding as blue stacked layers",
        "Three pink/red boxes labeled D_Gender, D_Occupation, D_Age",
        "Black arrows showing data flow from Input Graph through components",
        "Prohibition symbol between Filters and Discriminators"
      ],
      "queries": [
        {
          "query": "How do the three green attribute boxes relate to the compositional approach described for enforcing fairness constraints?",
          "answer": "The three green boxes (Gender, Occupation, Age) represent sensitive attributes that the compositional adversarial framework can filter in different combinations, allowing flexible generation of embeddings invariant to any subset of these attributes.",
          "visual_anchor": "three green boxes labeled Gender, Occupation, and Age positioned between Filters and Filtered Embedding",
          "text_evidence": "Our approach is compositi[onal] ... allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
          "query_type": "visual_definition"
        },
        {
          "query": "Why do the blue filter layers connect to pink discriminator boxes if the goal is preventing classification of sensitive information?",
          "answer": "The filters are trained adversarially against the discriminators (D_Gender, D_Occupation, D_Age) to prevent them from classifying sensitive information, with the prohibition symbol indicating this adversarial relationship that enforces invariance.",
          "visual_anchor": "blue stacked layers labeled Filters connecting through prohibition symbol to pink boxes labeled D_Gender, D_Occupation, D_Age",
          "text_evidence": "We train a set of 'filters' to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
          "answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
          "visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
          "text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How do the three green attribute boxes relate to the compositional approach described for enforcing fairness constraints?",
        "answer": "The three green boxes (Gender, Occupation, Age) represent sensitive attributes that the compositional adversarial framework can filter in different combinations, allowing flexible generation of embeddings invariant to any subset of these attributes.",
        "visual_anchor": "three green boxes labeled Gender, Occupation, and Age positioned between Filters and Filtered Embedding",
        "text_evidence": "Our approach is compositi[onal] ... allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
        "query_type": "visual_definition"
      },
      {
        "query": "Why do the blue filter layers connect to pink discriminator boxes if the goal is preventing classification of sensitive information?",
        "answer": "The filters are trained adversarially against the discriminators (D_Gender, D_Occupation, D_Age) to prevent them from classifying sensitive information, with the prohibition symbol indicating this adversarial relationship that enforces invariance.",
        "visual_anchor": "blue stacked layers labeled Filters connecting through prohibition symbol to pink boxes labeled D_Gender, D_Occupation, D_Age",
        "text_evidence": "We train a set of 'filters' to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the input graph's blue and red node structure support embedding generation that operates on social networks?",
        "answer": "Yes, the bipartite-style input graph with blue and red nodes represents the type of graph data structure used in social networks and recommender systems that the embedding technique is designed to handle.",
        "visual_anchor": "Input Graph box showing blue and red nodes connected in a network structure on the left side",
        "text_evidence": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Input Graph with blue and red nodes connected in a network",
      "Node Embedding represented as blue stacked layers",
      "Filters shown as blue stacked layers with arrows",
      "Three green boxes labeled Gender, Occupation, Age",
      "Filtered Embedding as blue stacked layers",
      "Three pink/red boxes labeled D_Gender, D_Occupation, D_Age",
      "Black arrows showing data flow from Input Graph through components",
      "Prohibition symbol between Filters and Discriminators"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_2",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg",
    "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Blue line labeled 'Gender Adversary'\", \"Green line labeled 'Age Adversary'\", \"Teal line labeled 'Occupation Adversary'\", \"Orange line labeled 'Compositional Adversary'\", \"Pink line labeled 'Baseline No Adversary'\", \"Y-axis labeled 'RMSE' ranging from 0.8 to 1.8\", \"X-axis labeled 'Epochs' ranging from 0 to 200\", \"All adversarial lines converge around RMSE 1.0\", \"Baseline line stays around RMSE 0.9\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the orange compositional adversary curve stabilize near RMSE 1.01 while baseline stays at 0.865?\",\n      \"answer\": \"The compositional adversary filters embeddings to make sensitive attributes nearly impossible to predict, causing RMSE to degrade from 0.865 to 1.01 as the cost of achieving on-par majority-vote classification accuracy for sensitive attributes.\",\n      \"visual_anchor\": \"Orange line labeled 'Compositional Adversary' stabilizing at RMSE ~1.01 versus pink baseline at ~0.865\",\n      \"text_evidence\": \"Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the blue, green, and teal lines converging near RMSE 1.0 demonstrate the invariance-accuracy tradeoff?\",\n      \"answer\": \"Yes, the convergence of all adversarial training curves (gender, age, occupation) near RMSE 1.0 demonstrates the tradeoff between achieving invariance to sensitive attributes and maintaining edge prediction accuracy, which is roughly 10% worse than baseline.\",\n      \"visual_anchor\": \"Blue (Gender), green (Age), and teal (Occupation) adversary lines all converging around RMSE 1.0\",\n      \"text_evidence\": \"In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\\\%$ worse than\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes all adversarial curves to drop sharply from epoch 0 to 50 before flattening?\",\n      \"answer\": \"The sharp initial drop reflects the rapid learning phase where the adversarial training mechanism quickly filters embeddings to achieve near-majority-vote classification accuracy for sensitive attributes, after which performance stabilizes as the model balances invariance with edge prediction task performance.\",\n      \"visual_anchor\": \"All colored adversarial lines (blue, green, teal, orange) dropping steeply from RMSE ~1.8-1.6 at epoch 0 to ~1.0 by epoch 50\",\n      \"text_evidence\": \"where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue line labeled 'Gender Adversary'",
        "Green line labeled 'Age Adversary'",
        "Teal line labeled 'Occupation Adversary'",
        "Orange line labeled 'Compositional Adversary'",
        "Pink line labeled 'Baseline No Adversary'",
        "Y-axis labeled 'RMSE' ranging from 0.8 to 1.8",
        "X-axis labeled 'Epochs' ranging from 0 to 200",
        "All adversarial lines converge around RMSE 1.0",
        "Baseline line stays around RMSE 0.9"
      ],
      "queries": [
        {
          "query": "Why does the orange compositional adversary curve stabilize near RMSE 1.01 while baseline stays at 0.865?",
          "answer": "The compositional adversary filters embeddings to make sensitive attributes nearly impossible to predict, causing RMSE to degrade from 0.865 to 1.01 as the cost of achieving on-par majority-vote classification accuracy for sensitive attributes.",
          "visual_anchor": "Orange line labeled 'Compositional Adversary' stabilizing at RMSE ~1.01 versus pink baseline at ~0.865",
          "text_evidence": "Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the blue, green, and teal lines converging near RMSE 1.0 demonstrate the invariance-accuracy tradeoff?",
          "answer": "Yes, the convergence of all adversarial training curves (gender, age, occupation) near RMSE 1.0 demonstrates the tradeoff between achieving invariance to sensitive attributes and maintaining edge prediction accuracy, which is roughly 10% worse than baseline.",
          "visual_anchor": "Blue (Gender), green (Age), and teal (Occupation) adversary lines all converging around RMSE 1.0",
          "text_evidence": "In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than",
          "query_type": "value_context"
        },
        {
          "query": "What causes all adversarial curves to drop sharply from epoch 0 to 50 before flattening?",
          "answer": "The sharp initial drop reflects the rapid learning phase where the adversarial training mechanism quickly filters embeddings to achieve near-majority-vote classification accuracy for sensitive attributes, after which performance stabilizes as the model balances invariance with edge prediction task performance.",
          "visual_anchor": "All colored adversarial lines (blue, green, teal, orange) dropping steeply from RMSE ~1.8-1.6 at epoch 0 to ~1.0 by epoch 50",
          "text_evidence": "where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the orange compositional adversary curve stabilize near RMSE 1.01 while baseline stays at 0.865?",
        "answer": "The compositional adversary filters embeddings to make sensitive attributes nearly impossible to predict, causing RMSE to degrade from 0.865 to 1.01 as the cost of achieving on-par majority-vote classification accuracy for sensitive attributes.",
        "visual_anchor": "Orange line labeled 'Compositional Adversary' stabilizing at RMSE ~1.01 versus pink baseline at ~0.865",
        "text_evidence": "Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the blue, green, and teal lines converging near RMSE 1.0 demonstrate the invariance-accuracy tradeoff?",
        "answer": "Yes, the convergence of all adversarial training curves (gender, age, occupation) near RMSE 1.0 demonstrates the tradeoff between achieving invariance to sensitive attributes and maintaining edge prediction accuracy, which is roughly 10% worse than baseline.",
        "visual_anchor": "Blue (Gender), green (Age), and teal (Occupation) adversary lines all converging around RMSE 1.0",
        "text_evidence": "In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than",
        "query_type": "value_context"
      },
      {
        "query": "What causes all adversarial curves to drop sharply from epoch 0 to 50 before flattening?",
        "answer": "The sharp initial drop reflects the rapid learning phase where the adversarial training mechanism quickly filters embeddings to achieve near-majority-vote classification accuracy for sensitive attributes, after which performance stabilizes as the model balances invariance with edge prediction task performance.",
        "visual_anchor": "All colored adversarial lines (blue, green, teal, orange) dropping steeply from RMSE ~1.8-1.6 at epoch 0 to ~1.0 by epoch 50",
        "text_evidence": "where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue line labeled 'Gender Adversary'",
      "Green line labeled 'Age Adversary'",
      "Teal line labeled 'Occupation Adversary'",
      "Orange line labeled 'Compositional Adversary'",
      "Pink line labeled 'Baseline No Adversary'",
      "Y-axis labeled 'RMSE' ranging from 0.8 to 1.8",
      "X-axis labeled 'Epochs' ranging from 0 to 200",
      "All adversarial lines converge around RMSE 1.0",
      "Baseline line stays around RMSE 0.9"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_3",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg",
    "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue line (Baseline) rising sharply from ~0.69 to ~0.81\",\n    \"Orange line (Non Compositional) gradually increasing from ~0.75 to ~0.80\",\n    \"Green line (Held Out Compositional) declining from ~0.77 to ~0.74\",\n    \"Brown line (No Held Out Compositional) declining from ~0.75 to ~0.74\",\n    \"X-axis: Epochs (5 to 50)\",\n    \"Y-axis: AUC (0.70 to 0.82)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the green Held Out Compositional line decline to ~0.74 AUC while addressing sensitive attribute invariance?\",\n      \"answer\": \"The compositional approach was trained with adversarial filtering to remove sensitive attribute information, which creates a tradeoff where invariance to sensitive attributes comes at the cost of edge prediction performance.\",\n      \"visual_anchor\": \"Green line (Held Out Compositional) declining from ~0.77 to ~0.74 AUC\",\n      \"text_evidence\": \"In order to quantify the extent to which the learned embeddings are invariant to the sensitive attributes (e.g., after adversarial training), we freeze the trained compositional encoder C-ENC and train an new MLP classifier to predict each sensitive attribute from the filtered embeddings\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the compositional approach's worse performance at ~0.74 AUC align with its reduced ability to remove sensitive information?\",\n      \"answer\": \"Yes, on Reddit data the compositional approach both performed worse at removing sensitive attribute information and showed a small drop in edge prediction task performance, confirming the observed tradeoff.\",\n      \"visual_anchor\": \"Green and brown lines both at ~0.74 AUC by epoch 50\",\n      \"text_evidence\": \"on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why is the Baseline's sharp rise to ~0.81 AUC evaluated using only AUC score on Reddit data?\",\n      \"answer\": \"The evaluation uses AUC score because the Reddit edge prediction task involves only one edge or relation type, making AUC the appropriate single metric.\",\n      \"visual_anchor\": \"Blue Baseline line rising sharply to ~0.81 AUC\",\n      \"text_evidence\": \"Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue line (Baseline) rising sharply from ~0.69 to ~0.81",
        "Orange line (Non Compositional) gradually increasing from ~0.75 to ~0.80",
        "Green line (Held Out Compositional) declining from ~0.77 to ~0.74",
        "Brown line (No Held Out Compositional) declining from ~0.75 to ~0.74",
        "X-axis: Epochs (5 to 50)",
        "Y-axis: AUC (0.70 to 0.82)"
      ],
      "queries": [
        {
          "query": "Why does the green Held Out Compositional line decline to ~0.74 AUC while addressing sensitive attribute invariance?",
          "answer": "The compositional approach was trained with adversarial filtering to remove sensitive attribute information, which creates a tradeoff where invariance to sensitive attributes comes at the cost of edge prediction performance.",
          "visual_anchor": "Green line (Held Out Compositional) declining from ~0.77 to ~0.74 AUC",
          "text_evidence": "In order to quantify the extent to which the learned embeddings are invariant to the sensitive attributes (e.g., after adversarial training), we freeze the trained compositional encoder C-ENC and train an new MLP classifier to predict each sensitive attribute from the filtered embeddings",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the compositional approach's worse performance at ~0.74 AUC align with its reduced ability to remove sensitive information?",
          "answer": "Yes, on Reddit data the compositional approach both performed worse at removing sensitive attribute information and showed a small drop in edge prediction task performance, confirming the observed tradeoff.",
          "visual_anchor": "Green and brown lines both at ~0.74 AUC by epoch 50",
          "text_evidence": "on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)",
          "query_type": "value_context"
        },
        {
          "query": "Why is the Baseline's sharp rise to ~0.81 AUC evaluated using only AUC score on Reddit data?",
          "answer": "The evaluation uses AUC score because the Reddit edge prediction task involves only one edge or relation type, making AUC the appropriate single metric.",
          "visual_anchor": "Blue Baseline line rising sharply to ~0.81 AUC",
          "text_evidence": "Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the green Held Out Compositional line decline to ~0.74 AUC while addressing sensitive attribute invariance?",
        "answer": "The compositional approach was trained with adversarial filtering to remove sensitive attribute information, which creates a tradeoff where invariance to sensitive attributes comes at the cost of edge prediction performance.",
        "visual_anchor": "Green line (Held Out Compositional) declining from ~0.77 to ~0.74 AUC",
        "text_evidence": "In order to quantify the extent to which the learned embeddings are invariant to the sensitive attributes (e.g., after adversarial training), we freeze the trained compositional encoder C-ENC and train an new MLP classifier to predict each sensitive attribute from the filtered embeddings",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the compositional approach's worse performance at ~0.74 AUC align with its reduced ability to remove sensitive information?",
        "answer": "Yes, on Reddit data the compositional approach both performed worse at removing sensitive attribute information and showed a small drop in edge prediction task performance, confirming the observed tradeoff.",
        "visual_anchor": "Green and brown lines both at ~0.74 AUC by epoch 50",
        "text_evidence": "on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)",
        "query_type": "value_context"
      },
      {
        "query": "Why is the Baseline's sharp rise to ~0.81 AUC evaluated using only AUC score on Reddit data?",
        "answer": "The evaluation uses AUC score because the Reddit edge prediction task involves only one edge or relation type, making AUC the appropriate single metric.",
        "visual_anchor": "Blue Baseline line rising sharply to ~0.81 AUC",
        "text_evidence": "Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue line (Baseline) rising sharply from ~0.69 to ~0.81",
      "Orange line (Non Compositional) gradually increasing from ~0.75 to ~0.80",
      "Green line (Held Out Compositional) declining from ~0.77 to ~0.74",
      "Brown line (No Held Out Compositional) declining from ~0.75 to ~0.74",
      "X-axis: Epochs (5 to 50)",
      "Y-axis: AUC (0.70 to 0.82)"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_4",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg",
    "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue bar labeled 'Baseline' at approximately 0.9 AUC\",\n    \"Orange bar labeled 'Non Compositional' at approximately 0.5 AUC\",\n    \"Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC\",\n    \"Orange bar labeled 'Held Out Compositional' at approximately 0.6 AUC\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the green bar at 0.6 AUC contradict the claim that compositional approaches performed worse on Reddit data?\",\n      \"answer\": \"No, the green bar shows the compositional approach achieved 0.6 AUC compared to baseline's 0.9, confirming worse performance in removing sensitive attribute information as stated.\",\n      \"visual_anchor\": \"Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC\",\n      \"text_evidence\": \"on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the 0.025 difference between the two rightmost orange bars support the generalization capability claim?\",\n      \"answer\": \"The small 0.025 AUC difference between 'No Held Out Compositional' (0.6) and 'Held Out Compositional' (0.6) bars demonstrates minimal performance drop, indicating effective generalization to unseen attribute combinations.\",\n      \"visual_anchor\": \"Two rightmost bars: green 'No Held Out Compositional' and orange 'Held Out Compositional' both near 0.6 AUC\",\n      \"text_evidence\": \"the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the blue baseline bar reach 0.9 while the orange non-compositional bar drops to 0.5?\",\n      \"answer\": \"The baseline retains sensitive attribute information (high AUC of 0.9), while the non-compositional approach successfully removes much of this information, reducing AUC to 0.5, demonstrating effective attribute masking.\",\n      \"visual_anchor\": \"Blue 'Baseline' bar at 0.9 AUC versus orange 'Non Compositional' bar at 0.5 AUC\",\n      \"text_evidence\": \"average AUC across the 10 binary sensitive attributes\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue bar labeled 'Baseline' at approximately 0.9 AUC",
        "Orange bar labeled 'Non Compositional' at approximately 0.5 AUC",
        "Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC",
        "Orange bar labeled 'Held Out Compositional' at approximately 0.6 AUC"
      ],
      "queries": [
        {
          "query": "Does the green bar at 0.6 AUC contradict the claim that compositional approaches performed worse on Reddit data?",
          "answer": "No, the green bar shows the compositional approach achieved 0.6 AUC compared to baseline's 0.9, confirming worse performance in removing sensitive attribute information as stated.",
          "visual_anchor": "Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC",
          "text_evidence": "on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the 0.025 difference between the two rightmost orange bars support the generalization capability claim?",
          "answer": "The small 0.025 AUC difference between 'No Held Out Compositional' (0.6) and 'Held Out Compositional' (0.6) bars demonstrates minimal performance drop, indicating effective generalization to unseen attribute combinations.",
          "visual_anchor": "Two rightmost bars: green 'No Held Out Compositional' and orange 'Held Out Compositional' both near 0.6 AUC",
          "text_evidence": "the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations",
          "query_type": "value_context"
        },
        {
          "query": "Why does the blue baseline bar reach 0.9 while the orange non-compositional bar drops to 0.5?",
          "answer": "The baseline retains sensitive attribute information (high AUC of 0.9), while the non-compositional approach successfully removes much of this information, reducing AUC to 0.5, demonstrating effective attribute masking.",
          "visual_anchor": "Blue 'Baseline' bar at 0.9 AUC versus orange 'Non Compositional' bar at 0.5 AUC",
          "text_evidence": "average AUC across the 10 binary sensitive attributes",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the green bar at 0.6 AUC contradict the claim that compositional approaches performed worse on Reddit data?",
        "answer": "No, the green bar shows the compositional approach achieved 0.6 AUC compared to baseline's 0.9, confirming worse performance in removing sensitive attribute information as stated.",
        "visual_anchor": "Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC",
        "text_evidence": "on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the 0.025 difference between the two rightmost orange bars support the generalization capability claim?",
        "answer": "The small 0.025 AUC difference between 'No Held Out Compositional' (0.6) and 'Held Out Compositional' (0.6) bars demonstrates minimal performance drop, indicating effective generalization to unseen attribute combinations.",
        "visual_anchor": "Two rightmost bars: green 'No Held Out Compositional' and orange 'Held Out Compositional' both near 0.6 AUC",
        "text_evidence": "the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations",
        "query_type": "value_context"
      },
      {
        "query": "Why does the blue baseline bar reach 0.9 while the orange non-compositional bar drops to 0.5?",
        "answer": "The baseline retains sensitive attribute information (high AUC of 0.9), while the non-compositional approach successfully removes much of this information, reducing AUC to 0.5, demonstrating effective attribute masking.",
        "visual_anchor": "Blue 'Baseline' bar at 0.9 AUC versus orange 'Non Compositional' bar at 0.5 AUC",
        "text_evidence": "average AUC across the 10 binary sensitive attributes",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue bar labeled 'Baseline' at approximately 0.9 AUC",
      "Orange bar labeled 'Non Compositional' at approximately 0.5 AUC",
      "Green bar labeled 'No Held Out Compositional' at approximately 0.6 AUC",
      "Orange bar labeled 'Held Out Compositional' at approximately 0.6 AUC"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_5",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg",
    "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid blue line labeled 'Compositional Gender AUC'\",\n    \"dashed red horizontal line labeled 'Gender Baseline AUC' at approximately 0.71\",\n    \"x-axis showing λ values from 10^0 to 10^4 on logarithmic scale\",\n    \"sharp drop in blue curve between λ=100 and λ=1000\",\n    \"blue curve minimum around 0.51 at λ=1000\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid blue Compositional Gender AUC curve drop sharply below 0.60 between λ=100 and λ=1000?\",\n      \"answer\": \"The sharp drop represents the tradeoff where increasing the regularization parameter λ forces the compositional adversary on MovieLens1M to sacrifice gender prediction accuracy to achieve its compositional objectives.\",\n      \"visual_anchor\": \"solid blue curve dropping from approximately 0.69 to 0.51 between λ=100 and λ=1000\",\n      \"text_evidence\": \"Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the dashed red baseline at 0.71 remain constant across all λ values tested on MovieLens1M?\",\n      \"answer\": \"Yes, the Gender Baseline AUC remains constant at approximately 0.71 across all λ values, serving as a reference point independent of the regularization parameter applied to the compositional adversary.\",\n      \"visual_anchor\": \"horizontal dashed red line at AUC ≈ 0.71\",\n      \"text_evidence\": \"Gender Baseline AUC\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"At which λ value does the compositional adversary's gender prediction performance reach its minimum on MovieLens1M?\",\n      \"answer\": \"The compositional adversary achieves its minimum Gender AUC of approximately 0.51 at λ=1000, representing the strongest tradeoff point before slight recovery at higher λ values.\",\n      \"visual_anchor\": \"lowest point of solid blue curve at λ=1000 with AUC ≈ 0.51\",\n      \"text_evidence\": \"Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid blue line labeled 'Compositional Gender AUC'",
        "dashed red horizontal line labeled 'Gender Baseline AUC' at approximately 0.71",
        "x-axis showing λ values from 10^0 to 10^4 on logarithmic scale",
        "sharp drop in blue curve between λ=100 and λ=1000",
        "blue curve minimum around 0.51 at λ=1000"
      ],
      "queries": [
        {
          "query": "Why does the solid blue Compositional Gender AUC curve drop sharply below 0.60 between λ=100 and λ=1000?",
          "answer": "The sharp drop represents the tradeoff where increasing the regularization parameter λ forces the compositional adversary on MovieLens1M to sacrifice gender prediction accuracy to achieve its compositional objectives.",
          "visual_anchor": "solid blue curve dropping from approximately 0.69 to 0.51 between λ=100 and λ=1000",
          "text_evidence": "Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the dashed red baseline at 0.71 remain constant across all λ values tested on MovieLens1M?",
          "answer": "Yes, the Gender Baseline AUC remains constant at approximately 0.71 across all λ values, serving as a reference point independent of the regularization parameter applied to the compositional adversary.",
          "visual_anchor": "horizontal dashed red line at AUC ≈ 0.71",
          "text_evidence": "Gender Baseline AUC",
          "query_type": "comparison_explanation"
        },
        {
          "query": "At which λ value does the compositional adversary's gender prediction performance reach its minimum on MovieLens1M?",
          "answer": "The compositional adversary achieves its minimum Gender AUC of approximately 0.51 at λ=1000, representing the strongest tradeoff point before slight recovery at higher λ values.",
          "visual_anchor": "lowest point of solid blue curve at λ=1000 with AUC ≈ 0.51",
          "text_evidence": "Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid blue Compositional Gender AUC curve drop sharply below 0.60 between λ=100 and λ=1000?",
        "answer": "The sharp drop represents the tradeoff where increasing the regularization parameter λ forces the compositional adversary on MovieLens1M to sacrifice gender prediction accuracy to achieve its compositional objectives.",
        "visual_anchor": "solid blue curve dropping from approximately 0.69 to 0.51 between λ=100 and λ=1000",
        "text_evidence": "Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the dashed red baseline at 0.71 remain constant across all λ values tested on MovieLens1M?",
        "answer": "Yes, the Gender Baseline AUC remains constant at approximately 0.71 across all λ values, serving as a reference point independent of the regularization parameter applied to the compositional adversary.",
        "visual_anchor": "horizontal dashed red line at AUC ≈ 0.71",
        "text_evidence": "Gender Baseline AUC",
        "query_type": "comparison_explanation"
      },
      {
        "query": "At which λ value does the compositional adversary's gender prediction performance reach its minimum on MovieLens1M?",
        "answer": "The compositional adversary achieves its minimum Gender AUC of approximately 0.51 at λ=1000, representing the strongest tradeoff point before slight recovery at higher λ values.",
        "visual_anchor": "lowest point of solid blue curve at λ=1000 with AUC ≈ 0.51",
        "text_evidence": "Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different λ",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid blue line labeled 'Compositional Gender AUC'",
      "dashed red horizontal line labeled 'Gender Baseline AUC' at approximately 0.71",
      "x-axis showing λ values from 10^0 to 10^4 on logarithmic scale",
      "sharp drop in blue curve between λ=100 and λ=1000",
      "blue curve minimum around 0.51 at λ=1000"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_6",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg",
    "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Solid blue line labeled 'Compositional Adversary' rising from ~0.87 to 1.00\",\n    \"Dashed red horizontal line labeled 'Baseline RMSE' at approximately 0.86\",\n    \"X-axis showing λ values from 10^0 to 10^4 on logarithmic scale\",\n    \"Y-axis showing RMSE values from 0.85 to 1.00\",\n    \"Sharp increase in blue curve between λ=100 and λ=1000\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the compositional adversary's RMSE exceeding 0.95 at λ=1000 demonstrate favorable performance compared to individual attribute enforcement?\",\n      \"answer\": \"Yes, the compositional approach performed favorably, and on MovieLens-1M the compositionally trained adversary outperformed individually trained adversaries in removing sensitive attribute information.\",\n      \"visual_anchor\": \"Solid blue line reaching above 0.95 RMSE at λ=1000\",\n      \"text_evidence\": \"In all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the solid blue compositional adversary curve remain below the dashed red baseline until approximately λ=100?\",\n      \"answer\": \"At lower λ values, the adversarial regularization is too weak to effectively remove sensitive attribute information, so the model maintains lower RMSE closer to baseline performance before the adversary becomes strong enough to degrade prediction accuracy.\",\n      \"visual_anchor\": \"Blue curve staying near 0.87-0.88 while red baseline is at 0.86 for λ < 100\",\n      \"text_evidence\": \"training a model to jointly remove information about multiple sensitive attributes can be more effective than training separate models for each attribute\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causes the steep rise in the blue RMSE curve between λ=100 and λ=1000 on MovieLens-1M data?\",\n      \"answer\": \"The sharp increase occurs because stronger adversarial regularization at higher λ values forces the model to remove more sensitive attribute information, which degrades performance on the original edge prediction task as these attributes are relevant to the main task.\",\n      \"visual_anchor\": \"Blue curve rising sharply from ~0.89 at λ=100 to ~1.00 at λ=1000\",\n      \"text_evidence\": \"This result is not entirely surprising, since for this dataset the \\\"sensitive\\\" attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid blue line labeled 'Compositional Adversary' rising from ~0.87 to 1.00",
        "Dashed red horizontal line labeled 'Baseline RMSE' at approximately 0.86",
        "X-axis showing λ values from 10^0 to 10^4 on logarithmic scale",
        "Y-axis showing RMSE values from 0.85 to 1.00",
        "Sharp increase in blue curve between λ=100 and λ=1000"
      ],
      "queries": [
        {
          "query": "Does the compositional adversary's RMSE exceeding 0.95 at λ=1000 demonstrate favorable performance compared to individual attribute enforcement?",
          "answer": "Yes, the compositional approach performed favorably, and on MovieLens-1M the compositionally trained adversary outperformed individually trained adversaries in removing sensitive attribute information.",
          "visual_anchor": "Solid blue line reaching above 0.95 RMSE at λ=1000",
          "text_evidence": "In all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes",
          "query_type": "value_context"
        },
        {
          "query": "Why does the solid blue compositional adversary curve remain below the dashed red baseline until approximately λ=100?",
          "answer": "At lower λ values, the adversarial regularization is too weak to effectively remove sensitive attribute information, so the model maintains lower RMSE closer to baseline performance before the adversary becomes strong enough to degrade prediction accuracy.",
          "visual_anchor": "Blue curve staying near 0.87-0.88 while red baseline is at 0.86 for λ < 100",
          "text_evidence": "training a model to jointly remove information about multiple sensitive attributes can be more effective than training separate models for each attribute",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causes the steep rise in the blue RMSE curve between λ=100 and λ=1000 on MovieLens-1M data?",
          "answer": "The sharp increase occurs because stronger adversarial regularization at higher λ values forces the model to remove more sensitive attribute information, which degrades performance on the original edge prediction task as these attributes are relevant to the main task.",
          "visual_anchor": "Blue curve rising sharply from ~0.89 at λ=100 to ~1.00 at λ=1000",
          "text_evidence": "This result is not entirely surprising, since for this dataset the \"sensitive\" attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the compositional adversary's RMSE exceeding 0.95 at λ=1000 demonstrate favorable performance compared to individual attribute enforcement?",
        "answer": "Yes, the compositional approach performed favorably, and on MovieLens-1M the compositionally trained adversary outperformed individually trained adversaries in removing sensitive attribute information.",
        "visual_anchor": "Solid blue line reaching above 0.95 RMSE at λ=1000",
        "text_evidence": "In all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes",
        "query_type": "value_context"
      },
      {
        "query": "Why does the solid blue compositional adversary curve remain below the dashed red baseline until approximately λ=100?",
        "answer": "At lower λ values, the adversarial regularization is too weak to effectively remove sensitive attribute information, so the model maintains lower RMSE closer to baseline performance before the adversary becomes strong enough to degrade prediction accuracy.",
        "visual_anchor": "Blue curve staying near 0.87-0.88 while red baseline is at 0.86 for λ < 100",
        "text_evidence": "training a model to jointly remove information about multiple sensitive attributes can be more effective than training separate models for each attribute",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causes the steep rise in the blue RMSE curve between λ=100 and λ=1000 on MovieLens-1M data?",
        "answer": "The sharp increase occurs because stronger adversarial regularization at higher λ values forces the model to remove more sensitive attribute information, which degrades performance on the original edge prediction task as these attributes are relevant to the main task.",
        "visual_anchor": "Blue curve rising sharply from ~0.89 at λ=100 to ~1.00 at λ=1000",
        "text_evidence": "This result is not entirely surprising, since for this dataset the \"sensitive\" attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid blue line labeled 'Compositional Adversary' rising from ~0.87 to 1.00",
      "Dashed red horizontal line labeled 'Baseline RMSE' at approximately 0.86",
      "X-axis showing λ values from 10^0 to 10^4 on logarithmic scale",
      "Y-axis showing RMSE values from 0.85 to 1.00",
      "Sharp increase in blue curve between λ=100 and λ=1000"
    ]
  },
  {
    "doc_id": "1905.10674",
    "figure_id": "1905.10674_fig_7",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg",
    "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three grouped bars for Gender, Age, and Occupation\",\n    \"Baseline (blue) bars with highest values\",\n    \"Single Adversary (orange) and Compositional Adversary (green) bars with lower values\",\n    \"Gender baseline bar at approximately 0.05\",\n    \"Age baseline bar at approximately 0.012\",\n    \"Occupation baseline bar at approximately 0.019\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue Gender bar reach 0.05 while Age only reaches 0.012 despite both being sensitive attributes?\",\n      \"answer\": \"The compositional encoder can flexibly generate embeddings invariant to any subset of the K sensitive attributes, but different attributes exhibit different baseline prediction biases. Gender shows the highest baseline bias at 0.05, requiring stronger regularization.\",\n      \"visual_anchor\": \"blue bars at Gender (~0.05) and Age (~0.012)\",\n      \"text_evidence\": \"One of the key benefits of the compositional encoder is that it can flexibly generate embeddings that are invariant to any subset of $S \\\\subseteq \\\\{ 1 , . . . , K \\\\}$ of the sensitive attributes in a domain.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the green bar being lower than orange for Gender support that compositional adversaries reduce prediction bias better?\",\n      \"answer\": \"Yes, the green Compositional Adversary bar at approximately 0.016 is lower than the orange Single Adversary bar at approximately 0.018 for Gender. This aligns with findings that compositional adversaries work better, hypothesized to be due to correlation handling.\",\n      \"visual_anchor\": \"green bar (~0.016) vs orange bar (~0.018) at Gender\",\n      \"text_evidence\": \"Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlatio\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the blue versus orange and green bars demonstrate that adversarial regularization drastically reduces prediction bias?\",\n      \"answer\": \"The blue Baseline bars are substantially higher (Gender at 0.05, Occupation at 0.019) compared to orange Single Adversary and green Compositional Adversary bars (both below 0.02 for Gender). This dramatic reduction across all attributes demonstrates the effectiveness of adversarial regularization.\",\n      \"visual_anchor\": \"blue bars at 0.05 (Gender) and 0.019 (Occupation) versus orange/green bars below 0.02\",\n      \"text_evidence\": \"Figure 7 highlights these results, which show that adversarial regularization does in-\\n\\ndeed drastically reduce prediction bias.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three grouped bars for Gender, Age, and Occupation",
        "Baseline (blue) bars with highest values",
        "Single Adversary (orange) and Compositional Adversary (green) bars with lower values",
        "Gender baseline bar at approximately 0.05",
        "Age baseline bar at approximately 0.012",
        "Occupation baseline bar at approximately 0.019"
      ],
      "queries": [
        {
          "query": "Why does the blue Gender bar reach 0.05 while Age only reaches 0.012 despite both being sensitive attributes?",
          "answer": "The compositional encoder can flexibly generate embeddings invariant to any subset of the K sensitive attributes, but different attributes exhibit different baseline prediction biases. Gender shows the highest baseline bias at 0.05, requiring stronger regularization.",
          "visual_anchor": "blue bars at Gender (~0.05) and Age (~0.012)",
          "text_evidence": "One of the key benefits of the compositional encoder is that it can flexibly generate embeddings that are invariant to any subset of $S \\subseteq \\{ 1 , . . . , K \\}$ of the sensitive attributes in a domain.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the green bar being lower than orange for Gender support that compositional adversaries reduce prediction bias better?",
          "answer": "Yes, the green Compositional Adversary bar at approximately 0.016 is lower than the orange Single Adversary bar at approximately 0.018 for Gender. This aligns with findings that compositional adversaries work better, hypothesized to be due to correlation handling.",
          "visual_anchor": "green bar (~0.016) vs orange bar (~0.018) at Gender",
          "text_evidence": "Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlatio",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the blue versus orange and green bars demonstrate that adversarial regularization drastically reduces prediction bias?",
          "answer": "The blue Baseline bars are substantially higher (Gender at 0.05, Occupation at 0.019) compared to orange Single Adversary and green Compositional Adversary bars (both below 0.02 for Gender). This dramatic reduction across all attributes demonstrates the effectiveness of adversarial regularization.",
          "visual_anchor": "blue bars at 0.05 (Gender) and 0.019 (Occupation) versus orange/green bars below 0.02",
          "text_evidence": "Figure 7 highlights these results, which show that adversarial regularization does in-\n\ndeed drastically reduce prediction bias.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue Gender bar reach 0.05 while Age only reaches 0.012 despite both being sensitive attributes?",
        "answer": "The compositional encoder can flexibly generate embeddings invariant to any subset of the K sensitive attributes, but different attributes exhibit different baseline prediction biases. Gender shows the highest baseline bias at 0.05, requiring stronger regularization.",
        "visual_anchor": "blue bars at Gender (~0.05) and Age (~0.012)",
        "text_evidence": "One of the key benefits of the compositional encoder is that it can flexibly generate embeddings that are invariant to any subset of $S \\subseteq \\{ 1 , . . . , K \\}$ of the sensitive attributes in a domain.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the green bar being lower than orange for Gender support that compositional adversaries reduce prediction bias better?",
        "answer": "Yes, the green Compositional Adversary bar at approximately 0.016 is lower than the orange Single Adversary bar at approximately 0.018 for Gender. This aligns with findings that compositional adversaries work better, hypothesized to be due to correlation handling.",
        "visual_anchor": "green bar (~0.016) vs orange bar (~0.018) at Gender",
        "text_evidence": "Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlatio",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the blue versus orange and green bars demonstrate that adversarial regularization drastically reduces prediction bias?",
        "answer": "The blue Baseline bars are substantially higher (Gender at 0.05, Occupation at 0.019) compared to orange Single Adversary and green Compositional Adversary bars (both below 0.02 for Gender). This dramatic reduction across all attributes demonstrates the effectiveness of adversarial regularization.",
        "visual_anchor": "blue bars at 0.05 (Gender) and 0.019 (Occupation) versus orange/green bars below 0.02",
        "text_evidence": "Figure 7 highlights these results, which show that adversarial regularization does in-\n\ndeed drastically reduce prediction bias.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three grouped bars for Gender, Age, and Occupation",
      "Baseline (blue) bars with highest values",
      "Single Adversary (orange) and Compositional Adversary (green) bars with lower values",
      "Gender baseline bar at approximately 0.05",
      "Age baseline bar at approximately 0.012",
      "Occupation baseline bar at approximately 0.019"
    ]
  },
  {
    "doc_id": "1905.12843",
    "figure_id": "1905.12843_fig_4",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
    "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid orange line (fair reg., oracle=CS, model=linear)\",\n    \"solid red line (fair reg., oracle=LS, model=linear)\",\n    \"dashed red line (fair reg., oracle=LS, model=tree ensemble)\",\n    \"solid blue line (SEO model=linear)\",\n    \"purple star (unconstrained reg., model=linear)\",\n    \"green triangle (unconstrained reg., model=tree ensemble)\",\n    \"bottom plots with solid blue line (fair reg., oracle=LR, model=linear)\",\n    \"bottom plots with dashed blue line (fair reg., oracle=LR, model=tree ensemble)\",\n    \"bottom plots with solid black line (fair class., oracle=LR, model=linear)\",\n    \"bottom plots with dashed black line (fair class., oracle=LR, model=tree ensemble)\",\n    \"95% confidence bands and intervals\",\n    \"Pareto fronts showing relative test loss vs worst constraint violation\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?\",\n      \"answer\": \"The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.\",\n      \"visual_anchor\": \"solid black line (fair class., oracle=LR, model=linear) in bottom plots\",\n      \"text_evidence\": \"Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the convex envelopes shown for fair classification and the proposed algorithm reflect different training data selection procedures?\",\n      \"answer\": \"Yes, both methods plot convex envelopes of predictors obtained on training data at various accuracy-fairness tradeoffs, then evaluate these selected predictors on the test set to produce the Pareto fronts shown.\",\n      \"visual_anchor\": \"convex envelope curves (solid blue, solid black lines in bottom plots)\",\n      \"text_evidence\": \"For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How do the confidence bands around the proposed method's curve relate to the statistical comparison with baseline methods?\",\n      \"answer\": \"The 95% confidence bands for relative loss and constraint violation intervals enable statistical comparison, showing that the proposed method's dominance or matching of baselines holds within these uncertainty bounds across datasets.\",\n      \"visual_anchor\": \"95% confidence bands around solid blue and solid black curves\",\n      \"text_evidence\": \"We show $9 5 \\\\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\\\%$ confidence intervals for constraint violation (the same for all methods).\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid orange line (fair reg., oracle=CS, model=linear)",
        "solid red line (fair reg., oracle=LS, model=linear)",
        "dashed red line (fair reg., oracle=LS, model=tree ensemble)",
        "solid blue line (SEO model=linear)",
        "purple star (unconstrained reg., model=linear)",
        "green triangle (unconstrained reg., model=tree ensemble)",
        "bottom plots with solid blue line (fair reg., oracle=LR, model=linear)",
        "bottom plots with dashed blue line (fair reg., oracle=LR, model=tree ensemble)",
        "bottom plots with solid black line (fair class., oracle=LR, model=linear)",
        "bottom plots with dashed black line (fair class., oracle=LR, model=tree ensemble)",
        "95% confidence bands and intervals",
        "Pareto fronts showing relative test loss vs worst constraint violation"
      ],
      "queries": [
        {
          "query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
          "answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
          "visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
          "text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the convex envelopes shown for fair classification and the proposed algorithm reflect different training data selection procedures?",
          "answer": "Yes, both methods plot convex envelopes of predictors obtained on training data at various accuracy-fairness tradeoffs, then evaluate these selected predictors on the test set to produce the Pareto fronts shown.",
          "visual_anchor": "convex envelope curves (solid blue, solid black lines in bottom plots)",
          "text_evidence": "For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.",
          "query_type": "visual_definition"
        },
        {
          "query": "How do the confidence bands around the proposed method's curve relate to the statistical comparison with baseline methods?",
          "answer": "The 95% confidence bands for relative loss and constraint violation intervals enable statistical comparison, showing that the proposed method's dominance or matching of baselines holds within these uncertainty bounds across datasets.",
          "visual_anchor": "95% confidence bands around solid blue and solid black curves",
          "text_evidence": "We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods).",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid black line in bottom plots perform better despite the claim of dominance on all datasets except adult?",
        "answer": "The adult dataset is the exception where fair classification (solid black line) slightly outperforms the proposed method. On all other datasets, the proposed method dominates or matches baselines up to statistical uncertainty.",
        "visual_anchor": "solid black line (fair class., oracle=LR, model=linear) in bottom plots",
        "text_evidence": "Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the convex envelopes shown for fair classification and the proposed algorithm reflect different training data selection procedures?",
        "answer": "Yes, both methods plot convex envelopes of predictors obtained on training data at various accuracy-fairness tradeoffs, then evaluate these selected predictors on the test set to produce the Pareto fronts shown.",
        "visual_anchor": "convex envelope curves (solid blue, solid black lines in bottom plots)",
        "text_evidence": "For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.",
        "query_type": "visual_definition"
      },
      {
        "query": "How do the confidence bands around the proposed method's curve relate to the statistical comparison with baseline methods?",
        "answer": "The 95% confidence bands for relative loss and constraint violation intervals enable statistical comparison, showing that the proposed method's dominance or matching of baselines holds within these uncertainty bounds across datasets.",
        "visual_anchor": "95% confidence bands around solid blue and solid black curves",
        "text_evidence": "We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods).",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid orange line (fair reg., oracle=CS, model=linear)",
      "solid red line (fair reg., oracle=LS, model=linear)",
      "dashed red line (fair reg., oracle=LS, model=tree ensemble)",
      "solid blue line (SEO model=linear)",
      "purple star (unconstrained reg., model=linear)",
      "green triangle (unconstrained reg., model=tree ensemble)",
      "bottom plots with solid blue line (fair reg., oracle=LR, model=linear)",
      "bottom plots with dashed blue line (fair reg., oracle=LR, model=tree ensemble)",
      "bottom plots with solid black line (fair class., oracle=LR, model=linear)",
      "bottom plots with dashed black line (fair class., oracle=LR, model=tree ensemble)",
      "95% confidence bands and intervals",
      "Pareto fronts showing relative test loss vs worst constraint violation"
    ]
  },
  {
    "doc_id": "1905.12843",
    "figure_id": "1905.12843_fig_10",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg",
    "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"solid orange line labeled 'fair reg. (oracle=CS, model=linear)'\",\n    \"solid red line labeled 'fair reg. (oracle=LS, model=linear)'\",\n    \"dashed red line labeled 'fair reg. (oracle=LS, model=tree ensemble)'\",\n    \"blue inverted triangle marker labeled 'SEO (model=linear)'\",\n    \"green triangle marker labeled 'unconstrained reg. (model=linear)'\",\n    \"white triangle marker labeled 'unconstrained reg. (model=tree ensemble)'\",\n    \"solid blue line labeled 'fair reg. (oracle=LR, model=linear)'\",\n    \"dashed blue line labeled 'fair reg. (oracle=LR, model==tree ensemble)'\",\n    \"black dashed line labeled 'fair class. (oracle=LR, model=linear)'\",\n    \"black dashed line labeled 'fair class. (oracle=LR, model=tree ensemble)'\",\n    \"x-axis labeled 'constraint violation'\",\n    \"y-axis labeled 'training loss'\",\n    \"Pareto frontier curves showing tradeoffs\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the solid blue curve's Pareto frontier demonstrate the algorithm's ability to vary fairness slackness?\",\n      \"answer\": \"The solid blue curve shows a continuous tradeoff between training loss and constraint violation, achieved by varying the fairness slackness parameter to generate different predictors along the Pareto frontier.\",\n      \"visual_anchor\": \"solid blue line labeled 'fair reg. (oracle=LR, model=linear)' showing Pareto frontier\",\n      \"text_evidence\": \"For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the blue inverted triangle SEO marker appear isolated compared to the Pareto frontier curves?\",\n      \"answer\": \"The SEO method returns a single predictor rather than a set of predictors with varying fairness-accuracy tradeoffs, unlike the algorithms that generate Pareto frontiers by varying parameters.\",\n      \"visual_anchor\": \"blue inverted triangle marker labeled 'SEO (model=linear)' at single point\",\n      \"text_evidence\": \"In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the black dashed Pareto frontiers support including fair classification reduction methods in the logistic regression experiments?\",\n      \"answer\": \"Yes, the black dashed lines show the Pareto frontiers of fair classification reduction methods specifically for logistic regression experiments, demonstrating their comparative performance against other approaches.\",\n      \"visual_anchor\": \"black dashed lines labeled 'fair class. (oracle=LR, model=linear)' and 'fair class. (oracle=LR, model=tree ensemble)'\",\n      \"text_evidence\": \"For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid orange line labeled 'fair reg. (oracle=CS, model=linear)'",
        "solid red line labeled 'fair reg. (oracle=LS, model=linear)'",
        "dashed red line labeled 'fair reg. (oracle=LS, model=tree ensemble)'",
        "blue inverted triangle marker labeled 'SEO (model=linear)'",
        "green triangle marker labeled 'unconstrained reg. (model=linear)'",
        "white triangle marker labeled 'unconstrained reg. (model=tree ensemble)'",
        "solid blue line labeled 'fair reg. (oracle=LR, model=linear)'",
        "dashed blue line labeled 'fair reg. (oracle=LR, model==tree ensemble)'",
        "black dashed line labeled 'fair class. (oracle=LR, model=linear)'",
        "black dashed line labeled 'fair class. (oracle=LR, model=tree ensemble)'",
        "x-axis labeled 'constraint violation'",
        "y-axis labeled 'training loss'",
        "Pareto frontier curves showing tradeoffs"
      ],
      "queries": [
        {
          "query": "How does the solid blue curve's Pareto frontier demonstrate the algorithm's ability to vary fairness slackness?",
          "answer": "The solid blue curve shows a continuous tradeoff between training loss and constraint violation, achieved by varying the fairness slackness parameter to generate different predictors along the Pareto frontier.",
          "visual_anchor": "solid blue line labeled 'fair reg. (oracle=LR, model=linear)' showing Pareto frontier",
          "text_evidence": "For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the blue inverted triangle SEO marker appear isolated compared to the Pareto frontier curves?",
          "answer": "The SEO method returns a single predictor rather than a set of predictors with varying fairness-accuracy tradeoffs, unlike the algorithms that generate Pareto frontiers by varying parameters.",
          "visual_anchor": "blue inverted triangle marker labeled 'SEO (model=linear)' at single point",
          "text_evidence": "In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the black dashed Pareto frontiers support including fair classification reduction methods in the logistic regression experiments?",
          "answer": "Yes, the black dashed lines show the Pareto frontiers of fair classification reduction methods specifically for logistic regression experiments, demonstrating their comparative performance against other approaches.",
          "visual_anchor": "black dashed lines labeled 'fair class. (oracle=LR, model=linear)' and 'fair class. (oracle=LR, model=tree ensemble)'",
          "text_evidence": "For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the solid blue curve's Pareto frontier demonstrate the algorithm's ability to vary fairness slackness?",
        "answer": "The solid blue curve shows a continuous tradeoff between training loss and constraint violation, achieved by varying the fairness slackness parameter to generate different predictors along the Pareto frontier.",
        "visual_anchor": "solid blue line labeled 'fair reg. (oracle=LR, model=linear)' showing Pareto frontier",
        "text_evidence": "For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the blue inverted triangle SEO marker appear isolated compared to the Pareto frontier curves?",
        "answer": "The SEO method returns a single predictor rather than a set of predictors with varying fairness-accuracy tradeoffs, unlike the algorithms that generate Pareto frontiers by varying parameters.",
        "visual_anchor": "blue inverted triangle marker labeled 'SEO (model=linear)' at single point",
        "text_evidence": "In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the black dashed Pareto frontiers support including fair classification reduction methods in the logistic regression experiments?",
        "answer": "Yes, the black dashed lines show the Pareto frontiers of fair classification reduction methods specifically for logistic regression experiments, demonstrating their comparative performance against other approaches.",
        "visual_anchor": "black dashed lines labeled 'fair class. (oracle=LR, model=linear)' and 'fair class. (oracle=LR, model=tree ensemble)'",
        "text_evidence": "For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid orange line labeled 'fair reg. (oracle=CS, model=linear)'",
      "solid red line labeled 'fair reg. (oracle=LS, model=linear)'",
      "dashed red line labeled 'fair reg. (oracle=LS, model=tree ensemble)'",
      "blue inverted triangle marker labeled 'SEO (model=linear)'",
      "green triangle marker labeled 'unconstrained reg. (model=linear)'",
      "white triangle marker labeled 'unconstrained reg. (model=tree ensemble)'",
      "solid blue line labeled 'fair reg. (oracle=LR, model=linear)'",
      "dashed blue line labeled 'fair reg. (oracle=LR, model==tree ensemble)'",
      "black dashed line labeled 'fair class. (oracle=LR, model=linear)'",
      "black dashed line labeled 'fair class. (oracle=LR, model=tree ensemble)'",
      "x-axis labeled 'constraint violation'",
      "y-axis labeled 'training loss'",
      "Pareto frontier curves showing tradeoffs"
    ]
  },
  {
    "doc_id": "1905.12843",
    "figure_id": "1905.12843_fig_13",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg",
    "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Red line with circle markers labeled 'Fair reg. (oracle=LS, class=linear)'\",\n    \"Orange line with circle markers labeled 'Fair reg. (oracle=CS, class=linear)'\",\n    \"Y-axis showing '# oracle calls' ranging from approximately 5 to 27.5\",\n    \"X-axis showing fairness slackness from 0.0 to 1.0\",\n    \"Both lines show decreasing trend from left to right\",\n    \"Red line starts at approximately 26 oracle calls at x=0.0\",\n    \"Orange line starts at approximately 25 oracle calls at x=0.0\",\n    \"Both lines converge to lower values as fairness slackness increases\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red LS oracle line require approximately 26 calls at zero slackness when subsampling law school data?\",\n      \"answer\": \"The law school subsampled dataset requires maximum oracle calls at zero fairness slackness because stricter fairness constraints demand more iterations to find feasible solutions that satisfy both accuracy and fairness requirements.\",\n      \"visual_anchor\": \"Red line at x=0.0 showing approximately 26 oracle calls\",\n      \"text_evidence\": \"law school subsampled\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the orange CS oracle requiring fewer calls than red LS at slackness 0.2 demonstrate oracle efficiency differences?\",\n      \"answer\": \"Yes, at slackness 0.2, the CS oracle (orange, approximately 12 calls) is more efficient than the LS oracle (red, approximately 12 calls), though the difference is minimal at this point.\",\n      \"visual_anchor\": \"Orange and red lines at x=0.2 showing approximately 12 oracle calls each\",\n      \"text_evidence\": \"Fair reg. (oracle=LS, class=linear) and Fair reg. (oracle=CS, class=linear)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the sharp drop in both oracle curves between slackness 0.0 and 0.1 compared to gradual decline afterward?\",\n      \"answer\": \"The sharp initial drop from approximately 26 to 15 oracle calls indicates that even small increases in fairness slackness dramatically reduce computational burden, while further relaxation yields diminishing returns in oracle efficiency.\",\n      \"visual_anchor\": \"Steep decline in both red and orange lines between x=0.0 and x=0.1\",\n      \"text_evidence\": \"Number of oracle calls versus specified value of fairness slackness\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Red line with circle markers labeled 'Fair reg. (oracle=LS, class=linear)'",
        "Orange line with circle markers labeled 'Fair reg. (oracle=CS, class=linear)'",
        "Y-axis showing '# oracle calls' ranging from approximately 5 to 27.5",
        "X-axis showing fairness slackness from 0.0 to 1.0",
        "Both lines show decreasing trend from left to right",
        "Red line starts at approximately 26 oracle calls at x=0.0",
        "Orange line starts at approximately 25 oracle calls at x=0.0",
        "Both lines converge to lower values as fairness slackness increases"
      ],
      "queries": [
        {
          "query": "Why does the red LS oracle line require approximately 26 calls at zero slackness when subsampling law school data?",
          "answer": "The law school subsampled dataset requires maximum oracle calls at zero fairness slackness because stricter fairness constraints demand more iterations to find feasible solutions that satisfy both accuracy and fairness requirements.",
          "visual_anchor": "Red line at x=0.0 showing approximately 26 oracle calls",
          "text_evidence": "law school subsampled",
          "query_type": "value_context"
        },
        {
          "query": "Does the orange CS oracle requiring fewer calls than red LS at slackness 0.2 demonstrate oracle efficiency differences?",
          "answer": "Yes, at slackness 0.2, the CS oracle (orange, approximately 12 calls) is more efficient than the LS oracle (red, approximately 12 calls), though the difference is minimal at this point.",
          "visual_anchor": "Orange and red lines at x=0.2 showing approximately 12 oracle calls each",
          "text_evidence": "Fair reg. (oracle=LS, class=linear) and Fair reg. (oracle=CS, class=linear)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the sharp drop in both oracle curves between slackness 0.0 and 0.1 compared to gradual decline afterward?",
          "answer": "The sharp initial drop from approximately 26 to 15 oracle calls indicates that even small increases in fairness slackness dramatically reduce computational burden, while further relaxation yields diminishing returns in oracle efficiency.",
          "visual_anchor": "Steep decline in both red and orange lines between x=0.0 and x=0.1",
          "text_evidence": "Number of oracle calls versus specified value of fairness slackness",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red LS oracle line require approximately 26 calls at zero slackness when subsampling law school data?",
        "answer": "The law school subsampled dataset requires maximum oracle calls at zero fairness slackness because stricter fairness constraints demand more iterations to find feasible solutions that satisfy both accuracy and fairness requirements.",
        "visual_anchor": "Red line at x=0.0 showing approximately 26 oracle calls",
        "text_evidence": "law school subsampled",
        "query_type": "value_context"
      },
      {
        "query": "Does the orange CS oracle requiring fewer calls than red LS at slackness 0.2 demonstrate oracle efficiency differences?",
        "answer": "Yes, at slackness 0.2, the CS oracle (orange, approximately 12 calls) is more efficient than the LS oracle (red, approximately 12 calls), though the difference is minimal at this point.",
        "visual_anchor": "Orange and red lines at x=0.2 showing approximately 12 oracle calls each",
        "text_evidence": "Fair reg. (oracle=LS, class=linear) and Fair reg. (oracle=CS, class=linear)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the sharp drop in both oracle curves between slackness 0.0 and 0.1 compared to gradual decline afterward?",
        "answer": "The sharp initial drop from approximately 26 to 15 oracle calls indicates that even small increases in fairness slackness dramatically reduce computational burden, while further relaxation yields diminishing returns in oracle efficiency.",
        "visual_anchor": "Steep decline in both red and orange lines between x=0.0 and x=0.1",
        "text_evidence": "Number of oracle calls versus specified value of fairness slackness",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Red line with circle markers labeled 'Fair reg. (oracle=LS, class=linear)'",
      "Orange line with circle markers labeled 'Fair reg. (oracle=CS, class=linear)'",
      "Y-axis showing '# oracle calls' ranging from approximately 5 to 27.5",
      "X-axis showing fairness slackness from 0.0 to 1.0",
      "Both lines show decreasing trend from left to right",
      "Red line starts at approximately 26 oracle calls at x=0.0",
      "Orange line starts at approximately 25 oracle calls at x=0.0",
      "Both lines converge to lower values as fairness slackness increases"
    ]
  },
  {
    "doc_id": "1906.02589",
    "figure_id": "1906.02589_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg",
    "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"z box with teal/dark green gradient bar labeled non-sensitive latents\",\n    \"b box with orange gradient bar labeled sensitive latents\",\n    \"x box with green gradient bar labeled non-sensitive observations\",\n    \"a box with dark red/coral gradient bar labeled sensitive observations\",\n    \"curved bidirectional arrow from z to x\",\n    \"straight arrows from z and b pointing to a\",\n    \"straight arrow from b to a\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do both z and b boxes point to the sensitive observations a box during test time?\",\n      \"answer\": \"At test time, the model modifies the learned representations using both non-sensitive latents z and sensitive latents b to achieve subgroup demographic parity with respect to multiple sensitive attributes.\",\n      \"visual_anchor\": \"arrows from z box (teal gradient) and b box (orange gradient) converging on a box (dark red gradient)\",\n      \"text_evidence\": \"they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the curved bidirectional arrow between z and x support learning compact representations for reconstruction?\",\n      \"answer\": \"The bidirectional arrow between non-sensitive latents z (teal bar) and non-sensitive observations x (green bar) enables the model to encode x into z and decode z back to x, facilitating reconstruction while maintaining useful representations.\",\n      \"visual_anchor\": \"curved bidirectional arrow connecting z box (teal gradient bar) and x box (green gradient bar)\",\n      \"text_evidence\": \"learning compact representations of datasets that are useful for reconstruction and prediction\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Does the orange sensitive latents b box receive input from non-sensitive observations x at train time?\",\n      \"answer\": \"No, the diagram shows no arrow from x to b at train time. The sensitive latents b and non-sensitive latents z operate on separate pathways, with z having a bidirectional connection to x only.\",\n      \"visual_anchor\": \"absence of arrow from x box (green gradient) to b box (orange gradient)\",\n      \"text_evidence\": \"we consider labeled examples $x , a , y \\\\sim p _ { \\\\mathrm { d a t a } }$ where $y \\\\in \\\\mathcal { V }$ are labels we wish to predict, $a \\\\in { \\\\mathcal { A } }$ are sensitive attributes, and $x \\\\in \\\\mathcal { X }$ are non-sensitive attributes\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "z box with teal/dark green gradient bar labeled non-sensitive latents",
        "b box with orange gradient bar labeled sensitive latents",
        "x box with green gradient bar labeled non-sensitive observations",
        "a box with dark red/coral gradient bar labeled sensitive observations",
        "curved bidirectional arrow from z to x",
        "straight arrows from z and b pointing to a",
        "straight arrow from b to a"
      ],
      "queries": [
        {
          "query": "Why do both z and b boxes point to the sensitive observations a box during test time?",
          "answer": "At test time, the model modifies the learned representations using both non-sensitive latents z and sensitive latents b to achieve subgroup demographic parity with respect to multiple sensitive attributes.",
          "visual_anchor": "arrows from z box (teal gradient) and b box (orange gradient) converging on a box (dark red gradient)",
          "text_evidence": "they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the curved bidirectional arrow between z and x support learning compact representations for reconstruction?",
          "answer": "The bidirectional arrow between non-sensitive latents z (teal bar) and non-sensitive observations x (green bar) enables the model to encode x into z and decode z back to x, facilitating reconstruction while maintaining useful representations.",
          "visual_anchor": "curved bidirectional arrow connecting z box (teal gradient bar) and x box (green gradient bar)",
          "text_evidence": "learning compact representations of datasets that are useful for reconstruction and prediction",
          "query_type": "visual_definition"
        },
        {
          "query": "Does the orange sensitive latents b box receive input from non-sensitive observations x at train time?",
          "answer": "No, the diagram shows no arrow from x to b at train time. The sensitive latents b and non-sensitive latents z operate on separate pathways, with z having a bidirectional connection to x only.",
          "visual_anchor": "absence of arrow from x box (green gradient) to b box (orange gradient)",
          "text_evidence": "we consider labeled examples $x , a , y \\sim p _ { \\mathrm { d a t a } }$ where $y \\in \\mathcal { V }$ are labels we wish to predict, $a \\in { \\mathcal { A } }$ are sensitive attributes, and $x \\in \\mathcal { X }$ are non-sensitive attributes",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do both z and b boxes point to the sensitive observations a box during test time?",
        "answer": "At test time, the model modifies the learned representations using both non-sensitive latents z and sensitive latents b to achieve subgroup demographic parity with respect to multiple sensitive attributes.",
        "visual_anchor": "arrows from z box (teal gradient) and b box (orange gradient) converging on a box (dark red gradient)",
        "text_evidence": "they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the curved bidirectional arrow between z and x support learning compact representations for reconstruction?",
        "answer": "The bidirectional arrow between non-sensitive latents z (teal bar) and non-sensitive observations x (green bar) enables the model to encode x into z and decode z back to x, facilitating reconstruction while maintaining useful representations.",
        "visual_anchor": "curved bidirectional arrow connecting z box (teal gradient bar) and x box (green gradient bar)",
        "text_evidence": "learning compact representations of datasets that are useful for reconstruction and prediction",
        "query_type": "visual_definition"
      },
      {
        "query": "Does the orange sensitive latents b box receive input from non-sensitive observations x at train time?",
        "answer": "No, the diagram shows no arrow from x to b at train time. The sensitive latents b and non-sensitive latents z operate on separate pathways, with z having a bidirectional connection to x only.",
        "visual_anchor": "absence of arrow from x box (green gradient) to b box (orange gradient)",
        "text_evidence": "we consider labeled examples $x , a , y \\sim p _ { \\mathrm { d a t a } }$ where $y \\in \\mathcal { V }$ are labels we wish to predict, $a \\in { \\mathcal { A } }$ are sensitive attributes, and $x \\in \\mathcal { X }$ are non-sensitive attributes",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "z box with teal/dark green gradient bar labeled non-sensitive latents",
      "b box with orange gradient bar labeled sensitive latents",
      "x box with green gradient bar labeled non-sensitive observations",
      "a box with dark red/coral gradient bar labeled sensitive observations",
      "curved bidirectional arrow from z to x",
      "straight arrows from z and b pointing to a",
      "straight arrow from b to a"
    ]
  },
  {
    "doc_id": "1906.02589",
    "figure_id": "1906.02589_fig_5",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg",
    "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Solid blue curve labeled FFVAE\", \"Dashed blue curve labeled FactorVAE\", \"Dotted brown curve labeled CVAE\", \"Dotted green curve labeled β-VAE\", \"Purple dotted curve labeled MLP\", \"X-axis labeled ΔDP ranging from 0.00 to 0.07\", \"Y-axis labeled Accuracy ranging from 0.70 to 1.00\", \"Pareto front curves\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid blue FFVAE curve nearly overlap with dashed FactorVAE across all ΔDP values?\",\n      \"answer\": \"Both models are sweeping hyperparameters to generate Pareto fronts for the fairness-accuracy tradeoff, with encoder outputs modified to remove information about the same sensitive attribute a.\",\n      \"visual_anchor\": \"Solid blue FFVAE curve and dashed blue FactorVAE curve overlapping from ΔDP=0.00 to 0.07\",\n      \"text_evidence\": \"We sweep a range of hyperparameters for each model and report Pareto fronts. For each model, encoder outputs are modified to remove information about a.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the purple MLP baseline reaching approximately 0.99 accuracy at ΔDP=0.07 demonstrate advantage over encoder-based models?\",\n      \"answer\": \"No, the MLP is a baseline classifier trained directly on input data without fairness constraints, so its high accuracy at higher ΔDP values reflects lack of fairness optimization rather than superiority.\",\n      \"visual_anchor\": \"Purple dotted MLP curve at approximately 0.99 accuracy when ΔDP=0.07\",\n      \"text_evidence\": \"MLP is a baseline classifier trained directly on the input data.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the optimal top-left corner location relative to all Pareto front curves shown in the plot?\",\n      \"answer\": \"The top-left corner represents perfect accuracy and fairness simultaneously, which is the ideal but unattainable goal. All models' Pareto fronts fall below this point, showing the inherent tradeoff between these objectives.\",\n      \"visual_anchor\": \"Top-left corner at coordinates (0.00, 1.00) with all curves positioned below and to the right\",\n      \"text_evidence\": \"Optimal point is the top left hand corner — this represents perfect accuracy and fairness.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Solid blue curve labeled FFVAE",
        "Dashed blue curve labeled FactorVAE",
        "Dotted brown curve labeled CVAE",
        "Dotted green curve labeled β-VAE",
        "Purple dotted curve labeled MLP",
        "X-axis labeled ΔDP ranging from 0.00 to 0.07",
        "Y-axis labeled Accuracy ranging from 0.70 to 1.00",
        "Pareto front curves"
      ],
      "queries": [
        {
          "query": "Why does the solid blue FFVAE curve nearly overlap with dashed FactorVAE across all ΔDP values?",
          "answer": "Both models are sweeping hyperparameters to generate Pareto fronts for the fairness-accuracy tradeoff, with encoder outputs modified to remove information about the same sensitive attribute a.",
          "visual_anchor": "Solid blue FFVAE curve and dashed blue FactorVAE curve overlapping from ΔDP=0.00 to 0.07",
          "text_evidence": "We sweep a range of hyperparameters for each model and report Pareto fronts. For each model, encoder outputs are modified to remove information about a.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the purple MLP baseline reaching approximately 0.99 accuracy at ΔDP=0.07 demonstrate advantage over encoder-based models?",
          "answer": "No, the MLP is a baseline classifier trained directly on input data without fairness constraints, so its high accuracy at higher ΔDP values reflects lack of fairness optimization rather than superiority.",
          "visual_anchor": "Purple dotted MLP curve at approximately 0.99 accuracy when ΔDP=0.07",
          "text_evidence": "MLP is a baseline classifier trained directly on the input data.",
          "query_type": "value_context"
        },
        {
          "query": "What explains the optimal top-left corner location relative to all Pareto front curves shown in the plot?",
          "answer": "The top-left corner represents perfect accuracy and fairness simultaneously, which is the ideal but unattainable goal. All models' Pareto fronts fall below this point, showing the inherent tradeoff between these objectives.",
          "visual_anchor": "Top-left corner at coordinates (0.00, 1.00) with all curves positioned below and to the right",
          "text_evidence": "Optimal point is the top left hand corner — this represents perfect accuracy and fairness.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid blue FFVAE curve nearly overlap with dashed FactorVAE across all ΔDP values?",
        "answer": "Both models are sweeping hyperparameters to generate Pareto fronts for the fairness-accuracy tradeoff, with encoder outputs modified to remove information about the same sensitive attribute a.",
        "visual_anchor": "Solid blue FFVAE curve and dashed blue FactorVAE curve overlapping from ΔDP=0.00 to 0.07",
        "text_evidence": "We sweep a range of hyperparameters for each model and report Pareto fronts. For each model, encoder outputs are modified to remove information about a.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the purple MLP baseline reaching approximately 0.99 accuracy at ΔDP=0.07 demonstrate advantage over encoder-based models?",
        "answer": "No, the MLP is a baseline classifier trained directly on input data without fairness constraints, so its high accuracy at higher ΔDP values reflects lack of fairness optimization rather than superiority.",
        "visual_anchor": "Purple dotted MLP curve at approximately 0.99 accuracy when ΔDP=0.07",
        "text_evidence": "MLP is a baseline classifier trained directly on the input data.",
        "query_type": "value_context"
      },
      {
        "query": "What explains the optimal top-left corner location relative to all Pareto front curves shown in the plot?",
        "answer": "The top-left corner represents perfect accuracy and fairness simultaneously, which is the ideal but unattainable goal. All models' Pareto fronts fall below this point, showing the inherent tradeoff between these objectives.",
        "visual_anchor": "Top-left corner at coordinates (0.00, 1.00) with all curves positioned below and to the right",
        "text_evidence": "Optimal point is the top left hand corner — this represents perfect accuracy and fairness.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Solid blue curve labeled FFVAE",
      "Dashed blue curve labeled FactorVAE",
      "Dotted brown curve labeled CVAE",
      "Dotted green curve labeled β-VAE",
      "Purple dotted curve labeled MLP",
      "X-axis labeled ΔDP ranging from 0.00 to 0.07",
      "Y-axis labeled Accuracy ranging from 0.70 to 1.00",
      "Pareto front curves"
    ]
  },
  {
    "doc_id": "1906.02589",
    "figure_id": "1906.02589_fig_14",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg",
    "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"solid blue line labeled FFVAE\", \"dashed red line labeled CVAE\", \"dotted brown line labeled E-VAE\", \"x-axis labeled λ_cox\", \"y-axis labeled Accuracy\", \"accuracy range 0.71 to 0.85\", \"λ_cox range 0.00 to 0.20\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the solid blue FFVAE curve plateau near 0.85 accuracy while violentCrimesPerCapita is the prediction target?\",\n      \"answer\": \"FFVAE achieves fairness-accuracy balance for predicting violentCrimesPerCapita using sensitive attributes racePctBlack, blackPerCapIncome, and pctNotSpeakEnglWell, reaching a stable maximum accuracy around 0.85.\",\n      \"visual_anchor\": \"solid blue line labeled FFVAE plateauing at approximately 0.85 accuracy\",\n      \"text_evidence\": \"Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the dotted brown E-VAE line starting at 0.73 accuracy reflect the first exploration of fair representation learning?\",\n      \"answer\": \"No, the E-VAE baseline shows standard performance. The first exploration of fair representation refers to CelebA experiments with Chubby, Eyeglasses, and Male attributes, not the Communities & Crime dataset shown here.\",\n      \"visual_anchor\": \"dotted brown line labeled E-VAE starting at approximately 0.73 accuracy at λ_cox=0.00\",\n      \"text_evidence\": \"We chose three attributes, Chubby, Eyeglasses, and Male as sensitive attributes7, and report fair classification results on 3 groups and 12 two-attribute-conjunction subgroups only (for brevity we omit three-attribute conjunctions). To our knowledge this is the first exploration of fair repre\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the dashed red CVAE curve reaching 0.82 accuracy relate to the CelebA dataset containing 200,000 images?\",\n      \"answer\": \"The dashed red CVAE curve shows performance on Communities & Crime data, not CelebA. The CelebA dataset with 200,000 celebrity face images is a separate experiment discussed in the subsequent section.\",\n      \"visual_anchor\": \"dashed red line labeled CVAE reaching approximately 0.82 accuracy at λ_cox=0.20\",\n      \"text_evidence\": \"The CelebA6 dataset contains over 200, 000 images of celebrity faces. Each image is associated with 40 human-labeled binary attributes (OvalFace, HeavyMakeup, etc.).\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid blue line labeled FFVAE",
        "dashed red line labeled CVAE",
        "dotted brown line labeled E-VAE",
        "x-axis labeled λ_cox",
        "y-axis labeled Accuracy",
        "accuracy range 0.71 to 0.85",
        "λ_cox range 0.00 to 0.20"
      ],
      "queries": [
        {
          "query": "Why does the solid blue FFVAE curve plateau near 0.85 accuracy while violentCrimesPerCapita is the prediction target?",
          "answer": "FFVAE achieves fairness-accuracy balance for predicting violentCrimesPerCapita using sensitive attributes racePctBlack, blackPerCapIncome, and pctNotSpeakEnglWell, reaching a stable maximum accuracy around 0.85.",
          "visual_anchor": "solid blue line labeled FFVAE plateauing at approximately 0.85 accuracy",
          "text_evidence": "Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.",
          "query_type": "value_context"
        },
        {
          "query": "Does the dotted brown E-VAE line starting at 0.73 accuracy reflect the first exploration of fair representation learning?",
          "answer": "No, the E-VAE baseline shows standard performance. The first exploration of fair representation refers to CelebA experiments with Chubby, Eyeglasses, and Male attributes, not the Communities & Crime dataset shown here.",
          "visual_anchor": "dotted brown line labeled E-VAE starting at approximately 0.73 accuracy at λ_cox=0.00",
          "text_evidence": "We chose three attributes, Chubby, Eyeglasses, and Male as sensitive attributes7, and report fair classification results on 3 groups and 12 two-attribute-conjunction subgroups only (for brevity we omit three-attribute conjunctions). To our knowledge this is the first exploration of fair repre",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the dashed red CVAE curve reaching 0.82 accuracy relate to the CelebA dataset containing 200,000 images?",
          "answer": "The dashed red CVAE curve shows performance on Communities & Crime data, not CelebA. The CelebA dataset with 200,000 celebrity face images is a separate experiment discussed in the subsequent section.",
          "visual_anchor": "dashed red line labeled CVAE reaching approximately 0.82 accuracy at λ_cox=0.20",
          "text_evidence": "The CelebA6 dataset contains over 200, 000 images of celebrity faces. Each image is associated with 40 human-labeled binary attributes (OvalFace, HeavyMakeup, etc.).",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the solid blue FFVAE curve plateau near 0.85 accuracy while violentCrimesPerCapita is the prediction target?",
        "answer": "FFVAE achieves fairness-accuracy balance for predicting violentCrimesPerCapita using sensitive attributes racePctBlack, blackPerCapIncome, and pctNotSpeakEnglWell, reaching a stable maximum accuracy around 0.85.",
        "visual_anchor": "solid blue line labeled FFVAE plateauing at approximately 0.85 accuracy",
        "text_evidence": "Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.",
        "query_type": "value_context"
      },
      {
        "query": "Does the dotted brown E-VAE line starting at 0.73 accuracy reflect the first exploration of fair representation learning?",
        "answer": "No, the E-VAE baseline shows standard performance. The first exploration of fair representation refers to CelebA experiments with Chubby, Eyeglasses, and Male attributes, not the Communities & Crime dataset shown here.",
        "visual_anchor": "dotted brown line labeled E-VAE starting at approximately 0.73 accuracy at λ_cox=0.00",
        "text_evidence": "We chose three attributes, Chubby, Eyeglasses, and Male as sensitive attributes7, and report fair classification results on 3 groups and 12 two-attribute-conjunction subgroups only (for brevity we omit three-attribute conjunctions). To our knowledge this is the first exploration of fair repre",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the dashed red CVAE curve reaching 0.82 accuracy relate to the CelebA dataset containing 200,000 images?",
        "answer": "The dashed red CVAE curve shows performance on Communities & Crime data, not CelebA. The CelebA dataset with 200,000 celebrity face images is a separate experiment discussed in the subsequent section.",
        "visual_anchor": "dashed red line labeled CVAE reaching approximately 0.82 accuracy at λ_cox=0.20",
        "text_evidence": "The CelebA6 dataset contains over 200, 000 images of celebrity faces. Each image is associated with 40 human-labeled binary attributes (OvalFace, HeavyMakeup, etc.).",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid blue line labeled FFVAE",
      "dashed red line labeled CVAE",
      "dotted brown line labeled E-VAE",
      "x-axis labeled λ_cox",
      "y-axis labeled Accuracy",
      "accuracy range 0.71 to 0.85",
      "λ_cox range 0.00 to 0.20"
    ]
  },
  {
    "doc_id": "1906.02589",
    "figure_id": "1906.02589_fig_29",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
    "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"solid blue line labeled TEXAS\", \"dashed orange line labeled X-VAE\", \"x-axis labeled Δ_CP ranging 0.00 to 0.40\", \"y-axis labeled Accuracy ranging 0.65 to 0.825\", \"blue curve plateaus around 0.80\", \"orange curve reaches approximately 0.775\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the solid blue TEXAS curve plateauing at 0.80 accuracy demonstrate flexible fairness with respect to Chubby, Eyeglasses, and Male attributes?\",\n      \"answer\": \"Yes, FFVAE (TEXAS) provides flexibly fair representations that can be modified compositionally at test time to yield fair representations with respect to multiple sensitive attributes including Chubby, Eyeglasses, and Male.\",\n      \"visual_anchor\": \"solid blue line labeled TEXAS plateauing at approximately 0.80 accuracy\",\n      \"text_evidence\": \"The proposed model, FFVAE, provides flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?\",\n      \"answer\": \"FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.\",\n      \"visual_anchor\": \"solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40\",\n      \"text_evidence\": \"we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the steep accuracy rise between Δ_CP 0.00 and 0.10 for both curves reflect test-time modification of sensitive attribute conjunctions?\",\n      \"answer\": \"Yes, the steep initial rise reflects the flexible modification capability at test time, enabling fair representations even when test-time sensitive attributes involve conjunctions not seen during training.\",\n      \"visual_anchor\": \"steep rise in both blue and orange curves between Δ_CP 0.00 (accuracy ~0.66) and 0.10 (accuracy ~0.77)\",\n      \"text_evidence\": \"flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions, even when test-time s\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "solid blue line labeled TEXAS",
        "dashed orange line labeled X-VAE",
        "x-axis labeled Δ_CP ranging 0.00 to 0.40",
        "y-axis labeled Accuracy ranging 0.65 to 0.825",
        "blue curve plateaus around 0.80",
        "orange curve reaches approximately 0.775"
      ],
      "queries": [
        {
          "query": "Does the solid blue TEXAS curve plateauing at 0.80 accuracy demonstrate flexible fairness with respect to Chubby, Eyeglasses, and Male attributes?",
          "answer": "Yes, FFVAE (TEXAS) provides flexibly fair representations that can be modified compositionally at test time to yield fair representations with respect to multiple sensitive attributes including Chubby, Eyeglasses, and Male.",
          "visual_anchor": "solid blue line labeled TEXAS plateauing at approximately 0.80 accuracy",
          "text_evidence": "The proposed model, FFVAE, provides flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions",
          "query_type": "value_context"
        },
        {
          "query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
          "answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
          "visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
          "text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the steep accuracy rise between Δ_CP 0.00 and 0.10 for both curves reflect test-time modification of sensitive attribute conjunctions?",
          "answer": "Yes, the steep initial rise reflects the flexible modification capability at test time, enabling fair representations even when test-time sensitive attributes involve conjunctions not seen during training.",
          "visual_anchor": "steep rise in both blue and orange curves between Δ_CP 0.00 (accuracy ~0.66) and 0.10 (accuracy ~0.77)",
          "text_evidence": "flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions, even when test-time s",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the solid blue TEXAS curve plateauing at 0.80 accuracy demonstrate flexible fairness with respect to Chubby, Eyeglasses, and Male attributes?",
        "answer": "Yes, FFVAE (TEXAS) provides flexibly fair representations that can be modified compositionally at test time to yield fair representations with respect to multiple sensitive attributes including Chubby, Eyeglasses, and Male.",
        "visual_anchor": "solid blue line labeled TEXAS plateauing at approximately 0.80 accuracy",
        "text_evidence": "The proposed model, FFVAE, provides flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions",
        "query_type": "value_context"
      },
      {
        "query": "Why does the solid blue TEXAS line maintain higher accuracy than dashed orange X-VAE across all Δ_CP values when predicting Heavy-Makeup?",
        "answer": "FFVAE's structured latent code using multiple sensitive attributes (Chubby, Eyeglasses, Male) enables better disentanglement, aligning with subgroup fair machine learning goals more effectively than X-VAE's approach.",
        "visual_anchor": "solid blue TEXAS line consistently above dashed orange X-VAE line from Δ_CP 0.00 to 0.40",
        "text_evidence": "we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the steep accuracy rise between Δ_CP 0.00 and 0.10 for both curves reflect test-time modification of sensitive attribute conjunctions?",
        "answer": "Yes, the steep initial rise reflects the flexible modification capability at test time, enabling fair representations even when test-time sensitive attributes involve conjunctions not seen during training.",
        "visual_anchor": "steep rise in both blue and orange curves between Δ_CP 0.00 (accuracy ~0.66) and 0.10 (accuracy ~0.77)",
        "text_evidence": "flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions, even when test-time s",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "solid blue line labeled TEXAS",
      "dashed orange line labeled X-VAE",
      "x-axis labeled Δ_CP ranging 0.00 to 0.40",
      "y-axis labeled Accuracy ranging 0.65 to 0.825",
      "blue curve plateaus around 0.80",
      "orange curve reaches approximately 0.775"
    ]
  },
  {
    "doc_id": "1906.02589",
    "figure_id": "1906.02589_fig_32",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg",
    "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Black dashed line indicating mean MIG\", \"Multiple colored curves with brighter colors for larger gamma values\", \"X-axis labeled alpha ranging from 0 to 1000\", \"Y-axis labeled Mutual Information Gap ranging from 0 to 0.7\", \"Curves showing peak around alpha=300-400 then declining\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black dashed mean line remain near 0.65 across alpha values despite models being trained on DspritesUnfair?\",\n      \"answer\": \"The mean MIG is calculated on Dsprites (not DspritesUnfair), evaluating how well the encoder learns ground truth factors of variation. The consistent mean suggests stable disentanglement quality across hyperparameters when evaluated on the clean dataset.\",\n      \"visual_anchor\": \"Black dashed line at approximately y=0.65\",\n      \"text_evidence\": \"Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG)\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the decline from peak MIG at alpha=400 to 0.55 at alpha=1000 contradict claims of representation invariance to interventions?\",\n      \"answer\": \"No, the decline suggests that extreme alpha values reduce disentanglement quality, but the maintained MIG above 0.5 still indicates partial invariance to interventions on alpha, supporting the partial invariance claim.\",\n      \"visual_anchor\": \"Curve peaks around alpha=300-400 then declining to approximately 0.55 at alpha=1000\",\n      \"text_evidence\": \"We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How do the brighter colored curves at higher gamma values relate to the FactorVAE baseline at alpha equals zero?\",\n      \"answer\": \"The brighter curves represent larger gamma values in the FFVAE model, while alpha=0 represents the FactorVAE baseline. The progression shows how FFVAE extends FactorVAE by introducing the alpha hyperparameter to control fairness.\",\n      \"visual_anchor\": \"Brighter colored curves and leftmost point at alpha=0\",\n      \"text_evidence\": \"In Fig. 6a, each line is a different value of γ ∈ [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ] , with brighter colors indicating larger values of γ. α = 0 is equivalent to the FactorVAE.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black dashed line indicating mean MIG",
        "Multiple colored curves with brighter colors for larger gamma values",
        "X-axis labeled alpha ranging from 0 to 1000",
        "Y-axis labeled Mutual Information Gap ranging from 0 to 0.7",
        "Curves showing peak around alpha=300-400 then declining"
      ],
      "queries": [
        {
          "query": "Why does the black dashed mean line remain near 0.65 across alpha values despite models being trained on DspritesUnfair?",
          "answer": "The mean MIG is calculated on Dsprites (not DspritesUnfair), evaluating how well the encoder learns ground truth factors of variation. The consistent mean suggests stable disentanglement quality across hyperparameters when evaluated on the clean dataset.",
          "visual_anchor": "Black dashed line at approximately y=0.65",
          "text_evidence": "Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG)",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the decline from peak MIG at alpha=400 to 0.55 at alpha=1000 contradict claims of representation invariance to interventions?",
          "answer": "No, the decline suggests that extreme alpha values reduce disentanglement quality, but the maintained MIG above 0.5 still indicates partial invariance to interventions on alpha, supporting the partial invariance claim.",
          "visual_anchor": "Curve peaks around alpha=300-400 then declining to approximately 0.55 at alpha=1000",
          "text_evidence": "We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How do the brighter colored curves at higher gamma values relate to the FactorVAE baseline at alpha equals zero?",
          "answer": "The brighter curves represent larger gamma values in the FFVAE model, while alpha=0 represents the FactorVAE baseline. The progression shows how FFVAE extends FactorVAE by introducing the alpha hyperparameter to control fairness.",
          "visual_anchor": "Brighter colored curves and leftmost point at alpha=0",
          "text_evidence": "In Fig. 6a, each line is a different value of γ ∈ [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ] , with brighter colors indicating larger values of γ. α = 0 is equivalent to the FactorVAE.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black dashed mean line remain near 0.65 across alpha values despite models being trained on DspritesUnfair?",
        "answer": "The mean MIG is calculated on Dsprites (not DspritesUnfair), evaluating how well the encoder learns ground truth factors of variation. The consistent mean suggests stable disentanglement quality across hyperparameters when evaluated on the clean dataset.",
        "visual_anchor": "Black dashed line at approximately y=0.65",
        "text_evidence": "Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG)",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the decline from peak MIG at alpha=400 to 0.55 at alpha=1000 contradict claims of representation invariance to interventions?",
        "answer": "No, the decline suggests that extreme alpha values reduce disentanglement quality, but the maintained MIG above 0.5 still indicates partial invariance to interventions on alpha, supporting the partial invariance claim.",
        "visual_anchor": "Curve peaks around alpha=300-400 then declining to approximately 0.55 at alpha=1000",
        "text_evidence": "We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How do the brighter colored curves at higher gamma values relate to the FactorVAE baseline at alpha equals zero?",
        "answer": "The brighter curves represent larger gamma values in the FFVAE model, while alpha=0 represents the FactorVAE baseline. The progression shows how FFVAE extends FactorVAE by introducing the alpha hyperparameter to control fairness.",
        "visual_anchor": "Brighter colored curves and leftmost point at alpha=0",
        "text_evidence": "In Fig. 6a, each line is a different value of γ ∈ [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ] , with brighter colors indicating larger values of γ. α = 0 is equivalent to the FactorVAE.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black dashed line indicating mean MIG",
      "Multiple colored curves with brighter colors for larger gamma values",
      "X-axis labeled alpha ranging from 0 to 1000",
      "Y-axis labeled Mutual Information Gap ranging from 0 to 0.7",
      "Curves showing peak around alpha=300-400 then declining"
    ]
  },
  {
    "doc_id": "1906.02589",
    "figure_id": "1906.02589_fig_34",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg",
    "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
    "original_quality_score": 0.5499999999999999,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Black dashed line showing mean MIG\",\n    \"Multiple colored lines with brighter colors for larger alpha values\",\n    \"Y-axis labeled 'Mutual Information Gap' ranging 0 to 0.7\",\n    \"X-axis labeled 'γ' ranging 0 to 100\",\n    \"Lines declining from left to right, especially after γ=60\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the black dashed mean line decline from approximately 0.5 to 0.15 as γ increases to 100?\",\n      \"answer\": \"The mean MIG (excluding outliers) shows that increasing γ generally degrades disentanglement performance across different α settings, indicating that higher γ values harm the model's ability to isolate interpretable factors.\",\n      \"visual_anchor\": \"Black dashed line declining from ~0.5 at γ=0 to ~0.15 at γ=100\",\n      \"text_evidence\": \"Black dashed line indicates mean (outliers excluded). $\\\\alpha = 0$ is equivalent to the FactorVAE.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the brighter colored lines representing larger alpha values from 0 to 200 consistently outperform darker lines?\",\n      \"answer\": \"No, the brighter lines (larger α values) do not consistently outperform darker ones. Performance varies across γ values, suggesting that simply increasing α does not guarantee better disentanglement as measured by MIG.\",\n      \"visual_anchor\": \"Brighter colored lines (larger α ∈ [0,50,100,150,200]) compared to darker lines across γ range\",\n      \"text_evidence\": \"each line is a different value of $\\\\alpha \\\\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\\\alpha$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does training on DspritesUnfair while evaluating on Dsprites affect interpretation of the declining MIG after γ equals 60?\",\n      \"answer\": \"The decline suggests that FFVAE's fairness regularization (via γ) trained on biased data increasingly harms disentanglement when evaluated on unbiased data, indicating a trade-off between fairness constraints and representational quality across distribution shifts.\",\n      \"visual_anchor\": \"All lines declining sharply after γ=60, dropping from ~0.4-0.7 to ~0.1-0.3\",\n      \"text_evidence\": \"Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Black dashed line showing mean MIG",
        "Multiple colored lines with brighter colors for larger alpha values",
        "Y-axis labeled 'Mutual Information Gap' ranging 0 to 0.7",
        "X-axis labeled 'γ' ranging 0 to 100",
        "Lines declining from left to right, especially after γ=60"
      ],
      "queries": [
        {
          "query": "Why does the black dashed mean line decline from approximately 0.5 to 0.15 as γ increases to 100?",
          "answer": "The mean MIG (excluding outliers) shows that increasing γ generally degrades disentanglement performance across different α settings, indicating that higher γ values harm the model's ability to isolate interpretable factors.",
          "visual_anchor": "Black dashed line declining from ~0.5 at γ=0 to ~0.15 at γ=100",
          "text_evidence": "Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the brighter colored lines representing larger alpha values from 0 to 200 consistently outperform darker lines?",
          "answer": "No, the brighter lines (larger α values) do not consistently outperform darker ones. Performance varies across γ values, suggesting that simply increasing α does not guarantee better disentanglement as measured by MIG.",
          "visual_anchor": "Brighter colored lines (larger α ∈ [0,50,100,150,200]) compared to darker lines across γ range",
          "text_evidence": "each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does training on DspritesUnfair while evaluating on Dsprites affect interpretation of the declining MIG after γ equals 60?",
          "answer": "The decline suggests that FFVAE's fairness regularization (via γ) trained on biased data increasingly harms disentanglement when evaluated on unbiased data, indicating a trade-off between fairness constraints and representational quality across distribution shifts.",
          "visual_anchor": "All lines declining sharply after γ=60, dropping from ~0.4-0.7 to ~0.1-0.3",
          "text_evidence": "Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the black dashed mean line decline from approximately 0.5 to 0.15 as γ increases to 100?",
        "answer": "The mean MIG (excluding outliers) shows that increasing γ generally degrades disentanglement performance across different α settings, indicating that higher γ values harm the model's ability to isolate interpretable factors.",
        "visual_anchor": "Black dashed line declining from ~0.5 at γ=0 to ~0.15 at γ=100",
        "text_evidence": "Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the brighter colored lines representing larger alpha values from 0 to 200 consistently outperform darker lines?",
        "answer": "No, the brighter lines (larger α values) do not consistently outperform darker ones. Performance varies across γ values, suggesting that simply increasing α does not guarantee better disentanglement as measured by MIG.",
        "visual_anchor": "Brighter colored lines (larger α ∈ [0,50,100,150,200]) compared to darker lines across γ range",
        "text_evidence": "each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does training on DspritesUnfair while evaluating on Dsprites affect interpretation of the declining MIG after γ equals 60?",
        "answer": "The decline suggests that FFVAE's fairness regularization (via γ) trained on biased data increasingly harms disentanglement when evaluated on unbiased data, indicating a trade-off between fairness constraints and representational quality across distribution shifts.",
        "visual_anchor": "All lines declining sharply after γ=60, dropping from ~0.4-0.7 to ~0.1-0.3",
        "text_evidence": "Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Black dashed line showing mean MIG",
      "Multiple colored lines with brighter colors for larger alpha values",
      "Y-axis labeled 'Mutual Information Gap' ranging 0 to 0.7",
      "X-axis labeled 'γ' ranging 0 to 100",
      "Lines declining from left to right, especially after γ=60"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg",
    "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Stacked bar chart with two panels (Black and White)\", \"Blue bars representing 'Reoffended'\", \"Dark blue bars representing 'Did not reoffend'\", \"Low and Medium/High risk categories on x-axis\", \"Number of defendants on y-axis (0-2000)\", \"Black defendants: Low (~1350 total, ~900 did not reoffend), Medium/High (~1800 total, ~650 did not reoffend)\", \"White defendants: Low (~1400 total, ~1000 did not reoffend), Medium/High (~700 total, ~300 did not reoffend)\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the approximately equal proportion of reoffenders within each risk category for both races demonstrate Y ⊥⊥ A | Ŷ?\",\n      \"answer\": \"Yes, the visual similarity in the proportion of light blue (reoffended) to dark blue (did not reoffend) within Low and Medium/High categories for both Black and White panels illustrates that within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race.\",\n      \"visual_anchor\": \"Proportion of light blue to dark blue bars within Low and Medium/High categories across both panels\",\n      \"text_evidence\": \"Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\\\perp \\\\perp A | \\\\hat { Y }$\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why is the Medium/High bar taller than Low for Black defendants but shorter for White defendants?\",\n      \"answer\": \"Black defendants are more likely to be classified as medium or high risk (58% vs. 33%), demonstrating that risk classification is not independent of race. This disparity shows that Ŷ ⊥⊥ A does not hold.\",\n      \"visual_anchor\": \"Total height comparison between Low and Medium/High bars in Black panel (~1350 vs ~1800) versus White panel (~1400 vs ~700)\",\n      \"text_evidence\": \"Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\\\hat { Y } \\\\mathcal { M } A$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the dark blue segment being larger in Medium/High than Low for Black defendants support the 44.9% versus 23.5% claim?\",\n      \"answer\": \"Yes, the dark blue (did not reoffend) segment is proportionally larger in the Medium/High category for Black defendants compared to White defendants, visually confirming that among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk.\",\n      \"visual_anchor\": \"Dark blue segment height in Medium/High category: Black panel (~650) versus White panel (~300), relative to total non-reoffenders\",\n      \"text_evidence\": \"Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\\\%$)\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Stacked bar chart with two panels (Black and White)",
        "Blue bars representing 'Reoffended'",
        "Dark blue bars representing 'Did not reoffend'",
        "Low and Medium/High risk categories on x-axis",
        "Number of defendants on y-axis (0-2000)",
        "Black defendants: Low (~1350 total, ~900 did not reoffend), Medium/High (~1800 total, ~650 did not reoffend)",
        "White defendants: Low (~1400 total, ~1000 did not reoffend), Medium/High (~700 total, ~300 did not reoffend)"
      ],
      "queries": [
        {
          "query": "Does the approximately equal proportion of reoffenders within each risk category for both races demonstrate Y ⊥⊥ A | Ŷ?",
          "answer": "Yes, the visual similarity in the proportion of light blue (reoffended) to dark blue (did not reoffend) within Low and Medium/High categories for both Black and White panels illustrates that within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race.",
          "visual_anchor": "Proportion of light blue to dark blue bars within Low and Medium/High categories across both panels",
          "text_evidence": "Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$",
          "query_type": "visual_definition"
        },
        {
          "query": "Why is the Medium/High bar taller than Low for Black defendants but shorter for White defendants?",
          "answer": "Black defendants are more likely to be classified as medium or high risk (58% vs. 33%), demonstrating that risk classification is not independent of race. This disparity shows that Ŷ ⊥⊥ A does not hold.",
          "visual_anchor": "Total height comparison between Low and Medium/High bars in Black panel (~1350 vs ~1800) versus White panel (~1400 vs ~700)",
          "text_evidence": "Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the dark blue segment being larger in Medium/High than Low for Black defendants support the 44.9% versus 23.5% claim?",
          "answer": "Yes, the dark blue (did not reoffend) segment is proportionally larger in the Medium/High category for Black defendants compared to White defendants, visually confirming that among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk.",
          "visual_anchor": "Dark blue segment height in Medium/High category: Black panel (~650) versus White panel (~300), relative to total non-reoffenders",
          "text_evidence": "Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$)",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the approximately equal proportion of reoffenders within each risk category for both races demonstrate Y ⊥⊥ A | Ŷ?",
        "answer": "Yes, the visual similarity in the proportion of light blue (reoffended) to dark blue (did not reoffend) within Low and Medium/High categories for both Black and White panels illustrates that within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race.",
        "visual_anchor": "Proportion of light blue to dark blue bars within Low and Medium/High categories across both panels",
        "text_evidence": "Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$",
        "query_type": "visual_definition"
      },
      {
        "query": "Why is the Medium/High bar taller than Low for Black defendants but shorter for White defendants?",
        "answer": "Black defendants are more likely to be classified as medium or high risk (58% vs. 33%), demonstrating that risk classification is not independent of race. This disparity shows that Ŷ ⊥⊥ A does not hold.",
        "visual_anchor": "Total height comparison between Low and Medium/High bars in Black panel (~1350 vs ~1800) versus White panel (~1400 vs ~700)",
        "text_evidence": "Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the dark blue segment being larger in Medium/High than Low for Black defendants support the 44.9% versus 23.5% claim?",
        "answer": "Yes, the dark blue (did not reoffend) segment is proportionally larger in the Medium/High category for Black defendants compared to White defendants, visually confirming that among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk.",
        "visual_anchor": "Dark blue segment height in Medium/High category: Black panel (~650) versus White panel (~300), relative to total non-reoffenders",
        "text_evidence": "Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$)",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Stacked bar chart with two panels (Black and White)",
      "Blue bars representing 'Reoffended'",
      "Dark blue bars representing 'Did not reoffend'",
      "Low and Medium/High risk categories on x-axis",
      "Number of defendants on y-axis (0-2000)",
      "Black defendants: Low (~1350 total, ~900 did not reoffend), Medium/High (~1800 total, ~650 did not reoffend)",
      "White defendants: Low (~1400 total, ~1000 did not reoffend), Medium/High (~700 total, ~300 did not reoffend)"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg",
    "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A with dashed arrow to F\", \"Node M with solid line to Y\", \"Red arrow from A to Y\", \"Solid blue arrow from F to Y\", \"Four nodes: A, M, F, Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red arrow from A directly to Y represent a statistical dependency in this Bayesian network?\",\n      \"answer\": \"In Bayesian networks, edges represent statistical dependencies between random variables. The red arrow from A to Y indicates that Y's conditional distribution depends on A as a parent.\",\n      \"visual_anchor\": \"red arrow from node A to node Y\",\n      \"text_evidence\": \"A Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does node Y's conditional distribution differ from node M's, given Y has three parent nodes while M has none?\",\n      \"answer\": \"Node Y's conditional distribution is p(Y|pa(Y)) with three parents (A, F, M), while M has no parents so its distribution is simply p(M). Y depends on more variables than M.\",\n      \"visual_anchor\": \"node Y with three incoming arrows versus node M with no incoming arrows\",\n      \"text_evidence\": \"Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\\\mathrm { p a } ( X _ { i } ) )$ , where $\\\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What role do the four nodes A, F, M, and Y play in computing the joint distribution for COMPAS dataset?\",\n      \"answer\": \"These four nodes represent random variables whose joint distribution p(A,F,M,Y) is computed as the product of all conditional distributions based on the parent-child relationships shown in the network.\",\n      \"visual_anchor\": \"four circular nodes labeled A, F, M, and Y\",\n      \"text_evidence\": \"The joint distribution of all nodes, $p ( X _ { 1 } , \\\\ldots , X _ { I } )$ , is given by the product of all conditi\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A with dashed arrow to F",
        "Node M with solid line to Y",
        "Red arrow from A to Y",
        "Solid blue arrow from F to Y",
        "Four nodes: A, M, F, Y"
      ],
      "queries": [
        {
          "query": "Why does the red arrow from A directly to Y represent a statistical dependency in this Bayesian network?",
          "answer": "In Bayesian networks, edges represent statistical dependencies between random variables. The red arrow from A to Y indicates that Y's conditional distribution depends on A as a parent.",
          "visual_anchor": "red arrow from node A to node Y",
          "text_evidence": "A Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does node Y's conditional distribution differ from node M's, given Y has three parent nodes while M has none?",
          "answer": "Node Y's conditional distribution is p(Y|pa(Y)) with three parents (A, F, M), while M has no parents so its distribution is simply p(M). Y depends on more variables than M.",
          "visual_anchor": "node Y with three incoming arrows versus node M with no incoming arrows",
          "text_evidence": "Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What role do the four nodes A, F, M, and Y play in computing the joint distribution for COMPAS dataset?",
          "answer": "These four nodes represent random variables whose joint distribution p(A,F,M,Y) is computed as the product of all conditional distributions based on the parent-child relationships shown in the network.",
          "visual_anchor": "four circular nodes labeled A, F, M, and Y",
          "text_evidence": "The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditi",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red arrow from A directly to Y represent a statistical dependency in this Bayesian network?",
        "answer": "In Bayesian networks, edges represent statistical dependencies between random variables. The red arrow from A to Y indicates that Y's conditional distribution depends on A as a parent.",
        "visual_anchor": "red arrow from node A to node Y",
        "text_evidence": "A Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does node Y's conditional distribution differ from node M's, given Y has three parent nodes while M has none?",
        "answer": "Node Y's conditional distribution is p(Y|pa(Y)) with three parents (A, F, M), while M has no parents so its distribution is simply p(M). Y depends on more variables than M.",
        "visual_anchor": "node Y with three incoming arrows versus node M with no incoming arrows",
        "text_evidence": "Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What role do the four nodes A, F, M, and Y play in computing the joint distribution for COMPAS dataset?",
        "answer": "These four nodes represent random variables whose joint distribution p(A,F,M,Y) is computed as the product of all conditional distributions based on the parent-child relationships shown in the network.",
        "visual_anchor": "four circular nodes labeled A, F, M, and Y",
        "text_evidence": "The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditi",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A with dashed arrow to F",
      "Node M with solid line to Y",
      "Red arrow from A to Y",
      "Solid blue arrow from F to Y",
      "Four nodes: A, M, F, Y"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_4",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg",
    "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .",
    "original_quality_score": 0.5,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node C at top with p(C) label\", \"Node A on bottom left with p(A|C) label\", \"Node Y on bottom right with p(Y|C,A) label\", \"Arrow from C to A labeled g\", \"Arrow from C to Y\", \"Arrow from A to Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does node C have only the marginal distribution p(C) while nodes A and Y have conditional distributions?\",\n      \"answer\": \"Node C is the root node with no parents, so it has only a marginal distribution p(C). Each node Xi in a Bayesian network is associated with the conditional distribution p(Xi|pa(Xi)), and since C has no parents, it requires only its marginal distribution.\",\n      \"visual_anchor\": \"Node C at top with label p(C), contrasted with node A labeled p(A|C) and node Y labeled p(Y|C,A)\",\n      \"text_evidence\": \"Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\\\mathrm { p a } ( X _ { i } ) )$ , where $\\\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the directed edge labeled g from C to A demonstrate that C is a confounder?\",\n      \"answer\": \"The edge g from C to A, combined with C's edge to Y, shows that C influences both treatment A and outcome Y, creating a spurious association. This defines C as a confounder that creates non-causal correlation between A and Y.\",\n      \"visual_anchor\": \"Arrow labeled g connecting node C to node A, and separate arrow from C to Y\",\n      \"text_evidence\": \"CBN with a confounder $C$ for the effect of $A$ on $Y$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does node Y's conditional distribution p(Y|C,A) depend on both parent nodes C and A?\",\n      \"answer\": \"Node Y has two incoming arrows, one from C and one from A, making both C and A parents of Y. The joint distribution is given by the product of all conditional distributions, so Y's distribution must condition on all its parents.\",\n      \"visual_anchor\": \"Node Y on bottom right with two incoming arrows from nodes C and A, labeled p(Y|C,A)\",\n      \"text_evidence\": \"The joint distribution of all nodes, $p ( X _ { 1 } , \\\\ldots , X _ { I } )$ , is given by the product of all conditi\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node C at top with p(C) label",
        "Node A on bottom left with p(A|C) label",
        "Node Y on bottom right with p(Y|C,A) label",
        "Arrow from C to A labeled g",
        "Arrow from C to Y",
        "Arrow from A to Y"
      ],
      "queries": [
        {
          "query": "Why does node C have only the marginal distribution p(C) while nodes A and Y have conditional distributions?",
          "answer": "Node C is the root node with no parents, so it has only a marginal distribution p(C). Each node Xi in a Bayesian network is associated with the conditional distribution p(Xi|pa(Xi)), and since C has no parents, it requires only its marginal distribution.",
          "visual_anchor": "Node C at top with label p(C), contrasted with node A labeled p(A|C) and node Y labeled p(Y|C,A)",
          "text_evidence": "Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the directed edge labeled g from C to A demonstrate that C is a confounder?",
          "answer": "The edge g from C to A, combined with C's edge to Y, shows that C influences both treatment A and outcome Y, creating a spurious association. This defines C as a confounder that creates non-causal correlation between A and Y.",
          "visual_anchor": "Arrow labeled g connecting node C to node A, and separate arrow from C to Y",
          "text_evidence": "CBN with a confounder $C$ for the effect of $A$ on $Y$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does node Y's conditional distribution p(Y|C,A) depend on both parent nodes C and A?",
          "answer": "Node Y has two incoming arrows, one from C and one from A, making both C and A parents of Y. The joint distribution is given by the product of all conditional distributions, so Y's distribution must condition on all its parents.",
          "visual_anchor": "Node Y on bottom right with two incoming arrows from nodes C and A, labeled p(Y|C,A)",
          "text_evidence": "The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditi",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does node C have only the marginal distribution p(C) while nodes A and Y have conditional distributions?",
        "answer": "Node C is the root node with no parents, so it has only a marginal distribution p(C). Each node Xi in a Bayesian network is associated with the conditional distribution p(Xi|pa(Xi)), and since C has no parents, it requires only its marginal distribution.",
        "visual_anchor": "Node C at top with label p(C), contrasted with node A labeled p(A|C) and node Y labeled p(Y|C,A)",
        "text_evidence": "Each node $X _ { i }$ in the graph is associated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the directed edge labeled g from C to A demonstrate that C is a confounder?",
        "answer": "The edge g from C to A, combined with C's edge to Y, shows that C influences both treatment A and outcome Y, creating a spurious association. This defines C as a confounder that creates non-causal correlation between A and Y.",
        "visual_anchor": "Arrow labeled g connecting node C to node A, and separate arrow from C to Y",
        "text_evidence": "CBN with a confounder $C$ for the effect of $A$ on $Y$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does node Y's conditional distribution p(Y|C,A) depend on both parent nodes C and A?",
        "answer": "Node Y has two incoming arrows, one from C and one from A, making both C and A parents of Y. The joint distribution is given by the product of all conditional distributions, so Y's distribution must condition on all its parents.",
        "visual_anchor": "Node Y on bottom right with two incoming arrows from nodes C and A, labeled p(Y|C,A)",
        "text_evidence": "The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditi",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node C at top with p(C) label",
      "Node A on bottom left with p(A|C) label",
      "Node Y on bottom right with p(Y|C,A) label",
      "Arrow from C to A labeled g",
      "Arrow from C to Y",
      "Arrow from A to Y"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_6",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg",
    "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node E\", \"Node C\", \"Node X\", \"Node A\", \"Node Y\", \"Node B\", \"Node Z\", \"Node M\", \"directed edges\", \"path E→C→X→Y\", \"path A→C→Y\", \"path A→C→X→Y\", \"direct path A→Y\", \"indirect path A→M→Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does conditioning on node C open the path from E through C to Y?\",\n      \"answer\": \"Conditioning on C closes the paths A→C→X→Y and A→C→Y but opens the path A←E→C←X→Y, creating a new association through the collider at C.\",\n      \"visual_anchor\": \"Node C with incoming edges from E and A, and outgoing edge to Y\",\n      \"text_evidence\": \"conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\\\left. C \\\\right. Y$ but opens the path $A \\\\left. E \\\\right.$ $C \\\\left. X \\\\right. Y$\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the direct path A→Y differ from the indirect path through M in isolating causal contributions?\",\n      \"answer\": \"The direct path A→Y represents immediate causal effect, while the indirect path through M allows isolation of mediated effects by performing different interventions along different causal paths.\",\n      \"visual_anchor\": \"Direct edge from node A to node Y versus the two-edge path A→M→Y\",\n      \"text_evidence\": \"by performing different interventions on $A$ along different causal paths, it is possible to isolate the contribution of the causal effect of $A$ on $Y$ along a group of paths\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What formula emerges when applying the back-door criterion to nodes A and Y with mediator M?\",\n      \"answer\": \"The back-door adjustment formula is retrieved, which allows computation of causal effects by conditioning on appropriate variables that block confounding paths while preserving causal paths.\",\n      \"visual_anchor\": \"Nodes A and Y connected through both direct edge and path through node M\",\n      \"text_evidence\": \"i.e. we retrieve the back-door adjustment formula\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node E",
        "Node C",
        "Node X",
        "Node A",
        "Node Y",
        "Node B",
        "Node Z",
        "Node M",
        "directed edges",
        "path E→C→X→Y",
        "path A→C→Y",
        "path A→C→X→Y",
        "direct path A→Y",
        "indirect path A→M→Y"
      ],
      "queries": [
        {
          "query": "Why does conditioning on node C open the path from E through C to Y?",
          "answer": "Conditioning on C closes the paths A→C→X→Y and A→C→Y but opens the path A←E→C←X→Y, creating a new association through the collider at C.",
          "visual_anchor": "Node C with incoming edges from E and A, and outgoing edge to Y",
          "text_evidence": "conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the direct path A→Y differ from the indirect path through M in isolating causal contributions?",
          "answer": "The direct path A→Y represents immediate causal effect, while the indirect path through M allows isolation of mediated effects by performing different interventions along different causal paths.",
          "visual_anchor": "Direct edge from node A to node Y versus the two-edge path A→M→Y",
          "text_evidence": "by performing different interventions on $A$ along different causal paths, it is possible to isolate the contribution of the causal effect of $A$ on $Y$ along a group of paths",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What formula emerges when applying the back-door criterion to nodes A and Y with mediator M?",
          "answer": "The back-door adjustment formula is retrieved, which allows computation of causal effects by conditioning on appropriate variables that block confounding paths while preserving causal paths.",
          "visual_anchor": "Nodes A and Y connected through both direct edge and path through node M",
          "text_evidence": "i.e. we retrieve the back-door adjustment formula",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does conditioning on node C open the path from E through C to Y?",
        "answer": "Conditioning on C closes the paths A→C→X→Y and A→C→Y but opens the path A←E→C←X→Y, creating a new association through the collider at C.",
        "visual_anchor": "Node C with incoming edges from E and A, and outgoing edge to Y",
        "text_evidence": "conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the direct path A→Y differ from the indirect path through M in isolating causal contributions?",
        "answer": "The direct path A→Y represents immediate causal effect, while the indirect path through M allows isolation of mediated effects by performing different interventions along different causal paths.",
        "visual_anchor": "Direct edge from node A to node Y versus the two-edge path A→M→Y",
        "text_evidence": "by performing different interventions on $A$ along different causal paths, it is possible to isolate the contribution of the causal effect of $A$ on $Y$ along a group of paths",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What formula emerges when applying the back-door criterion to nodes A and Y with mediator M?",
        "answer": "The back-door adjustment formula is retrieved, which allows computation of causal effects by conditioning on appropriate variables that block confounding paths while preserving causal paths.",
        "visual_anchor": "Nodes A and Y connected through both direct edge and path through node M",
        "text_evidence": "i.e. we retrieve the back-door adjustment formula",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node E",
      "Node C",
      "Node X",
      "Node A",
      "Node Y",
      "Node B",
      "Node Z",
      "Node M",
      "directed edges",
      "path E→C→X→Y",
      "path A→C→Y",
      "path A→C→X→Y",
      "direct path A→Y",
      "indirect path A→M→Y"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_8",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg",
    "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Red highlighted paths from A to Y through M and L\", \"Black arrows with theta coefficients\", \"Node labels A, M, L, Y, C\", \"Direct path A to Y\", \"Top and bottom CBN diagrams\"],\n  \"queries\": [\n    {\n      \"query\": \"Why are the paths A→M, M→L, and L→Y highlighted in red in the top diagram?\",\n      \"answer\": \"These red paths represent the indirect causal paths from A to Y that pass through M, which are being isolated to estimate path-specific effects alongside the direct A→Y path.\",\n      \"visual_anchor\": \"Red highlighted edges connecting A→M→L→Y in the top diagram\",\n      \"text_evidence\": \"assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A  Y$ and the pat\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the bottom diagram's theta coefficient notation relate to computing effects along the red paths shown above?\",\n      \"answer\": \"The PSE is obtained by summing over the three causal paths of interest (A→Y, A→M→Y, and A→M→L→Y) the product of all theta coefficients in each path, as shown in the annotated bottom diagram.\",\n      \"visual_anchor\": \"Theta coefficients (θ^m_a, θ^l_m, θ^y_l, θ^y_m, θ^y_a) labeling arrows in bottom diagram\",\n      \"text_evidence\": \"The PSE is obtained by summing over the three causal paths of interest ( $A  Y$ , $A  M  Y$ , and $A \\\\to M \\\\to L \\\\to Y$ ) the product of all coefficients in each path.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does node C's position above M and L in both diagrams explain its absence from fairness reasoning?\",\n      \"answer\": \"Yes, the hiring decision Y is only based on E (educational background), not on religious belief A or participation patterns influenced by C, as shown by the back-door path structure.\",\n      \"visual_anchor\": \"Node C at the top with arrows pointing down to M and L in both diagrams\",\n      \"text_evidence\": \"Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\\n\\nonly based on $E$\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Red highlighted paths from A to Y through M and L",
        "Black arrows with theta coefficients",
        "Node labels A, M, L, Y, C",
        "Direct path A to Y",
        "Top and bottom CBN diagrams"
      ],
      "queries": [
        {
          "query": "Why are the paths A→M, M→L, and L→Y highlighted in red in the top diagram?",
          "answer": "These red paths represent the indirect causal paths from A to Y that pass through M, which are being isolated to estimate path-specific effects alongside the direct A→Y path.",
          "visual_anchor": "Red highlighted edges connecting A→M→L→Y in the top diagram",
          "text_evidence": "assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A  Y$ and the pat",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the bottom diagram's theta coefficient notation relate to computing effects along the red paths shown above?",
          "answer": "The PSE is obtained by summing over the three causal paths of interest (A→Y, A→M→Y, and A→M→L→Y) the product of all theta coefficients in each path, as shown in the annotated bottom diagram.",
          "visual_anchor": "Theta coefficients (θ^m_a, θ^l_m, θ^y_l, θ^y_m, θ^y_a) labeling arrows in bottom diagram",
          "text_evidence": "The PSE is obtained by summing over the three causal paths of interest ( $A  Y$ , $A  M  Y$ , and $A \\to M \\to L \\to Y$ ) the product of all coefficients in each path.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does node C's position above M and L in both diagrams explain its absence from fairness reasoning?",
          "answer": "Yes, the hiring decision Y is only based on E (educational background), not on religious belief A or participation patterns influenced by C, as shown by the back-door path structure.",
          "visual_anchor": "Node C at the top with arrows pointing down to M and L in both diagrams",
          "text_evidence": "Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\n\nonly based on $E$",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why are the paths A→M, M→L, and L→Y highlighted in red in the top diagram?",
        "answer": "These red paths represent the indirect causal paths from A to Y that pass through M, which are being isolated to estimate path-specific effects alongside the direct A→Y path.",
        "visual_anchor": "Red highlighted edges connecting A→M→L→Y in the top diagram",
        "text_evidence": "assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A  Y$ and the pat",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the bottom diagram's theta coefficient notation relate to computing effects along the red paths shown above?",
        "answer": "The PSE is obtained by summing over the three causal paths of interest (A→Y, A→M→Y, and A→M→L→Y) the product of all theta coefficients in each path, as shown in the annotated bottom diagram.",
        "visual_anchor": "Theta coefficients (θ^m_a, θ^l_m, θ^y_l, θ^y_m, θ^y_a) labeling arrows in bottom diagram",
        "text_evidence": "The PSE is obtained by summing over the three causal paths of interest ( $A  Y$ , $A  M  Y$ , and $A \\to M \\to L \\to Y$ ) the product of all coefficients in each path.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does node C's position above M and L in both diagrams explain its absence from fairness reasoning?",
        "answer": "Yes, the hiring decision Y is only based on E (educational background), not on religious belief A or participation patterns influenced by C, as shown by the back-door path structure.",
        "visual_anchor": "Node C at the top with arrows pointing down to M and L in both diagrams",
        "text_evidence": "Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\n\nonly based on $E$",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Red highlighted paths from A to Y through M and L",
      "Black arrows with theta coefficients",
      "Node labels A, M, L, Y, C",
      "Direct path A to Y",
      "Top and bottom CBN diagrams"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_10",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg",
    "caption": "Fig. 6. CBN underlying a music degree scenario.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A with red arrow labeled 'a' to X\", \"Node M with black arrow labeled 'β' to X\", \"Node M with black arrow labeled 'λ' to Y\", \"Node X\", \"Node Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the red arrow labeled 'a' from A to X create an open back-door path making Y not independent of A?\",\n      \"answer\": \"The path A→X influences Y through the educational background pathway, and since there is no conditioning, this back-door path remains open. This prevents independence between the hiring decision Y and religious belief A.\",\n      \"visual_anchor\": \"red arrow labeled 'a' from node A to node X\",\n      \"text_evidence\": \"Whilst Y ✚⊥⊥A due to the open back-door path from A to Y, the hiring decision Y is only based on E\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the arrow from M to X labeled 'β' enable retrieval of M when A is used alongside X?\",\n      \"answer\": \"Yes, because M is d-separated from A given X (M ⊥⊥ A | X), using A in addition to X can provide information about the music degree M through the causal structure.\",\n      \"visual_anchor\": \"black arrow labeled 'β' from node M to node X\",\n      \"text_evidence\": \"Indeed, since M { \\\\mathrel { \\\\mathop { A C A } } } | X, using A in addition to X can give information about M\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the λ-labeled arrow from M to Y support fairness when the predictor is Ŷ = γM?\",\n      \"answer\": \"Since the predictor depends only on M (music degree) through the λ pathway, and not directly on the sensitive attribute A, this ensures fairness. The explicit use of A helps retrieve M without creating direct discrimination.\",\n      \"visual_anchor\": \"black arrow labeled 'λ' from node M to node Y\",\n      \"text_evidence\": \"this example shows how explicit use of the sensitive attribute in a model can ensure\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A with red arrow labeled 'a' to X",
        "Node M with black arrow labeled 'β' to X",
        "Node M with black arrow labeled 'λ' to Y",
        "Node X",
        "Node Y"
      ],
      "queries": [
        {
          "query": "Why does the red arrow labeled 'a' from A to X create an open back-door path making Y not independent of A?",
          "answer": "The path A→X influences Y through the educational background pathway, and since there is no conditioning, this back-door path remains open. This prevents independence between the hiring decision Y and religious belief A.",
          "visual_anchor": "red arrow labeled 'a' from node A to node X",
          "text_evidence": "Whilst Y ✚⊥⊥A due to the open back-door path from A to Y, the hiring decision Y is only based on E",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the arrow from M to X labeled 'β' enable retrieval of M when A is used alongside X?",
          "answer": "Yes, because M is d-separated from A given X (M ⊥⊥ A | X), using A in addition to X can provide information about the music degree M through the causal structure.",
          "visual_anchor": "black arrow labeled 'β' from node M to node X",
          "text_evidence": "Indeed, since M { \\mathrel { \\mathop { A C A } } } | X, using A in addition to X can give information about M",
          "query_type": "value_context"
        },
        {
          "query": "How does the λ-labeled arrow from M to Y support fairness when the predictor is Ŷ = γM?",
          "answer": "Since the predictor depends only on M (music degree) through the λ pathway, and not directly on the sensitive attribute A, this ensures fairness. The explicit use of A helps retrieve M without creating direct discrimination.",
          "visual_anchor": "black arrow labeled 'λ' from node M to node Y",
          "text_evidence": "this example shows how explicit use of the sensitive attribute in a model can ensure",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the red arrow labeled 'a' from A to X create an open back-door path making Y not independent of A?",
        "answer": "The path A→X influences Y through the educational background pathway, and since there is no conditioning, this back-door path remains open. This prevents independence between the hiring decision Y and religious belief A.",
        "visual_anchor": "red arrow labeled 'a' from node A to node X",
        "text_evidence": "Whilst Y ✚⊥⊥A due to the open back-door path from A to Y, the hiring decision Y is only based on E",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the arrow from M to X labeled 'β' enable retrieval of M when A is used alongside X?",
        "answer": "Yes, because M is d-separated from A given X (M ⊥⊥ A | X), using A in addition to X can provide information about the music degree M through the causal structure.",
        "visual_anchor": "black arrow labeled 'β' from node M to node X",
        "text_evidence": "Indeed, since M { \\mathrel { \\mathop { A C A } } } | X, using A in addition to X can give information about M",
        "query_type": "value_context"
      },
      {
        "query": "How does the λ-labeled arrow from M to Y support fairness when the predictor is Ŷ = γM?",
        "answer": "Since the predictor depends only on M (music degree) through the λ pathway, and not directly on the sensitive attribute A, this ensures fairness. The explicit use of A helps retrieve M without creating direct discrimination.",
        "visual_anchor": "black arrow labeled 'λ' from node M to node Y",
        "text_evidence": "this example shows how explicit use of the sensitive attribute in a model can ensure",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A with red arrow labeled 'a' to X",
      "Node M with black arrow labeled 'β' to X",
      "Node M with black arrow labeled 'λ' to Y",
      "Node X",
      "Node Y"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_11",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
    "caption": "Fig. 7. CBN underlying a college admission scenario.",
    "original_quality_score": 0.75,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Node A with red solid arrow to Y\", \"Node D with dashed arrow to Y\", \"Node Q with black solid arrow to Y\", \"Node D with black solid arrow to Y\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the red solid arrow from A to Y represent the direct path whose unfairness is quantified by path-specific effect?\",\n      \"answer\": \"Yes, the red solid arrow from A to Y represents the direct path A → Y, whose unfairness is quantified by the path-specific effect PSE^aa when the path A → D → Y is considered fair.\",\n      \"visual_anchor\": \"red solid arrow from node A to node Y\",\n      \"text_evidence\": \"in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does the dashed arrow from D to Y matter for retrieving M when A is explicitly used?\",\n      \"answer\": \"The dashed arrow from D to Y indicates D influences Y, and since M is conditionally independent of A given X, using A in addition to X can help retrieve M information that flows through D to Y.\",\n      \"visual_anchor\": \"dashed arrow from node D to node Y\",\n      \"text_evidence\": \"Indeed, since $M { \\\\mathrel { \\\\mathop { A C A } } } | X$ , using $A$ in addition to $X$ can give information about $M$\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the black arrow from Q to Y relate to the twin network containing counterfactual variable Q*?\",\n      \"answer\": \"The black arrow from Q to Y in the original CBN corresponds to the relationship maintained in the twin Bayesian network, where Q* represents the counterfactual world with intervention A = ā along A → Y.\",\n      \"visual_anchor\": \"black solid arrow from node Q to node Y\",\n      \"text_evidence\": \"the network contains the variables $Q ^ { * }$ , $D _ { a }$ and $Y _ { \\\\bar { a } } ( D _ { a } )$ corresponding to the counterfactual world in which $A = \\\\bar { a }$ along $A  Y$ , with $Q ^ { * } = \\\\t\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Node A with red solid arrow to Y",
        "Node D with dashed arrow to Y",
        "Node Q with black solid arrow to Y",
        "Node D with black solid arrow to Y"
      ],
      "queries": [
        {
          "query": "Does the red solid arrow from A to Y represent the direct path whose unfairness is quantified by path-specific effect?",
          "answer": "Yes, the red solid arrow from A to Y represents the direct path A → Y, whose unfairness is quantified by the path-specific effect PSE^aa when the path A → D → Y is considered fair.",
          "visual_anchor": "red solid arrow from node A to node Y",
          "text_evidence": "in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does the dashed arrow from D to Y matter for retrieving M when A is explicitly used?",
          "answer": "The dashed arrow from D to Y indicates D influences Y, and since M is conditionally independent of A given X, using A in addition to X can help retrieve M information that flows through D to Y.",
          "visual_anchor": "dashed arrow from node D to node Y",
          "text_evidence": "Indeed, since $M { \\mathrel { \\mathop { A C A } } } | X$ , using $A$ in addition to $X$ can give information about $M$",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the black arrow from Q to Y relate to the twin network containing counterfactual variable Q*?",
          "answer": "The black arrow from Q to Y in the original CBN corresponds to the relationship maintained in the twin Bayesian network, where Q* represents the counterfactual world with intervention A = ā along A → Y.",
          "visual_anchor": "black solid arrow from node Q to node Y",
          "text_evidence": "the network contains the variables $Q ^ { * }$ , $D _ { a }$ and $Y _ { \\bar { a } } ( D _ { a } )$ corresponding to the counterfactual world in which $A = \\bar { a }$ along $A  Y$ , with $Q ^ { * } = \\t",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the red solid arrow from A to Y represent the direct path whose unfairness is quantified by path-specific effect?",
        "answer": "Yes, the red solid arrow from A to Y represents the direct path A → Y, whose unfairness is quantified by the path-specific effect PSE^aa when the path A → D → Y is considered fair.",
        "visual_anchor": "red solid arrow from node A to node Y",
        "text_evidence": "in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does the dashed arrow from D to Y matter for retrieving M when A is explicitly used?",
        "answer": "The dashed arrow from D to Y indicates D influences Y, and since M is conditionally independent of A given X, using A in addition to X can help retrieve M information that flows through D to Y.",
        "visual_anchor": "dashed arrow from node D to node Y",
        "text_evidence": "Indeed, since $M { \\mathrel { \\mathop { A C A } } } | X$ , using $A$ in addition to $X$ can give information about $M$",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the black arrow from Q to Y relate to the twin network containing counterfactual variable Q*?",
        "answer": "The black arrow from Q to Y in the original CBN corresponds to the relationship maintained in the twin Bayesian network, where Q* represents the counterfactual world with intervention A = ā along A → Y.",
        "visual_anchor": "black solid arrow from node Q to node Y",
        "text_evidence": "the network contains the variables $Q ^ { * }$ , $D _ { a }$ and $Y _ { \\bar { a } } ( D _ { a } )$ corresponding to the counterfactual world in which $A = \\bar { a }$ along $A  Y$ , with $Q ^ { * } = \\t",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Node A with red solid arrow to Y",
      "Node D with dashed arrow to Y",
      "Node Q with black solid arrow to Y",
      "Node D with black solid arrow to Y"
    ]
  },
  {
    "doc_id": "1907.06430",
    "figure_id": "1907.06430_fig_13",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg",
    "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"Four nodes labeled X₁, X₃, X₂, X₄\", \"Directed arrows connecting nodes\", \"X₁ → X₃ → X₂ → X₄\", \"X₃ → X₄ direct connection\", \"Cyclic structure with X₂ → X₄ and X₃ → X₄\"],\n  \"queries\": [\n    {\n      \"query\": \"Does the directed path from X₁ through X₃ to X₄ illustrate what the definition describes as linked nodes?\",\n      \"answer\": \"Yes, the sequence X₁ → X₃ → X₄ represents a directed path, which is defined as a sequence of linked nodes starting at one node and ending at another.\",\n      \"visual_anchor\": \"directed arrows forming path X₁ → X₃ → X₄\",\n      \"text_evidence\": \"A path from node $X _ { i }$ to node $X _ { j }$ is a sequence of linked nodes starting at $X _ { i }$ and ending at $X _ { j }$ . A directed path is\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does the structure with X₃ connecting to both X₂ and X₄ create a cyclic pattern?\",\n      \"answer\": \"The structure creates a cycle because X₃ → X₂ → X₄ forms one path, while X₃ → X₄ forms another, and these paths loop back through the shared connections, creating a directed cycle.\",\n      \"visual_anchor\": \"node X₃ with arrows to both X₂ and X₄\",\n      \"text_evidence\": \"A graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the directed links from X₁ to X₃ and X₃ to X₂ relate to acknowledgments of discussions?\",\n      \"answer\": \"The directed links represent dependency relationships in Bayesian Networks, which connects to the collaborative discussions mentioned, as such networks model how variables influence each other through directed connections.\",\n      \"visual_anchor\": \"directed arrows X₁ → X₃ → X₂\",\n      \"text_evidence\": \"The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Stepleton, Tom Everitt, and Shira Mitchell for useful discussions.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Four nodes labeled X₁, X₃, X₂, X₄",
        "Directed arrows connecting nodes",
        "X₁ → X₃ → X₂ → X₄",
        "X₃ → X₄ direct connection",
        "Cyclic structure with X₂ → X₄ and X₃ → X₄"
      ],
      "queries": [
        {
          "query": "Does the directed path from X₁ through X₃ to X₄ illustrate what the definition describes as linked nodes?",
          "answer": "Yes, the sequence X₁ → X₃ → X₄ represents a directed path, which is defined as a sequence of linked nodes starting at one node and ending at another.",
          "visual_anchor": "directed arrows forming path X₁ → X₃ → X₄",
          "text_evidence": "A path from node $X _ { i }$ to node $X _ { j }$ is a sequence of linked nodes starting at $X _ { i }$ and ending at $X _ { j }$ . A directed path is",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does the structure with X₃ connecting to both X₂ and X₄ create a cyclic pattern?",
          "answer": "The structure creates a cycle because X₃ → X₂ → X₄ forms one path, while X₃ → X₄ forms another, and these paths loop back through the shared connections, creating a directed cycle.",
          "visual_anchor": "node X₃ with arrows to both X₂ and X₄",
          "text_evidence": "A graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the directed links from X₁ to X₃ and X₃ to X₂ relate to acknowledgments of discussions?",
          "answer": "The directed links represent dependency relationships in Bayesian Networks, which connects to the collaborative discussions mentioned, as such networks model how variables influence each other through directed connections.",
          "visual_anchor": "directed arrows X₁ → X₃ → X₂",
          "text_evidence": "The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Stepleton, Tom Everitt, and Shira Mitchell for useful discussions.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the directed path from X₁ through X₃ to X₄ illustrate what the definition describes as linked nodes?",
        "answer": "Yes, the sequence X₁ → X₃ → X₄ represents a directed path, which is defined as a sequence of linked nodes starting at one node and ending at another.",
        "visual_anchor": "directed arrows forming path X₁ → X₃ → X₄",
        "text_evidence": "A path from node $X _ { i }$ to node $X _ { j }$ is a sequence of linked nodes starting at $X _ { i }$ and ending at $X _ { j }$ . A directed path is",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does the structure with X₃ connecting to both X₂ and X₄ create a cyclic pattern?",
        "answer": "The structure creates a cycle because X₃ → X₂ → X₄ forms one path, while X₃ → X₄ forms another, and these paths loop back through the shared connections, creating a directed cycle.",
        "visual_anchor": "node X₃ with arrows to both X₂ and X₄",
        "text_evidence": "A graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the directed links from X₁ to X₃ and X₃ to X₂ relate to acknowledgments of discussions?",
        "answer": "The directed links represent dependency relationships in Bayesian Networks, which connects to the collaborative discussions mentioned, as such networks model how variables influence each other through directed connections.",
        "visual_anchor": "directed arrows X₁ → X₃ → X₂",
        "text_evidence": "The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Stepleton, Tom Everitt, and Shira Mitchell for useful discussions.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Four nodes labeled X₁, X₃, X₂, X₄",
      "Directed arrows connecting nodes",
      "X₁ → X₃ → X₂ → X₄",
      "X₃ → X₄ direct connection",
      "Cyclic structure with X₂ → X₄ and X₃ → X₄"
    ]
  },
  {
    "doc_id": "1907.09013",
    "figure_id": "1907.09013_fig_1",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg",
    "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Blue rounded rectangles for Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment\",\n    \"Gray cylindrical database icon labeled 'Data' in center\",\n    \"Gray directional arrows connecting process stages in circular flow\",\n    \"Bidirectional arrows between Business Understanding and Data Understanding\",\n    \"Outer circular arrow indicating iterative process\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do bidirectional gray arrows connect Business Understanding and Data Understanding at the top of the cycle?\",\n      \"answer\": \"These arrows represent the iterative relationship between understanding business requirements and data characteristics, which is fundamental to identifying discrimination causes during the initial CRISP-DM phases.\",\n      \"visual_anchor\": \"Bidirectional gray arrows between the two blue boxes at top (Business Understanding and Data Understanding)\",\n      \"text_evidence\": \"For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the central gray database icon relate to discrimination emerging throughout the blue-boxed process stages?\",\n      \"answer\": \"The central database represents the data that flows through all CRISP-DM stages, showing that unintended discrimination can emerge at any point in the data mining process where data issues or procedural failures occur.\",\n      \"visual_anchor\": \"Gray cylindrical database icon labeled 'Data' positioned in the center of the circular process\",\n      \"text_evidence\": \"A classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What discrimination source does the Deployment stage on the left represent beyond the building phases?\",\n      \"answer\": \"The Deployment stage represents procedural failures that occur when using the classifier, a distinct category from data issues and misspecification encountered during the building process.\",\n      \"visual_anchor\": \"Blue Deployment box positioned on the left side of the circular flow\",\n      \"text_evidence\": \"When we consider how the classifier is used, we include in our taxonomy \\\"procedural failures\\\"\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Blue rounded rectangles for Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment",
        "Gray cylindrical database icon labeled 'Data' in center",
        "Gray directional arrows connecting process stages in circular flow",
        "Bidirectional arrows between Business Understanding and Data Understanding",
        "Outer circular arrow indicating iterative process"
      ],
      "queries": [
        {
          "query": "Why do bidirectional gray arrows connect Business Understanding and Data Understanding at the top of the cycle?",
          "answer": "These arrows represent the iterative relationship between understanding business requirements and data characteristics, which is fundamental to identifying discrimination causes during the initial CRISP-DM phases.",
          "visual_anchor": "Bidirectional gray arrows between the two blue boxes at top (Business Understanding and Data Understanding)",
          "text_evidence": "For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the central gray database icon relate to discrimination emerging throughout the blue-boxed process stages?",
          "answer": "The central database represents the data that flows through all CRISP-DM stages, showing that unintended discrimination can emerge at any point in the data mining process where data issues or procedural failures occur.",
          "visual_anchor": "Gray cylindrical database icon labeled 'Data' positioned in the center of the circular process",
          "text_evidence": "A classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What discrimination source does the Deployment stage on the left represent beyond the building phases?",
          "answer": "The Deployment stage represents procedural failures that occur when using the classifier, a distinct category from data issues and misspecification encountered during the building process.",
          "visual_anchor": "Blue Deployment box positioned on the left side of the circular flow",
          "text_evidence": "When we consider how the classifier is used, we include in our taxonomy \"procedural failures\"",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do bidirectional gray arrows connect Business Understanding and Data Understanding at the top of the cycle?",
        "answer": "These arrows represent the iterative relationship between understanding business requirements and data characteristics, which is fundamental to identifying discrimination causes during the initial CRISP-DM phases.",
        "visual_anchor": "Bidirectional gray arrows between the two blue boxes at top (Business Understanding and Data Understanding)",
        "text_evidence": "For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the central gray database icon relate to discrimination emerging throughout the blue-boxed process stages?",
        "answer": "The central database represents the data that flows through all CRISP-DM stages, showing that unintended discrimination can emerge at any point in the data mining process where data issues or procedural failures occur.",
        "visual_anchor": "Gray cylindrical database icon labeled 'Data' positioned in the center of the circular process",
        "text_evidence": "A classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What discrimination source does the Deployment stage on the left represent beyond the building phases?",
        "answer": "The Deployment stage represents procedural failures that occur when using the classifier, a distinct category from data issues and misspecification encountered during the building process.",
        "visual_anchor": "Blue Deployment box positioned on the left side of the circular flow",
        "text_evidence": "When we consider how the classifier is used, we include in our taxonomy \"procedural failures\"",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Blue rounded rectangles for Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment",
      "Gray cylindrical database icon labeled 'Data' in center",
      "Gray directional arrows connecting process stages in circular flow",
      "Bidirectional arrows between Business Understanding and Data Understanding",
      "Outer circular arrow indicating iterative process"
    ]
  },
  {
    "doc_id": "1907.09013",
    "figure_id": "1907.09013_fig_2",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg",
    "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Diamond-shaped 'Discrim Potential?' decision nodes\",\n    \"Dashed boxes for 'Discrim Aware Pre-processing', 'Discrim Aware Training', and 'Discrim Reducing Post-Processing'\",\n    \"Standard Training box with solid border\",\n    \"Deploy oval in gray\",\n    \"Reprocess and/or Retrain oval\",\n    \"Training Data and Testing Data cylinders\",\n    \"Classifier Unit Tests Accuracy/Discrimination box\",\n    \"Flow arrows showing Yes/No paths from decision diamonds\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the first diamond decision node enable optional dashed-box pre-processing before reaching Training Data?\",\n      \"answer\": \"The decision node checks for discrimination potential in raw data. If discrimination is detected (Yes path), it routes to discrimination-aware pre-processing to mitigate bias before training begins.\",\n      \"visual_anchor\": \"First diamond-shaped 'Discrim Potential?' node with Yes arrow pointing to dashed 'Discrim Aware Pre-processing' box\",\n      \"text_evidence\": \"The greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior. This is an error that starts with the data scientist but should extend upwards to the executive team.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the dashed boxes for training relate to augmented cost functions that penalize classification differences?\",\n      \"answer\": \"The dashed 'Discrim Aware Training' box represents in-processing techniques like augmented cost functions. These add fairness regularizers that penalize differences between protected and non-protected class classifications during training.\",\n      \"visual_anchor\": \"Dashed 'Discrim Aware Training' box positioned between Training Data cylinder and Classifier\",\n      \"text_evidence\": \"Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a 'fairness' regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"What triggers the Yes path from the second diamond to the dashed post-processing box?\",\n      \"answer\": \"When Classifier Unit Tests detect discrimination potential, the Yes path activates discrimination-reducing post-processing. This modifies classifier outputs to reduce discriminatory behavior before potential reprocessing or deployment.\",\n      \"visual_anchor\": \"Second 'Discrim Potential?' diamond with Yes arrow leading to dashed 'Discrim Reducing Post-Processing' box\",\n      \"text_evidence\": \"Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Diamond-shaped 'Discrim Potential?' decision nodes",
        "Dashed boxes for 'Discrim Aware Pre-processing', 'Discrim Aware Training', and 'Discrim Reducing Post-Processing'",
        "Standard Training box with solid border",
        "Deploy oval in gray",
        "Reprocess and/or Retrain oval",
        "Training Data and Testing Data cylinders",
        "Classifier Unit Tests Accuracy/Discrimination box",
        "Flow arrows showing Yes/No paths from decision diamonds"
      ],
      "queries": [
        {
          "query": "Why does the first diamond decision node enable optional dashed-box pre-processing before reaching Training Data?",
          "answer": "The decision node checks for discrimination potential in raw data. If discrimination is detected (Yes path), it routes to discrimination-aware pre-processing to mitigate bias before training begins.",
          "visual_anchor": "First diamond-shaped 'Discrim Potential?' node with Yes arrow pointing to dashed 'Discrim Aware Pre-processing' box",
          "text_evidence": "The greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior. This is an error that starts with the data scientist but should extend upwards to the executive team.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the dashed boxes for training relate to augmented cost functions that penalize classification differences?",
          "answer": "The dashed 'Discrim Aware Training' box represents in-processing techniques like augmented cost functions. These add fairness regularizers that penalize differences between protected and non-protected class classifications during training.",
          "visual_anchor": "Dashed 'Discrim Aware Training' box positioned between Training Data cylinder and Classifier",
          "text_evidence": "Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a 'fairness' regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes",
          "query_type": "visual_definition"
        },
        {
          "query": "What triggers the Yes path from the second diamond to the dashed post-processing box?",
          "answer": "When Classifier Unit Tests detect discrimination potential, the Yes path activates discrimination-reducing post-processing. This modifies classifier outputs to reduce discriminatory behavior before potential reprocessing or deployment.",
          "visual_anchor": "Second 'Discrim Potential?' diamond with Yes arrow leading to dashed 'Discrim Reducing Post-Processing' box",
          "text_evidence": "Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the first diamond decision node enable optional dashed-box pre-processing before reaching Training Data?",
        "answer": "The decision node checks for discrimination potential in raw data. If discrimination is detected (Yes path), it routes to discrimination-aware pre-processing to mitigate bias before training begins.",
        "visual_anchor": "First diamond-shaped 'Discrim Potential?' node with Yes arrow pointing to dashed 'Discrim Aware Pre-processing' box",
        "text_evidence": "The greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior. This is an error that starts with the data scientist but should extend upwards to the executive team.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the dashed boxes for training relate to augmented cost functions that penalize classification differences?",
        "answer": "The dashed 'Discrim Aware Training' box represents in-processing techniques like augmented cost functions. These add fairness regularizers that penalize differences between protected and non-protected class classifications during training.",
        "visual_anchor": "Dashed 'Discrim Aware Training' box positioned between Training Data cylinder and Classifier",
        "text_evidence": "Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a 'fairness' regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes",
        "query_type": "visual_definition"
      },
      {
        "query": "What triggers the Yes path from the second diamond to the dashed post-processing box?",
        "answer": "When Classifier Unit Tests detect discrimination potential, the Yes path activates discrimination-reducing post-processing. This modifies classifier outputs to reduce discriminatory behavior before potential reprocessing or deployment.",
        "visual_anchor": "Second 'Discrim Potential?' diamond with Yes arrow leading to dashed 'Discrim Reducing Post-Processing' box",
        "text_evidence": "Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Diamond-shaped 'Discrim Potential?' decision nodes",
      "Dashed boxes for 'Discrim Aware Pre-processing', 'Discrim Aware Training', and 'Discrim Reducing Post-Processing'",
      "Standard Training box with solid border",
      "Deploy oval in gray",
      "Reprocess and/or Retrain oval",
      "Training Data and Testing Data cylinders",
      "Classifier Unit Tests Accuracy/Discrimination box",
      "Flow arrows showing Yes/No paths from decision diamonds"
    ]
  },
  {
    "doc_id": "1907.12059",
    "figure_id": "1907.12059_fig_8",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg",
    "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Top histogram showing dispersed distributions across 0.0 to 1.0 range\",\n    \"Bottom histogram showing concentrated distributions near 0.0\",\n    \"Red vertical bars clustered at left edge in bottom plot\",\n    \"Grid with dotted lines at regular intervals\",\n    \"Y-axis scale from 0.0 to 1.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the bottom histograms cluster near 0.0 after training with beta equals 100?\",\n      \"answer\": \"After 10,000 training steps with α=0 and β=100, each group histogram matches the Wasserstein-1 barycenter, which concentrates model beliefs near 0.0. This demonstrates that the Wasserstein-1 penalty forces all demographic groups to align their prediction distributions.\",\n      \"visual_anchor\": \"Red vertical bars concentrated at left edge (near 0.0) in bottom histogram\",\n      \"text_evidence\": \"After 10,000 training steps with $\\\\alpha = 0 , \\\\beta = 1 0 0$ each group histogram matches the barycenter\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the concentration near 0.0 in the bottom plot support minimal modification of model decisions?\",\n      \"answer\": \"Yes, the barycenter concentration demonstrates that the Wasserstein-1 approach achieves independence by aligning all group distributions to a common target. This enables reaching independence with minimal modifications as stated.\",\n      \"visual_anchor\": \"Bottom histogram with all groups aligned near 0.0 value\",\n      \"text_evidence\": \"using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the top histogram's spread across 0.2 to 1.0 relate to sensitive information dependence?\",\n      \"answer\": \"The dispersed distributions in the top histogram show that model outputs initially vary across demographic groups (Black females, Black males, White females, White males), indicating dependence on sensitive attributes. The Wasserstein-1 distance approach eliminates this dependence.\",\n      \"visual_anchor\": \"Top histogram showing distributions spread across 0.2 to 1.0 range\",\n      \"text_evidence\": \"ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Top histogram showing dispersed distributions across 0.0 to 1.0 range",
        "Bottom histogram showing concentrated distributions near 0.0",
        "Red vertical bars clustered at left edge in bottom plot",
        "Grid with dotted lines at regular intervals",
        "Y-axis scale from 0.0 to 1.0"
      ],
      "queries": [
        {
          "query": "Why do the bottom histograms cluster near 0.0 after training with beta equals 100?",
          "answer": "After 10,000 training steps with α=0 and β=100, each group histogram matches the Wasserstein-1 barycenter, which concentrates model beliefs near 0.0. This demonstrates that the Wasserstein-1 penalty forces all demographic groups to align their prediction distributions.",
          "visual_anchor": "Red vertical bars concentrated at left edge (near 0.0) in bottom histogram",
          "text_evidence": "After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the concentration near 0.0 in the bottom plot support minimal modification of model decisions?",
          "answer": "Yes, the barycenter concentration demonstrates that the Wasserstein-1 approach achieves independence by aligning all group distributions to a common target. This enables reaching independence with minimal modifications as stated.",
          "visual_anchor": "Bottom histogram with all groups aligned near 0.0 value",
          "text_evidence": "using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions",
          "query_type": "value_context"
        },
        {
          "query": "How does the top histogram's spread across 0.2 to 1.0 relate to sensitive information dependence?",
          "answer": "The dispersed distributions in the top histogram show that model outputs initially vary across demographic groups (Black females, Black males, White females, White males), indicating dependence on sensitive attributes. The Wasserstein-1 distance approach eliminates this dependence.",
          "visual_anchor": "Top histogram showing distributions spread across 0.2 to 1.0 range",
          "text_evidence": "ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the bottom histograms cluster near 0.0 after training with beta equals 100?",
        "answer": "After 10,000 training steps with α=0 and β=100, each group histogram matches the Wasserstein-1 barycenter, which concentrates model beliefs near 0.0. This demonstrates that the Wasserstein-1 penalty forces all demographic groups to align their prediction distributions.",
        "visual_anchor": "Red vertical bars concentrated at left edge (near 0.0) in bottom histogram",
        "text_evidence": "After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the concentration near 0.0 in the bottom plot support minimal modification of model decisions?",
        "answer": "Yes, the barycenter concentration demonstrates that the Wasserstein-1 approach achieves independence by aligning all group distributions to a common target. This enables reaching independence with minimal modifications as stated.",
        "visual_anchor": "Bottom histogram with all groups aligned near 0.0 value",
        "text_evidence": "using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions",
        "query_type": "value_context"
      },
      {
        "query": "How does the top histogram's spread across 0.2 to 1.0 relate to sensitive information dependence?",
        "answer": "The dispersed distributions in the top histogram show that model outputs initially vary across demographic groups (Black females, Black males, White females, White males), indicating dependence on sensitive attributes. The Wasserstein-1 distance approach eliminates this dependence.",
        "visual_anchor": "Top histogram showing distributions spread across 0.2 to 1.0 range",
        "text_evidence": "ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Top histogram showing dispersed distributions across 0.0 to 1.0 range",
      "Bottom histogram showing concentrated distributions near 0.0",
      "Red vertical bars clustered at left edge in bottom plot",
      "Grid with dotted lines at regular intervals",
      "Y-axis scale from 0.0 to 1.0"
    ]
  },
  {
    "doc_id": "1907.12059",
    "figure_id": "1907.12059_fig_11",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg",
    "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\"two curved functions f and g\", \"shaded region between curves\", \"horizontal hatching lines\", \"vertical dashed lines at x=1 and y=1\", \"axes labeled 0 and 1\", \"labels f and g on respective curves\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the horizontal hatching lines in the shaded region demonstrate the equality in Eq. (12)?\",\n      \"answer\": \"The horizontal hatching represents integrating |f⁻¹ - g⁻¹| along the x-axis (left interpretation) and integrating |f - g| along the y-axis (right interpretation), both computing the same shaded area and thus proving the equality.\",\n      \"visual_anchor\": \"horizontal hatching lines filling the shaded region between curves f and g\",\n      \"text_evidence\": \"Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12)\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the shaded region between curves f and g support the claim about two ways of computing?\",\n      \"answer\": \"The shaded region visually demonstrates that integrating the difference of inverse CDFs horizontally yields the same area as integrating the difference of CDFs vertically, providing geometric proof of the analytical equality.\",\n      \"visual_anchor\": \"shaded region bounded by curves f and g from origin to point (1,1)\",\n      \"text_evidence\": \"Intuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why must curves f and g both reach the point at coordinates (1,1) given they represent invertible CDFs?\",\n      \"answer\": \"Invertible CDFs are strictly increasing bijective functions mapping [0,1] to [0,1], so they must start at (0,0) and end at (1,1) to satisfy the probability space constraint Ω = [0,1].\",\n      \"visual_anchor\": \"convergence point of curves f and g at coordinates (1,1) with vertical dashed line\",\n      \"text_evidence\": \"Given two differentiable and invertible cumulative distribution functions $f , g$ over the probability space $\\\\Omega = [ 0 , 1 ]$ , thus $f , g : [ 0 , 1 ] \\\\to [ 0 , 1 ]$\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "two curved functions f and g",
        "shaded region between curves",
        "horizontal hatching lines",
        "vertical dashed lines at x=1 and y=1",
        "axes labeled 0 and 1",
        "labels f and g on respective curves"
      ],
      "queries": [
        {
          "query": "Why do the horizontal hatching lines in the shaded region demonstrate the equality in Eq. (12)?",
          "answer": "The horizontal hatching represents integrating |f⁻¹ - g⁻¹| along the x-axis (left interpretation) and integrating |f - g| along the y-axis (right interpretation), both computing the same shaded area and thus proving the equality.",
          "visual_anchor": "horizontal hatching lines filling the shaded region between curves f and g",
          "text_evidence": "Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12)",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the shaded region between curves f and g support the claim about two ways of computing?",
          "answer": "The shaded region visually demonstrates that integrating the difference of inverse CDFs horizontally yields the same area as integrating the difference of CDFs vertically, providing geometric proof of the analytical equality.",
          "visual_anchor": "shaded region bounded by curves f and g from origin to point (1,1)",
          "text_evidence": "Intuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why must curves f and g both reach the point at coordinates (1,1) given they represent invertible CDFs?",
          "answer": "Invertible CDFs are strictly increasing bijective functions mapping [0,1] to [0,1], so they must start at (0,0) and end at (1,1) to satisfy the probability space constraint Ω = [0,1].",
          "visual_anchor": "convergence point of curves f and g at coordinates (1,1) with vertical dashed line",
          "text_evidence": "Given two differentiable and invertible cumulative distribution functions $f , g$ over the probability space $\\Omega = [ 0 , 1 ]$ , thus $f , g : [ 0 , 1 ] \\to [ 0 , 1 ]$",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the horizontal hatching lines in the shaded region demonstrate the equality in Eq. (12)?",
        "answer": "The horizontal hatching represents integrating |f⁻¹ - g⁻¹| along the x-axis (left interpretation) and integrating |f - g| along the y-axis (right interpretation), both computing the same shaded area and thus proving the equality.",
        "visual_anchor": "horizontal hatching lines filling the shaded region between curves f and g",
        "text_evidence": "Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12)",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the shaded region between curves f and g support the claim about two ways of computing?",
        "answer": "The shaded region visually demonstrates that integrating the difference of inverse CDFs horizontally yields the same area as integrating the difference of CDFs vertically, providing geometric proof of the analytical equality.",
        "visual_anchor": "shaded region bounded by curves f and g from origin to point (1,1)",
        "text_evidence": "Intuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why must curves f and g both reach the point at coordinates (1,1) given they represent invertible CDFs?",
        "answer": "Invertible CDFs are strictly increasing bijective functions mapping [0,1] to [0,1], so they must start at (0,0) and end at (1,1) to satisfy the probability space constraint Ω = [0,1].",
        "visual_anchor": "convergence point of curves f and g at coordinates (1,1) with vertical dashed line",
        "text_evidence": "Given two differentiable and invertible cumulative distribution functions $f , g$ over the probability space $\\Omega = [ 0 , 1 ]$ , thus $f , g : [ 0 , 1 ] \\to [ 0 , 1 ]$",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "two curved functions f and g",
      "shaded region between curves",
      "horizontal hatching lines",
      "vertical dashed lines at x=1 and y=1",
      "axes labeled 0 and 1",
      "labels f and g on respective curves"
    ]
  },
  {
    "doc_id": "1909.05088",
    "figure_id": "1909.05088_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg",
    "caption": "Figure 1: Percentage of female and male speakers per age group",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Orange bars representing male speakers\",\n    \"Blue bars representing female speakers\",\n    \"Seven age groups on x-axis (20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-90)\",\n    \"Percentage scale from 0% to 100% on y-axis\",\n    \"Decreasing trend in blue (female) bars from left to right\",\n    \"20-30 age group showing approximately 65% female representation\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the 20-30 age group show 65% female representation despite representing a very small percentage of total sentences?\",\n      \"answer\": \"The 20-30 age group represents only about 0% of the total amount of sentences in the dataset, making this demographic pattern statistically insignificant despite its visual appearance in the chart.\",\n      \"visual_anchor\": \"Blue bar at approximately 65% in the 20-30 age group\",\n      \"text_evidence\": \"with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the declining blue bar trend from 65% to near 0% across age groups reflect gender unbalance in translations?\",\n      \"answer\": \"The decreasing female representation across age groups creates a gender imbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.\",\n      \"visual_anchor\": \"Decreasing blue (female) bars from 65% at 20-30 to nearly 0% at 80-90\",\n      \"text_evidence\": \"there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the predominantly orange composition after age 40 justify augmenting sentences with gender tags for speaker identification?\",\n      \"answer\": \"Yes, the gender imbalance shown across age groups necessitates augmenting every sentence with a tag on the English source side, identifying the gender of the speaker, to address personalization challenges.\",\n      \"visual_anchor\": \"Orange (male) bars dominating above 60% from age 40-50 onwards\",\n      \"text_evidence\": \"We augmented every sentence with a tag on the English source side, identifying the gender of the speaker, as illustrated in (1).\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Orange bars representing male speakers",
        "Blue bars representing female speakers",
        "Seven age groups on x-axis (20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-90)",
        "Percentage scale from 0% to 100% on y-axis",
        "Decreasing trend in blue (female) bars from left to right",
        "20-30 age group showing approximately 65% female representation"
      ],
      "queries": [
        {
          "query": "Why does the 20-30 age group show 65% female representation despite representing a very small percentage of total sentences?",
          "answer": "The 20-30 age group represents only about 0% of the total amount of sentences in the dataset, making this demographic pattern statistically insignificant despite its visual appearance in the chart.",
          "visual_anchor": "Blue bar at approximately 65% in the 20-30 age group",
          "text_evidence": "with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the declining blue bar trend from 65% to near 0% across age groups reflect gender unbalance in translations?",
          "answer": "The decreasing female representation across age groups creates a gender imbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.",
          "visual_anchor": "Decreasing blue (female) bars from 65% at 20-30 to nearly 0% at 80-90",
          "text_evidence": "there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the predominantly orange composition after age 40 justify augmenting sentences with gender tags for speaker identification?",
          "answer": "Yes, the gender imbalance shown across age groups necessitates augmenting every sentence with a tag on the English source side, identifying the gender of the speaker, to address personalization challenges.",
          "visual_anchor": "Orange (male) bars dominating above 60% from age 40-50 onwards",
          "text_evidence": "We augmented every sentence with a tag on the English source side, identifying the gender of the speaker, as illustrated in (1).",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the 20-30 age group show 65% female representation despite representing a very small percentage of total sentences?",
        "answer": "The 20-30 age group represents only about 0% of the total amount of sentences in the dataset, making this demographic pattern statistically insignificant despite its visual appearance in the chart.",
        "visual_anchor": "Blue bar at approximately 65% in the 20-30 age group",
        "text_evidence": "with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the declining blue bar trend from 65% to near 0% across age groups reflect gender unbalance in translations?",
        "answer": "The decreasing female representation across age groups creates a gender imbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.",
        "visual_anchor": "Decreasing blue (female) bars from 65% at 20-30 to nearly 0% at 80-90",
        "text_evidence": "there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the predominantly orange composition after age 40 justify augmenting sentences with gender tags for speaker identification?",
        "answer": "Yes, the gender imbalance shown across age groups necessitates augmenting every sentence with a tag on the English source side, identifying the gender of the speaker, to address personalization challenges.",
        "visual_anchor": "Orange (male) bars dominating above 60% from age 40-50 onwards",
        "text_evidence": "We augmented every sentence with a tag on the English source side, identifying the gender of the speaker, as illustrated in (1).",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Orange bars representing male speakers",
      "Blue bars representing female speakers",
      "Seven age groups on x-axis (20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-90)",
      "Percentage scale from 0% to 100% on y-axis",
      "Decreasing trend in blue (female) bars from left to right",
      "20-30 age group showing approximately 65% female representation"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg",
    "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Seven numbered examples (1-7) with names and activities\",\n    \"Label tags: CITY (examples 1,2,6), MISC (example 3), LOCATION (example 5)\",\n    \"Alternating white and gray background rows\",\n    \"Example 4 and 7 have no entity tags shown\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why is Charlotte in example 1 tagged as CITY when CoreNLP should recognize it as a person name?\",\n      \"answer\": \"CoreNLP wrongfully tags person entities as non-PERSON or NULL entities. The figure demonstrates this systematic bias by showing Charlotte incorrectly classified as CITY despite appearing in a human-like context (\\\"is a person\\\").\",\n      \"visual_anchor\": \"Example 1 with CITY label tag above 'Charlotte is a person'\",\n      \"text_evidence\": \"Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the gray-background rows showing Sofia and Rose illustrate the templated sentences starting with census names followed by human activities?\",\n      \"answer\": \"Yes, these examples follow the template structure where census names (Sofia, Rose) are followed by human-like activities (eating cupcake, playing with dolls). However, Sofia is misclassified as CITY and Rose has no entity tag.\",\n      \"visual_anchor\": \"Gray-background rows 2 and 4 showing 'Sofia eats her favorite cupcake' and 'Rose plays with her dolls'\",\n      \"text_evidence\": \"Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the LOCATION tag on Gracie in example 5 relate to measuring bias in NER systems?\",\n      \"answer\": \"The incorrect LOCATION tag on Gracie demonstrates the systematic bias being measured across five named entity models. This misclassification exemplifies how NER systems fail to recognize person names in human-activity contexts, which the study evaluates using 139 years of census data.\",\n      \"visual_anchor\": \"Example 5 with blue LOCATION label above 'Gracie is going to school'\",\n      \"text_evidence\": \"To measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Seven numbered examples (1-7) with names and activities",
        "Label tags: CITY (examples 1,2,6), MISC (example 3), LOCATION (example 5)",
        "Alternating white and gray background rows",
        "Example 4 and 7 have no entity tags shown"
      ],
      "queries": [
        {
          "query": "Why is Charlotte in example 1 tagged as CITY when CoreNLP should recognize it as a person name?",
          "answer": "CoreNLP wrongfully tags person entities as non-PERSON or NULL entities. The figure demonstrates this systematic bias by showing Charlotte incorrectly classified as CITY despite appearing in a human-like context (\"is a person\").",
          "visual_anchor": "Example 1 with CITY label tag above 'Charlotte is a person'",
          "text_evidence": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the gray-background rows showing Sofia and Rose illustrate the templated sentences starting with census names followed by human activities?",
          "answer": "Yes, these examples follow the template structure where census names (Sofia, Rose) are followed by human-like activities (eating cupcake, playing with dolls). However, Sofia is misclassified as CITY and Rose has no entity tag.",
          "visual_anchor": "Gray-background rows 2 and 4 showing 'Sofia eats her favorite cupcake' and 'Rose plays with her dolls'",
          "text_evidence": "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity.",
          "query_type": "value_context"
        },
        {
          "query": "How does the LOCATION tag on Gracie in example 5 relate to measuring bias in NER systems?",
          "answer": "The incorrect LOCATION tag on Gracie demonstrates the systematic bias being measured across five named entity models. This misclassification exemplifies how NER systems fail to recognize person names in human-activity contexts, which the study evaluates using 139 years of census data.",
          "visual_anchor": "Example 5 with blue LOCATION label above 'Gracie is going to school'",
          "text_evidence": "To measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why is Charlotte in example 1 tagged as CITY when CoreNLP should recognize it as a person name?",
        "answer": "CoreNLP wrongfully tags person entities as non-PERSON or NULL entities. The figure demonstrates this systematic bias by showing Charlotte incorrectly classified as CITY despite appearing in a human-like context (\"is a person\").",
        "visual_anchor": "Example 1 with CITY label tag above 'Charlotte is a person'",
        "text_evidence": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the gray-background rows showing Sofia and Rose illustrate the templated sentences starting with census names followed by human activities?",
        "answer": "Yes, these examples follow the template structure where census names (Sofia, Rose) are followed by human-like activities (eating cupcake, playing with dolls). However, Sofia is misclassified as CITY and Rose has no entity tag.",
        "visual_anchor": "Gray-background rows 2 and 4 showing 'Sofia eats her favorite cupcake' and 'Rose plays with her dolls'",
        "text_evidence": "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity.",
        "query_type": "value_context"
      },
      {
        "query": "How does the LOCATION tag on Gracie in example 5 relate to measuring bias in NER systems?",
        "answer": "The incorrect LOCATION tag on Gracie demonstrates the systematic bias being measured across five named entity models. This misclassification exemplifies how NER systems fail to recognize person names in human-activity contexts, which the study evaluates using 139 years of census data.",
        "visual_anchor": "Example 5 with blue LOCATION label above 'Gracie is going to school'",
        "text_evidence": "To measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Seven numbered examples (1-7) with names and activities",
      "Label tags: CITY (examples 1,2,6), MISC (example 3), LOCATION (example 5)",
      "Alternating white and gray background rows",
      "Example 4 and 7 have no entity tags shown"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_14",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg",
    "caption": "Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"dashed magenta line representing female error rates\", \"solid cyan line representing male error rates\", \"y-axis showing error rates from 0 to 0.35\", \"x-axis spanning years 1875 to 2015\", \"upward trend in both lines after 1975\", \"female line peaks around 0.33 in 2015\", \"male line reaches approximately 0.17 in 2015\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dashed magenta line peak at 0.33 in 2015 when template #4 was used with weighted cases?\",\n      \"answer\": \"The plot shows results from template #4 on weighted census data spanning 139 years, where female names consistently exhibit higher error rates. The 2015 peak reflects this persistent gender disparity in name tagging performance.\",\n      \"visual_anchor\": \"dashed magenta line peaking at approximately 0.33 in year 2015\",\n      \"text_evidence\": \"Results from different models that spanned the 139-year history of baby names from the census data on different error types for the weighted cases using template #4. Female names have higher error rates for all the cases.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the Flair model's behavior explain why the magenta line rises sharply after 1995 compared to the cyan line?\",\n      \"answer\": \"Yes, the Flair model frequently mistagged female names like Charlotte and Sofia as LOC (location) entities. This systematic error in the Flair model contributes to the steeper upward trajectory of female error rates after 1995.\",\n      \"visual_anchor\": \"magenta dashed line showing sharp upward trend from 1995 to 2015, diverging from cyan line\",\n      \"text_evidence\": \"We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our benchmark evaluated on template #4 in Table 2. Charlotte 12,940 Tagged as LOC, Sofia 7,621 Tagged as LOC\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causes the cyan male line to remain below 0.06 until 1955 despite the 139-year span?\",\n      \"answer\": \"Male names maintained relatively stable and low error rates in early decades because the models performed more consistently on male names across the historical census data. The error formula calculations remained low for male names during this period.\",\n      \"visual_anchor\": \"solid cyan line staying flat between 0.04 and 0.06 from 1875 to 1955\",\n      \"text_evidence\": \"The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "dashed magenta line representing female error rates",
        "solid cyan line representing male error rates",
        "y-axis showing error rates from 0 to 0.35",
        "x-axis spanning years 1875 to 2015",
        "upward trend in both lines after 1975",
        "female line peaks around 0.33 in 2015",
        "male line reaches approximately 0.17 in 2015"
      ],
      "queries": [
        {
          "query": "Why does the dashed magenta line peak at 0.33 in 2015 when template #4 was used with weighted cases?",
          "answer": "The plot shows results from template #4 on weighted census data spanning 139 years, where female names consistently exhibit higher error rates. The 2015 peak reflects this persistent gender disparity in name tagging performance.",
          "visual_anchor": "dashed magenta line peaking at approximately 0.33 in year 2015",
          "text_evidence": "Results from different models that spanned the 139-year history of baby names from the census data on different error types for the weighted cases using template #4. Female names have higher error rates for all the cases.",
          "query_type": "value_context"
        },
        {
          "query": "Does the Flair model's behavior explain why the magenta line rises sharply after 1995 compared to the cyan line?",
          "answer": "Yes, the Flair model frequently mistagged female names like Charlotte and Sofia as LOC (location) entities. This systematic error in the Flair model contributes to the steeper upward trajectory of female error rates after 1995.",
          "visual_anchor": "magenta dashed line showing sharp upward trend from 1995 to 2015, diverging from cyan line",
          "text_evidence": "We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our benchmark evaluated on template #4 in Table 2. Charlotte 12,940 Tagged as LOC, Sofia 7,621 Tagged as LOC",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causes the cyan male line to remain below 0.06 until 1955 despite the 139-year span?",
          "answer": "Male names maintained relatively stable and low error rates in early decades because the models performed more consistently on male names across the historical census data. The error formula calculations remained low for male names during this period.",
          "visual_anchor": "solid cyan line staying flat between 0.04 and 0.06 from 1875 to 1955",
          "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dashed magenta line peak at 0.33 in 2015 when template #4 was used with weighted cases?",
        "answer": "The plot shows results from template #4 on weighted census data spanning 139 years, where female names consistently exhibit higher error rates. The 2015 peak reflects this persistent gender disparity in name tagging performance.",
        "visual_anchor": "dashed magenta line peaking at approximately 0.33 in year 2015",
        "text_evidence": "Results from different models that spanned the 139-year history of baby names from the census data on different error types for the weighted cases using template #4. Female names have higher error rates for all the cases.",
        "query_type": "value_context"
      },
      {
        "query": "Does the Flair model's behavior explain why the magenta line rises sharply after 1995 compared to the cyan line?",
        "answer": "Yes, the Flair model frequently mistagged female names like Charlotte and Sofia as LOC (location) entities. This systematic error in the Flair model contributes to the steeper upward trajectory of female error rates after 1995.",
        "visual_anchor": "magenta dashed line showing sharp upward trend from 1995 to 2015, diverging from cyan line",
        "text_evidence": "We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our benchmark evaluated on template #4 in Table 2. Charlotte 12,940 Tagged as LOC, Sofia 7,621 Tagged as LOC",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causes the cyan male line to remain below 0.06 until 1955 despite the 139-year span?",
        "answer": "Male names maintained relatively stable and low error rates in early decades because the models performed more consistently on male names across the historical census data. The error formula calculations remained low for male names during this period.",
        "visual_anchor": "solid cyan line staying flat between 0.04 and 0.06 from 1875 to 1955",
        "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "dashed magenta line representing female error rates",
      "solid cyan line representing male error rates",
      "y-axis showing error rates from 0 to 0.35",
      "x-axis spanning years 1875 to 2015",
      "upward trend in both lines after 1975",
      "female line peaks around 0.33 in 2015",
      "male line reaches approximately 0.17 in 2015"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_31",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg",
    "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"dashed magenta line labeled 'female' ranging from ~0.2 to ~0.55\",\n    \"solid cyan line labeled 'male' ranging from ~0.1 to ~0.4\",\n    \"x-axis spanning 1875 to 2015\",\n    \"y-axis ranging from 0 to 0.6\",\n    \"gap between female and male lines throughout time period\",\n    \"both lines showing upward trend after 1975\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the magenta female line reach 0.55 in 2015 while male stays at 0.4?\",\n      \"answer\": \"Female names have consistently higher error rates than male names across nearly all time periods. The analysis uses template #4 for unweighted cases, which reveals this persistent gender disparity in naming error rates.\",\n      \"visual_anchor\": \"magenta dashed line at 0.55 and cyan solid line at 0.4 in year 2015\",\n      \"text_evidence\": \"Female names have higher error rates for all cases except four marginal cases.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the cyan line's rise from 0.15 to 0.4 between 1975 and 2015 reflect CoreNLP version changes?\",\n      \"answer\": \"The upward trend in male error rates from 1975 to 2015 could reflect model evolution. CoreNLP underwent version updates (e.g., 3.8 to 3.9) that changed tagging behavior, potentially affecting error patterns over time.\",\n      \"visual_anchor\": \"cyan solid line increasing from approximately 0.15 at 1975 to 0.4 at 2015\",\n      \"text_evidence\": \"As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the calculated error rates on the y-axis relate to machine translation performance issues?\",\n      \"answer\": \"The y-axis displays calculated error rates from specific formulas measuring naming biases. These error metrics are relevant to broader NLP applications including machine translation, where similar bias patterns can affect system performance.\",\n      \"visual_anchor\": \"y-axis ranging from 0 to 0.6 showing calculated error rates\",\n      \"text_evidence\": \"The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "dashed magenta line labeled 'female' ranging from ~0.2 to ~0.55",
        "solid cyan line labeled 'male' ranging from ~0.1 to ~0.4",
        "x-axis spanning 1875 to 2015",
        "y-axis ranging from 0 to 0.6",
        "gap between female and male lines throughout time period",
        "both lines showing upward trend after 1975"
      ],
      "queries": [
        {
          "query": "Why does the magenta female line reach 0.55 in 2015 while male stays at 0.4?",
          "answer": "Female names have consistently higher error rates than male names across nearly all time periods. The analysis uses template #4 for unweighted cases, which reveals this persistent gender disparity in naming error rates.",
          "visual_anchor": "magenta dashed line at 0.55 and cyan solid line at 0.4 in year 2015",
          "text_evidence": "Female names have higher error rates for all cases except four marginal cases.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the cyan line's rise from 0.15 to 0.4 between 1975 and 2015 reflect CoreNLP version changes?",
          "answer": "The upward trend in male error rates from 1975 to 2015 could reflect model evolution. CoreNLP underwent version updates (e.g., 3.8 to 3.9) that changed tagging behavior, potentially affecting error patterns over time.",
          "visual_anchor": "cyan solid line increasing from approximately 0.15 at 1975 to 0.4 at 2015",
          "text_evidence": "As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the calculated error rates on the y-axis relate to machine translation performance issues?",
          "answer": "The y-axis displays calculated error rates from specific formulas measuring naming biases. These error metrics are relevant to broader NLP applications including machine translation, where similar bias patterns can affect system performance.",
          "visual_anchor": "y-axis ranging from 0 to 0.6 showing calculated error rates",
          "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the magenta female line reach 0.55 in 2015 while male stays at 0.4?",
        "answer": "Female names have consistently higher error rates than male names across nearly all time periods. The analysis uses template #4 for unweighted cases, which reveals this persistent gender disparity in naming error rates.",
        "visual_anchor": "magenta dashed line at 0.55 and cyan solid line at 0.4 in year 2015",
        "text_evidence": "Female names have higher error rates for all cases except four marginal cases.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the cyan line's rise from 0.15 to 0.4 between 1975 and 2015 reflect CoreNLP version changes?",
        "answer": "The upward trend in male error rates from 1975 to 2015 could reflect model evolution. CoreNLP underwent version updates (e.g., 3.8 to 3.9) that changed tagging behavior, potentially affecting error patterns over time.",
        "visual_anchor": "cyan solid line increasing from approximately 0.15 at 1975 to 0.4 at 2015",
        "text_evidence": "As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the calculated error rates on the y-axis relate to machine translation performance issues?",
        "answer": "The y-axis displays calculated error rates from specific formulas measuring naming biases. These error metrics are relevant to broader NLP applications including machine translation, where similar bias patterns can affect system performance.",
        "visual_anchor": "y-axis ranging from 0 to 0.6 showing calculated error rates",
        "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "dashed magenta line labeled 'female' ranging from ~0.2 to ~0.55",
      "solid cyan line labeled 'male' ranging from ~0.1 to ~0.4",
      "x-axis spanning 1875 to 2015",
      "y-axis ranging from 0 to 0.6",
      "gap between female and male lines throughout time period",
      "both lines showing upward trend after 1975"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_35",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg",
    "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue dashed line (TMPL1) showing decline from ~0.05 to ~0.02 between 1895-1975\",\n    \"Multiple solid lines (TMPL2-TMPL9) clustered between 0.01-0.03 before 1995\",\n    \"Sharp upward trend in blue dashed line after 1995 reaching ~0.08 by 2015\",\n    \"All solid lines showing increase after 1995, reaching 0.03-0.05 by 2015\",\n    \"Y-axis ranging from 0.01 to 0.09\",\n    \"X-axis spanning 1875 to 2015\",\n    \"Nine different templates represented by different colored lines\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the blue dashed TMPL1 line's unique upward spike to 0.08 after 1995 demonstrate how context helps some models?\",\n      \"answer\": \"Yes, the blue dashed TMPL1 line shows a dramatic increase to 0.08 after 1995, while other templates remain below 0.05, illustrating that context in some templates helped certain models but showed negative effects on others.\",\n      \"visual_anchor\": \"Blue dashed line (TMPL1) reaching approximately 0.08 by 2015\",\n      \"text_evidence\": \"Notice how context in some of the templates helped some models, but showed negative effects on other models.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do all nine template lines cluster between 0.01-0.03 before 1995 despite representing different models?\",\n      \"answer\": \"The clustering suggests relatively uniform low performance across all templates and models during the 1875-1995 period. This pattern changes after 1995, showing an interesting trend in how different templates perform over the 139-year span for male names.\",\n      \"visual_anchor\": \"All nine lines (TMPL1-TMPL9) clustered between 0.01 and 0.03 from 1875 to 1995\",\n      \"text_evidence\": \"That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the diverging performance lines after 1995 reflect dataset biases rather than real-world gender name distributions?\",\n      \"answer\": \"Yes, the diverging performance patterns after 1995, with TMPL1 reaching 0.08 while others stay lower, suggest the datasets used do not reflect the real world but rather the opposite, indicating bias toward specific groups.\",\n      \"visual_anchor\": \"Diverging lines after 1995 with TMPL1 reaching 0.08 and others at 0.03-0.05\",\n      \"text_evidence\": \"Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue dashed line (TMPL1) showing decline from ~0.05 to ~0.02 between 1895-1975",
        "Multiple solid lines (TMPL2-TMPL9) clustered between 0.01-0.03 before 1995",
        "Sharp upward trend in blue dashed line after 1995 reaching ~0.08 by 2015",
        "All solid lines showing increase after 1995, reaching 0.03-0.05 by 2015",
        "Y-axis ranging from 0.01 to 0.09",
        "X-axis spanning 1875 to 2015",
        "Nine different templates represented by different colored lines"
      ],
      "queries": [
        {
          "query": "Does the blue dashed TMPL1 line's unique upward spike to 0.08 after 1995 demonstrate how context helps some models?",
          "answer": "Yes, the blue dashed TMPL1 line shows a dramatic increase to 0.08 after 1995, while other templates remain below 0.05, illustrating that context in some templates helped certain models but showed negative effects on others.",
          "visual_anchor": "Blue dashed line (TMPL1) reaching approximately 0.08 by 2015",
          "text_evidence": "Notice how context in some of the templates helped some models, but showed negative effects on other models.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do all nine template lines cluster between 0.01-0.03 before 1995 despite representing different models?",
          "answer": "The clustering suggests relatively uniform low performance across all templates and models during the 1875-1995 period. This pattern changes after 1995, showing an interesting trend in how different templates perform over the 139-year span for male names.",
          "visual_anchor": "All nine lines (TMPL1-TMPL9) clustered between 0.01 and 0.03 from 1875 to 1995",
          "text_evidence": "That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the diverging performance lines after 1995 reflect dataset biases rather than real-world gender name distributions?",
          "answer": "Yes, the diverging performance patterns after 1995, with TMPL1 reaching 0.08 while others stay lower, suggest the datasets used do not reflect the real world but rather the opposite, indicating bias toward specific groups.",
          "visual_anchor": "Diverging lines after 1995 with TMPL1 reaching 0.08 and others at 0.03-0.05",
          "text_evidence": "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the blue dashed TMPL1 line's unique upward spike to 0.08 after 1995 demonstrate how context helps some models?",
        "answer": "Yes, the blue dashed TMPL1 line shows a dramatic increase to 0.08 after 1995, while other templates remain below 0.05, illustrating that context in some templates helped certain models but showed negative effects on others.",
        "visual_anchor": "Blue dashed line (TMPL1) reaching approximately 0.08 by 2015",
        "text_evidence": "Notice how context in some of the templates helped some models, but showed negative effects on other models.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do all nine template lines cluster between 0.01-0.03 before 1995 despite representing different models?",
        "answer": "The clustering suggests relatively uniform low performance across all templates and models during the 1875-1995 period. This pattern changes after 1995, showing an interesting trend in how different templates perform over the 139-year span for male names.",
        "visual_anchor": "All nine lines (TMPL1-TMPL9) clustered between 0.01 and 0.03 from 1875 to 1995",
        "text_evidence": "That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the diverging performance lines after 1995 reflect dataset biases rather than real-world gender name distributions?",
        "answer": "Yes, the diverging performance patterns after 1995, with TMPL1 reaching 0.08 while others stay lower, suggest the datasets used do not reflect the real world but rather the opposite, indicating bias toward specific groups.",
        "visual_anchor": "Diverging lines after 1995 with TMPL1 reaching 0.08 and others at 0.03-0.05",
        "text_evidence": "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue dashed line (TMPL1) showing decline from ~0.05 to ~0.02 between 1895-1975",
      "Multiple solid lines (TMPL2-TMPL9) clustered between 0.01-0.03 before 1995",
      "Sharp upward trend in blue dashed line after 1995 reaching ~0.08 by 2015",
      "All solid lines showing increase after 1995, reaching 0.03-0.05 by 2015",
      "Y-axis ranging from 0.01 to 0.09",
      "X-axis spanning 1875 to 2015",
      "Nine different templates represented by different colored lines"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_41",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg",
    "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Green dashed line representing Version 3.8\",\n    \"Yellow/orange solid line representing Version 3.9\",\n    \"Y-axis ranging from 0 to 0.14\",\n    \"X-axis showing years from 1875 to 2015\",\n    \"Both lines showing U-shaped pattern with minimum around 1955-1975\",\n    \"Convergence of both lines at higher values near 2015\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the green dashed and yellow solid lines nearly overlap throughout 1875-2015 despite Version 3.9's entity tagging improvements?\",\n      \"answer\": \"Despite Version 3.9 trying to tag more entities and achieving slight improvement in Error Type-3, degraded performance in Error Type-2 offset the gains, resulting in overall no change in Error Type-1, which is the super-set shown in this plot.\",\n      \"visual_anchor\": \"overlapping green dashed (Version 3.8) and yellow solid (Version 3.9) lines across entire timeline\",\n      \"text_evidence\": \"This then degraded performance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered to be the super-set.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the minimal separation between Version 3.8 and 3.9 lines reflect the subtle boost mentioned for Error Type-3?\",\n      \"answer\": \"No, this figure shows Error Type-1 (the super-set) where no change occurred. The subtle boost in Error Type-3 performance was offset by degradation in Error Type-2, resulting in the overlapping lines observed here.\",\n      \"visual_anchor\": \"near-identical trajectories of green dashed and yellow solid lines from 0.02 to 0.12\",\n      \"text_evidence\": \"Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does the U-shaped pattern with minimum near 1955-1975 relate to CoreNLP's increased entity tagging attempts?\",\n      \"answer\": \"The U-shaped pattern reflects temporal variation in name recognition errors across different birth year cohorts. CoreNLP's attempt to tag more entities in Version 3.9 did not change this overall pattern, as improvements in one error type were offset by degradation in another.\",\n      \"visual_anchor\": \"both lines reaching minimum values around 0.02 near year 1955-1975, then rising to 0.12 by 2015\",\n      \"text_evidence\": \"Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Green dashed line representing Version 3.8",
        "Yellow/orange solid line representing Version 3.9",
        "Y-axis ranging from 0 to 0.14",
        "X-axis showing years from 1875 to 2015",
        "Both lines showing U-shaped pattern with minimum around 1955-1975",
        "Convergence of both lines at higher values near 2015"
      ],
      "queries": [
        {
          "query": "Why do the green dashed and yellow solid lines nearly overlap throughout 1875-2015 despite Version 3.9's entity tagging improvements?",
          "answer": "Despite Version 3.9 trying to tag more entities and achieving slight improvement in Error Type-3, degraded performance in Error Type-2 offset the gains, resulting in overall no change in Error Type-1, which is the super-set shown in this plot.",
          "visual_anchor": "overlapping green dashed (Version 3.8) and yellow solid (Version 3.9) lines across entire timeline",
          "text_evidence": "This then degraded performance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered to be the super-set.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the minimal separation between Version 3.8 and 3.9 lines reflect the subtle boost mentioned for Error Type-3?",
          "answer": "No, this figure shows Error Type-1 (the super-set) where no change occurred. The subtle boost in Error Type-3 performance was offset by degradation in Error Type-2, resulting in the overlapping lines observed here.",
          "visual_anchor": "near-identical trajectories of green dashed and yellow solid lines from 0.02 to 0.12",
          "text_evidence": "Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does the U-shaped pattern with minimum near 1955-1975 relate to CoreNLP's increased entity tagging attempts?",
          "answer": "The U-shaped pattern reflects temporal variation in name recognition errors across different birth year cohorts. CoreNLP's attempt to tag more entities in Version 3.9 did not change this overall pattern, as improvements in one error type were offset by degradation in another.",
          "visual_anchor": "both lines reaching minimum values around 0.02 near year 1955-1975, then rising to 0.12 by 2015",
          "text_evidence": "Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the green dashed and yellow solid lines nearly overlap throughout 1875-2015 despite Version 3.9's entity tagging improvements?",
        "answer": "Despite Version 3.9 trying to tag more entities and achieving slight improvement in Error Type-3, degraded performance in Error Type-2 offset the gains, resulting in overall no change in Error Type-1, which is the super-set shown in this plot.",
        "visual_anchor": "overlapping green dashed (Version 3.8) and yellow solid (Version 3.9) lines across entire timeline",
        "text_evidence": "This then degraded performance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered to be the super-set.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the minimal separation between Version 3.8 and 3.9 lines reflect the subtle boost mentioned for Error Type-3?",
        "answer": "No, this figure shows Error Type-1 (the super-set) where no change occurred. The subtle boost in Error Type-3 performance was offset by degradation in Error Type-2, resulting in the overlapping lines observed here.",
        "visual_anchor": "near-identical trajectories of green dashed and yellow solid lines from 0.02 to 0.12",
        "text_evidence": "Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does the U-shaped pattern with minimum near 1955-1975 relate to CoreNLP's increased entity tagging attempts?",
        "answer": "The U-shaped pattern reflects temporal variation in name recognition errors across different birth year cohorts. CoreNLP's attempt to tag more entities in Version 3.9 did not change this overall pattern, as improvements in one error type were offset by degradation in another.",
        "visual_anchor": "both lines reaching minimum values around 0.02 near year 1955-1975, then rising to 0.12 by 2015",
        "text_evidence": "Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Green dashed line representing Version 3.8",
      "Yellow/orange solid line representing Version 3.9",
      "Y-axis ranging from 0 to 0.14",
      "X-axis showing years from 1875 to 2015",
      "Both lines showing U-shaped pattern with minimum around 1955-1975",
      "Convergence of both lines at higher values near 2015"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_44",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg",
    "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"V2.0 green dashed line\",\n    \"V2.1 orange solid line\",\n    \"Female Medium Spacy top panel\",\n    \"Male Medium Spacy bottom panel\",\n    \"Y-axis scale 0 to 0.8\",\n    \"Time period 1875-2015 on X-axis\",\n    \"V2.1 reaching ~0.65 by 2015 in female panel\",\n    \"V2.0 reaching ~0.55 by 2015 in female panel\",\n    \"V2.1 reaching ~0.70 by 2015 in male panel\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the V2.1 orange line reaching 0.70 for males versus 0.55 for females by 2015 demonstrate bias twice as large?\",\n      \"answer\": \"Yes, the V2.1 version shows substantially higher error rates for female names (~0.65-0.70) compared to male names (~0.70), supporting the claim that bias against female names was on average twice that of male names.\",\n      \"visual_anchor\": \"V2.1 orange solid line at 2015: ~0.70 in Female panel versus ~0.70 in Male panel\",\n      \"text_evidence\": \"The bias against female names was on average twice that of male names.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the V2.1 orange line remain higher than V2.0 green across all years if newer versions aimed to tag more entities?\",\n      \"answer\": \"The version update tried to tag more entities, resulting in more PERSON entities being incorrectly tagged as non-PERSON entities. This degraded performance with regard to Error Type-2 in the newer version, explaining the consistently higher error rates shown by the orange line.\",\n      \"visual_anchor\": \"V2.1 orange solid line consistently above V2.0 green dashed line from 1875 to 2015\",\n      \"text_evidence\": \"Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded performance with regard to Error Type-2 in the newer version\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the V2.0 green dashed decline from 0.45 to 0.32 during 1875-1955 relate to Error Type-1 Weighted performance?\",\n      \"answer\": \"The newer version (V2.1) is more erroneous when it comes to Error Type-1 Weighted case, which is a superset of all error types. The decline in V2.0 reflects baseline performance, while V2.1's consistently higher values indicate worse overall weighted error rates.\",\n      \"visual_anchor\": \"V2.0 green dashed line declining from ~0.45 to ~0.32 between 1875 and 1955 in Female panel\",\n      \"text_evidence\": \"For instance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "V2.0 green dashed line",
        "V2.1 orange solid line",
        "Female Medium Spacy top panel",
        "Male Medium Spacy bottom panel",
        "Y-axis scale 0 to 0.8",
        "Time period 1875-2015 on X-axis",
        "V2.1 reaching ~0.65 by 2015 in female panel",
        "V2.0 reaching ~0.55 by 2015 in female panel",
        "V2.1 reaching ~0.70 by 2015 in male panel"
      ],
      "queries": [
        {
          "query": "Does the V2.1 orange line reaching 0.70 for males versus 0.55 for females by 2015 demonstrate bias twice as large?",
          "answer": "Yes, the V2.1 version shows substantially higher error rates for female names (~0.65-0.70) compared to male names (~0.70), supporting the claim that bias against female names was on average twice that of male names.",
          "visual_anchor": "V2.1 orange solid line at 2015: ~0.70 in Female panel versus ~0.70 in Male panel",
          "text_evidence": "The bias against female names was on average twice that of male names.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the V2.1 orange line remain higher than V2.0 green across all years if newer versions aimed to tag more entities?",
          "answer": "The version update tried to tag more entities, resulting in more PERSON entities being incorrectly tagged as non-PERSON entities. This degraded performance with regard to Error Type-2 in the newer version, explaining the consistently higher error rates shown by the orange line.",
          "visual_anchor": "V2.1 orange solid line consistently above V2.0 green dashed line from 1875 to 2015",
          "text_evidence": "Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded performance with regard to Error Type-2 in the newer version",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the V2.0 green dashed decline from 0.45 to 0.32 during 1875-1955 relate to Error Type-1 Weighted performance?",
          "answer": "The newer version (V2.1) is more erroneous when it comes to Error Type-1 Weighted case, which is a superset of all error types. The decline in V2.0 reflects baseline performance, while V2.1's consistently higher values indicate worse overall weighted error rates.",
          "visual_anchor": "V2.0 green dashed line declining from ~0.45 to ~0.32 between 1875 and 1955 in Female panel",
          "text_evidence": "For instance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the V2.1 orange line reaching 0.70 for males versus 0.55 for females by 2015 demonstrate bias twice as large?",
        "answer": "Yes, the V2.1 version shows substantially higher error rates for female names (~0.65-0.70) compared to male names (~0.70), supporting the claim that bias against female names was on average twice that of male names.",
        "visual_anchor": "V2.1 orange solid line at 2015: ~0.70 in Female panel versus ~0.70 in Male panel",
        "text_evidence": "The bias against female names was on average twice that of male names.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the V2.1 orange line remain higher than V2.0 green across all years if newer versions aimed to tag more entities?",
        "answer": "The version update tried to tag more entities, resulting in more PERSON entities being incorrectly tagged as non-PERSON entities. This degraded performance with regard to Error Type-2 in the newer version, explaining the consistently higher error rates shown by the orange line.",
        "visual_anchor": "V2.1 orange solid line consistently above V2.0 green dashed line from 1875 to 2015",
        "text_evidence": "Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded performance with regard to Error Type-2 in the newer version",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the V2.0 green dashed decline from 0.45 to 0.32 during 1875-1955 relate to Error Type-1 Weighted performance?",
        "answer": "The newer version (V2.1) is more erroneous when it comes to Error Type-1 Weighted case, which is a superset of all error types. The decline in V2.0 reflects baseline performance, while V2.1's consistently higher values indicate worse overall weighted error rates.",
        "visual_anchor": "V2.0 green dashed line declining from ~0.45 to ~0.32 between 1875 and 1955 in Female panel",
        "text_evidence": "For instance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "V2.0 green dashed line",
      "V2.1 orange solid line",
      "Female Medium Spacy top panel",
      "Male Medium Spacy bottom panel",
      "Y-axis scale 0 to 0.8",
      "Time period 1875-2015 on X-axis",
      "V2.1 reaching ~0.65 by 2015 in female panel",
      "V2.0 reaching ~0.55 by 2015 in female panel",
      "V2.1 reaching ~0.70 by 2015 in male panel"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_60",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg",
    "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"dashed magenta line labeled female\", \"solid cyan line labeled male\", \"y-axis error rates from 0 to 0.25\", \"x-axis years from 1875 to 2015\", \"female line peaks around 0.22 in 2015\", \"male line reaches about 0.10 in 2015\", \"both lines show minimum around 1955\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the magenta dashed line remain consistently above the cyan line across all years from 1875 to 2015?\",\n      \"answer\": \"Female names have higher error rates for all cases when using template #5 over the 139-year history, as shown by the magenta line staying above the cyan line throughout the entire time period.\",\n      \"visual_anchor\": \"magenta dashed line (female) positioned above cyan solid line (male) across entire x-axis\",\n      \"text_evidence\": \"Female names have higher error rates for all the cases. The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the sharp upward trend in the magenta line after 1975 align with weighted template 5 calculations?\",\n      \"answer\": \"Yes, the magenta line's rise from approximately 0.08 in 1975 to 0.22 in 2015 reflects the weighted case error rates calculated using template #5 for female baby names from the census data.\",\n      \"visual_anchor\": \"magenta dashed line increasing steeply from 1975 to 2015, rising from ~0.08 to ~0.22\",\n      \"text_evidence\": \"Results from different models ran over 139-year history of baby names from the census data on different error types for the weighted case using template # 5\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes both the cyan and magenta lines to reach their minimum values around 1955?\",\n      \"answer\": \"Both gender error rates drop to their lowest points around 1955, with the cyan line near 0.02 and magenta near 0.04. This temporal pattern corresponds to when the baby names were given in the census data.\",\n      \"visual_anchor\": \"both cyan and magenta lines reaching minimum values around 1955 on x-axis\",\n      \"text_evidence\": \"the x axis represents the year in which the baby name was given. Results from different models ran over 139-year history of baby names from the census data\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "dashed magenta line labeled female",
        "solid cyan line labeled male",
        "y-axis error rates from 0 to 0.25",
        "x-axis years from 1875 to 2015",
        "female line peaks around 0.22 in 2015",
        "male line reaches about 0.10 in 2015",
        "both lines show minimum around 1955"
      ],
      "queries": [
        {
          "query": "Why does the magenta dashed line remain consistently above the cyan line across all years from 1875 to 2015?",
          "answer": "Female names have higher error rates for all cases when using template #5 over the 139-year history, as shown by the magenta line staying above the cyan line throughout the entire time period.",
          "visual_anchor": "magenta dashed line (female) positioned above cyan solid line (male) across entire x-axis",
          "text_evidence": "Female names have higher error rates for all the cases. The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the sharp upward trend in the magenta line after 1975 align with weighted template 5 calculations?",
          "answer": "Yes, the magenta line's rise from approximately 0.08 in 1975 to 0.22 in 2015 reflects the weighted case error rates calculated using template #5 for female baby names from the census data.",
          "visual_anchor": "magenta dashed line increasing steeply from 1975 to 2015, rising from ~0.08 to ~0.22",
          "text_evidence": "Results from different models ran over 139-year history of baby names from the census data on different error types for the weighted case using template # 5",
          "query_type": "value_context"
        },
        {
          "query": "What causes both the cyan and magenta lines to reach their minimum values around 1955?",
          "answer": "Both gender error rates drop to their lowest points around 1955, with the cyan line near 0.02 and magenta near 0.04. This temporal pattern corresponds to when the baby names were given in the census data.",
          "visual_anchor": "both cyan and magenta lines reaching minimum values around 1955 on x-axis",
          "text_evidence": "the x axis represents the year in which the baby name was given. Results from different models ran over 139-year history of baby names from the census data",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the magenta dashed line remain consistently above the cyan line across all years from 1875 to 2015?",
        "answer": "Female names have higher error rates for all cases when using template #5 over the 139-year history, as shown by the magenta line staying above the cyan line throughout the entire time period.",
        "visual_anchor": "magenta dashed line (female) positioned above cyan solid line (male) across entire x-axis",
        "text_evidence": "Female names have higher error rates for all the cases. The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the sharp upward trend in the magenta line after 1975 align with weighted template 5 calculations?",
        "answer": "Yes, the magenta line's rise from approximately 0.08 in 1975 to 0.22 in 2015 reflects the weighted case error rates calculated using template #5 for female baby names from the census data.",
        "visual_anchor": "magenta dashed line increasing steeply from 1975 to 2015, rising from ~0.08 to ~0.22",
        "text_evidence": "Results from different models ran over 139-year history of baby names from the census data on different error types for the weighted case using template # 5",
        "query_type": "value_context"
      },
      {
        "query": "What causes both the cyan and magenta lines to reach their minimum values around 1955?",
        "answer": "Both gender error rates drop to their lowest points around 1955, with the cyan line near 0.02 and magenta near 0.04. This temporal pattern corresponds to when the baby names were given in the census data.",
        "visual_anchor": "both cyan and magenta lines reaching minimum values around 1955 on x-axis",
        "text_evidence": "the x axis represents the year in which the baby name was given. Results from different models ran over 139-year history of baby names from the census data",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "dashed magenta line labeled female",
      "solid cyan line labeled male",
      "y-axis error rates from 0 to 0.25",
      "x-axis years from 1875 to 2015",
      "female line peaks around 0.22 in 2015",
      "male line reaches about 0.10 in 2015",
      "both lines show minimum around 1955"
    ]
  },
  {
    "doc_id": "1910.10872",
    "figure_id": "1910.10872_fig_79",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg",
    "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
    "original_quality_score": 0.6,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"dashed magenta line representing female error rates\",\n    \"solid cyan line representing male error rates\",\n    \"y-axis ranging from 0 to 0.5 showing error rates\",\n    \"x-axis spanning 1875 to 2015 showing years\",\n    \"peak in female line around 1995-2015 reaching approximately 0.45\",\n    \"relatively flat male line staying below 0.2 until 1975\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the dashed magenta line remain consistently above the solid cyan line across the 139-year census history?\",\n      \"answer\": \"Female names have higher error rates for all cases except three marginal cases, indicating overall performance is more biased towards female names throughout the 139-year baby name history.\",\n      \"visual_anchor\": \"dashed magenta line (female) positioned above solid cyan line (male) from 1875 to 2015\",\n      \"text_evidence\": \"Female names have higher error rates for all the cases except three marginal cases. Overall performance is more biased towards female names.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the magenta line's peak near 0.45 around 2005 reflect calculated error rates from template #5 formulas?\",\n      \"answer\": \"Yes, the y-axis shows the calculated error rates for each error type as described in their corresponding formulas, and the peak represents the highest bias against female names in recent decades using template #5.\",\n      \"visual_anchor\": \"magenta line peak at approximately 0.45 between 1995 and 2015\",\n      \"text_evidence\": \"The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the cyan line's sharp upward trend starting around 1975, given the unweighted case methodology?\",\n      \"answer\": \"The increase in male error rates after 1975 reflects changes in baby naming patterns over time, captured by running models over the 139-year census data using the unweighted case with template #5.\",\n      \"visual_anchor\": \"solid cyan line showing steep increase from approximately 0.15 to 0.35 between 1975 and 2015\",\n      \"text_evidence\": \"Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "dashed magenta line representing female error rates",
        "solid cyan line representing male error rates",
        "y-axis ranging from 0 to 0.5 showing error rates",
        "x-axis spanning 1875 to 2015 showing years",
        "peak in female line around 1995-2015 reaching approximately 0.45",
        "relatively flat male line staying below 0.2 until 1975"
      ],
      "queries": [
        {
          "query": "Why does the dashed magenta line remain consistently above the solid cyan line across the 139-year census history?",
          "answer": "Female names have higher error rates for all cases except three marginal cases, indicating overall performance is more biased towards female names throughout the 139-year baby name history.",
          "visual_anchor": "dashed magenta line (female) positioned above solid cyan line (male) from 1875 to 2015",
          "text_evidence": "Female names have higher error rates for all the cases except three marginal cases. Overall performance is more biased towards female names.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the magenta line's peak near 0.45 around 2005 reflect calculated error rates from template #5 formulas?",
          "answer": "Yes, the y-axis shows the calculated error rates for each error type as described in their corresponding formulas, and the peak represents the highest bias against female names in recent decades using template #5.",
          "visual_anchor": "magenta line peak at approximately 0.45 between 1995 and 2015",
          "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
          "query_type": "value_context"
        },
        {
          "query": "What explains the cyan line's sharp upward trend starting around 1975, given the unweighted case methodology?",
          "answer": "The increase in male error rates after 1975 reflects changes in baby naming patterns over time, captured by running models over the 139-year census data using the unweighted case with template #5.",
          "visual_anchor": "solid cyan line showing steep increase from approximately 0.15 to 0.35 between 1975 and 2015",
          "text_evidence": "Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the dashed magenta line remain consistently above the solid cyan line across the 139-year census history?",
        "answer": "Female names have higher error rates for all cases except three marginal cases, indicating overall performance is more biased towards female names throughout the 139-year baby name history.",
        "visual_anchor": "dashed magenta line (female) positioned above solid cyan line (male) from 1875 to 2015",
        "text_evidence": "Female names have higher error rates for all the cases except three marginal cases. Overall performance is more biased towards female names.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the magenta line's peak near 0.45 around 2005 reflect calculated error rates from template #5 formulas?",
        "answer": "Yes, the y-axis shows the calculated error rates for each error type as described in their corresponding formulas, and the peak represents the highest bias against female names in recent decades using template #5.",
        "visual_anchor": "magenta line peak at approximately 0.45 between 1995 and 2015",
        "text_evidence": "The y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
        "query_type": "value_context"
      },
      {
        "query": "What explains the cyan line's sharp upward trend starting around 1975, given the unweighted case methodology?",
        "answer": "The increase in male error rates after 1975 reflects changes in baby naming patterns over time, captured by running models over the 139-year census data using the unweighted case with template #5.",
        "visual_anchor": "solid cyan line showing steep increase from approximately 0.15 to 0.35 between 1975 and 2015",
        "text_evidence": "Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "dashed magenta line representing female error rates",
      "solid cyan line representing male error rates",
      "y-axis ranging from 0 to 0.5 showing error rates",
      "x-axis spanning 1875 to 2015 showing years",
      "peak in female line around 1995-2015 reaching approximately 0.45",
      "relatively flat male line staying below 0.2 until 1975"
    ]
  },
  {
    "doc_id": "2005.07293",
    "figure_id": "2005.07293_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
    "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"diagram\",\n  \"visual_elements\": [\n    \"Left panel labeled 'Equality' showing three figures on equal-height boxes\",\n    \"Right panel labeled 'Equity' showing three figures on different-height boxes\",\n    \"Apple tree with red apples at top\",\n    \"Blue, purple, and yellow colored figures\",\n    \"Mathematical notation p(Y|A) on left, p(Ŷ|A) + p(Y|A) on right\",\n    \"Brown boxes of varying heights under each figure\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?\",\n      \"answer\": \"The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.\",\n      \"visual_anchor\": \"Right panel 'Equity' showing yellow figure standing on tallest brown box\",\n      \"text_evidence\": \"Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"Why does the right panel's formalization include both p(Ŷ|A=blue) and p(Y|A=blue) terms unlike the left panel's single term?\",\n      \"answer\": \"The right panel formalizes equity by learning existing biases from historical data p(Y|A) and compensating for them in model predictions p(Ŷ|A), whereas equality only considers predictions without accounting for pre-existing biases.\",\n      \"visual_anchor\": \"Mathematical notation showing p(Ŷ|A=blue) + p(Y|A=blue) under 'Equity' versus p(Ŷ|A=blue) under 'Equality'\",\n      \"text_evidence\": \"To operationalize equity, we learn the existing biases from the historical data and compensate for them in the predictions generated by the model.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the equal-height box arrangement in the left panel address the pre-existing biases that the proposed definition aims to account for?\",\n      \"answer\": \"No, the equal-height boxes represent equality that does not consider existing biases in the data, whereas the proposed equity definition attempts to make equitable decisions accounting for these pre-existing biases.\",\n      \"visual_anchor\": \"Left panel 'Equality' showing all three figures standing on identical-height brown boxes\",\n      \"text_evidence\": \"We propose a new fairness definition, motivated by the principle of equity, that considers existing biases in the data and attempts to make equitable decisions that account for these pre\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "diagram",
      "visual_elements": [
        "Left panel labeled 'Equality' showing three figures on equal-height boxes",
        "Right panel labeled 'Equity' showing three figures on different-height boxes",
        "Apple tree with red apples at top",
        "Blue, purple, and yellow colored figures",
        "Mathematical notation p(Y|A) on left, p(Ŷ|A) + p(Y|A) on right",
        "Brown boxes of varying heights under each figure"
      ],
      "queries": [
        {
          "query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
          "answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
          "visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
          "text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
          "query_type": "visual_definition"
        },
        {
          "query": "Why does the right panel's formalization include both p(Ŷ|A=blue) and p(Y|A=blue) terms unlike the left panel's single term?",
          "answer": "The right panel formalizes equity by learning existing biases from historical data p(Y|A) and compensating for them in model predictions p(Ŷ|A), whereas equality only considers predictions without accounting for pre-existing biases.",
          "visual_anchor": "Mathematical notation showing p(Ŷ|A=blue) + p(Y|A=blue) under 'Equity' versus p(Ŷ|A=blue) under 'Equality'",
          "text_evidence": "To operationalize equity, we learn the existing biases from the historical data and compensate for them in the predictions generated by the model.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the equal-height box arrangement in the left panel address the pre-existing biases that the proposed definition aims to account for?",
          "answer": "No, the equal-height boxes represent equality that does not consider existing biases in the data, whereas the proposed equity definition attempts to make equitable decisions accounting for these pre-existing biases.",
          "visual_anchor": "Left panel 'Equality' showing all three figures standing on identical-height brown boxes",
          "text_evidence": "We propose a new fairness definition, motivated by the principle of equity, that considers existing biases in the data and attempts to make equitable decisions that account for these pre",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "How does the different-height box arrangement under the yellow figure in the right panel operationalize compensation for historically disadvantaged groups?",
        "answer": "The taller box under the yellow figure represents resource distribution to overcome obstacles and raise opportunities for access, compensating historically disadvantaged groups so they can reach the apples like others.",
        "visual_anchor": "Right panel 'Equity' showing yellow figure standing on tallest brown box",
        "text_evidence": "Equity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals.",
        "query_type": "visual_definition"
      },
      {
        "query": "Why does the right panel's formalization include both p(Ŷ|A=blue) and p(Y|A=blue) terms unlike the left panel's single term?",
        "answer": "The right panel formalizes equity by learning existing biases from historical data p(Y|A) and compensating for them in model predictions p(Ŷ|A), whereas equality only considers predictions without accounting for pre-existing biases.",
        "visual_anchor": "Mathematical notation showing p(Ŷ|A=blue) + p(Y|A=blue) under 'Equity' versus p(Ŷ|A=blue) under 'Equality'",
        "text_evidence": "To operationalize equity, we learn the existing biases from the historical data and compensate for them in the predictions generated by the model.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the equal-height box arrangement in the left panel address the pre-existing biases that the proposed definition aims to account for?",
        "answer": "No, the equal-height boxes represent equality that does not consider existing biases in the data, whereas the proposed equity definition attempts to make equitable decisions accounting for these pre-existing biases.",
        "visual_anchor": "Left panel 'Equality' showing all three figures standing on identical-height brown boxes",
        "text_evidence": "We propose a new fairness definition, motivated by the principle of equity, that considers existing biases in the data and attempts to make equitable decisions that account for these pre",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "diagram",
    "visual_elements": [
      "Left panel labeled 'Equality' showing three figures on equal-height boxes",
      "Right panel labeled 'Equity' showing three figures on different-height boxes",
      "Apple tree with red apples at top",
      "Blue, purple, and yellow colored figures",
      "Mathematical notation p(Y|A) on left, p(Ŷ|A) + p(Y|A) on right",
      "Brown boxes of varying heights under each figure"
    ]
  },
  {
    "doc_id": "2005.07293",
    "figure_id": "2005.07293_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg",
    "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Blue line with square markers labeled Equity\", \"Red line with X markers labeled Parity\", \"Green line with triangle markers labeled Classifier\", \"X-axis labeled Beta ranging from 0 to 0.9\", \"Y-axis labeled %(Accuracy) ranging from 76 to 86\", \"Title 'Adult'\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue Equity line drop from 85% to 79% as beta increases, while Parity and Classifier remain stable?\",\n      \"answer\": \"The Equity method trades off accuracy for fairness as beta increases, while Parity and Classifier maintain accuracy by not effectively reducing disparities among demographics, as measured by fairness gain comparing historical and future predictive outcomes.\",\n      \"visual_anchor\": \"Blue line with square markers declining from approximately 85% at beta=0.1 to 79% at beta=0.9\",\n      \"text_evidence\": \"we measure how effective a method was in reducing disparities among demographics compared to a classifier with no fairness constraint. Note how this measure is similar to the statistical parity except considering both the history and future predictive outcome.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the near-flat red and green lines at 84% across all beta values support consistent performance across random splits?\",\n      \"answer\": \"Yes, the flat Parity and Classifier lines demonstrate consistent performance. Each point represents the average value of 10 experiments performed on the same 10 random splits across different beta values, ensuring robustness.\",\n      \"visual_anchor\": \"Red line with X markers and green line with triangle markers both maintaining approximately 84-85% accuracy from beta=0 to beta=0.9\",\n      \"text_evidence\": \"Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different beta values.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What explains the blue Equity curve's steeper decline after beta=0.4 compared to the gradual drop before that threshold?\",\n      \"answer\": \"The steeper decline after beta=0.4 indicates increasing fairness constraint weight, causing more aggressive accuracy sacrifice. Robustness across 10 random train, validation, and test splits ensures the significance of this observed trend.\",\n      \"visual_anchor\": \"Blue line showing gradual decline from beta=0 to 0.4 (85% to 82%), then steeper descent from beta=0.4 to 0.9 (82% to 79%)\",\n      \"text_evidence\": \"For robustness purposes, each of our experiments were performed on 10 random train, validation, and test splits for each of the datasets and the significance of our hypotheses were reported accordingly.\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue line with square markers labeled Equity",
        "Red line with X markers labeled Parity",
        "Green line with triangle markers labeled Classifier",
        "X-axis labeled Beta ranging from 0 to 0.9",
        "Y-axis labeled %(Accuracy) ranging from 76 to 86",
        "Title 'Adult'"
      ],
      "queries": [
        {
          "query": "Why does the blue Equity line drop from 85% to 79% as beta increases, while Parity and Classifier remain stable?",
          "answer": "The Equity method trades off accuracy for fairness as beta increases, while Parity and Classifier maintain accuracy by not effectively reducing disparities among demographics, as measured by fairness gain comparing historical and future predictive outcomes.",
          "visual_anchor": "Blue line with square markers declining from approximately 85% at beta=0.1 to 79% at beta=0.9",
          "text_evidence": "we measure how effective a method was in reducing disparities among demographics compared to a classifier with no fairness constraint. Note how this measure is similar to the statistical parity except considering both the history and future predictive outcome.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the near-flat red and green lines at 84% across all beta values support consistent performance across random splits?",
          "answer": "Yes, the flat Parity and Classifier lines demonstrate consistent performance. Each point represents the average value of 10 experiments performed on the same 10 random splits across different beta values, ensuring robustness.",
          "visual_anchor": "Red line with X markers and green line with triangle markers both maintaining approximately 84-85% accuracy from beta=0 to beta=0.9",
          "text_evidence": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different beta values.",
          "query_type": "value_context"
        },
        {
          "query": "What explains the blue Equity curve's steeper decline after beta=0.4 compared to the gradual drop before that threshold?",
          "answer": "The steeper decline after beta=0.4 indicates increasing fairness constraint weight, causing more aggressive accuracy sacrifice. Robustness across 10 random train, validation, and test splits ensures the significance of this observed trend.",
          "visual_anchor": "Blue line showing gradual decline from beta=0 to 0.4 (85% to 82%), then steeper descent from beta=0.4 to 0.9 (82% to 79%)",
          "text_evidence": "For robustness purposes, each of our experiments were performed on 10 random train, validation, and test splits for each of the datasets and the significance of our hypotheses were reported accordingly.",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue Equity line drop from 85% to 79% as beta increases, while Parity and Classifier remain stable?",
        "answer": "The Equity method trades off accuracy for fairness as beta increases, while Parity and Classifier maintain accuracy by not effectively reducing disparities among demographics, as measured by fairness gain comparing historical and future predictive outcomes.",
        "visual_anchor": "Blue line with square markers declining from approximately 85% at beta=0.1 to 79% at beta=0.9",
        "text_evidence": "we measure how effective a method was in reducing disparities among demographics compared to a classifier with no fairness constraint. Note how this measure is similar to the statistical parity except considering both the history and future predictive outcome.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the near-flat red and green lines at 84% across all beta values support consistent performance across random splits?",
        "answer": "Yes, the flat Parity and Classifier lines demonstrate consistent performance. Each point represents the average value of 10 experiments performed on the same 10 random splits across different beta values, ensuring robustness.",
        "visual_anchor": "Red line with X markers and green line with triangle markers both maintaining approximately 84-85% accuracy from beta=0 to beta=0.9",
        "text_evidence": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different beta values.",
        "query_type": "value_context"
      },
      {
        "query": "What explains the blue Equity curve's steeper decline after beta=0.4 compared to the gradual drop before that threshold?",
        "answer": "The steeper decline after beta=0.4 indicates increasing fairness constraint weight, causing more aggressive accuracy sacrifice. Robustness across 10 random train, validation, and test splits ensures the significance of this observed trend.",
        "visual_anchor": "Blue line showing gradual decline from beta=0 to 0.4 (85% to 82%), then steeper descent from beta=0.4 to 0.9 (82% to 79%)",
        "text_evidence": "For robustness purposes, each of our experiments were performed on 10 random train, validation, and test splits for each of the datasets and the significance of our hypotheses were reported accordingly.",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue line with square markers labeled Equity",
      "Red line with X markers labeled Parity",
      "Green line with triangle markers labeled Classifier",
      "X-axis labeled Beta ranging from 0 to 0.9",
      "Y-axis labeled %(Accuracy) ranging from 76 to 86",
      "Title 'Adult'"
    ]
  },
  {
    "doc_id": "2005.07293",
    "figure_id": "2005.07293_fig_9",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg",
    "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue line labeled Equity\",\n    \"Red line labeled Parity\",\n    \"Green line labeled Classifier\",\n    \"Adult dataset plot on right\",\n    \"Y-axis showing %(Bias) from 16 to 21\",\n    \"X-axis showing Iteration from 0 to 9\",\n    \"All three lines declining from iteration 0 to 9\",\n    \"Lines starting near 20% bias at iteration 0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the blue Equity line in Adult dataset decline more steeply than the red Parity line across iterations?\",\n      \"answer\": \"The steeper decline of Equity demonstrates its superior effectiveness in reducing bias over iterations, as the feedback loop simulation reveals Equity outperforms Parity in bias reduction.\",\n      \"visual_anchor\": \"Blue Equity line showing steeper decline compared to red Parity line from iteration 0 to 9\",\n      \"text_evidence\": \"As expected higher $\\\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Do the averaged points on the Adult plot confirm consistent results across the ten random split experiments conducted?\",\n      \"answer\": \"Yes, each point represents the average of 10 experiments performed on 10 random splits, ensuring consistency. The same random split sets were used across different beta values for fair comparison.\",\n      \"visual_anchor\": \"Data points on Adult plot showing smooth declining trends from iteration 0 to 9\",\n      \"text_evidence\": \"Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\\\beta$ values.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the green Classifier line starting near 20% bias validate the feedback loop phenomenon simulation in Adult dataset?\",\n      \"answer\": \"Yes, the Classifier line starting at approximately 20% bias and declining through iterations demonstrates the simulated feedback loop phenomenon, showing how bias reduces over time even without explicit fairness objectives.\",\n      \"visual_anchor\": \"Green Classifier line beginning at approximately 20% bias at iteration 0 in Adult plot\",\n      \"text_evidence\": \"Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue line labeled Equity",
        "Red line labeled Parity",
        "Green line labeled Classifier",
        "Adult dataset plot on right",
        "Y-axis showing %(Bias) from 16 to 21",
        "X-axis showing Iteration from 0 to 9",
        "All three lines declining from iteration 0 to 9",
        "Lines starting near 20% bias at iteration 0"
      ],
      "queries": [
        {
          "query": "Why does the blue Equity line in Adult dataset decline more steeply than the red Parity line across iterations?",
          "answer": "The steeper decline of Equity demonstrates its superior effectiveness in reducing bias over iterations, as the feedback loop simulation reveals Equity outperforms Parity in bias reduction.",
          "visual_anchor": "Blue Equity line showing steeper decline compared to red Parity line from iteration 0 to 9",
          "text_evidence": "As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Do the averaged points on the Adult plot confirm consistent results across the ten random split experiments conducted?",
          "answer": "Yes, each point represents the average of 10 experiments performed on 10 random splits, ensuring consistency. The same random split sets were used across different beta values for fair comparison.",
          "visual_anchor": "Data points on Adult plot showing smooth declining trends from iteration 0 to 9",
          "text_evidence": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.",
          "query_type": "value_context"
        },
        {
          "query": "Does the green Classifier line starting near 20% bias validate the feedback loop phenomenon simulation in Adult dataset?",
          "answer": "Yes, the Classifier line starting at approximately 20% bias and declining through iterations demonstrates the simulated feedback loop phenomenon, showing how bias reduces over time even without explicit fairness objectives.",
          "visual_anchor": "Green Classifier line beginning at approximately 20% bias at iteration 0 in Adult plot",
          "text_evidence": "Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the blue Equity line in Adult dataset decline more steeply than the red Parity line across iterations?",
        "answer": "The steeper decline of Equity demonstrates its superior effectiveness in reducing bias over iterations, as the feedback loop simulation reveals Equity outperforms Parity in bias reduction.",
        "visual_anchor": "Blue Equity line showing steeper decline compared to red Parity line from iteration 0 to 9",
        "text_evidence": "As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Do the averaged points on the Adult plot confirm consistent results across the ten random split experiments conducted?",
        "answer": "Yes, each point represents the average of 10 experiments performed on 10 random splits, ensuring consistency. The same random split sets were used across different beta values for fair comparison.",
        "visual_anchor": "Data points on Adult plot showing smooth declining trends from iteration 0 to 9",
        "text_evidence": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.",
        "query_type": "value_context"
      },
      {
        "query": "Does the green Classifier line starting near 20% bias validate the feedback loop phenomenon simulation in Adult dataset?",
        "answer": "Yes, the Classifier line starting at approximately 20% bias and declining through iterations demonstrates the simulated feedback loop phenomenon, showing how bias reduces over time even without explicit fairness objectives.",
        "visual_anchor": "Green Classifier line beginning at approximately 20% bias at iteration 0 in Adult plot",
        "text_evidence": "Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue line labeled Equity",
      "Red line labeled Parity",
      "Green line labeled Classifier",
      "Adult dataset plot on right",
      "Y-axis showing %(Bias) from 16 to 21",
      "X-axis showing Iteration from 0 to 9",
      "All three lines declining from iteration 0 to 9",
      "Lines starting near 20% bias at iteration 0"
    ]
  },
  {
    "doc_id": "2005.07293",
    "figure_id": "2005.07293_fig_13",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg",
    "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Blue violin plot for Parity\", \"Orange violin plot for Equity\", \"Median lines in both violins\", \"Y-axis scale from 0 to 4\", \"Human Ratings for Scenario 3 label\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the orange Equity distribution show higher median ratings than the blue Parity distribution in Scenario 3?\",\n      \"answer\": \"The Mann-Whitney U test demonstrates that Equity is significantly more effective at reducing bias in the feedback loop compared to Parity across different iterations, which explains the higher human fairness ratings for Equity.\",\n      \"visual_anchor\": \"Orange violin plot (Equity) with higher median line compared to blue violin plot (Parity)\",\n      \"text_evidence\": \"Table 4 shows the significance of these results for COM-PAS and Adult datasets for β value of 0.5 for different iterations supporting our hypothesis.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the Equity violin's concentration near rating 4 align with its performance superiority over Parity and Classifier losses?\",\n      \"answer\": \"Yes, the orange Equity violin shows higher ratings clustered near 4 (Completely Fair), which is consistent with Table 4's Mann-Whitney U test results demonstrating Equity's effectiveness in reducing bias compared to both Parity and Classifier losses.\",\n      \"visual_anchor\": \"Orange violin plot (Equity) showing distribution concentrated at upper end near rating 4\",\n      \"text_evidence\": \"Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the blue Parity ratings near 2 relate to the earlier finding about beta value zero?\",\n      \"answer\": \"The lower Parity ratings (blue violin centered around 2-3) reflect weaker fairness perception. This is consistent with findings that β = 0 (representing no equity consideration) produces different fairness outcomes than incorporating equity measures.\",\n      \"visual_anchor\": \"Blue violin plot (Parity) with median and distribution centered around rating 2-3\",\n      \"text_evidence\": \"This is consistent with our earlier finding that β = 0 .\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue violin plot for Parity",
        "Orange violin plot for Equity",
        "Median lines in both violins",
        "Y-axis scale from 0 to 4",
        "Human Ratings for Scenario 3 label"
      ],
      "queries": [
        {
          "query": "Why does the orange Equity distribution show higher median ratings than the blue Parity distribution in Scenario 3?",
          "answer": "The Mann-Whitney U test demonstrates that Equity is significantly more effective at reducing bias in the feedback loop compared to Parity across different iterations, which explains the higher human fairness ratings for Equity.",
          "visual_anchor": "Orange violin plot (Equity) with higher median line compared to blue violin plot (Parity)",
          "text_evidence": "Table 4 shows the significance of these results for COM-PAS and Adult datasets for β value of 0.5 for different iterations supporting our hypothesis.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the Equity violin's concentration near rating 4 align with its performance superiority over Parity and Classifier losses?",
          "answer": "Yes, the orange Equity violin shows higher ratings clustered near 4 (Completely Fair), which is consistent with Table 4's Mann-Whitney U test results demonstrating Equity's effectiveness in reducing bias compared to both Parity and Classifier losses.",
          "visual_anchor": "Orange violin plot (Equity) showing distribution concentrated at upper end near rating 4",
          "text_evidence": "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.",
          "query_type": "value_context"
        },
        {
          "query": "How do the blue Parity ratings near 2 relate to the earlier finding about beta value zero?",
          "answer": "The lower Parity ratings (blue violin centered around 2-3) reflect weaker fairness perception. This is consistent with findings that β = 0 (representing no equity consideration) produces different fairness outcomes than incorporating equity measures.",
          "visual_anchor": "Blue violin plot (Parity) with median and distribution centered around rating 2-3",
          "text_evidence": "This is consistent with our earlier finding that β = 0 .",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the orange Equity distribution show higher median ratings than the blue Parity distribution in Scenario 3?",
        "answer": "The Mann-Whitney U test demonstrates that Equity is significantly more effective at reducing bias in the feedback loop compared to Parity across different iterations, which explains the higher human fairness ratings for Equity.",
        "visual_anchor": "Orange violin plot (Equity) with higher median line compared to blue violin plot (Parity)",
        "text_evidence": "Table 4 shows the significance of these results for COM-PAS and Adult datasets for β value of 0.5 for different iterations supporting our hypothesis.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the Equity violin's concentration near rating 4 align with its performance superiority over Parity and Classifier losses?",
        "answer": "Yes, the orange Equity violin shows higher ratings clustered near 4 (Completely Fair), which is consistent with Table 4's Mann-Whitney U test results demonstrating Equity's effectiveness in reducing bias compared to both Parity and Classifier losses.",
        "visual_anchor": "Orange violin plot (Equity) showing distribution concentrated at upper end near rating 4",
        "text_evidence": "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.",
        "query_type": "value_context"
      },
      {
        "query": "How do the blue Parity ratings near 2 relate to the earlier finding about beta value zero?",
        "answer": "The lower Parity ratings (blue violin centered around 2-3) reflect weaker fairness perception. This is consistent with findings that β = 0 (representing no equity consideration) produces different fairness outcomes than incorporating equity measures.",
        "visual_anchor": "Blue violin plot (Parity) with median and distribution centered around rating 2-3",
        "text_evidence": "This is consistent with our earlier finding that β = 0 .",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue violin plot for Parity",
      "Orange violin plot for Equity",
      "Median lines in both violins",
      "Y-axis scale from 0 to 4",
      "Human Ratings for Scenario 3 label"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_2",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg",
    "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Box plots for Origin, Gender, Religion, and Profession categories\",\n    \"Blue boxes representing Negative sentiment\",\n    \"Red boxes representing Positive sentiment\",\n    \"Y-axis showing Sentiment percentage (0-30%)\",\n    \"Outlier points marked with asterisks\",\n    \"Profession category showing highest median and largest box span\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the Profession category's blue box spanning roughly 2-17% demonstrate the large variation in negative triples for groups?\",\n      \"answer\": \"Yes, the Profession category shows the largest span among blue boxes, illustrating substantial disparity in negative triple counts. This visual evidence directly supports the observation about large variation within the same category.\",\n      \"visual_anchor\": \"Blue box for Profession category with span from approximately 2% to 17%\",\n      \"text_evidence\": \"We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why do the red outlier points above 20% in Origin and Profession categories illustrate overgeneralization severity?\",\n      \"answer\": \"These outlier points represent target groups with exceptionally high positive sentiment percentages, demonstrating extreme cases of biased generalizations. The outliers directly show how certain groups receive disproportionately positive characterizations in the knowledge bases.\",\n      \"visual_anchor\": \"Red outlier asterisk points above 20% in Origin and Profession categories\",\n      \"text_evidence\": \"We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"How does the median positive sentiment around 5-7% for Religion relate to examining whether targets are perceived positively?\",\n      \"answer\": \"The Religion category's red box shows a median around 5-7%, indicating that religious groups receive relatively low positive sentiment in statements. This quantifies how targets like religious identities are perceived in the overgeneralization analysis covering stereotyping and favoritism.\",\n      \"visual_anchor\": \"Red box for Religion category with median approximately 5-7%\",\n      \"text_evidence\": \"We consider overgeneralization that directly examines whether targets such as \\\"lawyer\\\" or \\\"lady\\\" are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Box plots for Origin, Gender, Religion, and Profession categories",
        "Blue boxes representing Negative sentiment",
        "Red boxes representing Positive sentiment",
        "Y-axis showing Sentiment percentage (0-30%)",
        "Outlier points marked with asterisks",
        "Profession category showing highest median and largest box span"
      ],
      "queries": [
        {
          "query": "Does the Profession category's blue box spanning roughly 2-17% demonstrate the large variation in negative triples for groups?",
          "answer": "Yes, the Profession category shows the largest span among blue boxes, illustrating substantial disparity in negative triple counts. This visual evidence directly supports the observation about large variation within the same category.",
          "visual_anchor": "Blue box for Profession category with span from approximately 2% to 17%",
          "text_evidence": "We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why do the red outlier points above 20% in Origin and Profession categories illustrate overgeneralization severity?",
          "answer": "These outlier points represent target groups with exceptionally high positive sentiment percentages, demonstrating extreme cases of biased generalizations. The outliers directly show how certain groups receive disproportionately positive characterizations in the knowledge bases.",
          "visual_anchor": "Red outlier asterisk points above 20% in Origin and Profession categories",
          "text_evidence": "We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "How does the median positive sentiment around 5-7% for Religion relate to examining whether targets are perceived positively?",
          "answer": "The Religion category's red box shows a median around 5-7%, indicating that religious groups receive relatively low positive sentiment in statements. This quantifies how targets like religious identities are perceived in the overgeneralization analysis covering stereotyping and favoritism.",
          "visual_anchor": "Red box for Religion category with median approximately 5-7%",
          "text_evidence": "We consider overgeneralization that directly examines whether targets such as \"lawyer\" or \"lady\" are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the Profession category's blue box spanning roughly 2-17% demonstrate the large variation in negative triples for groups?",
        "answer": "Yes, the Profession category shows the largest span among blue boxes, illustrating substantial disparity in negative triple counts. This visual evidence directly supports the observation about large variation within the same category.",
        "visual_anchor": "Blue box for Profession category with span from approximately 2% to 17%",
        "text_evidence": "We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why do the red outlier points above 20% in Origin and Profession categories illustrate overgeneralization severity?",
        "answer": "These outlier points represent target groups with exceptionally high positive sentiment percentages, demonstrating extreme cases of biased generalizations. The outliers directly show how certain groups receive disproportionately positive characterizations in the knowledge bases.",
        "visual_anchor": "Red outlier asterisk points above 20% in Origin and Profession categories",
        "text_evidence": "We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "How does the median positive sentiment around 5-7% for Religion relate to examining whether targets are perceived positively?",
        "answer": "The Religion category's red box shows a median around 5-7%, indicating that religious groups receive relatively low positive sentiment in statements. This quantifies how targets like religious identities are perceived in the overgeneralization analysis covering stereotyping and favoritism.",
        "visual_anchor": "Red box for Religion category with median approximately 5-7%",
        "text_evidence": "We consider overgeneralization that directly examines whether targets such as \"lawyer\" or \"lady\" are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Box plots for Origin, Gender, Religion, and Profession categories",
      "Blue boxes representing Negative sentiment",
      "Red boxes representing Positive sentiment",
      "Y-axis showing Sentiment percentage (0-30%)",
      "Outlier points marked with asterisks",
      "Profession category showing highest median and largest box span"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_5",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg",
    "caption": "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral. Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning. We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the “Religion” category and find similar polarized perceptions of certain groups, despite a much larger percentage range for GenericsKB.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Scatter plot with x-axis 'Negative Regard (%)' and y-axis 'Positive Regard (%)'\",\n    \"Four colored regions: green (Negligible Bias), cyan (Favoritism), purple (Prejudice), magenta (Both)\",\n    \"Profession labels: eco, doctor, nurse, teacher, psychologist, physician, priest, chemist, electrician, hairdresser, sociologist, professor, comedian, policeman, actor, judge, politician, fisherman, performing_artist, detective, advocate, librarian, pharmacist\",\n    \"Region boundaries separating favoritism (left), prejudice (bottom-left), both (bottom-right), and negligible bias (top-right)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the purple prejudice region placement of electrician and hairdresser align with polarized perceptions found in GenericsKB?\",\n      \"answer\": \"Yes, the figure shows electrician and hairdresser in the prejudice region (low positive, low negative regard), consistent with polarized perceptions observed across both ConceptNet and GenericsKB for certain groups.\",\n      \"visual_anchor\": \"Purple prejudice region in bottom-left containing electrician and hairdresser labels\",\n      \"text_evidence\": \"We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the 'Religion' category and find similar polarized perceptions of certain groups\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does eco appear in the cyan favoritism region with approximately 12% positive regard despite low negative regard?\",\n      \"answer\": \"Eco shows high positive regard with minimal negative regard, placing it in the favoritism-leaning region. Higher positive regard percentages indicate favoritism-leaning targets according to the regard measure labeling system.\",\n      \"visual_anchor\": \"Cyan favoritism region containing eco label at approximately 12% positive regard and 2% negative regard\",\n      \"text_evidence\": \"Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the magenta Both region placement of politician relate to targets showing simultaneous favoritism and prejudice?\",\n      \"answer\": \"Politician appears in the magenta Both region at high negative and positive regard percentages, indicating it receives both prejudice and favoritism. This region specifically captures targets from the Profession and Religion categories that exhibit dual biased perceptions.\",\n      \"visual_anchor\": \"Magenta Both region in bottom-right corner containing politician label at approximately 18% negative and 1% positive regard\",\n      \"text_evidence\": \"Examples of targets from the 'Profession' and 'Religion' categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral\",\n      \"query_type\": \"visual_definition\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Scatter plot with x-axis 'Negative Regard (%)' and y-axis 'Positive Regard (%)'",
        "Four colored regions: green (Negligible Bias), cyan (Favoritism), purple (Prejudice), magenta (Both)",
        "Profession labels: eco, doctor, nurse, teacher, psychologist, physician, priest, chemist, electrician, hairdresser, sociologist, professor, comedian, policeman, actor, judge, politician, fisherman, performing_artist, detective, advocate, librarian, pharmacist",
        "Region boundaries separating favoritism (left), prejudice (bottom-left), both (bottom-right), and negligible bias (top-right)"
      ],
      "queries": [
        {
          "query": "Does the purple prejudice region placement of electrician and hairdresser align with polarized perceptions found in GenericsKB?",
          "answer": "Yes, the figure shows electrician and hairdresser in the prejudice region (low positive, low negative regard), consistent with polarized perceptions observed across both ConceptNet and GenericsKB for certain groups.",
          "visual_anchor": "Purple prejudice region in bottom-left containing electrician and hairdresser labels",
          "text_evidence": "We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the 'Religion' category and find similar polarized perceptions of certain groups",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does eco appear in the cyan favoritism region with approximately 12% positive regard despite low negative regard?",
          "answer": "Eco shows high positive regard with minimal negative regard, placing it in the favoritism-leaning region. Higher positive regard percentages indicate favoritism-leaning targets according to the regard measure labeling system.",
          "visual_anchor": "Cyan favoritism region containing eco label at approximately 12% positive regard and 2% negative regard",
          "text_evidence": "Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning",
          "query_type": "value_context"
        },
        {
          "query": "How does the magenta Both region placement of politician relate to targets showing simultaneous favoritism and prejudice?",
          "answer": "Politician appears in the magenta Both region at high negative and positive regard percentages, indicating it receives both prejudice and favoritism. This region specifically captures targets from the Profession and Religion categories that exhibit dual biased perceptions.",
          "visual_anchor": "Magenta Both region in bottom-right corner containing politician label at approximately 18% negative and 1% positive regard",
          "text_evidence": "Examples of targets from the 'Profession' and 'Religion' categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral",
          "query_type": "visual_definition"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the purple prejudice region placement of electrician and hairdresser align with polarized perceptions found in GenericsKB?",
        "answer": "Yes, the figure shows electrician and hairdresser in the prejudice region (low positive, low negative regard), consistent with polarized perceptions observed across both ConceptNet and GenericsKB for certain groups.",
        "visual_anchor": "Purple prejudice region in bottom-left containing electrician and hairdresser labels",
        "text_evidence": "We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the 'Religion' category and find similar polarized perceptions of certain groups",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does eco appear in the cyan favoritism region with approximately 12% positive regard despite low negative regard?",
        "answer": "Eco shows high positive regard with minimal negative regard, placing it in the favoritism-leaning region. Higher positive regard percentages indicate favoritism-leaning targets according to the regard measure labeling system.",
        "visual_anchor": "Cyan favoritism region containing eco label at approximately 12% positive regard and 2% negative regard",
        "text_evidence": "Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning",
        "query_type": "value_context"
      },
      {
        "query": "How does the magenta Both region placement of politician relate to targets showing simultaneous favoritism and prejudice?",
        "answer": "Politician appears in the magenta Both region at high negative and positive regard percentages, indicating it receives both prejudice and favoritism. This region specifically captures targets from the Profession and Religion categories that exhibit dual biased perceptions.",
        "visual_anchor": "Magenta Both region in bottom-right corner containing politician label at approximately 18% negative and 1% positive regard",
        "text_evidence": "Examples of targets from the 'Profession' and 'Religion' categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral",
        "query_type": "visual_definition"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Scatter plot with x-axis 'Negative Regard (%)' and y-axis 'Positive Regard (%)'",
      "Four colored regions: green (Negligible Bias), cyan (Favoritism), purple (Prejudice), magenta (Both)",
      "Profession labels: eco, doctor, nurse, teacher, psychologist, physician, priest, chemist, electrician, hairdresser, sociologist, professor, comedian, policeman, actor, judge, politician, fisherman, performing_artist, detective, advocate, librarian, pharmacist",
      "Region boundaries separating favoritism (left), prejudice (bottom-left), both (bottom-right), and negligible bias (top-right)"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_9",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg",
    "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Gender subplot with negative (purple) and positive (blue) sentiment lines showing overgeneralization\",\n    \"ConceptNet Sentiment bar chart comparing woman and man with action vs. description bars\",\n    \"Text annotations: 'Slut is related to woman. Beauty is related to woman.' and 'Cock is related to man. Man is related to integrity.'\",\n    \"Sentiment percentage scale on y-axis reaching up to 30%\",\n    \"Four category panels: Origin, Gender, Religion, Profession\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do both purple and blue sentiment lines in the Gender panel peak above 5%, demonstrating overgeneralization toward both target groups?\",\n      \"answer\": \"The Gender category demonstrates extreme overgeneralization where both target groups (woman and man) show elevated sentiment associations, as evidenced by the overlapping distributions and accompanying biased concept associations like 'Slut is related to woman' and 'Man is related to integrity.'\",\n      \"visual_anchor\": \"Purple (negative) and blue (positive) sentiment lines both peaking above 5% in Gender subplot\",\n      \"text_evidence\": \"In \\\"Gender\\\" category both target groups are overgeneralized, in \\\"Religion\\\" extreme prejudice toward \\\"muslim,\\\" and in \\\"Profession\\\" extreme favoritism toward \\\"teacher\\\" target group.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the ConceptNet Sentiment bar chart showing approximately 2.0 for woman and 2.5 for man support the favoritism toward teacher observed?\",\n      \"answer\": \"No, the ConceptNet Sentiment bar chart compares gender bias (woman vs. man), not profession bias. The favoritism toward 'teacher' is demonstrated separately in the Profession category panel, showing how sentiment analysis can detect different types of bias across categories.\",\n      \"visual_anchor\": \"ConceptNet Sentiment bar chart with red and blue bars at approximately 2.0-2.5 height\",\n      \"text_evidence\": \"Each case is accompanied with an example of negative and positive associations detected by sentiment. Saffron terror is related to hindu. Teacher causes the desire to study.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the extreme prejudice toward muslim relate to the overgeneralization severity shown in the Gender panel's overlapping distributions?\",\n      \"answer\": \"Both demonstrate severe bias manifestations: extreme prejudice shows concentrated negative sentiment toward one group (muslim), while overgeneralization in the Gender panel shows both target groups receiving elevated sentiment associations, illustrating different aspects of how bias appears in word embeddings.\",\n      \"visual_anchor\": \"Gender panel showing overlapping purple and blue sentiment distributions with peaks above 5%\",\n      \"text_evidence\": \"Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgeneralization can be, along with some concrete examples.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Gender subplot with negative (purple) and positive (blue) sentiment lines showing overgeneralization",
        "ConceptNet Sentiment bar chart comparing woman and man with action vs. description bars",
        "Text annotations: 'Slut is related to woman. Beauty is related to woman.' and 'Cock is related to man. Man is related to integrity.'",
        "Sentiment percentage scale on y-axis reaching up to 30%",
        "Four category panels: Origin, Gender, Religion, Profession"
      ],
      "queries": [
        {
          "query": "Why do both purple and blue sentiment lines in the Gender panel peak above 5%, demonstrating overgeneralization toward both target groups?",
          "answer": "The Gender category demonstrates extreme overgeneralization where both target groups (woman and man) show elevated sentiment associations, as evidenced by the overlapping distributions and accompanying biased concept associations like 'Slut is related to woman' and 'Man is related to integrity.'",
          "visual_anchor": "Purple (negative) and blue (positive) sentiment lines both peaking above 5% in Gender subplot",
          "text_evidence": "In \"Gender\" category both target groups are overgeneralized, in \"Religion\" extreme prejudice toward \"muslim,\" and in \"Profession\" extreme favoritism toward \"teacher\" target group.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the ConceptNet Sentiment bar chart showing approximately 2.0 for woman and 2.5 for man support the favoritism toward teacher observed?",
          "answer": "No, the ConceptNet Sentiment bar chart compares gender bias (woman vs. man), not profession bias. The favoritism toward 'teacher' is demonstrated separately in the Profession category panel, showing how sentiment analysis can detect different types of bias across categories.",
          "visual_anchor": "ConceptNet Sentiment bar chart with red and blue bars at approximately 2.0-2.5 height",
          "text_evidence": "Each case is accompanied with an example of negative and positive associations detected by sentiment. Saffron terror is related to hindu. Teacher causes the desire to study.",
          "query_type": "value_context"
        },
        {
          "query": "How does the extreme prejudice toward muslim relate to the overgeneralization severity shown in the Gender panel's overlapping distributions?",
          "answer": "Both demonstrate severe bias manifestations: extreme prejudice shows concentrated negative sentiment toward one group (muslim), while overgeneralization in the Gender panel shows both target groups receiving elevated sentiment associations, illustrating different aspects of how bias appears in word embeddings.",
          "visual_anchor": "Gender panel showing overlapping purple and blue sentiment distributions with peaks above 5%",
          "text_evidence": "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgeneralization can be, along with some concrete examples.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do both purple and blue sentiment lines in the Gender panel peak above 5%, demonstrating overgeneralization toward both target groups?",
        "answer": "The Gender category demonstrates extreme overgeneralization where both target groups (woman and man) show elevated sentiment associations, as evidenced by the overlapping distributions and accompanying biased concept associations like 'Slut is related to woman' and 'Man is related to integrity.'",
        "visual_anchor": "Purple (negative) and blue (positive) sentiment lines both peaking above 5% in Gender subplot",
        "text_evidence": "In \"Gender\" category both target groups are overgeneralized, in \"Religion\" extreme prejudice toward \"muslim,\" and in \"Profession\" extreme favoritism toward \"teacher\" target group.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the ConceptNet Sentiment bar chart showing approximately 2.0 for woman and 2.5 for man support the favoritism toward teacher observed?",
        "answer": "No, the ConceptNet Sentiment bar chart compares gender bias (woman vs. man), not profession bias. The favoritism toward 'teacher' is demonstrated separately in the Profession category panel, showing how sentiment analysis can detect different types of bias across categories.",
        "visual_anchor": "ConceptNet Sentiment bar chart with red and blue bars at approximately 2.0-2.5 height",
        "text_evidence": "Each case is accompanied with an example of negative and positive associations detected by sentiment. Saffron terror is related to hindu. Teacher causes the desire to study.",
        "query_type": "value_context"
      },
      {
        "query": "How does the extreme prejudice toward muslim relate to the overgeneralization severity shown in the Gender panel's overlapping distributions?",
        "answer": "Both demonstrate severe bias manifestations: extreme prejudice shows concentrated negative sentiment toward one group (muslim), while overgeneralization in the Gender panel shows both target groups receiving elevated sentiment associations, illustrating different aspects of how bias appears in word embeddings.",
        "visual_anchor": "Gender panel showing overlapping purple and blue sentiment distributions with peaks above 5%",
        "text_evidence": "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgeneralization can be, along with some concrete examples.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Gender subplot with negative (purple) and positive (blue) sentiment lines showing overgeneralization",
      "ConceptNet Sentiment bar chart comparing woman and man with action vs. description bars",
      "Text annotations: 'Slut is related to woman. Beauty is related to woman.' and 'Cock is related to man. Man is related to integrity.'",
      "Sentiment percentage scale on y-axis reaching up to 30%",
      "Four category panels: Origin, Gender, Religion, Profession"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_13",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg",
    "caption": "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB. We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Box plots for Gender and Profession categories\", \"Number of Statements on y-axis ranging 0-6000\", \"ConceptNet and GenericsKB labels on x-axis\", \"Wider spread boxes for GenericsKB compared to ConceptNet\", \"Outlier points marked with circles\"],\n  \"queries\": [\n    {\n      \"query\": \"Why does the Gender box plot show outliers near 6000 when GenericsKB has much higher variance than ConceptNet?\",\n      \"answer\": \"The outliers near 6000 in the Gender category reflect the much higher variance in GenericsKB compared to ConceptNet, demonstrating severe representation disparities between the two knowledge bases.\",\n      \"visual_anchor\": \"outlier points near 6000 on the Gender box plot\",\n      \"text_evidence\": \"We find that GenericsKB has much higher variance compared to ConceptNet.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the narrower ConceptNet boxes compared to wider GenericsKB boxes support the claim about sentence count range differences?\",\n      \"answer\": \"Yes, the narrower ConceptNet boxes versus wider GenericsKB boxes visually confirm that the number of sentences ranges much more for GenericsKB compared to ConceptNet, as stated in the disparity analysis.\",\n      \"visual_anchor\": \"narrower boxes for ConceptNet versus wider boxes for GenericsKB\",\n      \"text_evidence\": \"We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How do the Profession category boxes illustrate the overgeneralization disparity measured using Equation 4 across different targets?\",\n      \"answer\": \"The Profession category boxes show varying spreads between ConceptNet and GenericsKB, visually demonstrating the disparities amongst targets in terms of overgeneralization, with GenericsKB exhibiting greater variation in representation.\",\n      \"visual_anchor\": \"Profession category box plots showing different spreads\",\n      \"text_evidence\": \"We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Box plots for Gender and Profession categories",
        "Number of Statements on y-axis ranging 0-6000",
        "ConceptNet and GenericsKB labels on x-axis",
        "Wider spread boxes for GenericsKB compared to ConceptNet",
        "Outlier points marked with circles"
      ],
      "queries": [
        {
          "query": "Why does the Gender box plot show outliers near 6000 when GenericsKB has much higher variance than ConceptNet?",
          "answer": "The outliers near 6000 in the Gender category reflect the much higher variance in GenericsKB compared to ConceptNet, demonstrating severe representation disparities between the two knowledge bases.",
          "visual_anchor": "outlier points near 6000 on the Gender box plot",
          "text_evidence": "We find that GenericsKB has much higher variance compared to ConceptNet.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the narrower ConceptNet boxes compared to wider GenericsKB boxes support the claim about sentence count range differences?",
          "answer": "Yes, the narrower ConceptNet boxes versus wider GenericsKB boxes visually confirm that the number of sentences ranges much more for GenericsKB compared to ConceptNet, as stated in the disparity analysis.",
          "visual_anchor": "narrower boxes for ConceptNet versus wider boxes for GenericsKB",
          "text_evidence": "We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How do the Profession category boxes illustrate the overgeneralization disparity measured using Equation 4 across different targets?",
          "answer": "The Profession category boxes show varying spreads between ConceptNet and GenericsKB, visually demonstrating the disparities amongst targets in terms of overgeneralization, with GenericsKB exhibiting greater variation in representation.",
          "visual_anchor": "Profession category box plots showing different spreads",
          "text_evidence": "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the Gender box plot show outliers near 6000 when GenericsKB has much higher variance than ConceptNet?",
        "answer": "The outliers near 6000 in the Gender category reflect the much higher variance in GenericsKB compared to ConceptNet, demonstrating severe representation disparities between the two knowledge bases.",
        "visual_anchor": "outlier points near 6000 on the Gender box plot",
        "text_evidence": "We find that GenericsKB has much higher variance compared to ConceptNet.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the narrower ConceptNet boxes compared to wider GenericsKB boxes support the claim about sentence count range differences?",
        "answer": "Yes, the narrower ConceptNet boxes versus wider GenericsKB boxes visually confirm that the number of sentences ranges much more for GenericsKB compared to ConceptNet, as stated in the disparity analysis.",
        "visual_anchor": "narrower boxes for ConceptNet versus wider boxes for GenericsKB",
        "text_evidence": "We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How do the Profession category boxes illustrate the overgeneralization disparity measured using Equation 4 across different targets?",
        "answer": "The Profession category boxes show varying spreads between ConceptNet and GenericsKB, visually demonstrating the disparities amongst targets in terms of overgeneralization, with GenericsKB exhibiting greater variation in representation.",
        "visual_anchor": "Profession category box plots showing different spreads",
        "text_evidence": "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Box plots for Gender and Profession categories",
      "Number of Statements on y-axis ranging 0-6000",
      "ConceptNet and GenericsKB labels on x-axis",
      "Wider spread boxes for GenericsKB compared to ConceptNet",
      "Outlier points marked with circles"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_17",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg",
    "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Box plots for Negative (blue) and Positive (red) categories\",\n    \"Four demographic categories: Origin, Gender, Religion, Profession\",\n    \"Y-axis showing Regard percentage from 0 to 50\",\n    \"Red boxes showing higher median values than blue boxes across all categories\",\n    \"Variance indicated by box heights and whisker lengths\",\n    \"Outlier points marked as dots above whiskers\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Do the red and blue box plot variances across Origin, Gender, Religion, and Profession demonstrate disparity in overgeneralization?\",\n      \"answer\": \"Yes, the variance in both regard measures shown by the different box heights and whisker lengths across categories indicates disparity in overgeneralization, as COMeT produces inconsistent sentiment patterns across demographic groups.\",\n      \"visual_anchor\": \"Box plot variance shown by different heights and whisker lengths in red (positive) and blue (negative) boxes across all four categories\",\n      \"text_evidence\": \"For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Why does the filtering technique target the variance patterns visible in the red and blue boxes for mitigation?\",\n      \"answer\": \"The filtering technique aims to reduce representational harms by addressing the disparate variance patterns in sentiment and regard across demographic categories, which reflect biased overgeneralization from ConceptNet propagating through COMeT.\",\n      \"visual_anchor\": \"Variance patterns in red positive and blue negative boxes across Origin, Gender, Religion, and Profession categories\",\n      \"text_evidence\": \"To mitigate the observed representational harms in ConceptNet and their effects on downstream tasks, we propose a pre-processing data filtering technique that reduces the effect of existing representational harms in ConceptNet.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the higher median in red boxes compared to blue across all categories confirm COMeT's effectiveness for the case study?\",\n      \"answer\": \"The higher positive regard medians demonstrate COMeT's behavior patterns, making it an appropriate case study for testing the filtering mitigation technique on CSKB completion tasks with observable bias effects.\",\n      \"visual_anchor\": \"Red boxes showing consistently higher median values (around 15-25%) compared to blue boxes (around 3-7%) across all four demographic categories\",\n      \"text_evidence\": \"We apply our mitigation technique on COMeT as a case study. Table 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT. COMeT-Filtered is effective at reducing overgeneralization\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Box plots for Negative (blue) and Positive (red) categories",
        "Four demographic categories: Origin, Gender, Religion, Profession",
        "Y-axis showing Regard percentage from 0 to 50",
        "Red boxes showing higher median values than blue boxes across all categories",
        "Variance indicated by box heights and whisker lengths",
        "Outlier points marked as dots above whiskers"
      ],
      "queries": [
        {
          "query": "Do the red and blue box plot variances across Origin, Gender, Religion, and Profession demonstrate disparity in overgeneralization?",
          "answer": "Yes, the variance in both regard measures shown by the different box heights and whisker lengths across categories indicates disparity in overgeneralization, as COMeT produces inconsistent sentiment patterns across demographic groups.",
          "visual_anchor": "Box plot variance shown by different heights and whisker lengths in red (positive) and blue (negative) boxes across all four categories",
          "text_evidence": "For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Why does the filtering technique target the variance patterns visible in the red and blue boxes for mitigation?",
          "answer": "The filtering technique aims to reduce representational harms by addressing the disparate variance patterns in sentiment and regard across demographic categories, which reflect biased overgeneralization from ConceptNet propagating through COMeT.",
          "visual_anchor": "Variance patterns in red positive and blue negative boxes across Origin, Gender, Religion, and Profession categories",
          "text_evidence": "To mitigate the observed representational harms in ConceptNet and their effects on downstream tasks, we propose a pre-processing data filtering technique that reduces the effect of existing representational harms in ConceptNet.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the higher median in red boxes compared to blue across all categories confirm COMeT's effectiveness for the case study?",
          "answer": "The higher positive regard medians demonstrate COMeT's behavior patterns, making it an appropriate case study for testing the filtering mitigation technique on CSKB completion tasks with observable bias effects.",
          "visual_anchor": "Red boxes showing consistently higher median values (around 15-25%) compared to blue boxes (around 3-7%) across all four demographic categories",
          "text_evidence": "We apply our mitigation technique on COMeT as a case study. Table 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT. COMeT-Filtered is effective at reducing overgeneralization",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Do the red and blue box plot variances across Origin, Gender, Religion, and Profession demonstrate disparity in overgeneralization?",
        "answer": "Yes, the variance in both regard measures shown by the different box heights and whisker lengths across categories indicates disparity in overgeneralization, as COMeT produces inconsistent sentiment patterns across demographic groups.",
        "visual_anchor": "Box plot variance shown by different heights and whisker lengths in red (positive) and blue (negative) boxes across all four categories",
        "text_evidence": "For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Why does the filtering technique target the variance patterns visible in the red and blue boxes for mitigation?",
        "answer": "The filtering technique aims to reduce representational harms by addressing the disparate variance patterns in sentiment and regard across demographic categories, which reflect biased overgeneralization from ConceptNet propagating through COMeT.",
        "visual_anchor": "Variance patterns in red positive and blue negative boxes across Origin, Gender, Religion, and Profession categories",
        "text_evidence": "To mitigate the observed representational harms in ConceptNet and their effects on downstream tasks, we propose a pre-processing data filtering technique that reduces the effect of existing representational harms in ConceptNet.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the higher median in red boxes compared to blue across all categories confirm COMeT's effectiveness for the case study?",
        "answer": "The higher positive regard medians demonstrate COMeT's behavior patterns, making it an appropriate case study for testing the filtering mitigation technique on CSKB completion tasks with observable bias effects.",
        "visual_anchor": "Red boxes showing consistently higher median values (around 15-25%) compared to blue boxes (around 3-7%) across all four demographic categories",
        "text_evidence": "We apply our mitigation technique on COMeT as a case study. Table 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT. COMeT-Filtered is effective at reducing overgeneralization",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Box plots for Negative (blue) and Positive (red) categories",
      "Four demographic categories: Origin, Gender, Religion, Profession",
      "Y-axis showing Regard percentage from 0 to 50",
      "Red boxes showing higher median values than blue boxes across all categories",
      "Variance indicated by box heights and whisker lengths",
      "Outlier points marked as dots above whiskers"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_20",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg",
    "caption": "Figure 6: Examples from ConceptNet.   ",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0\",\n    \"AtLocation relation between 'child molesters' and 'church' with weight 1.0\",\n    \"AtLocation relation between 'corrupt politicians' and 'Greece' with weight 1.0\",\n    \"CapableOf relation between 'My grandmother' and 'forward numerous annoying prayer emails' with weight 1.0\",\n    \"CapableOf relation between 'A politician' and 'tell lies' with weight 1.0\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the CapableOf relation linking priests to sexual fun exemplify allocative versus representational harms?\",\n      \"answer\": \"The relation demonstrates representational harm by encoding harmful stereotypes about religious figures in machine learning systems. Barocas et al. distinguish this from allocative harms that affect resource distribution.\",\n      \"visual_anchor\": \"CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0\",\n      \"text_evidence\": \"Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The problem with bias: Allocative versus representational harms in machine learning.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the AtLocation relation placing child molesters in churches contrast with the generic knowledge approach?\",\n      \"answer\": \"The relation encodes harmful stereotypical associations rather than generic statements. GenericKB aims to capture broad, non-biased knowledge, unlike this specific harmful ConceptNet example.\",\n      \"visual_anchor\": \"AtLocation relation between 'child molesters' and 'church' with weight 1.0\",\n      \"text_evidence\": \"Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. 2020. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causes these five ConceptNet examples to require the mitigation framework addressing language technology bias?\",\n      \"answer\": \"These examples encode harmful stereotypes about priests, religious institutions, nationalities, elderly people, and politicians. Blodgett et al. address how language technology perpetuates such biases, necessitating filtering frameworks.\",\n      \"visual_anchor\": \"All five relations with weight 1.0 showing stereotypical associations\",\n      \"text_evidence\": \"Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technolo\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0",
        "AtLocation relation between 'child molesters' and 'church' with weight 1.0",
        "AtLocation relation between 'corrupt politicians' and 'Greece' with weight 1.0",
        "CapableOf relation between 'My grandmother' and 'forward numerous annoying prayer emails' with weight 1.0",
        "CapableOf relation between 'A politician' and 'tell lies' with weight 1.0"
      ],
      "queries": [
        {
          "query": "Why does the CapableOf relation linking priests to sexual fun exemplify allocative versus representational harms?",
          "answer": "The relation demonstrates representational harm by encoding harmful stereotypes about religious figures in machine learning systems. Barocas et al. distinguish this from allocative harms that affect resource distribution.",
          "visual_anchor": "CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0",
          "text_evidence": "Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The problem with bias: Allocative versus representational harms in machine learning.",
          "query_type": "value_context"
        },
        {
          "query": "How does the AtLocation relation placing child molesters in churches contrast with the generic knowledge approach?",
          "answer": "The relation encodes harmful stereotypical associations rather than generic statements. GenericKB aims to capture broad, non-biased knowledge, unlike this specific harmful ConceptNet example.",
          "visual_anchor": "AtLocation relation between 'child molesters' and 'church' with weight 1.0",
          "text_evidence": "Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. 2020. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causes these five ConceptNet examples to require the mitigation framework addressing language technology bias?",
          "answer": "These examples encode harmful stereotypes about priests, religious institutions, nationalities, elderly people, and politicians. Blodgett et al. address how language technology perpetuates such biases, necessitating filtering frameworks.",
          "visual_anchor": "All five relations with weight 1.0 showing stereotypical associations",
          "text_evidence": "Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technolo",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the CapableOf relation linking priests to sexual fun exemplify allocative versus representational harms?",
        "answer": "The relation demonstrates representational harm by encoding harmful stereotypes about religious figures in machine learning systems. Barocas et al. distinguish this from allocative harms that affect resource distribution.",
        "visual_anchor": "CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0",
        "text_evidence": "Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The problem with bias: Allocative versus representational harms in machine learning.",
        "query_type": "value_context"
      },
      {
        "query": "How does the AtLocation relation placing child molesters in churches contrast with the generic knowledge approach?",
        "answer": "The relation encodes harmful stereotypical associations rather than generic statements. GenericKB aims to capture broad, non-biased knowledge, unlike this specific harmful ConceptNet example.",
        "visual_anchor": "AtLocation relation between 'child molesters' and 'church' with weight 1.0",
        "text_evidence": "Sumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. 2020. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causes these five ConceptNet examples to require the mitigation framework addressing language technology bias?",
        "answer": "These examples encode harmful stereotypes about priests, religious institutions, nationalities, elderly people, and politicians. Blodgett et al. address how language technology perpetuates such biases, necessitating filtering frameworks.",
        "visual_anchor": "All five relations with weight 1.0 showing stereotypical associations",
        "text_evidence": "Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technolo",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "CapableOf relation between 'A priest' and 'guess that sex is fun' with weight 1.0",
      "AtLocation relation between 'child molesters' and 'church' with weight 1.0",
      "AtLocation relation between 'corrupt politicians' and 'Greece' with weight 1.0",
      "CapableOf relation between 'My grandmother' and 'forward numerous annoying prayer emails' with weight 1.0",
      "CapableOf relation between 'A politician' and 'tell lies' with weight 1.0"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_23",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg",
    "caption": "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"X-axis labeled 'Negative Regard (%)' ranging 0-60\",\n    \"Y-axis labeled 'Positive Regard (%)' ranging 0-60\",\n    \"Purple shaded background region labeled 'Negligible Bias'\",\n    \"Red region labeled 'Prejudice'\",\n    \"Green region labeled 'Favoritism'\",\n    \"Purple region labeled 'Both'\",\n    \"Data points: 'girlfriend' at ~20% negative/20% positive\",\n    \"Data points: 'father' at ~20% negative/10% positive\",\n    \"Data points: 'brother' at ~5% negative/8% positive\",\n    \"Data points: 'woman' and 'man' at ~50% negative/7% positive\",\n    \"Data points: 'male' at ~50% negative/2% positive\",\n    \"Data points: 'girl' at ~35% negative/2% positive\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does girlfriend at 20% negative regard fall in negligible bias rather than prejudice or favoritism regions?\",\n      \"answer\": \"The negligible bias region (purple background) encompasses moderate levels of both positive and negative regard. Girlfriend's position at 20% negative and 20% positive regard places it within this balanced zone, indicating neither strong prejudice nor favoritism.\",\n      \"visual_anchor\": \"girlfriend data point at approximately 20% negative regard and 20% positive regard within purple shaded region\",\n      \"text_evidence\": \"Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does woman's position near 50% negative regard demonstrate the distinction between regard and sentiment label agreement?\",\n      \"answer\": \"Woman's placement at approximately 50% negative regard and 7% positive regard shows high negative bias. This spatial positioning illustrates how regard measures capture bias patterns that can be compared against human labels for agreement assessment.\",\n      \"visual_anchor\": \"woman data point at approximately 50% negative regard and 7% positive regard\",\n      \"text_evidence\": \"Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does male's lowest positive regard at 2% compared to man's 7% reflect filtering effects on generated triples?\",\n      \"answer\": \"Male shows approximately 2% positive regard while man shows 7%, both at high negative regard around 50%. This difference in regard distributions between similar gender terms could reflect how COMeT-Filtered versus unfiltered triples capture different bias patterns.\",\n      \"visual_anchor\": \"male at ~50% negative/2% positive versus man at ~50% negative/7% positive\",\n      \"text_evidence\": \"Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "X-axis labeled 'Negative Regard (%)' ranging 0-60",
        "Y-axis labeled 'Positive Regard (%)' ranging 0-60",
        "Purple shaded background region labeled 'Negligible Bias'",
        "Red region labeled 'Prejudice'",
        "Green region labeled 'Favoritism'",
        "Purple region labeled 'Both'",
        "Data points: 'girlfriend' at ~20% negative/20% positive",
        "Data points: 'father' at ~20% negative/10% positive",
        "Data points: 'brother' at ~5% negative/8% positive",
        "Data points: 'woman' and 'man' at ~50% negative/7% positive",
        "Data points: 'male' at ~50% negative/2% positive",
        "Data points: 'girl' at ~35% negative/2% positive"
      ],
      "queries": [
        {
          "query": "Why does girlfriend at 20% negative regard fall in negligible bias rather than prejudice or favoritism regions?",
          "answer": "The negligible bias region (purple background) encompasses moderate levels of both positive and negative regard. Girlfriend's position at 20% negative and 20% positive regard places it within this balanced zone, indicating neither strong prejudice nor favoritism.",
          "visual_anchor": "girlfriend data point at approximately 20% negative regard and 20% positive regard within purple shaded region",
          "text_evidence": "Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
          "query_type": "value_context"
        },
        {
          "query": "How does woman's position near 50% negative regard demonstrate the distinction between regard and sentiment label agreement?",
          "answer": "Woman's placement at approximately 50% negative regard and 7% positive regard shows high negative bias. This spatial positioning illustrates how regard measures capture bias patterns that can be compared against human labels for agreement assessment.",
          "visual_anchor": "woman data point at approximately 50% negative regard and 7% positive regard",
          "text_evidence": "Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does male's lowest positive regard at 2% compared to man's 7% reflect filtering effects on generated triples?",
          "answer": "Male shows approximately 2% positive regard while man shows 7%, both at high negative regard around 50%. This difference in regard distributions between similar gender terms could reflect how COMeT-Filtered versus unfiltered triples capture different bias patterns.",
          "visual_anchor": "male at ~50% negative/2% positive versus man at ~50% negative/7% positive",
          "text_evidence": "Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does girlfriend at 20% negative regard fall in negligible bias rather than prejudice or favoritism regions?",
        "answer": "The negligible bias region (purple background) encompasses moderate levels of both positive and negative regard. Girlfriend's position at 20% negative and 20% positive regard places it within this balanced zone, indicating neither strong prejudice nor favoritism.",
        "visual_anchor": "girlfriend data point at approximately 20% negative regard and 20% positive regard within purple shaded region",
        "text_evidence": "Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
        "query_type": "value_context"
      },
      {
        "query": "How does woman's position near 50% negative regard demonstrate the distinction between regard and sentiment label agreement?",
        "answer": "Woman's placement at approximately 50% negative regard and 7% positive regard shows high negative bias. This spatial positioning illustrates how regard measures capture bias patterns that can be compared against human labels for agreement assessment.",
        "visual_anchor": "woman data point at approximately 50% negative regard and 7% positive regard",
        "text_evidence": "Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does male's lowest positive regard at 2% compared to man's 7% reflect filtering effects on generated triples?",
        "answer": "Male shows approximately 2% positive regard while man shows 7%, both at high negative regard around 50%. This difference in regard distributions between similar gender terms could reflect how COMeT-Filtered versus unfiltered triples capture different bias patterns.",
        "visual_anchor": "male at ~50% negative/2% positive versus man at ~50% negative/7% positive",
        "text_evidence": "Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "X-axis labeled 'Negative Regard (%)' ranging 0-60",
      "Y-axis labeled 'Positive Regard (%)' ranging 0-60",
      "Purple shaded background region labeled 'Negligible Bias'",
      "Red region labeled 'Prejudice'",
      "Green region labeled 'Favoritism'",
      "Purple region labeled 'Both'",
      "Data points: 'girlfriend' at ~20% negative/20% positive",
      "Data points: 'father' at ~20% negative/10% positive",
      "Data points: 'brother' at ~5% negative/8% positive",
      "Data points: 'woman' and 'man' at ~50% negative/7% positive",
      "Data points: 'male' at ~50% negative/2% positive",
      "Data points: 'girl' at ~35% negative/2% positive"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_26",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg",
    "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ",
    "original_quality_score": 0.65,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"girlfriend at ~1% negative, ~60% positive in purple Negligible Bias region\",\n    \"mommy at ~2% negative, ~50% positive in purple Negligible Bias region\",\n    \"brother at ~2% negative, ~22% positive in green Favoritism region\",\n    \"father at ~20% negative, ~22% positive in purple Negligible Bias region\",\n    \"woman at ~35% negative, ~18% positive in orange Prejudice region\",\n    \"aunt at ~10% negative, ~10% positive in purple Negligible Bias region\",\n    \"grandmother at ~15% negative, ~4% positive in orange Prejudice region\",\n    \"bride at ~10% negative, ~2% positive in orange Prejudice region\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does grandmother fall in the orange Prejudice region despite ConceptNet containing destructive triples for many targets?\",\n      \"answer\": \"Grandmother's position at approximately 15% negative and 4% positive sentiment places it in the Prejudice region. The destructive triples in ConceptNet likely contribute to this negative bias pattern shown in the qualitative results.\",\n      \"visual_anchor\": \"grandmother at ~15% negative, ~4% positive in orange Prejudice region\",\n      \"text_evidence\": \"For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Do the purple Negligible Bias region targets like girlfriend and father demonstrate the sentiment-based categorization method described?\",\n      \"answer\": \"Yes, girlfriend (~1% negative, ~60% positive) and father (~20% negative, ~22% positive) both fall within the purple Negligible Bias region, exemplifying how sentiment measures determine category placement across the three defined regions.\",\n      \"visual_anchor\": \"girlfriend and father in purple Negligible Bias region\",\n      \"text_evidence\": \"Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does woman's placement in orange Prejudice region at 35% negative sentiment compare to brother's green Favoritism region position?\",\n      \"answer\": \"Woman shows significantly higher negative sentiment (~35%) with lower positive sentiment (~18%) placing it in the Prejudice region, while brother has minimal negative sentiment (~2%) with higher positive sentiment (~22%) in the Favoritism region, demonstrating contrasting bias patterns.\",\n      \"visual_anchor\": \"woman at ~35% negative in orange vs brother at ~2% negative in green\",\n      \"text_evidence\": \"The corresponding regions are: prejudice, favoritism, and negligible bias regions.\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "girlfriend at ~1% negative, ~60% positive in purple Negligible Bias region",
        "mommy at ~2% negative, ~50% positive in purple Negligible Bias region",
        "brother at ~2% negative, ~22% positive in green Favoritism region",
        "father at ~20% negative, ~22% positive in purple Negligible Bias region",
        "woman at ~35% negative, ~18% positive in orange Prejudice region",
        "aunt at ~10% negative, ~10% positive in purple Negligible Bias region",
        "grandmother at ~15% negative, ~4% positive in orange Prejudice region",
        "bride at ~10% negative, ~2% positive in orange Prejudice region"
      ],
      "queries": [
        {
          "query": "Why does grandmother fall in the orange Prejudice region despite ConceptNet containing destructive triples for many targets?",
          "answer": "Grandmother's position at approximately 15% negative and 4% positive sentiment places it in the Prejudice region. The destructive triples in ConceptNet likely contribute to this negative bias pattern shown in the qualitative results.",
          "visual_anchor": "grandmother at ~15% negative, ~4% positive in orange Prejudice region",
          "text_evidence": "For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Do the purple Negligible Bias region targets like girlfriend and father demonstrate the sentiment-based categorization method described?",
          "answer": "Yes, girlfriend (~1% negative, ~60% positive) and father (~20% negative, ~22% positive) both fall within the purple Negligible Bias region, exemplifying how sentiment measures determine category placement across the three defined regions.",
          "visual_anchor": "girlfriend and father in purple Negligible Bias region",
          "text_evidence": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
          "query_type": "visual_definition"
        },
        {
          "query": "How does woman's placement in orange Prejudice region at 35% negative sentiment compare to brother's green Favoritism region position?",
          "answer": "Woman shows significantly higher negative sentiment (~35%) with lower positive sentiment (~18%) placing it in the Prejudice region, while brother has minimal negative sentiment (~2%) with higher positive sentiment (~22%) in the Favoritism region, demonstrating contrasting bias patterns.",
          "visual_anchor": "woman at ~35% negative in orange vs brother at ~2% negative in green",
          "text_evidence": "The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does grandmother fall in the orange Prejudice region despite ConceptNet containing destructive triples for many targets?",
        "answer": "Grandmother's position at approximately 15% negative and 4% positive sentiment places it in the Prejudice region. The destructive triples in ConceptNet likely contribute to this negative bias pattern shown in the qualitative results.",
        "visual_anchor": "grandmother at ~15% negative, ~4% positive in orange Prejudice region",
        "text_evidence": "For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Do the purple Negligible Bias region targets like girlfriend and father demonstrate the sentiment-based categorization method described?",
        "answer": "Yes, girlfriend (~1% negative, ~60% positive) and father (~20% negative, ~22% positive) both fall within the purple Negligible Bias region, exemplifying how sentiment measures determine category placement across the three defined regions.",
        "visual_anchor": "girlfriend and father in purple Negligible Bias region",
        "text_evidence": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
        "query_type": "visual_definition"
      },
      {
        "query": "How does woman's placement in orange Prejudice region at 35% negative sentiment compare to brother's green Favoritism region position?",
        "answer": "Woman shows significantly higher negative sentiment (~35%) with lower positive sentiment (~18%) placing it in the Prejudice region, while brother has minimal negative sentiment (~2%) with higher positive sentiment (~22%) in the Favoritism region, demonstrating contrasting bias patterns.",
        "visual_anchor": "woman at ~35% negative in orange vs brother at ~2% negative in green",
        "text_evidence": "The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "girlfriend at ~1% negative, ~60% positive in purple Negligible Bias region",
      "mommy at ~2% negative, ~50% positive in purple Negligible Bias region",
      "brother at ~2% negative, ~22% positive in green Favoritism region",
      "father at ~20% negative, ~22% positive in purple Negligible Bias region",
      "woman at ~35% negative, ~18% positive in orange Prejudice region",
      "aunt at ~10% negative, ~10% positive in purple Negligible Bias region",
      "grandmother at ~15% negative, ~4% positive in orange Prejudice region",
      "bride at ~10% negative, ~2% positive in orange Prejudice region"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_30",
    "figure_number": 11,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg",
    "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Box plot for Religion category on left showing median around 200\",\n    \"Box plot for Origin category not visible (only Religion shown)\",\n    \"Inset bar chart showing bible (blue bar ~800) vs quran (small bar ~50)\",\n    \"Y-axis labeled 'Number of Triples' ranging 0-1200\",\n    \"X-axis labeled 'Religion'\",\n    \"Outlier markers above 1000\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Does the Religion box plot's median near 200 triples support the claim about representation disparity across Origin and Religion categories?\",\n      \"answer\": \"Yes, the box plot demonstrates the representation disparity in terms of number of triples for Religion categories from ConceptNet and GenericsKB, with the median around 200 but significant outliers above 1000.\",\n      \"visual_anchor\": \"Religion box plot median around 200 with outliers above 1000\",\n      \"text_evidence\": \"Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does the inset chart show bible at approximately 800 triples versus quran at roughly 50, given both are religious categories?\",\n      \"answer\": \"The disparity reflects unequal representation in the knowledge bases. The mitigation framework shown in Figure 7 and filtering technique results in Table 11 address such imbalances through COMet_Filtered comparisons.\",\n      \"visual_anchor\": \"Inset bar chart with blue bible bar ~800 and small quran bar ~50\",\n      \"text_evidence\": \"In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What causes the Religion box plot outliers exceeding 1000 triples when the median remains below 200?\",\n      \"answer\": \"Certain religious categories have dramatically more triples in ConceptNet and GenericsKB than others. The detailed mitigation results in Table 11 measure sentiment and regard across all categories to address these disparities.\",\n      \"visual_anchor\": \"Outlier points above 1000 on Religion box plot while median is near 200\",\n      \"text_evidence\": \"Table 11 contains detailed results for the sentiment and regard measures over all the categories\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Box plot for Religion category on left showing median around 200",
        "Box plot for Origin category not visible (only Religion shown)",
        "Inset bar chart showing bible (blue bar ~800) vs quran (small bar ~50)",
        "Y-axis labeled 'Number of Triples' ranging 0-1200",
        "X-axis labeled 'Religion'",
        "Outlier markers above 1000"
      ],
      "queries": [
        {
          "query": "Does the Religion box plot's median near 200 triples support the claim about representation disparity across Origin and Religion categories?",
          "answer": "Yes, the box plot demonstrates the representation disparity in terms of number of triples for Religion categories from ConceptNet and GenericsKB, with the median around 200 but significant outliers above 1000.",
          "visual_anchor": "Religion box plot median around 200 with outliers above 1000",
          "text_evidence": "Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.",
          "query_type": "value_context"
        },
        {
          "query": "Why does the inset chart show bible at approximately 800 triples versus quran at roughly 50, given both are religious categories?",
          "answer": "The disparity reflects unequal representation in the knowledge bases. The mitigation framework shown in Figure 7 and filtering technique results in Table 11 address such imbalances through COMet_Filtered comparisons.",
          "visual_anchor": "Inset bar chart with blue bible bar ~800 and small quran bar ~50",
          "text_evidence": "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What causes the Religion box plot outliers exceeding 1000 triples when the median remains below 200?",
          "answer": "Certain religious categories have dramatically more triples in ConceptNet and GenericsKB than others. The detailed mitigation results in Table 11 measure sentiment and regard across all categories to address these disparities.",
          "visual_anchor": "Outlier points above 1000 on Religion box plot while median is near 200",
          "text_evidence": "Table 11 contains detailed results for the sentiment and regard measures over all the categories",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Does the Religion box plot's median near 200 triples support the claim about representation disparity across Origin and Religion categories?",
        "answer": "Yes, the box plot demonstrates the representation disparity in terms of number of triples for Religion categories from ConceptNet and GenericsKB, with the median around 200 but significant outliers above 1000.",
        "visual_anchor": "Religion box plot median around 200 with outliers above 1000",
        "text_evidence": "Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.",
        "query_type": "value_context"
      },
      {
        "query": "Why does the inset chart show bible at approximately 800 triples versus quran at roughly 50, given both are religious categories?",
        "answer": "The disparity reflects unequal representation in the knowledge bases. The mitigation framework shown in Figure 7 and filtering technique results in Table 11 address such imbalances through COMet_Filtered comparisons.",
        "visual_anchor": "Inset bar chart with blue bible bar ~800 and small quran bar ~50",
        "text_evidence": "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What causes the Religion box plot outliers exceeding 1000 triples when the median remains below 200?",
        "answer": "Certain religious categories have dramatically more triples in ConceptNet and GenericsKB than others. The detailed mitigation results in Table 11 measure sentiment and regard across all categories to address these disparities.",
        "visual_anchor": "Outlier points above 1000 on Religion box plot while median is near 200",
        "text_evidence": "Table 11 contains detailed results for the sentiment and regard measures over all the categories",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Box plot for Religion category on left showing median around 200",
      "Box plot for Origin category not visible (only Religion shown)",
      "Inset bar chart showing bible (blue bar ~800) vs quran (small bar ~50)",
      "Y-axis labeled 'Number of Triples' ranging 0-1200",
      "X-axis labeled 'Religion'",
      "Outlier markers above 1000"
    ]
  },
  {
    "doc_id": "2103.11320",
    "figure_id": "2103.11320_fig_36",
    "figure_number": 12,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig35.jpg",
    "caption": "Figure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"wordcloud\",\n  \"visual_elements\": [\n    \"offensive (large, top-left)\",\n    \"sex (large, center-top)\",\n    \"bitch (large, center)\",\n    \"alienatress (left)\",\n    \"femalefist (left-center)\",\n    \"accuseress (left)\",\n    \"impersonator (center-left)\",\n    \"blasphemeress (top-right)\",\n    \"assassinatress (right)\",\n    \"admonitrix (center-right)\",\n    \"barbarianess (bottom-right)\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do words like offensive and bitch appear prominently when the targets are british and female?\",\n      \"answer\": \"These phrases appear in triples with negative regard and sentiment labels for \\\"british\\\" and \\\"female\\\" targets, reflecting biased associations in the dataset.\",\n      \"visual_anchor\": \"offensive (large, top-left) and bitch (large, center)\",\n      \"text_evidence\": \"Wordcloud of phrases that appear in triples with negative regard and sentiment labels for \\\"british\\\" and \\\"female\\\" targets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the prominence of femalefist and accuseress relate to gender-specific bias in the profession category?\",\n      \"answer\": \"These gendered negative terms appear in triples with female targets, indicating bias. The profession category includes targets like barber, coach, and businessperson where such bias manifests.\",\n      \"visual_anchor\": \"femalefist (left-center) and accuseress (left)\",\n      \"text_evidence\": \"Table 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the appearance of sex and impersonator support filtering efforts to improve neutral sentiment for gender category?\",\n      \"answer\": \"Yes, these negative phrases justify the COMeT-Filtered model's improved neutral sentiment mean from 59.169 to higher values by removing such biased triples from the gender category.\",\n      \"visual_anchor\": \"sex (large, center-top) and impersonator (center-left)\",\n      \"text_evidence\": \"COMeT 64.527 58.578 59.169 61.610 COMeT-Filtered 65.257 59.485\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "wordcloud",
      "visual_elements": [
        "offensive (large, top-left)",
        "sex (large, center-top)",
        "bitch (large, center)",
        "alienatress (left)",
        "femalefist (left-center)",
        "accuseress (left)",
        "impersonator (center-left)",
        "blasphemeress (top-right)",
        "assassinatress (right)",
        "admonitrix (center-right)",
        "barbarianess (bottom-right)"
      ],
      "queries": [
        {
          "query": "Why do words like offensive and bitch appear prominently when the targets are british and female?",
          "answer": "These phrases appear in triples with negative regard and sentiment labels for \"british\" and \"female\" targets, reflecting biased associations in the dataset.",
          "visual_anchor": "offensive (large, top-left) and bitch (large, center)",
          "text_evidence": "Wordcloud of phrases that appear in triples with negative regard and sentiment labels for \"british\" and \"female\" targets.",
          "query_type": "value_context"
        },
        {
          "query": "How does the prominence of femalefist and accuseress relate to gender-specific bias in the profession category?",
          "answer": "These gendered negative terms appear in triples with female targets, indicating bias. The profession category includes targets like barber, coach, and businessperson where such bias manifests.",
          "visual_anchor": "femalefist (left-center) and accuseress (left)",
          "text_evidence": "Table 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the appearance of sex and impersonator support filtering efforts to improve neutral sentiment for gender category?",
          "answer": "Yes, these negative phrases justify the COMeT-Filtered model's improved neutral sentiment mean from 59.169 to higher values by removing such biased triples from the gender category.",
          "visual_anchor": "sex (large, center-top) and impersonator (center-left)",
          "text_evidence": "COMeT 64.527 58.578 59.169 61.610 COMeT-Filtered 65.257 59.485",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do words like offensive and bitch appear prominently when the targets are british and female?",
        "answer": "These phrases appear in triples with negative regard and sentiment labels for \"british\" and \"female\" targets, reflecting biased associations in the dataset.",
        "visual_anchor": "offensive (large, top-left) and bitch (large, center)",
        "text_evidence": "Wordcloud of phrases that appear in triples with negative regard and sentiment labels for \"british\" and \"female\" targets.",
        "query_type": "value_context"
      },
      {
        "query": "How does the prominence of femalefist and accuseress relate to gender-specific bias in the profession category?",
        "answer": "These gendered negative terms appear in triples with female targets, indicating bias. The profession category includes targets like barber, coach, and businessperson where such bias manifests.",
        "visual_anchor": "femalefist (left-center) and accuseress (left)",
        "text_evidence": "Table 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the appearance of sex and impersonator support filtering efforts to improve neutral sentiment for gender category?",
        "answer": "Yes, these negative phrases justify the COMeT-Filtered model's improved neutral sentiment mean from 59.169 to higher values by removing such biased triples from the gender category.",
        "visual_anchor": "sex (large, center-top) and impersonator (center-left)",
        "text_evidence": "COMeT 64.527 58.578 59.169 61.610 COMeT-Filtered 65.257 59.485",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "wordcloud",
    "visual_elements": [
      "offensive (large, top-left)",
      "sex (large, center-top)",
      "bitch (large, center)",
      "alienatress (left)",
      "femalefist (left-center)",
      "accuseress (left)",
      "impersonator (center-left)",
      "blasphemeress (top-right)",
      "assassinatress (right)",
      "admonitrix (center-right)",
      "barbarianess (bottom-right)"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_1",
    "figure_number": 1,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
    "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
    "original_quality_score": 1.0,
    "raw_response": "```json\n{\n  \"figure_type\": \"architecture\",\n  \"visual_elements\": [\n    \"Three blue circles labeled f1, f2, ..., fm at the top\",\n    \"Three red boxes labeled e1, e2, ..., em below the blue circles\",\n    \"Orange Attention layer in the middle\",\n    \"Two yellow Dense Layer boxes\",\n    \"Red 'out' box at the bottom\",\n    \"Black arrows showing flow from embeddings through attention to dense layers\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do the red boxes e1 through em have dimension d^e before entering the orange Attention layer?\",\n      \"answer\": \"Each feature value is treated as an individual entity and learns a fixed-size embedding vector in R^(d^e), similar to how words are embedded in text classification tasks.\",\n      \"visual_anchor\": \"red boxes labeled e1, e2, ..., em positioned between blue circles and orange Attention layer\",\n      \"text_evidence\": \"We consider each feature value as an individual entity (like the words are considered in text-classification) and learn a fixed-size embedding {e_k}_{k=1}^m, e_k ∈ R^{d^e} for each feature, {f_k}_{k=1}^m\",\n      \"query_type\": \"visual_definition\"\n    },\n    {\n      \"query\": \"How does the orange Attention layer produce its output representation before the yellow Dense Layers?\",\n      \"answer\": \"The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.\",\n      \"visual_anchor\": \"orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes\",\n      \"text_evidence\": \"These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What purpose does zeroing attention weights serve in the Attribution framework variant of this architecture?\",\n      \"answer\": \"Zeroing or reducing attention weights allows observation of specific feature effects on fairness outcomes. The resulting differences in accuracy and fairness measures indicate the effect of the zeroed-out features.\",\n      \"visual_anchor\": \"orange Attention layer where weights can be manipulated\",\n      \"text_evidence\": \"To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change.\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "architecture",
      "visual_elements": [
        "Three blue circles labeled f1, f2, ..., fm at the top",
        "Three red boxes labeled e1, e2, ..., em below the blue circles",
        "Orange Attention layer in the middle",
        "Two yellow Dense Layer boxes",
        "Red 'out' box at the bottom",
        "Black arrows showing flow from embeddings through attention to dense layers"
      ],
      "queries": [
        {
          "query": "Why do the red boxes e1 through em have dimension d^e before entering the orange Attention layer?",
          "answer": "Each feature value is treated as an individual entity and learns a fixed-size embedding vector in R^(d^e), similar to how words are embedded in text classification tasks.",
          "visual_anchor": "red boxes labeled e1, e2, ..., em positioned between blue circles and orange Attention layer",
          "text_evidence": "We consider each feature value as an individual entity (like the words are considered in text-classification) and learn a fixed-size embedding {e_k}_{k=1}^m, e_k ∈ R^{d^e} for each feature, {f_k}_{k=1}^m",
          "query_type": "visual_definition"
        },
        {
          "query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
          "answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
          "visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
          "text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What purpose does zeroing attention weights serve in the Attribution framework variant of this architecture?",
          "answer": "Zeroing or reducing attention weights allows observation of specific feature effects on fairness outcomes. The resulting differences in accuracy and fairness measures indicate the effect of the zeroed-out features.",
          "visual_anchor": "orange Attention layer where weights can be manipulated",
          "text_evidence": "To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change.",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the red boxes e1 through em have dimension d^e before entering the orange Attention layer?",
        "answer": "Each feature value is treated as an individual entity and learns a fixed-size embedding vector in R^(d^e), similar to how words are embedded in text classification tasks.",
        "visual_anchor": "red boxes labeled e1, e2, ..., em positioned between blue circles and orange Attention layer",
        "text_evidence": "We consider each feature value as an individual entity (like the words are considered in text-classification) and learn a fixed-size embedding {e_k}_{k=1}^m, e_k ∈ R^{d^e} for each feature, {f_k}_{k=1}^m",
        "query_type": "visual_definition"
      },
      {
        "query": "How does the orange Attention layer produce its output representation before the yellow Dense Layers?",
        "answer": "The attention layer computes weights and takes a weighted combination of the input feature embeddings to produce a d^e-dimensional vector representation for the sample instance.",
        "visual_anchor": "orange Attention layer box positioned between red embedding boxes and yellow Dense Layer boxes",
        "text_evidence": "These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What purpose does zeroing attention weights serve in the Attribution framework variant of this architecture?",
        "answer": "Zeroing or reducing attention weights allows observation of specific feature effects on fairness outcomes. The resulting differences in accuracy and fairness measures indicate the effect of the zeroed-out features.",
        "visual_anchor": "orange Attention layer where weights can be manipulated",
        "text_evidence": "To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change.",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "architecture",
    "visual_elements": [
      "Three blue circles labeled f1, f2, ..., fm at the top",
      "Three red boxes labeled e1, e2, ..., em below the blue circles",
      "Orange Attention layer in the middle",
      "Two yellow Dense Layer boxes",
      "Red 'out' box at the bottom",
      "Black arrows showing flow from embeddings through attention to dense layers"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_3",
    "figure_number": 2,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg",
    "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"ŷ_o at approximately (0.87, 0.5) marked with red circle\",\n    \"ŷ_z^2 at approximately (0.53, 1.0) marked with blue diamond\",\n    \"ŷ_z^{1,2} at approximately (0.53, 0.1) marked with blue diamond\",\n    \"ŷ_z^1 at approximately (0.92, 0.05) marked with blue circle\",\n    \"ŷ_z^3 at approximately (0.9, 0.5) marked with blue diamond\",\n    \"Cyan background representing the plot area\",\n    \"Accuracy on x-axis, SPD on y-axis\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does removing f_2 move the model from accuracy 0.87 to 0.53 as shown by ŷ_z^2?\",\n      \"answer\": \"Feature f_2 is correctly attributed to being responsible for the accuracy in Scenario 1, so removing it causes accuracy to drop drastically from the original model outcome to approximately 0.53.\",\n      \"visual_anchor\": \"ŷ_z^2 blue diamond at (0.53, 1.0) compared to ŷ_o red circle at (0.87, 0.5)\",\n      \"text_evidence\": \"In Scenario 1, as expected, $f _ { 2 }$ is correctly attributed to being responsible for the accuracy and removing it hurts the accuracy drastically.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the position of ŷ_o at SPD 0.5 validate that zeroing attention weights changes model fairness?\",\n      \"answer\": \"Yes, the original model ŷ_o has SPD around 0.5, while zeroing different features' attention weights produces outcomes with varying SPD values (from 0.05 to 1.0), demonstrating how attention weight exclusion affects statistical parity difference.\",\n      \"visual_anchor\": \"ŷ_o red circle at SPD=0.5 compared to other blue diamonds at different SPD levels\",\n      \"text_evidence\": \"The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"What causes ŷ_z^1 to maintain high accuracy at 0.92 while other zeroed features drop below 0.6?\",\n      \"answer\": \"The synthetic experiment tests the method's ability to capture correct attributions in controlled conditions. Feature f_1 is likely not responsible for accuracy in Scenario 1, so zeroing its attention weights minimally impacts performance compared to features like f_2.\",\n      \"visual_anchor\": \"ŷ_z^1 blue circle at (0.92, 0.05) versus ŷ_z^2 and ŷ_z^{1,2} both near 0.53 accuracy\",\n      \"text_evidence\": \"First, we test our method's ability to capture correct attributions in controlled experiments with synthetic data (described in Sec. 3.2).\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "ŷ_o at approximately (0.87, 0.5) marked with red circle",
        "ŷ_z^2 at approximately (0.53, 1.0) marked with blue diamond",
        "ŷ_z^{1,2} at approximately (0.53, 0.1) marked with blue diamond",
        "ŷ_z^1 at approximately (0.92, 0.05) marked with blue circle",
        "ŷ_z^3 at approximately (0.9, 0.5) marked with blue diamond",
        "Cyan background representing the plot area",
        "Accuracy on x-axis, SPD on y-axis"
      ],
      "queries": [
        {
          "query": "Why does removing f_2 move the model from accuracy 0.87 to 0.53 as shown by ŷ_z^2?",
          "answer": "Feature f_2 is correctly attributed to being responsible for the accuracy in Scenario 1, so removing it causes accuracy to drop drastically from the original model outcome to approximately 0.53.",
          "visual_anchor": "ŷ_z^2 blue diamond at (0.53, 1.0) compared to ŷ_o red circle at (0.87, 0.5)",
          "text_evidence": "In Scenario 1, as expected, $f _ { 2 }$ is correctly attributed to being responsible for the accuracy and removing it hurts the accuracy drastically.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the position of ŷ_o at SPD 0.5 validate that zeroing attention weights changes model fairness?",
          "answer": "Yes, the original model ŷ_o has SPD around 0.5, while zeroing different features' attention weights produces outcomes with varying SPD values (from 0.05 to 1.0), demonstrating how attention weight exclusion affects statistical parity difference.",
          "visual_anchor": "ŷ_o red circle at SPD=0.5 compared to other blue diamonds at different SPD levels",
          "text_evidence": "The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
          "query_type": "value_context"
        },
        {
          "query": "What causes ŷ_z^1 to maintain high accuracy at 0.92 while other zeroed features drop below 0.6?",
          "answer": "The synthetic experiment tests the method's ability to capture correct attributions in controlled conditions. Feature f_1 is likely not responsible for accuracy in Scenario 1, so zeroing its attention weights minimally impacts performance compared to features like f_2.",
          "visual_anchor": "ŷ_z^1 blue circle at (0.92, 0.05) versus ŷ_z^2 and ŷ_z^{1,2} both near 0.53 accuracy",
          "text_evidence": "First, we test our method's ability to capture correct attributions in controlled experiments with synthetic data (described in Sec. 3.2).",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does removing f_2 move the model from accuracy 0.87 to 0.53 as shown by ŷ_z^2?",
        "answer": "Feature f_2 is correctly attributed to being responsible for the accuracy in Scenario 1, so removing it causes accuracy to drop drastically from the original model outcome to approximately 0.53.",
        "visual_anchor": "ŷ_z^2 blue diamond at (0.53, 1.0) compared to ŷ_o red circle at (0.87, 0.5)",
        "text_evidence": "In Scenario 1, as expected, $f _ { 2 }$ is correctly attributed to being responsible for the accuracy and removing it hurts the accuracy drastically.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the position of ŷ_o at SPD 0.5 validate that zeroing attention weights changes model fairness?",
        "answer": "Yes, the original model ŷ_o has SPD around 0.5, while zeroing different features' attention weights produces outcomes with varying SPD values (from 0.05 to 1.0), demonstrating how attention weight exclusion affects statistical parity difference.",
        "visual_anchor": "ŷ_o red circle at SPD=0.5 compared to other blue diamonds at different SPD levels",
        "text_evidence": "The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
        "query_type": "value_context"
      },
      {
        "query": "What causes ŷ_z^1 to maintain high accuracy at 0.92 while other zeroed features drop below 0.6?",
        "answer": "The synthetic experiment tests the method's ability to capture correct attributions in controlled conditions. Feature f_1 is likely not responsible for accuracy in Scenario 1, so zeroing its attention weights minimally impacts performance compared to features like f_2.",
        "visual_anchor": "ŷ_z^1 blue circle at (0.92, 0.05) versus ŷ_z^2 and ŷ_z^{1,2} both near 0.53 accuracy",
        "text_evidence": "First, we test our method's ability to capture correct attributions in controlled experiments with synthetic data (described in Sec. 3.2).",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "ŷ_o at approximately (0.87, 0.5) marked with red circle",
      "ŷ_z^2 at approximately (0.53, 1.0) marked with blue diamond",
      "ŷ_z^{1,2} at approximately (0.53, 0.1) marked with blue diamond",
      "ŷ_z^1 at approximately (0.92, 0.05) marked with blue circle",
      "ŷ_z^3 at approximately (0.9, 0.5) marked with blue diamond",
      "Cyan background representing the plot area",
      "Accuracy on x-axis, SPD on y-axis"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_5",
    "figure_number": 3,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg",
    "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"FCRL red circle markers\",\n    \"CVIB blue plus markers\",\n    \"MaxEnt-ARL gray square markers\",\n    \"Adversarial Forgetting cyan triangle markers\",\n    \"Attention (Ours) purple plus markers\",\n    \"MIFR green circle markers\",\n    \"LAFTR yellow diamond markers\",\n    \"Accuracy range from 0.75 to 0.85\",\n    \"Statistical Parity Difference range from 0.0 to 0.20\",\n    \"Purple markers showing horizontal error bars around 0.82-0.85 accuracy\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does FCRL with red circles remain below 0.04 parity difference until accuracy exceeds 0.81 on UCI Adult?\",\n      \"answer\": \"FCRL demonstrates effective fairness control at lower accuracy levels, maintaining low statistical parity difference while gradually trading off accuracy. This behavior reflects the accuracy-fairness tradeoff characteristic of the UCI Adult dataset experiments.\",\n      \"visual_anchor\": \"FCRL red circle markers clustered below 0.04 Statistical Parity Difference in the 0.80-0.82 accuracy range\",\n      \"text_evidence\": \"Accuracy vs parity curves for UCI Adult and Heritage Health datasets\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the purple Attention method's horizontal error bars spanning 0.82 to 0.85 accuracy support flexible fairness mitigation across modalities?\",\n      \"answer\": \"Yes, the wide horizontal spread of Attention (Ours) demonstrates its ability to achieve various accuracy-fairness tradeoffs, which aligns with the claim that the approach is flexible and useful for controlling fairness beyond tabular datasets.\",\n      \"visual_anchor\": \"Purple plus markers with horizontal error bars extending from approximately 0.82 to 0.85 accuracy\",\n      \"text_evidence\": \"In addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does MIFR's green curve reaching 0.18 parity at 0.83 accuracy compare to the biosbias dataset bias mitigation goal?\",\n      \"answer\": \"MIFR shows relatively high parity difference at high accuracy on UCI Adult, suggesting it may struggle with fairness-accuracy tradeoffs. This contrasts with the goal of reducing observed biases in classification tasks like biosbias, where effective mitigation is needed.\",\n      \"visual_anchor\": \"Green circle markers (MIFR) reaching approximately 0.18 Statistical Parity Difference at 0.83 accuracy\",\n      \"text_evidence\": \"We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "FCRL red circle markers",
        "CVIB blue plus markers",
        "MaxEnt-ARL gray square markers",
        "Adversarial Forgetting cyan triangle markers",
        "Attention (Ours) purple plus markers",
        "MIFR green circle markers",
        "LAFTR yellow diamond markers",
        "Accuracy range from 0.75 to 0.85",
        "Statistical Parity Difference range from 0.0 to 0.20",
        "Purple markers showing horizontal error bars around 0.82-0.85 accuracy"
      ],
      "queries": [
        {
          "query": "Why does FCRL with red circles remain below 0.04 parity difference until accuracy exceeds 0.81 on UCI Adult?",
          "answer": "FCRL demonstrates effective fairness control at lower accuracy levels, maintaining low statistical parity difference while gradually trading off accuracy. This behavior reflects the accuracy-fairness tradeoff characteristic of the UCI Adult dataset experiments.",
          "visual_anchor": "FCRL red circle markers clustered below 0.04 Statistical Parity Difference in the 0.80-0.82 accuracy range",
          "text_evidence": "Accuracy vs parity curves for UCI Adult and Heritage Health datasets",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the purple Attention method's horizontal error bars spanning 0.82 to 0.85 accuracy support flexible fairness mitigation across modalities?",
          "answer": "Yes, the wide horizontal spread of Attention (Ours) demonstrates its ability to achieve various accuracy-fairness tradeoffs, which aligns with the claim that the approach is flexible and useful for controlling fairness beyond tabular datasets.",
          "visual_anchor": "Purple plus markers with horizontal error bars extending from approximately 0.82 to 0.85 accuracy",
          "text_evidence": "In addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets",
          "query_type": "value_context"
        },
        {
          "query": "How does MIFR's green curve reaching 0.18 parity at 0.83 accuracy compare to the biosbias dataset bias mitigation goal?",
          "answer": "MIFR shows relatively high parity difference at high accuracy on UCI Adult, suggesting it may struggle with fairness-accuracy tradeoffs. This contrasts with the goal of reducing observed biases in classification tasks like biosbias, where effective mitigation is needed.",
          "visual_anchor": "Green circle markers (MIFR) reaching approximately 0.18 Statistical Parity Difference at 0.83 accuracy",
          "text_evidence": "We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does FCRL with red circles remain below 0.04 parity difference until accuracy exceeds 0.81 on UCI Adult?",
        "answer": "FCRL demonstrates effective fairness control at lower accuracy levels, maintaining low statistical parity difference while gradually trading off accuracy. This behavior reflects the accuracy-fairness tradeoff characteristic of the UCI Adult dataset experiments.",
        "visual_anchor": "FCRL red circle markers clustered below 0.04 Statistical Parity Difference in the 0.80-0.82 accuracy range",
        "text_evidence": "Accuracy vs parity curves for UCI Adult and Heritage Health datasets",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the purple Attention method's horizontal error bars spanning 0.82 to 0.85 accuracy support flexible fairness mitigation across modalities?",
        "answer": "Yes, the wide horizontal spread of Attention (Ours) demonstrates its ability to achieve various accuracy-fairness tradeoffs, which aligns with the claim that the approach is flexible and useful for controlling fairness beyond tabular datasets.",
        "visual_anchor": "Purple plus markers with horizontal error bars extending from approximately 0.82 to 0.85 accuracy",
        "text_evidence": "In addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets",
        "query_type": "value_context"
      },
      {
        "query": "How does MIFR's green curve reaching 0.18 parity at 0.83 accuracy compare to the biosbias dataset bias mitigation goal?",
        "answer": "MIFR shows relatively high parity difference at high accuracy on UCI Adult, suggesting it may struggle with fairness-accuracy tradeoffs. This contrasts with the goal of reducing observed biases in classification tasks like biosbias, where effective mitigation is needed.",
        "visual_anchor": "Green circle markers (MIFR) reaching approximately 0.18 Statistical Parity Difference at 0.83 accuracy",
        "text_evidence": "We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "FCRL red circle markers",
      "CVIB blue plus markers",
      "MaxEnt-ARL gray square markers",
      "Adversarial Forgetting cyan triangle markers",
      "Attention (Ours) purple plus markers",
      "MIFR green circle markers",
      "LAFTR yellow diamond markers",
      "Accuracy range from 0.75 to 0.85",
      "Statistical Parity Difference range from 0.0 to 0.20",
      "Purple markers showing horizontal error bars around 0.82-0.85 accuracy"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_7",
    "figure_number": 4,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg",
    "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”",
    "original_quality_score": 0.95,
    "raw_response": "```json\n{\n  \"figure_type\": \"example\",\n  \"visual_elements\": [\n    \"Three text boxes showing Post-Processing (Ours), Pre-Processing, and Not Debiased Model\",\n    \"Green highlighted words in each text box\",\n    \"Words 'in', 'Apo', 'R.N.', 'License' highlighted in Post-Processing\",\n    \"Words 'Europe', 'License', 'Colorado' highlighted in Pre-Processing\",\n    \"Words 'She', 'Rebekah Bushey', 'she' highlighted in Not Debiased Model\",\n    \"Words 'inpatient', 'Nursing', 'health' highlighted in bottom row examples\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the Post-Processing model highlight R.N. instead of gendered words like the Not Debiased Model does?\",\n      \"answer\": \"The Post-Processing model uses attention weights to focus on profession-based words like R.N. (Registered Nurse) to correctly predict \\\"nurse,\\\" while the Not Debiased Model mostly focuses on gendered words for its prediction.\",\n      \"visual_anchor\": \"Green highlight on 'R.N.' in Post-Processing box versus 'She', 'Rebekah Bushey', 'she' in Not Debiased Model box\",\n      \"text_evidence\": \"While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict \\\"nurse.\\\"\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"How does highlighting Nursing in all three models demonstrate the proposed mitigation technique reduces observed biases?\",\n      \"answer\": \"All three models highlight \\\"Nursing\\\" as relevant, but the Post-Processing model also emphasizes profession-specific terms while avoiding gendered words. This demonstrates the mitigation technique's effectiveness in reducing bias on the biosbias dataset classification task.\",\n      \"visual_anchor\": \"Green highlight on 'Nursing' appearing in all three bottom text boxes\",\n      \"text_evidence\": \"We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Does the attention weight manipulation shown in green highlights validate the post-processing bias mitigation strategy on text data?\",\n      \"answer\": \"Yes, the green highlights show that attention weight manipulation successfully shifts focus from gendered terms to profession-based words. This validates the proposed framework by demonstrating effectiveness on both tabular and text data through the attention mechanism.\",\n      \"visual_anchor\": \"Green highlighted words distributed across all three columns showing different attention patterns\",\n      \"text_evidence\": \"Using this interpretable attribution framework we then introduced a post-processing bias mitigation strategy based on attention weight manipulation. We validated the proposed framework by conducting experimen\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "example",
      "visual_elements": [
        "Three text boxes showing Post-Processing (Ours), Pre-Processing, and Not Debiased Model",
        "Green highlighted words in each text box",
        "Words 'in', 'Apo', 'R.N.', 'License' highlighted in Post-Processing",
        "Words 'Europe', 'License', 'Colorado' highlighted in Pre-Processing",
        "Words 'She', 'Rebekah Bushey', 'she' highlighted in Not Debiased Model",
        "Words 'inpatient', 'Nursing', 'health' highlighted in bottom row examples"
      ],
      "queries": [
        {
          "query": "Why does the Post-Processing model highlight R.N. instead of gendered words like the Not Debiased Model does?",
          "answer": "The Post-Processing model uses attention weights to focus on profession-based words like R.N. (Registered Nurse) to correctly predict \"nurse,\" while the Not Debiased Model mostly focuses on gendered words for its prediction.",
          "visual_anchor": "Green highlight on 'R.N.' in Post-Processing box versus 'She', 'Rebekah Bushey', 'she' in Not Debiased Model box",
          "text_evidence": "While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict \"nurse.\"",
          "query_type": "comparison_explanation"
        },
        {
          "query": "How does highlighting Nursing in all three models demonstrate the proposed mitigation technique reduces observed biases?",
          "answer": "All three models highlight \"Nursing\" as relevant, but the Post-Processing model also emphasizes profession-specific terms while avoiding gendered words. This demonstrates the mitigation technique's effectiveness in reducing bias on the biosbias dataset classification task.",
          "visual_anchor": "Green highlight on 'Nursing' appearing in all three bottom text boxes",
          "text_evidence": "We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset.",
          "query_type": "value_context"
        },
        {
          "query": "Does the attention weight manipulation shown in green highlights validate the post-processing bias mitigation strategy on text data?",
          "answer": "Yes, the green highlights show that attention weight manipulation successfully shifts focus from gendered terms to profession-based words. This validates the proposed framework by demonstrating effectiveness on both tabular and text data through the attention mechanism.",
          "visual_anchor": "Green highlighted words distributed across all three columns showing different attention patterns",
          "text_evidence": "Using this interpretable attribution framework we then introduced a post-processing bias mitigation strategy based on attention weight manipulation. We validated the proposed framework by conducting experimen",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the Post-Processing model highlight R.N. instead of gendered words like the Not Debiased Model does?",
        "answer": "The Post-Processing model uses attention weights to focus on profession-based words like R.N. (Registered Nurse) to correctly predict \"nurse,\" while the Not Debiased Model mostly focuses on gendered words for its prediction.",
        "visual_anchor": "Green highlight on 'R.N.' in Post-Processing box versus 'She', 'Rebekah Bushey', 'she' in Not Debiased Model box",
        "text_evidence": "While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict \"nurse.\"",
        "query_type": "comparison_explanation"
      },
      {
        "query": "How does highlighting Nursing in all three models demonstrate the proposed mitigation technique reduces observed biases?",
        "answer": "All three models highlight \"Nursing\" as relevant, but the Post-Processing model also emphasizes profession-specific terms while avoiding gendered words. This demonstrates the mitigation technique's effectiveness in reducing bias on the biosbias dataset classification task.",
        "visual_anchor": "Green highlight on 'Nursing' appearing in all three bottom text boxes",
        "text_evidence": "We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset.",
        "query_type": "value_context"
      },
      {
        "query": "Does the attention weight manipulation shown in green highlights validate the post-processing bias mitigation strategy on text data?",
        "answer": "Yes, the green highlights show that attention weight manipulation successfully shifts focus from gendered terms to profession-based words. This validates the proposed framework by demonstrating effectiveness on both tabular and text data through the attention mechanism.",
        "visual_anchor": "Green highlighted words distributed across all three columns showing different attention patterns",
        "text_evidence": "Using this interpretable attribution framework we then introduced a post-processing bias mitigation strategy based on attention weight manipulation. We validated the proposed framework by conducting experimen",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "example",
    "visual_elements": [
      "Three text boxes showing Post-Processing (Ours), Pre-Processing, and Not Debiased Model",
      "Green highlighted words in each text box",
      "Words 'in', 'Apo', 'R.N.', 'License' highlighted in Post-Processing",
      "Words 'Europe', 'License', 'Colorado' highlighted in Pre-Processing",
      "Words 'She', 'Rebekah Bushey', 'she' highlighted in Not Debiased Model",
      "Words 'inpatient', 'Nursing', 'health' highlighted in bottom row examples"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_8",
    "figure_number": 5,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg",
    "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ",
    "original_quality_score": 0.8,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Purple points labeled 'Attention (Ours)' clustered at accuracy ~0.84-0.845 and equality opportunity difference ~0.02-0.18\",\n    \"Yellow points labeled 'LAFTR' scattered at accuracy ~0.84 and equality opportunity difference ~0.0-0.17\",\n    \"Green points labeled 'MIFR' ranging from accuracy ~0.795-0.835 and equality opportunity difference ~0.05-0.25\",\n    \"Error bars on all data points showing variance\",\n    \"Three distinct method clusters with different accuracy-fairness trade-offs\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does MIFR show the widest accuracy spread from 0.795 to 0.835 despite the job classification task using bio texts?\",\n      \"answer\": \"MIFR's wide accuracy spread likely reflects varying performance when attention weights focus on different top three words in bio texts for job prediction, as shown in the green regions of qualitative results.\",\n      \"visual_anchor\": \"Green points (MIFR) spanning horizontally from accuracy 0.795 to 0.835\",\n      \"text_evidence\": \"Green regions represent top three words that the model used for its prediction based on the attention weights.\",\n      \"query_type\": \"anomaly_cause\"\n    },\n    {\n      \"query\": \"Does the purple cluster's concentration at accuracy 0.84-0.845 align with the cardiac telemetry internship context in the bio sample?\",\n      \"answer\": \"The purple cluster represents the Attention method's performance on job classification, not the specific bio content about cardiac telemetry internship, which is merely example input text the model processes.\",\n      \"visual_anchor\": \"Purple points (Attention) clustered tightly at accuracy range 0.84-0.845\",\n      \"text_evidence\": \"Following graduation she completed a cardiac telemetry internship at Texas Health Resources Fort Worth. She continued to work there for two years taking care of postsurgical cardiac patients\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does LAFTR's near-zero equality opportunity difference at 0.84 accuracy relate to the Baylor University nursing degree mentioned?\",\n      \"answer\": \"The LAFTR yellow point at near-zero fairness represents algorithmic performance metrics, independent of the specific biographical content about Baylor University nursing education that serves as input data.\",\n      \"visual_anchor\": \"Yellow point (LAFTR) at accuracy ~0.84 and equality opportunity difference ~0.0\",\n      \"text_evidence\": \"She graduated from Baylor University Louise Herrington School of Nursing in with her Bachelors of Science in Nursing\",\n      \"query_type\": \"value_context\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Purple points labeled 'Attention (Ours)' clustered at accuracy ~0.84-0.845 and equality opportunity difference ~0.02-0.18",
        "Yellow points labeled 'LAFTR' scattered at accuracy ~0.84 and equality opportunity difference ~0.0-0.17",
        "Green points labeled 'MIFR' ranging from accuracy ~0.795-0.835 and equality opportunity difference ~0.05-0.25",
        "Error bars on all data points showing variance",
        "Three distinct method clusters with different accuracy-fairness trade-offs"
      ],
      "queries": [
        {
          "query": "Why does MIFR show the widest accuracy spread from 0.795 to 0.835 despite the job classification task using bio texts?",
          "answer": "MIFR's wide accuracy spread likely reflects varying performance when attention weights focus on different top three words in bio texts for job prediction, as shown in the green regions of qualitative results.",
          "visual_anchor": "Green points (MIFR) spanning horizontally from accuracy 0.795 to 0.835",
          "text_evidence": "Green regions represent top three words that the model used for its prediction based on the attention weights.",
          "query_type": "anomaly_cause"
        },
        {
          "query": "Does the purple cluster's concentration at accuracy 0.84-0.845 align with the cardiac telemetry internship context in the bio sample?",
          "answer": "The purple cluster represents the Attention method's performance on job classification, not the specific bio content about cardiac telemetry internship, which is merely example input text the model processes.",
          "visual_anchor": "Purple points (Attention) clustered tightly at accuracy range 0.84-0.845",
          "text_evidence": "Following graduation she completed a cardiac telemetry internship at Texas Health Resources Fort Worth. She continued to work there for two years taking care of postsurgical cardiac patients",
          "query_type": "value_context"
        },
        {
          "query": "How does LAFTR's near-zero equality opportunity difference at 0.84 accuracy relate to the Baylor University nursing degree mentioned?",
          "answer": "The LAFTR yellow point at near-zero fairness represents algorithmic performance metrics, independent of the specific biographical content about Baylor University nursing education that serves as input data.",
          "visual_anchor": "Yellow point (LAFTR) at accuracy ~0.84 and equality opportunity difference ~0.0",
          "text_evidence": "She graduated from Baylor University Louise Herrington School of Nursing in with her Bachelors of Science in Nursing",
          "query_type": "value_context"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does MIFR show the widest accuracy spread from 0.795 to 0.835 despite the job classification task using bio texts?",
        "answer": "MIFR's wide accuracy spread likely reflects varying performance when attention weights focus on different top three words in bio texts for job prediction, as shown in the green regions of qualitative results.",
        "visual_anchor": "Green points (MIFR) spanning horizontally from accuracy 0.795 to 0.835",
        "text_evidence": "Green regions represent top three words that the model used for its prediction based on the attention weights.",
        "query_type": "anomaly_cause"
      },
      {
        "query": "Does the purple cluster's concentration at accuracy 0.84-0.845 align with the cardiac telemetry internship context in the bio sample?",
        "answer": "The purple cluster represents the Attention method's performance on job classification, not the specific bio content about cardiac telemetry internship, which is merely example input text the model processes.",
        "visual_anchor": "Purple points (Attention) clustered tightly at accuracy range 0.84-0.845",
        "text_evidence": "Following graduation she completed a cardiac telemetry internship at Texas Health Resources Fort Worth. She continued to work there for two years taking care of postsurgical cardiac patients",
        "query_type": "value_context"
      },
      {
        "query": "How does LAFTR's near-zero equality opportunity difference at 0.84 accuracy relate to the Baylor University nursing degree mentioned?",
        "answer": "The LAFTR yellow point at near-zero fairness represents algorithmic performance metrics, independent of the specific biographical content about Baylor University nursing education that serves as input data.",
        "visual_anchor": "Yellow point (LAFTR) at accuracy ~0.84 and equality opportunity difference ~0.0",
        "text_evidence": "She graduated from Baylor University Louise Herrington School of Nursing in with her Bachelors of Science in Nursing",
        "query_type": "value_context"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Purple points labeled 'Attention (Ours)' clustered at accuracy ~0.84-0.845 and equality opportunity difference ~0.02-0.18",
      "Yellow points labeled 'LAFTR' scattered at accuracy ~0.84 and equality opportunity difference ~0.0-0.17",
      "Green points labeled 'MIFR' ranging from accuracy ~0.795-0.835 and equality opportunity difference ~0.05-0.25",
      "Error bars on all data points showing variance",
      "Three distinct method clusters with different accuracy-fairness trade-offs"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_9",
    "figure_number": 6,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg",
    "caption": "Figure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Purple markers with crosses labeled 'Attention (Ours)'\",\n    \"Green markers with circles labeled 'MIFR'\",\n    \"X-axis showing Accuracy from 0.66 to 0.78\",\n    \"Y-axis showing Equality Opportunity Difference from 0.0 to 0.7\",\n    \"Error bars on both curves\",\n    \"Green curve reaching approximately 0.69 accuracy at 0.69 EQOP\",\n    \"Purple curve reaching approximately 0.77 accuracy at 0.65 EQOP\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does the purple Attention curve achieve 0.77 accuracy at lower EQOP than MIFR's 0.69 accuracy peak?\",\n      \"answer\": \"The Attention method maintains higher accuracy (0.77) while achieving better equality of opportunity (lower EQOP around 0.65) compared to MIFR, demonstrating superior fairness-accuracy trade-offs on the Heritage Health dataset.\",\n      \"visual_anchor\": \"Purple 'Attention (Ours)' markers reaching 0.77 accuracy at approximately 0.65 EQOP versus green 'MIFR' markers at 0.69 accuracy\",\n      \"text_evidence\": \"Attention (Ours) 0.77 (0.006) ... 0.81 (0.013) ... when all problematic features are zeroed out\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the green MIFR curve's steep climb from 0.05 to 0.69 EQOP support post-processing approaches outperforming attention?\",\n      \"answer\": \"No, the MIFR curve shows worse performance. Despite the steep climb, MIFR achieves lower maximum accuracy and higher EQOP values compared to the Attention method, contradicting any claim of superiority.\",\n      \"visual_anchor\": \"Green MIFR curve showing steep increase from approximately 0.05 to 0.69 on the EQOP axis\",\n      \"text_evidence\": \"Adult results on post-processing approach from Hardt et al. vs our attention method\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"What explains the purple curve's horizontal error bars widening significantly between 0.68 and 0.76 accuracy regions?\",\n      \"answer\": \"The widening horizontal error bars indicate increasing variance in accuracy measurements as the Attention method explores different fairness-accuracy trade-off points, reflecting uncertainty in model performance across different configurations on the Heritage Health dataset.\",\n      \"visual_anchor\": \"Purple markers showing progressively wider horizontal error bars in the 0.68 to 0.76 accuracy range\",\n      \"text_evidence\": \"Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Purple markers with crosses labeled 'Attention (Ours)'",
        "Green markers with circles labeled 'MIFR'",
        "X-axis showing Accuracy from 0.66 to 0.78",
        "Y-axis showing Equality Opportunity Difference from 0.0 to 0.7",
        "Error bars on both curves",
        "Green curve reaching approximately 0.69 accuracy at 0.69 EQOP",
        "Purple curve reaching approximately 0.77 accuracy at 0.65 EQOP"
      ],
      "queries": [
        {
          "query": "Why does the purple Attention curve achieve 0.77 accuracy at lower EQOP than MIFR's 0.69 accuracy peak?",
          "answer": "The Attention method maintains higher accuracy (0.77) while achieving better equality of opportunity (lower EQOP around 0.65) compared to MIFR, demonstrating superior fairness-accuracy trade-offs on the Heritage Health dataset.",
          "visual_anchor": "Purple 'Attention (Ours)' markers reaching 0.77 accuracy at approximately 0.65 EQOP versus green 'MIFR' markers at 0.69 accuracy",
          "text_evidence": "Attention (Ours) 0.77 (0.006) ... 0.81 (0.013) ... when all problematic features are zeroed out",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the green MIFR curve's steep climb from 0.05 to 0.69 EQOP support post-processing approaches outperforming attention?",
          "answer": "No, the MIFR curve shows worse performance. Despite the steep climb, MIFR achieves lower maximum accuracy and higher EQOP values compared to the Attention method, contradicting any claim of superiority.",
          "visual_anchor": "Green MIFR curve showing steep increase from approximately 0.05 to 0.69 on the EQOP axis",
          "text_evidence": "Adult results on post-processing approach from Hardt et al. vs our attention method",
          "query_type": "comparison_explanation"
        },
        {
          "query": "What explains the purple curve's horizontal error bars widening significantly between 0.68 and 0.76 accuracy regions?",
          "answer": "The widening horizontal error bars indicate increasing variance in accuracy measurements as the Attention method explores different fairness-accuracy trade-off points, reflecting uncertainty in model performance across different configurations on the Heritage Health dataset.",
          "visual_anchor": "Purple markers showing progressively wider horizontal error bars in the 0.68 to 0.76 accuracy range",
          "text_evidence": "Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does the purple Attention curve achieve 0.77 accuracy at lower EQOP than MIFR's 0.69 accuracy peak?",
        "answer": "The Attention method maintains higher accuracy (0.77) while achieving better equality of opportunity (lower EQOP around 0.65) compared to MIFR, demonstrating superior fairness-accuracy trade-offs on the Heritage Health dataset.",
        "visual_anchor": "Purple 'Attention (Ours)' markers reaching 0.77 accuracy at approximately 0.65 EQOP versus green 'MIFR' markers at 0.69 accuracy",
        "text_evidence": "Attention (Ours) 0.77 (0.006) ... 0.81 (0.013) ... when all problematic features are zeroed out",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the green MIFR curve's steep climb from 0.05 to 0.69 EQOP support post-processing approaches outperforming attention?",
        "answer": "No, the MIFR curve shows worse performance. Despite the steep climb, MIFR achieves lower maximum accuracy and higher EQOP values compared to the Attention method, contradicting any claim of superiority.",
        "visual_anchor": "Green MIFR curve showing steep increase from approximately 0.05 to 0.69 on the EQOP axis",
        "text_evidence": "Adult results on post-processing approach from Hardt et al. vs our attention method",
        "query_type": "comparison_explanation"
      },
      {
        "query": "What explains the purple curve's horizontal error bars widening significantly between 0.68 and 0.76 accuracy regions?",
        "answer": "The widening horizontal error bars indicate increasing variance in accuracy measurements as the Attention method explores different fairness-accuracy trade-off points, reflecting uncertainty in model performance across different configurations on the Heritage Health dataset.",
        "visual_anchor": "Purple markers showing progressively wider horizontal error bars in the 0.68 to 0.76 accuracy range",
        "text_evidence": "Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Purple markers with crosses labeled 'Attention (Ours)'",
      "Green markers with circles labeled 'MIFR'",
      "X-axis showing Accuracy from 0.66 to 0.78",
      "Y-axis showing Equality Opportunity Difference from 0.0 to 0.7",
      "Error bars on both curves",
      "Green curve reaching approximately 0.69 accuracy at 0.69 EQOP",
      "Purple curve reaching approximately 0.77 accuracy at 0.65 EQOP"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_11",
    "figure_number": 7,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg",
    "caption": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\"Purple points with error bars labeled 'Attention (Ours)'\", \"Green points with error bars labeled 'MIFR'\", \"X-axis showing Accuracy from 0.66 to 0.77\", \"Y-axis showing Equalized Odds Difference from 0.0 to 0.7\", \"Error bars on both methods\", \"Two distinct clusters of points\"],\n  \"queries\": [\n    {\n      \"query\": \"Why do the purple Attention points cluster between 0.68-0.76 accuracy when Heritage Health dataset results are presented?\",\n      \"answer\": \"The plot shows accuracy versus equalized odds curves specifically for the Heritage Health dataset, which explains why the Attention method's performance spans this accuracy range with varying fairness trade-offs.\",\n      \"visual_anchor\": \"Purple points labeled 'Attention (Ours)' spanning accuracy range 0.68-0.76\",\n      \"text_evidence\": \"Accuracy vs EQOD (Heritage Health)   \\nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Do the green MIFR points at accuracy 0.66-0.68 support the UCI Adult dataset's inclusion in this comparison?\",\n      \"answer\": \"Yes, the leftmost cluster of green MIFR points represents results from the UCI Adult dataset, which is explicitly stated as one of the two datasets being compared in this accuracy-fairness trade-off analysis.\",\n      \"visual_anchor\": \"Green MIFR points clustered at lower accuracy values 0.66-0.68\",\n      \"text_evidence\": \"Accuracy vs EQOD (UCI Adult)\\nText after: Accuracy vs EQOD (Heritage Health)   \\nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How does the purple Attention method's maximum accuracy of approximately 0.76 compare to MIFR's 0.77 on Heritage Health?\",\n      \"answer\": \"The green MIFR points reach slightly higher accuracy (approximately 0.77) compared to the purple Attention method's maximum (approximately 0.76), though both methods show similar equalized odds differences around 0.65-0.70 at their highest accuracy levels.\",\n      \"visual_anchor\": \"Rightmost purple and green points showing maximum accuracy values around 0.76-0.77\",\n      \"text_evidence\": \"Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.\\nReferences: - Accuracy vs EQOD (Heritage Health)\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Purple points with error bars labeled 'Attention (Ours)'",
        "Green points with error bars labeled 'MIFR'",
        "X-axis showing Accuracy from 0.66 to 0.77",
        "Y-axis showing Equalized Odds Difference from 0.0 to 0.7",
        "Error bars on both methods",
        "Two distinct clusters of points"
      ],
      "queries": [
        {
          "query": "Why do the purple Attention points cluster between 0.68-0.76 accuracy when Heritage Health dataset results are presented?",
          "answer": "The plot shows accuracy versus equalized odds curves specifically for the Heritage Health dataset, which explains why the Attention method's performance spans this accuracy range with varying fairness trade-offs.",
          "visual_anchor": "Purple points labeled 'Attention (Ours)' spanning accuracy range 0.68-0.76",
          "text_evidence": "Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
          "query_type": "value_context"
        },
        {
          "query": "Do the green MIFR points at accuracy 0.66-0.68 support the UCI Adult dataset's inclusion in this comparison?",
          "answer": "Yes, the leftmost cluster of green MIFR points represents results from the UCI Adult dataset, which is explicitly stated as one of the two datasets being compared in this accuracy-fairness trade-off analysis.",
          "visual_anchor": "Green MIFR points clustered at lower accuracy values 0.66-0.68",
          "text_evidence": "Accuracy vs EQOD (UCI Adult)\nText after: Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
          "query_type": "value_context"
        },
        {
          "query": "How does the purple Attention method's maximum accuracy of approximately 0.76 compare to MIFR's 0.77 on Heritage Health?",
          "answer": "The green MIFR points reach slightly higher accuracy (approximately 0.77) compared to the purple Attention method's maximum (approximately 0.76), though both methods show similar equalized odds differences around 0.65-0.70 at their highest accuracy levels.",
          "visual_anchor": "Rightmost purple and green points showing maximum accuracy values around 0.76-0.77",
          "text_evidence": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.\nReferences: - Accuracy vs EQOD (Heritage Health)",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do the purple Attention points cluster between 0.68-0.76 accuracy when Heritage Health dataset results are presented?",
        "answer": "The plot shows accuracy versus equalized odds curves specifically for the Heritage Health dataset, which explains why the Attention method's performance spans this accuracy range with varying fairness trade-offs.",
        "visual_anchor": "Purple points labeled 'Attention (Ours)' spanning accuracy range 0.68-0.76",
        "text_evidence": "Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
        "query_type": "value_context"
      },
      {
        "query": "Do the green MIFR points at accuracy 0.66-0.68 support the UCI Adult dataset's inclusion in this comparison?",
        "answer": "Yes, the leftmost cluster of green MIFR points represents results from the UCI Adult dataset, which is explicitly stated as one of the two datasets being compared in this accuracy-fairness trade-off analysis.",
        "visual_anchor": "Green MIFR points clustered at lower accuracy values 0.66-0.68",
        "text_evidence": "Accuracy vs EQOD (UCI Adult)\nText after: Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
        "query_type": "value_context"
      },
      {
        "query": "How does the purple Attention method's maximum accuracy of approximately 0.76 compare to MIFR's 0.77 on Heritage Health?",
        "answer": "The green MIFR points reach slightly higher accuracy (approximately 0.77) compared to the purple Attention method's maximum (approximately 0.76), though both methods show similar equalized odds differences around 0.65-0.70 at their highest accuracy levels.",
        "visual_anchor": "Rightmost purple and green points showing maximum accuracy values around 0.76-0.77",
        "text_evidence": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.\nReferences: - Accuracy vs EQOD (Heritage Health)",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Purple points with error bars labeled 'Attention (Ours)'",
      "Green points with error bars labeled 'MIFR'",
      "X-axis showing Accuracy from 0.66 to 0.77",
      "Y-axis showing Equalized Odds Difference from 0.0 to 0.7",
      "Error bars on both methods",
      "Two distinct clusters of points"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_13",
    "figure_number": 8,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig12.jpg",
    "caption": "Figure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.",
    "original_quality_score": 0.85,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Three grouped bar charts showing Statistical Parity, Equality of Opportunity, and Equalized Odds\",\n    \"Red bars for dds_max feature (~4%, ~4%, ~4%)\",\n    \"Yellow bars for dds_range feature (~3.2%, ~3%, ~3%)\",\n    \"Blue bars for LOS_TOT_UNKNOWN, drugsdont_maths, drugsdont_maths (~2.8%, ~2.8%, ~2.8%)\",\n    \"Y-axis showing % improvement after removal (0% to 5%)\",\n    \"X-axis labels with feature names rotated diagonally\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why does removing dds_max yield approximately 4% improvement across all three fairness definitions shown in red bars?\",\n      \"answer\": \"The dds_max feature is identified as the top problematic feature for adult and heritage health datasets, causing the most benefit when removed across Statistical Parity, Equality of Opportunity, and Equalized Odds.\",\n      \"visual_anchor\": \"Red bars at approximately 4% height in all three grouped bar charts\",\n      \"text_evidence\": \"Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition.\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the yellow bar showing dds_range at roughly 3% improvement align with it being a top problematic feature?\",\n      \"answer\": \"Yes, dds_range consistently ranks as the second most problematic feature across all fairness definitions, with its removal providing approximately 3% improvement as marked on the y-axis.\",\n      \"visual_anchor\": \"Yellow bars at approximately 3% height across all three fairness definition charts\",\n      \"text_evidence\": \"The percentage of improvement upon removal is marked on the y-axis for adult and heritage health datasets.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"How do the blue bars for LOS_TOT_UNKNOWN and drugsdont_maths at 2.8% compare as third-ranked problematic features?\",\n      \"answer\": \"Both features show nearly identical improvement percentages (~2.8%) when removed, indicating they have similar negative impacts on fairness despite being from different datasets (adult vs. health).\",\n      \"visual_anchor\": \"Blue bars at approximately 2.8% in Statistical Parity (LOS_TOT_UNKNOWN) and other charts (drugsdont_maths)\",\n      \"text_evidence\": \"Top Problematic Features from Adult ... Top Problematic Features from Health\",\n      \"query_type\": \"comparison_explanation\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Three grouped bar charts showing Statistical Parity, Equality of Opportunity, and Equalized Odds",
        "Red bars for dds_max feature (~4%, ~4%, ~4%)",
        "Yellow bars for dds_range feature (~3.2%, ~3%, ~3%)",
        "Blue bars for LOS_TOT_UNKNOWN, drugsdont_maths, drugsdont_maths (~2.8%, ~2.8%, ~2.8%)",
        "Y-axis showing % improvement after removal (0% to 5%)",
        "X-axis labels with feature names rotated diagonally"
      ],
      "queries": [
        {
          "query": "Why does removing dds_max yield approximately 4% improvement across all three fairness definitions shown in red bars?",
          "answer": "The dds_max feature is identified as the top problematic feature for adult and heritage health datasets, causing the most benefit when removed across Statistical Parity, Equality of Opportunity, and Equalized Odds.",
          "visual_anchor": "Red bars at approximately 4% height in all three grouped bar charts",
          "text_evidence": "Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition.",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the yellow bar showing dds_range at roughly 3% improvement align with it being a top problematic feature?",
          "answer": "Yes, dds_range consistently ranks as the second most problematic feature across all fairness definitions, with its removal providing approximately 3% improvement as marked on the y-axis.",
          "visual_anchor": "Yellow bars at approximately 3% height across all three fairness definition charts",
          "text_evidence": "The percentage of improvement upon removal is marked on the y-axis for adult and heritage health datasets.",
          "query_type": "value_context"
        },
        {
          "query": "How do the blue bars for LOS_TOT_UNKNOWN and drugsdont_maths at 2.8% compare as third-ranked problematic features?",
          "answer": "Both features show nearly identical improvement percentages (~2.8%) when removed, indicating they have similar negative impacts on fairness despite being from different datasets (adult vs. health).",
          "visual_anchor": "Blue bars at approximately 2.8% in Statistical Parity (LOS_TOT_UNKNOWN) and other charts (drugsdont_maths)",
          "text_evidence": "Top Problematic Features from Adult ... Top Problematic Features from Health",
          "query_type": "comparison_explanation"
        }
      ]
    },
    "queries": [
      {
        "query": "Why does removing dds_max yield approximately 4% improvement across all three fairness definitions shown in red bars?",
        "answer": "The dds_max feature is identified as the top problematic feature for adult and heritage health datasets, causing the most benefit when removed across Statistical Parity, Equality of Opportunity, and Equalized Odds.",
        "visual_anchor": "Red bars at approximately 4% height in all three grouped bar charts",
        "text_evidence": "Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition.",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the yellow bar showing dds_range at roughly 3% improvement align with it being a top problematic feature?",
        "answer": "Yes, dds_range consistently ranks as the second most problematic feature across all fairness definitions, with its removal providing approximately 3% improvement as marked on the y-axis.",
        "visual_anchor": "Yellow bars at approximately 3% height across all three fairness definition charts",
        "text_evidence": "The percentage of improvement upon removal is marked on the y-axis for adult and heritage health datasets.",
        "query_type": "value_context"
      },
      {
        "query": "How do the blue bars for LOS_TOT_UNKNOWN and drugsdont_maths at 2.8% compare as third-ranked problematic features?",
        "answer": "Both features show nearly identical improvement percentages (~2.8%) when removed, indicating they have similar negative impacts on fairness despite being from different datasets (adult vs. health).",
        "visual_anchor": "Blue bars at approximately 2.8% in Statistical Parity (LOS_TOT_UNKNOWN) and other charts (drugsdont_maths)",
        "text_evidence": "Top Problematic Features from Adult ... Top Problematic Features from Health",
        "query_type": "comparison_explanation"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Three grouped bar charts showing Statistical Parity, Equality of Opportunity, and Equalized Odds",
      "Red bars for dds_max feature (~4%, ~4%, ~4%)",
      "Yellow bars for dds_range feature (~3.2%, ~3%, ~3%)",
      "Blue bars for LOS_TOT_UNKNOWN, drugsdont_maths, drugsdont_maths (~2.8%, ~2.8%, ~2.8%)",
      "Y-axis showing % improvement after removal (0% to 5%)",
      "X-axis labels with feature names rotated diagonally"
    ]
  },
  {
    "doc_id": "2109.03952",
    "figure_id": "2109.03952_fig_14",
    "figure_number": 9,
    "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg",
    "caption": "Figure 9: Results from the real-world datasets. Note that in our $\\hat { y } _ { z }$ notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema, but the feature names are more universal and discriptive for this case. Labels on the points represent the feature name that was removed (zeroed out) according to our $\\hat { y } _ { z }$ notation. The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
    "original_quality_score": 0.7,
    "raw_response": "```json\n{\n  \"figure_type\": \"plot\",\n  \"visual_elements\": [\n    \"Blue points representing features removed with labels like 'capital loss', 'capital gain', 'workclass', 'occupation', 'relationship', 'marital status', 'sex', 'fnlwgt'\",\n    \"Red point labeled 'original' at approximately (0.85, 0.21)\",\n    \"X-axis showing Accuracy ranging from 0.77 to 0.86\",\n    \"Y-axis showing SPD ranging from 0.15 to 0.36\",\n    \"Cyan background region\"\n  ],\n  \"queries\": [\n    {\n      \"query\": \"Why do capital loss and capital gain points cluster near 0.78 accuracy, given removal improves fairness definitions?\",\n      \"answer\": \"Removing capital loss and capital gain features substantially reduces accuracy but improves statistical parity difference (SPD) to around 0.30-0.33, showing these features contribute to both model performance and unfairness.\",\n      \"visual_anchor\": \"Blue points labeled 'capital loss' and 'capital gain' at approximately (0.78, 0.33) and (0.79, 0.31)\",\n      \"text_evidence\": \"Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the y-axis\",\n      \"query_type\": \"comparison_explanation\"\n    },\n    {\n      \"query\": \"Does the original red point at 0.85 accuracy confirm that feature exclusion changes fairness in statistical parity difference?\",\n      \"answer\": \"Yes, the original model at (0.85, 0.21) SPD serves as the baseline, and all blue points show how zeroing out individual features shifts both accuracy and SPD, demonstrating feature-specific fairness impacts.\",\n      \"visual_anchor\": \"Red point labeled 'original' at approximately (0.85, 0.21)\",\n      \"text_evidence\": \"The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.\",\n      \"query_type\": \"value_context\"\n    },\n    {\n      \"query\": \"Why does removing sex reduce SPD to approximately 0.17 while maintaining 0.85 accuracy compared to other features?\",\n      \"answer\": \"The sex feature appears to encode substantial bias with minimal contribution to accuracy. Its removal improves fairness significantly while preserving model performance, unlike capital features which trade accuracy for fairness.\",\n      \"visual_anchor\": \"Blue point labeled 'sex' at approximately (0.85, 0.17)\",\n      \"text_evidence\": \"In our ŷ_z notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema\",\n      \"query_type\": \"anomaly_cause\"\n    }\n  ]\n}\n```",
    "parsed": {
      "figure_type": "plot",
      "visual_elements": [
        "Blue points representing features removed with labels like 'capital loss', 'capital gain', 'workclass', 'occupation', 'relationship', 'marital status', 'sex', 'fnlwgt'",
        "Red point labeled 'original' at approximately (0.85, 0.21)",
        "X-axis showing Accuracy ranging from 0.77 to 0.86",
        "Y-axis showing SPD ranging from 0.15 to 0.36",
        "Cyan background region"
      ],
      "queries": [
        {
          "query": "Why do capital loss and capital gain points cluster near 0.78 accuracy, given removal improves fairness definitions?",
          "answer": "Removing capital loss and capital gain features substantially reduces accuracy but improves statistical parity difference (SPD) to around 0.30-0.33, showing these features contribute to both model performance and unfairness.",
          "visual_anchor": "Blue points labeled 'capital loss' and 'capital gain' at approximately (0.78, 0.33) and (0.79, 0.31)",
          "text_evidence": "Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the y-axis",
          "query_type": "comparison_explanation"
        },
        {
          "query": "Does the original red point at 0.85 accuracy confirm that feature exclusion changes fairness in statistical parity difference?",
          "answer": "Yes, the original model at (0.85, 0.21) SPD serves as the baseline, and all blue points show how zeroing out individual features shifts both accuracy and SPD, demonstrating feature-specific fairness impacts.",
          "visual_anchor": "Red point labeled 'original' at approximately (0.85, 0.21)",
          "text_evidence": "The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
          "query_type": "value_context"
        },
        {
          "query": "Why does removing sex reduce SPD to approximately 0.17 while maintaining 0.85 accuracy compared to other features?",
          "answer": "The sex feature appears to encode substantial bias with minimal contribution to accuracy. Its removal improves fairness significantly while preserving model performance, unlike capital features which trade accuracy for fairness.",
          "visual_anchor": "Blue point labeled 'sex' at approximately (0.85, 0.17)",
          "text_evidence": "In our ŷ_z notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema",
          "query_type": "anomaly_cause"
        }
      ]
    },
    "queries": [
      {
        "query": "Why do capital loss and capital gain points cluster near 0.78 accuracy, given removal improves fairness definitions?",
        "answer": "Removing capital loss and capital gain features substantially reduces accuracy but improves statistical parity difference (SPD) to around 0.30-0.33, showing these features contribute to both model performance and unfairness.",
        "visual_anchor": "Blue points labeled 'capital loss' and 'capital gain' at approximately (0.78, 0.33) and (0.79, 0.31)",
        "text_evidence": "Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the y-axis",
        "query_type": "comparison_explanation"
      },
      {
        "query": "Does the original red point at 0.85 accuracy confirm that feature exclusion changes fairness in statistical parity difference?",
        "answer": "Yes, the original model at (0.85, 0.21) SPD serves as the baseline, and all blue points show how zeroing out individual features shifts both accuracy and SPD, demonstrating feature-specific fairness impacts.",
        "visual_anchor": "Red point labeled 'original' at approximately (0.85, 0.21)",
        "text_evidence": "The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
        "query_type": "value_context"
      },
      {
        "query": "Why does removing sex reduce SPD to approximately 0.17 while maintaining 0.85 accuracy compared to other features?",
        "answer": "The sex feature appears to encode substantial bias with minimal contribution to accuracy. Its removal improves fairness significantly while preserving model performance, unlike capital features which trade accuracy for fairness.",
        "visual_anchor": "Blue point labeled 'sex' at approximately (0.85, 0.17)",
        "text_evidence": "In our ŷ_z notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema",
        "query_type": "anomaly_cause"
      }
    ],
    "figure_type_mllm": "plot",
    "visual_elements": [
      "Blue points representing features removed with labels like 'capital loss', 'capital gain', 'workclass', 'occupation', 'relationship', 'marital status', 'sex', 'fnlwgt'",
      "Red point labeled 'original' at approximately (0.85, 0.21)",
      "X-axis showing Accuracy ranging from 0.77 to 0.86",
      "Y-axis showing SPD ranging from 0.15 to 0.36",
      "Cyan background region"
    ]
  }
]