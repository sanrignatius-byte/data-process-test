{"query_id": "l1_1412.3756_1412.3756_fig_5_0", "query": "What accuracy does the RLR method achieve at a fairness value of 0.95 in the Previous Work plot for Adult Income Data, and what does the text state about algorithm-dependent performance differences?", "answer": "RLR achieves an accuracy of 0.68 at 0.95 fairness, and the text states that repair performance varies substantially depending on the specific algorithms chosen.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "visual_anchor": "purple square (RLR) marker at fairness 0.95 in Previous Work plot for Adult Income Data", "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose.", "query_type": "value_context"}
{"query_id": "l1_1412.3756_1412.3756_fig_5_1", "query": "In the Geometric plot for German Credit Data, what accuracy does SVM reach at a fairness value of 0.85, and how does the text explain the significance of this result?", "answer": "SVM achieves 0.75 accuracy at 0.85 fairness, and the text states that repair performance varies substantially depending on the specific algorithms chosen, highlighting the need for algorithm-specific analysis.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "visual_anchor": "red SVM marker at fairness 0.85 in Geometric plot for German Credit Data", "text_evidence": "Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose.", "query_type": "value_context"}
{"query_id": "l1_1611.07438_1611.07438_fig_1_2", "query": "In Figure 1, which variable mediates the causal path from gender to admission, and what does the text state about the requirement for this mediator in the non-discrimination criterion?", "answer": "The 'major' variable mediates the effect; the text requires that |ΔP_b| < τ for each subpopulation when conditioning on this mediator as part of block set B.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg", "caption": "Figure 1: Causal graph of an example university admission system.", "figure_type": "diagram", "visual_anchor": "the node labeled 'major' with arrows from gender to major and major to admission", "text_evidence": "if each block set B is examined and ensures that |ΔP_b| < τ holds for each subpopulation b", "query_type": "visual_definition"}
{"query_id": "l1_1703.06856_1703.06856_fig_8_3", "query": "Which node in the causal model is exclusively caused by Criminality but not by Race, as shown by the single arrow originating from Criminality? [from figure], and how does the text confirm this node represents a directly observed outcome in the NYPD dataset?", "answer": "Weapon. The text states that officers record 'if a weapon was found' as part of the stop-and-frisk data collection process.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "visual_anchor": "node labeled Weapon with arrow only from Criminality", "text_evidence": "if a weapon was found", "query_type": "visual_definition"}
{"query_id": "l1_1703.06856_1703.06856_fig_8_4", "query": "What outcome node is directly influenced by Race but not by Criminality, as indicated by the single arrow from Race to this node? [from figure], and what does the text specify about this variable's role in the stop-and-frisk process?", "answer": "Arrest. The text states that officers record 'whether an arrest was made' as a key outcome of stops.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "visual_anchor": "node labeled Arrest with arrow only from Race", "text_evidence": "whether an arrest was made", "query_type": "visual_definition"}
{"query_id": "l1_1706.02744_1706.02744_fig_1_5", "query": "Which node in Figure 1 is both affected by A and influences R, and how does the text define its role in the causal structure?", "answer": "Node X; the text states it is a department choice affected by gender A that influences admission decision R.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "visual_anchor": "node X", "text_evidence": "admission decision R does not only directly depend on gender A, but also on department choice X, which in turn is also affected by gender A", "query_type": "visual_definition"}
{"query_id": "l1_1706.02744_1706.02744_fig_1_6", "query": "Identify the arrow in Figure 1 that represents the direct causal link between A and R as specified to exclude paths through X.", "answer": "The arrow from A to R; the text specifies direct effect cannot be ascribed to X.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "visual_anchor": "arrow from A to R", "text_evidence": "direct effect of the protected attribute (here, gender A) on the decision (here, college admission R) that cannot be ascribed to a resolving variable such as department choice X", "query_type": "visual_definition"}
{"query_id": "l1_1706.02744_1706.02744_fig_1_7", "query": "What specific path in Figure 1 is excluded from the direct effect of A on R according to the text's definition?", "answer": "The path A→X→R; the text defines direct effect as excluding paths through X.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "visual_anchor": "path A→X→R", "text_evidence": "direct effect of the protected attribute (here, gender A) on the decision (here, college admission R) that cannot be ascribed to a resolving variable such as department choice X", "query_type": "visual_definition"}
{"query_id": "l1_1706.02744_1706.02744_fig_4_8", "query": "In the right graph of Figure 3, what does the label 'g' on the P→R edge indicate about the intervention that cancels the influence along P→X→R, as explained in the text?", "answer": "The label 'g' represents the adjusted P→R edge that cancels the influence along P→X→R, as stated in the text's description of the intervention.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "visual_anchor": "label 'g' on the P→R edge in the right graph", "text_evidence": "we want to guarantee that the proxy P has no overall influence on the prediction, by adjusting P→R to cancel the influence along P→X→R in the intervened graph", "query_type": "value_context"}
{"query_id": "l1_1706.02744_1706.02744_fig_4_9", "query": "In the right graph of Figure 3, why is the arrow from P to X colored red, and how does this relate to the intervention described in the text?", "answer": "The red arrow indicates the P→X influence path that the P→R adjustment cancels, as explained in the text's description of the intervention.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "visual_anchor": "red arrow from P to X in the right graph", "text_evidence": "adjusting P→R to cancel the influence along P→X→R in the intervened graph", "query_type": "comparison_explanation"}
{"query_id": "l1_1706.02744_1706.02744_fig_4_10", "query": "What does the double red circle around P in the right graph of Figure 3 signify about the transformation from G̃ to G, as described in the text?", "answer": "The double red circle indicates the intervention on P that transforms the original graph G̃ into the intervened graph G, as referenced in the text.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "visual_anchor": "double red circle around P in the right graph", "text_evidence": "A template graph G̃ for proxy discrimination (left) with its intervened version G (right)", "query_type": "visual_definition"}
{"query_id": "l1_1809.02208_1809.02208_fig_8_11", "query": "What probability value does the Japanese-STEM cell represent, and why does the text state this cell is painted in a darker shade of blue?", "answer": "The cell shows near 0% probability, and the text explains this dark blue color indicates the complementary null hypothesis was rejected.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "heatmap", "visual_anchor": "the cell for Japanese in the STEM category showing dark blue", "text_evidence": "Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue", "query_type": "visual_definition"}
{"query_id": "l1_1809.02208_1809.02208_fig_8_12", "query": "What is the probability for Basque in the Construction Extraction category, and why does the text state this cell is painted dark blue?", "answer": "The probability is near 0% (dark blue), and the text explains this color indicates rejection of the complementary null hypothesis.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "heatmap", "visual_anchor": "the cell for Basque in the Construction Extraction category showing dark blue", "text_evidence": "Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue", "query_type": "visual_definition"}
{"query_id": "l1_1809.02208_1809.02208_fig_8_13", "query": "Why is the Finnish-Healthcare cell painted dark red while the Estonian-Legal cell is light blue, according to the text's explanation of color coding?", "answer": "The dark red Finnish-Healthcare cell indicates high probability (not rejected null hypothesis), while light blue Estonian-Legal indicates lower probability where the complementary null hypothesis was rejected.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "heatmap", "visual_anchor": "the cell for Finnish in Healthcare (dark red) versus Estonian in Legal (light blue)", "text_evidence": "Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_14_14", "query": "What is the Disparate Impact value for the 'mitigated' condition in the figure (teal bar), and what specific mitigation technique is referenced in the text as the cause of this value?", "answer": "The 'mitigated' bar shows a value of 0.85, achieved through adversarial debiasing mitigation as described in the text.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "visual_anchor": "the teal 'mitigated' bar in the Disparate Impact plot showing 0.85", "text_evidence": "After adversarial debiasing mitigation", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_14_15", "query": "The original bar in Figure 6 is 0.59 and the mitigated bar is 0.85. What mitigation method from the text explains this increase in Disparate Impact?", "answer": "The increase from 0.59 to 0.85 is due to adversarial debiasing mitigation as described in the text.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "visual_anchor": "the gray 'original' bar at 0.59 and teal 'mitigated' bar at 0.85", "text_evidence": "After adversarial debiasing mitigation", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_14_16", "query": "The 'Fair' threshold in the figure is marked at 0.85. What mitigation technique in the text achieves this Fair value?", "answer": "Adversarial debiasing mitigation is the technique referenced in the text that results in the Disparate Impact value of 0.85 reaching the Fair threshold.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "visual_anchor": "the 'Fair' label positioned next to the 0.85 value on the y-axis", "text_evidence": "After adversarial debiasing mitigation", "query_type": "visual_definition"}
{"query_id": "l1_1810.01943_1810.01943_fig_15_17", "query": "What do the favorable_label and unfavorable_label attributes in the BinaryLabelDataset class (visible in the figure) represent in the context of bias mitigation algorithms, as described in the text where it mentions these attributes are placeholders for real algorithms?", "answer": "The favorable_label and unfavorable_label attributes represent the values used to identify favorable and unfavorable outcomes in binary classification tasks, which are essential for calculating fairness metrics in bias mitigation algorithms as described in the text.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "visual_anchor": "the BinaryLabelDataset class with attributes +favorable_label: np.float64 and +unfavorable_label: np.float64", "text_evidence": "GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_15_18", "query": "How does the difference() method in DatasetMetric (visible in the figure) function as a metametric according to the text description of metametrics, and how does this differ from the binary_confusion_matrix() method in ClassificationMetric?", "answer": "The difference() method acts as a metametric by calculating the difference between privileged and unprivileged groups' metrics, while binary_confusion_matrix() provides the raw confusion matrix data for classification tasks.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "visual_anchor": "the difference() method in DatasetMetric versus the binary_confusion_matrix() method in ClassificationMetric", "text_evidence": "Some methods are 'metametrics' — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference", "query_type": "comparison_explanation"}
{"query_id": "l1_1810.01943_1810.01943_fig_15_19", "query": "What is the purpose of the memoize() decorator applied to the Metric class (visible in the figure), and how does this relate to the text's explanation that it's a Python decorator function automatically applied to every function in the class?", "answer": "The memoize decorator caches the results of expensive metric calculations to improve performance, as explained in the text where it states memoize is a Python decorator function automatically applied to every function in their respective classes.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "visual_anchor": "the memoize() method in the Metric class", "text_evidence": "memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes", "query_type": "visual_definition"}
{"query_id": "l1_1810.01943_1810.01943_fig_18_20", "query": "What is the balanced accuracy for LR, Disparate impact remover when statistical parity difference is 0.05 in the top-left plot, and why does the caption's definition of ideal fairness values matter for interpreting this result?", "answer": "The balanced accuracy is approximately 0.55. Since the caption states the ideal value for statistical parity difference is 0, this near-zero deviation indicates fair performance despite lower accuracy.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "visual_anchor": "LR, Disparate impact remover (green line) with mean at x=0.05 (statistical parity difference)", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_18_21", "query": "In the top-right plot, what balanced accuracy corresponds to RF, Reject option classification when equal opportunity difference is -0.15, and how does the caption explain the significance of this metric's ideal value?", "answer": "The balanced accuracy is approximately 0.62. The caption specifies the ideal value for equal opportunity difference is 0, so this small negative deviation indicates minimal unfairness.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "visual_anchor": "RF, Reject option classification (brown line) with mean at x=-0.15 (equal opportunity difference)", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "value_context"}
{"query_id": "l1_1810.01943_1810.01943_fig_18_22", "query": "What is the balanced accuracy for LR, Optimized pre-processing when statistical parity difference is -0.1 in the bottom-left plot, and how does the caption's ideal value guidance apply here?", "answer": "The balanced accuracy is approximately 0.65. The caption states the ideal statistical parity difference is 0, so this value indicates fair performance after mitigation.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "visual_anchor": "LR, Optimized pre-processing (orange line) with mean at x=-0.1 (statistical parity difference)", "text_evidence": "The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0.", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_7_23", "query": "What is the standard deviation (σ) value in the figure's statistics box, and how does the 'Validated Effect Size' section explain its significance in measuring bias removal?", "answer": "The standard deviation is 0.96661, and the 'Validated Effect Size' section states this value quantifies the spread of differential bias removal across documents.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "visual_anchor": "σ: 0.96661 in the statistics box", "text_evidence": "Validated Effect Size", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_7_24", "query": "The histogram peaks at x=0 with N=29344 documents. How does the 'Validated Effect Size' section interpret this peak as evidence of minimal bias?", "answer": "The peak at x=0 indicates most documents have near-zero differential bias removal, and the 'Validated Effect Size' section attributes this to effective bias mitigation.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "visual_anchor": "peak at x=0 with N=29344", "text_evidence": "Validated Effect Size", "query_type": "value_context"}
{"query_id": "l1_1810.03611_1810.03611_fig_7_25", "query": "What is the mean (μ) value displayed in the figure's statistics box, and how does the 'Validated Effect Size' section define its role in validating the bias removal process?", "answer": "The mean is -0.00236, and the 'Validated Effect Size' section specifies this near-zero value confirms the removal process does not introduce significant bias.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "visual_anchor": "μ: -0.00236 in the statistics box", "text_evidence": "Validated Effect Size", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_4_26", "query": "What percentage of participants in Study 1 were aged 70 and above [from the segment labeled '70 and above'], and what platform (from the text) enforced the location eligibility?", "answer": "2% and TurkPrime", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "pie chart", "visual_anchor": "the segment labeled '70 and above' in the pie chart", "text_evidence": "We stipulated this restriction via TurkPrime, which is a platform for performing crowdsourced research when using Amazon Mechanical Turk.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_4_27", "query": "What percentage of participants in Study 1 were aged 20-29 [from the segment labeled '20-29'], and what location-based eligibility criterion (from the text) applied to them?", "answer": "28% and being located in the United States of America", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "pie chart", "visual_anchor": "the segment labeled '20-29' in the pie chart", "text_evidence": "To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America.", "query_type": "value_context"}
{"query_id": "l1_1811.03654_1811.03654_fig_4_28", "query": "What percentage of participants in Study 1 were aged 50-59 [from the segment labeled '50-59'], and what location-based eligibility requirement (from the text) did they meet?", "answer": "13% and being located in the United States of America", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "pie chart", "visual_anchor": "the segment labeled '50-59' in the pie chart", "text_evidence": "To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_1_29", "query": "What minimum face region size (in pixels) was required for inclusion in the dataset according to the text, as demonstrated by the red bounding box in Figure 1?", "answer": "50x50 pixels. The text specifies that faces with region size less than 50x50 were discarded, and the red bounding box in Figure 1 represents a valid face meeting this criterion.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg", "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "figure_type": "photo", "visual_anchor": "red bounding box around face", "text_evidence": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded.", "query_type": "value_context"}
{"query_id": "l1_1901.10436_1901.10436_fig_8_30", "query": "The green inner outline of the left eyebrow in Figure 4 measures the inner region contrast for Coding Scheme 5. According to the text, what does high contrast in this region indicate about perceived age?", "answer": "High contrast in the eyebrow region correlates with faces being judged as younger, per the text's finding on cross-cultural age perception.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_8", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg", "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "figure_type": "photo", "visual_anchor": "green inner outline of the left eyebrow in Figure 4", "text_evidence": "high-contrast faces were judged to be younger than low-contrast face", "query_type": "visual_definition"}
{"query_id": "l1_1901.10436_1901.10436_fig_64_31", "query": "What is the exact count of female faces in the gender annotation (from the figure), and why does the text state that gender annotations have lower diversity than craniofacial schemes?", "answer": "The female face count is 4.5 x 10^6 (from the figure), and the text explains that gender annotations have lower diversity because craniofacial schemes have many dimensions with higher diversity scores compared to gender's two categories.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_64", "figure_number": 14, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg", "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.", "figure_type": "plot", "visual_anchor": "the blue bar for Female reaching 4.5 x 10^6", "text_evidence": "the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes", "query_type": "value_context"}
{"query_id": "l1_2103.11320_2103.11320_fig_30_32", "query": "What is the median number of triples for the 'bible' category in Figure 11, and how does the 'british-negative sentiment' mentioned in the text correspond to this median value?", "answer": "The median number of triples for 'bible' is approximately 600, and the text indicates this category is associated with 'british-negative sentiment', reflecting a higher representation of sentiment-related triples.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_30", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg", "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "figure_type": "plot", "visual_anchor": "median line of 'bible' box plot at ~600 triples", "text_evidence": "british-negative sentiment", "query_type": "value_context"}
