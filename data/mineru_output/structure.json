{
  "doc_id": "2602_01068",
  "pdf_path": "data/mineru_output/2602_01068/2602_01068/hybrid_auto/2602_01068_origin.pdf",
  "output_path": "data/mineru_output/2602_01068",
  "total_pages": 0,
  "elements": [
    {
      "element_id": "2602_01068_formula_0",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {E} \\leftarrow \\pi_ {\\mathrm {e}} (\\mathcal {C} \\oplus s \\oplus \\{t \\}), \\tag {1}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {E} \\leftarrow \\pi_ {\\mathrm {e}} (\\mathcal {C} \\oplus s \\oplus \\{t \\}), \\tag {1}"
      }
    },
    {
      "element_id": "2602_01068_fig_1",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/7d8fbf11a2f3588800d1c8859b5910f42b85f916f8f96271f0e76eedf527722d.jpg]",
      "bbox": null,
      "image_path": "images/7d8fbf11a2f3588800d1c8859b5910f42b85f916f8f96271f0e76eedf527722d.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/7d8fbf11a2f3588800d1c8859b5910f42b85f916f8f96271f0e76eedf527722d.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_2",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/fb27b3f87c8db286c45ca99642fd28d1952b3884fe64fed1c42623952630bd8c.jpg]",
      "bbox": null,
      "image_path": "images/fb27b3f87c8db286c45ca99642fd28d1952b3884fe64fed1c42623952630bd8c.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/fb27b3f87c8db286c45ca99642fd28d1952b3884fe64fed1c42623952630bd8c.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_3",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/6a6c8eeed40f74477eb4566a8f8e4747607335e357309ae66904b493b2c14d34.jpg]",
      "bbox": null,
      "image_path": "images/6a6c8eeed40f74477eb4566a8f8e4747607335e357309ae66904b493b2c14d34.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/6a6c8eeed40f74477eb4566a8f8e4747607335e357309ae66904b493b2c14d34.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_4",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/a43d037c3c916c96e9360d3f79b11b4cc9bab60f52101bd01fbb8dc93d4934c2.jpg]",
      "bbox": null,
      "image_path": "images/a43d037c3c916c96e9360d3f79b11b4cc9bab60f52101bd01fbb8dc93d4934c2.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/a43d037c3c916c96e9360d3f79b11b4cc9bab60f52101bd01fbb8dc93d4934c2.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_5",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/9a5942abbc11f8569b345c68d0707a0c06365dc4d475c16d47ada1a7959be2ff.jpg]",
      "bbox": null,
      "image_path": "images/9a5942abbc11f8569b345c68d0707a0c06365dc4d475c16d47ada1a7959be2ff.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/9a5942abbc11f8569b345c68d0707a0c06365dc4d475c16d47ada1a7959be2ff.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_6",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/652cd22d36f2cee77a1184dd546465187805a935433d670422cfa9b00745eb3c.jpg]",
      "bbox": null,
      "image_path": "images/652cd22d36f2cee77a1184dd546465187805a935433d670422cfa9b00745eb3c.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/652cd22d36f2cee77a1184dd546465187805a935433d670422cfa9b00745eb3c.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_7",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/204cb8db0fd4ada8d0803a34a1529aa8a92ca9481f4cc415992b0cf7f509a9af.jpg]",
      "bbox": null,
      "image_path": "images/204cb8db0fd4ada8d0803a34a1529aa8a92ca9481f4cc415992b0cf7f509a9af.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/204cb8db0fd4ada8d0803a34a1529aa8a92ca9481f4cc415992b0cf7f509a9af.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_8",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/9d814f9507aa9f656dba05f49ba5344d94d7e531eab093a58c849e55d52b5e14.jpg]",
      "bbox": null,
      "image_path": "images/9d814f9507aa9f656dba05f49ba5344d94d7e531eab093a58c849e55d52b5e14.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/9d814f9507aa9f656dba05f49ba5344d94d7e531eab093a58c849e55d52b5e14.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_9",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/24dd0ca8d34bfb916b5c0b5224c4660598f8db85ae39f46d37a2c630d10c1429.jpg]",
      "bbox": null,
      "image_path": "images/24dd0ca8d34bfb916b5c0b5224c4660598f8db85ae39f46d37a2c630d10c1429.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/24dd0ca8d34bfb916b5c0b5224c4660598f8db85ae39f46d37a2c630d10c1429.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_10",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/56f05fcb52c421af9772cdcbec655a8d439e02e0d076d622bfdb09123fa7616b.jpg]",
      "bbox": null,
      "image_path": "images/56f05fcb52c421af9772cdcbec655a8d439e02e0d076d622bfdb09123fa7616b.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/56f05fcb52c421af9772cdcbec655a8d439e02e0d076d622bfdb09123fa7616b.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_11",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/12dba081a5b2ac67ff44390a1c7951911af57b3ee46b70fcbaff9e296a36de8c.jpg]",
      "bbox": null,
      "image_path": "images/12dba081a5b2ac67ff44390a1c7951911af57b3ee46b70fcbaff9e296a36de8c.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/12dba081a5b2ac67ff44390a1c7951911af57b3ee46b70fcbaff9e296a36de8c.jpg"
      }
    },
    {
      "element_id": "2602_01068_formula_12",
      "page_idx": 0,
      "type": "formula",
      "content": "w \\left(s _ {i}\\right) = \\mathbf {1} \\left(s _ {i}\\right) \\cdot \\delta \\left(s _ {i}\\right), \\tag {2}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "w \\left(s _ {i}\\right) = \\mathbf {1} \\left(s _ {i}\\right) \\cdot \\delta \\left(s _ {i}\\right), \\tag {2}"
      }
    },
    {
      "element_id": "2602_01068_formula_13",
      "page_idx": 0,
      "type": "formula",
      "content": "\\delta \\left(s _ {i}\\right) = \\frac {\\left| \\mathcal {T} _ {i} \\right|}{\\sum_ {j = 1} ^ {n} \\left| \\mathcal {T} _ {j} \\right|}. \\tag {3}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\delta \\left(s _ {i}\\right) = \\frac {\\left| \\mathcal {T} _ {i} \\right|}{\\sum_ {j = 1} ^ {n} \\left| \\mathcal {T} _ {j} \\right|}. \\tag {3}"
      }
    },
    {
      "element_id": "2602_01068_formula_14",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\mathrm {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\mathrm {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, S (x)\\right) \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left[ \\sum_ {i = 1} ^ {n} w \\left(s _ {i}\\right) \\cdot \\log \\sigma \\left(\\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {\\left(\\mathrm {c}\\right)} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {r e f}} \\left(t _ {i} ^ {\\left(\\mathrm {c}\\right)} \\mid p _ {i}\\right)} - \\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {\\left(\\mathrm {r}\\right)} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {r e f}} \\left(t _ {i} ^ {\\left(\\mathrm {r}\\right)} \\mid p _ {i}\\right)}\\right) \\right]. \\tag {4}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\mathrm {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\mathrm {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, S (x)\\right) \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left[ \\sum_ {i = 1} ^ {n} w \\left(s _ {i}\\right) \\cdot \\log \\sigma \\left(\\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {\\left(\\mathrm {c}\\right)} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {r e f}} \\left(t _ {i} ^ {\\left(\\mathrm {c}\\right)} \\mid p _ {i}\\right)} - \\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {\\left(\\mathrm {r}\\right)} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {r e f}} \\left(t _ {i} ^ {\\left(\\mathrm {r}\\right)} \\mid p _ {i}\\right)}\\right) \\right]. \\tag {4}"
      }
    },
    {
      "element_id": "2602_01068_formula_15",
      "page_idx": 0,
      "type": "formula",
      "content": "\\beta_ {i} = \\frac {r \\left(s _ {i} , t _ {i} ^ {\\left(\\mathrm {c}\\right)}\\right) - r \\left(s _ {i} , t _ {i} ^ {\\left(\\mathrm {r}\\right)}\\right)}{\\max  \\left\\{r \\left(s _ {j} , t _ {j} ^ {\\left(\\mathrm {c}\\right)}\\right) - r \\left(s _ {j} , t _ {j} ^ {\\left(\\mathrm {r}\\right)}\\right) \\mid j \\in [ n ] \\right\\}}, \\tag {5}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\beta_ {i} = \\frac {r \\left(s _ {i} , t _ {i} ^ {\\left(\\mathrm {c}\\right)}\\right) - r \\left(s _ {i} , t _ {i} ^ {\\left(\\mathrm {r}\\right)}\\right)}{\\max  \\left\\{r \\left(s _ {j} , t _ {j} ^ {\\left(\\mathrm {c}\\right)}\\right) - r \\left(s _ {j} , t _ {j} ^ {\\left(\\mathrm {r}\\right)}\\right) \\mid j \\in [ n ] \\right\\}}, \\tag {5}"
      }
    },
    {
      "element_id": "2602_01068_formula_16",
      "page_idx": 0,
      "type": "formula",
      "content": "p _ {i} = x, \\hat {t} _ {1}, \\dots , \\hat {t} _ {i - 1}, \\quad \\hat {t} _ {j} \\leftarrow \\operatorname {M i x} \\left(t _ {j} ^ {(c)}, t _ {j} \\sim \\mathcal {T}, \\lambda\\right). \\tag {6}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "p _ {i} = x, \\hat {t} _ {1}, \\dots , \\hat {t} _ {i - 1}, \\quad \\hat {t} _ {j} \\leftarrow \\operatorname {M i x} \\left(t _ {j} ^ {(c)}, t _ {j} \\sim \\mathcal {T}, \\lambda\\right). \\tag {6}"
      }
    },
    {
      "element_id": "2602_01068_fig_17",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/5d1c04ac21a2361a069989c5152af60d068f048252fb99ea8469ab033d34ded4.jpg]",
      "bbox": null,
      "image_path": "images/5d1c04ac21a2361a069989c5152af60d068f048252fb99ea8469ab033d34ded4.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/5d1c04ac21a2361a069989c5152af60d068f048252fb99ea8469ab033d34ded4.jpg"
      }
    },
    {
      "element_id": "2602_01068_formula_18",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\mathrm {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, S (x)\\right) \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left(\\sum_ {i = 1} ^ {n} w \\left(s _ {i}\\right) \\cdot \\mathcal {L} _ {\\mathrm {p o}} \\left(s _ {i}\\right)\\right), \\tag {7}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\mathrm {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, S (x)\\right) \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left(\\sum_ {i = 1} ^ {n} w \\left(s _ {i}\\right) \\cdot \\mathcal {L} _ {\\mathrm {p o}} \\left(s _ {i}\\right)\\right), \\tag {7}"
      }
    },
    {
      "element_id": "2602_01068_formula_19",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\mathrm {p o}} (s _ {i}) =",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\mathrm {p o}} (s _ {i}) ="
      }
    },
    {
      "element_id": "2602_01068_formula_20",
      "page_idx": 0,
      "type": "formula",
      "content": "\\frac {1}{| \\mathcal {T} _ {i} |} \\sum_ {j} \\left[ \\min  \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\operatorname {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)} A _ {j}, \\operatorname {c l i p} \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\operatorname {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}, 1 - \\varepsilon , 1 + \\varepsilon\\right) A _ {j}\\right) - \\eta \\mathrm {K L} \\left(\\pi_ {\\theta} \\mid \\mid \\pi_ {\\text {r e f}}\\right) \\right], \\tag {8}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\frac {1}{| \\mathcal {T} _ {i} |} \\sum_ {j} \\left[ \\min  \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\operatorname {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)} A _ {j}, \\operatorname {c l i p} \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\operatorname {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}, 1 - \\varepsilon , 1 + \\varepsilon\\right) A _ {j}\\right) - \\eta \\mathrm {K L} \\left(\\pi_ {\\theta} \\mid \\mid \\pi_ {\\text {r e f}}\\right) \\right], \\tag {8}"
      }
    },
    {
      "element_id": "2602_01068_formula_21",
      "page_idx": 0,
      "type": "formula",
      "content": "A _ {j} = \\frac {\\mathcal {E} _ {i} ^ {j} - \\operatorname {m e a n} \\left(\\mathcal {E} _ {i}\\right)}{\\operatorname {s t d} \\left(\\mathcal {E} _ {i}\\right)}, \\quad \\mathrm {K L} \\left(\\pi_ {\\theta} \\| \\pi_ {\\text {r e f}}\\right) = \\frac {\\pi_ {\\text {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)} - \\log \\frac {\\pi_ {\\text {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)} - 1, \\tag {9}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "A _ {j} = \\frac {\\mathcal {E} _ {i} ^ {j} - \\operatorname {m e a n} \\left(\\mathcal {E} _ {i}\\right)}{\\operatorname {s t d} \\left(\\mathcal {E} _ {i}\\right)}, \\quad \\mathrm {K L} \\left(\\pi_ {\\theta} \\| \\pi_ {\\text {r e f}}\\right) = \\frac {\\pi_ {\\text {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)} - \\log \\frac {\\pi_ {\\text {r e f}} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)}{\\pi_ {\\theta} \\left(t _ {i} ^ {j} \\mid p _ {i}\\right)} - 1, \\tag {9}"
      }
    },
    {
      "element_id": "2602_01068_fig_22",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/99fd1dd21dad6e2de9b92801d1c8b83564d9a4f74d7bd0cddb95587bf61ddafe.jpg]",
      "bbox": null,
      "image_path": "images/99fd1dd21dad6e2de9b92801d1c8b83564d9a4f74d7bd0cddb95587bf61ddafe.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/99fd1dd21dad6e2de9b92801d1c8b83564d9a4f74d7bd0cddb95587bf61ddafe.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_23",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/e20bc68cb7efbd08cf77dc8fc96723e6bfb8d697f6e4f4c5785a91e7afe71d15.jpg]",
      "bbox": null,
      "image_path": "images/e20bc68cb7efbd08cf77dc8fc96723e6bfb8d697f6e4f4c5785a91e7afe71d15.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/e20bc68cb7efbd08cf77dc8fc96723e6bfb8d697f6e4f4c5785a91e7afe71d15.jpg"
      }
    },
    {
      "element_id": "2602_01068_fig_24",
      "page_idx": 0,
      "type": "figure",
      "content": "[Image: images/25390320867b1174e829e353dd409bb9f3f0e2b0ab71adf2216d4444da309331.jpg]",
      "bbox": null,
      "image_path": "images/25390320867b1174e829e353dd409bb9f3f0e2b0ab71adf2216d4444da309331.jpg",
      "metadata": {
        "caption": "",
        "reference": "images/25390320867b1174e829e353dd409bb9f3f0e2b0ab71adf2216d4444da309331.jpg"
      }
    },
    {
      "element_id": "2602_01068_formula_25",
      "page_idx": 0,
      "type": "formula",
      "content": "\\begin{array}{l} \\delta_ {i} = - \\mathcal {E} _ {i} + \\gamma V _ {\\phi} (p _ {i + 1}) - V _ {\\phi} (p _ {i}), \\\\ A _ {i} = \\sum_ {l = 0} ^ {n - i - 1} (\\gamma \\lambda) ^ {l} \\delta_ {i + l}. \\end{array} \\tag {10}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\begin{array}{l} \\delta_ {i} = - \\mathcal {E} _ {i} + \\gamma V _ {\\phi} (p _ {i + 1}) - V _ {\\phi} (p _ {i}), \\\\ A _ {i} = \\sum_ {l = 0} ^ {n - i - 1} (\\gamma \\lambda) ^ {l} \\delta_ {i + l}. \\end{array} \\tag {10}"
      }
    },
    {
      "element_id": "2602_01068_formula_26",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\mathrm {c l i p}} (\\theta) = - \\mathbb {E} _ {x \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left[ \\sum_ {i = 1} ^ {n} \\min  \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {o l d}} \\left(t _ {i} \\mid p _ {i}\\right)} A _ {i}, \\operatorname {c l i p} \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {o l d}} \\left(t _ {i} \\mid p _ {i}\\right)}, 1 - \\epsilon , 1 + \\epsilon\\right) A _ {i}\\right) \\right]. \\tag {11}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\mathrm {c l i p}} (\\theta) = - \\mathbb {E} _ {x \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left[ \\sum_ {i = 1} ^ {n} \\min  \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {o l d}} \\left(t _ {i} \\mid p _ {i}\\right)} A _ {i}, \\operatorname {c l i p} \\left(\\frac {\\pi_ {\\theta} \\left(t _ {i} \\mid p _ {i}\\right)}{\\pi_ {\\mathrm {o l d}} \\left(t _ {i} \\mid p _ {i}\\right)}, 1 - \\epsilon , 1 + \\epsilon\\right) A _ {i}\\right) \\right]. \\tag {11}"
      }
    },
    {
      "element_id": "2602_01068_formula_27",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {V} (\\phi) = \\mathbb {E} _ {x \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left[ \\sum_ {i = 1} ^ {n} \\left(V _ {\\phi} \\left(p _ {i}\\right) - \\hat {V} _ {i}\\right) ^ {2} \\right]. \\tag {12}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {V} (\\phi) = \\mathbb {E} _ {x \\sim \\mathbb {D} _ {\\mathrm {a l p o}}} \\left[ \\sum_ {i = 1} ^ {n} \\left(V _ {\\phi} \\left(p _ {i}\\right) - \\hat {V} _ {i}\\right) ^ {2} \\right]. \\tag {12}"
      }
    },
    {
      "element_id": "2602_01068_formula_28",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\text {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, y _ {1: n} ^ {w}, y _ {1: n} ^ {l}\\right) \\sim \\mathbb {D}} \\left[ \\sum_ {i = 1} ^ {n} \\mathbf {1} \\left(p _ {i}\\right) \\cdot \\log \\sigma \\left(\\beta \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)} - \\beta \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}\\right) \\right], \\tag {13}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\text {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, y _ {1: n} ^ {w}, y _ {1: n} ^ {l}\\right) \\sim \\mathbb {D}} \\left[ \\sum_ {i = 1} ^ {n} \\mathbf {1} \\left(p _ {i}\\right) \\cdot \\log \\sigma \\left(\\beta \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)} - \\beta \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}\\right) \\right], \\tag {13}"
      }
    },
    {
      "element_id": "2602_01068_formula_29",
      "page_idx": 0,
      "type": "formula",
      "content": "\\pi_ {\\theta} (y \\mid x) = \\prod_ {i = 1} ^ {n} \\pi_ {\\theta} \\left(y _ {i} \\mid x, y _ {1}, \\dots , y _ {i - 1}\\right). \\tag {14}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\pi_ {\\theta} (y \\mid x) = \\prod_ {i = 1} ^ {n} \\pi_ {\\theta} \\left(y _ {i} \\mid x, y _ {1}, \\dots , y _ {i - 1}\\right). \\tag {14}"
      }
    },
    {
      "element_id": "2602_01068_formula_30",
      "page_idx": 0,
      "type": "formula",
      "content": "\\theta^ {*} = \\arg \\max  _ {\\theta} \\mathbb {E} _ {x \\sim p (x)} \\left[ \\mathbb {E} _ {y \\sim \\pi_ {\\theta} (\\cdot | x)} \\left[ \\sum_ {i = 1} ^ {n} r \\left(x _ {i}, y _ {i}\\right) \\right] \\right], \\tag {15}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\theta^ {*} = \\arg \\max  _ {\\theta} \\mathbb {E} _ {x \\sim p (x)} \\left[ \\mathbb {E} _ {y \\sim \\pi_ {\\theta} (\\cdot | x)} \\left[ \\sum_ {i = 1} ^ {n} r \\left(x _ {i}, y _ {i}\\right) \\right] \\right], \\tag {15}"
      }
    },
    {
      "element_id": "2602_01068_formula_31",
      "page_idx": 0,
      "type": "formula",
      "content": "\\pi_ {\\theta} \\left(y ^ {w} \\mid x\\right) = \\prod_ {i = 1} ^ {n} \\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid x, y _ {1} ^ {w}, \\dots , y _ {i - 1} ^ {w}\\right), \\pi_ {\\theta} \\left(y ^ {l} \\mid x\\right) = \\prod_ {i = 1} ^ {n} \\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid x, y _ {1} ^ {l}, \\dots , y _ {i - 1} ^ {l}\\right). \\tag {16}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\pi_ {\\theta} \\left(y ^ {w} \\mid x\\right) = \\prod_ {i = 1} ^ {n} \\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid x, y _ {1} ^ {w}, \\dots , y _ {i - 1} ^ {w}\\right), \\pi_ {\\theta} \\left(y ^ {l} \\mid x\\right) = \\prod_ {i = 1} ^ {n} \\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid x, y _ {1} ^ {l}, \\dots , y _ {i - 1} ^ {l}\\right). \\tag {16}"
      }
    },
    {
      "element_id": "2602_01068_formula_32",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\mathrm {D P O}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, y ^ {w}, y ^ {l}\\right) \\sim \\mathbb {D}} \\left[ \\log \\sigma \\left(\\beta \\log \\frac {\\pi_ {\\theta} \\left(y ^ {w} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {w} \\mid x\\right)} - \\beta \\log \\frac {\\pi_ {\\theta} \\left(y ^ {l} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {l} \\mid x\\right)}\\right) \\right], \\tag {17}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\mathrm {D P O}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, y ^ {w}, y ^ {l}\\right) \\sim \\mathbb {D}} \\left[ \\log \\sigma \\left(\\beta \\log \\frac {\\pi_ {\\theta} \\left(y ^ {w} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {w} \\mid x\\right)} - \\beta \\log \\frac {\\pi_ {\\theta} \\left(y ^ {l} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {l} \\mid x\\right)}\\right) \\right], \\tag {17}"
      }
    },
    {
      "element_id": "2602_01068_formula_33",
      "page_idx": 0,
      "type": "formula",
      "content": "\\log \\frac {\\pi_ {\\theta} \\left(y ^ {w} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {w} \\mid x\\right)} - \\log \\frac {\\pi_ {\\theta} \\left(y ^ {l} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {l} \\mid x\\right)} = \\sum_ {i = 1} ^ {n} \\left[ \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid x , y _ {1 : i - 1} ^ {w}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {w} \\mid x , y _ {1 : i - 1} ^ {w}\\right)} - \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid x , y _ {1 : i - 1} ^ {l}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {l} \\mid x , y _ {1 : i - 1} ^ {l}\\right)} \\right], \\tag {18}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\log \\frac {\\pi_ {\\theta} \\left(y ^ {w} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {w} \\mid x\\right)} - \\log \\frac {\\pi_ {\\theta} \\left(y ^ {l} \\mid x\\right)}{\\pi_ {\\text {r e f}} \\left(y ^ {l} \\mid x\\right)} = \\sum_ {i = 1} ^ {n} \\left[ \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid x , y _ {1 : i - 1} ^ {w}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {w} \\mid x , y _ {1 : i - 1} ^ {w}\\right)} - \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid x , y _ {1 : i - 1} ^ {l}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {l} \\mid x , y _ {1 : i - 1} ^ {l}\\right)} \\right], \\tag {18}"
      }
    },
    {
      "element_id": "2602_01068_formula_34",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathcal {L} _ {\\text {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, y _ {1: n} ^ {w}, y _ {1: n} ^ {l}\\right) \\sim \\mathbb {D}} \\left[ \\sum_ {i = 1} ^ {n} w \\left(p _ {i}\\right) \\cdot \\log \\sigma \\left(\\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)} - \\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}\\right) \\right], \\tag {19}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathcal {L} _ {\\text {a l p o}} \\left(\\pi_ {\\theta}; \\pi_ {\\text {r e f}}\\right) = - \\mathbb {E} _ {\\left(x, y _ {1: n} ^ {w}, y _ {1: n} ^ {l}\\right) \\sim \\mathbb {D}} \\left[ \\sum_ {i = 1} ^ {n} w \\left(p _ {i}\\right) \\cdot \\log \\sigma \\left(\\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {w} \\mid p _ {i}\\right)} - \\beta_ {i} \\log \\frac {\\pi_ {\\theta} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}{\\pi_ {\\text {r e f}} \\left(y _ {i} ^ {l} \\mid p _ {i}\\right)}\\right) \\right], \\tag {19}"
      }
    },
    {
      "element_id": "2602_01068_formula_35",
      "page_idx": 0,
      "type": "formula",
      "content": "\\mathbb {E} _ {p \\sim d _ {\\theta *}} \\left[ \\mathcal {L} _ {\\text {a l p o}} \\left(\\pi_ {\\theta} (\\cdot \\mid p)\\right) \\right], \\tag {20}",
      "bbox": null,
      "image_path": null,
      "metadata": {
        "latex": "\\mathbb {E} _ {p \\sim d _ {\\theta *}} \\left[ \\mathcal {L} _ {\\text {a l p o}} \\left(\\pi_ {\\theta} (\\cdot \\mid p)\\right) \\right], \\tag {20}"
      }
    },
    {
      "element_id": "2602_01068_text_36",
      "page_idx": 0,
      "type": "text",
      "content": "# FROM UTTERANCE TO VIVIDITY: TRAINING EXPRES-SIVE SUBTITLE TRANSLATION LLM VIA ADAPTIVE LOCAL PREFERENCE OPTIMIZATION\n\nChaoqun $\\mathbf { C u i } ^ { 1 , 2 }$ , Shijing Wang3, Liangbin Huang3, Qingqing $\\mathbf { G u ^ { 4 } }$ , Zhaolong Huang3, Xiao Zeng3, Wenji Mao1,2,∗\n\n1MAIS, Institute of Automation, Chinese Academy of Sciences\n\n2School of Artificial Intelligence, University of Chinese Academy of Sciences\n\n3Hujing Digital Media & Entertainment Group\n\n4Geely AI lab\n\ncuichaoqun2025@ia.ac.cn, wenji.mao@ia.ac.cn\n\n# ABSTRACT\n\nThe rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.\n\n# 1 INTRODUCTION\n\nIn recent years, the rapid advancement of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation (Xu et al., 2024c; Enis & Hopkins, 2024; Feng et al., 2024). Representative LLMs such as GPT-4 (Achiam et al., 2023) and Qwen3 (Yang et al., 2025), trained on massive cross-lingual data, have demonstrated exceptional contextual comprehension and generation abilities in multilingual translation tasks. Their translation quality in general domains (e.g., news, daily conversations) has approached human-level performance (Xu et al., 2024a;b). However, as applications penetrate vertical domains, the limitations of LLMs in specialized translation scenarios have become increasingly apparent: critical issues include inadequate consistency in specialized terminology (An et al., 2024; Huang et al., 2024), deviations from industry-standard expressions (Ling et al., 2023; Pang et al., 2025), and weak style adaptability (Zhang et al., 2024). Consequently, LLM-based machine translation research has increasingly focused on addressing domain-specific customization requirements, such as developing legislation-focused LLMs that strictly adhere to clause formulation norms or medicine-oriented LLMs that precisely align with medical terminology.\n\nIn this study, we focus on visual media subtitle translation task, which aims to translate the lines in subtitles of visual media programs across genres such as movies, TV series, and documentaries from the source language into the target language. This task plays a crucial role in the localization of entertainment programs and in fostering global cultural dissemination and exchange (Federico et al., 2020; Wu et al., 2023; Mhaskar et al., 2024), yet it remains an underexplored subfield in machine translation. Similar to literary translation, subtitle translation requires a localized and stylistic liberal translation of program lines to convey the atmosphere, emotions, and tone of the original lines.\n\nHowever, while LLMs can achieve high translation accuracy, they tend to favor literal translation (as we quantitatively examined in Section 3.2). Therefore, exploring how to train a customized subtitle translation LLM with strong expressiveness and vividness is our primary challenge.\n\nWe employ LLM-as-a-Judge (using LLMs as evaluators) and preference optimization techniques to build a customized subtitle translation LLM. We conduct a quantitative investigation into the degree of liberal translation in human-translated texts across different domains and the liberal translation performance of various LLMs in subtitle translation. The findings reveal that: (1) Compared to the translation of serious texts requiring high accuracy (literal translation), such as legislation, news, and medicine, subtitle translation tends to favor more liberal translation; (2) In subtitle translation, the translations generated by chat LLMs tend to be more literal compared to those produced by reason LLMs and humans. We also experimentally validate the reliability of LLMs as reward models and evaluators for subtitle translation, as well as their alignment with human preferences. This enables automated construction of preference data for preference alignment training, thereby enhancing the expressiveness and vividness of subtitle translation LLMs. Additionally, for the task of translating short subtitle lines, which requires fine-grained local preference alignment, we propose a novel preference alignment strategy called Adaptive Local Preference Optimization (ALPO).\n\nIn summary, the main contributions of this study are as follows:\n\n• We introduce the visual media subtitle translation task for the first time and investigate the extent of liberal translation and the reliability of LLM-as-a-Judge in this domain.   \n• We propose ALPO, a local preference optimization method, provide a formal explanation of its effectiveness, and use it to train a subtitle translation LLM with high vividness.   \n• We release subtitle parallel corpora in multiple directions to support community research, and established a multidimensional evaluation framework based on LLM-as-a-Judge.   \n• Experimental results demonstrate that our 14B LLM, trained with ALPO, achieves significant improvements across multiple dimensions and outperforms SOTA LLMs.\n\n# 2 RELATED WORK\n\n# 2.1 LLM-AS-A-JUDGE\n\nThe capacity of LLMs to emulate human reasoning and evaluate specific inputs against a set of predefined rules has paved the way for \"LLM-as-a-Judge\" (Zheng et al., 2023a; Gu et al., 2024). Existing studies demonstrate that LLMs’ scalability, adaptability, and cost-effectiveness render them particularly suitable for handling increasing volumes of assessment tasks that humans conventionally performed (Chen et al., 2024a; Li et al., 2024a; Wang et al., 2024c; Zhu et al., 2025). These capabilities are crucial for deploying LLMs flexibly across diverse evaluation scenarios and objectives, driving their rapid adoption in practical evaluation scenarios (Wang et al., 2024b; Chen et al., 2024b; Khattak et al., 2023).\n\nOriginally developed for language generation and comprehension tasks, LLMs have evolved significantly through advanced training methodologies such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), enhancing their alignment with human values and reasoning processes. This alignment has allowed LLMs to transition from generative tasks to evaluative roles. Fundamentally, LLM-as-a-Judge refers to the deployment of LLMs to assess objects, actions, or decisions according to predefined rules, criteria, or preferences (Kant, 1908; 1987). This framework encompasses a wide range of evaluative roles, including: Graders (Dong et al., 2023; Luong et al., 2024), Evaluators/Assessors (Li et al., 2024b; Zhang et al., 2023), Critics (Ke et al., 2024; Putta et al., 2024; Xiong et al., 2024), Verifiers (Shinn et al., 2023; Wang et al., 2024d), Examiners (Bai et al., 2023), Reward/Ranking Models (Luo et al., 2023; Sun et al., 2024; Yang et al., 2024b), etc.\n\n# 2.2 LANGUAGE MODEL PREFERENCE OPTIMIZATION\n\nReinforcement learning provides an effective solution for aligning LLMs with human values and controlling text generation (Bender et al., 2021; Bommasani et al., 2021; Thoppilan et al., 2022; Taori et al., 2023; Chiang et al., 2023; Ji et al., 2023). To this end, RLHF framework based on human feedback reward models has been established (Ouyang et al., 2022; Christiano et al., 2017;\n\nMacGlashan et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020; Zheng et al., 2023b). However, despite its effectiveness, the complexity, instability, and hyperparameter sensitivity of RLHF remain insufficiently addressed (Andrychowicz et al., 2021; Engstrom et al., 2020). Recently proposed method named Direct Preference Optimization (DPO) (Rafailov et al., 2024) simplifies the RLHF framework by eliminating the need for explicit reward model construction or reinforcement learning procedures, thereby avoiding dependence on reward models. Several variants have subsequently emerged, such as SimPO, KTO, and IPO (Meng et al., 2024; Azar et al., 2024; Ethayarajh et al., 2024). Nevertheless, when applied to local preference alignment task, these methods still exhibit limitations, including coarse granularity and gradient dilution (see Appendix D for formal specification).\n\n# 3 EMPIRICAL INVESTIGATION\n\n# 3.1 LLM IS EXCELLENT TRANSLATION EVALUATOR\n\nWe aim to verify whether LLMs can achieve high consistency with human preferences in quality assessment across different dimensions of subtitle translation, thereby enabling LLMs to serve as both reward models and evaluators for subtitle translation model alignment training. We begin with a formal definition of LLM-as-Evaluator:\n\n\nwhere $\\pi _ { \\mathrm { e } }$ is the LLM used for evaluation, $\\mathcal { C }$ is the introduction and instruction in the prompt that guides the LLM’s evaluation, $s$ is the line under evaluation, $\\{ t \\}$ represents multiple translations of $s$ , and $\\mathcal { E }$ denotes the evaluation score given by $\\pi _ { \\mathrm { e } }$ .\n\nWe investigated the correlation between human evaluators and LLM evaluators in terms of vividness scores for 10 different translations of 500 lines from our self-constructed Multilingual Subtitle Corpus (MuSC) dataset in Appendix A. Each evaluator assigned a score from 0 to 100 for the 10 translations of each line under evaluation, and then we computed the average Spearman rank correlation $\\rho$ across evaluators. We present the Spearman rank correlation results in Figure 1. The detailed experimental settings are provided in Appendix B.2.1. The translation directions include $\\tt e n \\Rightarrow \\tt z h$ , $\\tt e n \\Rightarrow \\tt d e$ , $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ , and $z \\mathrm { h } { \\Rightarrow } \\mathrm { t h }$ , where Chinese and English, German, and Thai correspond to high-, medium-, and low-resource languages, respectively.\n\n(a) en⇒zh\n\n(b) en⇒de\n\n(c) zh⇒en\n\n(d) zh⇒th   \nFigure 1: Investigation of the consistency of multiple evaluators with Spearman rank correlation $\\rho$ .\n\nThe results show that the LLM evaluator exhibits high consistency with human evaluators across multiple directions, indicating its ability to handle deviations in cultural and linguistic conventions across languages. Notably, even the Qwen3-14B model, with 14B parameters, achieves high agreement with both human evaluators and other SOTA LLMs across all directions $( \\rho \\ge 0 . 8 2 )$ . This indicates that a 14B model, which incurs relatively low inference cost, can serve as an efficient reward model, forming the foundation of ALPO. Furthermore, Figure 2 presents the Bland-Altman plots of Qwen3-14B and human evaluators for identical line translations in $\\tt e n \\Rightarrow \\tt z h$ and $z \\mathrm { h } { \\Rightarrow } \\mathrm { t h }$ . The results show that the mean difference (MD) between the two is very low, indicating minimal systematic bias, and its limits of agreement (LoA) are within acceptable error margins for scoring on a scale of 100. This further confirms the consistency between the 14B model and human evaluators.\n\n(a) en⇒zh\n\n(b) zh⇒th   \nFigure 2: Bland-Altman plot comparing Qwen3-14B with human evaluator.\n\n# 3.2 PARALLEL CORPORA ARE ACTUALLY NOT PARALLEL\n\nLiteral translation focuses on translation accuracy, striving to preserve the original form, while liberal translation emphasizes conveying meaning and flexibly adjusting the expression (Baker & Saldanha, 2019; Munday et al., 2022). Based on our experience and observation, translations in different domains exhibit varying degrees of liberal translation: domains such as subtitle and literature translation often lean towards liberal translation, whereas legislation, medicine, and technical texts typically require highly accurate literal translation.\n\nWe designed relevant experiments to quantitatively validate this hypothesis. We investigated the back-translation consistency of parallel corpora from different domains. Specifically, for a particular translation direction, we utilized GPT-4o to directly translate the target language translation back to the source language, and then calculated the BLEU and ${ \\mathrm { C h r F } } + +$ scores between the back-translated text and the original text. Lower scores indicate a higher degree of liberal translation. Detailed experimental settings are provided in Appendix B.2.2.\n\nThe investigation results in Table 1 indicate that the back-translated texts in domains such as visual media, literature, and religion show lower similarity to the original texts compared to those in legislation, news, and medicine. This suggests a higher degree of liberal translation. For visual media subtitle translation, translators integrate video information into their translations, aiming to convey the atmosphere, emotions, and tone of the original text; this results in greater expressiveness and vividness, sometimes at the cost of sacrificing some degree of accuracy.\n\n# 3.3 CHAT LLMS FAVOR LITERAL, REASON LLMS EXCEL IN LIBERAL\n\nGenerally, LLMs capable of effectively performing liberal translation tend to generate more expressive translations. We aim to verify the liberal translation capabilities of different LLMs. Our investigation spans multiple translation directions, where we sampled 2,000 lines from MuSC test set for each direction and prompted various LLMs to produce expressive and vivid translations (see the prompt in Appendix F.1). We then computed the pairwise BLEU similarity between human and multiple LLM-generated translations. The LLMs included are chat models such as GPT-4o (Achiam et al., 2023), Qwen-Max (Yang et al., 2024a), and Claude Opus 4.1 (Anthropic, 2025), as well as reasoning models like GPT-5 Thinking (OpenAI, 2025) and DeepSeek-R1 (Guo et al., 2025). Additionally, we incorporated our supervised fine-tuning (SFT) Qwen2.5-14B model trained on MuSC.\n\nThe results in Figure 3 indicate that translations generated by chat models exhibit higher similarity among themselves, suggesting a tendency toward more literal translation. In contrast, translations produced by reasoning models show lower similarity with other translations, demonstrating that these models, through thinking, better adhere to the instruction of liberal translation. This validates that LLM’s inference-time scaling can effectively enhance translation performance. The human translation exhibits lower similarity with other translations, indicating its proficiency in liberal translation. We present demos of different translations in Appendix C.2 to visually demonstrate these findings.\n\nTable 1: Domain liberal translation investigation. The highest and lowest are in blue and orange.   \n\n<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Domain</td><td colspan=\"2\">en⇒de</td><td colspan=\"2\">en⇒fr</td><td colspan=\"2\">en⇒es</td></tr><tr><td>BLEU</td><td>ChrF++</td><td>BLEU</td><td>ChrF++</td><td>BLEU</td><td>ChrF++</td></tr><tr><td>OpenSubtitles</td><td>Visual Media</td><td>15.00</td><td>37.08</td><td>17.84</td><td>37.80</td><td>21.60</td><td>43.78</td></tr><tr><td>Books</td><td>Literature</td><td>17.22</td><td>43.20</td><td>21.51</td><td>47.14</td><td>18.60</td><td>43.00</td></tr><tr><td>bible-udin</td><td>Religion</td><td>22.55</td><td>49.57</td><td>22.07</td><td>48.24</td><td>22.31</td><td>51.03</td></tr><tr><td>DGT</td><td rowspan=\"2\">Legislation</td><td>19.85</td><td>50.05</td><td>22.44</td><td>48.10</td><td>21.88</td><td>52.24</td></tr><tr><td>JRC-Acquis</td><td>27.55</td><td>61.95</td><td>28.83</td><td>69.65</td><td>25.16</td><td>65.30</td></tr><tr><td>News-Commentary</td><td>News</td><td>25.00</td><td>55.23</td><td>24.87</td><td>54.37</td><td>35.90</td><td>65.84</td></tr><tr><td>ECDC</td><td rowspan=\"2\">Medicine</td><td>23.18</td><td>60.77</td><td>23.86</td><td>62.97</td><td>27.20</td><td>65.39</td></tr><tr><td>EMEA</td><td>25.20</td><td>59.36</td><td>24.73</td><td>61.65</td><td>31.46</td><td>69.15</td></tr><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Domain</td><td colspan=\"2\">de⇒en</td><td colspan=\"2\">fr⇒en</td><td colspan=\"2\">es⇒en</td></tr><tr><td>BLEU</td><td>ChrF++</td><td>BLEU</td><td>ChrF++</td><td>BLEU</td><td>ChrF++</td></tr><tr><td>OpenSubtitles</td><td>Visual Media</td><td>15.25</td><td>39.41</td><td>16.60</td><td>40.65</td><td>22.18</td><td>47.45</td></tr><tr><td>Books</td><td>Literature</td><td>14.50</td><td>40.50</td><td>20.12</td><td>48.29</td><td>15.84</td><td>43.99</td></tr><tr><td>bible-udin</td><td>Religion</td><td>15.86</td><td>47.14</td><td>22.79</td><td>51.85</td><td>24.55</td><td>53.77</td></tr><tr><td>DGT</td><td rowspan=\"2\">Legislation</td><td>19.70</td><td>47.77</td><td>23.31</td><td>48.91</td><td>22.87</td><td>52.49</td></tr><tr><td>JRC-Acquis</td><td>27.54</td><td>63.43</td><td>31.33</td><td>64.15</td><td>27.68</td><td>64.46</td></tr><tr><td>News-Commentary</td><td>News</td><td>22.64</td><td>54.05</td><td>29.34</td><td>56.20</td><td>41.75</td><td>66.35</td></tr><tr><td>ECDC</td><td rowspan=\"2\">Medicine</td><td>21.09</td><td>57.41</td><td>29.13</td><td>64.81</td><td>29.62</td><td>64.20</td></tr><tr><td>EMEA</td><td>24.94</td><td>60.16</td><td>26.24</td><td>62.96</td><td>29.86</td><td>67.09</td></tr></table>\n\n(a) $\\tt e n \\Rightarrow \\tt z h$\n\n(b) en⇒de\n\n(c) zh⇒en\n\n(d) zh⇒th   \nFigure 3: Pairwise BLEU scores between translations.\n\n# 4 METHOD\n\n# 4.1 NOTATIONS\n\nA bilingual dataset $\\mathbb { D }$ for subtitle translation comprises collections of source language lines $\\mathcal { L } _ { \\mathrm { s r c } }$ and target language lines ${ \\mathcal { L } } _ { \\mathrm { t g t } }$ from multiple programs of genres such as films or TV series, represented by $\\mathbb { D } \\equiv \\{ ( \\ell _ { \\mathrm { s r c } } , \\ell _ { \\mathrm { t g t } } ) \\in \\mathcal { L } _ { \\mathrm { s r c } } \\times \\mathcal { L } _ { \\mathrm { t g t } } \\}$ . The lines in multilingual subtitles of the same program are often not in one-to-one correspondence. To address this, we have developed a bilingual parallel corpus construction algorithm with $O ( N )$ complexity; details can be found in Appendix A.2. We will utilize the dataset $\\mathbb { D }$ to perform SFT and ALPO training on off-the-shelf LLMs. The input prompt $x$ for the LLM includes the context $\\mathcal { C }$ (comprising introduction and instructions) and a set of $n$ source lines $\\{ s _ { i } \\mid i \\in [ n ] \\}$ that need to be translated, i.e., $x \\equiv \\mathcal { C } \\oplus \\{ s _ { i } \\}$ . The response $y$ from the LLM includes translations for each line $\\{ t _ { i } \\mid i \\in [ n ] \\}$ , i.e., $y \\equiv \\{ t _ { i } \\}$ . See Appendix F.1 for detailed $x$ and $y$ format.\n\n# 4.2 OVERALL FRAMEWORK\n\nObviously, constructing an expressive translation LLM is a preference optimization task. However, we cannot directly apply general methods (e.g., PPO (Ouyang et al., 2022) or DPO (Rafailov et al., 2024)), primarily because the translation of each line of subtitle depends on its context. The input $x$ of the SFT model $\\pi _ { \\mathrm { s f t } }$ must include multiple lines of subtitle $\\left\\{ s _ { i } \\right\\}$ , necessitating finer-grained alignment for each line in the response $y$ . Consequently, outcome-supervised methods like PPO and DPO, which optimize the complete output of LLM, are inadequate for this task. We present experimental\n\nFigure 4: The overall framework of ALPO.\n\nand theoretical validation of this in Appendix C.4 and Appendix D. To address this fine-grained preference optimization task, we propose the process-supervised ALPO method. ALPO introduces a novel paradigm that leverages a segment-wise sampling strategy and fine-grained alignment loss to train a high-quality subtitle translation LLM. The overall framework of ALPO is shown in Figure 4.\n\n# 4.3 SAMPLING STRATEGY\n\nWe refer to a task requiring multi-segment local preference alignment of LLM response as a local preference optimization task. ALPO provides an effective paradigm for handling such tasks. We initially divide the parallel corpus $\\mathbb { D }$ into a demonstration dataset $\\mathbb { D } _ { \\mathrm { s f t } }$ and an alignment dataset $\\mathbb { D } _ { \\mathrm { a l p o } }$ (approximately 8:2), using $\\mathbb { D } _ { \\mathrm { s f t } }$ to train a SFT model $\\pi _ { \\mathrm { s f t } }$ (see Appendix F.1 for input-output format).\n\nIn sampling phase, for a $x \\in \\mathbb { D } _ { \\mathrm { a l p o } }$ , which contains $n$ lines $s _ { 1 } , \\ldots , s _ { n }$ . For each $s _ { i }$ , using $p _ { i } =$ $x , t _ { 1 } ^ { ( \\mathrm { c } ) } , \\ldots , t _ { i - 1 } ^ { ( \\mathrm { c } ) }$ as prefix, $k$ translations are sampled $k = 1 5$ in experiments), i.e., $\\pi _ { \\mathrm { s f t } } ( t _ { i } ^ { j } \\ | \\ p _ { i } )$ , $j = 1 , \\dots , k$ , resulting in a candidate set $\\{ t _ { i } ^ { j } \\mid j = 1 , \\ldots , k \\}$ . Then, the candidate set is deduplicated; if human reference $t _ { i } ^ { 0 }$ is available, it can be added to the candidate set, forming $\\mathcal { T } _ { i } = \\{ t _ { i } ^ { j } \\ | \\ j =$ $0 , 1 , \\ldots \\}$ . Subsequently, a Qwen3-14B model is deployed as an evaluator $\\pi _ { \\mathrm { e } }$ (or can be regarded as a reward model) to evaluate vividness of $\\mathcal { T } _ { i }$ , obtaining the score sequence $\\mathcal { E } _ { i }$ (see Table 2 for demonstration). Based on $\\mathcal { E } _ { i }$ , we select a superior translation $t _ { i } ^ { \\mathrm { ( c ) } }$ for $s _ { i }$ (randomly chosen from the top 3 scores) to serve as the prefix for the next sampling cycle. Ultimately, for a $x \\in \\mathbb { D } _ { \\mathrm { a l p o } }$ , we obtain the sampling result ${ \\mathcal { S } } ( x ) \\equiv \\mathbf { \\bar { \\{ } }  ( s _ { i } , { \\mathcal { T } } _ { i } , \\mathcal { E } _ { i } ) \\mid i = 1 , \\ldots , n \\mathbf  \\bar { \\} }$ . Algorithm 1 details the entire process.\n\nTable 2: The $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ evaluation demonstration of the evaluator $\\pi _ { \\mathrm { e } }$   \n\n<table><tr><td>Line</td><td>Evaluation of πe</td></tr><tr><td>历史虽然会重演，但是人类是无法回到过去的。</td><td>-</td></tr><tr><td>History repeats, but we can&#x27;t go back to what was.</td><td>70</td></tr><tr><td>History might replay, but mankind cannot go back in time.</td><td>85</td></tr><tr><td>The wheel of history may turn full circle, but the door to the past stays forever locked.</td><td>88</td></tr><tr><td>Although history may repeat itself, humans cannot return to the past.</td><td>82</td></tr><tr><td>History often echoes, yet there&#x27;s no way for us to turn back the clock.</td><td>92</td></tr></table>\n\n# 4.4 ADAPTIVE ALIGNMENT LOSS\n\nThe reward signal acts on segments of model output (process-supervised) rather than the complete response (outcome-supervised), which constitutes the main challenge for vividness enhancement. ALPO enables process-supervised alignment across multiple segments in translation LLM’s response $y$ . Specifically, for all lines $\\{ s _ { i } | i = \\bar { 1 } , \\ldots , n \\}$ in an $x \\in \\mathbb { D } _ { \\mathrm { a l p o } }$ , we assign each $s _ { i }$ an adaptive weight $w ( s _ { i } )$ , defined as the product of a gating function $\\mathbf { 1 } ( s _ { i } )$ and an importance score $\\delta ( s _ { i } )$ :\n\n\nwhere $\\mathbf { 1 } ( s _ { i } )$ acts as a gate to determine whether $s _ { i }$ participates in optimization. When the sampled translations lack diversity or clear quality distinction, i.e., $\\vert \\mathcal { T } _ { i } \\vert \\le 3$ or $\\operatorname* { m a x } ( \\mathcal { E } _ { i } ) - \\operatorname* { m i n } ( \\mathcal { E } _ { i } ) \\leq 5$ , we\n\nAlgorithm 1 ALPO Sampling Strategy.   \nInput: SFT model $\\pi_{\\mathrm{sft}}$ , evaluation LLM $\\pi_{\\mathrm{e}}$ , alignment dataset $\\mathbb{D}_{\\mathrm{alpo}}$ , sample number $k$ Output: sampled segment-level candidate set $S(x)$ 1: for any $x\\in \\mathbb{D}_{\\mathrm{alpo}}$ do // Iterate through the alignment dataset $\\mathbb{D}_{\\mathrm{alpo}}$ 2: for $i = 1$ to $n$ do // Iterate through the subtitle lines in $x$ 3: for $j = 1$ to $k$ do // Sample multiple translation candidates.   \n4: Sample $\\pi_{\\mathrm{sft}}(t_i^j\\mid x,t_1^{(\\mathrm{c})},\\ldots ,t_{i - 1}^{(\\mathrm{c})}).$ 5: end for   \n6: Deduplicate the candidate set $\\{t_i^j\\mid j = 1,\\dots ,k\\}$ 7: (Optional) Add human reference $t_i^0$ to the candidate set, get $\\mathcal{T}_i = \\{t_i^j\\mid j = 0,1,\\dots \\}$ 8: Evaluate $\\mathcal{T}_i$ by $\\pi_{\\mathrm{e}}$ , get score sequence $\\mathcal{E}_i$ 9: Select a superior translation $t_i^{(\\mathrm{c})}$ (random from top 3) as the prefix of next sampling cycle.   \n10: end for   \n11: end for   \n12: return $S(x)\\equiv \\{(s_i,\\mathcal{T}_i,\\mathcal{E}_i)\\mid i = 1,2,\\ldots ,n\\}$\n\nset $\\mathbf { 1 } ( s _ { i } ) = 0$ ; otherwise, $\\mathbf { 1 } ( s _ { i } ) = 1$ . The importance score $\\delta ( s _ { i } )$ depends on the diversity of sampled translations. Since lines with richer translations provide more potential for vividness enhancement, we assign higher $\\delta ( s _ { i } )$ to lines with a larger number of distinct sampled translations after deduplication:\n\n\nWe then apply a Bradley–Terry (DPO-style) preference optimization loss to achieve fine-grained preference alignment of the translation model:\n\n\nwhere $\\pi _ { \\theta }$ is the policy model, and $\\pi _ { \\mathrm { r e f } }$ is the reference model. The chosen translation $t _ { i } ^ { \\mathrm { ( c ) } }$ is randomly sampled from the top 3 scored translations in $\\mathcal { E } _ { i }$ , while the rejected translation $t _ { i } ^ { \\mathrm { ( r ) } }$ is selected as the third-lowest scored candidate (excluding the lowest one to avoid overly trivial contrastive pairs). $\\beta _ { i }$ is a hyperparameter controlling sensitivity to reward differences. Since the reward gap between $t _ { i } ^ { \\mathrm { ( c ) } }$ and $t _ { i } ^ { \\mathrm { ( r ) } }$ also reflects the diversity of translations, we set $\\beta _ { i }$ dynamically as follows:\n\n\nwhere $\\mathcal { L } _ { \\mathrm { a l p o } }$ as $r ( s _ { i } , t _ { i } ^ { \\mathrm { ( c ) } } )$ $p _ { i } = x , t _ { 1 } ^ { \\mathrm { ( c ) } } , \\dots , t _ { i - 1 } ^ { \\mathrm { ( c ) } }$ denotes the score of i , we adopt a scheduled prefix mixing strategy to mitigate exposure bias(c) $t _ { i } ^ { \\mathrm { ( c ) } }$ in $\\mathcal { E } _ { i }$ . Moreover, instead of directly setting the prefix in when non-chosen translations are generated during inference. Specifically, with probability λ, $t _ { i } ^ { \\mathrm { ( c ) } }$ is appended to the prefix, and with probability $1 - \\lambda$ , a translation sampled from $\\mathcal { T } _ { i }$ is appended (in experiments, $\\lambda$ is increased from 0.2 to 0.6 as training progresses), i.e.,\n\n\n# 5 EXPERIMENTS\n\n# 5.1 EXPERIMENTAL SETTINGS\n\nWe run experiments using our customized MuSC dataset, which comprises subtitle corpora from the online video platform Youku for multiple directions, including en⇒de, ${ \\tt e n } \\Rightarrow \\pounds _ { \\tt T }$ , $\\tt e n \\Rightarrow \\tt z h$ , $\\mathtt { k o } \\Rightarrow \\mathtt { z h }$ , $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ , and $z { \\mathrm { h } } { \\Rightarrow } \\pm { \\mathrm { h } }$ . Each direction includes 100–200 programs across various genres, with $10 \\%$ of the programs reserved as the test set. See Appendix A for details of MuSC.\n\nWe compare our model with the following baselines:\n\n• VideoDubber (Wu et al., 2023) constructs a length-controlled translation model (the work most closely related to our newly proposed subtitle translation task).   \n• NLLB-3.3B (Costa-jussà et al., 2022) is a translation model of Meta AI ${ \\tt > } 2 0 0 $ languages).   \n• MADLAD-10B (Kudugunta et al., 2024) is a translation model of Google $\\mathord { > } 4 5 0$ languages).   \n• Google Translate (Google LLC, 2023) is a multilingual translation service of Google.   \n• GPT-4o, Qwen-Max, and DeepSeek-V3.1 (Liu et al., 2024) are current SOTA chat models.   \n• DeepSeek-R1 and GPT-5 Thinking are SOTA reasoning models. We use the prompt in Appendix F.1 to drive these LLMs for subtitle translation.   \n• Qwen2.5-14B serves as the backbone for ALPO.\n\nDetails of the experimental setup are provided in Appendix B. The source code for ALPO and MuSC dataset are available at https://github.com/CcQunResearch/ALPO.\n\n# 5.2 TRANSLATION QUALITY EVALUATION\n\nUnlike the strict requirements for accuracy in the translation of legal texts and technical documents, subtitle translation focuses more on the localized representation of the translation (Pérez-González, 2019; Bajciˇ c & Golenko, 2024). Specifically, subtitle translation demands not only the conveyance of ´ the original meaning but also expects to reflect the emotions and tone of the original subtitle, aiming for translations that are more fluid, dynamic, and expressive. Given that we have verified the reliability of LLMs as evaluators in Section 3.1, we developed a multidimensional quality evaluation system for subtitle translation based on LLM-as-a-Judge scoring. We primarily assess three dimensions of subtitle translation: (1) Accuracy: whether the translation accurately conveys the original meaning of the subtitle; (2) Naturalness: whether the expression in the translation is natural and fluent, aligning with the grammatical structure and lexical conventions of the target language; (3) Vividness: whether the translation is expressive and successfully conveys the emotions and atmosphere of the original subtitle. We employ DeepSeek-V3.1, Claude Sonnet 4, and GPT-5 Instant as evaluation models to score subtitle segments in the test set along three dimensions, assigning a score between 0 and 100 for each. Then, we show the average score across all models in Table 3.\n\nTable 3: Multidimensional quality evaluation. The 1st and 2nd best results are denoted as blue and orange. ST for supervised training. ICL for in-context learning.   \n\n<table><tr><td rowspan=\"2\">Models</td><td rowspan=\"2\">Training</td><td colspan=\"3\">en→de</td><td colspan=\"3\">en→fr</td><td colspan=\"3\">en→zh</td></tr><tr><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td></tr><tr><td>Gold Reference</td><td>Human</td><td>84.8</td><td>83.8</td><td>73.1</td><td>83.5</td><td>85.0</td><td>74.8</td><td>83.6</td><td>82.6</td><td>71.5</td></tr><tr><td>VideoDubber</td><td></td><td>41.5</td><td>35.5</td><td>41.0</td><td>48.2</td><td>47.3</td><td>48.5</td><td>46.9</td><td>51.9</td><td>49.7</td></tr><tr><td>NLLB-3.3B</td><td>ST</td><td>75.8</td><td>70.1</td><td>59.2</td><td>76.9</td><td>74.6</td><td>61.8</td><td>61.4</td><td>54.0</td><td>43.7</td></tr><tr><td>MADLAD-10B</td><td></td><td>73.2</td><td>65.6</td><td>54.5</td><td>73.6</td><td>67.8</td><td>57.4</td><td>59.7</td><td>55.5</td><td>46.3</td></tr><tr><td>Google Translate</td><td></td><td>89.9</td><td>80.6</td><td>62.6</td><td>91.9</td><td>84.8</td><td>64.3</td><td>84.2</td><td>79.7</td><td>54.4</td></tr><tr><td>GPT-4o</td><td></td><td>94.1</td><td>86.7</td><td>66.9</td><td>93.2</td><td>88.8</td><td>69.5</td><td>89.3</td><td>82.3</td><td>59.8</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>95.5</td><td>89.2</td><td>68.9</td><td>94.1</td><td>89.9</td><td>71.6</td><td>91.9</td><td>84.4</td><td>61.3</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>94.8</td><td>89.2</td><td>67.4</td><td>94.9</td><td>90.7</td><td>72.3</td><td>91.2</td><td>85.3</td><td>63.5</td></tr><tr><td>DeepSeek-R1</td><td>ICL (R)</td><td>92.8</td><td>87.4</td><td>70.0</td><td>93.6</td><td>90.0</td><td>73.4</td><td>90.5</td><td>85.7</td><td>70.8</td></tr><tr><td>GPT-5</td><td></td><td>93.6</td><td>88.6</td><td>72.7</td><td>92.2</td><td>90.4</td><td>75.8</td><td>92.4</td><td>87.0</td><td>71.1</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>87.5</td><td>83.6</td><td>64.4</td><td>86.2</td><td>85.4</td><td>67.7</td><td>86.4</td><td>82.0</td><td>59.1</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>95.4</td><td>88.4</td><td>74.8</td><td>94.1</td><td>89.2</td><td>78.8</td><td>90.6</td><td>84.3</td><td>76.6</td></tr><tr><td rowspan=\"2\">Models</td><td rowspan=\"2\">Training</td><td colspan=\"3\">ko→zh</td><td colspan=\"3\">zh→en</td><td colspan=\"3\">zh→th</td></tr><tr><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td></tr><tr><td>Gold Reference</td><td>Human</td><td>78.0</td><td>77.8</td><td>65.8</td><td>83.0</td><td>80.3</td><td>73.3</td><td>76.6</td><td>75.1</td><td>66.3</td></tr><tr><td>VideoDubber</td><td></td><td>39.6</td><td>45.2</td><td>48.2</td><td>53.6</td><td>54.8</td><td>50.1</td><td>34.1</td><td>34.9</td><td>41.5</td></tr><tr><td>NLLB-3.3B</td><td>ST</td><td>33.1</td><td>26.1</td><td>25.4</td><td>29.1</td><td>21.7</td><td>20.8</td><td>42.6</td><td>33.9</td><td>40.5</td></tr><tr><td>MADLAD-10B</td><td></td><td>44.9</td><td>42.9</td><td>46.7</td><td>45.1</td><td>38.9</td><td>37.6</td><td>47.9</td><td>50.8</td><td>51.0</td></tr><tr><td>Google Translate</td><td></td><td>54.9</td><td>52.8</td><td>52.0</td><td>79.8</td><td>66.3</td><td>50.2</td><td>55.2</td><td>56.2</td><td>54.5</td></tr><tr><td>GPT-4o</td><td></td><td>80.0</td><td>79.9</td><td>58.1</td><td>88.5</td><td>83.0</td><td>64.6</td><td>88.0</td><td>84.4</td><td>67.9</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>83.7</td><td>82.5</td><td>61.8</td><td>90.0</td><td>85.0</td><td>66.8</td><td>91.3</td><td>85.8</td><td>69.1</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>83.1</td><td>82.2</td><td>57.2</td><td>89.5</td><td>84.1</td><td>63.0</td><td>89.9</td><td>84.6</td><td>67.1</td></tr><tr><td>DeepSeek-R1</td><td>ICL (R)</td><td>79.8</td><td>81.6</td><td>65.6</td><td>88.5</td><td>85.6</td><td>73.5</td><td>87.6</td><td>84.0</td><td>71.0</td></tr><tr><td>GPT-5</td><td></td><td>84.5</td><td>82.6</td><td>65.0</td><td>89.1</td><td>86.1</td><td>75.2</td><td>88.7</td><td>83.9</td><td>73.0</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>80.9</td><td>76.1</td><td>53.9</td><td>85.2</td><td>80.1</td><td>54.8</td><td>87.3</td><td>82.6</td><td>66.0</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>84.3</td><td>83.3</td><td>70.5</td><td>88.3</td><td>86.8</td><td>81.7</td><td>91.9</td><td>84.7</td><td>74.2</td></tr></table>\n\nThe results indicate that ALPO and cutting-edge LLMs like GPT-4o significantly outperform traditional translation models like MADLAD, and surpass human translation in accuracy and naturalness. However, human translation excelled in vividness, possibly suggesting that human translations often incorporate more liberal translation by integrating video content. Reason models achieved a better vividness score compared to chat models, although its accuracy and naturalness were slightly lower\n\nin some aspects. This aligns with the conclusions in Section 3.3. Models trained with ALPO demonstrated marked improvement not only in vividness but also in accuracy and naturalness compared to the SFT model. ALPO attained the highest vividness scores across all directions and occasionally surpassed other cutting-edge LLMs in other dimensions, particularly excelling in relatively low-resource language directions such as $\\mathtt { k o } \\Rightarrow \\mathtt { z h }$ and $z \\mathrm { h } { \\Rightarrow } \\mathrm { t h }$ . For the complete results of quality assessment and the prompt design of the LLM evaluator, please refer to Appendix C.1 and Appendix F.3.\n\n# 5.3 HUMAN EVALUATION OF TRANSLATION QUALITY\n\nWe present the human evaluation results of the translation quality of ALPO for $\\tt e n \\Rightarrow \\tt z h$ and $z \\mathrm { h } \\Longrightarrow$ th in Table 4. Due to varying subjective preferences among evaluators, we do not adopt a scoring system. Instead, we perform pairwise comparisons of different translations to assess the win rate metric. We evaluate the ALPO model alongside four baselines across dimensions of accuracy, naturalness, and vividness, and conduct a comprehensive assessment. The human evaluation results are consistent with those shown in Table 3, which supports the reliability of our multidimensional evaluation system based on LLM-as-a-Judge. Specific experimental settings can be found in Appendix B.3.\n\nTable 4: Human translation quality evaluation, reporting win rate (win:tie:loss). The winning and losing contrasts are marked in blue and orange, respectively.   \n\n<table><tr><td rowspan=\"2\">Challenger</td><td rowspan=\"2\">Competitors</td><td colspan=\"5\">en→zh</td><td colspan=\"5\">zh→th</td></tr><tr><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Comprehensive</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Comprehensive</td><td></td><td></td></tr><tr><td rowspan=\"4\">ALPO(Qwen2.5-14B)</td><td>Gold Reference</td><td>29:49:22</td><td>28:50:22</td><td>32:42:26</td><td>31:46:23</td><td>28:48:24</td><td>26:52:22</td><td>32:39:29</td><td>29:49:22</td><td></td><td></td></tr><tr><td>SFT Model πsft</td><td>26:50:24</td><td>31:48:21</td><td>38:41:21</td><td>37:43:20</td><td>26:55:19</td><td>27:51:22</td><td>39:42:19</td><td>30:50:20</td><td></td><td></td></tr><tr><td>GPT-4o</td><td>22:54:24</td><td>20:57:23</td><td>29:51:20</td><td>26:54:23</td><td>23:57:20</td><td>23:51:26</td><td>36:37:27</td><td>30:45:25</td><td></td><td></td></tr><tr><td>DeepSeek-R1</td><td>22:55:23</td><td>19:57:24</td><td>22:58:20</td><td>20:59:21</td><td>23:55:22</td><td>18:62:20</td><td>28:50:22</td><td>27:47:24</td><td></td><td></td></tr></table>\n\n# 5.4 ABLATION STUDY\n\n# 5.4.1 ADAPTIVE STRATEGY\n\nWe validate the impact of several adaptive strategies in ALPO on model performance in Table 5. Results show that the gating function $\\mathbf { 1 } ( s _ { i } )$ has the greatest effect, indicating that the gating mechanism effectively reduces noise from low-diversity irrelevant lines (e.g., simple lines such as “Good morning.”). Moreover, ablating the importance score $\\delta ( s _ { i } )$ , dynamic $\\beta$ , and scheduled prefix mixing also leads to varying degrees of performance drop, highlighting the necessity of fine-grained control in training.\n\nTable 5: Impact of adaptive strategies.   \n\n<table><tr><td></td><td colspan=\"3\">en→zh</td><td colspan=\"3\">zh→th</td></tr><tr><td></td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td></tr><tr><td>SFT</td><td>86.5</td><td>82.1</td><td>59.2</td><td>87.2</td><td>82.4</td><td>66.0</td></tr><tr><td>ALPO</td><td>90.6</td><td>84.2</td><td>76.6</td><td>91.9</td><td>84.7</td><td>74.1</td></tr><tr><td>w/o w(s_i)</td><td>88.1</td><td>83.2</td><td>67.4(↓9.2)</td><td>89.2</td><td>83.4</td><td>70.3(↓3.8)</td></tr><tr><td>w/o 1(s_i)</td><td>89.1</td><td>83.4</td><td>70.2(↓6.4)</td><td>89.9</td><td>83.9</td><td>71.2(↓2.9)</td></tr><tr><td>w/o δ(s_i)</td><td>89.4</td><td>83.5</td><td>72.4(↓4.2)</td><td>90.2</td><td>83.1</td><td>71.9(↓2.2)</td></tr><tr><td>fixed βi (β = 0.5)</td><td>90.0</td><td>83.4</td><td>74.7(↓1.9)</td><td>89.8</td><td>83.4</td><td>72.1(↓2.0)</td></tr><tr><td>w/o prefix mixing</td><td>89.5</td><td>83.7</td><td>73.4(↓3.2)</td><td>88.8</td><td>83.7</td><td>71.6(↓2.5)</td></tr></table>\n\n# 5.4.2 BACKBONE MODELS\n\nWe compare the performance of different backbone models in Table 6. The results demonstrate that ALPO enables all backbone models to achieve significant performance improvements compared to the SFT model (Qwen2.5-14B). Notably, the larger Qwen2.5-14B model with more parameters delivers superior and stable outcomes. LLaMA-3.1-8B exhibits underperformance in Chinese-related tasks, resulting in inferior results compared to the other two models in both $\\tt e n \\Rightarrow \\tt c h$ and $z \\operatorname { h } { \\Rightarrow } \\mathrm { e n }$ translation directions. Nevertheless, it still achieves measurable improvements over the SFT baseline.\n\nTable 6: Impact of backbone models. The 1st and 2nd best results are marked as blue and orange.   \n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Backbone</td><td colspan=\"3\">en⇒de</td><td colspan=\"3\">en⇒zh</td><td colspan=\"3\">zh⇒en</td></tr><tr><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td></tr><tr><td>SFT</td><td>-</td><td>87.7</td><td>83.4</td><td>64.4</td><td>86.5</td><td>82.1</td><td>59.2</td><td>85.2</td><td>80.1</td><td>54.9</td></tr><tr><td rowspan=\"3\">ALPO</td><td>LLaMA-3.1-8B (Dubey et al., 2024)</td><td>94.7</td><td>87.2</td><td>73.9</td><td>88.0</td><td>83.2</td><td>72.3</td><td>87.2</td><td>85.3</td><td>77.6</td></tr><tr><td>GLM4-9B (GLM et al., 2024)</td><td>93.2</td><td>86.1</td><td>73.2</td><td>88.2</td><td>84.4</td><td>74.4</td><td>89.2</td><td>85.7</td><td>78.2</td></tr><tr><td>Qwen2.5-14B (Yang et al., 2024a)</td><td>95.2</td><td>88.3</td><td>74.8</td><td>90.6</td><td>84.2</td><td>76.6</td><td>88.3</td><td>86.8</td><td>81.6</td></tr></table>\n\n# 5.5 ADDITIONAL EXPERIMENTS\n\nWe perform the following extended experiments in the appendix: 1) In Appendix C.1, we present full results of quality evaluation with further analysis. 2) In Appendix C.2, we show demonstration examples of ALPO translations. 3) In Appendix C.3, We conduct ablation studies for other factors like samping and model size. 4) In Appendix C.4, we perform performance evaluation comparing DPO and PPO. 5) In Appendix C.5, we validate the effectiveness of ALPO in other application task.\n\n# 6 CONCLUSION\n\nIn this study, we investigate literal and liberal translation in subtitle translation as well as other domains, and validate the reliability of large language models as evaluators and reward models for translation quality. Building upon this, we propose the Adaptive Local Preference Optimization (ALPO) method for training expressive and vivid subtitle translation. Our experiments, conducted under the established translation quality evaluation framework, validate the effectiveness of ALPO.\n\n# ETHICS STATEMENT\n\nThis study utilizes subtitle data obtained exclusively from the online video platform Youku. All data employed in this study were acquired with proper authorization and comply with the terms and conditions set forth by Youku. The data were used solely for academic purposes and were handled in a manner that ensures compliance with relevant ethical and legal standards.\n\nNo personal or sensitive information was collected or processed in this research. All analyses were performed on accessible content provided by Youku, and no attempts were made to infer or reveal any private or identifying information about individuals or entities featured in the materials.\n\nWe affirm that this study adheres to the principles of responsible and ethical research and complies with all applicable institutional and regulatory guidelines.\n\n# REPRODUCIBILITY STATEMENT\n\nWe release all code, training scripts, and the MuSC dataset used in this study to facilitate reproducibility and further research. Detailed experimental settings, model configurations, and evaluation protocols are provided in the main text and appendices. The source code and dataset are publicly accessible at https://github.com/CcQunResearch/ALPO.\n\n# ACKNOWLEDGMENTS\n\nThe authors would like to thank the anonymous reviewers for their helpful comments and suggestions, which have improved the quality of this manuscript.\n\n# REFERENCES\n\nAtef Odeh AbuSa’aleek. The adequacy and acceptability of machine translation in translating the islamic texts. Int. J. Engl. Linguist, 6:185–193, 2016.\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nZhiyu An, Xianzhong Ding, Yen-Chun Fu, Cheng-Chung Chu, Yan Li, and Wan Du. Golden-retriever: High-fidelity agentic retrieval augmented generation for industrial knowledge base. arXiv preprint arXiv:2408.00798, 2024.   \nMarcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphaël Marinier, ´ Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters for on-policy deep actor-critic methods? a large-scale study. In International conference on learning representations, 2021.   \nAnthropic. Anthropic’s transparency hub: Model report, 2025. URL https://www.anthropic. com/transparency/model-report.   \nMohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 4447–4455. PMLR, 2024.   \nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. Advances in Neural Information Processing Systems, 36:78142–78167, 2023.   \nMartina Bajciˇ c and Dejana Golenko. Applying large language models in legal translation: The ´ state-of-the-art. International Journal of Language & Law (JLL), 13:171–196, 2024.   \nMona Baker and Gabriela Saldanha. Routledge encyclopedia of translation studies. Routledge, 2019.   \nSusan Bassnett. Translation studies. routledge, 2013.   \nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610–623, 2021.   \nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \nDaoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, et al. Data-juicer: A one-stop data processing system for large language models. In Companion of the 2024 International Conference on Management of Data, pp. 120–134, 2024a.   \nDongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024b.   \nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \\%$ * chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.   \nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \nChristos Christodouloupoulos and Mark Steedman. A massively parallel corpus: the bible in 100 languages. Language resources and evaluation, 49:375–395, 2015.   \nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.   \nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023, 2023.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \nLogan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case study on ppo and trpo. In International Conference on Learning Representations, 2020.   \nMaxim Enis and Mark Hopkins. From llm to nmt: Advancing low-resource machine translation with claude. arXiv preprint arXiv:2404.13813, 2024.   \nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   \nMarcello Federico, Robert Enyedi, Roberto Barra-Chicote, Ritwik Giri, Umut Isik, Arvindh Krishnaswamy, and Hassan Sawaf. From speech-to-speech translation to automatic dubbing. In Proceedings of the 17th International Conference on Spoken Language Translation, pp. 257–264, 2020.   \nZhaopeng Feng, Ruizhe Chen, Yan Zhang, Zijie Meng, and Zuozhu Liu. Ladder: A model-agnostic framework boosting llm-based machine translation to the next level. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 15377–15393, 2024.   \nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024.   \nGoogle LLC. Google translate, 2023. URL https://translate.google.com. Accessed: 2025-05-15.   \nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. A survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024.   \nNuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979–995, 2024.   \nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.   \nKaiyu Huang, Fengran Mo, Xinyu Zhang, Hongliang Li, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao, Jinchen Liu, Yuzhuang Xu, et al. A survey on large language models with multilingualism: Recent advances and new frontiers. arXiv preprint arXiv:2405.10936, 2024.   \nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li. Belle: Be everyone’s large language model engine, 2023.   \nImmanuel Kant. Critique of pure reason. 1781. Modern Classical Philosophers, Cambridge, MA: Houghton Mifflin, pp. 370–456, 1908.   \nImmanuel Kant. Critique of judgment. Hackett Publishing, 1987.   \nPei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13034– 13054, 2024.   \nMuhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19113–19122, 2023.\n\nAobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, and Fei Huang. Sdpo: Segment-level direct preference optimization for social agents. arXiv preprint arXiv:2501.01821, 2025.   \nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. Madlad-400: A multilingual and document-level large audited dataset. Advances in Neural Information Processing Systems, 36, 2024.   \nJunlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Pengfei Liu, et al. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations, 2024a.   \nLijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 3923–3954, 2024b.   \nChen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703, 2023.   \nPierre Lison and Jörg Tiedemann. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pp. 923–929, 2016.   \nAiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Xiaoming Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, et al. Tis-dpo: Token-level importance sampling for direct preference optimization with estimated weights. In The Thirteenth International Conference on Learning Representations, 2025.   \nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.   \nKeming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. arXiv preprint arXiv:2401.12474, 2024.   \nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.   \nTrung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024.   \nJames MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E Taylor, and Michael L Littman. Interactive learning from policy-dependent human feedback. In International conference on machine learning, pp. 2285–2294. PMLR, 2017.   \nYu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a referencefree reward. arXiv preprint arXiv:2405.14734, 2024.   \nShivam Mhaskar, Nirmesh Shah, Mohammadi Zaki, Ashishkumar Gudmalwar, Pankaj Wasnik, and Rajiv Shah. Isometric neural machine translation using phoneme count ratio reward-based reinforcement learning. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 3966–3976, 2024.   \nJeremy Munday, Sara Ramos Pinto, and Jacob Blakesley. Introducing translation studies: Theories and applications. Routledge, 2022.   \nOpenAI. Gpt-5, 2025. URL https://openai.com/gpt-5.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730– 27744, 2022.   \nJianhui Pang, Fanghua Ye, Derek Fai Wong, Dian Yu, Shuming Shi, Zhaopeng Tu, and Longyue Wang. Salute the classic: Revisiting challenges of machine translation in the age of large language models. Transactions of the Association for Computational Linguistics, 13:73–95, 2025.   \nLuis Pérez-González. Audiovisual translation. In Routledge encyclopedia of translation studies, pp. 30–34. Routledge, 2019.   \nPranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024.   \nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \nEman Rashed Alkatheery. Google translate errors in legal texts: Machine translation quality assessment. AWEJ for Translation & Literary Studies, 7(1), 2023.   \nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \nWentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, and Fuli Feng. Direct multi-turn preference optimization for language agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2312–2324, 2024.   \nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634–8652, 2023.   \nYifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7584– 7600, 2024.   \nRalf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dániel Varga. The jrc-acquis: A multilingual aligned parallel corpus with $^ { 2 0 + }$ languages. arXiv preprint cs/0609058, 2006.   \nRalf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, and Patrick Schlüter. Dgt-tm: A freely available translation memory in 22 languages. arXiv preprint arXiv:1309.5226, 2013.   \nRalf Steinberger, Mohamed Ebrahim, Alexandros Poulis, Manuel Carrasco-Benitez, Patrick Schlüter, Marek Przybyszewski, and Signe Gilbro. An overview of the european union’s highly multilingual parallel corpora. Language resources and evaluation, 48:679–707, 2014.   \nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.   \nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In Findings of the Association for Computational Linguistics ACL 2024, pp. 13088–13110, 2024.   \nRichard S Sutton. Reinforcement learning: An introduction. A Bradford Book, 2018.   \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.   \nJörg Tiedemann. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may 2012. European Language Resources Association (ELRA). ISBN 978-2-9517408-7-7.   \nRuiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. Sotopia-pi: Interactive learning of socially intelligent language agents. arXiv preprint arXiv:2403.08715, 2024a.   \nTianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b.   \nYidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. In The Twelfth International Conference on Learning Representations, 2024c.   \nZilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing retrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223, 2024d.   \nYihan Wu, Junliang Guo, Xu Tan, Chen Zhang, Bohan Li, Ruihua Song, Lei He, Sheng Zhao, Arul Menezes, and Jiang Bian. Videodubber: Machine translation with speech-aware length control for video dubbing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13772–13779, 2023.   \nTianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024.   \nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations, 2024a.   \nHaoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, and Huda Khayrallah. X-alma: Plug & play modules and adaptive rejection for quality translation at scale. arXiv preprint arXiv:2410.03115, 2024b.   \nHaoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: pushing the boundaries of llm performance in machine translation. In Proceedings of the 41st International Conference on Machine Learning, pp. 55204–55224, 2024c.   \nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a.   \nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b.   \nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.   \nYongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct preference optimization. In Proceedings of the 41st International Conference on Machine Learning, pp. 58348–58365, 2024.\n\nBeichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. Advances in Neural Information Processing Systems, 36:23570–23589, 2023.   \nRan Zhang, Wei Zhao, and Steffen Eger. How good are llms for literary translation, really? literary translation evaluation with humans and llms. arXiv preprint arXiv:2410.18697, 2024.   \nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595–46623, 2023a.   \nRui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023b.   \nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024.   \nLianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. In The Thirteenth International Conference on Learning Representations, 2025.   \nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n\n# APPENDIX CONTENTS\n\n<table><tr><td>Appendix Sections</td><td>Contents</td></tr><tr><td>Appendix A</td><td>Data Source and Dataset Construction</td></tr><tr><td>Appendix B</td><td>Additional Details of Experimental Settings</td></tr><tr><td>Appendix C</td><td>Extended Evaluation Experiments for ALPO</td></tr><tr><td>Appendix D</td><td>Theory: Adaptive Local Preference Optimization</td></tr><tr><td>Appendix E</td><td>Further Discussions on ALPO</td></tr><tr><td>Appendix F</td><td>Prompts and Instructions in ALPO</td></tr></table>\n\n# A DATASET CONSTRUCTION\n\nIn this section, we will introduce the details related to the datasets used for model training.\n\n# A.1 DATA SOURCES\n\nIn this study, we utilize the MuSC dataset from the online video platform Youku for experiments, which includes source language subtitles and multilingual translated subtitles of the platform’s programs. Table 7 presents samples of English and Chinese subtitles from the MuSC dataset (in the ass file format), where the \"Start\" and \"End\" columns indicate the start and end times of the subtitle within the program. The translation directions covered in MuSC include en⇒de, ${ \\tt e n } \\Rightarrow \\tt f r$ , $\\tt e n \\Rightarrow \\tt c h$ , $\\mathrm { k o } \\Rightarrow \\mathrm { z h }$ , $z \\operatorname { h } { \\Rightarrow } \\in \\operatorname { n }$ , and $z \\mathrm { h } { \\Rightarrow } \\mathrm { t h }$ . These translation directions encompass both cross-language family and cross-language branch scenarios. Subtitle translation, as a data-rich task, makes it easy for leading online video platforms to gather a substantial amount of multilingual subtitle corpus. The MuSC dataset includes over 100 programs in each direction, spanning genres such as film, TV series, documentary, and animation over multiple years. Statistics for MuSC dataset are provided in Table 8.\n\nUnlike the translation of serious texts in fields such as legislation and medicine, which require a high degree of accuracy (AbuSa’aleek, 2016; Rashed Alkatheery, 2023; Bajciˇ c & Golenko, 2024), subtitle ´\n\ntext in visual media programs is deeply bound to video and audio modal information, allowing for some tolerance of accuracy loss in translation. However, it is crucial that the translated subtitle accurately conveys the emotions, tone, atmosphere, and cultural context of the original text. Therefore, human translation does not simply involve direct translation of the original subtitle but rather includes polishing, rewriting, and adaptation to meet the localization needs of the target audience (Bassnett, 2013; Pérez-González, 2019). As a result, the gold reference translations produced by human translators are refined versions of the original text, implying that the parallel corpus used for training is not truly \"parallel\" (the conclusion addressed in Section 3.2), and there is information asymmetry between the original text and its translation (see Table 7 for an example). Consequently, while constructing the training set, each direction’s program is a native-language program.\n\nTable 7: Examples of multilingual subtitles.   \n\n<table><tr><td>Start</td><td>End</td><td>Text</td></tr><tr><td>0:17:33.25</td><td>0:17:34.50</td><td>Please, let me speak!</td></tr><tr><td>0:17:39.54</td><td>0:17:42.83</td><td>As a lower house, my voice doesn&#x27;t carry much weight here.</td></tr><tr><td>0:17:42.91</td><td>0:17:46.45</td><td>But as a mother, I have a voice that matters deeply.</td></tr><tr><td>0:17:47.50</td><td>0:17:50.25</td><td>My son isn&#x27;t in his right mind.</td></tr><tr><td>0:17:51.04</td><td>0:17:53.95</td><td>His entire life, he&#x27;s chased an impossible dream.</td></tr><tr><td>0:17:54.91</td><td>0:17:59.00</td><td>What he did was, uh, foolish and unwise.</td></tr><tr><td>0:17:59.83</td><td>0:18:04.20</td><td>But he has a good heart. Please, let him come home.</td></tr><tr><td>0:18:04.29</td><td>0:18:08.66</td><td>A crime like this can&#x27;t be overlooked. The boy must be punished.</td></tr><tr><td>0:18:08.75</td><td>0:18:12.16</td><td>A violation of the Ethos calls for banishment,</td></tr><tr><td>0:18:12.25</td><td>0:18:16.25</td><td>but I can sympathize with a young man&#x27;s dream to change the world.</td></tr><tr><td>0:18:16.83</td><td>0:18:20.58</td><td>Perhaps in this matter, a lesser sentence may suffice.</td></tr><tr><td>0:17:42.91</td><td>0:17:46.50</td><td>我以母亲的身份恳请在座的各位议员们听一听</td></tr><tr><td>0:17:47.00</td><td>0:17:50.25</td><td>我这儿子确实犯了不可饶恕的错误</td></tr><tr><td>0:17:51.04</td><td>0:17:53.95</td><td>他这辈子都在追逐一个不可能实现的梦想</td></tr><tr><td>0:17:54.91</td><td>0:17:59.33</td><td>他的所作所为是很愚蠢很不明智的</td></tr><tr><td>0:17:59.83</td><td>0:18:04.25</td><td>但他并没有坏心求你们放他回家吧</td></tr><tr><td>0:18:04.33</td><td>0:18:06.70</td><td>这样严重的罪行不能轻易放过</td></tr><tr><td>0:18:06.79</td><td>0:18:08.66</td><td>这小子必须受到惩罚</td></tr><tr><td>0:18:08.75</td><td>0:18:12.20</td><td>违反社会共识的人确实应该遭到驱逐</td></tr><tr><td>0:18:12.29</td><td>0:18:16.25</td><td>但我也能体会一个年轻人梦想改变世界的雄心</td></tr><tr><td>0:18:16.33</td><td>0:18:20.58</td><td>就这个案子来说还是酌情予以轻判为好</td></tr></table>\n\nTable 8: Statistics of the datasets. FM, TV, DO, and AN are abbreviations for film, TV series, documentary, and animation, respectively.   \n\n<table><tr><td>Statistic</td><td>en⇒de</td><td>en⇒fr</td><td>en⇒zh</td><td>ko⇒zh</td><td>zh⇒en</td><td>zh⇒th</td></tr><tr><td>cross language family</td><td>✘</td><td>✘</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>cross language branch</td><td>✘</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>period</td><td>2013-2024</td><td>2015-2024</td><td>2020-2024</td><td>2017-2024</td><td>2021-2024</td><td>2021-2024</td></tr><tr><td># programs</td><td>210</td><td>207</td><td>161</td><td>139</td><td>225</td><td>218</td></tr><tr><td>program types</td><td>FM,TV,AN</td><td>FM,TV,AN</td><td>FM,TV,AN</td><td>TV</td><td>FM,TV,DO</td><td>FM,TV,DO</td></tr><tr><td># lines</td><td>1.87M</td><td>1.72M</td><td>1.02M</td><td>1.54M</td><td>1.26M</td><td>1.21M</td></tr><tr><td>total length (h)</td><td>1156</td><td>1033</td><td>852</td><td>802</td><td>501</td><td>479</td></tr><tr><td># avg source token</td><td>7.82</td><td>7.67</td><td>7.77</td><td>9.87</td><td>6.19</td><td>6.08</td></tr><tr><td># avg target token</td><td>10.83</td><td>10.64</td><td>6.41</td><td>6.51</td><td>8.17</td><td>21.11</td></tr></table>\n\n# A.2 BILINGUAL PARALLEL CORPUS CONSTRUCTION\n\nTypically, the multilingual subtitles for the same program do not correspond line by line. This is mainly due to linguistic differences, which may lead to the merger or splitting of original line during human translation. Additionally, subtitle files may contain noise such as scene titles and annotations. To automate the line-by-line alignment of multilingual subtitles, we designed a subtitle alignment algorithm that utilizes subtitle timing.\n\nFor the source language and target language line sets $\\mathcal { L } _ { \\mathrm { s r c } }$ and ${ \\mathcal { L } } _ { { \\mathrm { t g t } } }$ , we first determine the row margin $\\mathbf { M } = \\mathrm { a b s } ( | \\mathcal { L } _ { \\mathrm { s r c } } | - | \\mathcal { L } _ { \\mathrm { t g t } } | )$ . Then, for each line $\\ell _ { \\mathrm { s r c } }$ in $\\mathcal { L } _ { \\mathrm { s r c } }$ , we search within a window of size 2M around the corresponding index in ${ \\mathcal { L } } _ { { \\mathrm { t g t } } }$ for a line whose start time differs from that of $\\ell _ { \\mathrm { s r c } }$ by within 0.7 seconds to serve as $\\ell _ { \\mathrm { t g t } }$ . The reason for choosing 0.7 seconds as the threshold is that the shortest sentence in various languages (such as \"Hello\") typically has a duration of about 0.7 seconds. Through\n\nthis process, we can automatically gather a training dataset $\\mathbb { D } \\equiv \\{ ( \\ell _ { \\mathrm { s r c } } , \\ell _ { \\mathrm { t g t } } ) \\in \\mathcal { L } _ { \\mathrm { s r c } } \\times \\mathcal { L } _ { \\mathrm { t g t } } \\}$ from bilingual subtitles in programs. The complexity of this process is $O ( N )$ .\n\n# B ADDITIONAL DETAILS OF EXPERIMENTAL SETTINGS\n\nIn this section, we introduce the main settings used in the experiments of this paper. Except for certain experiments with special hyperparameters, the same hyperparameters are adopted to ensure consistency and fairness in experimental comparisons.\n\n# B.1 MAIN SETTING\n\nWe primarily employ the PyTorch1, $\\mathrm { v l l m } ^ { 2 }$ , and Transformers3 libraries to implement our approach, while utilizing DeepSpeed4 for multi-GPU parallel training. All experiments are conducted on a single machine equipped with eight A800 80GB SXM GPUs. The ALPO sampling phase for each translation direction takes approximately 1.5 hours, while the training phase requires about 2 hours. To ensure fairness and consistency in comparisons, we maintain identical irrelevant parameters across the main experiments, ablation studies, and extended experiments wherever possible. All critical hyperparameter settings are presented in Table 9.\n\nTable 9: Hyperparameter configuration in experiments.   \n\n<table><tr><td>Phase</td><td>Hyperparameter</td><td>Value</td><td>Remark</td></tr><tr><td rowspan=\"5\">SFT</td><td>n</td><td>35</td><td>Number of lines in a prompt x</td></tr><tr><td>optimizer</td><td>AdamW</td><td>-</td></tr><tr><td>learning rate</td><td>1e-6</td><td>-</td></tr><tr><td>epoch</td><td>4</td><td>-</td></tr><tr><td>batch size</td><td>96</td><td>-</td></tr><tr><td rowspan=\"8\">ALPO</td><td>k</td><td>15</td><td>Sampling size</td></tr><tr><td>temperature</td><td>1.0</td><td></td></tr><tr><td>top k</td><td>40</td><td>Sampling generation hyperparameters</td></tr><tr><td>top p</td><td>0.9</td><td></td></tr><tr><td>optimizer</td><td>AdamW</td><td>-</td></tr><tr><td>learning rate</td><td>1e-6</td><td>-</td></tr><tr><td>epoch</td><td>1</td><td>-</td></tr><tr><td>batch size</td><td>96</td><td>Segment-level batch size</td></tr></table>\n\nWe re-implemented the VideoDubber method and utilized paid APIs from Alibaba Cloud, DeepSeek, OpenAI, and Anthropic to obtain experimental and evaluation results for models such as Qwen, DeepSeek, GPT, and Claude. When eliciting translations from these models on the test set via ICL, we employed prompt in Appendix F.1, with a one-shot example to ensure structured output formatting. All LLMs used in our experiments were the latest versions as of Sep 1, 2025, specifically: qwen-max-2025-01-255, DeepSeek-V3.16, gpt-4o-2024-08-067, GPT- ${ . 5 ^ { 8 } }$ , and Claude $4 / 4 . { \\overset { \\cdot } { 1 } } ^ { 9 }$ .\n\nFor translation quality evaluation, we reserve $10 \\%$ of each dataset’s programs as the test set. Due to constraints including paid API costs and inference time, we perform sampling-based evaluation rather than full-scale assessment. From these test programs, we randomly select 2,000 subtitle segments, each containing 35 lines of subtitles, thereby constructing a test set of approximately 70,000 sentences per direction. The final evaluation results represent the average scores across these 2,000 segments.\n\n# B.2 EXPERIMENTS IN EMPIRICAL INVESTIGATION\n\n# B.2.1 INVESTIGATION ON LLM-AS-A-JUDGE\n\nIn $\\tt e n \\Rightarrow \\tt z h$ , en⇒de, $z \\operatorname { h } { \\Rightarrow } \\in \\operatorname { n }$ and $z \\mathrm { h } \\Longrightarrow$ th directions, we investigated the evaluation consistency of different LLMs $\\pi _ { \\mathrm { e } }$ (including DeepSeek-R1, GPT-5 Thinking, Claude Opus 4.1, and Qwen3-14B) compared to human evaluators on multiple translations of lines. Specifically, for each direction, each evaluator scored 500 lines $\\{ s _ { i } \\mid i \\in [ n ] , 1 \\leq i \\leq 5 0 0 \\}$ across 10 different translations on a scale of 0-100. These translations included human references, Google Translate, an SFT model $\\pi _ { \\mathrm { s f t } }$ , and multiple LLMs (including DeepSeek-R1, GPT-5 Thinking, Qwen3-235B-A22B Thinking, GPT-4o, DeepSeek-V3, Claude Sonnet 4, and Qwen-Max). The SFT model $\\pi _ { \\mathrm { s f t } }$ was trained using the MuSC dataset. The evaluation LLMs assessed translations based on ICL, and since lines are typically closely related to their contextual environment, we also provided the context lines of $s _ { i }$ to enable the LLM to reference the semantic environment of $s _ { i }$ . The prompt is detailed in Appendix F.2. The scoring criteria are:\n\n• Principle 1 (Accuracy): The translation must ensure basic accuracy (Weight: $30 \\%$ );   \n• Principle 2 (Colloquial Appropriateness): The translation must adhere to the expression habits of human colloquial speech (Weight: $30 \\%$ );   \n• Principle 3 (Expressive Power): The translation needs to be expressive and vivid (Weight: $40 \\%$ ).\n\nThe term \"weight\" here serves to provide the model with an indicative reference for importance, rather than being utilized for explicit weighting operations.\n\nFor the $\\tt e n \\Rightarrow \\tt c h$ and $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ directions, the human evaluator group consisted of four evaluators (the same four individuals), while the $\\mathtt { e n } \\Rightarrow \\mathrm { d e }$ and $z \\mathrm { h } { \\Rightarrow } \\mathrm { t h }$ directions each included two evaluators. All evaluators were undergraduate or master’s students specializing in English, German, or Thai translation, with Chinese as their native language. The human scores were averaged across evaluators. In Figure 5, we verify the consistency among the four evaluators for $\\tt e n \\Rightarrow \\tt c h$ and $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ , confirming the reliability of the human evaluation\n\nresults. For each evaluation LLM $\\pi _ { \\mathrm { e } }$ , we shuffled the order of translations four times and took the average across four scores. Each evaluator assigns a score sequence $\\mathcal { E }$ to multiple translations of $s _ { i }$ . Since the scores from different evaluators often fall into different ranges within the 0–100 scale, we focus only on the relative ranking of $\\mathcal { E }$ rather than the absolute values. Therefore, we evaluate the Spearman rank correlation $\\rho$ between different evaluators instead of using metrics such as ICC that are sensitive to absolute scores.\n\n(a) en⇒zh   \n(b) zh⇒en   \nFigure 5: Human assessment consistency verification.\n\n# B.2.2 INVESTIGATION ON BACK-TRANSLATION CONSISTENCY\n\nWe investigated the back-translation consistency of parallel corpora from different domains. Specifically, for a translation direction, we utilized GPT-4o with a simple prompt in the following text box to translate the target language text back into the source language. We then calculated the BLEU and ${ \\mathrm { C h r F } } + +$ scores between the back-translated text and the original text. Lower scores indicate a higher degree of liberal translation. We evaluated 1000 sentence pairs per domain. The datasets from different domains involved in the evaluation include:\n\n• OpenSubtitles (Lison & Tiedemann, 2016) is a collection of translated movie subtitles across 60 languages.   \n• Books (Tiedemann, 2012) is a collection of copyright free books in the literature domain.   \n• bible-uedin (Christodouloupoulos & Steedman, 2015) is a multilingual parallel corpus created from translations of the Bible.\n\n• DGT (Steinberger et al., 2013) is a publicly accessible multilingual translation memory of the Acquis Communautaire.   \n• JRC-Acquis (Steinberger et al., 2006) is a collection of legislative texts of the European Union, comprising selected documents written from the 1950s to the present.   \n• News-Commentary (Tiedemann, 2012) is a parallel corpus in the news domain provided by WMT.   \n• ECDC (Steinberger et al., 2014) is a translation memory from the European Centre for Disease Prevention and Control.   \n• EMEA (Tiedemann, 2012) is a parallel corpus made from documents of the European Medicines Agency.\n\n# The en⇒de back-translation prompt demonstration for GPT-4o.\n\nPlease literally translate the following English text into German, maintaining the highest degree of literal correspondence without any localized rewriting: Now the earth was formless and empty. Darkness was on the surface of the deep. God’s Spirit was hovering over the surface of the waters.\n\n# B.3 HUMAN TRANSLATION QUALITY EVALUATION\n\nWe conducted human evaluation of ALPO translation quality in the $\\tt e n \\Rightarrow \\tt c h$ and $z \\mathrm { h } \\Longrightarrow$ th directions. For each direction, we hired four evaluators, all of whom are undergraduate or master’s degree professionals specializing in English or Thai translation, and whose native language is Chinese. We assessed the ALPO model against four baselines (gold reference, SFT model $\\pi _ { \\mathrm { s f t } }$ , GPT-4o, and DeepSeek-R1) across dimensions of accuracy, naturalness, and vividness, as well as through a comprehensive evaluation. The comprehensive evaluation instructions provided to the evaluators are detailed in Appendix F.3, with similar instructions for other dimensions. There were a total of 32 evaluation tasks in the multidimensional assessment across both directions. Each evaluator was assigned 4 evaluation tasks. In a single evaluation task, we provided evaluators with challenger and competitor translations of 400 subtitle segments sourced from the MuSC test set, with each segment consisting of 20 subtitle lines. Evaluators were required to choose the better translation between the two options, or mark them as \"no significant difference\". The subtitle segments selected for different evaluation tasks were random subsets from the test set.\n\n# C EXTENDED EXPERIMENTS\n\nIn this section, we will present additional evaluation experiments for ALPO.\n\n# C.1 FULL EVALUATION OF TRANSLATION QUALITY\n\nWe present the full results of the translation quality evaluation in Table 10. Our assessment of accuracy includes results from three LLM evaluators as well as traditional translation evaluation model XCOMET (Guerreiro et al., 2024). The results in Table 10 show that Google Translate achieved substantial performance in translation directions within the same language family, such as $\\tt e n \\Rightarrow \\tt d e$ and ${ \\tt e n } \\Rightarrow \\tt f r$ . However, its performance was poor for other languages, which might be due to the relatively simpler nature of translations between languages of the same language family. Traditional sequence-to-sequence architectures like MADLAD performed significantly worse than modern LLMs, and even VideoDubber, specifically designed for subtitle translation, did not demonstrate noticeably better performance than NLLB and MADLAD models. The comparison between ALPO and cutting-edge LLMs led to several clear conclusions:\n\n• Different LLM evaluators provide consistent ranking of translations by varying models across multiple dimensions, further validating the conclusions in Section 3.1.\n\n• Despite the receiving lower accuracy and naturalness scores, human translations excelled in the vividness dimension, which could indicate that human translators incorporate more interpretive translations based on the video content.   \n• Reason models scored higher than chat models in vividness but lower in other dimensions, suggesting that reason models adhere more to liberal translation instructions compared to chat models. This aligns with the conclusion in Section 3.3, as performing liberal translations can compromise accuracy to some extent.\n\n# C.2 TRANSLATION DEMONSTRATION\n\nIn Table 11, we present the translation demonstrations for the $\\tt e n \\Rightarrow \\tt c h$ and $z \\operatorname { h } { \\Rightarrow } \\in \\operatorname { n }$ directions including human translations, chat model GPT-4o, reason model GPT-5 Thinking, and our ALPO model. It can be intuitively observed that the translations by GPT-4o tend more towards literal translation and fail to convey the emotional ambiance and stylistic information of the original subtitle through liberal translation like the other translations do. The model trained with ALPO aligns better with the subtitle translation scenario, which includes:\n\n• The translations are more conversational, resembling human character dialogues rather than formal translations.   \n• The wording and phrasing are more vivid, offering expressive articulation.   \n• It does not excessively pursue accuracy, allowing for free translation based on context.\n\nFurthermore, Table 11 illustrates some limitations of LLM translations. LLM translation relies solely on subtitle text and cannot reference information from video context and scenes as human translators do. For instance, LLM translates \"nature\" to \" 然\", whereas based on the video context, it should 自be translated to \"天性\". This issue surpasses what preference optimization techniques can address. Therefore, we believe multimodal machine translation might be a valuable direction for advancing subtitle translation research.\n\n# C.3 EXTENDED ABLATION STUDY\n\n# C.3.1 ALIGNMENT LOSS\n\nWhen simplified, ALPO can be viewed as a framework compatible with various plug-and-play preference optimization losses. Specifically, the ALPO loss can be expressed as:\n\n\nwhere $\\mathcal { L } _ { \\mathrm { p o } }$ can adopt multiple types of preference optimization losses, such as DPO (Rafailov et al., 2024), SimPO (Meng et al., 2024), and GRPO (Shao et al., 2024). For the advantage-based GRPO loss, we compute it as:\n\n\n\n\nwhere $\\varepsilon$ and $\\eta$ are adjustable hyperparameters, and $A _ { j }$ denotes the advantage estimated from the score sequence $\\mathcal { E } _ { i }$ .\n\nWe compare the performance of ALPO with different preference alignment losses $\\mathcal { L } _ { \\mathrm { p o } }$ in Table 12. The experimental results demonstrate that all loss variants achieve substantial improvements over the SFT model, validating ALPO’s broad compatibility. The DPO loss exhibits stable and superior performance across multiple translation directions. While GRPO effectively utilizes all sampled outputs rather than focusing solely on preference pairs, it only achieves optimal performance in the $\\tt e n \\Rightarrow \\tt c h$ direction.\n\nTable 10: Full quality evaluation results. The 1st and 2nd best results are marked as blue and orange, respectively.   \n\n<table><tr><td rowspan=\"3\">Models</td><td rowspan=\"3\">Training</td><td colspan=\"12\">en⇒de</td><td></td></tr><tr><td colspan=\"5\">Accuracy</td><td colspan=\"4\">Naturalness</td><td colspan=\"3\">Vividness</td><td></td></tr><tr><td>XCOMET</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td></tr><tr><td>Gold Reference</td><td>Human</td><td>81.4</td><td>83.4</td><td>88.6</td><td>85.6</td><td>84.8</td><td>83.6</td><td>85.5</td><td>82.2</td><td>83.8</td><td>64.4</td><td>77.1</td><td>77.7</td><td>73.1</td></tr><tr><td>VideoDubber</td><td></td><td>43.2</td><td>46.1</td><td>55.1</td><td>21.4</td><td>41.5</td><td>44.7</td><td>40.2</td><td>21.6</td><td>35.5</td><td>42.2</td><td>47.1</td><td>33.7</td><td>41.0</td></tr><tr><td>NLLB-3.3B</td><td>ST</td><td>70.3</td><td>73.5</td><td>88.1</td><td>71.4</td><td>75.8</td><td>72.2</td><td>75.2</td><td>63.0</td><td>70.1</td><td>58.2</td><td>60.4</td><td>58.9</td><td>59.2</td></tr><tr><td>MADLAD-10B</td><td></td><td>68.9</td><td>70.0</td><td>87.3</td><td>66.4</td><td>73.2</td><td>68.4</td><td>71.1</td><td>57.2</td><td>65.6</td><td>55.2</td><td>55.7</td><td>52.6</td><td>54.5</td></tr><tr><td>Google Translate</td><td></td><td>88.3</td><td>89.9</td><td>89.9</td><td>91.5</td><td>89.9</td><td>80.4</td><td>81.7</td><td>79.8</td><td>80.6</td><td>60.9</td><td>64.2</td><td>62.8</td><td>62.6</td></tr><tr><td>GPT-4o</td><td></td><td>91.2</td><td>96.8</td><td>96.5</td><td>91.8</td><td>94.1</td><td>89.1</td><td>88.7</td><td>82.2</td><td>86.7</td><td>56.2</td><td>71.3</td><td>73.3</td><td>66.9</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>91.7</td><td>97.8</td><td>97.9</td><td>94.7</td><td>95.5</td><td>89.6</td><td>90.1</td><td>87.8</td><td>89.2</td><td>58.0</td><td>74.0</td><td>74.7</td><td>68.9</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>90.2</td><td>97.3</td><td>96.9</td><td>94.9</td><td>94.8</td><td>89.8</td><td>89.9</td><td>87.9</td><td>89.2</td><td>56.5</td><td>71.4</td><td>74.2</td><td>67.4</td></tr><tr><td>DeepSeek-R1</td><td>ICL (R)</td><td>89.3</td><td>94.4</td><td>94.4</td><td>93.0</td><td>92.8</td><td>88.0</td><td>88.7</td><td>85.5</td><td>87.4</td><td>61.1</td><td>74.9</td><td>74.1</td><td>70.0</td></tr><tr><td>GPT-5</td><td></td><td>90.7</td><td>95.7</td><td>94.6</td><td>93.4</td><td>93.6</td><td>89.1</td><td>89.3</td><td>87.5</td><td>88.6</td><td>64.8</td><td>76.5</td><td>76.9</td><td>72.7</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>84.7</td><td>87.6</td><td>91.1</td><td>86.7</td><td>87.5</td><td>83.9</td><td>86.3</td><td>80.6</td><td>83.6</td><td>56.0</td><td>68.9</td><td>68.2</td><td>64.4</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>91.9</td><td>98.0</td><td>97.0</td><td>94.8</td><td>95.4</td><td>88.9</td><td>89.3</td><td>87.1</td><td>88.4</td><td>68.3</td><td>78.0</td><td>78.1</td><td>74.8</td></tr></table>\n\n<table><tr><td rowspan=\"3\">Models</td><td rowspan=\"3\">Training</td><td colspan=\"13\">en=fr</td></tr><tr><td colspan=\"5\">Accuracy</td><td colspan=\"4\">Naturalness</td><td colspan=\"4\">Vividness</td></tr><tr><td>XCOMET</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td></tr><tr><td>Gold Reference</td><td>Human</td><td>76.4</td><td>84.4</td><td>89.0</td><td>84.1</td><td>83.5</td><td>85.5</td><td>86.8</td><td>82.7</td><td>85.0</td><td>68.1</td><td>78.6</td><td>77.6</td><td>74.8</td></tr><tr><td>VideoDubber</td><td></td><td>42.3</td><td>46.9</td><td>62.1</td><td>41.7</td><td>48.2</td><td>48.1</td><td>52.5</td><td>41.4</td><td>47.3</td><td>47.4</td><td>53.4</td><td>44.8</td><td>48.5</td></tr><tr><td>NLLB-3.3B</td><td>ST</td><td>62.2</td><td>76.9</td><td>89.6</td><td>78.7</td><td>76.9</td><td>75.0</td><td>77.9</td><td>70.8</td><td>74.6</td><td>61.4</td><td>63.3</td><td>60.7</td><td>61.8</td></tr><tr><td>MADLAD-10B</td><td></td><td>58.1</td><td>75.6</td><td>87.5</td><td>73.3</td><td>73.6</td><td>70.3</td><td>71.4</td><td>61.7</td><td>67.8</td><td>55.7</td><td>59.4</td><td>57.1</td><td>57.4</td></tr><tr><td>Google Translate</td><td></td><td>81.4</td><td>93.5</td><td>95.9</td><td>96.7</td><td>91.9</td><td>84.5</td><td>85.8</td><td>84.2</td><td>84.8</td><td>64.3</td><td>64.6</td><td>64.0</td><td>64.3</td></tr><tr><td>GPT-40</td><td></td><td>86.4</td><td>97.7</td><td>96.3</td><td>92.6</td><td>93.2</td><td>89.8</td><td>90.3</td><td>86.4</td><td>88.8</td><td>59.0</td><td>74.9</td><td>74.7</td><td>69.5</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>86.1</td><td>97.5</td><td>97.3</td><td>95.6</td><td>94.1</td><td>90.4</td><td>90.4</td><td>89.0</td><td>89.9</td><td>61.1</td><td>76.8</td><td>76.8</td><td>71.6</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>87.6</td><td>97.2</td><td>98.9</td><td>95.9</td><td>94.9</td><td>90.8</td><td>90.6</td><td>90.7</td><td>90.7</td><td>61.7</td><td>77.3</td><td>77.9</td><td>72.3</td></tr><tr><td>DeepSeek-R1</td><td></td><td>87.1</td><td>96.7</td><td>95.3</td><td>95.1</td><td>93.6</td><td>89.7</td><td>90.8</td><td>89.6</td><td>90.0</td><td>64.5</td><td>77.3</td><td>78.3</td><td>73.4</td></tr><tr><td>GPT-5</td><td>ICL (R)</td><td>83.8</td><td>96.4</td><td>95.0</td><td>93.8</td><td>92.2</td><td>90.1</td><td>90.8</td><td>90.4</td><td>90.4</td><td>68.4</td><td>79.3</td><td>79.8</td><td>75.8</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>77.7</td><td>89.7</td><td>90.6</td><td>86.7</td><td>86.2</td><td>85.9</td><td>86.8</td><td>83.6</td><td>85.4</td><td>57.4</td><td>71.7</td><td>74.0</td><td>67.7</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>86.5</td><td>97.9</td><td>96.7</td><td>95.4</td><td>94.1</td><td>88.7</td><td>91.5</td><td>87.4</td><td>89.2</td><td>73.7</td><td>82.0</td><td>80.7</td><td>78.8</td></tr></table>\n\n<table><tr><td rowspan=\"3\">Models</td><td rowspan=\"3\">Training</td><td colspan=\"12\">en⇒zh</td><td></td></tr><tr><td colspan=\"5\">Accuracy</td><td colspan=\"4\">Naturalness</td><td colspan=\"3\">Vividness</td><td></td></tr><tr><td>XCOMET</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td></tr><tr><td>Gold Reference</td><td>Human</td><td>79.0</td><td>84.3</td><td>88.5</td><td>82.6</td><td>83.6</td><td>81.6</td><td>85.7</td><td>80.6</td><td>82.6</td><td>68.9</td><td>69.7</td><td>76.0</td><td></td></tr><tr><td>VideoDubber</td><td></td><td>40.9</td><td>47.5</td><td>64.1</td><td>35.0</td><td>46.9</td><td>32.4</td><td>61.3</td><td>42.0</td><td>51.9</td><td>49.7</td><td>52.2</td><td>47.1</td><td></td></tr><tr><td>NLLB-3.3B</td><td>ST</td><td>61.9</td><td>65.2</td><td>70.4</td><td>48.2</td><td>61.4</td><td>59.5</td><td>56.8</td><td>45.6</td><td>54.0</td><td>46.3</td><td>45.2</td><td>39.5</td><td></td></tr><tr><td>MADLAD-10B</td><td></td><td>53.5</td><td>61.0</td><td>79.5</td><td>44.7</td><td>59.7</td><td>58.3</td><td>65.5</td><td>42.7</td><td>55.5</td><td>47.4</td><td>46.7</td><td>44.8</td><td></td></tr><tr><td>Google Translate</td><td></td><td>79.3</td><td>81.4</td><td>96.1</td><td>80.0</td><td>84.2</td><td>79.4</td><td>85.1</td><td>74.6</td><td>79.7</td><td>52.3</td><td>51.8</td><td>59.0</td><td></td></tr><tr><td>GPT-4o</td><td></td><td>84.1</td><td>92.1</td><td>92.6</td><td>88.6</td><td>89.3</td><td>85.0</td><td>84.2</td><td>79.6</td><td>82.3</td><td>56.3</td><td>57.7</td><td>65.3</td><td></td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>88.3</td><td>95.3</td><td>93.5</td><td>90.3</td><td>91.9</td><td>85.8</td><td>85.8</td><td>81.6</td><td>84.4</td><td>59.1</td><td>61.7</td><td>63.1</td><td></td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>85.9</td><td>95.6</td><td>93.6</td><td>89.5</td><td>91.2</td><td>86.9</td><td>86.2</td><td>82.8</td><td>85.3</td><td>59.5</td><td>61.8</td><td>69.2</td><td></td></tr><tr><td>DeepSeek-R1</td><td>ICL (R)</td><td>86.9</td><td>92.3</td><td>92.9</td><td>90.0</td><td>90.5</td><td>87.2</td><td>86.0</td><td>83.9</td><td>85.7</td><td>68.1</td><td>70.0</td><td>74.3</td><td></td></tr><tr><td>GPT-5</td><td></td><td>88.4</td><td>96.7</td><td>93.6</td><td>90.8</td><td>92.4</td><td>88.4</td><td>87.1</td><td>85.4</td><td>87.0</td><td>69.2</td><td>69.9</td><td>74.3</td><td></td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>82.4</td><td>89.9</td><td>90.1</td><td>83.4</td><td>86.4</td><td>83.3</td><td>83.3</td><td>79.5</td><td>82.0</td><td>57.0</td><td>56.0</td><td>64.2</td><td></td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>84.8</td><td>93.9</td><td>93.1</td><td>90.8</td><td>90.6</td><td>84.2</td><td>85.0</td><td>83.8</td><td>84.3</td><td>73.6</td><td>76.9</td><td>79.2</td><td></td></tr></table>\n\n<table><tr><td rowspan=\"2\">Models</td><td rowspan=\"2\">Training</td><td colspan=\"5\">Accuracy</td><td colspan=\"7\">ko⇒zh</td></tr><tr><td>XCOMET</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td colspan=\"4\">Naturalness</td><td colspan=\"3\">Vividness</td></tr><tr><td>Gold Reference</td><td>Human</td><td>68.6</td><td>80.4</td><td>88.4</td><td>74.6</td><td>78.0</td><td>78.8</td><td>79.9</td><td>74.7</td><td>77.8</td><td>65.6</td><td>58.4</td><td>73.5</td></tr><tr><td>VideoDubber</td><td></td><td>35.4</td><td>47.0</td><td>56.8</td><td>19.3</td><td>39.6</td><td>48.9</td><td>38.2</td><td>28.5</td><td>45.2</td><td>47.4</td><td>56.0</td><td>41.1</td></tr><tr><td>NLLB-3.3B</td><td>ST</td><td>30.9</td><td>39.8</td><td>40.1</td><td>21.8</td><td>33.1</td><td>30.3</td><td>29.5</td><td>18.5</td><td>26.1</td><td>27.6</td><td>30.6</td><td>17.9</td></tr><tr><td>MADLAD-10B</td><td></td><td>37.2</td><td>47.6</td><td>66.1</td><td>28.8</td><td>44.9</td><td>44.7</td><td>57.3</td><td>26.8</td><td>42.9</td><td>44.4</td><td>55.6</td><td>40.2</td></tr><tr><td>Google Translate</td><td></td><td>42.4</td><td>50.6</td><td>76.9</td><td>49.6</td><td>54.9</td><td>49.5</td><td>61.8</td><td>47.2</td><td>52.8</td><td>46.3</td><td>57.3</td><td>52.4</td></tr><tr><td>GPT-4o</td><td></td><td>70.5</td><td>87.5</td><td>86.5</td><td>75.4</td><td>80.0</td><td>81.7</td><td>81.7</td><td>76.4</td><td>79.9</td><td>55.5</td><td>51.8</td><td>67.0</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>74.5</td><td>89.7</td><td>88.8</td><td>81.7</td><td>83.7</td><td>83.4</td><td>83.7</td><td>80.5</td><td>82.5</td><td>57.1</td><td>57.6</td><td>70.7</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>73.5</td><td>91.9</td><td>87.7</td><td>79.1</td><td>83.1</td><td>84.7</td><td>82.2</td><td>79.8</td><td>82.2</td><td>55.6</td><td>49.6</td><td>66.4</td></tr><tr><td>DeepSeek-R1</td><td>ICL (R)</td><td>71.1</td><td>86.0</td><td>86.4</td><td>75.9</td><td>79.8</td><td>84.0</td><td>82.3</td><td>78.6</td><td>81.6</td><td>64.0</td><td>60.0</td><td>72.7</td></tr><tr><td>GPT-5</td><td></td><td>76.3</td><td>93.5</td><td>88.7</td><td>79.5</td><td>84.5</td><td>85.1</td><td>83.2</td><td>79.6</td><td>82.6</td><td>63.1</td><td>59.3</td><td>72.5</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>72.9</td><td>84.8</td><td>88.1</td><td>77.7</td><td>80.9</td><td>78.5</td><td>78.7</td><td>71.2</td><td>76.1</td><td>53.8</td><td>47.0</td><td>61.0</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>76.0</td><td>89.8</td><td>89.0</td><td>82.3</td><td>84.3</td><td>83.6</td><td>84.7</td><td>81.7</td><td>83.3</td><td>67.6</td><td>67.5</td><td>76.3</td></tr></table>\n\n<table><tr><td rowspan=\"3\">Models</td><td rowspan=\"3\">Training</td><td colspan=\"12\">zh=en</td></tr><tr><td colspan=\"5\">Accuracy</td><td colspan=\"4\">Naturalness</td><td colspan=\"3\">Vividness</td></tr><tr><td>XCOMET</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td></tr><tr><td>Gold Reference</td><td>Human</td><td>78.0</td><td>87.8</td><td>86.6</td><td>79.4</td><td>83.0</td><td>82.0</td><td>81.9</td><td>77.1</td><td>80.3</td><td>71.9</td><td>71.1</td><td>76.9</td></tr><tr><td>VideoDubber</td><td rowspan=\"4\">ST</td><td>54.1</td><td>54.3</td><td>66.5</td><td>39.5</td><td>53.6</td><td>59.2</td><td>60.5</td><td>44.6</td><td>54.8</td><td>53.9</td><td>52.8</td><td>43.6</td></tr><tr><td>NLLB-3.3B</td><td>35.9</td><td>30.8</td><td>36.8</td><td>12.7</td><td>29.1</td><td>26.4</td><td>26.1</td><td>12.5</td><td>21.7</td><td>26.4</td><td>24.4</td><td>11.5</td></tr><tr><td>MADLAD-10B</td><td>41.6</td><td>49.6</td><td>56.0</td><td>33.3</td><td>45.1</td><td>45.7</td><td>40.9</td><td>30.2</td><td>38.9</td><td>43.5</td><td>38.2</td><td>31.2</td></tr><tr><td>Google Translate</td><td>77.2</td><td>75.6</td><td>88.1</td><td>78.2</td><td>79.8</td><td>70.0</td><td>67.6</td><td>61.3</td><td>66.3</td><td>56.2</td><td>45.4</td><td>49.1</td></tr><tr><td>GPT-4o</td><td></td><td>84.8</td><td>92.3</td><td>90.3</td><td>86.7</td><td>88.5</td><td>85.2</td><td>83.9</td><td>79.9</td><td>83.0</td><td>61.8</td><td>63.4</td><td>68.6</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>86.4</td><td>93.2</td><td>92.6</td><td>87.9</td><td>90.0</td><td>87.0</td><td>85.2</td><td>82.7</td><td>85.0</td><td>63.7</td><td>67.2</td><td>69.5</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>85.3</td><td>92.9</td><td>91.4</td><td>88.3</td><td>89.5</td><td>85.8</td><td>84.9</td><td>81.5</td><td>84.1</td><td>59.0</td><td>63.8</td><td>66.2</td></tr><tr><td>DeepSeek-R1</td><td></td><td>83.2</td><td>91.6</td><td>89.6</td><td>89.8</td><td>88.5</td><td>85.6</td><td>83.9</td><td>87.4</td><td>85.6</td><td>69.5</td><td>71.3</td><td>79.7</td></tr><tr><td>GPT-5</td><td>ICL (R)</td><td>84.9</td><td>92.0</td><td>90.9</td><td>88.5</td><td>89.1</td><td>87.2</td><td>86.8</td><td>84.3</td><td>86.1</td><td>72.7</td><td>75.3</td><td>77.5</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>80.4</td><td>88.7</td><td>88.2</td><td>83.5</td><td>85.2</td><td>81.9</td><td>82.0</td><td>76.3</td><td>80.1</td><td>51.5</td><td>52.3</td><td>60.7</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>85.1</td><td>90.9</td><td>89.8</td><td>87.6</td><td>88.3</td><td>86.1</td><td>87.1</td><td>87.2</td><td>86.8</td><td>79.2</td><td>83.0</td><td>82.8</td></tr></table>\n\n<table><tr><td rowspan=\"2\">Models</td><td rowspan=\"2\">Training</td><td colspan=\"5\">Accuracy</td><td colspan=\"4\">Naturalness</td><td colspan=\"4\">Vividness</td></tr><tr><td>XCOMET</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td><td>DeepSeek</td><td>Claude</td><td>GPT-5</td><td>Avg.</td></tr><tr><td>Gold Reference</td><td>Human</td><td>70.5</td><td>77.9</td><td>78.0</td><td>80.0</td><td>76.6</td><td>74.3</td><td>74.7</td><td>76.3</td><td>75.1</td><td>60.8</td><td>67.5</td><td>70.6</td><td>66.3</td></tr><tr><td>VideoDubber</td><td></td><td>40.2</td><td>38.2</td><td>42.5</td><td>15.7</td><td>34.1</td><td>39.0</td><td>48.7</td><td>17.0</td><td>34.9</td><td>43.7</td><td>49.8</td><td>31.0</td><td>41.5</td></tr><tr><td>NLLB-3.3B</td><td rowspan=\"3\">ST</td><td>41.8</td><td>50.2</td><td>59.8</td><td>18.7</td><td>42.6</td><td>48.9</td><td>41.1</td><td>11.7</td><td>33.9</td><td>49.3</td><td>41.7</td><td>30.5</td><td>40.5</td></tr><tr><td>MADLAD-10B</td><td>45.6</td><td>51.4</td><td>69.0</td><td>25.7</td><td>47.9</td><td>54.7</td><td>64.7</td><td>33.0</td><td>50.8</td><td>52.7</td><td>58.1</td><td>42.2</td><td>51.0</td></tr><tr><td>Google Translate</td><td>52.5</td><td>50.1</td><td>71.4</td><td>46.7</td><td>55.2</td><td>53.6</td><td>65.9</td><td>49.2</td><td>56.2</td><td>53.5</td><td>57.6</td><td>52.5</td><td>54.5</td></tr><tr><td>GPT-4o</td><td></td><td>80.9</td><td>90.8</td><td>89.6</td><td>90.7</td><td>88.0</td><td>85.1</td><td>84.7</td><td>83.5</td><td>84.4</td><td>62.6</td><td>67.1</td><td>73.9</td><td>67.9</td></tr><tr><td>Qwen-Max</td><td>ICL (C)</td><td>84.3</td><td>95.1</td><td>91.4</td><td>94.5</td><td>91.3</td><td>85.9</td><td>86.1</td><td>85.5</td><td>85.8</td><td>63.8</td><td>70.0</td><td>73.6</td><td>69.1</td></tr><tr><td>DeepSeek-V3.1</td><td></td><td>84.8</td><td>92.6</td><td>89.3</td><td>93.1</td><td>89.9</td><td>85.0</td><td>85.2</td><td>83.5</td><td>84.6</td><td>61.8</td><td>66.6</td><td>72.8</td><td>67.1</td></tr><tr><td>DeepSeek-R1</td><td rowspan=\"2\">ICL (R)</td><td>82.2</td><td>89.2</td><td>88.1</td><td>90.8</td><td>87.6</td><td>85.5</td><td>84.4</td><td>84.0</td><td>84.0</td><td>64.4</td><td>72.3</td><td>76.2</td><td>71.0</td></tr><tr><td>GPT-5</td><td>83.3</td><td>92.9</td><td>88.1</td><td>90.5</td><td>88.7</td><td>85.1</td><td>82.4</td><td>84.1</td><td>83.9</td><td>67.3</td><td>73.7</td><td>78.1</td><td>73.0</td></tr><tr><td>Qwen2.5-14B</td><td>SFT</td><td>81.7</td><td>90.0</td><td>88.8</td><td>88.7</td><td>87.3</td><td>82.6</td><td>83.8</td><td>81.4</td><td>82.6</td><td>59.5</td><td>66.3</td><td>72.2</td><td>66.0</td></tr><tr><td>Qwen2.5-14B</td><td>ALPO</td><td>85.0</td><td>95.0</td><td>92.7</td><td>95.1</td><td>91.9</td><td>84.3</td><td>85.3</td><td>84.4</td><td>84.7</td><td>69.0</td><td>75.4</td><td>78.1</td><td>74.2</td></tr></table>\n\nTable 11: Multiple translation demonstration of $\\tt e n \\Rightarrow \\tt c h$ and $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ . High-quality translations and inadequate translations are marked in blue and orange, respectively.   \n\n<table><tr><td>Source</td><td>Human</td><td>GPT-4o</td><td>GPT-5</td><td>ALPO (Ours)</td></tr><tr><td>Piltover was founded to escape the warmongering of mages.</td><td>当初建立皮尔特沃夫就是为了避开那群好战的魔法师。</td><td>建立皮尔特沃夫正是为了逃离法师的战争</td><td>皮尔特沃夫是为了逃避法师的战争狂潮而建立的</td><td>皮尔特沃夫的创立是为了逃离术士们的穷兵黩武</td></tr><tr><td>The Arcane was founded the curse of our world. My race was nearly destroyed by it.</td><td>奥术对我们的世界赛说就是谁诅我的族人几乎因此而灭绝</td><td>奥术是我们世界的诅咒我的族群几乎被它摧毁</td><td>奥术是我们世界的诅咒我的族人几乎因此灭绝</td><td>奥术是世界的诅咒我的族人曾险些被它摧毁殆尽</td></tr><tr><td>Councilors Hoskel and Salo. Don&#x27;t they hate each other?</td><td>霍斯卡尔议员和萨罗议员他俩不是死对头吗</td><td>议员们霍斯克和萨罗他们不是互相讨厌吗</td><td>霍斯卡尔议员和萨罗议员他俩不是水火不容吗</td><td>霍斯卡尔议员和萨罗议员他们不是水火不容吗</td></tr><tr><td>His entire life, he&#x27;s chased an impossible dream.</td><td>他这案子都在追逐一个不可能实现的梦想</td><td>他一生都在追逐一个不可能的梦想</td><td>他这案子都在追逐不切实际的幻想</td><td>他一生都在追逐一个可能的梦</td></tr><tr><td>It was the ship of dreams to everyone else. To me, it was a slave ship.</td><td>它是所有人心目中的梦想之船对我只是一条贩奴船</td><td>对其他人来说这是一种梦想之船对我来说它是一艘奴船</td><td>在别人眼中这是艘梦想之船对我来说它却是艘奴船</td><td>对旁人来说这艘船承载着梦想而我于其看来是艘奴隶船</td></tr><tr><td>I would have gone overboard but Mr. Dawson here saved me.</td><td>我差点儿摔下去</td><td>如果不是道森先生救了我我早就掉下水了</td><td>要不是迈进道森先生救我</td><td>要不是道森先生及时相救我差点葬身大海</td></tr><tr><td>Nature has made us intolerant to change, but fortunately, we have the capacity to change our nature.</td><td>天性会让我们排斥异变但好在我有能力改变自己的天性</td><td>大自然让我们对变化难以接受但幸运的是我们有能力改变我们的本性</td><td>自然让我们无法适应剧变但幸运的是我们拥有改变天性的能力</td><td>自然造物使我们抗拒改变但我们幸而拥有改变天性的能力</td></tr><tr><td>You made me want to see the world in more than just black and white.</td><td>你让我超越黑白分明的视角看世界</td><td>你让我想要从不是非黑即白的角度看世界</td><td>你让我学会用更丰富的眼光看世界</td><td>你让我渴望用缤纷的色彩而不只是黑白两色来观照世界</td></tr><tr><td>Don&#x27;t be scared. Think of yourself as a caterpillar about to transform into a butterfly.</td><td>别害怕想象自己是即将变成蝴蝶的毛毛虫</td><td>别害怕想象你自己是一只即将化蛹为蝶的毛毛虫</td><td>别害怕把自己想象成即将化身成蝶的毛毛虫</td><td>别害怕把自己想象成一只即将破茧成蝶的毛毛虫</td></tr><tr><td>As long as we protected each other, we would stay immortal...and young.</td><td>只要我们互相保护就会永远年轻</td><td>只要我们保护彼此就可以保持不死且年轻</td><td>只要互相守护就能永葆青春与不朽</td><td>只要我们彼此守护就能长生不老永葆青春</td></tr><tr><td>You two think you&#x27;re better than us, but really, you&#x27;re just a pig and a pig.</td><td>你们俩以为自己比我们强但你们只是猎豹</td><td>你们俩为自己比我们好但事实上你们不过就是一个牛鼻子老学究和一个小猪</td><td>你们俩自以为高人一等其实一个假清高一个真肥猪</td><td>你们俩自以为高人一等其实何你俩一个通貌浑然一个猪狗不知</td></tr><tr><td>One day you&#x27;re on top...the next, you&#x27;re a clown.</td><td>今日风光无限</td><td>有一天你在上面...</td><td>今天你风光无限...</td><td>今天你风光无限...</td></tr><tr><td>Gotham loves a comeback story.</td><td>明朝小丑相见</td><td>下一天你就是个小丑</td><td>明天就沦为小丑</td><td>明日就沦为小丑一枝</td></tr><tr><td>You&#x27;re the de facto head of the Council. People notice where you go.</td><td>你是议会真正的领袖行踪备受关注</td><td>你是议会的实际领导人注意你去了哪里</td><td>身为议会实际上的话事人行踪自然会受关注</td><td>你是议会实际上的领袖一举一动自然引人瞩目</td></tr><tr><td>我那个时候就有一种强烈的预感那个球啊我一定能投进</td><td>At that moment, I had a really strong intuition. That I would be able to score the shot.</td><td>At that moment, I had a strong feeling that I&#x27;d definitely make that shot.</td><td>Back then, I just had this strong feeling--that shot, you know, I was gonna make it.</td><td>At that moment, I had a strong hunch that I&#x27;d make that shot.</td></tr><tr><td>霜华清光凝息止气</td><td>The luminous Frost drives away all impurities. Seal her aura.</td><td>Frost&#x27;s pure light, calming breath and energy.</td><td>Frostlight gathers, breath stills.</td><td>Frost Illume, still the breath, quell the air.</td></tr><tr><td>人生很多重要的决定都是要在这种稀里糊涂的时候马上做的</td><td>There are many important decisions in life that we have to make immediately when the situation is still blurry.</td><td>Many important decisions in life have to be made in these muddled moments.</td><td>Many crucial life decisions must be made precisely in these muddled moments immediately.</td><td>Many crucial decisions in life must be made on the spot, in moments of confusion and bewilderment.</td></tr><tr><td>现在恐怕还不是谈情说爱的时候</td><td>I&#x27;m afraid this isn&#x27;t the time for you two to be lovey-dovey.</td><td>Now&#x27;s probably not the time for romance.</td><td>It&#x27;s probably not the time yet to talk about love.</td><td>I&#x27;m afraid it&#x27;s not the right time for lovey-dovey stuff just yet.</td></tr><tr><td>你我皆是被命运捉弄之人为何不与我一起颠覆了这世间</td><td>Both you and I were toyed by fate. Why don&#x27;t you join me in oversetting this world?</td><td>You and I are both playthings of fate. Why not join me to overthrow this world?</td><td>Both of us are victims of fate&#x27;s cruelty. Why not join me to overthrow this world?</td><td>Both of us are pawns in the whims of fate. Why not join forces with me and turn this world upside down together?</td></tr><tr><td>只可惜这和戴的暖阳以后再也看不到了</td><td>It&#x27;s a pity that I won&#x27;t be able to feel the warm sunshine anymore.</td><td>It&#x27;s such a pity that this warm sunlight, I will never see again.</td><td>It&#x27;s just a pity this warm, gentle sunlight--I&#x27;ll never see it again.</td><td>what&#x27;s a shame, I&#x27;ll never get to bask in the warm sunshine ever again...</td></tr><tr><td>你不要看我赌运差但是我看的人的眼光可是很准的</td><td>I might be unlucky when it comes to gambling, but I&#x27;m pretty good at judging characters.</td><td>Don&#x27;t judge me by my bad luck at gambling, but my eye for people is pretty sharp.</td><td>Don&#x27;t judge me by my bad gambling luck--my judgment of people is razor-sharp.</td><td>Just because I&#x27;m not a lucky gambler doesn&#x27;t mean I&#x27;m bad at judging people. My instincts are spot-on.</td></tr><tr><td>青山常在绿水青山长流</td><td>The world will remain the same.</td><td>The mountains are green, and the waters flow long.</td><td>The green hills endure, the blue rivers flow.</td><td>May the mountains stand tall and the waters flow on...</td></tr><tr><td>她为了保护你被千夫所指因为担心你而食不下咽</td><td>She&#x27;s facing a backlash for trying to keep you safe. And she&#x27;s too worried about you to eat.</td><td>She took all the blame to protect you, worried sick, unable to eat.</td><td>She endured public scorn to protect you, lost her appetite worrying about you.</td><td>She&#x27;s been vilified for protecting you, and she&#x27;s lost her appetite out of worry for you.</td></tr><tr><td>她一定有三头六臂</td><td>I&#x27;m sure she has many arms and legs.</td><td>She must have three heads and six arms.</td><td>She must have three heads and six arms or something.</td><td>She must be a superwoman.</td></tr><tr><td>青丘军队全部停止前行原地扎寨不得违令</td><td>All the Qingquiu troops shall stop advancing and camp where you are. The order must be obeyed.</td><td>The Qingquiu army must halt and set up camp immediately, no disobedience allowed.</td><td>All Qingquiu troops halt your advance, set camp immediately. Disobedience forbidden.</td><td>Qingquiu&#x27;s armies, halt your advance at once. Set up camp where you stand. No disobedience allowed.</td></tr><tr><td>人间的桂花开了</td><td>The osmanthus in the human realm is blooming.</td><td>The osmanthus flowers are blooming in the Mortal Realm.</td><td>The osmanthus in the Mortal Realm is blooming.</td><td>The osmanthus blooms in the Mortal Realm.</td></tr></table>\n\nTable 12: Impact of ${ \\mathcal L } _ { \\mathrm { p o } }$ . The 1st and 2nd best results are denoted as blue and orange.   \n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Lpo</td><td colspan=\"3\">en⇒de</td><td colspan=\"3\">en⇒zh</td><td colspan=\"3\">zh⇒en</td></tr><tr><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Accuracy</td><td>Naturalness</td><td>Vividness</td></tr><tr><td rowspan=\"2\">SFT</td><td>-</td><td>87.7</td><td>83.4</td><td>64.4</td><td>86.5</td><td>82.1</td><td>59.2</td><td>85.2</td><td>80.1</td><td>54.9</td></tr><tr><td>DPO</td><td>95.2</td><td>88.3</td><td>74.8</td><td>90.6</td><td>84.2</td><td>76.6</td><td>88.3</td><td>86.8</td><td>81.6</td></tr><tr><td rowspan=\"2\">ALPO</td><td>SimPO</td><td>93.2</td><td>87.4</td><td>73.9</td><td>90.7</td><td>83.9</td><td>74.3</td><td>86.2</td><td>84.2</td><td>80.9</td></tr><tr><td>GRPO</td><td>94.6</td><td>87.9</td><td>74.3</td><td>91.2</td><td>85.0</td><td>77.1</td><td>87.2</td><td>84.9</td><td>81.3</td></tr></table>\n\n# C.3.2 SAMPLING SIZE\n\nThe sampling size of ALPO directly affects the diversity of sampled translations. In Figure 6, we examine the impact of sampling size $k$ on translation quality in the $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ direction. Results show that as $k$ increases, translation quality improves across multiple dimensions, with the most significant gain observed in vividness, and stabilizes around $k = 1 2$ . The computation of adaptive hyperparameters $w ( s _ { i } )$ and $\\beta _ { i }$ during ALPO training depends on the number and quality variation of sampled translations. A larger sampling size directly improves the quality of training translations and leads to more suitable $w ( s _ { i } )$ and $\\beta _ { i }$ . Since ALPO loss relies on the relative quality of chosen and rejected translations, performance stabilizes once the sampling size is sufficient. Based on these results, we set $k = 1 5$ for all other experiments.\n\nFigure 6: Impact of sampling size on translation quality.\n\n# C.3.3 MODEL SIZE\n\nIn Figure 7, we investigate the impact of ALPO’s backbone model size on translation quality. Using the Qwen2.5 series models ranging from 1.5B to 72B as ALPO’s base models, we evaluate translation performance in both $\\tt e n \\Rightarrow \\tt c h$ and $z \\operatorname { h } { \\Rightarrow } \\mathrm { e n }$ directions. Experimental results demonstrate that as model size increases, scores across multiple dimensions consistently improve, though the growth rate gradually diminishes. The 14B or 32B model generally achieves performance comparable to DeepSeek-R1, further validating ALPO’s significance in enhancing translation quality. ALPO employs a fully autonomous approach to preference labeling of alignment data, enabling cost-effective training of high-quality subtitle translation models. Overall, the 14B base model strikes a balanced trade-off between cost and performance. However, as subtitle translation is an offline task, larger-scale models may be adopted when pursuing SOTA performance.\n\nFigure 7: Impact of model size on translation quality.\n\n# C.3.4 DATA VOLUME\n\nWe present the translation quality scores across training steps in Figure 8 to investigate the impact of training data scale. The results demonstrate that as training data volume increases, model accuracy initially rises before declining, naturalness shows modest improvement, while vividness exhibits substantial yet gradually decelerating growth. Although enhanced training data yields higher vividness scores, we observe pronounced hallucination phenomena in the model outputs, which explains the accuracy deterioration. Comprehensive analysis indicates that training for 75 steps (corresponding to 7,000 prompts with a batch size of 96) achieves optimal balance, which serves as the default configuration for all our experiments.\n\nFigure 8: Impact of training data volume on translation quality.\n\n# C.4 FURTHER EXPLORATION\n\n# C.4.1 VANILLA DPO TRAINING\n\nTraining expressive subtitle translation LLM as a preference optimization problem, we want to explore whether vanilla DPO can directly solve this issue. Motivated by this inquiry, we designed and conducted relevant experiments. ALPO achieves significant improvements in translation quality through its segment-wise sampling strategy and fine-grained alignment loss, and we aim to verify the impact of these two components. Following the standard DPO training protocol (Rafailov et al., 2024), we perform post-training on the SFT model $\\pi _ { \\mathrm { s f t } }$ . Specifically, we employ either coarse-grained or fine-grained sampling to generate a chosen response $y ^ { \\mathrm { ( c ) } }$ and a rejected response $y ^ { \\mathrm { ( r ) } }$ for each sample $x \\in \\mathbb { D } _ { \\mathrm { a l p o } }$ in the alignment dataset, and subsequently optimize the policy model using the standard DPO loss.\n\nThe coarse-grained and fine-grained sampling procedures are illustrated in Algorithm 2 and Algorithm 3. In the coarse-grained sampling approach, we directly sample k complete responses for a given prompt x. Subsequently, a Qwen2-14B-Instruct model is utilized to score all subtitle lines across these k responses. The summation of scores for n subtitle lines within each response is calculated, with the highest-scoring response selected as the chosen response $y ^ { \\mathrm { ( c ) } }$ and the lowest-scoring as the rejected response $y ^ { \\mathrm { ( r ) } }$ . For fine-grained sampling, we perform two separate segment-wise sampling iterations similar to Algorithm 1 for each prompt x. During these iterations, the sampling of subsequent subtitle lines employs either the highest-scored or lowest-scored existing line as the prefix, ultimately yielding the segment-wise sampled chosen response $y ^ { \\mathrm { ( c ) } }$ and rejected response $y ^ { \\mathrm { ( r ) } }$ .\n\n# C.4.2 ADVANTAGE-BASED PPO TRAINING\n\nIn the literature, it is commonly acknowledged that in preference optimization techniques, RLHF (primarily based on PPO), despite its intricate procedures, outperforms DPO (Ouyang et al., 2022; Bender et al., 2021). This motivates our investigation into whether PPO can achieve superior performance to ALPO. To address this, we designed and conducted relevant experiments. We adopt\n\nAlgorithm 2 Coarse-grained Sampling for Vanilla DPO.   \nAlgorithm 3 Fine-grained Sampling for Vanilla DPO.   \nInput: SFT model $\\pi_{\\mathrm{sft}}$ , evaluation LLM $\\pi_{\\mathrm{e}}$ , alignment dataset $\\mathbb{D}_{\\mathrm{alpo}}$ , sample number $k$ Output: sampled response pairs set $S(x)$ 1: for any $x\\in \\mathbb{D}_{\\mathrm{alpo}}$ do // Iterate through the alignment dataset $\\mathbb{D}_{\\mathrm{alpo}}$ 2: for $i = 1$ to $k$ do // Sample multiple candidate responses.   \n3: Sample $\\pi_{\\mathrm{sft}}(y\\mid x)$ 4: end for   \n5: Measure the sum of $\\pi_{\\mathrm{e}}$ score $\\mathcal{E}$ for each line of $y^{i}$ in the candidate set $\\{y^i |i = 1,2,\\dots ,k\\}$ 6: Select chosen $y^{(\\mathrm{c})}$ and rejected $y^{(\\mathrm{r})}$ 7: end for   \n8: return $S(x)\\equiv \\{y^{(\\mathrm{c})},y^{(\\mathrm{r})}\\}$\n\nInput: SFT model $\\pi_{\\mathrm{sft}}$ , evaluation LLM $\\pi_{\\mathrm{e}}$ , alignment dataset $\\mathbb{D}_{\\mathrm{alpo}}$ , sample number $k$ Output: sampled response pairs set $S(x)$ 1: for any $x\\in \\mathbb{D}_{\\mathrm{alpo}}$ do // Iterate through the alignment dataset $\\mathbb{D}_{\\mathrm{alpo}}$ 2: for $i = 1$ to $n$ do // The first sampling cycle used to obtain $y^{(\\mathrm{c})}$ 3: for $j = 1$ to $k$ do   \n4: Sample $\\pi_{\\mathrm{sft}}(t_i^j\\mid x,t_1^{(\\mathrm{c})},\\dots ,t_{i - 1}^{(\\mathrm{c})}).$ 5: end for   \n6: Deduplicate the candidate set $\\{t_i^j\\mid j = 1,2,\\ldots ,k\\}$ 7: Add human reference $t_i^0$ to the candidate set, get $\\{t_i^j\\mid j = 0,1,\\dots \\}$ 8: Measure $\\{t_i^j\\mid j = 0,1,\\dots \\}$ by $\\pi_{\\mathrm{e}}$ , get score sequence $\\mathcal{E}_i$ 9: Select a chosen translation $t_i^{(\\mathrm{c})}$ (random from top 3 of $\\mathcal{E}_i$ .   \n10: end for   \n11: Concatenate $\\{t_i^{(\\mathrm{c})}|i = 1,2,\\dots ,n\\}$ yields $y^{(\\mathrm{c})}$ 12: for $i = 1$ to $n$ do // The second sampling cycle used to obtain $y^{(\\mathrm{r})}$ 13: for $j = 1$ to $k$ do   \n14: Sample $\\pi_{\\mathrm{sft}}(t_i^j\\mid x,t_1^{(\\mathrm{r})},\\dots ,t_{i - 1}^{(\\mathrm{r})}).$ 15: end for   \n16: Deduplicate the candidate set $\\{t_i^j\\mid j = 1,2,\\dots ,k\\}$ 17: Add human reference $t_i^0$ to the candidate set, get $\\{t_i^j\\mid j = 0,1,\\dots \\}$ 18: Measure $\\{t_i^j\\mid j = 0,1,\\dots \\}$ by $\\pi_{\\mathrm{e}}$ , get score sequence $\\mathcal{E}_i$ 19: Select a rejected translation $t_i^{(\\mathrm{r})}$ (random from bottom 3 of $\\mathcal{E}_i$ ).   \n20: end for   \n21: Concatenate $\\{t_i^{(\\mathrm{r})}|i = 1,2,\\dots ,n\\}$ yields $y^{(\\mathrm{r})}$ 22: end for   \n23: return $S(x)\\equiv \\{y^{(\\mathrm{c})},y^{(\\mathrm{r})}\\}$\n\nan advantage-based PPO training pipeline (Zheng et al., 2023b). Specifically, we implement the PPO training process through the following steps:\n\n1. Rollout — Use Algorithm 4 to sample a trajectory $\\tau$ for each input $x \\in \\mathbb { D } _ { \\mathrm { a l p o } }$ .   \n2. Computing Returns and Advantages — Perform Generalized Advantage Estimation (GAE) on $\\tau$ using the value network $V _ { \\phi }$ (base model: Qwen2.5-7B-Instruct (Yang et al., 2024a)):\n\n\n3. Updating Policy Network — Let $\\pi _ { \\mathrm { o l d } }$ denote the fixed old policy during sampling. Minimize the clipped loss function (KL divergence constraint between $\\pi _ { \\theta }$ and $\\pi _ { \\mathrm { o l d } }$ is omitted below):\n\n\n4. Updating Value Network — Fit $V _ { \\phi } ( p _ { i } )$ to the GAE-based estimate $\\hat { V } _ { i } = A _ { i } + V _ { \\phi } ( p _ { i } )$ via mean squared error:\n\n\n5. Repeat for Multiple Epochs — Continuously collect new data and update the value and policy networks until convergence.\n\nAlgorithm 4 Sampling for PPO solution.\n\nInput: SFT model $\\pi _ { \\mathrm { s f t } }$ , evaluation LLM $\\pi _ { \\mathrm { e } }$ , alignment dataset $\\mathbb { D } _ { \\mathrm { a l p o } }$\n\nOutput: sampled trajectory set $S ( x ) \\equiv \\{ \\tau \\}$ .\n\n1: for any $x \\in \\mathbb { D } _ { \\mathrm { a l p o } }$ do // Iterate through the DA dataset $\\mathbb { D } _ { \\mathrm { a l p o } }$   \n2: for $i = 1$ to $n$ do // Iterate through the subtitle lines in $x$   \n3: Sample $\\pi _ { \\operatorname { s f t } } ( t _ { i } \\mid x , t _ { 1 } , \\dots , t _ { i - 1 } )$ .   \n4: Measure $t _ { i }$ by $\\pi _ { \\mathrm { e } }$ , get score $\\mathcal { E } _ { i }$   \n5: end for   \n6: Obtain trajectory $\\tau = \\{ ( p _ { i } , t _ { i } , \\mathcal { E } _ { i } ) \\mid i = 1 , 2 , \\cdot \\cdot \\cdot , n \\} .$   \n7: end for   \n8: return $S ( x ) \\equiv \\{ \\tau \\}$\n\n# C.4.3 EVALUATION\n\nWe evaluated the performance of $\\tt e n \\Rightarrow \\tt c h$ and $z \\operatorname { h } { \\Rightarrow } \\mathrm { e n }$ translation under \"Vanilla DPO Training\" and \"Advantage-based PPO Training\" configurations using the Qwen2.5-14B model. In addition, we also compared two token-level DPO-based methods, TDPO (Zeng et al., 2024) and TIS-DPO (Liu et al., 2025). Experimental results presented in Table 13 demonstrate that while DPO and PPO training with segment-level sampling strategy achieved moderate performance improvements compared to the SFT model, they still exhibited significant gaps compared to ALPO with fine-grained alignment loss. ALPO also significantly outperformed the two token-level methods. This validates the effectiveness of both segment-wise sampling strategy and fine-grained alignment loss for the local preference optimization task.\n\nTable 13: Experimental evaluation results of alternative solutions. C for Coarse-grained, while F for Fine-grained. The 1st and 2nd best results are denoted as blue and orange.   \n\n<table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Train</td><td rowspan=\"2\">Sampling</td><td colspan=\"3\">en=&gt;zh</td><td rowspan=\"2\">Accuracy</td><td colspan=\"2\">zh=&gt;en</td></tr><tr><td>Accuracy</td><td>Naturalness</td><td>Vividness</td><td>Naturalness</td><td>Vividness</td></tr><tr><td>Gold Reference</td><td>-</td><td>-</td><td>83.6</td><td>82.9</td><td>71.6</td><td>82.9</td><td>80.2</td><td>73.2</td></tr><tr><td>GPT-4o</td><td>-</td><td>-</td><td>89.5</td><td>82.4</td><td>59.5</td><td>88.6</td><td>82.9</td><td>64.6</td></tr><tr><td>DeepSeek-R1</td><td>-</td><td>-</td><td>90.4</td><td>85.6</td><td>70.7</td><td>88.5</td><td>85.6</td><td>73.6</td></tr><tr><td rowspan=\"7\">Qwen2.5-14B</td><td>SFT</td><td>-</td><td>86.5</td><td>82.1</td><td>59.2</td><td>85.2</td><td>80.1</td><td>54.9</td></tr><tr><td rowspan=\"2\">DPO</td><td>C</td><td>87.1</td><td>82.2</td><td>63.3</td><td>86.1</td><td>81.1</td><td>60.2</td></tr><tr><td>F</td><td>88.2</td><td>82.5</td><td>69.4</td><td>86.3</td><td>83.1</td><td>68.4</td></tr><tr><td>PPO</td><td>F</td><td>88.3</td><td>83.2</td><td>70.1</td><td>85.9</td><td>82.3</td><td>70.1</td></tr><tr><td>TDPO</td><td>F</td><td>87.4</td><td>82.7</td><td>64.0</td><td>86.0</td><td>81.3</td><td>61.2</td></tr><tr><td>TIS-DPO</td><td>F</td><td>88.5</td><td>83.1</td><td>65.9</td><td>86.2</td><td>83.3</td><td>62.4</td></tr><tr><td>ALPO</td><td>F</td><td>90.6</td><td>84.2</td><td>76.6</td><td>88.3</td><td>86.8</td><td>81.6</td></tr></table>\n\n# C.5 FURTHER APPLICATION OF ALPO IN MULTI-TURN INTERACTION\n\n# C.5.1 OVERVIEW\n\nTo validate the generality of the ALPO method, we conducted experiments on another local preference optimization task: social language agent multi-turn interaction (Zhou et al., 2024; Wang et al., 2024a;\n\nLu et al., 2024; Kong et al., 2025). This task requires LLM agents to dynamically adjust generation strategies through multi-turn interactions in an interactive environment, guided by character profiles, contextual scenarios, and private social objectives. The objective is to enable the agent to more effectively accomplish predefined goals (e.g., persuasion, agreement attainment) within specific social scenarios (e.g., negotiation, collaboration, competition), while simultaneously maintaining or improving the relationship between both dialogue parties.\n\n# C.5.2 METHOD\n\nFor the multi-turn interaction task, we primarily complete preference alignment data sampling through two steps: 1) Error Location; 2) Preference Data Sampling. For a multi-turn dialogue in the training set, we first employ GPT-4o to identify the starting position of erroneous turns, following the same criteria as Kong et al. (Kong et al., 2025): 1) The turn is critical to achieving the role’s goal; 2) There remains room for improvement in the relationship between goal completion and the role.\n\nIf both criteria are satisfied, we take the dialogue history preceding this turn as the initial sampling prefix $p _ { 1 }$ , then utilize our LLM agent to sequentially sample 5 single-turn responses for each turn. During the $i$ -th sampling iteration, we employ GPT-4o to determine whether the current turn qualifies as crucial based on the sampled response: responses containing primarily pleasantries or similar content are deemed non-crucial, with their indicator function $\\mathbf { 1 } ( p _ { i } )$ set to 0; otherwise, it is set to 1. We then select the response with the highest combined goal and relationship score as the preferred response $y _ { i } ^ { w }$ , and the lowest as the dispreferred response $y _ { i } ^ { l }$ , with goal completion prioritized over relationship. The prefix $p _ { i }$ for the $i$ -th iteration becomes $p _ { i - 1 } , y _ { i - 1 } ^ { w }$ $y _ { i - 1 } ^ { w }$ (including interlocutor behaviors, omitted here). Finally, we apply the following simplified ALPO loss function for alignment training:\n\n\nwhere $\\mathbb { D }$ denotes the dataset, and both $\\pi _ { \\theta }$ and $\\pi _ { \\mathrm { r e f } }$ are initialized from behavioral cloning (BC) models.\n\n# C.5.3 EXPERIMENTS\n\nWe utilize 100 out of the 410 scenarios from SOTOPIA- $\\pi$ (Wang et al., 2024a) for behavioral cloning (10 role pairs per scenario) and 310 scenarios for alignment (8 role pairs per scenario). We adopt SOTOPIA (Zhou et al., 2024) as the test set, containing 90 scenarios with 5 role pairs per scenario, totaling 450 self-chat tasks and 900 non-self-chat tasks. We evaluate both goal and relationship dimensions on the same baselines, with experimental results presented in Table 14. Baseline results are sourced from Kong et al. (Kong et al., 2025).\n\nTable 14: The performance of various methods on SOTOPIA across the goal and relationship dimensions. The 1st and 2nd best results are denoted as blue and orange, respectively.   \n\n<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Self-Chat</td><td colspan=\"2\">GPT-4o</td></tr><tr><td>Goal</td><td>Rel</td><td>Goal</td><td>Rel</td></tr><tr><td>GPT-3.5-turbo</td><td>6.38</td><td>1.36</td><td>7.19</td><td>2.05</td></tr><tr><td>GPT-4o-mini</td><td>6.98</td><td>2.11</td><td>7.44</td><td>2.36</td></tr><tr><td>GPT-4-turbo</td><td>8.18</td><td>2.96</td><td>7.92</td><td>2.79</td></tr><tr><td>GPT-4o</td><td>7.90</td><td>2.67</td><td>7.90</td><td>2.67</td></tr><tr><td>LLaMA-8B</td><td>7.24</td><td>1.94</td><td>7.70</td><td>2.49</td></tr><tr><td>LLaMA-8B+BC</td><td>7.81</td><td>3.05</td><td>7.53</td><td>2.78</td></tr><tr><td>LLaMA-8B+BC+Preferred-SFT</td><td>7.76</td><td>3.05</td><td>7.65</td><td>2.88</td></tr><tr><td>LLaMA-8B+BC+DPO (Rafailov et al., 2024)</td><td>7.95</td><td>3.28</td><td>7.80</td><td>2.97</td></tr><tr><td>LLaMA-8B+BC+ETO (Song et al., 2024)</td><td>8.29</td><td>3.39</td><td>8.02</td><td>3.03</td></tr><tr><td>LLaMA-8B+BC+DMPO (Shi et al., 2024)</td><td>8.28</td><td>3.37</td><td>8.00</td><td>2.98</td></tr><tr><td>LLaMA-8B+BC+SDPO (Kong et al., 2025)</td><td>8.56</td><td>3.69</td><td>8.13</td><td>3.16</td></tr><tr><td>LLaMA-8B+BC+ALPO</td><td>8.41</td><td>3.56</td><td>8.21</td><td>3.19</td></tr></table>\n\nThe experimental results demonstrate the effectiveness of ALPO on other application tasks. ALPO employs a progressive optimization strategy transitioning from local to global optima, while utilizing\n\nan indicator function to adaptively determine the participation of each segment in the loss computation. This approach enables effective handling of multi-segment local optimization tasks.\n\n# D THEORY: ADAPTIVE LOCAL PREFERENCE OPTIMIZATION\n\nIn this section, we formalize the local preference optimization problem and validate the effectiveness of ALPO and the limitations of general preference optimization approaches. Note that for the sake of expository convenience, the notations used in this section may differ from those in previous sections.\n\n# D.1 LOCAL PREFERENCE OPTIMIZATION PROBLEM\n\nFor a given input $x \\in \\mathcal { X }$ ( $\\mathcal { X }$ denotes the input space) to the language model $\\pi _ { \\theta }$ , assume it consists of $n$ interrelated segments, i.e., $x = ( x _ { 1 } , \\cdots , x _ { n } )$ . Correspondingly, its output $y \\in \\mathcal { V }$ $\\mathcal { V }$ being the output space) is also composed of $n$ interrelated segments, i.e., $y = \\left( y _ { 1 } , \\cdots , y _ { n } \\right)$ , where $x _ { i }$ and $y _ { i }$ exhibit a one-to-one correspondence. Additionally, the generation of $y _ { i }$ is influenced by $y _ { 1 } , \\cdots , y _ { i - 1 }$ (note that this influence arises not only from the autoregressive nature of language model but also potentially from semantic dependencies between output segments, among other factors), i.e.:\n\n\nConsider a segment-level preference metric $r ( x _ { i } , y _ { i } )$ (a reward signal or other quantitative measure) that evaluates how well the segment output $y _ { i }$ aligns with the optimization objective for its corresponding segment input $x _ { i }$ . The goal of the local preference optimization problem is to adjust the parameters $\\theta$ such that $\\pi _ { \\theta }$ is optimized to maximize $\\dot { \\boldsymbol { r } } ( x _ { i } , y _ { i } )$ . This objective is formally expressed as:\n\n\nwhere $p ( x )$ represents the true distribution of the input $x$ .\n\n# D.2 INEFFECTIVENESS OF OUTCOME-SUPERVISED PREFERENCE OPTIMIZATION METHODS\n\nWe take DPO (Rafailov et al., 2024) as an example to illustrate the limitations of general preference optimization methods when addressing local preference optimization problems. When performing local preference optimization, DPO first annotates preferred responses $y ^ { w } = ( y _ { 1 } ^ { w } , \\cdot \\cdot \\cdot , y _ { n } ^ { w } )$ and non-preferred responses $y ^ { l } = ( y _ { 1 } ^ { l } , \\cdot \\cdot \\cdot , y _ { n } ^ { l } )$ corresponding to $x = ( x _ { 1 } , \\cdots , x _ { n } )$ based on $r ( x _ { i } , y _ { i } )$ . These responses inherently satisfy:\n\n\nThen, DPO optimizes $\\pi _ { \\theta }$ through a single-stage policy optimization approach:\n\n\nwhere $\\pi _ { \\mathrm { r e f } }$ denotes a frozen reference model. For the contrastive terms in the loss function, compute:\n\n\nwhere $y _ { 1 : i - 1 } ^ { w } = ( y _ { 1 } ^ { w } , \\cdot \\cdot \\cdot , y _ { i - 1 } ^ { w } )$ and $y _ { 1 : i - 1 } ^ { l } = ( y _ { 1 } ^ { l } , \\cdot \\cdot \\cdot , y _ { i - 1 } ^ { l } )$ . The limitations of this approach are:\n\n• Lack of Fine-Grained Contrast on Aligned Prefixes: The generation probability of the $i$ -th segment is conditioned on its own preferred or dispreferred prefix $( y _ { 1 : i - 1 } ^ { w }$ or $y _ { 1 : i - 1 } ^ { l } )$ ), rather than a shared prefix. If $y _ { i } ^ { w }$ and $y _ { i } ^ { l }$ exhibit significant divergence in their prefixes (e.g., differing styles or topic branches), the contrast at the $i$ -th segment ceases to be a fair comparison under identical conditions.\n\n• Inability to Apply Training Signals Per Segment: DPO performs a holistic preference judgment over complete sequences $( y ^ { w } , y ^ { l } )$ , depriving the model of segment-level corrective guidance. Specifically, while the model recognizes the global superiority of $y ^ { w }$ over $y ^ { l }$ , it lacks localized feedback to optimize decisions at individual segments.   \n• Gradient Dilution and Noise: When the divergence between $y ^ { w }$ and $y ^ { l }$ occurs only at specific segments, the global contrast aggregates gradients from numerous irrelevant segments (nearly identical in generation). This drowns critical alignment signals, hindering effective identification and correction of misaligned segments.\n\nIn summary, DPO underperforms in local preference optimization tasks due to misaligned generative conditions caused by prefix divergence and the absence of stepwise \"correct vs. incorrect\" decision learning (a limitation shared by other outcome-supervised preference optimization losses).\n\n# D.3 ADAPTIVE LOCAL PREFERENCE OPTIMIZATION\n\nUnlike vanilla DPO, ALPO labels the preferred response $y _ { i } ^ { w }$ and dispreferred response $y _ { i } ^ { l }$ for each segment using $y _ { 1 : i - 1 } ^ { w }$ as the prefix, instead of generating the entire response sequence. During training, the prefix for the $i$ -th segment is fixed as $p _ { i } = ( x , \\hat { y } _ { 1 } , \\cdot \\cdot \\cdot , \\hat { y } _ { i - 1 } )$ , where $\\hat { y } _ { i }$ is obtained through the scheduled prefix mixing strategy. This ensures that comparisons for each segment are made under the same preferred prefix, eliminating unfair competition between “preferred prefix vs. dispreferred prefix”. Under the same prefix, the preference alignment loss contrasts $\\pi _ { \\boldsymbol { \\theta } } ( y _ { i } ^ { w } \\bar { | } \\ p _ { i } )$ and $\\pi _ { \\boldsymbol { \\theta } } ( y _ { i } ^ { l } \\mid p _ { i } )$ conditioned on $p _ { i }$ . Finally, the summation of segment-wise losses imposes preference constraints at each segment:\n\n\nwhere $w ( p _ { i } )$ and $\\beta _ { i }$ are adaptive hyperparameters that determine the contribution of each segment to the optimization process based on specific criteria.\n\nThe reason for using scheduled prefix mixing to obtain $\\hat { y } _ { 1 : i - 1 }$ as the prefix is that, as optimization proceeds, the model tends to generate $y _ { i } ^ { w }$ , making the post-training distribution more consistent with the $y _ { 1 : i - 1 } ^ { w }$ path. Since $y _ { 1 : i - 1 } ^ { w }$ is also the prefix path most likely to be followed, this ensures that the model learns to generate the preferred segment $y _ { i } ^ { w }$ rather than $y _ { i } ^ { l }$ under the prefixes it visits most frequently. This aligns with a common principle in reinforcement learning and preference optimization: the state (or prefix) distribution during training should match the distribution most likely to be visited by the final policy (on-policy) (Christiano et al., 2017; Sutton, 2018; Ouyang et al., 2022). Formally, during training we aim to minimize:\n\n\nwhere $d _ { \\theta ^ { * } }$ denotes the prefix distribution induced by the final model. Since $d _ { \\theta ^ { * } }$ is likely to include preferred prefixes upon convergence, selecting $\\hat { y } _ { 1 : i - 1 }$ as the prefix essentially samples (or directly uses) states from the preferred path, ensuring consistency with the final distribution $d _ { \\theta ^ { * } }$ .\n\n# E DISCUSSION\n\nIn this section, we provide further discussion on the ALPO method.\n\n# E.1 CONCLUSION\n\nThe visual media industry, serving as a pivotal medium for human cultural exchange and dissemination, is witnessing automation and industrialization as critical future trends. Against this backdrop, we propose ALPO, a novel training paradigm for translation models, by employing techniques including LLM-as-a-Judge and preference alignment. Through comprehensive experimentation and theoretical analysis, we validate the effectiveness of ALPO. We believe this study will not only positively contribute to the advancement of visual media technologies but also promote research on domain-specific translation models in other fields.\n\n# E.2 RECOMMENDATIONS\n\nWe would like to offer some developer suggestions for technicians using ALPO:\n\n• When employing LLMs for preference annotation, shuffle the order of translations across multiple inference passes and average the results to obtain robust evaluation outcomes.   \n• The training prompt should utilize the language best suited to the selected backbone, e.g., Chinese prompts for Qwen-series models and English prompts for LLaMA-series models.   \n• Our open-source implementation reveals additional processing details, including handling of terminology translation, subtitle segmentation, and semantic integrity preservation. Refer to the source code for specific implementations.\n\nSubtitle texts are inherently multimodal content closely intertwined with the visual and auditory elements of media programs. Videos provide supplementary information about characters, scenes, and plot progression, while audio conveys emotional and tonal cues. Future research should therefore focus on harnessing these multimodal data resources to enhance subtitle translation, e.g., leveraging video and audio modalities to enable speaker annotation for subtitle lines, and utilizing audiovisual context to facilitate subtitle translation.\n\n# E.3 LIMITATIONS\n\nALPO has the following limitations:\n\n• The visual media data and associated copyrights required for subtitle translation are predominantly concentrated within large enterprises, creating data accessibility barriers.   \n• Similar to other SOTA LLMs, ALPO lacks the capability to integrate multimodal information (e.g., video) during the translation process.\n\n# F PROMPTS AND INSTRUCTIONS\n\nIn this section, we present the input and output formats used for LLM and human evaluators.\n\n# F.1 INPUT AND OUTPUT OF TRANSLATION LLM\n\nWe illustrate the prompt and response format of the $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ translation model below (similar formats apply to other languages). The red text appears only in LLM ICL. The original and translated subtitles of the program are formatted as shown in the text boxes for use in SFT and ALPO model training, as well as other baseline LLM ICL.\n\n# The zh⇒en translation prompt demonstration of translation model.\n\n<INSTRUCTION>\n\nPlease translate the following multiple Chinese movie/TV lines into English according to the following requirements:\n\n1. The English translation should be colloquial, easily understandable, and that the language style is consistent with the Chinese lines.   \n2. The English translation must be expressive and vivid, effectively conveying the atmosphere, emotions, and tone of the original Chinese lines.   \n3. Output the translation with its line number, ensuring that the number of lines in the translation matches the number of lines in the Chinese original, without merging any lines.\n\n<EXAMPLE>\n\nOriginal text:\n\n1.请给你 一点和平相处的时间  \n我2. 们\n\n# 34.奥术在你体内涌动\n\n# 35.请你离海克斯 门远点\n\nTranslation results:\n\n1.Allow us a moment of civility.   \n2.We must talk.\n\n34.The Arcane stirs within you.   \n35.Stay away from the Hexgates.\n\n<TASK>\n\nAccording to the requirements, complete the following translation.\n\nOriginal text:\n\n1.你的病情 都听说了  \n我2. 真的 遗憾 孩子\n\n34.令人 痛的是 越是光芒万丈的人\n\n心35.越会更 地燃 生命的能量\n\nTranslation results:\n\n# The zh⇒en translation response demonstration of SFT model.\n\n1.I heard your prognosis.   \n2.I’m terribly sorry, my boy.\n\n..   \n34.It’s a sad truth that   \n35.those who shine brightest often burn fastest.\n\n# F.2 INPUT OF EVALUATION LLM\n\nWe present the prompt format for LLM evaluation of vividness in $z \\mathrm { h } { \\Rightarrow } { \\in } \\mathrm { n }$ translation below (similar formats apply to other languages). When using LLM to assess the vividness of a line, we provide the contextual lines surrounding it.\n\n# The zh⇒en prompt demonstration of evaluation LLM.\n\n<INSTRUCTION>\n\nPlease assign a vividness score (0-100, integer) to multiple English translations of a Chinese subtitle line. The original Chinese line and its surrounding context will be provided for reference, marked with [To be evaluated] and [Context] respectively. The scoring principles for translation vividness are as follows: Principle 1 (Accuracy): The translation can be moderately liberal but must maintain reasonable accuracy in conveying the original meaning without additions or omissions (Weight:30%);\n\nPrinciple 2 (Colloquial Appropriateness): Evaluate whether the translation uses natural spoken language suitable for the subtitle context and effectively conveys the original emotion, atmosphere, and tone (Weight:30%);\n\nPrinciple 3 (Expressive Power): Assess whether the translation is emotionally resonant, employs vivid phrasing, and has literary qualities that engage the audience (Weight: $40 \\%$ ).\n\nScoring Criteria (Vividness):\n\n100: Exceptionally expressive and impactful translation with rich emotional layers that deeply resonate with the audience.   \n50: Moderately expressive translation with subdued emotional delivery that partially conveys the intended feelings.   \n0: Inaccurate translation lacking emotional depth or expressive\n\nqualities, failing to evoke any audience connection.\n\n<EXAMPLE>\n\nChinese original text:\n\n[Context] 被你到处践踏的遗迹 可是无价之宝\n\n[Context] 是无法用价值来衡量的珍宝\n\n[To be evaluated] 历史虽然会重演 但是人类是无法回到过去的\n\n[Context] 看来你是不会明白的\n\n[Context] 明...明白... 明白了\n\nEnglish translation:\n\nTranslation A: History repeats, but we can’t go back to what was.\n\nTranslation B: History often echoes, yet there’s no way for us to turn back the clock.\n\nEvaluation score:\n\n{\"A\": 70, \"B\": 92, \"C\": 85, ...}\n\n<TASK>\n\nAccording to the criterion, complete the following evaluation. Chinese original text:\n\n[Context] 的贡献注定是转瞬即逝 连您都会 遗\n\n我[Context] 见过的学生 多\n\n我 很[To be evaluated] 令人 痛的是 越是光芒万丈的人 越会更 地燃 生命的能量\n\n心[Context] 不知道你还是个艺术家\n\n我[Context] 的事你不知道的多着呢\n\nEnglish translation:\n\nTranslation A: It’s a sad truth that those who shine brightest often burn fastest.\n\nTranslation B: The heartbreaking truth is that those who shine the brightest tend to burn through their life’s energy all the faster.\n\n...\n\nNote, you need to output the ratings in JSON format: {\"A\": score, \"B\": score, e \"C\": score,\n\nEvaluation score (only output the rating, no other content):\n\n# F.3 INSTRUCTION AND PROMPT FOR QUALITY EVALUATION\n\nFor human evaluation of translation quality, it is essential to provide evaluators with clear instructions specifying the evaluation perspectives, criteria, and format. These instructions directly influence the focus and emphasis of evaluators during the quality assessment process. The instructions provided to evaluators are shown in the first text box, while the prompts used for multidimensional automated evaluation with LLMs are presented in the three subsequent boxes.\n\n# Instruction for human evaluation of translation quality.\n\n# [Evaluation Criteria]\n\n# 1. Accuracy\n\nWhen evaluateing the accuracy of audiovisual subtitle translation, consider the following dimensions:\n\n•Semantic Equivalence: Evaluate whether the meaning of the original subtitle is accurately conveyed in the translated version, and if the semantic content of the source subtitle is precisely\n\nexpressed in the target subtitle.\n\n•Grammatical Correctness: Evaluate the grammatical accuracy of the translated subtitle, including sentence structure, tense, voice, and other grammatical aspects.\n\n•Terminology Translation: Evaluate whether proper nouns have been accurately translated, maintaining the semantics and context of the original terms.\n\n# 2. Naturalness\n\nWhen evaluateing the naturalness of audiovisual subtitle translation, consider the following dimensions:\n\n•Coherence: Evaluate whether the translated subtitle reads as if written by a native speaker of the target language, and evaluate the logical relationships between sentences.\n\n•Readability: Evaluate whether the translated subtitle is easy to read and understand, and if the word choice and expressions conform to the conventions of the target language.\n\n•Fluency: Evaluate whether the translated subtitle flows smoothly, if sentences are well-constructed, and if there are any obvious grammatical errors or unnatural expressions.\n\n# 3. Vividness\n\nWhen evaluateing the vividness of audiovisual subtitle translation, consider the following dimensions:\n\n•Stylistic Consistency: Evaluate whether the translated subtitle maintains the style and characteristics of the original, including consistency in character tone and emotional nuances.\n\n•Expressiveness: Evaluate whether the translation conveys the essence and atmosphere of the original lines, avoiding mechanical literal translation, thereby making it easier for the audience to understand and find engaging.\n\n•Emotion: Assess whether the translation faithfully conveys the character’s emotions, aligns with the scene and character context, and resonates emotionally with the target language audience.\n\n# [Task]\n\nFor each set of original subtitles, two different translations (A and B) are provided. Please refer to the multiple evaluation dimensions specified in the [Evaluation Criteria] to evaluate the two translations for each set of original subtitles. Indicate your evaluation results for translations A and B by marking [A is better], [B is better], or [No significant difference between A and B]. Note that you only need to evaluate the overall performance of each set of subtitles, not each individual line of subtitle.\n\n# The prompt of en⇒zh subtitle translation accuracy evaluation.\n\n[English to Chinese Subtitle Translation Accuracy Evaluation]\n\nPlease rate the accuracy of the following English to Chinese subtitle translation, using integer scores from 0 to 100. The translation accuracy rating criteria include evaluating whether the Chinese translation accurately conveys the original meaning of the English subtitle. Additionally, pay attention to whether the terminology (e.g., names of people, places, organizations, items, etc.) in the English original subtitle is accurately translated in the Chinese translation.\n\n# Scoring Criteria (Accuracy):\n\n100: The translation is completely accurate, covers all information, provides a coherent translation, and the proper nouns are accurately translated.\n\n50: The translation is mostly accurate, with only minor omissions\n\nor unclear context, and proper nouns are partially inaccurately translated, but the overall meaning is still understandable. 0: The translation is severely distorted, misinterprets the main meaning of the original text, and the translation of proper nouns is poor.\n\nOriginal English Line: A crime like this can’t be overlooked. The boy must be punished. A violation of the Ethos calls for banishment, but I can sympathize with a young man’s dream to change the world. · · Yeah, I must admit, his theory intrigues. If dangerous ideas didn’t excite the imagination, we would never wander astray.\n\nChinese Translation:这样严重的罪行不能轻易放过违反社会共识的人确实应该遭到 逐驱但 也能体会一个年轻人梦想改变世界的雄\n\n不 不说 他那套学说非常有意思我 得如果危险的 头不曾 人遐想念 引也 没有误入 途一说了\n\nNote: You need to output in JSON format: {\"Score\": evaluation score} Score (only output the score, no further explanation required):\n\n# The prompt of en⇒zh subtitle translation naturalness evaluation.\n\n[English to Chinese Subtitle Translation Naturalness Evaluation]\n\nPlease rate the naturalness of the following English to Chinese subtitle translation using an integer score from 0 to 100. The naturalness score of the subtitle translation should consider whether the translated text adequately takes into account contextual factors, including cultural background and context, and whether it ensures natural and fluent language expression that conforms to Chinese grammatical structures and word usage habits. It should be easy to understand and close to the culture and expression habits of the Chinese audience.\n\nScoring Criteria (Naturalness): 100: The translation fully considers context and cultural background, with smooth and natural language that aligns with Chinese usage habits, and contains no grammatical or lexical errors.\n\n50: The translation considers the context and is basically fluent, but some expressions may be slightly awkward or unnatural, with potential minor grammatical errors. 0: The translation does not effectively consider the context or culture, is not fluent, has many grammatical errors, is rigid, and difficult to understand.\n\nOriginal English Line: A crime like this can’t be overlooked. The boy must be punished. A violation of the Ethos calls for banishment, but I can sympathize with a young man’s dream to change the world. Yeah, I must admit, his theory intrigues. If dangerous ideas didn’t excite the imagination, we would never wander astray.\n\nChinese Translation:\n\n这样严重的罪行不能轻易放过\n\n违反社会共识的人确实应该遭到 逐\n\n驱但 也能体会一个年轻人梦想改变世界的雄\n\n不 不说 他那套学说非常有意思\n\n我 得如果危险的 头不曾 人遐想\n\n念 引也 没有误入 途一说了\n\nNote: You need to output in JSON format: {\"Score\": evaluation score}\n\nScore (only output the score, no further explanation required):\n\n# The prompt of en⇒zh subtitle translation vividness evaluation.\n\n[English to Chinese Subtitle Translation Vividness Evaluation]\n\nPlease rate the vividness of the English to Chinese subtitle translation below, using an integer score from 0 to 100. The score for translation vividness does not take into account the accuracy of the translation; it only evaluates whether the translation is expressive, emotionally rich, and more capable of engaging the audience.\n\nScoring Criteria (Vividness):\n\n100: The translation is highly expressive and impactful, with rich emotions, capable of strongly moving the audience.\n\n50: The translation has some expressiveness, and the emotional expression is relatively flat but still conveys some emotion.\n\n0: The translation lacks expressiveness and emotion, failing to evoke any emotional resonance from the audience.\n\nOriginal English Line:\n\nA crime like this can’t be overlooked. The boy must be punished. A violation of the Ethos calls for banishment,\n\nbut I can sympathize with a young man’s dream to change the world.\n\nYeah, I must admit, his theory intrigues.\n\nIf dangerous ideas didn’t excite the imagination, we would never wander astray.\n\nChinese Translation:\n\n这样严重的罪行不能轻易放过\n\n违反社会共识的人确实应该遭到 逐\n\n驱但 也能体会一个年轻人梦想改变世界的雄\n\n不 不说 他那套学说非常有意思\n\n我 得如果危险的 头不曾 人遐想\n\n念 引也 没有误入 途一说了\n\nNote: You need to output in JSON format: {\"Score\": evaluation score}\n\nScore (only output the score, no further explanation required):",
      "bbox": null,
      "image_path": null,
      "metadata": {}
    }
  ]
}