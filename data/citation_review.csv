Review_Label,Confidence,Match_Method,Source_ID,Target_ID,Matched_Title,Context_Snippet,Notes
1,0.714,title_fuzzy,1709.02012,1706.02744,"Avoiding discrimination through causal reasoning.
\newblock In \emph{NIPS",,
1,0.667,title_fuzzy,1705.10378,1701.08230,"2017.
\newblock Algorithmic decision making and the cost of fairness.
\newblock {\em arxiv preprint: 1701.08230","As statistical and machine learning models become an increasingly ubiquitous part of our lives, policymakers, regulators, and advocates have expressed concerns about the impact of deployment of such m",
1,0.667,title_fuzzy,1903.12262,1810.03993,"Model cards for model reporting.
\newblock In \emph{FAT*","In comparison to data, the market for oil is heavily regulated throughout its supply and extraction chain. Regulation is driven by the need to reduce transaction friction, prevent security issues, reg",
1,0.562,title_fuzzy,1808.08166,1609.05807,"Inherent trade-offs in the fair determination of risk scores.
\newblock In \emph{8th Innovations in Theoretical Computer Science Conference,
  {ITCS","The most common definitions of fairness in machine learning are statistical in nature.  They proceed by fixing a small number of ``protected subgroups'' (such as racial or gender groups), and then ask || The most common ...",
1,0.667,title_fuzzy,1910.10872,1903.08136,"2019a.
\newblock Debiasing community detection: The importance of lowly-connected
  nodes.
\newblock {\em arXiv preprint arXiv:1903.08136","\end{table} In \citet{mehrabi2019survey}, the authors created a taxonomy on fairness and bias that discusses how researchers have addressed fairness related issues in different fields. From representa",
1,0.625,title_fuzzy,1711.07076,1706.02744,"Avoiding discrimination through causal reasoning.
\newblock \emph{arXiv preprint arXiv:1706.02744","in which the algorithms are developed and meant to operate.  Recent work on identifying proxy discrimination \citep{datta2017proxy} and causal formulations of fairness \citep{nabi2017fair,kilbertus201",
1,0.700,title_fuzzy,1904.03310,1903.10561,"On measuring social biases in sentence encoders.
\newblock \emph{arXiv preprint arXiv:1903.10561","For example, \newcite{ZWYOC18} and \newcite{rudinger-EtAl:2018:N18} show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, \citet",
1,0.615,title_fuzzy,1907.12059,1701.08230,"Algorithmic decision making and the cost of fairness.
\newblock In \emph{Proceedings of the 23rd {ACM SIGKDD","The second approach consists in performing a post-processing of the model outputs \citep{chiappa19path,doherty12information,feldman15computational,hardt16equality,kusner17counterfactual}.  The third a",
?,0.800,title_fuzzy,1703.06856,1610.07524,"Fair prediction with disparate impact: a study of bias in recidivism
  prediction instruments.
\newblock \emph{Big Data","These criteria can be incompatible in general, as discussed in \cite{kleinberg:17, berk:17, chouldechova:17}.  Following the motivation of IF and \cite{johnson2016impartial}, we propose that knowledge || following proble...",version mismatch? context key=chouldechova:17 target=1610.07524(Oct2016) vs 1703.00056(Mar2017) -- same paper different arXiv submissions
1,0.647,title_fuzzy,1703.06856,1607.06520,"Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock In \emph{Advances in Neural Information Processing Systems","fair predictions \cite{hardt2016equality,dwork2012fairness,joseph2016rawlsian,kamishima2011fairness,zliobaite2015survey,zafar2016fairness,zafar2015learning,grgiccase,kleinberg:17,calders2010three,kami",
1,0.800,title_fuzzy,1709.02012,1701.08230,"Algorithmic decision making and the cost of fairness.
\newblock In \emph{KDD",,
1,0.647,title_fuzzy,1805.05859,1607.06520,"Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock In {\em Advances in Neural Information Processing Systems",demonstrated in natural language processing systems \citep{bolukbasi2016man} (where algorithms associate men with technical occupations like `computer programmer' and women with domestic occupations l,context key bolukbasi2016man confirms match
1,0.647,title_fuzzy,1809.01496,1607.06520,"Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock In \emph{Advances in Neural Information Processing Systems","Word embedding models have been designed for representing the meaning of words in a vector space. These models have become a fundamental NLP technique and have been widely used in various applications || Word embedding, ...",
1,0.647,title_fuzzy,2103.11320,1607.06520,"Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock In \emph{Advances in neural information processing systems","\section{Related Work} Work on fairness in NLP has expanded to different applications and domains including coreference resolution \cite{zhao2018gender}, named entity recognition \cite{mehrabi2020man}",
?,0.571,title_fuzzy,1805.05859,1511.00830,"The variational fair autoencoder.
\newblock {\em arXiv preprint arXiv:1511.00830",,low confidence 0.571 (near threshold 0.55); no context to verify
1,0.692,title_fuzzy,1811.03654,1609.05807,"2016.
\newblock Inherent trade-offs in the fair determination of risk scores.
\newblock {\em arXiv preprint arXiv:1609.05807","The above scenario is not a rare case. Given the increasing pervasiveness of automated decision-making systems, there's a growing concern among both computer scientists and the public for how to ensur || \textbf{Calibrat...",
1,0.611,title_fuzzy,1801.07593,1607.06520,"2016.
\newblock Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock In {\em Advances in Neural Information Processing Systems","It is known that word embeddings reflect or amplify problematic biases from the data they are trained on, for example, gender \cite{bolukbasi2016man}. We seek to train a model that can still solve ana",context key bolukbasi2016man confirms match
?,0.750,title_fuzzy,1709.02012,1610.07524,"Fair prediction with disparate impact: A study of bias in recidivism
  prediction instruments.
\newblock \emph{arXiv preprint arXiv:1703.00056",,bib_title explicitly contains arXiv:1703.00056 != target 1610.07524; possibly same paper two submissions; no context to confirm
1,0.636,title_fuzzy,1910.10872,1903.10561,"2019.
\newblock On measuring social biases in sentence encoders.
\newblock {\em arXiv preprint arXiv:1903.10561","\section{Introduction} Machine learning and AI systems are becoming omnipresent in everyday lives. Recently, attention has been directed to problems concerning fairness and algorithmic bias. Some prog || \end{figure} We ...",
1,0.818,title_fuzzy,1805.05859,1609.05807,"Inherent trade-offs in the fair determination of risk scores.
\newblock {\em arXiv preprint:1609.05807",fundamental importance as many of these different criteria can not be satisfied at the same time \cite{kleinberg2016inherent}. || score satisfied calibration and that they did not discriminate. Kleinberg et al. \citep{kl...,context key kleinberg2016inherent + Kleinberg et al. confirms match
