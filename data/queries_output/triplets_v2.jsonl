{"query": "What is the median resolution of the original images in the dataset?", "query_type": "factual", "positive": {"text": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/30eb71dbd58b6ba814e5cc9d8492159182c8ee704ea7b9c6c5705bb9e41364a4.jpg", "metadata": {"caption": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall  @ 1 . ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/34a03eee5f50cfa07b03dd18d8696f24aa5223be030e5e951fe07321e12a46a6.jpg", "negative_type": "random", "metadata": {"caption": "Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall  @ 1 . ", "negative_type": "random"}}, {"text": "Importantly, LLL improves accuracy even when the model is finetuned only on RefCOCO and evaluated directly on POPE  \\mathbf { ( R e f C O C O - p O P E ) } , while NTPonly finetuning reduces accuracy relative to the base model, showing that LLL counteracts grounding degradation and enables cross-task generalization.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/908b76acee6299e852e66c96e13e01a55e4d7ed1262dac7c14f55eeb369861b2.jpg", "negative_type": "random", "metadata": {"caption": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which contextual change label occurs more frequently: Weather-atmosphere or Viewpoint-camera?", "query_type": "comparative", "positive": {"text": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/30eb71dbd58b6ba814e5cc9d8492159182c8ee704ea7b9c6c5705bb9e41364a4.jpg", "metadata": {"caption": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8f5ccd4c030144c7202b7599927545bc9d8bccd92eaad1329e6395ea232eb9dd.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "Figure 3: Distribution of variation types in the image variation dataset. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/9c6ec1937031e091e2186dad2a9714e127a969158ab4e1f5af77d2534670bb5f.jpg", "negative_type": "random", "metadata": {"caption": "Figure 3: Distribution of variation types in the image variation dataset. ", "negative_type": "random"}}, {"text": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "context": "Let [math] , where [math] is the unembedding matrix, [math] , and [math] . Using the standard softmax cross-entropy derivative, \\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9} where [math] is the one-hot vector for token [math] and vector [math] [math] denotes the softmax probability distribution over the vocabulary: chain rule, [math] . By the", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "If there are 2,854 total image pairs and 336 unique originals, what is the average number of variants per original image?", "query_type": "computational", "positive": {"text": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/30eb71dbd58b6ba814e5cc9d8492159182c8ee704ea7b9c6c5705bb9e41364a4.jpg", "metadata": {"caption": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE V: Defense-aware black-box attacks against victim VLMs. We evaluate Ens.,  \\mathrm { A S R } _ { \\mathrm { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } under four preprocessing-based defenses: Bit Reduction, JPEG Compression, ComDefend, and DiffPure. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e4d34d340bba0793995839925b70fd4839d82698d1ffa465b25cb2982bb47853.jpg", "negative_type": "random", "metadata": {"caption": "TABLE V: Defense-aware black-box attacks against victim VLMs. We evaluate Ens.,  \\mathrm { A S R } _ { \\mathrm { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } under four preprocessing-based defenses: Bit Reduction, JPEG Compression, ComDefend, and DiffPure. ", "negative_type": "random"}}, {"text": "\\left. \\nabla _ { h _ { L } ( j ) } L _ { \\mathrm { N T P } } \\right. is capped by  { \\sqrt { 2 } } \\| U \\| for all  j \\in \\mathcal T (ignoring any extra scaling tricks); so it is never exploding. Therefore, the gradient is strongly attenuated, since the attention weights  a _ { j s } are tiny for most visual tokens  s (Esmaeilkhani & Latecki, 2025). Layer-wise visual attention analysis is provided in appendix.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "During generation, at each step  i , the model attends only to tokens at positions  < i and selects the most probable next token from its vocabulary. More precisely, an unembedding matrix  U _ { \\theta } \\in \\mathbb { R } ^ { | \\mathcal { V } | \\times d } maps the latent representation  h _ { L } ( t _ { i - 1 } ) of token  t _ { i - 1 } to a probability distribution over the vocabulary  \\nu for the prediction of next token  t _ { i } , where  h _ { L } ( t _ { i - 1 } ) \\in \\mathbb { R } ^ { d } is obtained by transforming the previously generated token  t _ { i - 1 } through layers 1 to  L of the LLM backbone. So, at each step  i the model predicts  t _ { i } conditioned", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the axis specificity value for the Mood-atmosphere contextual axis?", "query_type": "factual", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8f5ccd4c030144c7202b7599927545bc9d8bccd92eaad1329e6395ea232eb9dd.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE IV: Quantitative performance comparison on commercial black-box VLMs. We compare methods under Standard (  \\epsilon = 8 / 2 5 5 ) ), Unrestricted, and Enhanced  ( \\epsilon = 1 6 / 2 5 5 ) settings. Ens. denotes the Ensemble CLIP Score. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/635f25bfec78454db491c97ca636fb1a442d722a73fa899ee7c5d85b2aaa064a.jpg", "negative_type": "random", "metadata": {"caption": "TABLE IV: Quantitative performance comparison on commercial black-box VLMs. We compare methods under Standard (  \\epsilon = 8 / 2 5 5 ) ), Unrestricted, and Enhanced  ( \\epsilon = 1 6 / 2 5 5 ) settings. Ens. denotes the Ensemble CLIP Score. ", "negative_type": "hard_same_modal"}}, {"text": "Our approach differs from the majority of visual grounding works in that it intrinsically links visual and text tokens by making their deep embeddings more aligned, while works like (Peng et al., 2023; Rasheed et al., 2024; Lai et al., 2024; Wu et al., 2025b) use the LLM predictions (or VLM external modules) to predict object locations as bounding box (BB) coordinates or as segmentation masks. In these works, the LLM backbone of a VLM predicts BB coordinates without directly linking them to actual visual patches containing the target object. So, this can be illustrated as:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 11. Ablation studies on supporting box regression. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/80767085b0feebbfc90081dfb853ceff29cb51034cf3475f4045aba7089d4cac.jpg", "negative_type": "random", "metadata": {"caption": "Table 11. Ablation studies on supporting box regression. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which contextual axis has the highest prompt alignment (Cohen's d) value?", "query_type": "comparative", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8f5ccd4c030144c7202b7599927545bc9d8bccd92eaad1329e6395ea232eb9dd.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 6. Ablation studies on different object token designs. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/c0109746b758d382be093f6db84eef6792741078abc5bdaf8c819351b2919ca3.jpg", "negative_type": "random", "metadata": {"caption": "Table 6. Ablation studies on different object token designs. ", "negative_type": "hard_same_modal"}}, {"text": "where  y _ { i j } = 1 if  \\mathrm { I o U } ( p _ { j } , B ^ { i } ) > 0 . 5 , and 0 otherwise.  M is the number of annotations and  N is the number of proposals.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "context": "For a target vocabulary token [math] and layer-l representation [math] , Logit Lens Loss (LLL) is defined as \\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array} When the queried concept spans multiple tokens (e.g., sub-", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the difference between the highest and lowest axis entropy values shown in the table?", "query_type": "computational", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8f5ccd4c030144c7202b7599927545bc9d8bccd92eaad1329e6395ea232eb9dd.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/91ac1101af0d3cf406b1c84278aa298c3c5da39478b87366ae72b96ef2a3e8fe.jpg", "negative_type": "random", "metadata": {"caption": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "negative_type": "hard_same_modal"}}, {"text": "As illustrated in Fig. 2, we present SGHA-Attack, a transferbased targeted attack framework designed to craft an adversarial example  \\mathbf { \\boldsymbol { x } } _ { \\mathrm { a d v } } by injecting a human-imperceptible perturbation  \\pmb { \\delta } into a clean image  _ { \\textbf { \\em x } } (i.e.,  { \\pmb x } _ { \\mathrm { a d v } } \\ = \\ { \\pmb x } + \\delta subject to  \\| \\pmb { \\delta } \\| _ { \\infty } \\leq \\epsilon ) , aiming to mislead victim VLMs into generating attacker-specified outputs. The pipeline commences by constructing a visually grounded reference pool  \\mathcal { X } _ { r e f } via a frozen Text-to-Image (T2I) model conditioned on the target text  T _ { \\mathrm { t g t } } , from which the Top-  K most semantically relevant instances are adaptively selected as anchors. Subsequently, we perform iterative optimization on a CLIP surrogate, where the perturbation  \\delta is updated via gradient backpropagation to strictly align the hierarchical features of  \\pmb { x } _ { \\mathrm { a d v } } with these weighted anchor references while synchronizing visual and textual tokens across multiple depths, ultimately yielding the final transferable adversarial image. In the following subsections, we systematically elaborate on the three synergistic components that constitute this framework.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2a608ccbf757c46af78127158327e61aea30e521fd6abe118c6f44c82c9e570b.jpg", "negative_type": "random", "metadata": {"caption": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the standard deviation of LAION Aesthetics v2 scores for the Lunara-I dataset?", "query_type": "factual", "positive": {"text": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/91ac1101af0d3cf406b1c84278aa298c3c5da39478b87366ae72b96ef2a3e8fe.jpg", "metadata": {"caption": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 7. Ablation studies on instructions. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/8644dc897bcd0933b4eac5a976db0cd98ae4b4a1d13f7841fb708b2bc0cd6ac6.jpg", "negative_type": "random", "metadata": {"caption": "Table 7. Ablation studies on instructions. ", "negative_type": "random"}}, {"text": "Ablation studies on object token design. As objects are sensitive to localization quality, an object embedding model should have the ability to assess the quality of boxes. To achieve the goal, we change classification labels in sigmoid focal loss to box IoUs, which increases  5 . 2 \\% mAP on COCO, as shown in Table 6. However, the classification and box localization may have conflicting training objectives. We find that decoupling an object into two tokens, one for classification and the other for IoU regression, further improves  3 . 2 \\% mAP on COCO. And placing the classification token in front of the IoU token is slightly better. Since REC places less emphasis on precise localization, its performance is relatively robust to the choice of IoU design.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "context": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which dataset has the highest mean LAION Aesthetics v2 score and which has the lowest?", "query_type": "comparative", "positive": {"text": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/91ac1101af0d3cf406b1c84278aa298c3c5da39478b87366ae72b96ef2a3e8fe.jpg", "metadata": {"caption": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/9353ed44b779c874f584c841c70214b802d067abd8c3143b6ce9850589c7b2b6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "tion from visual content without arousing suspicion. For instance, in Figure 5(a), GPT-4o incorrectly describes a monkey as a “baseball player” preparing to pitch. Similarly, Gemini-2.0 describes a box as a person “launching a kite”, and Claude-3.5 perceives a “cat sitting” near a window. These examples confirm that our method can induce targeted descriptions while preserving the visual realism of the original images.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "These metrics assess whether the correct target images are ranked highly in the retrieval results, without considering bounding box localization accuracy. In our framework, we compute the overall image similarity by taking the maximum matching score among all objects. As shown in Table 4, we compare ObjEmbed with other global embedding models that represent the entire image as a single, holistic feature vector. However, such global representations suffer from semantic ambiguity and fail to capture fine-grained details, leading to poor performance in local retrieval tasks. Even FG-CLIP2 (Xie et al., 2025a), which incorporates regional contrastive learning, underperforms significantly in T2I retrieval when using object embeddings extracted by RoIAlign with the same object proposals and scoring strategy as ours. We hypothesize that this is due to misalignment between region features and complex queries. In contrast, our model represents each object with fine-grained details and gets an average score of 68.5, surpassing other models by around 20 points. For the image-based retrieval (I2I) task, we use global image embeddings to represent image exemplars and to retrieve target images. Surprisingly, although our model does not optimize for image-based retrieval tasks, the model aligns global text embeddings and global image embeddings in a unified semantic space. Therefore, it can transfer from text-based retrieval tasks to image-based retrieval tasks successfully.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the difference between the 95th percentile and 5th percentile scores for the CC3M dataset?", "query_type": "computational", "positive": {"text": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/91ac1101af0d3cf406b1c84278aa298c3c5da39478b87366ae72b96ef2a3e8fe.jpg", "metadata": {"caption": "Table 3: Full distribution statistics of LAION Aesthetics v2 scores. P05 and P95 denote the 5th and 95th percentiles, respectively. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 7. Ablation studies on instructions. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/8644dc897bcd0933b4eac5a976db0cd98ae4b4a1d13f7841fb708b2bc0cd6ac6.jpg", "negative_type": "random", "metadata": {"caption": "Table 7. Ablation studies on instructions. ", "negative_type": "random"}}, {"text": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/7b8d345cae457d8ff8d0352846a2217174c7af5ee47880a9a7a217ecdf38c555.jpg", "negative_type": "random", "metadata": {"caption": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "negative_type": "hard_same_modal"}}, {"text": "(c) NTP + LLL  Figure 5. Attention maps of the last output token with respect to visual tokens under different settings. Queried object is “Cake”. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/2856382f0af857ad6defaf68b96ec6fd0c9d5b678dee48e5e593340cf4daea86.jpg", "negative_type": "random", "metadata": {"caption": "(c) NTP + LLL  Figure 5. Attention maps of the last output token with respect to visual tokens under different settings. Queried object is “Cake”. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the percentage of correct target attribute realization for Weather when the Identity Stability is 4.68?", "query_type": "factual", "positive": {"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/1318302589467126045a52f86557a19358e73cb6d536448347f7168445a1aaf3.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "LLL to NTP leads to a threefold increase in object confidence score ratio. This significant result confirms the huge impact on LLL on the object confidence maps shown in Fig. 2. Moreover, the fact that NTP did not improve the ratio confirms the fact that NTP provides only a weak supervisory signal for visual cues during fine-tuning. The results were obtained after finetuning on POPE.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "2) Beyond success rates, our method consistently achieves the highest Ensemble CLIP Scores, supporting the generality of the learned hierarchical semantic alignment. For instance, under the Enhanced setting for GPT-4o, we reach an Ensemble Score of 0.6098, surpassing M-Attack (0.5714). This confirms that our hierarchical alignment ensures the target semantics are robustly recognized across diverse visual encoders, rather than overfitting to a specific surrogate.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which target attribute has the highest percentage of correct realization: Color or Mood?", "query_type": "comparative", "positive": {"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "negative_type": "random", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}, {"text": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e32d6a90817b410428eebd4faee60ee22f19f87074a780afc0245db94fa102ae.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "negative_type": "random"}}, {"text": "In this section, we propose three mechanisms to optimize HippoRAG 2’s retrieval on a knowledge graph: Symbolic Anchoring, Query-Aware Dynamic Edge Weighting and Key-Fact Passage Weight Enhancement, also present in Figure1.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the difference between the highest and lowest target attribute realization percentages?", "query_type": "computational", "positive": {"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/908b76acee6299e852e66c96e13e01a55e4d7ed1262dac7c14f55eeb369861b2.jpg", "negative_type": "random", "metadata": {"caption": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "negative_type": "hard_same_modal"}}, {"text": "Assessing box quality via IoU embeddings. To equip the model with location awareness, we explicitly model object location by predicting an IoU score for each detected object. We empirically find that using a single token to jointly learn location and classification leads to optimization conflicts. To mitigate this, we introduce a dedicated special token ⟨iou⟩ to represent the quality of each bounding box. This token immediately follows the corresponding object token, forming the structured sequence: “Object i: ⟨object⟩⟨iou⟩.” The final IoU score is computed by applying a linear head to the IoU embedding and then multiplying it by the classification score to produce the final object matching score.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "AttackVLM [13] shows that transfer-based targeted attacks can be driven by either text–image matching or image–image", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of architectural structure is prominently featured in the center of this landscape image?", "query_type": "identification", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/95cdf8052a61aad6d0c7185f2958289b0f8d50a0507fb4913dc15871502e9540.jpg", "metadata": {"caption": "", "negative_type": "random"}}, "negatives": [{"text": "(f) Change of weather and viewpoint camera  Figure 4: Contextual variations in original photographs. Each pair shows (a) and (b). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/19d5764f6dd344bbf2f17247a09e205722897eae87115599a4e775c6ad851684.jpg", "negative_type": "random", "metadata": {"caption": "(f) Change of weather and viewpoint camera  Figure 4: Contextual variations in original photographs. Each pair shows (a) and (b). ", "negative_type": "random"}}, {"text": "Independent researchers evaluate Lunara Aesthetic II at the level of an anchor-linked variation set (an anchor image and its associated contextual variants). Evaluation focuses on two dimensions of contextual consistency: Identity Stability, which measures whether identity is preserved across variants, and Target Attribute Realization, which measures whether the intended contextual change is clearly expressed.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "This modulation is asymmetric, applied only to forward edges originating from the seed set. By suppressing irrelevant edges and amplifying critical ones, we actively steer the PPR propagation, ensuring the traversal tunnels through the graph along the query’s intent rather than diffusing into topological sinks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Describe the atmospheric conditions and lighting depicted in this scene, including the sky and overall mood.", "query_type": "descriptive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/95cdf8052a61aad6d0c7185f2958289b0f8d50a0507fb4913dc15871502e9540.jpg", "metadata": {"caption": "", "negative_type": "random"}}, "negatives": [{"text": "(e) Query: ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/b956ea3a579e33871d20b057c2ba58c6b975698dcfaeb56150e983e2621f1d1d.jpg", "negative_type": "random", "metadata": {"caption": "(e) Query: ", "negative_type": "random"}}, {"text": "Strict Reasoning Completeness Evaluation. While standard metrics indicate general relevance, they often mask a critical failure mode in multi-hop retrieval: the loss of intermediate \"bridge\" documents that connect disjoint facts. To assess the recovery of the full evidence paths, we evaluate FCR and JSR in Table 4. CatRAG effectively mitigates probability dilution, achieving an FCR of  34 . 6 \\% compared to  3 0 . 5 \\% for HippoRAG 2. The gain is most pronounced on HoVer, where precise 3–4 hop claim verification is required. CatRAG improves JSR to  3 1 . 1 \\% , a relative gain of  1 8 . 7 \\% over the HippoRAG2. These results confirm that our dynamic steering successfully anchors the traversal to the specific bridge documents required for grounded reasoning.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "context": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the integration of the wooden building within the natural forest setting suggest about the relationship between human habitation and the natural environment?", "query_type": "interpretive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/95cdf8052a61aad6d0c7185f2958289b0f8d50a0507fb4913dc15871502e9540.jpg", "metadata": {"caption": "", "negative_type": "random"}}, "negatives": [{"text": "Figure on page 7", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/93563440ca172d385ea20919327b3156b26481b07b860d936db27a779e6811e3.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "Compared to the state-of-the-art static baseline, HippoRAG 2 across all benchmarks, CatRAG raises Recall  \\textcircled { a } 5 to  8 9 . 5 \\% on HotpotQA and  7 6 . 8 \\% on HoVer. This retrieval quality directly translates to downstream performance, where CatRAG yields the highest F1 scores across all datasets (e.g.,  4 5 . 0 \\% on MuSiQue), validating that queryconditional edge weighting surfaces relevant evidence without disrupting structural integrity.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "T ARGE Vision-Language Models [1] have emerged as L a foundational technology, powering critical applications from autonomous agents to content moderation [2]– [4]. As these systems are increasingly entrusted with highstakes decision-making [5], [6], their inherent vulnerability to adversarial perturbations poses severe security risks [7]. Beyond causing benign classification errors [8], imperceptible perturbations can effectively hijack a model’s semantic understanding, enabling adversaries to circumvent safety guardrails (i.e., jailbreaking) and elicit policy-violating outputs [9]. In the black-box scenario, targeted transfer-based attacks [10] represent a particularly practical threat. By optimizing on an accessible surrogate, attackers can precisely steer proprietary", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What architectural elements and lighting features are visible in the castle structure depicted in this nighttime scene?", "query_type": "descriptive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/d8de03bd2fe4be2558ba6a5f3154949f3bc9dbdd2bfbe525cb6b71b270a6f722.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 7", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/fc9464cc44e2d69cdba797b0271ca1a7952827cfbcc5948f4881938fdb113e1a.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/1f08b9dbf2064eb2c65438489a4ffed9793bc3d7c8e3b947503513e33d48fec2.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "\\hat { \\mathbf { F } } _ { a n c } ^ { l } ( \\mathbf { f } _ { c l s } ^ { l } , \\hat { \\mathbf { f } } _ { c l s } ^ { l } ) \\in \\mathbb { R } ^ { D } and spatial tokensch tokens to obtain  ( \\bar { \\mathbf { F } } _ { s p a } ^ { l } , \\hat { \\mathbf { F } } _ { s p a } ^ { l } ) \\in \\mathbb { R } ^ { N \\times D } a single spatial descriptor per layer:  \\begin{array} { r } { \\overline { { \\mathbf { F } } } _ { s p a } ^ { l } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\mathbf { F } _ { s p a } ^ { l } [ i ] } \\end{array} (and similarly for  \\overline { { \\hat { \\mathbf { F } } } } _ { s p a } ^ { l } ) . Mean pooling provides a compact spatial summary without requiring strict patch-to-patch correspondence, making the alignment more robust to differences in patch size and common preprocessing (e.g., resizing or cropping) across models. The HVSA loss is then defined as:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of celestial objects can be identified in the sky above the illuminated castle?", "query_type": "identification", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/d8de03bd2fe4be2558ba6a5f3154949f3bc9dbdd2bfbe525cb6b71b270a6f722.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/c77dda22663b04389dc142b987ab48fd696f5e21239cbe88f41398e546e79271.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "We propose to use the  L _ { \\mathrm { L L L L } } as the auxiliary loss in addition to the next token prediction loss  L _ { \\mathrm { N T P } } ( \\theta ) of the VLM to optimize the VLM’s parameters for both objectives:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "The same VLM was used to identify contextual differences between each original–variation pair, followed by manual verification and refinement to ensure that visual contextual variations were accurately captured in the final labels. Finally, These variation outputs were manually filtered to retain 2.8K images exhibiting meaningful contextual changes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the contrast between the dark stormy clouds and the brightly lit castle below contribute to the overall mood and atmosphere of this scene?", "query_type": "interpretive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/d8de03bd2fe4be2558ba6a5f3154949f3bc9dbdd2bfbe525cb6b71b270a6f722.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/42262ab9b0dddc38f445e39af17ac02008f7ea5622b0f7a4d0f6e91f037d3472.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, {"text": "Fig. 6: Impact of hyperparameter  K and  \\tau settings on attack performance evaluated on UniDiffuser. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/d92c9b1919e800c2ae3dd7d26f96ee9a70401e7f18fa09e56a715a84987b3fc1.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 6: Impact of hyperparameter  K and  \\tau settings on attack performance evaluated on UniDiffuser. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What visual elements are prominently displayed in the center of this image?", "query_type": "descriptive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/b74569adbd31fcdaba8ae8b639c3343b2de79a0cf8523979ce1b05f2527f42c0.jpg", "metadata": {"caption": ""}}, "negatives": [{"text": "Figure 2. Logit Lens object confidence maps produced by Qwen2.5-VL-7B. First column: input image with the queried object text; second: base model; third: NTP-finetuned; fourth: NTP+LLL-finetuned. Compared to the base and NTP models, LLL produces sharply localized, object-aligned Logit Lens maps by preserving patch-level semantics in visual tokens despite cross-modal mixing in the LLM. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/abf8f08587ec45a13a0ea5fd3962790fe6ae24cd2739d7b4a8f0f5d1fa75b880.jpg", "negative_type": "random", "metadata": {"caption": "Figure 2. Logit Lens object confidence maps produced by Qwen2.5-VL-7B. First column: input image with the queried object text; second: base model; third: NTP-finetuned; fourth: NTP+LLL-finetuned. Compared to the base and NTP models, LLL produces sharply localized, object-aligned Logit Lens maps by preserving patch-level semantics in visual tokens despite cross-modal mixing in the LLM. ", "negative_type": "hard_same_modal"}}, {"text": "LLL applies the loss directly to visual tokens, while NTP applies loss directly only at text positions, Minimizing  L _ { \\mathrm { L L L } } applies cross-entropy through the unembedding matrix to visual tokens, whose gradient explicitly pulls the patch embedding  h _ { L } ( s ) toward the unembedding vector of the target word  v , thereby enforcing semantic alignment in the same space the LLM uses to generate language.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/eb4f9b51f221cacb01b5fd0359f7bd1957707779bd2c00667245e5e4c84dbd89.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the name of the company or brand shown in this logo design?", "query_type": "identification", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/b74569adbd31fcdaba8ae8b639c3343b2de79a0cf8523979ce1b05f2527f42c0.jpg", "metadata": {"caption": ""}}, "negatives": [{"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a30e7f526e4c349069796b721d190b4b999a613d34cd1ecee346284ea52fc89d.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "The model must output the corresponding  ( x , y ) coordinates of the referred object’s center in the pixel space.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Target：a baseball player gets ready to throw a pitch. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0267226f5909578a6dd9935fabf5fe38c7e14d3202f52279e842b413e916242c.jpg", "negative_type": "random", "metadata": {"caption": "Target：a baseball player gets ready to throw a pitch. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the combination of the lunar eclipse symbol and Asian-inspired landscape suggest about the company's focus or market positioning?", "query_type": "interpretive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/b74569adbd31fcdaba8ae8b639c3343b2de79a0cf8523979ce1b05f2527f42c0.jpg", "metadata": {"caption": ""}}, "negatives": [{"text": "Figure 2: Conditional probability of variations co-occurring. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/c05a50cb395df4816dce13891472d9033490527ec8d7a73154acc9be11df3749.jpg", "negative_type": "random", "metadata": {"caption": "Figure 2: Conditional probability of variations co-occurring. ", "negative_type": "hard_same_modal"}}, {"text": "Table 3 reports full distribution statistics of LAION Aesthetics v2 scores across datasets. Lunara achieves the highest mean aesthetic score (6.32) and the largest proportion of images exceeding the commonly used threshold of 6.5 (33.99%), substantially outperforming CC3M, WIT, and LAION-2B-Aesthetic. Lunara-AIV exhibits a lower mean score (5.91) and a smaller high-aesthetic tail (5.56%), reflecting its focus on controlled variation rather than aesthetic maximization. These results indicate that controlled contextual variation does not inherently degrade perceptual quality and that the Lunara dataset maintains strong aesthetic properties relative to widely used benchmarks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 2. Performance comparison on the POPE based on Popular sampling from MSCOCO images. Higher accuracy indicates fewer hallucinated object predictions. RefCOCO → POPE denotes zeroshot transfer: models are trained only on RefCOCO and evaluated on POPE.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the main landscape features visible in this winter scene?", "query_type": "descriptive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/19138e423fe9d1655543c61b1a2f64b39dd92a6244fd7d551fae5b42215edc96.jpg", "metadata": {"caption": ""}}, "negatives": [{"text": "Fig. 5: Visualization of targeted adversarial attacks on commercial black-box VLMs. We display the adversarial image, the target text, and the actual response generated by the model. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/3b0a69b3b706acc82cfdb21c8a38c51c63b0765b4843a6d4543724742624525a.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 5: Visualization of targeted adversarial attacks on commercial black-box VLMs. We display the adversarial image, the target text, and the actual response generated by the model. ", "negative_type": "random"}}, {"text": "targeted attacks. We conduct experiments across six representative open-source VLMs ranging from 1.4B to 72.3B parameters, including UniDiffuser, BLIP-2, InstructBLIP, MiniGPT-4, LLaVA, and LLaVA-NeXT. To ensure a consistent evaluation setting, we employ a unified prompt: “What is the content of the image?” during the inference phase.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "layer, ignoring intermediate features, this can cause overfitting to the surrogate and weaken targeted transfer to instructiontuned or proprietary VLMs.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of geographical environment does this image represent?", "query_type": "identification", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/19138e423fe9d1655543c61b1a2f64b39dd92a6244fd7d551fae5b42215edc96.jpg", "metadata": {"caption": ""}}, "negatives": [{"text": "Figure 4. Bar plots illustrating the ratio of the average attention of target tokens within the bounding box of the target object w.r.t. the answer token to the average attention across all visual tokens. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c7076a1399e9363f345fd51225e393b1c9f524c62886506d12e1b1d1002e4d95.jpg", "negative_type": "random", "metadata": {"caption": "Figure 4. Bar plots illustrating the ratio of the average attention of target tokens within the bounding box of the target object w.r.t. the answer token to the average attention across all visual tokens. ", "negative_type": "hard_same_modal"}}, {"text": "Table 3 reports full distribution statistics of LAION Aesthetics v2 scores across datasets. Lunara achieves the highest mean aesthetic score (6.32) and the largest proportion of images exceeding the commonly used threshold of 6.5 (33.99%), substantially outperforming CC3M, WIT, and LAION-2B-Aesthetic. Lunara-AIV exhibits a lower mean score (5.91) and a smaller high-aesthetic tail (5.56%), reflecting its focus on controlled variation rather than aesthetic maximization. These results indicate that controlled contextual variation does not inherently degrade perceptual quality and that the Lunara dataset maintains strong aesthetic properties relative to widely used benchmarks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 1. The overview of ObjEmbed training dataset. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/120ffad156187adf11f4108680e45a1cf7ee3c11605469ca52e8372d40e6daa1.jpg", "negative_type": "random", "metadata": {"caption": "Table 1. The overview of ObjEmbed training dataset. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the atmospheric perspective in this image contribute to creating a sense of depth and distance in the mountainous landscape?", "query_type": "interpretive", "positive": {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/19138e423fe9d1655543c61b1a2f64b39dd92a6244fd7d551fae5b42215edc96.jpg", "metadata": {"caption": ""}}, "negatives": [{"text": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/eb4f9b51f221cacb01b5fd0359f7bd1957707779bd2c00667245e5e4c84dbd89.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "negative_type": "hard_same_modal"}}, {"text": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination. Code is available at https: //github.com/WeChatCV/ObjEmbed.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "ing set, ObjEmbed achieves an overall score of 81.7 points, which is highly competitive with both traditional CLIPstyle models and recent LMM-based embedding approaches. Moreover, it demonstrates robust generalization across varying text lengths and multiple languages.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How many anchor-linked variation pairs does the Lunara Aesthetic II dataset contain?", "query_type": "factual", "positive": {"text": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara’s signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https:// huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "Early research on VLM robustness primarily focused on untargeted degradation [38]–[42]. Recently, significant progress has been made in targeted transfer-based attacks. Representative works like AttackVLM [13] craft targeted examples by aligning global image embeddings with a specific target reference. Subsequent methods such as Chain-of-Attack (COA) [14] improve semantic consistency through iterative optimization and modality fusion, while IPGA [43] exploits intermediate projector representations (e.g., Q-Former) for finegrained alignment. Others, including AdvDiffVLM [16] and M-Attack [15], further enhance transferability via diffusionbased generation or robust input transformations. However, these methods predominantly operate on the encoder’s output or the subsequent projection stage, treating the visual backbone itself as a monolithic feature extractor. This strategy overlooks the rich hierarchical representations within the visual encoder, where features evolve from low-level structures to high-level semantics. In contrast, our work explicitly opens the visual backbone and synchronizes its intermediate hierarchical features with corresponding textual representations, thereby enforcing deep semantic consistency throughout the entire feature extraction process.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Lunara-I exhibits the highest overall aesthetic quality, with the largest mean score (6.32) and a pronounced upper tail, as reflected by its high median (6.31) and 95th percentile (7.18). This distribution is consistent with strong aesthetic adherence during dataset construction, favoring visually striking and artistically composed imagery.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/42262ab9b0dddc38f445e39af17ac02008f7ea5622b0f7a4d0f6e91f037d3472.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the primary purpose of using identity-preserving contextual variation as a supervision signal in this dataset?", "query_type": "conceptual", "positive": {"text": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara’s signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https:// huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "searches. CatRAG instead introduces a \"one-shot\" context-aware graph modification that dynamically re-weights edges before traversal. Unlike iterative cycles, our approach maintains the efficiency of a single retrieval pass, effectively combining the reasoning precision of adaptive methods with the speed and structural integrity of graph-based retrieval.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Anchor Settings  K and  \\tau ). We investigate the influence of the anchor count  K and temperature  \\tau on the UniDiffuser. As shown in Fig. 6, introducing even a single anchor  K = 1 ) boosts the Ensemble CLIP Score from the baseline 0.6970 to 0.7418. The performance peaks at  K = 5 but degrades slightly with larger  K , as excessive anchors introduce less relevant anchors. Regarding  \\tau , extreme values lead to suboptimal weighting: small  \\tau yields an overly sharp distribution that underutilizes the semantic diversity of the anchor set, while large  \\tau produces an overly uniform distribution, failing to emphasize the most semantically relevant anchors. The results find  \\tau = 5 as the optimal balance. Thus, we adopt  K = 5 and  \\tau = 5 as the default settings.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We do not intend to teach VLMs new concepts but to preserve their localized image understanding, and consequently, make Logit Lens visualisation useful in VLM explainability. We focus on finetuning, even though the proposed LLL could be applied in pre-training, it is beyond the scope of this paper.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the dataset's design and intended applications, what can be inferred about the current limitations in existing image generation systems that this dataset aims to address?", "query_type": "inferential", "positive": {"text": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara’s signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https:// huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "In the directed graph setting, a seed entity node  u \\in V _ { E } may connect to multiple passage nodes  V _ { P } via context edges. We aim to bias the walk towards passages containing \"Key Facts\"—fact triplets that were explicitly identified and filtered during the filtering Recognition memory filtering proposed in HippoRAG 2.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/95cdf8052a61aad6d0c7185f2958289b0f8d50a0507fb4913dc15871502e9540.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which specific companies and models are mentioned as examples of frontier text-to-image systems?", "query_type": "factual", "positive": {"text": "Recent frontier text-to-image systems, including OpenAI’s GPT-4o (OpenAI (2025a,b)) and Google’s Gemini image models (Google (2025)), exhibit substantial advances in instruction following, compositional reasoning, and visual consistency. The outputs of such systems, however, are typically governed by restrictive usage terms that prohibit their use for training or developing competing models. While recent open-source efforts such as Z-image (Team (2025b)) and Qwen-Image/Edit (Wu et al. (2025)) represent important", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "As a pioneering MLLM-based object embedding model, ObjEmbed can be further improved in the following directions:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "context": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "negative_type": "random"}}, {"text": "using the top-5 retrieved passages. Key hyperparameters include: symbolic anchor reset probability  \\epsilon = 0 . 2 (weighted by inverse passage count  | P _ { i } | ^ { - 1 } ) , boost factor  \\beta \\ : = \\ : 2 . 5 , dynamic weighting limits  N _ { s e e d } = 5 and  K _ { e d g e } \\mathrm { ~ = ~ } 1 5 . More implementation details and hyperparameters are provided in Appendix A.1.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the fundamental tension between the capabilities of frontier text-to-image systems and their accessibility for model development?", "query_type": "conceptual", "positive": {"text": "Recent frontier text-to-image systems, including OpenAI’s GPT-4o (OpenAI (2025a,b)) and Google’s Gemini image models (Google (2025)), exhibit substantial advances in instruction following, compositional reasoning, and visual consistency. The outputs of such systems, however, are typically governed by restrictive usage terms that prohibit their use for training or developing competing models. While recent open-source efforts such as Z-image (Team (2025b)) and Qwen-Image/Edit (Wu et al. (2025)) represent important", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "Region-level contrastive learning. Unlike traditional contrastive language-image pre-training, where image-caption pairs are one-to-one matched, an object description may correspond to multiple instances, while some proposals remain unmatched due to missing annotations or low-quality bounding boxes. To handle this partial and many-to-one matching, we employ sigmoid focal loss (Lin et al., 2017) for supervi-", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "(b) Change of viewpoint camera  ^ + color-tone ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/f44d3cb0ffeb104d7d6b293e92f6ea87cb0117a999252a7191af76aa64c908aa.jpg", "negative_type": "random", "metadata": {"caption": "(b) Change of viewpoint camera  ^ + color-tone ", "negative_type": "random"}}, {"text": "From an efficiency perspective, SGAI introduces limited overhead. Anchor features  \\phi ( \\pmb { x } _ { a n c } ^ { k } ) can be computed once and detached, and the iterative optimization only requires computing  \\phi ( { \\pmb x } _ { a d v } ) each iteration. We report the runtime in Table VII.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the contrast presented between proprietary and open-source systems, what challenge does this suggest for researchers trying to build upon state-of-the-art text-to-image technology?", "query_type": "inferential", "positive": {"text": "Recent frontier text-to-image systems, including OpenAI’s GPT-4o (OpenAI (2025a,b)) and Google’s Gemini image models (Google (2025)), exhibit substantial advances in instruction following, compositional reasoning, and visual consistency. The outputs of such systems, however, are typically governed by restrictive usage terms that prohibit their use for training or developing competing models. While recent open-source efforts such as Z-image (Team (2025b)) and Qwen-Image/Edit (Wu et al. (2025)) represent important", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "ILIAS (Kordopatis-Zilos et al., 2025) supports both text-based and image-based local retrieval, with queries formulated as a natural language description or a large image exemplar. The original dataset includes 5 million distractors, making full-scale evaluation computationally infeasible. Instead, we construct a manageable gallery using all 4,715 positive images. Different from the benchmarks mentioned above, ILIAS contains only 1,232 queries, each potentially matching multiple positive images. We evaluate using mAP@50, which computes mean average precision over the top 50 retrieved results.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "imposes two limitations. First, edge relevance is treated as context-independent. Consider the query: “Which university did Marie Curie’s doctoral advisor attend?” This requires a precise two-step traversal: Marie Curie Gabriel Lippmann (Advisor) École Normale Supérieure (University). Yet, in a static graph, generic edges like Marie Curie Radioactivity often possess dominant weights. Consequently, the random walk often suffers from semantic drift: it effectively retrieves the initial entity but is statistically diverted into irrelevant clusters before reaching the second-hop evidence. This results in a common failure mode where retrieval metrics (like Recall) appear high due to partial matches, yet the reasoning chain is broken. Second, traversal is susceptible to the \"hub node\" problem, where high-degree entities (e.g., Nobel Prize, French) act as semantic sinks, disproportionately diluting the probability mass and causing the retrieval to drift into irrelevant documents.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We show that the model finetuned with  \\mathrm { N T P + L L L } on Ref-COCO is able to improve performance on a new localization task on a new set of images. We constructed a subset of 1,500 images from PixMo-Points (Deitke et al., 2025) for evaluation. We randomly sample from 150 object categories with the highest occurrence frequency. We stress that the images in PixMo-Points dataset have no known overlap with MS COCO. Each sample provides a short textual prompt and a human-annotated point marking the object’s center.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the size and composition of the Lunara Aesthetic II dataset?", "query_type": "factual", "positive": {"text": "progress, the field continues to lack high-quality, high-aesthetic training data at sufficient signal density. As such, we publicly release Lunara Aesthetic II, a highly curated dataset comprising 2.8K anchor-linked variation sets generated from original artwork, photographs, and the Moonworks Lunara model. The dataset is explicitly designed to operationalize identity-preserving contextual variation, enabling image generation and editing systems to be trained and evaluated on genuine contextual generalization rather than surface-level visual memorization.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "Index Terms—Adversarial attack, Cross-modal synchronization, Hierarchical alignment, Vision-language models.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "ObjEmbed is finetuned from Qwen3-VL-Instruct (Bai et al., 2025a) by first initializing the object projector following WeDetect-Ref (Fu et al., 2025a) and then training under the objective described in Equation (4). The model is trained using 16 GPUs, with a batch size of 2 images per GPU, and an initial learning rate of  2 e ^ { - 5 } . All parameters are updated during training except those in the frozen vision encoder. Training proceeds for two epochs. The loss weights  \\lambda _ { 1 } ,  \\lambda _ { 2 } , and  \\lambda _ { 3 } are set to 1.0, 1.0 and 0.25. Input images are resized such that the number of visual tokens ranges from 900 to 1200, corresponding to  9 0 0 ^ { * } 3 2 ^ { * } 3 2 to  1 2 0 0 ^ { * } 3 2 ^ { * } 3 2 pixels, ensuring adaptive computation based on image content.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "context": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the core design principle behind identity-preserving contextual variation in the dataset?", "query_type": "conceptual", "positive": {"text": "progress, the field continues to lack high-quality, high-aesthetic training data at sufficient signal density. As such, we publicly release Lunara Aesthetic II, a highly curated dataset comprising 2.8K anchor-linked variation sets generated from original artwork, photographs, and the Moonworks Lunara model. The dataset is explicitly designed to operationalize identity-preserving contextual variation, enabling image generation and editing systems to be trained and evaluated on genuine contextual generalization rather than surface-level visual memorization.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "Rather than treating NER as an alternative retrieval path, we utilize extracted entities as strictly auxiliary topological anchors. We extract a set of entities and inject them as weak seed, assigning reset probabilities for retrieval. We assign these symbolic anchors with small reset probabilities  \\epsilon to ensure that their influence is subordinate to the initial entity from contextual triples.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "In a future release, Moonworks will introduce the Lunara model, a novel diffusion mixture architecture trained with a new learning algorithm and used in the construction of both Lunara-I and Lunara-II.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(b) Change of illumination time ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/a30f887e5446dc31bc564675c2d80be69c5a6031881864966d49d1ee3f1914e8.jpg", "negative_type": "random", "metadata": {"caption": "(b) Change of illumination time ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why might the distinction between contextual generalization and surface-level visual memorization be important for advancing image generation systems?", "query_type": "inferential", "positive": {"text": "progress, the field continues to lack high-quality, high-aesthetic training data at sufficient signal density. As such, we publicly release Lunara Aesthetic II, a highly curated dataset comprising 2.8K anchor-linked variation sets generated from original artwork, photographs, and the Moonworks Lunara model. The dataset is explicitly designed to operationalize identity-preserving contextual variation, enabling image generation and editing systems to be trained and evaluated on genuine contextual generalization rather than surface-level visual memorization.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "The dataset spans six topic categories: Nature and Landscape comprising 36.7% (1,048 images), City, Building, and Architecture 30.1% (858), Everyday Objects and Daily Life 16.0% (457), Spiritual and Heritage Sites 7.5% (213), Work, Hobby, and Transport 4.6% (131), and Portraits and Human Figures 3.5% (101).", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "The contextual variations are generated using Moonworks Lunara, a sub-10B parameter image model employing a novel diffusion mixture architecture trained on similarly structured variation data. Lunara Aesthetic II is organized into explicit classes of visual context change, enabling systematic evaluation of model behavior under controlled transformations. This structure provides substantially stronger learning and evaluation signals than conventional i.i.d. datasets, particularly for assessing identity preservation, attribute controllability, and edit robustness. Both automated metrics and human evaluation validate the quality of the dataset, yielding high identity stability (4.65/5), accurate attribute realization (87.2% on average), and strong aesthetic performance (mean score 5.9). Consistent with prior Lunara release (Wang et al. (2026)), Lunara Aesthetic II is publicly released under the Apache 2.0 license to facilitate reproducible research and broad commercial adoption.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Our approach builds on the idea that every deep embedding  h _ { l } ( x ) within the LLM implicitly encodes a distribution over vocabulary tokens via the Logit Lens projection  U _ { \\theta } h _ { l } ( x ) . By treating this distribution as the “semantic fingerprint” of a patch, we define a loss that encourages visual tokens corresponding to object regions to produce vocabulary distributions consistent with their ground-truth labels (e.g., high probability for the word “cat” inside a cat region, and low elsewhere). Meanwhile, the loss discourages patches unrelated to a given object category from developing unrelated connections with that category. In essence, it drives each patch to focus more on objects present within its region and less on those absent. This yields a simple yet effective auxiliary objective, termed Logit Lens Loss, that complements the standard NTP loss by grounding visual embeddings directly in the vocabulary space.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the two fundamental model capabilities that Lunara Aesthetic II extends toward while maintaining high aesthetic quality?", "query_type": "factual", "positive": {"text": "Following the release of Lunara Aesthetic I (Wang et al. (2026)), which emphasizes aesthetic quality and artistic capabilities, Lunara Aesthetic II extends this work toward fundamental model capabilities such as contextual consistency and semantic transformation, while maintaining high aesthetic quality. Lunara Aesthetic II is derived from the original Moonworks image collection and consists entirely of real-world photographs and art contributed with explicit consent for research/commercial use, without personally identifiable information. This supports model development while adhering to ethical data collection and privacy considerations, and demonstrates the viability of ethical, high-signal data construction.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "To qualitatively measure the impact of LLL on object localization, we consider the ratio of the average object bounding box Logit Lens score to the average score over the whole image as also used in (Choe et al., 2020; Jung & Oh, 2021). We call it the object confidence ratio.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "context": "For a target vocabulary token [math] and layer-l representation [math] , Logit Lens Loss (LLL) is defined as \\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array} When the queried concept spans multiple tokens (e.g., sub-", "negative_type": "random"}}, {"text": "source-image characteristics rather than the variation procedure itself. In a small number of cases, the anchor images contain inherent ambiguity or reduced visual fidelity—such as low resolution, slight motion blur from handheld capture, or sketch-like/handwritten content with unclear boundaries. These factors can make the underlying identity less sharply defined and may lead to identity drift across variations. We retain these groups to reflect realistic acquisition conditions, but note that they may be less suitable for evaluations requiring strict pixel-level or fine-grained identity preservation.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the approach to data collection described for Lunara Aesthetic II demonstrate the relationship between ethical considerations and model development effectiveness?", "query_type": "conceptual", "positive": {"text": "Following the release of Lunara Aesthetic I (Wang et al. (2026)), which emphasizes aesthetic quality and artistic capabilities, Lunara Aesthetic II extends this work toward fundamental model capabilities such as contextual consistency and semantic transformation, while maintaining high aesthetic quality. Lunara Aesthetic II is derived from the original Moonworks image collection and consists entirely of real-world photographs and art contributed with explicit consent for research/commercial use, without personally identifiable information. This supports model development while adhering to ethical data collection and privacy considerations, and demonstrates the viability of ethical, high-signal data construction.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "Lunara Aesthetic II is intentionally released separately from the Lunara Aesthetics Dataset Wang et al. (2026). While the Aesthetics dataset targets stylistic conditioning and aesthetic control through broad prompt and artistic diversity, Lunara Aesthetic II focuses on contextual variation, controlled semantic transformation, and identity preservation under change. This separation clarifies intended use and evaluation: improvements on Lunara primarily reflect advances in style modeling, whereas improvements on Lunara Aesthetic II more directly measure contextual consistency and transformation behavior while maintaining high visual quality.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "We evaluate CatRAG across multiple multi-hop benchmarks. Results demonstrate that while our approach yields consistent gains in standard retrieval metrics, it achieves a significant breakthrough in reasoning sufficiency. CatRAG substantially improves Full Chain Retrieval—the ability to retrieve complete evidence chains—confirming that dynamic graph steering effectively mitigates semantic drift where static baselines fail.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "context": "For a target vocabulary token [math] and layer-l representation [math] , Logit Lens Loss (LLL) is defined as \\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array} When the queried concept spans multiple tokens (e.g., sub-", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the progression from Lunara Aesthetic I to Lunara Aesthetic II, what can be inferred about the research priorities and development strategy of this project?", "query_type": "inferential", "positive": {"text": "Following the release of Lunara Aesthetic I (Wang et al. (2026)), which emphasizes aesthetic quality and artistic capabilities, Lunara Aesthetic II extends this work toward fundamental model capabilities such as contextual consistency and semantic transformation, while maintaining high aesthetic quality. Lunara Aesthetic II is derived from the original Moonworks image collection and consists entirely of real-world photographs and art contributed with explicit consent for research/commercial use, without personally identifiable information. This supports model development while adhering to ethical data collection and privacy considerations, and demonstrates the viability of ethical, high-signal data construction.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "Quantitative Attack Performance. We first conduct a quantitative comparison to assess the attack effectiveness. As reported in Table II, our method demonstrates superior performance across all metrics.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "As our confidence maps demonstrate in Fig. 2, VLMs only partially focus on the correct image patches corresponding", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 2. Performance comparison on the POPE based on Popular sampling from MSCOCO images. Higher accuracy indicates fewer hallucinated object predictions. RefCOCO → POPE denotes zeroshot transfer: models are trained only on RefCOCO and evaluated on POPE.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How many passages are in the MuSiQue dataset?", "query_type": "factual", "positive": {"text": "Table 1: Dataset statistics. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/4ce5a2da7b6431dd6df5a1e868faa1e2dfa60bb501afebadebd4df61fca3a2ed.jpg", "metadata": {"caption": "Table 1: Dataset statistics. ", "negative_type": "random"}}, "negatives": [{"text": "Table 11. Ablation studies on supporting box regression. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/80767085b0feebbfc90081dfb853ceff29cb51034cf3475f4045aba7089d4cac.jpg", "negative_type": "random", "metadata": {"caption": "Table 11. Ablation studies on supporting box regression. ", "negative_type": "hard_same_modal"}}, {"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, {"text": "Visual Quality Performance. We evaluate the visual quality and targeted effectiveness of the generated adversarial examples through both quantitative metrics reported in Table III and qualitative visualizations presented in Fig. 3 and Fig. 4.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which dataset has the smallest number of passages?", "query_type": "comparative", "positive": {"text": "Table 1: Dataset statistics. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/4ce5a2da7b6431dd6df5a1e868faa1e2dfa60bb501afebadebd4df61fca3a2ed.jpg", "metadata": {"caption": "Table 1: Dataset statistics. ", "negative_type": "random"}}, "negatives": [{"text": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/dd6b9ac6a9a67b9f0c40d35b345feeb0ef3e178cb7ed5ab19250580d4daa9849.jpg", "negative_type": "random", "metadata": {"caption": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "negative_type": "hard_same_modal"}}, {"text": "matching on a surrogate model. For text–image matching, we align the surrogate embedding of the adversarial image with the target text embedding. Let  \\phi ( \\cdot ) and  \\psi ( \\cdot ) denote the surrogate image and text encoders, respectively, and define cosine distance as  \\mathcal { D } ( \\mathbf { a } , \\mathbf { b } ) ~ = ~ 1 - \\cos ( \\mathbf { a } , \\mathbf { b } ) . Here,  \\phi ( \\cdot ) produces a global image embedding used for the following matching objectives. A standard objective is", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We analyze the conditional co-occurrence structure of contextual variation labels in the dataset (see Figure 2). Substantial off-diagonal mass indicates non-trivial dependencies between change types. Illumination–time emerges as a central factor, exhibiting consistently high conditional probabilities with mood–atmosphere, object–composition, viewpoint–camera, and weather–atmosphere changes (approximately 0.53–0.59), suggesting that lighting variations frequently accompany other contextual edits. Scene-level and atmospheric categories show moderate coupling with one another (roughly 0.33–0.48), reflecting coordinated broad-level modifications. Color–tone displays selective coupling—most notably with illumination–time ( 0.49), while remaining more weakly associated with other categories. Viewpoint–camera changes exhibit moderate associations with object–composition and weather–atmosphere, indicating that geometric adjustments often co-occur with broader scene alterations. Overall, these patterns reveal structured, non-uniform dependencies among contextual variations, highlighting the compositional nature of the dataset.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the total number of passages across all four datasets combined?", "query_type": "computational", "positive": {"text": "Table 1: Dataset statistics. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/4ce5a2da7b6431dd6df5a1e868faa1e2dfa60bb501afebadebd4df61fca3a2ed.jpg", "metadata": {"caption": "Table 1: Dataset statistics. ", "negative_type": "random"}}, "negatives": [{"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, {"text": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/d19f22bd6f4ce2c5a6deb364b1a420116fcadd34bf212a2e60c49172e50f1448.jpg", "negative_type": "random", "metadata": {"caption": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "negative_type": "random"}}, {"text": "Evaluating the semantic relevance of every edge using an LLM is computationally prohibitive. Therefore, we first apply a topological filter to constrain the search space to the most plausible local neighborhoods. We define two hyperparameters: the maximum number of seed entities  N _ { s e e d } and the maximum number of edges per seed  K _ { e d g e } for finegrained alignment. First, we select the top-  . N _ { s e e d } entity nodes based on their initial reset probabilities (derived from the dense retrieval alignment). Let  u be such a selected seed. For the seed phrase  u within top-  . N _ { s e e d } , if the number of outgoing relation edges exceeds a threshold  K _ { e d g e } , we prune its outgoing edges by prioritizing the top-  K _ { e d g e } neighbors based on the vector similarity between the query embedding and fact embeddings of relation edges. Neighbors  v \\not \\in \\mathcal { N } _ { t o p } ( u ) are bypassed by the scoring module and assigned a minimal Weak weight. This step acts as a low-pass structural filter, discarding statistically improbable paths before the intensive semantic scoring.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the Recall@5 score for CatRAG on the HotpotQA dataset?", "query_type": "factual", "positive": {"text": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/6ad2f7076f02a0f4d64eb9596150789143764f481ce4816594fa8b61211e1341.jpg", "metadata": {"caption": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "negative_type": "random"}}, "negatives": [{"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "negative_type": "random", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}, {"text": "Target：A cat sitting between a window and blinds. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e9e1ca3aef8e42df8bcbf059d58c917208b88824d24dbd0a742f31d61be854aa.jpg", "negative_type": "random", "metadata": {"caption": "Target：A cat sitting between a window and blinds. ", "negative_type": "random"}}, {"text": "We adopt the Personalized PageRank (PPR) algorithm to model the retrieval process. The probability distribution over nodes at step  k is updated as:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method performs better on the 2Wiki dataset: RAPTOR or text-embedding-3-small?", "query_type": "comparative", "positive": {"text": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/6ad2f7076f02a0f4d64eb9596150789143764f481ce4816594fa8b61211e1341.jpg", "metadata": {"caption": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "negative_type": "random"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "image. (3) Global image retrieval: Since ObjEmbed also retains global image embeddings, they can be directly used for standard image-text retrieval tasks, ensuring compatibility with conventional benchmarks and applications.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(e) Change of viewpoint-camera and weather ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/82ae7dbd09c4b97c839bccd4a200fe1cca2528ba47cf5c14b068f65dbf25e258.jpg", "negative_type": "random", "metadata": {"caption": "(e) Change of viewpoint-camera and weather ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the average Recall@5 score for HippoRAG 2 across all four datasets?", "query_type": "computational", "positive": {"text": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/6ad2f7076f02a0f4d64eb9596150789143764f481ce4816594fa8b61211e1341.jpg", "metadata": {"caption": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "negative_type": "random"}}, "negatives": [{"text": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4e1634418e9d611556004f04860f67e8a5360f67d8cff842bead42c27ae133c6.jpg", "negative_type": "random", "metadata": {"caption": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "negative_type": "random"}}, {"text": "Visualizations of referring expression comprehension results. In addition to strong performance on standard referring benchmarks (e.g., RefCOCO, RefCOCO+, RefCOCOg), in Figure 3, we demonstrate that ObjEmbed exhibits not only strong OCR capabilities (a, b, d), but also commonsense reasoning (c) and image-image matching abilities (e, f), highlighting its generalizability and versatility.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Local image retrieval is a challenging task in which the textual or visual query corresponds to only a small region or specific object within an image, rather than the entire scene. In this work, we evaluate on three established benchmarks:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the performance score of CatRAG on the MuSiQue dataset?", "query_type": "factual", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/1318302589467126045a52f86557a19358e73cb6d536448347f7168445a1aaf3.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "T ARGE Vision-Language Models [1] have emerged as L a foundational technology, powering critical applications from autonomous agents to content moderation [2]– [4]. As these systems are increasingly entrusted with highstakes decision-making [5], [6], their inherent vulnerability to adversarial perturbations poses severe security risks [7]. Beyond causing benign classification errors [8], imperceptible perturbations can effectively hijack a model’s semantic understanding, enabling adversaries to circumvent safety guardrails (i.e., jailbreaking) and elicit policy-violating outputs [9]. In the black-box scenario, targeted transfer-based attacks [10] represent a particularly practical threat. By optimizing on an accessible surrogate, attackers can precisely steer proprietary", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "commercial APIs to generate specific target captions without knowledge of the victim’s internal parameters. This transferability threatens the integrity of downstream ecosystems, including automated decision support [11] and tool-augmented agentic systems [12]. To unveil the severity of this threat, we introduce SGHA-Attack, as illustrated in Fig. 1, a novel framework designed to effectively mislead diverse black-box VLMs to generate attacker-specified target captions.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method performs best on the 2Wiki dataset, and how does it compare to the worst performing method on the same dataset?", "query_type": "comparative", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/1318302589467126045a52f86557a19358e73cb6d536448347f7168445a1aaf3.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 4: Reasoning Completeness Evaluation. We evaluation the Full Chain Retrieval (FCR) and Joint Success Rate (JSR) on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/36f7e779793ed3f4786a8804036f42ea2e74c57dac798b2dbabc12cbe5469205.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Reasoning Completeness Evaluation. We evaluation the Full Chain Retrieval (FCR) and Joint Success Rate (JSR) on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results. ", "negative_type": "hard_same_modal"}}, {"text": "\"hub\" nodes that have high similarity but lack precise relevance to the query. To mitigate this, we introduce Symbolic Anchoring, a regularization strategy that grounds the stochastic walk using explicit query constraints.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/16fffc87cdc5af791add394b2c563d5244a87d2baeeb536590792eba6a44c274.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the average performance of HippoRAG 2 across all four datasets (MuSiQue, 2Wiki, HotpotQA, and HoVer)?", "query_type": "computational", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/1318302589467126045a52f86557a19358e73cb6d536448347f7168445a1aaf3.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8f5ccd4c030144c7202b7599927545bc9d8bccd92eaad1329e6395ea232eb9dd.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "(c) Query: The car's license plate in HAWAII ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/3bf77e210ce9cc4f4e9f7616d469eb00f0a3cfc6b1ee24970eea6382100aeaf0.jpg", "negative_type": "random", "metadata": {"caption": "(c) Query: The car's license plate in HAWAII ", "negative_type": "random"}}, {"text": "(d) Change of scene-composition and color tone ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/f04cd0317f5af82ae55647b4ba577a2055c73867cbfd86c4cecfcbf5f49bbc6d.jpg", "negative_type": "random", "metadata": {"caption": "(d) Change of scene-composition and color tone ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the three key mechanisms that CatRAG uses to prevent semantic drift during graph traversal?", "query_type": "identification", "positive": {"text": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/415b8f34a76d2c3e8943c0a8e01fc6eda9e47ee2a264b555f97b1a9e624e44fa.jpg", "metadata": {"caption": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "(c) Change of weather and scene-composition ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8191e7ebd6af32ffc97c399e2914e994457665d8256a23d07989ca9300bf6056.jpg", "negative_type": "random", "metadata": {"caption": "(c) Change of weather and scene-composition ", "negative_type": "hard_same_modal"}}, {"text": "To address these divergent requirements and mitigate potential task conflicts, we introduce task-specific instructions to guide the model in generating context-aware embeddings tailored to each task. The instructions used during training are listed as follows:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/16fffc87cdc5af791add394b2c563d5244a87d2baeeb536590792eba6a44c274.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the retrieval process differ between HippoRAG 2 and CatRAG when processing the multi-hop query about Marie Curie's doctoral advisor?", "query_type": "descriptive", "positive": {"text": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/415b8f34a76d2c3e8943c0a8e01fc6eda9e47ee2a264b555f97b1a9e624e44fa.jpg", "metadata": {"caption": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "(b) Change of illumination time ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/a30f887e5446dc31bc564675c2d80be69c5a6031881864966d49d1ee3f1914e8.jpg", "negative_type": "random", "metadata": {"caption": "(b) Change of illumination time ", "negative_type": "random"}}, {"text": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/dd6b9ac6a9a67b9f0c40d35b345feeb0ef3e178cb7ed5ab19250580d4daa9849.jpg", "negative_type": "random", "metadata": {"caption": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "negative_type": "hard_same_modal"}}, {"text": "Output ONE line per neighbor: ‘ID (Entity Name): Score | (Reasoning if Score  > = ~ 4 \\AA _ { e } )‘", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why does HippoRAG 2's static graph structure lead to missing the downstream evidence for ENS, and how does CatRAG's approach solve this problem?", "query_type": "interpretive", "positive": {"text": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/415b8f34a76d2c3e8943c0a8e01fc6eda9e47ee2a264b555f97b1a9e624e44fa.jpg", "metadata": {"caption": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/e44ceccc00f64484610e3c3299292db91889128d682b9b6f0fcf2dbaea7b1c63.jpg", "negative_type": "random", "metadata": {"caption": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "negative_type": "hard_same_modal"}}, {"text": "(adv) The image features a horse with a jockey riding it, possibly in a race or training session. The horse is wearing a saddle and is positioned on a track ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0224c8f3eb0f6b66aee7d39967126471d99c670cb6134afd9739109ad6912070.jpg", "negative_type": "random", "metadata": {"caption": "(adv) The image features a horse with a jockey riding it, possibly in a race or training session. The horse is wearing a saddle and is positioned on a track ", "negative_type": "random"}}, {"text": "Let  \\mathcal { P } denote the set of all visual (patch) tokens in the image and  \\mathcal { P } ^ { \\prime } \\subseteq \\mathcal { P } the subset of patches inside the ground-truth region (e.g., bounding box) of the queried object (when present). In other words, we obtain  \\mathcal { P } ^ { \\prime } by projecting the ground-truth bounding box onto the visual-token grid and selecting tokens whose receptive fields overlap with the annotated object region.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the leftward shift of the CatRAG distribution compared to HippoRAG 2 indicate about hub node retrieval?", "query_type": "interpretive", "positive": {"text": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/d19f22bd6f4ce2c5a6deb364b1a420116fcadd34bf212a2e60c49172e50f1448.jpg", "metadata": {"caption": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "negative_type": "random"}}, "negatives": [{"text": "Figure 5. Visualizations of self-annotated data. Each image is annotated with high-quality image-level and object-level captions. Images come from SA-1B (Kirillov et al., 2023). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/aa06a8d0fdeea94ef5b0eb312fcb805b6c19c8f1ce2ef9c5cfc8f5379654daab.jpg", "negative_type": "random", "metadata": {"caption": "Figure 5. Visualizations of self-annotated data. Each image is annotated with high-quality image-level and object-level captions. Images come from SA-1B (Kirillov et al., 2023). ", "negative_type": "hard_same_modal"}}, {"text": "To train ObjEmbed, each image in the training dataset is annotated with a long caption, a short caption, and several regions of interest, where each region is associated with a corresponding object description. To minimize false-negative conflicts during contrastive learning, captions are designed to be as diverse and distinctive as possible. Furthermore, they should exclude subjective or interpretive content to ensure objectivity and consistency. To achieve high-quality annotations, we carefully design structured prompts and employ a state-of-the-art multimodal large language model, Qwen-VL-235B (Bai et al., 2025a), as the automated annotator. The annotation prompts are as follows:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "In this section, we propose three mechanisms to optimize HippoRAG 2’s retrieval on a knowledge graph: Symbolic Anchoring, Query-Aware Dynamic Edge Weighting and Key-Fact Passage Weight Enhancement, also present in Figure1.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method shows a higher mean PPR-Weighted Strength based on the dashed vertical lines in the figure?", "query_type": "identification", "positive": {"text": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/d19f22bd6f4ce2c5a6deb364b1a420116fcadd34bf212a2e60c49172e50f1448.jpg", "metadata": {"caption": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "negative_type": "random"}}, "negatives": [{"text": "(a) Out-of-the-box ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/6f937087b3419008a975d61482b091b1353652a4984895117f41d99f3384e2ab.jpg", "negative_type": "random", "metadata": {"caption": "(a) Out-of-the-box ", "negative_type": "random"}}, {"text": "(b) Change of viewpoint camera  ^ + color-tone ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/f44d3cb0ffeb104d7d6b293e92f6ea87cb0117a999252a7191af76aa64c908aa.jpg", "negative_type": "random", "metadata": {"caption": "(b) Change of viewpoint camera  ^ + color-tone ", "negative_type": "random"}}, {"text": "Ablation studies on image-level supervision design. In this work, we use both local and global text embeddings and long and short captions as global text embeddings to learn global image embeddings. We study their effects in Table 9. As local text embeddings match with object embeddings while global text embeddings match with global image embeddings, we find that not sharing the text tokens can mitigate task discrepancies and get higher performance on COCO  ( + 0 . 3 \\% mAP) and RefCOCO  ( + 0 . 8 ) , comparing Exp1 and Exp2. Further, using only short captions, long captions, or a mixture of them can not handle complex retrieval requirements. Using both captions for supervision achieves the highest 80.6 global retrieval results (Exp2-Exp5). Finally, using two global image tokens (Exp6), one for short captions and one for long captions, gets the highest results. And we find that the performance on object detection and REC is quite robust to the image-level designs.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How do the probability density distributions of PPR-Weighted Node Strength compare between the two methods shown in the figure?", "query_type": "descriptive", "positive": {"text": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/d19f22bd6f4ce2c5a6deb364b1a420116fcadd34bf212a2e60c49172e50f1448.jpg", "metadata": {"caption": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "negative_type": "random"}}, "negatives": [{"text": "(adv) a bathroom with tiled walls, toilet, sink and bathtub ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/98d6d3be8ab944961e82e1cd018069f191225d1ec39fbc100bec2294543c7717.jpg", "negative_type": "random", "metadata": {"caption": "(adv) a bathroom with tiled walls, toilet, sink and bathtub ", "negative_type": "hard_same_modal"}}, {"text": "Visualizations of self-annotated data. Figure 5 shows representative examples from our self-annotated dataset. Thanks to carefully designed prompts and the use of frontier MLLMs, the generated captions are both accurate and highly distinctive.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\left\\| \\nabla_ {h _ {L} (s)} L _ {\\mathrm {N T P}} \\right\\| \\leq C _ {\\text {a t t}} \\sum_ {j \\in \\mathcal {T}} \\left\\| \\nabla_ {h _ {L} (j)} L _ {\\mathrm {N T P}} \\right\\| a _ {j s}. \\tag {13}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\left\\| \\nabla_ {h _ {L} (s)} L _ {\\mathrm {N T P}} \\right\\| \\leq C _ {\\text {a t t}} \\sum_ {j \\in \\mathcal {T}} \\left\\| \\nabla_ {h _ {L} (j)} L _ {\\mathrm {N T P}} \\right\\| a _ {j s}. \\tag {13}", "context": "\\left\\| \\nabla_ {h _ {L} (s)} L _ {\\mathrm {N T P}} \\right\\| \\leq C _ {\\text {a t t}} \\sum_ {j \\in \\mathcal {T}} \\left\\| \\nabla_ {h _ {L} (j)} L _ {\\mathrm {N T P}} \\right\\| a _ {j s}. \\tag {13}", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does this formula calculate and what type of iterative process does it represent?", "query_type": "semantic", "positive": {"text": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "context": "We adopt the Personalized PageRank (PPR) algorithm to model the retrieval process. The probability distribution over nodes at step [math] is updated as: \\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1} where [math] is the personalized probability distribution over seed nodes, and T is the row-normalized transition matrix. In the standard framework, T is static. Our work focuses on dynamically refining T into a query-specific transition matrix [math] to better capture the reasoning requirements of the user query."}}, "negatives": [{"text": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "context": "matching on a surrogate model. For text–image matching, we align the surrogate embedding of the adversarial image with the target text embedding. Let [math] and [math] denote the surrogate image and text encoders, respectively, and define cosine distance as [math] . Here, [math] produces a global image embedding used for the following matching objectives. A standard objective is \\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1} For image–image matching, the adversarial image is guided toward a reference image by matching their surrogate embeddings, which provides more visually grounded supervision. However, existing methods [13]–[16] often rely on a single reference instance, leading to narrow guidance: one image may capture only one appearance of the target concept, and the optimization can become sensitive to that particular target. This sensitivity makes targeted transfer less reliable when the victim model differs in backbone, alignment module, or processing pipeline.", "negative_type": "hard_same_modal"}}, {"text": "(ori) a herd of goats grazing on a rocky hillside ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/d700751fbe00d151ce79aeae7139503bb7007df866147bef003fd9760c4af177.jpg", "negative_type": "random", "metadata": {"caption": "(ori) a herd of goats grazing on a rocky hillside ", "negative_type": "hard_same_modal"}}, {"text": "The dataset annotations cover a diverse set of contextual change labels (Table 1 and Figure 3). Illumination–time changes are the most frequent, accounting for 49.68% of", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the variables d, e_s, and T represent in this formula, and what are their likely interpretations in the context of graph algorithms?", "query_type": "variable", "positive": {"text": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "context": "We adopt the Personalized PageRank (PPR) algorithm to model the retrieval process. The probability distribution over nodes at step [math] is updated as: \\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1} where [math] is the personalized probability distribution over seed nodes, and T is the row-normalized transition matrix. In the standard framework, T is static. Our work focuses on dynamically refining T into a query-specific transition matrix [math] to better capture the reasoning requirements of the user query."}}, "negatives": [{"text": "L _ {\\text {L L L}} (\\theta) = \\frac {1}{\\left| \\mathcal {P} ^ {\\prime} \\right|} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} \\big (<   \\operatorname {c a t} > | h _ {l} (s) \\big). \\tag {5}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\text {L L L}} (\\theta) = \\frac {1}{\\left| \\mathcal {P} ^ {\\prime} \\right|} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} \\big (<   \\operatorname {c a t} > | h _ {l} (s) \\big). \\tag {5}", "context": "L _ {\\text {L L L}} (\\theta) = \\frac {1}{\\left| \\mathcal {P} ^ {\\prime} \\right|} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} \\big (<   \\operatorname {c a t} > | h _ {l} (s) \\big). \\tag {5}", "negative_type": "hard_same_modal"}}, {"text": "To address these divergent requirements and mitigate potential task conflicts, we introduce task-specific instructions to guide the model in generating context-aware embeddings tailored to each task. The instructions used during training are listed as follows:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We aim to address this issue by constraining the mixing of visual and language tokens in order to prevent", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you implement this iterative formula computationally, and what would be the termination condition for the iteration process?", "query_type": "application", "positive": {"text": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "context": "We adopt the Personalized PageRank (PPR) algorithm to model the retrieval process. The probability distribution over nodes at step [math] is updated as: \\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1} where [math] is the personalized probability distribution over seed nodes, and T is the row-normalized transition matrix. In the standard framework, T is static. Our work focuses on dynamically refining T into a query-specific transition matrix [math] to better capture the reasoning requirements of the user query."}}, "negatives": [{"text": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "context": "Both terms in Eq. (6) are nicely aligned since both are based on increasing the probabilities of generating selected vocabulary tokens, as illustrated in Fig. 1. Formally, since both visual and textual embeddings can be projected into probability distributions over the vocabulary via the Logit Lens, they share a common semantic space. We denote this shared space with [math] of all probability distribution functions (pdfs) [math] over the vocabulary tokens [math] , i.e., \\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7} Let [math] be defined as [math] following the process in Sec. 2.2. Space [math] is a common space to link the visual and language tokens, since [math] and [math] for all visual tokens [math] and text tokens [math]", "negative_type": "hard_same_modal"}}, {"text": "aware dynamic edge weighting requires run-time LLM inference to assess semantic relevance, which incurs additional computational overhead and latency compared to purely static graph traversals. Although we mitigate this via coarse-grained pruning, the approach remains more computationally intensive than standard dense retrieval. Furthermore, our experimental evaluation intentionally utilized standard embedding models (text-embedding-3-small) rather than larger, state-of-the-art embedding models to strictly isolate the topological gains provided by our framework from the raw semantic capacity of the encoder. Consequently, while our results demonstrate the superiority of dynamic traversal, the absolute performance ceiling of CatRAG could potentially be further elevated by integrating these larger foundational models in future work. Due to proprietary data protection policies, the full source code cannot be publicly released. To mitigate this, we have provided full hyperparameter tables in to facilitate reimplementation.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "This modulation is asymmetric, applied only to forward edges originating from the seed set. By suppressing irrelevant edges and amplifying critical ones, we actively steer the PPR propagation, ensuring the traversal tunnels through the graph along the query’s intent rather than diffusing into topological sinks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the function C(v) calculate and how does it decide between two different processing methods?", "query_type": "semantic", "positive": {"text": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "context": "the context content [math] as: C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2} where [math] is a density threshold. For informationdense nodes [math] , we generate a concise summary; for sparse nodes, we use raw triples. This hybrid approach balances context completeness with token efficiency.", "negative_type": "random"}}, "negatives": [{"text": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "context": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "negative_type": "hard_same_modal"}}, {"text": "The dataset’s moderate scale is also intentional. While larger datasets are essential for training frontier-scale models, they often limit interpretability and make it difficult to reason about what a model learned from any particular subset. Lunara Aesthetic II instead targets focused analysis and benchmarking, where the objective is not to saturate performance through scale, but to enable careful study of contextual behavior under controlled conditions. This makes the this release of dataset well suited for ablations, fine-tuning and adaptation studies, and evaluation of editing pipelines that must preserve identity while applying contextual changes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We integrate Symbolic Anchoring, Dynamic Edge Weighting, and Passage Enhancement to construct a query-adapted graph. Standard PPR (Eq. 1) is executed on this refined structure. The resulting stationary distribution of PPR provides the final passage ranking, prioritizing nodes reachable via semantically relevant reasoning paths.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the threshold parameter τ (tau) represent in this formula, and how does it influence the choice between Summary and Concat operations?", "query_type": "variable", "positive": {"text": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "context": "the context content [math] as: C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2} where [math] is a density threshold. For informationdense nodes [math] , we generate a concise summary; for sparse nodes, we use raw triples. This hybrid approach balances context completeness with token efficiency.", "negative_type": "random"}}, "negatives": [{"text": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "context": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "negative_type": "hard_same_modal"}}, {"text": "To mitigate the constraints imposed by static graph topologies, we develop CatRAG (Context-Aware Traversal for robust RAG). This framework extends the HippoRAG 2 (Gutiérrez et al., 2025) paradigm by integrating a novel optimization layer tailored for context-driven navigation. First, we introduce Symbolic Anchoring. By injecting explicitly recognized entities as weak topological anchors, we constrain the starting distribution to prevent immediate drift into generic hubs. Second, we introduce Query-Aware Dynamic Edge Weighting. By employing an LLM to assess the relevance of outgoing edges from seed entities, we dynamically modulate the graph edge weight, effectively pruning irrelevant paths while amplifying those aligned with the query’s intent. Third, we propose Key-Fact Passage Weight Enhancement, a costefficient method to structurally anchor the random walk to documents containing verified evidentiary triples. It guides the random walk to documents that provide distinct evidence, rather than those containing only superficial mentions of seed entities.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "targeted attacks. We conduct experiments across six representative open-source VLMs ranging from 1.4B to 72.3B parameters, including UniDiffuser, BLIP-2, InstructBLIP, MiniGPT-4, LLaVA, and LLaVA-NeXT. To ensure a consistent evaluation setting, we employ a unified prompt: “What is the content of the image?” during the inference phase.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Given a node v where |ℱ(v)| = 150 and τ = 100, which operation would be applied to compute C(v), and what would be the input to that operation?", "query_type": "application", "positive": {"text": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "context": "the context content [math] as: C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2} where [math] is a density threshold. For informationdense nodes [math] , we generate a concise summary; for sparse nodes, we use raw triples. This hybrid approach balances context completeness with token efficiency.", "negative_type": "random"}}, "negatives": [{"text": "\\hat {w} _ {u p} = w _ {u p} \\cdot (1 + \\beta \\cdot \\mathbb {I} (u, p \\in \\mathcal {T} _ {\\text {s e e d}})) \\tag {3}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\hat {w} _ {u p} = w _ {u p} \\cdot (1 + \\beta \\cdot \\mathbb {I} (u, p \\in \\mathcal {T} _ {\\text {s e e d}})) \\tag {3}", "context": "\\hat {w} _ {u p} = w _ {u p} \\cdot (1 + \\beta \\cdot \\mathbb {I} (u, p \\in \\mathcal {T} _ {\\text {s e e d}})) \\tag {3}", "negative_type": "hard_same_modal"}}, {"text": "Can we directly use ObjEmbed to regress high-quality bounding boxes, similar to object detectors? Given the importance of proposal quality, a natural question arises: can the embedding model itself be leveraged to refine proposals by predicting bounding box offsets, thereby improving localization accuracy? To investigate this, we conduct experiments where the model uses the IoU embeddings to predict both IoU scores and bounding box offsets simultaneously. The offset regression head is trained with a combination of L1 loss and IoU loss, following the standard practice in DETR (Carion et al., 2020). Similar to IoU regression loss, the bounding box regression loss is applied only to positive proposals with an IoU greater than 0.5. As a result, the regression head can refine existing bounding boxes but is unable to generate missing ones. As shown in Table 11, we find that incorporating box regression degrades overall performance, possibly due to learning conflicts.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "The key idea of the proposed loss is to increase the association of each visual token with the language concepts describing its content. For example, if a given image patch contains part of a cat, its visual token should yield a high probability of the vocabulary token  < c a t > . To formulate this idea, we map deep embeddings of visual tokens to probability distributions (pdf) over language vocabulary tokens to recover textual semantics directly from visual embeddings, see Fig.1. Here we utilize Logit Lens (nostalgebraist, 2020) that was originally proposed for LLMs. Logit Lens demonstrated that mapping of a deep embedding to the pdf space is not only useful for the next-token prediction when applied to the deep embedding of the previous token, but can also reveal the content of other LLM tokens. As shown in (Jiang et al., 2025), Logit Lens provides insight into the representation of visual tokens in VLMs as well. If we focus on a selected text token, e.g.,  < c a t > , Logit Lens enables us to generate object confidence (or localization) maps over the input image, showing for each patch how strongly it is associated with the selected token. However, the Logit Lens visualization does not work well, in particular, if one moves away from LLava, used in (Jiang et al., 2025), to newer VLMs like Qwen2.5-VL, as demonstrated in Fig. 2. The reason is the diffusion of localized image information away from visual to text tokens.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does this formula calculate in terms of the relationship between the LLM output and the static weight component?", "query_type": "semantic", "positive": {"text": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "context": "In the second stage, we refine the weights of the surviving edges in [math] to minimize semantic drift. While vector similarity (Stage I) captures general relatedness, it often fails to distinguish between generic associations and precise evidentiary links. We employ a Large Language Model (LLM) as a discrete approximation of the conditional transition probability [math] . The LLM evaluates the necessity of the transition [math] given the query [math] and the neighbor’s summary [math] . We prompt the model to classify the relationship into discrete tiers [math] {Irrelevant, Weak, High, Direct}. We define a mapping function [math] to project these judgments into scalar weights. The updated dynamic weight [math] is computed as: \\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)} This modulation is asymmetric, applied only to forward edges originating from the seed set. By suppressing irrelevant edges and amplifying critical ones, we actively steer the PPR propagation, ensuring the traversal tunnels through the graph along the query’s intent rather than diffusing into topological sinks."}}, "negatives": [{"text": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "context": "During VLM training, the standard next-token prediction (NTP) loss is used: L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1} where [math] is the probability of generating the next ground truth token [math] , [math] denotes the model parameters, and [math] is the length of the answer sequence. We observe that the NTP loss does not include any incentives for preserving the image content initially encoded in visual tokens [math] .", "negative_type": "hard_same_modal"}}, {"text": "The same VLM was used to identify contextual differences between each original–variation pair, followed by manual verification and refinement to ensure that visual contextual variations were accurately captured in the final labels. Finally, These variation outputs were manually filtered to retain 2.8K images exhibiting meaningful contextual changes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Quantifying Semantic Drift. To assess whether our proposed framework mitigates this drift, we analyzed the topological properties of the top-10 retrieved entity nodes after PPR across 100 randomly sampled queries from MuSiQue. We introduce PPR-Weighted Strength  ( S _ { p p r } ) to measure the effective structural prominence of the retrieved context:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the variable C(v) represent in the LLM function call, and how does it relate to node v?", "query_type": "variable", "positive": {"text": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "context": "In the second stage, we refine the weights of the surviving edges in [math] to minimize semantic drift. While vector similarity (Stage I) captures general relatedness, it often fails to distinguish between generic associations and precise evidentiary links. We employ a Large Language Model (LLM) as a discrete approximation of the conditional transition probability [math] . The LLM evaluates the necessity of the transition [math] given the query [math] and the neighbor’s summary [math] . We prompt the model to classify the relationship into discrete tiers [math] {Irrelevant, Weak, High, Direct}. We define a mapping function [math] to project these judgments into scalar weights. The updated dynamic weight [math] is computed as: \\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)} This modulation is asymmetric, applied only to forward edges originating from the seed set. By suppressing irrelevant edges and amplifying critical ones, we actively steer the PPR propagation, ensuring the traversal tunnels through the graph along the query’s intent rather than diffusing into topological sinks."}}, "negatives": [{"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "negative_type": "hard_same_modal"}}, {"text": "Figure 3. Bar plots illustrating the ratio of the object confidence scores of objects present in images within their bounding boxes to the confidence measure of the same objects averaged across all visual tokens in each image. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a93a08a0fc6df7953f00b1810970485d7252f72e3ac51008d7734967507ff467.jpg", "negative_type": "random", "metadata": {"caption": "Figure 3. Bar plots illustrating the ratio of the object confidence scores of objects present in images within their bounding boxes to the confidence measure of the same objects averaged across all visual tokens in each image. ", "negative_type": "hard_same_modal"}}, {"text": "ing set, ObjEmbed achieves an overall score of 81.7 points, which is highly competitive with both traditional CLIPstyle models and recent LMM-based embedding approaches. Moreover, it demonstrates robust generalization across varying text lengths and multiple languages.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you apply this formula to update edge weights in a graph where you have a query q, two nodes u and v, and existing static weights?", "query_type": "application", "positive": {"text": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "context": "In the second stage, we refine the weights of the surviving edges in [math] to minimize semantic drift. While vector similarity (Stage I) captures general relatedness, it often fails to distinguish between generic associations and precise evidentiary links. We employ a Large Language Model (LLM) as a discrete approximation of the conditional transition probability [math] . The LLM evaluates the necessity of the transition [math] given the query [math] and the neighbor’s summary [math] . We prompt the model to classify the relationship into discrete tiers [math] {Irrelevant, Weak, High, Direct}. We define a mapping function [math] to project these judgments into scalar weights. The updated dynamic weight [math] is computed as: \\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)} This modulation is asymmetric, applied only to forward edges originating from the seed set. By suppressing irrelevant edges and amplifying critical ones, we actively steer the PPR propagation, ensuring the traversal tunnels through the graph along the query’s intent rather than diffusing into topological sinks."}}, "negatives": [{"text": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "context": "We show that the gradient of [math] provides a direct signal to visual tokens. For simplicity, let us consider a single positive visual token [math] in the last layer [math] . Then [math] reduces to \\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8} Let [math] , where [math] is the unembedding matrix, [math] , and [math] . Using the standard softmax cross-entropy derivative,", "negative_type": "hard_same_modal"}}, {"text": "Dynamic Edge Scoring Schedule. To translate the LLM’s semantic assessment into topological structure, we employ a tiered projection strategy. We define four distinct semantic tiers—Irrelevant, Weak, High, and Direct—and map the discrete LLM scores  s \\in \\{ 0 , \\ldots , 1 0 \\} to specific weight intervals (Table 7). This non-linear mapping acts as a high-pass filter, strictly pruning noise (scores  \\leq 3 ) while exponentially amplifying high-confidence evidence paths.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "searches. CatRAG instead introduces a \"one-shot\" context-aware graph modification that dynamically re-weights edges before traversal. Unlike iterative cycles, our approach maintains the efficiency of a single retrieval pass, effectively combining the reasoning precision of adaptive methods with the speed and structural integrity of graph-based retrieval.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How many different institutional affiliations are represented among the authors listed in this passage?", "query_type": "factual", "positive": {"text": "Kwun Hang Lau 1,2†, Fangyuan Zhang 1, Boyu Ruan 1, Yingli Zhou3, Qintian  \\mathbf { G u o } ^ { 2 } , Ruiyuan Zhang2, Xiaofang Zhou2", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "confirming that SGAI, HVSA, and CLSS are complementary for crafting transferable targeted adversarial examples.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\hat { \\mathbf { F } } _ { a n c } ^ { l } ( \\mathbf { f } _ { c l s } ^ { l } , \\hat { \\mathbf { f } } _ { c l s } ^ { l } ) \\in \\mathbb { R } ^ { D } and spatial tokensch tokens to obtain  ( \\bar { \\mathbf { F } } _ { s p a } ^ { l } , \\hat { \\mathbf { F } } _ { s p a } ^ { l } ) \\in \\mathbb { R } ^ { N \\times D } a single spatial descriptor per layer:  \\begin{array} { r } { \\overline { { \\mathbf { F } } } _ { s p a } ^ { l } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\mathbf { F } _ { s p a } ^ { l } [ i ] } \\end{array} (and similarly for  \\overline { { \\hat { \\mathbf { F } } } } _ { s p a } ^ { l } ) . Mean pooling provides a compact spatial summary without requiring strict patch-to-patch correspondence, making the alignment more robust to differences in patch size and common preprocessing (e.g., resizing or cropping) across models. The HVSA loss is then defined as:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "word splits or multi-word phrases), we use a token set  V \\subseteq \\nu and apply the loss to each token independently and average across tokens.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the dagger symbol (†) after Kwun Hang Lau's name typically signify in academic paper authorship conventions?", "query_type": "conceptual", "positive": {"text": "Kwun Hang Lau 1,2†, Fangyuan Zhang 1, Boyu Ruan 1, Yingli Zhou3, Qintian  \\mathbf { G u o } ^ { 2 } , Ruiyuan Zhang2, Xiaofang Zhou2", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "where  y _ { i j } = 1 if  \\mathrm { I o U } ( p _ { j } , B ^ { i } ) > 0 . 5 , and 0 otherwise.  M is the number of annotations and  N is the number of proposals.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(ori) a yellow taxi cab parked on the street ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/d5fea4c11f1b8ed5ebf65eab59e3d883ed993b3ab518a7323c895ba260a31f29.jpg", "negative_type": "random", "metadata": {"caption": "(ori) a yellow taxi cab parked on the street ", "negative_type": "random"}}, {"text": "The key idea of the proposed loss is to increase the association of each visual token with the language concepts describing its content. For example, if a given image patch contains part of a cat, its visual token should yield a high probability of the vocabulary token  < c a t > . To formulate this idea, we map deep embeddings of visual tokens to probability distributions (pdf) over language vocabulary tokens to recover textual semantics directly from visual embeddings, see Fig.1. Here we utilize Logit Lens (nostalgebraist, 2020) that was originally proposed for LLMs. Logit Lens demonstrated that mapping of a deep embedding to the pdf space is not only useful for the next-token prediction when applied to the deep embedding of the previous token, but can also reveal the content of other LLM tokens. As shown in (Jiang et al., 2025), Logit Lens provides insight into the representation of visual tokens in VLMs as well. If we focus on a selected text token, e.g.,  < c a t > , Logit Lens enables us to generate object confidence (or localization) maps over the input image, showing for each patch how strongly it is associated with the selected token. However, the Logit Lens visualization does not work well, in particular, if one moves away from LLava, used in (Jiang et al., 2025), to newer VLMs like Qwen2.5-VL, as demonstrated in Fig. 2. The reason is the diffusion of localized image information away from visual to text tokens.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the author affiliations and the formatting style used, what can be inferred about the collaborative nature of this research work?", "query_type": "inferential", "positive": {"text": "Kwun Hang Lau 1,2†, Fangyuan Zhang 1, Boyu Ruan 1, Yingli Zhou3, Qintian  \\mathbf { G u o } ^ { 2 } , Ruiyuan Zhang2, Xiaofang Zhou2", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "The dataset annotations cover a diverse set of contextual change labels (Table 1 and Figure 3). Illumination–time changes are the most frequent, accounting for 49.68% of", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Table 1. The overview of ObjEmbed training dataset. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/120ffad156187adf11f4108680e45a1cf7ee3c11605469ca52e8372d40e6daa1.jpg", "negative_type": "random", "metadata": {"caption": "Table 1. The overview of ObjEmbed training dataset. ", "negative_type": "hard_same_modal"}}, {"text": "Interestingly, adding LLL to NTP also improves the attention of answer tokens to the correct image regions, even though LLL does not explicitly supervise attention. We measure the ratio of the last answer token’s attention to visual tokens inside the object bounding box versus all image tokens (Fig. 4); values above 1 indicate correct spatial focus. Only the NTP+LLL model exceeds this threshold, showing that object-region tokens exert stronger influence on the generated answer. This is notable since no attention-sink mitigation is applied (Kang et al., 2025). Fig. 5 visualizes this effect; sink tokens appear in the top-left, and the answer token is  { < y } { \\in } S > for the selected samples.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the 'Static Graph Fallacy' and how does it affect retrieval performance in existing RAG methods?", "query_type": "conceptual", "positive": {"text": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query’s intent; and (3) Key-Fact Passage Weight Enhancement, cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state-of-the-art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness—the capacity to recover entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "To evaluate the ability of CatRAG to maintain precise retrieval in multi-hop scenarios, we conduct experiments on four benchmarks across two challenge types: Multi-hop QA and Multi-hop Fact Verification. We summarize the key statistics of these datasets in Table 1.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "The term weighted by  \\lambda _ { c l s } aligns the CLS token, encouraging the adversarial example to match the target at a global, image-level semantics across selected layers. The term weighted by  \\lambda _ { s p a } aligns the pooled patch tokens, encouraging consistency in spatial cues related to the target. We separate these two terms to provide complementary supervision: the CLS term focuses on global semantics, while the spatial term preserves coarse spatial cues, leading to a more balanced alignment signal.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We empirically set  \\lambda to 0.5, which we found effective across all tasks. Like  \\boldsymbol { L } _ { \\mathrm { N T P } } , the proposed  L _ { \\mathrm { L L L } } is only applied to the last VLM layer, i.e.,  l = L in Eq. (4). Moreover, after finetuning with LLL, it is also sufficient to apply Logit Lens to the last layer. In contrast, in (Jiang et al., 2025), Logit Lens is computed as the maximum over all layers.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the three specific components of the multi-faceted framework that CatRAG uses to steer random walks?", "query_type": "factual", "positive": {"text": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query’s intent; and (3) Key-Fact Passage Weight Enhancement, cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state-of-the-art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness—the capacity to recover entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "To train ObjEmbed, each image in the training dataset is annotated with a long caption, a short caption, and several regions of interest, where each region is associated with a corresponding object description. To minimize false-negative conflicts during contrastive learning, captions are designed to be as diverse and distinctive as possible. Furthermore, they should exclude subjective or interpretive content to ensure objectivity and consistency. To achieve high-quality annotations, we carefully design structured prompts and employ a state-of-the-art multimodal large language model, Qwen-VL-235B (Bai et al., 2025a), as the automated annotator. The annotation prompts are as follows:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "A person in a dark military uniform with a peaked cap stands in the foreground on a paved airfield, holding a water bottle. Behind them, a gray single-engine jet fighter with its cockpit canopy open is parked. A yellow ladder leans against the aircraft, and at least two other individuals are visible near the plane. The background consists of a grassy field and a line of trees under an overcast sky.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "CLSS imposes cross-modal alignment at intermediate layers: after projection into the shared latent subspace, intermediate adversarial visual features are aligned with the target text features, instead of relying only on final-layer matching. During optimization,  { \\mathcal { L } } _ { m i d } provides cross-modal gradients at multiple depths, which improves targeted transfer when victim VLMs use different projection modules or cross-modal alignment schemes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the problems described with existing methods and CatRAG's proposed solutions, why would dynamic edge weighting likely be more effective than static transition probabilities for multi-hop queries?", "query_type": "inferential", "positive": {"text": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query’s intent; and (3) Key-Fact Passage Weight Enhancement, cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state-of-the-art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness—the capacity to recover entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "We do not intend to teach VLMs new concepts but to preserve their localized image understanding, and consequently, make Logit Lens visualisation useful in VLM explainability. We focus on finetuning, even though the proposed LLL could be applied in pre-training, it is beyond the scope of this paper.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "In Table 5, we evaluate ObjEmbed on traditional imagetext retrieval benchmarks, including long text retrieval (ShareGPT4V (Chen et al., 2024a) and DCI (Urbanek et al., 2024)), short text retrieval (COCO (Chen et al., 2015) and Flickr30K (Young et al., 2014)), and multilingual text retrieval (COCO-CN (Li et al., 2019) and Flikr30K-CN (Lan et al., 2017)). Despite using a relatively small-scale train-", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "where  y _ { i j } = 1 if  \\mathrm { I o U } ( p _ { j } , B ^ { i } ) > 0 . 5 , and 0 otherwise.  M is the number of annotations and  N is the number of proposals.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the two main causes of hallucinations in LLM response generation mentioned in the passage?", "query_type": "factual", "positive": {"text": "Large Language Models (LLMs) have demonstrated transformative capabilities across a spectrum of natural language tasks, ranging from creative composition to complex code generation (Joel et al., 2025; Li et al., 2024; Ren et al., 2024; Touvron et al., 2023; Brown et al., 2020). Despite these advances, the widespread deployment of LLMs still remains restricted by hallucinations(Xu et al., 2025; Liu et al., 2024) in response generation, often caused by outdated training data or lack of domain-specific knowledge, resulting in seemingly plausible but actually incorrect content. Retrieval-Augmented Generation (RAG) (Gao et al., 2024; Fan et al., 2024) has emerged as a feasible solution to mitigate the issue, which incorporates external, reliable documents within LLM prompts for response generation.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "This work demonstrates that targeted, surrogate-driven perturbations can reliably steer the outputs of diverse VLMs, and that improving cross-modal control requires going beyond final-layer embedding matching. Existing targeted transfer attacks commonly narrow guidance to a single target reference and focus optimization on the final layer, which makes the optimization sensitive to the surrogate-specific embedding space and leaves intermediate semantics underutilized when transferring across heterogeneous VLMs.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "The dataset annotations cover a diverse set of contextual change labels (Table 1 and Figure 3). Illumination–time changes are the most frequent, accounting for 49.68% of", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "where  e _ { v } is the one-hot vector for token  v and vector  p \\in \\mathbb { R } ^ { | V | } denotes the softmax probability distribution over the vocabulary: chain rule,  p = \\mathrm { s o f t m a x } \\bar { ( } U h _ { L } ( s ) \\bar { ) } , \\sum _ { j = 1 } ^ { | V | } p _ { j } = 1 . By the", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does Retrieval-Augmented Generation (RAG) address the fundamental problem of LLM hallucinations?", "query_type": "conceptual", "positive": {"text": "Large Language Models (LLMs) have demonstrated transformative capabilities across a spectrum of natural language tasks, ranging from creative composition to complex code generation (Joel et al., 2025; Li et al., 2024; Ren et al., 2024; Touvron et al., 2023; Brown et al., 2020). Despite these advances, the widespread deployment of LLMs still remains restricted by hallucinations(Xu et al., 2025; Liu et al., 2024) in response generation, often caused by outdated training data or lack of domain-specific knowledge, resulting in seemingly plausible but actually incorrect content. Retrieval-Augmented Generation (RAG) (Gao et al., 2024; Fan et al., 2024) has emerged as a feasible solution to mitigate the issue, which incorporates external, reliable documents within LLM prompts for response generation.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "To overcome these issues, we develop SGHA-Attack by combining anchor-based guidance with multi-granularity alignment. Rather than optimizing toward a single reference, the attack constructs a visually grounded reference pool and uses a weighted mixture to provide stable target guidance. To enforce hierarchical consistency, the attack aligns intermediate visual features at both global and spatial granularities across multiple depths, and further synchronizes visual and textual features within a shared latent subspace. This ensures that cross-modal supervision is injected early in the pipeline rather than being restricted to the final projection.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "To reduce annotation costs, we aggregate existing opensource object detection and referring expression comprehension datasets that provide region-level annotations. Due to limited data coverage, we further curate  3 0 0 \\mathrm { k } images from", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Lunara-II achieves a slightly lower mean aesthetic score (5.91) with substantially reduced variance, indicating a tighter but more moderate aesthetic distribution. This reduction relative to Lunara is intentional: Lunara-II explicitly includes everyday objects, routine environments, and visually ordinary scenes in order to support systematic evaluation of contextual variation. As a result, images are not optimized for visual appeal but instead reflect common real-world visual contexts, leading to relatively lower—but still high aesthetic scores.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why might the transformative capabilities of LLMs in creative composition and code generation be undermined by their current limitations?", "query_type": "inferential", "positive": {"text": "Large Language Models (LLMs) have demonstrated transformative capabilities across a spectrum of natural language tasks, ranging from creative composition to complex code generation (Joel et al., 2025; Li et al., 2024; Ren et al., 2024; Touvron et al., 2023; Brown et al., 2020). Despite these advances, the widespread deployment of LLMs still remains restricted by hallucinations(Xu et al., 2025; Liu et al., 2024) in response generation, often caused by outdated training data or lack of domain-specific knowledge, resulting in seemingly plausible but actually incorrect content. Retrieval-Augmented Generation (RAG) (Gao et al., 2024; Fan et al., 2024) has emerged as a feasible solution to mitigate the issue, which incorporates external, reliable documents within LLM prompts for response generation.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "A major direction for improving grounding is adding localization heads and finetuning on region-annotated data. LISA (Lai et al., 2024) introduces a [SEG] token and mask decoder; F-LMM (Wu et al., 2025a) refines attentionderived masks; and GLAMM (Rasheed et al., 2024) adds a grounding encoder and pixel decoder. While effective, these methods predict boxes or masks externally and do not bind language concepts to visual tokens inside the LLM.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 1: Dataset statistics. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/4ce5a2da7b6431dd6df5a1e868faa1e2dfa60bb501afebadebd4df61fca3a2ed.jpg", "negative_type": "random", "metadata": {"caption": "Table 1: Dataset statistics. ", "negative_type": "random"}}, {"text": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/dd6b9ac6a9a67b9f0c40d35b345feeb0ef3e178cb7ed5ab19250580d4daa9849.jpg", "negative_type": "random", "metadata": {"caption": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What algorithm does the HippoRAG framework use over Knowledge Graphs to distinguish itself from other Structure-Aware RAG approaches?", "query_type": "factual", "positive": {"text": "Standard dense retrieval methods, which select document chunks based on semantic similarity (Izacard et al., 2022a), frequently fail in multi-hop reasoning scenarios when the answer relies on connecting disjoint facts. To overcome this limitation, recent research has shifted towards Structure-Aware RAG, which organizes information into hierarchical trees (Sarthi et al., 2024) or global knowledge graphs (Guo et al., 2025) to capture longrange dependencies. Among these, the HippoRAG framework (Gutiérrez et al., 2024, 2025) distinguishes itself by leveraging Personalized PageRank (PPR) over Knowledge Graphs. HippoRAG simulates neurobiological memory mechanism, enabling deeper and more efficient knowledge integration that vector similarity alone cannot resolve.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "The dataset annotations cover a diverse set of contextual change labels (Table 1 and Figure 3). Illumination–time changes are the most frequent, accounting for 49.68% of", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e86430fc48af9395b15cc063e03a2b6e7a68b47f009161e4dd69d8dba896fa8b.jpg", "negative_type": "random", "metadata": {"caption": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "negative_type": "random"}}, {"text": "Fig. 8: Validation of the strategy on different architectures. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/7f9f4564945b6c1dd5820acea2a0eca05fb2bb6a405f5ee9553d012c77dbec12.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 8: Validation of the strategy on different architectures. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does Structure-Aware RAG fundamentally differ from standard dense retrieval methods in terms of information organization?", "query_type": "conceptual", "positive": {"text": "Standard dense retrieval methods, which select document chunks based on semantic similarity (Izacard et al., 2022a), frequently fail in multi-hop reasoning scenarios when the answer relies on connecting disjoint facts. To overcome this limitation, recent research has shifted towards Structure-Aware RAG, which organizes information into hierarchical trees (Sarthi et al., 2024) or global knowledge graphs (Guo et al., 2025) to capture longrange dependencies. Among these, the HippoRAG framework (Gutiérrez et al., 2024, 2025) distinguishes itself by leveraging Personalized PageRank (PPR) over Knowledge Graphs. HippoRAG simulates neurobiological memory mechanism, enabling deeper and more efficient knowledge integration that vector similarity alone cannot resolve.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "For example, if a given visual token s corresponds to an image patch containing part of a cat, we want  p _ { \\theta } ( < c a t > | h _ { l } ( s ) ) to have a high value, which is analogous to increasing", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "using the top-5 retrieved passages. Key hyperparameters include: symbolic anchor reset probability  \\epsilon = 0 . 2 (weighted by inverse passage count  | P _ { i } | ^ { - 1 } ) , boost factor  \\beta \\ : = \\ : 2 . 5 , dynamic weighting limits  N _ { s e e d } = 5 and  K _ { e d g e } \\mathrm { ~ = ~ } 1 5 . More implementation details and hyperparameters are provided in Appendix A.1.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "where scalar  p _ { j } is the  j -th component of  p , corresponding to the probability assigned to vocabulary token  j .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why might HippoRAG's neurobiological memory simulation approach be more effective than vector similarity for multi-hop reasoning scenarios?", "query_type": "inferential", "positive": {"text": "Standard dense retrieval methods, which select document chunks based on semantic similarity (Izacard et al., 2022a), frequently fail in multi-hop reasoning scenarios when the answer relies on connecting disjoint facts. To overcome this limitation, recent research has shifted towards Structure-Aware RAG, which organizes information into hierarchical trees (Sarthi et al., 2024) or global knowledge graphs (Guo et al., 2025) to capture longrange dependencies. Among these, the HippoRAG framework (Gutiérrez et al., 2024, 2025) distinguishes itself by leveraging Personalized PageRank (PPR) over Knowledge Graphs. HippoRAG simulates neurobiological memory mechanism, enabling deeper and more efficient knowledge integration that vector similarity alone cannot resolve.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "where the NTP loss gradient is summed over text tokens  \\tau because only text tokens receive the language-model", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "SORCE-1K (Liu et al., 2025a) comprises 1,023 carefully curated images with complex backgrounds and textual queries that describe less prominent small objects with minimum surrounding context. The target objects typically occupy less than  10 \\% of the image area, posing significant challenges for global image embedding models that focus on holistic scene understanding. Each query is associated with exactly one positive image, and performance is evaluated using Recall  @ 1 .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "(ori) The image features a close-up view of a yellow mushroom growing in a grassy area. The mushroom is surrounded by dry grass and appears to be growing on a tree branch ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/a4bb424f2166bf1d1ecbb158eb90cd28f6c36a6efe0f6db7925cde63631f40ff.jpg", "negative_type": "random", "metadata": {"caption": "(ori) The image features a close-up view of a yellow mushroom growing in a grassy area. The mushroom is surrounded by dry grass and appears to be growing on a tree branch ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the RefCOCO score for the NTP + LLL + SAM method?", "query_type": "factual", "positive": {"text": "Table 1. Referring Expression Segmentation results of supervised and training-free VLM-based methods in cIoU  ( \\% ) . SAM indicates that a frozen SAM is used to generate masks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/8f7d4b5730be4b236b6450c54ee8051a45c5364331a8e3668b34aa0517347445.jpg", "metadata": {"caption": "Table 1. Referring Expression Segmentation results of supervised and training-free VLM-based methods in cIoU  ( \\% ) . SAM indicates that a frozen SAM is used to generate masks. "}}, "negatives": [{"text": "Table 6. Ablation studies on different object token designs. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/c0109746b758d382be093f6db84eef6792741078abc5bdaf8c819351b2919ca3.jpg", "negative_type": "random", "metadata": {"caption": "Table 6. Ablation studies on different object token designs. ", "negative_type": "hard_same_modal"}}, {"text": "A major direction for improving grounding is adding localization heads and finetuning on region-annotated data. LISA (Lai et al., 2024) introduces a [SEG] token and mask decoder; F-LMM (Wu et al., 2025a) refines attentionderived masks; and GLAMM (Rasheed et al., 2024) adds a grounding encoder and pixel decoder. While effective, these methods predict boxes or masks externally and do not bind language concepts to visual tokens inside the LLM.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Can we directly use ObjEmbed to regress high-quality bounding boxes, similar to object detectors? Given the importance of proposal quality, a natural question arises: can the embedding model itself be leveraged to refine proposals by predicting bounding box offsets, thereby improving localization accuracy? To investigate this, we conduct experiments where the model uses the IoU embeddings to predict both IoU scores and bounding box offsets simultaneously. The offset regression head is trained with a combination of L1 loss and IoU loss, following the standard practice in DETR (Carion et al., 2020). Similar to IoU regression loss, the bounding box regression loss is applied only to positive proposals with an IoU greater than 0.5. As a result, the regression head can refine existing bounding boxes but is unable to generate missing ones. As shown in Table 11, we find that incorporating box regression degrades overall performance, possibly due to learning conflicts.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method performs better on RefCOCO+: GLaMM or PSALM?", "query_type": "comparative", "positive": {"text": "Table 1. Referring Expression Segmentation results of supervised and training-free VLM-based methods in cIoU  ( \\% ) . SAM indicates that a frozen SAM is used to generate masks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/8f7d4b5730be4b236b6450c54ee8051a45c5364331a8e3668b34aa0517347445.jpg", "metadata": {"caption": "Table 1. Referring Expression Segmentation results of supervised and training-free VLM-based methods in cIoU  ( \\% ) . SAM indicates that a frozen SAM is used to generate masks. "}}, "negatives": [{"text": "Table 11. Ablation studies on supporting box regression. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/80767085b0feebbfc90081dfb853ceff29cb51034cf3475f4045aba7089d4cac.jpg", "negative_type": "random", "metadata": {"caption": "Table 11. Ablation studies on supporting box regression. ", "negative_type": "hard_same_modal"}}, {"text": "Existing transfer-based attacks can be broadly categorized into perturbation-constrained methods and unrestricted diffusion-based approaches. In the constrained category, methods like AttackVLM [13] optimize adversarial perturbations by aligning global image embeddings with a target text embedding within a restricted  \\ell _ { \\infty } budget. Chain-of-Attack (COA) [14] further enhances this by incorporating modality fusion and iterative optimization strategies to improve semantic consistency. M-Attack [15] introduces robust input transformations, such as random resizing and cropping, to bypass the preprocessing pipelines of commercial models. In the unrestricted category, approaches like AdvDiffVLM [16] leverage latent diffusion models to generate adversarial examples. Specifically, they utilize attention-guided masking to preserve critical features, fundamentally operating by reconstructing image content rather than adding imperceptible noise. Despite different designs, these transfer-based attacks have two common limitations. First, they rely on a single target reference, which can be sensitive to the surrogate model and thus transfer poorly to other VLMs. Second, they match embedding similarity only at the surrogate encoder’s final", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "This weak seeding serves a specific regulatory function: it aligns the PPR propagation with the query’s intent. By placing a non-zero probability on the exact named entities mentioned in the query, we create a gravitational pull that resists the diffusion of probability mass into generic graph hubs. Even as the random walk explores the neighborhood defined by the static graph, these weak anchors ensure the traversal recurrently grounded to the specific entities in the query, effectively suppressing semantic drift. As a secondary benefit, this mechanism naturally balance the system’s capability: it retains the triplet-based strength in interpreting implicit clues while ensuring robust coverage for containing explicit entity mentions.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the average RefCOCOg score across all the finetuned-based VLM methods (LISA-7B, GSVA-7B, F-LMM-7B, LISA-13B, GSVA-13B, GLaMM, and PSALM)?", "query_type": "computational", "positive": {"text": "Table 1. Referring Expression Segmentation results of supervised and training-free VLM-based methods in cIoU  ( \\% ) . SAM indicates that a frozen SAM is used to generate masks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/8f7d4b5730be4b236b6450c54ee8051a45c5364331a8e3668b34aa0517347445.jpg", "metadata": {"caption": "Table 1. Referring Expression Segmentation results of supervised and training-free VLM-based methods in cIoU  ( \\% ) . SAM indicates that a frozen SAM is used to generate masks. "}}, "negatives": [{"text": "Table 7. Ablation studies on instructions. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/8644dc897bcd0933b4eac5a976db0cd98ae4b4a1d13f7841fb708b2bc0cd6ac6.jpg", "negative_type": "random", "metadata": {"caption": "Table 7. Ablation studies on instructions. ", "negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "context": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "negative_type": "hard_same_modal"}}, {"text": "Strict Reasoning Completeness Evaluation. While standard metrics indicate general relevance, they often mask a critical failure mode in multi-hop retrieval: the loss of intermediate \"bridge\" documents that connect disjoint facts. To assess the recovery of the full evidence paths, we evaluate FCR and JSR in Table 4. CatRAG effectively mitigates probability dilution, achieving an FCR of  34 . 6 \\% compared to  3 0 . 5 \\% for HippoRAG 2. The gain is most pronounced on HoVer, where precise 3–4 hop claim verification is required. CatRAG improves JSR to  3 1 . 1 \\% , a relative gain of  1 8 . 7 \\% over the HippoRAG2. These results confirm that our dynamic steering successfully anchors the traversal to the specific bridge documents required for grounded reasoning.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the accuracy of the LLaVA-v1.5-7B base model?", "query_type": "factual", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4e1634418e9d611556004f04860f67e8a5360f67d8cff842bead42c27ae133c6.jpg", "negative_type": "random", "metadata": {"caption": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "negative_type": "random"}}, {"text": "You are a precise, factual image cataloger. Your task is to generate a literal description of the image for a visual database. You should generate a **short caption** and a **long caption** for each image.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(b) NTP ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/4cdef342aaa29d0a244047b907abb430d106e92ffc514a06c16fff0a2496b84f.jpg", "negative_type": "random", "metadata": {"caption": "(b) NTP ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method achieves the highest accuracy overall, and how does it compare to the lowest performing method?", "query_type": "comparative", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/7b8d345cae457d8ff8d0352846a2217174c7af5ee47880a9a7a217ecdf38c555.jpg", "negative_type": "random", "metadata": {"caption": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "negative_type": "hard_same_modal"}}, {"text": "Image-level contrastive learning. In line with region-level contrastive learning, we also use sigmoid focal loss (Zhai et al., 2023) for image-level supervision  \\mathcal { L } _ { \\mathrm { i m a g e } } . Specifically, the first global image embedding is only supervised by short captions while the second global image embedding is only supervised by long captions. And we will collect captions from other GPUs to enlarge the negative samples.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "racy of 89.5, surpassing both specialized MLLMs designed for referring tasks and significantly larger general-purpose multimodal models. This strong performance demonstrates the high semantic discriminability of our object embeddings.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the average accuracy improvement when adding NTP to the base models for both LLaVA-v1.5-7B and Qwen2.5-VL-7B-Instr?", "query_type": "computational", "positive": {"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4c4a0fe40380c638454206d543e835e8a75efba86610dce02362d98ad8d42ecc.jpg", "negative_type": "random", "metadata": {"caption": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "negative_type": "random"}}, {"text": "L _ {\\text {L L L}} (\\theta) = \\frac {1}{\\left| \\mathcal {P} ^ {\\prime} \\right|} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} \\big (<   \\operatorname {c a t} > | h _ {l} (s) \\big). \\tag {5}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\text {L L L}} (\\theta) = \\frac {1}{\\left| \\mathcal {P} ^ {\\prime} \\right|} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} \\big (<   \\operatorname {c a t} > | h _ {l} (s) \\big). \\tag {5}", "context": "For simplicity, let’s say [math] is a vocabulary token representing the word ”cat” and [math] is a subset of visual tokens corresponding to image patches depicting the cat in the input image, then Eq. (4) reduces to L _ {\\text {L L L}} (\\theta) = \\frac {1}{\\left| \\mathcal {P} ^ {\\prime} \\right|} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} \\big (<   \\operatorname {c a t} > | h _ {l} (s) \\big). \\tag {5} It aims at increasing the probability of token [math] cat > in the vocabulary distribution for each deep embedding [math] of image patch [math] that depicts the cat in the image. We stress that this does not suppress other concepts like ”kitten” or ”cat ear”. The expressive power of a VLM comes from its pre-training; the LLL only helps to preserve the imagegrounded locality of visual deep embeddings, and does not aim at teaching new concepts to VLMs.", "negative_type": "random"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7774dfe10ce42975d4a99329740bd13736a30d3144217d59f41cd0eb4689ffa3.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the median distance for the LLaVA-v1.5-7B Base Model?", "query_type": "factual", "positive": {"text": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e86430fc48af9395b15cc063e03a2b6e7a68b47f009161e4dd69d8dba896fa8b.jpg", "metadata": {"caption": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "negative_type": "random"}}, "negatives": [{"text": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4e1634418e9d611556004f04860f67e8a5360f67d8cff842bead42c27ae133c6.jpg", "negative_type": "random", "metadata": {"caption": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "context": "The total training objective is a weighted combination: \\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4} where [math] , and [math] are hyperparameters.", "negative_type": "random"}}, {"text": "Ablation studies on image-level supervision design. In this work, we use both local and global text embeddings and long and short captions as global text embeddings to learn global image embeddings. We study their effects in Table 9. As local text embeddings match with object embeddings while global text embeddings match with global image embeddings, we find that not sharing the text tokens can mitigate task discrepancies and get higher performance on COCO  ( + 0 . 3 \\% mAP) and RefCOCO  ( + 0 . 8 ) , comparing Exp1 and Exp2. Further, using only short captions, long captions, or a mixture of them can not handle complex retrieval requirements. Using both captions for supervision achieves the highest 80.6 global retrieval results (Exp2-Exp5). Finally, using two global image tokens (Exp6), one for short captions and one for long captions, gets the highest results. And we find that the performance on object detection and REC is quite robust to the image-level designs.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method achieves the lowest median distance overall across all models tested?", "query_type": "comparative", "positive": {"text": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e86430fc48af9395b15cc063e03a2b6e7a68b47f009161e4dd69d8dba896fa8b.jpg", "metadata": {"caption": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "negative_type": "random"}}, "negatives": [{"text": "Table 4: Reasoning Completeness Evaluation. We evaluation the Full Chain Retrieval (FCR) and Joint Success Rate (JSR) on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/36f7e779793ed3f4786a8804036f42ea2e74c57dac798b2dbabc12cbe5469205.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Reasoning Completeness Evaluation. We evaluation the Full Chain Retrieval (FCR) and Joint Success Rate (JSR) on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results. ", "negative_type": "hard_same_modal"}}, {"text": "Let  \\mathcal { T } _ { s e e d } be the set of verified seed triples. We identify a \"Key Fact\" connection if the edge  E _ { c t x } from seed entity  u to passage  p is supported by a triple in  \\mathcal { T } _ { s e e d } . We enhance the weight of such edges:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Can we directly use ObjEmbed to regress high-quality bounding boxes, similar to object detectors? Given the importance of proposal quality, a natural question arises: can the embedding model itself be leveraged to refine proposals by predicting bounding box offsets, thereby improving localization accuracy? To investigate this, we conduct experiments where the model uses the IoU embeddings to predict both IoU scores and bounding box offsets simultaneously. The offset regression head is trained with a combination of L1 loss and IoU loss, following the standard practice in DETR (Carion et al., 2020). Similar to IoU regression loss, the bounding box regression loss is applied only to positive proposals with an IoU greater than 0.5. As a result, the regression head can refine existing bounding boxes but is unable to generate missing ones. As shown in Table 11, we find that incorporating box regression degrades overall performance, possibly due to learning conflicts.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the difference in median distance between the Qwen2.5-VL-7B-Instr Base Model and its NTP + LLL variant?", "query_type": "computational", "positive": {"text": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e86430fc48af9395b15cc063e03a2b6e7a68b47f009161e4dd69d8dba896fa8b.jpg", "metadata": {"caption": "Table 3. Zero-shot performance on the Pixmo. LLaVA-v1.5 and Qwen2.5-VL were finetuned on RefCOCO training set. ", "negative_type": "random"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/9353ed44b779c874f584c841c70214b802d067abd8c3143b6ce9850589c7b2b6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "ChatGPT ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0b170a051fd8530bd76ed8e3463c3fad0068920b2738ac88ba8ac0a2508b1f12.jpg", "negative_type": "random", "metadata": {"caption": "ChatGPT ", "negative_type": "hard_same_modal"}}, {"text": "Visualizations of referring expression comprehension results. In addition to strong performance on standard referring benchmarks (e.g., RefCOCO, RefCOCO+, RefCOCOg), in Figure 3, we demonstrate that ObjEmbed exhibits not only strong OCR capabilities (a, b, d), but also commonsense reasoning (c) and image-image matching abilities (e, f), highlighting its generalizability and versatility.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the two types of loss functions shown in this Vision-Language Model architecture?", "query_type": "identification", "positive": {"text": "Figure 1. VLM components with the added Logit Lens Loss (LLL) shown in cyan. While NTP loss maximizes the probability of generating ground truth text tokens, LLL increases the probability of visual tokens  s to predict text tokens describing their content, e.g., if visual token s represents a patch containing part of a cat, the probability of predicting token  < c a t > is increased. This way LLL maximizes the alignment of deep embeddings of image patches containing the cat with text token  < c a t > . ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e4c10f84b9c9667db5db3fb02182b9fdb0c2a51d8863f06e441b8b9bfdcd2fbf.jpg", "metadata": {"caption": "Figure 1. VLM components with the added Logit Lens Loss (LLL) shown in cyan. While NTP loss maximizes the probability of generating ground truth text tokens, LLL increases the probability of visual tokens  s to predict text tokens describing their content, e.g., if visual token s represents a patch containing part of a cat, the probability of predicting token  < c a t > is increased. This way LLL maximizes the alignment of deep embeddings of image patches containing the cat with text token  < c a t > . "}}, "negatives": [{"text": "(ori) The image shows a close up of a car wheel with a white background ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/12b4116103bc22881c42a890428ac93458a96d423e41a8e8b27246d1287057ad.jpg", "negative_type": "random", "metadata": {"caption": "(ori) The image shows a close up of a car wheel with a white background ", "negative_type": "hard_same_modal"}}, {"text": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/d19f22bd6f4ce2c5a6deb364b1a420116fcadd34bf212a2e60c49172e50f1448.jpg", "negative_type": "random", "metadata": {"caption": "Figure 2: Distribution of PPR-Weighted Node Strength  ( S _ { p p r } ) . Comparison of the HippoRAG 2 versus CatRAG. The distribution for CatRAG is shifted to the left, indicating a reduction in the retrieval of highdegree \"Hub\" nodes. The dashed lines represent the mean  \\boldsymbol { S _ { p p r } } for each method. ", "negative_type": "random"}}, {"text": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e32d6a90817b410428eebd4faee60ee22f19f87074a780afc0245db94fa102ae.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the Logit Lens Loss mechanism work to align visual patches with corresponding text tokens?", "query_type": "descriptive", "positive": {"text": "Figure 1. VLM components with the added Logit Lens Loss (LLL) shown in cyan. While NTP loss maximizes the probability of generating ground truth text tokens, LLL increases the probability of visual tokens  s to predict text tokens describing their content, e.g., if visual token s represents a patch containing part of a cat, the probability of predicting token  < c a t > is increased. This way LLL maximizes the alignment of deep embeddings of image patches containing the cat with text token  < c a t > . ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e4c10f84b9c9667db5db3fb02182b9fdb0c2a51d8863f06e441b8b9bfdcd2fbf.jpg", "metadata": {"caption": "Figure 1. VLM components with the added Logit Lens Loss (LLL) shown in cyan. While NTP loss maximizes the probability of generating ground truth text tokens, LLL increases the probability of visual tokens  s to predict text tokens describing their content, e.g., if visual token s represents a patch containing part of a cat, the probability of predicting token  < c a t > is increased. This way LLL maximizes the alignment of deep embeddings of image patches containing the cat with text token  < c a t > . "}}, "negatives": [{"text": "Gemini ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/b2341cf85b51e1dae2a6081165abeeae62413ab15b78323a9321e1dc47030c11.jpg", "negative_type": "random", "metadata": {"caption": "Gemini ", "negative_type": "hard_same_modal"}}, {"text": "Standard Retrieval and QA. Table 2 and Table 3 demonstrate that CatRAG consistently outperforms all baselines across standard metrics. On the complex MuSiQue dataset (2–4 hops), CatRAG achieves a Recall  \\textcircled { \\omega } 5 of  6 4 . 9 \\% , surpassing the dense retriever text-embedding-3-small by a substantial  8 . 1 \\% margin and confirming the necessity of structure-aware methods.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Kwun Hang Lau 1,2†, Fangyuan Zhang 1, Boyu Ruan 1, Yingli Zhou3, Qintian  \\mathbf { G u o } ^ { 2 } , Ruiyuan Zhang2, Xiaofang Zhou2", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why would maximizing the probability of visual tokens predicting relevant text tokens improve the overall performance of vision-language models compared to using only NTP loss?", "query_type": "interpretive", "positive": {"text": "Figure 1. VLM components with the added Logit Lens Loss (LLL) shown in cyan. While NTP loss maximizes the probability of generating ground truth text tokens, LLL increases the probability of visual tokens  s to predict text tokens describing their content, e.g., if visual token s represents a patch containing part of a cat, the probability of predicting token  < c a t > is increased. This way LLL maximizes the alignment of deep embeddings of image patches containing the cat with text token  < c a t > . ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/e4c10f84b9c9667db5db3fb02182b9fdb0c2a51d8863f06e441b8b9bfdcd2fbf.jpg", "metadata": {"caption": "Figure 1. VLM components with the added Logit Lens Loss (LLL) shown in cyan. While NTP loss maximizes the probability of generating ground truth text tokens, LLL increases the probability of visual tokens  s to predict text tokens describing their content, e.g., if visual token s represents a patch containing part of a cat, the probability of predicting token  < c a t > is increased. This way LLL maximizes the alignment of deep embeddings of image patches containing the cat with text token  < c a t > . "}}, "negatives": [{"text": "Figure on page 7", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/fc9464cc44e2d69cdba797b0271ca1a7952827cfbcc5948f4881938fdb113e1a.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "ILIAS (Kordopatis-Zilos et al., 2025) supports both text-based and image-based local retrieval, with queries formulated as a natural language description or a large image exemplar. The original dataset includes 5 million distractors, making full-scale evaluation computationally infeasible. Instead, we construct a manageable gallery using all 4,715 positive images. Different from the benchmarks mentioned above, ILIAS contains only 1,232 queries, each potentially matching multiple positive images. We evaluate using mAP@50, which computes mean average precision over the top 50 retrieved results.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 3 presents zero-shot evaluation results on the PixMo-Points subset. All evaluations are performed in pixel space, with coordinates parsed from the model’s textual output using a fixed prompt template. We report the median distance in pixels. Both base models, LLaVA-v1.5 and Qwen2.5- VL, show limited ability to localize object centers without grounding-oriented supervision. Incorporating LLL during fine-tuning substantially improves both models, reducing median localization error. Notably, Qwen2.5-VL with LLL achieves a lower median distance than several proprietary and task-specific baselines, and it outperforms Molmo-7B, which was trained with explicit coordinate-level supervision on PixMo. These results indicate that LLL enables strong transfer of spatial grounding learned from RefCOCO to previously unseen object categories in PixMo", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What items are visible on the white plates in this outdoor dining scene?", "query_type": "descriptive", "positive": {"text": "Object: Cake ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/7021a26586cdb7920f451d0d9a5882c7e03717bcb4a90dbae75c1ca26f859067.jpg", "metadata": {"caption": "Object: Cake "}}, "negatives": [{"text": "Figure 3. Bar plots illustrating the ratio of the object confidence scores of objects present in images within their bounding boxes to the confidence measure of the same objects averaged across all visual tokens in each image. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a93a08a0fc6df7953f00b1810970485d7252f72e3ac51008d7734967507ff467.jpg", "negative_type": "random", "metadata": {"caption": "Figure 3. Bar plots illustrating the ratio of the object confidence scores of objects present in images within their bounding boxes to the confidence measure of the same objects averaged across all visual tokens in each image. ", "negative_type": "hard_same_modal"}}, {"text": "Haobo Wang and Weiqi Luo are with Guangdong Province Key Lab of Information Security Technology, and School of Computer Science and Engineering, Sun Yat-sen University, Guangdong 510006, China (e-mail:wanghb69@mail2.sysu.edu.cn; luoweiqi@mail.sysu.edu.cn.) Corresponding author: Weiqi Luo.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\"hub\" nodes that have high similarity but lack precise relevance to the query. To mitigate this, we introduce Symbolic Anchoring, a regularization strategy that grounds the stochastic walk using explicit query constraints.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of dessert is being served alongside the beverage?", "query_type": "identification", "positive": {"text": "Object: Cake ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/7021a26586cdb7920f451d0d9a5882c7e03717bcb4a90dbae75c1ca26f859067.jpg", "metadata": {"caption": "Object: Cake "}}, "negatives": [{"text": "Figure 3. Visualizations of referring expression comprehension results with text queries and image queries. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/c9662dc2e32f96842f20563837ad39cdfb0b2e3f87183ed0739cb2a03e3007fe.jpg", "negative_type": "random", "metadata": {"caption": "Figure 3. Visualizations of referring expression comprehension results with text queries and image queries. ", "negative_type": "hard_same_modal"}}, {"text": "A uniformed officer stands on a tarmac near a gray military jet with its cockpit open, while other personnel work nearby. long caption", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "1) As shown in Table III, performance varies significantly by attack category. Unrestricted methods like AdvDiffVLM suffer from noticeable structural degradation (SSIM 0.69) due to their diffusion-based generation process. In contrast, perturbation-constrained methods operate under a standard budget  ( \\epsilon = 8 / 2 5 5 ) and consistently", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the setting and presentation, what does this scene suggest about the dining experience or occasion?", "query_type": "interpretive", "positive": {"text": "Object: Cake ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/7021a26586cdb7920f451d0d9a5882c7e03717bcb4a90dbae75c1ca26f859067.jpg", "metadata": {"caption": "Object: Cake "}}, "negatives": [{"text": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2b95146b666baa981627d602298c523fa3b8fc7c34d72258dc3fadd470663c59.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "negative_type": "hard_same_modal"}}, {"text": "consistently outperform other heuristic choices, validating the generality of our strategy across architectures.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "progress, the field continues to lack high-quality, high-aesthetic training data at sufficient signal density. As such, we publicly release Lunara Aesthetic II, a highly curated dataset comprising 2.8K anchor-linked variation sets generated from original artwork, photographs, and the Moonworks Lunara model. The dataset is explicitly designed to operationalize identity-preserving contextual variation, enabling image generation and editing systems to be trained and evaluated on genuine contextual generalization rather than surface-level visual memorization.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What color patterns are visible on the circular plates in this image?", "query_type": "descriptive", "positive": {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a0fba5b03295bd90f717e3cadfa3b27f8e56142a32459cff5df84d96b283b8b7.jpg", "metadata": {"caption": "", "negative_type": "random"}}, "negatives": [{"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/84f706f2865987e50963581f4d0cfa1940e75762d81645d90ba648b916dfd4da.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "To reduce annotation costs, we aggregate existing opensource object detection and referring expression comprehension datasets that provide region-level annotations. Due to limited data coverage, we further curate  3 0 0 \\mathrm { k } images from", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "context": "IoU regression. The IoU loss [math] is also formulated as sigmoid focal loss with ground truth IoUs [math] between proposals and corresponding boxes as labels: \\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3} where [math] is predicted IoU scores. Since not all objects in an image are annotated, [math] is only applied to [math] positive proposals.", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of imaging technique or visualization method appears to be used to create the colored heat map overlays on these objects?", "query_type": "identification", "positive": {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a0fba5b03295bd90f717e3cadfa3b27f8e56142a32459cff5df84d96b283b8b7.jpg", "metadata": {"caption": "", "negative_type": "random"}}, "negatives": [{"text": "Figure on page 10", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/70c333b0acefb8cf2d30c89511f545845c8e6b151c9baf201c600a67dcb0bdce.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "context": "\\hat {w} _ {u v} = \\phi \\left(\\operatorname {L L M} (q, u, v, C (v))\\right) \\cdot w _ {u v} ^ {(s t a t i c)}", "negative_type": "random"}}, {"text": "Overall, adding any single component on top of MFit consistently improves targeted control and semantic consistency. In particular, introducing SGAI  ( + \\mathcal { L } _ { a n c } ) yields a large gain across all three models, e.g., on LLaVA, ASRtarget increases from  2 7 . 6 0 \\% to  6 6 . 1 0 \\% . HVSA  ( + \\mathcal { L } _ { f e a t } ) provides the strongest boost on the challenging instruction-tuned model, raising  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } on LLaVA to  9 2 . 9 0 \\% , and combining SGAI and HVSA further improves it to  9 6 . 2 0 \\% . CLSS  ( + \\mathcal { L } _ { m i d } ) offers complementary cross-modal guidance, improving the overall balance of Ensemble CLIP Score and targeted success when combined with the other terms. Finally, the full objective achieves the best overall performance across models,", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the heat map patterns shown, what might these colored regions indicate about the temperature distribution or activity levels across the surface of these circular objects?", "query_type": "interpretive", "positive": {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a0fba5b03295bd90f717e3cadfa3b27f8e56142a32459cff5df84d96b283b8b7.jpg", "metadata": {"caption": "", "negative_type": "random"}}, "negatives": [{"text": "(d) Change of scene-composition and viewpoint camera  Figure 5: contextual variations in original art. Each pair shows (a) and (b). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/d7e3e89bb2a0d6035c7b398397c93a095183bde13ef92bcf26ee1d8d92f86373.jpg", "negative_type": "random", "metadata": {"caption": "(d) Change of scene-composition and viewpoint camera  Figure 5: contextual variations in original art. Each pair shows (a) and (b). ", "negative_type": "hard_same_modal"}}, {"text": "Let  \\mathcal { P } denote the set of all visual (patch) tokens in the image and  \\mathcal { P } ^ { \\prime } \\subseteq \\mathcal { P } the subset of patches inside the ground-truth region (e.g., bounding box) of the queried object (when present). In other words, we obtain  \\mathcal { P } ^ { \\prime } by projecting the ground-truth bounding box onto the visual-token grid and selecting tokens whose receptive fields overlap with the annotated object region.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "As our confidence maps demonstrate in Fig. 2, VLMs only partially focus on the correct image patches corresponding", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the formula L_NTP(θ) calculate, and what does the negative log probability represent in the context of this loss function?", "query_type": "semantic", "positive": {"text": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "context": "During VLM training, the standard next-token prediction (NTP) loss is used: L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1} where [math] is the probability of generating the next ground truth token [math] , [math] denotes the model parameters, and [math] is the length of the answer sequence. We observe that the NTP loss does not include any incentives for preserving the image content initially encoded in visual tokens [math] .", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "context": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "negative_type": "hard_same_modal"}}, {"text": "POPE (Li et al., 2023b) was originally designed to make the VQA evaluation fair and simple. It evaluates VLMs using binary “yes/no” object queries instead of caption generation. Each question follows the schema “Is there a <object> in the image?”, with balanced “Yes/No” answers for present and non-present objects. Finetuning and evaluation follow the protocol in POPE (Li et al., 2023b). We evaluate on the Popular Sampling split of POPE (3,000 samples). For training, we use MS COCO 2014 images with objects sampled from the 80 classes, selecting images with at least three objects and generating two “Yes” and two “No” questions per image  \\approx 1 6 0 \\mathrm { k } pairs).", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We implement CatRAG upon the HippoRAG 2 architecture, using GPT-4o-mini2 as the backbone for all LLM components and text-embedding-3- small as the retriever. While newer open-weight models like NV-Embed-v2 (Lee et al., 2025) show strong performance, our primary objective is to isolate the topological gains provided by the CatRAG mechanism from the raw semantic capacity of the underlying encoder. For fair comparison, all structure-augmented baselines are reproduced using the same extractor and retriever. Downstream responses are generated by Llama-3.3-70B-Instruct", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the notation 𝒜_{a<i} represent in the conditional probability term, and how does it relate to the indexing variable i in the summation?", "query_type": "variable", "positive": {"text": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "context": "During VLM training, the standard next-token prediction (NTP) loss is used: L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1} where [math] is the probability of generating the next ground truth token [math] , [math] denotes the model parameters, and [math] is the length of the answer sequence. We observe that the NTP loss does not include any incentives for preserving the image content initially encoded in visual tokens [math] .", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "context": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "negative_type": "hard_same_modal"}}, {"text": "Impact of Layer Selection (UniDiffuser) ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/c5c371321129f96b16a47bab9c8a21625e7df6af637749ed26b9603f078f2eca.jpg", "negative_type": "random", "metadata": {"caption": "Impact of Layer Selection (UniDiffuser) ", "negative_type": "random"}}, {"text": "Figure on page 17", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/38adc1074b58b36601d59812a137d1d07736d8f7c02b1145b03e70df4322f676.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you apply this NTP loss function during training - what would you need to compute at each step i, and what role does the input X play in the probability estimation?", "query_type": "application", "positive": {"text": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "context": "During VLM training, the standard next-token prediction (NTP) loss is used: L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1} where [math] is the probability of generating the next ground truth token [math] , [math] denotes the model parameters, and [math] is the length of the answer sequence. We observe that the NTP loss does not include any incentives for preserving the image content initially encoded in visual tokens [math] .", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "context": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "negative_type": "hard_same_modal"}}, {"text": "word splits or multi-word phrases), we use a token set  V \\subseteq \\nu and apply the loss to each token independently and average across tokens.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Ablation studies on different training objectives. In this work, we build a universal object embedding model along with the global image representation ability. In Table 8,", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the formula U_θ h_l(x) compute, and what is the significance of having |V| logit values in the output vector?", "query_type": "semantic", "positive": {"text": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "context": "We use the Logit Lens (Jiang et al., 2025) to reveal how visual tokens evolve into textual concepts within a VLM. Originally designed to analyze intermediate representations in language models, Logit Lens (nostalgebraist, 2020) offers a direct way to project latent embeddings into the LLM’s vocabulary space. It takes an embedding [math] at any LLM layer and projects it through the model’s unembedding matrix [math] , which is the same matrix as used at the final LLM layer to predict vocabulary logits. This gives us a logit vector over the vocabulary of text tokens: U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2} where [math] corresponds to token [math] in the vocabulary [math] . This vector represents the model’s predicted logit distribution over tokens after layer [math] ."}}, "negatives": [{"text": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "context": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "negative_type": "hard_same_modal"}}, {"text": "At the final VLM layer, the dependence of a text token  j on a visual token  s arises through a cross-attention block. For such a block, the Jacobian of the hidden state satisfies", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "is first passed through a frozen visual encoder (typically a ViT, CLIP, or DINOv2) to extract image tokens and map them into a language embedding space with a linear projection module. Subsequently, the mapped image tokens, often called visual tokens, are used as part of the LLM input, along with the text input (and special tokens). As visual tokens are transformed through the layers of an LLM, their embeddings change. In particular, the embeddings of visual tokens are mixed with the embeddings of language tokens in the attention layers of the LLM module. However, there is a risk that information carried by visual tokens becomes diluted among the predominantly textual tokens as they propagate through the LLM layers. In particular, the information of visual tokens may be transferred to language tokens, as nicely demonstrated in (Kaduri et al., 2025). In an extreme case, the visual tokens are ignored, and the LLM can even hallucinate an image description for an empty image, as was demonstrated in (Liu et al., 2024; Tong et al., 2024). Further evidence that visual tokens are often ignored among all LLM tokens is the fact that after removing half of the visual tokens, the VLM performance does not decrease (Chen et al., 2024). An unintended consequence is that the connection of visual tokens to the content of corresponding image regions is lost in this process. As pointed out in (Fu et al., 2025), The LLM’s ability to use its vision representations is a limiting factor in VLM performance.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the variable V represent in this formula, and how does its cardinality |V| determine the dimensionality of the logit vector?", "query_type": "variable", "positive": {"text": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "context": "We use the Logit Lens (Jiang et al., 2025) to reveal how visual tokens evolve into textual concepts within a VLM. Originally designed to analyze intermediate representations in language models, Logit Lens (nostalgebraist, 2020) offers a direct way to project latent embeddings into the LLM’s vocabulary space. It takes an embedding [math] at any LLM layer and projects it through the model’s unembedding matrix [math] , which is the same matrix as used at the final LLM layer to predict vocabulary logits. This gives us a logit vector over the vocabulary of text tokens: U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2} where [math] corresponds to token [math] in the vocabulary [math] . This vector represents the model’s predicted logit distribution over tokens after layer [math] ."}}, "negatives": [{"text": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "context": "Both terms in Eq. (6) are nicely aligned since both are based on increasing the probabilities of generating selected vocabulary tokens, as illustrated in Fig. 1. Formally, since both visual and textual embeddings can be projected into probability distributions over the vocabulary via the Logit Lens, they share a common semantic space. We denote this shared space with [math] of all probability distribution functions (pdfs) [math] over the vocabulary tokens [math] , i.e., \\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7} Let [math] be defined as [math] following the process in Sec. 2.2. Space [math] is a common space to link the visual and language tokens, since [math] and [math] for all visual tokens [math] and text tokens [math]", "negative_type": "hard_same_modal"}}, {"text": "where  \\hat { p } ( v ) is the PPR probability mass of node  v re-normalized over the retrieved set  \\mathcal { V } _ { t o p } (i.e.,  \\begin{array} { r } { \\sum \\hat { p } ( v ) = 1 , } \\end{array} ), and Strength  ( v ) is the weighted degree of the node. A higher  \\boldsymbol { S _ { p p r } } indicates that the PPR result is more reliant on generic, highconnectivity nodes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(f) Change of weather and viewpoint camera  Figure 4: Contextual variations in original photographs. Each pair shows (a) and (b). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/19d5764f6dd344bbf2f17247a09e205722897eae87115599a4e775c6ad851684.jpg", "negative_type": "random", "metadata": {"caption": "(f) Change of weather and viewpoint camera  Figure 4: Contextual variations in original photographs. Each pair shows (a) and (b). ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you use this formula to obtain probability distributions over a vocabulary, and what post-processing step would typically follow the computation of these logits?", "query_type": "application", "positive": {"text": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "context": "We use the Logit Lens (Jiang et al., 2025) to reveal how visual tokens evolve into textual concepts within a VLM. Originally designed to analyze intermediate representations in language models, Logit Lens (nostalgebraist, 2020) offers a direct way to project latent embeddings into the LLM’s vocabulary space. It takes an embedding [math] at any LLM layer and projects it through the model’s unembedding matrix [math] , which is the same matrix as used at the final LLM layer to predict vocabulary logits. This gives us a logit vector over the vocabulary of text tokens: U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2} where [math] corresponds to token [math] in the vocabulary [math] . This vector represents the model’s predicted logit distribution over tokens after layer [math] ."}}, "negatives": [{"text": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "context": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "negative_type": "random"}}, {"text": "\\hat {\\mathbf {F}} _ {a n c} ^ {l} = \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\phi^ {l} \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right). \\tag {5}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\hat {\\mathbf {F}} _ {a n c} ^ {l} = \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\phi^ {l} \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right). \\tag {5}", "context": "\\hat {\\mathbf {F}} _ {a n c} ^ {l} = \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\phi^ {l} \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right). \\tag {5}", "negative_type": "random"}}, {"text": "In this section, we present a comprehensive evaluation of our proposed method against state-of-the-art transfer-based", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does this formula calculate in terms of probability distribution, and why does it use the exponential of logarithms instead of direct values?", "query_type": "semantic", "positive": {"text": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "context": "Given the logits vector obtained from the matrix-vector multiplication [math] , the probability distribution over the vocabulary is computed using the softmax function: p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3} This yields the model’s predicted token probabilities at layer [math] , where [math] denotes the probability of token [math] conditioned on the latent representation [math] .", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, {"text": "consistently outperform other heuristic choices, validating the generality of our strategy across architectures.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/20c5a09a141a08df300b7ab68032b28605089ba4ec660c503809113fd331402e.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the variables θ, h_l(x), V, and logit_j represent in this probability formula?", "query_type": "variable", "positive": {"text": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "context": "Given the logits vector obtained from the matrix-vector multiplication [math] , the probability distribution over the vocabulary is computed using the softmax function: p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3} This yields the model’s predicted token probabilities at layer [math] , where [math] denotes the probability of token [math] conditioned on the latent representation [math] .", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\left\\| \\nabla_ {h _ {L} (s)} L _ {\\mathrm {N T P}} \\right\\| \\leq C _ {\\text {a t t}} \\sum_ {j \\in \\mathcal {T}} \\left\\| \\nabla_ {h _ {L} (j)} L _ {\\mathrm {N T P}} \\right\\| a _ {j s}. \\tag {13}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\left\\| \\nabla_ {h _ {L} (s)} L _ {\\mathrm {N T P}} \\right\\| \\leq C _ {\\text {a t t}} \\sum_ {j \\in \\mathcal {T}} \\left\\| \\nabla_ {h _ {L} (j)} L _ {\\mathrm {N T P}} \\right\\| a _ {j s}. \\tag {13}", "context": "Substituting Eq. (12) into Eq. (11) gives \\left\\| \\nabla_ {h _ {L} (s)} L _ {\\mathrm {N T P}} \\right\\| \\leq C _ {\\text {a t t}} \\sum_ {j \\in \\mathcal {T}} \\left\\| \\nabla_ {h _ {L} (j)} L _ {\\mathrm {N T P}} \\right\\| a _ {j s}. \\tag {13} [math] is capped by [math] for all [math] (ignoring any extra scaling tricks); so it is never exploding. Therefore, the gradient is strongly attenuated, since the attention weights [math] are tiny for most visual tokens [math] (Esmaeilkhani & Latecki, 2025). Layer-wise visual attention analysis is provided in appendix.", "negative_type": "hard_same_modal"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/1c136aa9e21bfafde8d4c032479d7e3fd81174f1db6b67070c1ba09c46b20579.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "context": "We integrate the three modules, SGAI, HVSA, and CLSS, into a unified objective function: \\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8} where λanc, λf eat, and [math] balance anchor guidance, hierarchical visual alignment, and intermediate cross-modal synchronization. We optimize [math] using projected gradient descent under an [math] budget ϵ. At each iteration, the adversarial image is updated using the sign of the gradient and projected back to the feasible [math] ball around [math] , pixel values are also clipped to the valid range. Algorithm 1 summarizes the complete procedure.", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you apply this formula to compute the probability of selecting the third vocabulary item (v_3) given a hidden state representation, assuming you have the logit values for all vocabulary items?", "query_type": "application", "positive": {"text": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "context": "Given the logits vector obtained from the matrix-vector multiplication [math] , the probability distribution over the vocabulary is computed using the softmax function: p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3} This yields the model’s predicted token probabilities at layer [math] , where [math] denotes the probability of token [math] conditioned on the latent representation [math] .", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\mathcal {L} _ {a n c} = 1 - \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right)\\right). \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {a n c} = 1 - \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right)\\right). \\tag {4}", "context": "\\mathcal {L} _ {a n c} = 1 - \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right)\\right). \\tag {4}", "negative_type": "hard_same_modal"}}, {"text": "Let  f : \\mathbb { R } ^ { d }  \\mathcal { F } be defined as  f = \\operatorname { s o f t m a x } \\circ U _ { \\theta } following the process in Sec. 2.2. Space  \\mathcal { F } is a common space to link the visual and language tokens, since  f ( s ) \\in { \\mathcal { F } } and  f ( t ) \\in \\mathcal { F } for all visual tokens  s \\in \\mathcal { P } and text tokens  t \\in \\tau", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\left\\| \\frac {\\partial h _ {L} (j)}{\\partial h _ {L} (s)} \\right\\| \\leq C _ {\\text {a t t}} a _ {j s}, \\tag {12}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\left\\| \\frac {\\partial h _ {L} (j)}{\\partial h _ {L} (s)} \\right\\| \\leq C _ {\\text {a t t}} a _ {j s}, \\tag {12}", "context": "At the final VLM layer, the dependence of a text token [math] on a visual token [math] arises through a cross-attention block. For such a block, the Jacobian of the hidden state satisfies \\left\\| \\frac {\\partial h _ {L} (j)}{\\partial h _ {L} (s)} \\right\\| \\leq C _ {\\text {a t t}} a _ {j s}, \\tag {12} where [math] is the attention weight from text token [math] to visual token [math] , and [math] is a constant depending on the operator norms of the projection matrices [math] and the layer normalization. So, the Jacobian [math] is dominated by the attention weight [math] from token [math] to [math] , times bounded transformation matrices [math] .", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What specific problem does Logit Lens Loss (LLL) address when applying Logit Lens to autoregressive Vision-Language Models?", "query_type": "factual", "positive": {"text": "Logit Lens has been proposed for visualizing tokens that contribute most to the LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly distroyed, which renders Logit Lens’s visualization unusable for explainability. To address this issue, we introduce a complementary loss to the next-token prediction (NTP) to prevent the visual tokens from losing visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word “cat”), without requiring any architectural modification or large-scale training. This way LLL constraints the mixing of image and text tokens in the selfattention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "A key challenge is that intermediate visual and textual features are not directly comparable before projection, since they may differ in dimension and representation space. To make them comparable, we map intermediate features into a shared latent subspace using projection heads available in the surrogate. For each layer  l \\in \\mathcal L , we extract the intermediate CLS visual token  \\mathbf { f } _ { c l s } ^ { l } from the surrogate image encoder, and an intermediate text feature  \\mathbf { t } _ { e o s } ^ { l } from the surrogate text encoder, taken from the EOS token at the corresponding depth.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/908b76acee6299e852e66c96e13e01a55e4d7ed1262dac7c14f55eeb369861b2.jpg", "negative_type": "random", "metadata": {"caption": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "negative_type": "hard_same_modal"}}, {"text": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10}", "context": "where [math] is the one-hot vector for token [math] and vector [math] [math] denotes the softmax probability distribution over the vocabulary: chain rule, [math] . By the \\frac {\\partial \\mathcal {L} _ {\\mathrm {L L L}} (s)}{\\partial h _ {L} (s)} = U ^ {\\top} \\left(p - e _ {v}\\right) = \\sum_ {j} p _ {j} U _ {j} ^ {\\top} - U _ {v} ^ {\\top}, \\tag {10} where scalar [math] is the [math] -th component of [math] , corresponding to the probability assigned to vocabulary token [math] .", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the mixing of image and text tokens in self-attention layers relate to the loss of localized visual information in VLMs?", "query_type": "conceptual", "positive": {"text": "Logit Lens has been proposed for visualizing tokens that contribute most to the LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly distroyed, which renders Logit Lens’s visualization unusable for explainability. To address this issue, we introduce a complementary loss to the next-token prediction (NTP) to prevent the visual tokens from losing visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word “cat”), without requiring any architectural modification or large-scale training. This way LLL constraints the mixing of image and text tokens in the selfattention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "As demonstrated in Table 2, finetuning with LLL in addition to NTP increased the accuracy of “Yes/No” answers on POPE dataset. POPE accuracy measure was introduced in (Li et al., 2023b) as a measure of hallucinations, where the higher accuracy means fewer hallucinations. So, better accuracy coincides with a reduction in object hallucinations and, consequently, better performance on VQA tasks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Evaluations on both open-source and commercial blackbox VLMs confirm that this design yields stronger targeted transferability and maintains robustness under common preprocessing and purification defenses. While highly effective, the improvements can be less pronounced on advanced commercial systems that incorporate proprietary defense policies and complex transformations, motivating future work on more adaptive transfer strategies for such policy-rich black-box settings. Ultimately, these results highlight the need for defenses that explicitly safeguard internal feature hierarchies against semantic-level hijacking.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We integrate Symbolic Anchoring, Dynamic Edge Weighting, and Passage Enhancement to construct a query-adapted graph. Standard PPR (Eq. 1) is executed on this refined structure. The resulting stationary distribution of PPR provides the final passage ranking, prioritizing nodes reachable via semantically relevant reasoning paths.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why might LLL's ability to improve segmentation performance without special heads suggest broader implications for vision-language model architecture design?", "query_type": "inferential", "positive": {"text": "Logit Lens has been proposed for visualizing tokens that contribute most to the LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly distroyed, which renders Logit Lens’s visualization unusable for explainability. To address this issue, we introduce a complementary loss to the next-token prediction (NTP) to prevent the visual tokens from losing visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word “cat”), without requiring any architectural modification or large-scale training. This way LLL constraints the mixing of image and text tokens in the selfattention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "We thank Ying Jin for help with this work. This work was partially supported by the NSF award IIS-2331768.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(f) Query: ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/b87bef94bda1cb9cfad72cab73d341ba0eb84ce2ea19180466c2423b2b23eca2.jpg", "negative_type": "random", "metadata": {"caption": "(f) Query: ", "negative_type": "random"}}, {"text": "where scalar  p _ { j } is the  j -th component of  p , corresponding to the probability assigned to vocabulary token  j .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the three specific Vision-Language Models mentioned as examples in this research paper?", "query_type": "factual", "positive": {"text": "We focus on autoregressive Vision-Language Models (VLMs) likse Llava (Liu et al., 2023), MiniGPT4 (Zhu et al., 2023), and Qwen-VL (Bai et al., 2023). Fig. 1 illustrates their processing flow. The input image (or images)", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "We use the Logit Lens (Jiang et al., 2025) to reveal how visual tokens evolve into textual concepts within a VLM. Originally designed to analyze intermediate representations in language models, Logit Lens (nostalgebraist, 2020) offers a direct way to project latent embeddings into the LLM’s vocabulary space. It takes an embedding  h _ { l } ( x ) \\in \\mathbb { R } ^ { d } at any LLM layer and projects it through the model’s unembedding matrix  U _ { \\theta } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { \\nu } , which is the same matrix as used at the final LLM layer to predict vocabulary logits. This gives us a logit vector over the vocabulary of text tokens:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "the probability of generating token  t _ { i } = < c a t > as the next ground truth token, which is given by  p _ { \\theta } \\left( t _ { i } \\mid \\mathcal { A } _ { a < i } , \\mathbf { X } \\right) = p _ { \\theta } \\left( t _ { i } \\mid h _ { L } ( t _ { i - 1 } ) \\right) in the next-token prediction loss  \\boldsymbol { L } _ { \\mathrm { N T P } } .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(a) Change of scene-composition ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/3e36bfbd23ab5169608f18298ebf183ed1d0816bb781ed7b2e7969974f9f6511.jpg", "negative_type": "random", "metadata": {"caption": "(a) Change of scene-composition ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of Vision-Language Models does this research focus on, and what distinguishes them from other VLM approaches?", "query_type": "conceptual", "positive": {"text": "We focus on autoregressive Vision-Language Models (VLMs) likse Llava (Liu et al., 2023), MiniGPT4 (Zhu et al., 2023), and Qwen-VL (Bai et al., 2023). Fig. 1 illustrates their processing flow. The input image (or images)", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "Dynamic Edge Scoring Schedule. To translate the LLM’s semantic assessment into topological structure, we employ a tiered projection strategy. We define four distinct semantic tiers—Irrelevant, Weak, High, and Direct—and map the discrete LLM scores  s \\in \\{ 0 , \\ldots , 1 0 \\} to specific weight intervals (Table 7). This non-linear mapping acts as a high-pass filter, strictly pruning noise (scores  \\leq 3 ) while exponentially amplifying high-confidence evidence paths.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, {"text": "Fig. 5: Visualization of targeted adversarial attacks on commercial black-box VLMs. We display the adversarial image, the target text, and the actual response generated by the model. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/3b0a69b3b706acc82cfdb21c8a38c51c63b0765b4843a6d4543724742624525a.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 5: Visualization of targeted adversarial attacks on commercial black-box VLMs. We display the adversarial image, the target text, and the actual response generated by the model. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the mention of Figure 1 and input images, what can be inferred about the methodology used to explain these Vision-Language Models?", "query_type": "inferential", "positive": {"text": "We focus on autoregressive Vision-Language Models (VLMs) likse Llava (Liu et al., 2023), MiniGPT4 (Zhu et al., 2023), and Qwen-VL (Bai et al., 2023). Fig. 1 illustrates their processing flow. The input image (or images)", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "[Entity Name] is a [core type/description] known for [key attributes]. It [main relationships/activities] with entities such as [notable connections]...", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "context": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "negative_type": "random"}}, {"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "negative_type": "random", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What types of visual encoders are typically used to extract image tokens before mapping them into language embedding space?", "query_type": "factual", "positive": {"text": "is first passed through a frozen visual encoder (typically a ViT, CLIP, or DINOv2) to extract image tokens and map them into a language embedding space with a linear projection module. Subsequently, the mapped image tokens, often called visual tokens, are used as part of the LLM input, along with the text input (and special tokens). As visual tokens are transformed through the layers of an LLM, their embeddings change. In particular, the embeddings of visual tokens are mixed with the embeddings of language tokens in the attention layers of the LLM module. However, there is a risk that information carried by visual tokens becomes diluted among the predominantly textual tokens as they propagate through the LLM layers. In particular, the information of visual tokens may be transferred to language tokens, as nicely demonstrated in (Kaduri et al., 2025). In an extreme case, the visual tokens are ignored, and the LLM can even hallucinate an image description for an empty image, as was demonstrated in (Liu et al., 2024; Tong et al., 2024). Further evidence that visual tokens are often ignored among all LLM tokens is the fact that after removing half of the visual tokens, the VLM performance does not decrease (Chen et al., 2024). An unintended consequence is that the connection of visual tokens to the content of corresponding image regions is lost in this process. As pointed out in (Fu et al., 2025), The LLM’s ability to use its vision representations is a limiting factor in VLM performance.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "We obtain that minimizing  { \\mathcal { L } } _ { \\mathrm { L L L L } } directly pushes the lastlayer patch representation  h _ { L } ( s ) toward the unembedding direction corresponding to the target vocabulary token  v .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/7b8d345cae457d8ff8d0352846a2217174c7af5ee47880a9a7a217ecdf38c555.jpg", "negative_type": "random", "metadata": {"caption": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "negative_type": "hard_same_modal"}}, {"text": "Standard dense retrieval methods, which select document chunks based on semantic similarity (Izacard et al., 2022a), frequently fail in multi-hop reasoning scenarios when the answer relies on connecting disjoint facts. To overcome this limitation, recent research has shifted towards Structure-Aware RAG, which organizes information into hierarchical trees (Sarthi et al., 2024) or global knowledge graphs (Guo et al., 2025) to capture longrange dependencies. Among these, the HippoRAG framework (Gutiérrez et al., 2024, 2025) distinguishes itself by leveraging Personalized PageRank (PPR) over Knowledge Graphs. HippoRAG simulates neurobiological memory mechanism, enabling deeper and more efficient knowledge integration that vector similarity alone cannot resolve.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the mixing of visual and language token embeddings in attention layers contribute to the dilution of visual information in vision-language models?", "query_type": "conceptual", "positive": {"text": "is first passed through a frozen visual encoder (typically a ViT, CLIP, or DINOv2) to extract image tokens and map them into a language embedding space with a linear projection module. Subsequently, the mapped image tokens, often called visual tokens, are used as part of the LLM input, along with the text input (and special tokens). As visual tokens are transformed through the layers of an LLM, their embeddings change. In particular, the embeddings of visual tokens are mixed with the embeddings of language tokens in the attention layers of the LLM module. However, there is a risk that information carried by visual tokens becomes diluted among the predominantly textual tokens as they propagate through the LLM layers. In particular, the information of visual tokens may be transferred to language tokens, as nicely demonstrated in (Kaduri et al., 2025). In an extreme case, the visual tokens are ignored, and the LLM can even hallucinate an image description for an empty image, as was demonstrated in (Liu et al., 2024; Tong et al., 2024). Further evidence that visual tokens are often ignored among all LLM tokens is the fact that after removing half of the visual tokens, the VLM performance does not decrease (Chen et al., 2024). An unintended consequence is that the connection of visual tokens to the content of corresponding image regions is lost in this process. As pointed out in (Fu et al., 2025), The LLM’s ability to use its vision representations is a limiting factor in VLM performance.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "conduct this evaluation on a subset of 100 randomly sampled image-text pairs. To ensure a fair and rigorous assessment against these strongly aligned models, we adopt the experimental protocols established in M-Attack [15], utilizing an ensemble of three CLIP visual encoders as surrogates.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Table 3 presents zero-shot evaluation results on the PixMo-Points subset. All evaluations are performed in pixel space, with coordinates parsed from the model’s textual output using a fixed prompt template. We report the median distance in pixels. Both base models, LLaVA-v1.5 and Qwen2.5- VL, show limited ability to localize object centers without grounding-oriented supervision. Incorporating LLL during fine-tuning substantially improves both models, reducing median localization error. Notably, Qwen2.5-VL with LLL achieves a lower median distance than several proprietary and task-specific baselines, and it outperforms Molmo-7B, which was trained with explicit coordinate-level supervision on PixMo. These results indicate that LLL enables strong transfer of spatial grounding learned from RefCOCO to previously unseen object categories in PixMo", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Evaluating the semantic relevance of every edge using an LLM is computationally prohibitive. Therefore, we first apply a topological filter to constrain the search space to the most plausible local neighborhoods. We define two hyperparameters: the maximum number of seed entities  N _ { s e e d } and the maximum number of edges per seed  K _ { e d g e } for finegrained alignment. First, we select the top-  . N _ { s e e d } entity nodes based on their initial reset probabilities (derived from the dense retrieval alignment). Let  u be such a selected seed. For the seed phrase  u within top-  . N _ { s e e d } , if the number of outgoing relation edges exceeds a threshold  K _ { e d g e } , we prune its outgoing edges by prioritizing the top-  K _ { e d g e } neighbors based on the vector similarity between the query embedding and fact embeddings of relation edges. Neighbors  v \\not \\in \\mathcal { N } _ { t o p } ( u ) are bypassed by the scoring module and assigned a minimal Weak weight. This step acts as a low-pass structural filter, discarding statistically improbable paths before the intensive semantic scoring.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Given that removing half of the visual tokens doesn't decrease VLM performance and that LLMs can hallucinate descriptions for empty images, what does this suggest about the fundamental architecture limitations of current vision-language models?", "query_type": "inferential", "positive": {"text": "is first passed through a frozen visual encoder (typically a ViT, CLIP, or DINOv2) to extract image tokens and map them into a language embedding space with a linear projection module. Subsequently, the mapped image tokens, often called visual tokens, are used as part of the LLM input, along with the text input (and special tokens). As visual tokens are transformed through the layers of an LLM, their embeddings change. In particular, the embeddings of visual tokens are mixed with the embeddings of language tokens in the attention layers of the LLM module. However, there is a risk that information carried by visual tokens becomes diluted among the predominantly textual tokens as they propagate through the LLM layers. In particular, the information of visual tokens may be transferred to language tokens, as nicely demonstrated in (Kaduri et al., 2025). In an extreme case, the visual tokens are ignored, and the LLM can even hallucinate an image description for an empty image, as was demonstrated in (Liu et al., 2024; Tong et al., 2024). Further evidence that visual tokens are often ignored among all LLM tokens is the fact that after removing half of the visual tokens, the VLM performance does not decrease (Chen et al., 2024). An unintended consequence is that the connection of visual tokens to the content of corresponding image regions is lost in this process. As pointed out in (Fu et al., 2025), The LLM’s ability to use its vision representations is a limiting factor in VLM performance.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "T ARGE Vision-Language Models [1] have emerged as L a foundational technology, powering critical applications from autonomous agents to content moderation [2]– [4]. As these systems are increasingly entrusted with highstakes decision-making [5], [6], their inherent vulnerability to adversarial perturbations poses severe security risks [7]. Beyond causing benign classification errors [8], imperceptible perturbations can effectively hijack a model’s semantic understanding, enabling adversaries to circumvent safety guardrails (i.e., jailbreaking) and elicit policy-violating outputs [9]. In the black-box scenario, targeted transfer-based attacks [10] represent a particularly practical threat. By optimizing on an accessible surrogate, attackers can precisely steer proprietary", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Strict Reasoning Completeness Evaluation. While standard metrics indicate general relevance, they often mask a critical failure mode in multi-hop retrieval: the loss of intermediate \"bridge\" documents that connect disjoint facts. To assess the recovery of the full evidence paths, we evaluate FCR and JSR in Table 4. CatRAG effectively mitigates probability dilution, achieving an FCR of  34 . 6 \\% compared to  3 0 . 5 \\% for HippoRAG 2. The gain is most pronounced on HoVer, where precise 3–4 hop claim verification is required. CatRAG improves JSR to  3 1 . 1 \\% , a relative gain of  1 8 . 7 \\% over the HippoRAG2. These results confirm that our dynamic steering successfully anchors the traversal to the specific bridge documents required for grounded reasoning.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We thank Ying Jin for help with this work. This work was partially supported by the NSF award IIS-2331768.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the total number of samples in the DET type dataset?", "query_type": "factual", "positive": {"text": "Table 1. The overview of ObjEmbed training dataset. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/120ffad156187adf11f4108680e45a1cf7ee3c11605469ca52e8372d40e6daa1.jpg", "metadata": {"caption": "Table 1. The overview of ObjEmbed training dataset. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE V: Defense-aware black-box attacks against victim VLMs. We evaluate Ens.,  \\mathrm { A S R } _ { \\mathrm { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } under four preprocessing-based defenses: Bit Reduction, JPEG Compression, ComDefend, and DiffPure. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e4d34d340bba0793995839925b70fd4839d82698d1ffa465b25cb2982bb47853.jpg", "negative_type": "random", "metadata": {"caption": "TABLE V: Defense-aware black-box attacks against victim VLMs. We evaluate Ens.,  \\mathrm { A S R } _ { \\mathrm { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } under four preprocessing-based defenses: Bit Reduction, JPEG Compression, ComDefend, and DiffPure. ", "negative_type": "random"}}, {"text": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "context": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "negative_type": "random"}}, {"text": "(c) Change of illumination time and mood ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/fbdac410498d70ef899a4a35340d89309f279836077f030acdd116283325b314.jpg", "negative_type": "random", "metadata": {"caption": "(c) Change of illumination time and mood ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which type has the largest number of samples: DET, REC, or SUM?", "query_type": "comparative", "positive": {"text": "Table 1. The overview of ObjEmbed training dataset. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/120ffad156187adf11f4108680e45a1cf7ee3c11605469ca52e8372d40e6daa1.jpg", "metadata": {"caption": "Table 1. The overview of ObjEmbed training dataset. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "(e) Query: ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/b956ea3a579e33871d20b057c2ba58c6b975698dcfaeb56150e983e2621f1d1d.jpg", "negative_type": "random", "metadata": {"caption": "(e) Query: ", "negative_type": "random"}}, {"text": "In comparison, while COA exhibits comparable fooling rates, its performance in targeted attacks decreases markedly under defense mechanisms. We attribute this to COA’s reliance on high-frequency priors from an auxiliary network, which tends to yield high-frequency noise patterns. While effective", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the total number of samples across all three dataset types (DET, REC, and SUM)?", "query_type": "computational", "positive": {"text": "Table 1. The overview of ObjEmbed training dataset. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/120ffad156187adf11f4108680e45a1cf7ee3c11605469ca52e8372d40e6daa1.jpg", "metadata": {"caption": "Table 1. The overview of ObjEmbed training dataset. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "negative_type": "random", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}, {"text": "Recent frontier text-to-image systems, including OpenAI’s GPT-4o (OpenAI (2025a,b)) and Google’s Gemini image models (Google (2025)), exhibit substantial advances in instruction following, compositional reasoning, and visual consistency. The outputs of such systems, however, are typically governed by restrictive usage terms that prohibit their use for training or developing competing models. While recent open-source efforts such as Z-image (Team (2025b)) and Qwen-Image/Edit (Wu et al. (2025)) represent important", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "In contrast to large-scale image–text datasets such as LAION-5B Schuhmann et al. (2022a), which prioritize scale and coverage, and web-harvested caption datasets such as Conceptual Captions and YFCC100M Sharma et al. (2018); Thomee et al. (2016), which lack explicit supervision for identity or contextual consistency, Lunara Aesthetic II is explicitly constructed around anchor-linked, identity-preserving variation. Instructionbased image editing benchmarks, including InstructPix2Pix and MagicBrush Brooks et al. (2023); Zhang et al. (2023), introduce text-guided edits and largely consist of isolated, single-step transformations. Lunara Aesthetic II is designed to unify high aesthetic quality with structured, identity-preserving contextual variation, enabling principled training and evaluation of controlled generalization in image generation and editing models.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the AP score for DINO-R50 on the COCO dataset for medium objects (AP_m)?", "query_type": "factual", "positive": {"text": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4c4a0fe40380c638454206d543e835e8a75efba86610dce02362d98ad8d42ecc.jpg", "metadata": {"caption": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "negative_type": "random"}}, "negatives": [{"text": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/7b8d345cae457d8ff8d0352846a2217174c7af5ee47880a9a7a217ecdf38c555.jpg", "negative_type": "random", "metadata": {"caption": "TABLE VII: Average attack runtime (seconds) per optimization step and per adversarial image. ", "negative_type": "hard_same_modal"}}, {"text": "LLL to NTP leads to a threefold increase in object confidence score ratio. This significant result confirms the huge impact on LLL on the object confidence maps shown in Fig. 2. Moreover, the fact that NTP did not improve the ratio confirms the fact that NTP provides only a weak supervisory signal for visual cues during fine-tuning. The results were obtained after finetuning on POPE.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Target：A cat sitting between a window and blinds. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e9e1ca3aef8e42df8bcbf059d58c917208b88824d24dbd0a742f31d61be854aa.jpg", "negative_type": "random", "metadata": {"caption": "Target：A cat sitting between a window and blinds. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method achieves the highest overall AP score on the COCO dataset among all the methods listed?", "query_type": "comparative", "positive": {"text": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4c4a0fe40380c638454206d543e835e8a75efba86610dce02362d98ad8d42ecc.jpg", "metadata": {"caption": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "negative_type": "random"}}, "negatives": [{"text": "Table 5: Ablations. We report passage recall  \\textcircled { \\alpha } 5 on multi-hop benchmarks using several alternatives to our final design in dynamic update. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/46eccb8a5ee0cace49b092233169f9bffa061678dcd0a0ac068109ce0715391a.jpg", "negative_type": "random", "metadata": {"caption": "Table 5: Ablations. We report passage recall  \\textcircled { \\alpha } 5 on multi-hop benchmarks using several alternatives to our final design in dynamic update. ", "negative_type": "hard_same_modal"}}, {"text": "where  \\hat { p } ( v ) is the PPR probability mass of node  v re-normalized over the retrieved set  \\mathcal { V } _ { t o p } (i.e.,  \\begin{array} { r } { \\sum \\hat { p } ( v ) = 1 , } \\end{array} ), and Strength  ( v ) is the weighted degree of the node. A higher  \\boldsymbol { S _ { p p r } } indicates that the PPR result is more reliant on generic, highconnectivity nodes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "ObjEmbed is finetuned from Qwen3-VL-Instruct (Bai et al., 2025a) by first initializing the object projector following WeDetect-Ref (Fu et al., 2025a) and then training under the objective described in Equation (4). The model is trained using 16 GPUs, with a batch size of 2 images per GPU, and an initial learning rate of  2 e ^ { - 5 } . All parameters are updated during training except those in the frozen vision encoder. Training proceeds for two epochs. The loss weights  \\lambda _ { 1 } ,  \\lambda _ { 2 } , and  \\lambda _ { 3 } are set to 1.0, 1.0 and 0.25. Input images are resized such that the number of visual tokens ranges from 900 to 1200, corresponding to  9 0 0 ^ { * } 3 2 ^ { * } 3 2 to  1 2 0 0 ^ { * } 3 2 ^ { * } 3 2 pixels, ensuring adaptive computation based on image content.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the difference between ObjEmbed-2B and ObjEmbed-4B's performance on FG-OVD hard difficulty?", "query_type": "computational", "positive": {"text": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4c4a0fe40380c638454206d543e835e8a75efba86610dce02362d98ad8d42ecc.jpg", "metadata": {"caption": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "negative_type": "random"}}, "negatives": [{"text": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/908b76acee6299e852e66c96e13e01a55e4d7ed1262dac7c14f55eeb369861b2.jpg", "negative_type": "random", "metadata": {"caption": "Table 8. Ablation studies on different training objectives. LIR∗ denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks.  Table 9. Ablation studies on image-level supervision design. ‘Share’ denotes whether global text embeddings and local text embeddings share the same special token. ‘#token’ denotes the number of global image tokens and ‘Type’ can be a single long caption (long), a single short caption (short), a single randomly selected caption (mix), or two captions used simultaneously (both). ‘LIR’ denotes the average scores of local image retrieval tasks. ‘GIR’ denotes the average scores of global image retrieval tasks. ", "negative_type": "hard_same_modal"}}, {"text": "Two people in dark coats and face masks share a black umbrella while walking on a snowy sidewalk next to a city street.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Recent frontier text-to-image systems, including OpenAI’s GPT-4o (OpenAI (2025a,b)) and Google’s Gemini image models (Google (2025)), exhibit substantial advances in instruction following, compositional reasoning, and visual consistency. The outputs of such systems, however, are typically governed by restrictive usage terms that prohibit their use for training or developing competing models. While recent open-source efforts such as Z-image (Team (2025b)) and Qwen-Image/Edit (Wu et al. (2025)) represent important", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the Top-1 accuracy of InternVL3.5-38B on the RefCOCO+ testA dataset?", "query_type": "factual", "positive": {"text": "Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/091b766ac2ac0842a8d66069ee1f28c176be2290c6b773a4debf02c3b534918a.jpg", "metadata": {"caption": "Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. "}}, "negatives": [{"text": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/30eb71dbd58b6ba814e5cc9d8492159182c8ee704ea7b9c6c5705bb9e41364a4.jpg", "negative_type": "random", "metadata": {"caption": "Table 1: Summary statistics of the image-variation dataset and frequency of contextual change labels. ", "negative_type": "hard_same_modal"}}, {"text": "Fig. 8: Validation of the strategy on different architectures. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/7f9f4564945b6c1dd5820acea2a0eca05fb2bb6a405f5ee9553d012c77dbec12.jpg", "negative_type": "random", "metadata": {"caption": "Fig. 8: Validation of the strategy on different architectures. ", "negative_type": "random"}}, {"text": "In practice, modality alignment is commonly implemented via a learnable projector (or adapter) that connects the visual encoder to the LLM. Two representative design families are widely adopted. The first family, exemplified by LLaVA [3] and MiniGPT-4 [27], uses lightweight mappings such as linear layers or MLPs to project visual tokens into the LLM embedding space, offering simplicity and computational efficiency. The second family, used by BLIP-2 [28] and InstructBLIP [29], introduces a query-based module (e.g., Q-Former) that employs learnable queries and cross-attention to selectively aggregate visual information before interfacing with the LLM. In this work, we study adversarial transferability across these architectures, with particular attention to how their alignment modules and intermediate representations influence vulnerability and generalization of attacks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which model achieves the highest average accuracy across all datasets and test sets?", "query_type": "comparative", "positive": {"text": "Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/091b766ac2ac0842a8d66069ee1f28c176be2290c6b773a4debf02c3b534918a.jpg", "metadata": {"caption": "Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. "}}, "negatives": [{"text": "Table 4. Evaluation results on local image retrieval datasets. The evaluation metric for SORCE-1K and REIRCOCO is Recall  @ 1 while the evaluation metric for ILIAS is mAP@50, a metric evaluating the top 50 predictions. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/2a402e36ccd504767c6a956368b3e2a1dc2c33d36cc6873451c58802810e9ddb.jpg", "negative_type": "random", "metadata": {"caption": "Table 4. Evaluation results on local image retrieval datasets. The evaluation metric for SORCE-1K and REIRCOCO is Recall  @ 1 while the evaluation metric for ILIAS is mAP@50, a metric evaluating the top 50 predictions. ", "negative_type": "hard_same_modal"}}, {"text": "Anchor Settings  K and  \\tau ). We investigate the influence of the anchor count  K and temperature  \\tau on the UniDiffuser. As shown in Fig. 6, introducing even a single anchor  K = 1 ) boosts the Ensemble CLIP Score from the baseline 0.6970 to 0.7418. The performance peaks at  K = 5 but degrades slightly with larger  K , as excessive anchors introduce less relevant anchors. Regarding  \\tau , extreme values lead to suboptimal weighting: small  \\tau yields an overly sharp distribution that underutilizes the semantic diversity of the anchor set, while large  \\tau produces an overly uniform distribution, failing to emphasize the most semantically relevant anchors. The results find  \\tau = 5 as the optimal balance. Thus, we adopt  K = 5 and  \\tau = 5 as the default settings.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/6ad2f7076f02a0f4d64eb9596150789143764f481ce4816594fa8b61211e1341.jpg", "negative_type": "random", "metadata": {"caption": "Table 2: Retrieval Performance (Recall  \\boldsymbol { \\ @ 5 } ). Retrieval performance on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results.  Table 3: Downstream QA Performance. QA performance on multi-hop QA and fact verification datasets using Llama-3.3-70B-Instruct as the QA reader. We report F1 for QA datasets, accuracy for the HoVer dataset. * denotes the results from (Gutiérrez et al., 2025). ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the difference in average accuracy between ObjEmbed-4B and ChatRex 7B?", "query_type": "computational", "positive": {"text": "Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/091b766ac2ac0842a8d66069ee1f28c176be2290c6b773a4debf02c3b534918a.jpg", "metadata": {"caption": "Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. "}}, "negatives": [{"text": "Table 6. Ablation studies on different object token designs. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/c0109746b758d382be093f6db84eef6792741078abc5bdaf8c819351b2919ca3.jpg", "negative_type": "random", "metadata": {"caption": "Table 6. Ablation studies on different object token designs. ", "negative_type": "hard_same_modal"}}, {"text": "The same VLM was used to identify contextual differences between each original–variation pair, followed by manual verification and refinement to ensure that visual contextual variations were accurately captured in the final labels. Finally, These variation outputs were manually filtered to retain 2.8K images exhibiting meaningful contextual changes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the overall shape and structure of this performance comparison visualization?", "query_type": "descriptive", "positive": {"text": "Figure 1. ObjEmbed achieves balanced and superior performance across a wide span of benchmarks. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/da2363a13a82d54e70b1dc3d8abc6576f4d8d24264783c3a07ef25e1fd83663e.jpg", "metadata": {"caption": "Figure 1. ObjEmbed achieves balanced and superior performance across a wide span of benchmarks. "}}, "negatives": [{"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/95cdf8052a61aad6d0c7185f2958289b0f8d50a0507fb4913dc15871502e9540.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "Impact of Layer Selection (UniDiffuser) ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/c5c371321129f96b16a47bab9c8a21625e7df6af637749ed26b9603f078f2eca.jpg", "negative_type": "random", "metadata": {"caption": "Impact of Layer Selection (UniDiffuser) ", "negative_type": "random"}}, {"text": "We aim to address this issue by constraining the mixing of visual and language tokens in order to prevent", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which model is represented by the blue line that forms the outermost boundary across most benchmarks?", "query_type": "identification", "positive": {"text": "Figure 1. ObjEmbed achieves balanced and superior performance across a wide span of benchmarks. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/da2363a13a82d54e70b1dc3d8abc6576f4d8d24264783c3a07ef25e1fd83663e.jpg", "metadata": {"caption": "Figure 1. ObjEmbed achieves balanced and superior performance across a wide span of benchmarks. "}}, "negatives": [{"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/1c136aa9e21bfafde8d4c032479d7e3fd81174f1db6b67070c1ba09c46b20579.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "directly in the vocabulary space, enabling stronger visual– language alignment within standard VLM backbones. Although we use bounding boxes as lightweight supervision to indicate which visual tokens correspond to an object during training, LLL does not introduce any detection heads or localization modules, and the original VLM architecture remains unchanged. This separates our approach from prior work and is clearly reflected in the object confidence maps shown in Fig. 2, where LLL produces significantly more accurate and interpretable localization signals.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "To address these divergent requirements and mitigate potential task conflicts, we introduce task-specific instructions to guide the model in generating context-aware embeddings tailored to each task. The instructions used during training are listed as follows:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the radar chart patterns, what can be inferred about ObjEmbed-4B's performance consistency compared to other models across different benchmark categories?", "query_type": "interpretive", "positive": {"text": "Figure 1. ObjEmbed achieves balanced and superior performance across a wide span of benchmarks. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/da2363a13a82d54e70b1dc3d8abc6576f4d8d24264783c3a07ef25e1fd83663e.jpg", "metadata": {"caption": "Figure 1. ObjEmbed achieves balanced and superior performance across a wide span of benchmarks. "}}, "negatives": [{"text": "ChatGPT ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0b170a051fd8530bd76ed8e3463c3fad0068920b2738ac88ba8ac0a2508b1f12.jpg", "negative_type": "random", "metadata": {"caption": "ChatGPT ", "negative_type": "hard_same_modal"}}, {"text": "We evaluate image aesthetics using the LAION Aesthetics v2 predictor, a CLIP-based model trained to approximate aggregate human judgments of visual appeal. We compare Lunara-AIV against several widely used vision–language datasets, including Lunara Art", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Multi-hop Fact Verification. We extend our evaluation to the HoVer dataset (Jiang et al., 2020) to test the robustness of our model in a claim verification setting. HoVer is adapted from HotpotQA but increases reasoning complexity by substituting named entities in the original claims with details from linked Wikipedia articles, thereby extending the reasoning chain to 3 and 4 hops. This substitution process creates deep, fragile reasoning chains where a single missed retrieval step results in failure. Following the protocol in HippoRAG, we randomly sample 1,000 claims from the dataset (specifically 3 and 4 hops) and form the retrieval corpus by collecting all candidate passages (supporting evidence and distractors) associated with the original lineage questions of selected claims.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the main components that make up the ObjEmbed architecture shown in the figure?", "query_type": "descriptive", "positive": {"text": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/e44ceccc00f64484610e3c3299292db91889128d682b9b6f0fcf2dbaea7b1c63.jpg", "metadata": {"caption": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 10", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/70c333b0acefb8cf2d30c89511f545845c8e6b151c9baf201c600a67dcb0bdce.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "Generate a concise, entity-focused summary that captures the core identity and key relationships of a given entity based on its associated fact triplets.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Representing text queries as embeddings. Finally, we introduce a local text token ⟨local text⟩ and a global text token ⟨global text⟩ to represent text queries. The local text token is used to match object embeddings while the global text token is used to match global image embeddings. The encoding templates are “Find an object that matches the given caption. CAPTION ⟨local text⟩” and “Find an image that matches the given caption. CAPTION ⟨global text⟩”. We use the same model to encode text and visual embeddings.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which component in the architecture is responsible for converting visual features into object embeddings using RoI features?", "query_type": "identification", "positive": {"text": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/e44ceccc00f64484610e3c3299292db91889128d682b9b6f0fcf2dbaea7b1c63.jpg", "metadata": {"caption": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/d8de03bd2fe4be2558ba6a5f3154949f3bc9dbdd2bfbe525cb6b71b270a6f722.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "Adversarial attacks aim to mislead models via imperceptible input perturbations. Depending on the adversary’s knowledge, they are classified as white-box (full parameter access) [30], [31] or black-box (no access to model parameters or gradients) [32], [33]. Regarding objectives, attacks distinguish between untargeted ones that merely degrade performance and targeted ones that steer outputs toward specific malicious goals, posing greater safety risks. In realistic black-box scenarios, transfer-based attacks [34]–[36] have emerged as the dominant paradigm over query-based methods [37], as the latter are often impractical for large VLMs due to prohibitive inference latency and API costs. Consequently, our work focuses on transfer-based targeted attacks, which optimize perturbations on an accessible surrogate (e.g., CLIP) to compromise unseen victim VLMs via cross-model transferability.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "context": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the single-tower design of ObjEmbed enable efficient processing compared to traditional dual-tower approaches for vision-language tasks?", "query_type": "interpretive", "positive": {"text": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/e44ceccc00f64484610e3c3299292db91889128d682b9b6f0fcf2dbaea7b1c63.jpg", "metadata": {"caption": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a30e7f526e4c349069796b721d190b4b999a613d34cd1ecee346284ea52fc89d.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "using the top-5 retrieved passages. Key hyperparameters include: symbolic anchor reset probability  \\epsilon = 0 . 2 (weighted by inverse passage count  | P _ { i } | ^ { - 1 } ) , boost factor  \\beta \\ : = \\ : 2 . 5 , dynamic weighting limits  N _ { s e e d } = 5 and  K _ { e d g e } \\mathrm { ~ = ~ } 1 5 . More implementation details and hyperparameters are provided in Appendix A.1.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "The same VLM was used to identify contextual differences between each original–variation pair, followed by manual verification and refinement to ensure that visual contextual variations were accurately captured in the final labels. Finally, These variation outputs were manually filtered to retain 2.8K images exhibiting meaningful contextual changes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the primary jersey colors worn by the two teams in this soccer match?", "query_type": "descriptive", "positive": {"text": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/23926d8a91fdf9d8f7cff6709a58a8d68de2b60080cf1bc1b189a7cdd05329b8.jpg", "metadata": {"caption": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "negative_type": "random"}}, "negatives": [{"text": "(b) NTP ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/4cdef342aaa29d0a244047b907abb430d106e92ffc514a06c16fff0a2496b84f.jpg", "negative_type": "random", "metadata": {"caption": "(b) NTP ", "negative_type": "hard_same_modal"}}, {"text": "Evaluation metrics. Following standard protocols [13], [14], we employ CLIP-Score [13] to measure semantic consistency and the LLM-based (GPT-4) Attack Success Rate (ASR) [14] to assess effectiveness, specifically utilizing  \\mathrm { \\bf A S R _ { f o o l } } for untargeted attack success and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } for targeted attack success. Additionally, we utilize Structural Similarity (SSIM) [50] and Peak Signal-to-Noise Ratio (PSNR) to evaluate the visual imperceptibility of the generated adversarial examples.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(ori) The image shows a close up of a car wheel with a white background ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/12b4116103bc22881c42a890428ac93458a96d423e41a8e8b27246d1287057ad.jpg", "negative_type": "random", "metadata": {"caption": "(ori) The image shows a close up of a car wheel with a white background ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which player is wearing jersey number 33 and what team does he appear to be playing for based on his uniform color?", "query_type": "identification", "positive": {"text": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/23926d8a91fdf9d8f7cff6709a58a8d68de2b60080cf1bc1b189a7cdd05329b8.jpg", "metadata": {"caption": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "negative_type": "random"}}, "negatives": [{"text": "Figure 4. Bar plots illustrating the ratio of the average attention of target tokens within the bounding box of the target object w.r.t. the answer token to the average attention across all visual tokens. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c7076a1399e9363f345fd51225e393b1c9f524c62886506d12e1b1d1002e4d95.jpg", "negative_type": "random", "metadata": {"caption": "Figure 4. Bar plots illustrating the ratio of the average attention of target tokens within the bounding box of the target object w.r.t. the answer token to the average attention across all visual tokens. ", "negative_type": "hard_same_modal"}}, {"text": "where the NTP loss gradient is summed over text tokens  \\tau because only text tokens receive the language-model", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(c) Change of illumination time and mood ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/fbdac410498d70ef899a4a35340d89309f279836077f030acdd116283325b314.jpg", "negative_type": "random", "metadata": {"caption": "(c) Change of illumination time and mood ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the positioning and body language of the players in this scene, what type of play situation appears to be developing?", "query_type": "interpretive", "positive": {"text": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/23926d8a91fdf9d8f7cff6709a58a8d68de2b60080cf1bc1b189a7cdd05329b8.jpg", "metadata": {"caption": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "negative_type": "random"}}, "negatives": [{"text": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/415b8f34a76d2c3e8943c0a8e01fc6eda9e47ee2a264b555f97b1a9e624e44fa.jpg", "negative_type": "random", "metadata": {"caption": "Figure 1: Comparison of graph traversal between HippoRAG 2 and CatRAG. We illustrate the retrieval process for the multi-hop query “Which university did Marie Curie’s doctoral advisor attend?”. In HippoRAG 2 (top), the static graph structure causes semantic drift; probability mass is diverted to high-weight generic edges (e.g., Marie Curie → Radioactivity), missing the downstream evidence ENS. CatRAG (bottom) prevents this by applying (1) Symbolic Anchoring, injecting \"University\" as a weak seed, (2) Query-Aware Dynamic Edge Weighting amplifying relevant paths (e.g., Attend in ENS) while pruning irrelevant ones, and (3) Key-Fact Passage Weight Enhancement to strength, boosting relevant context edge. This steers the random walk to successfully retrieve the complete evidence chain for ENS. ", "negative_type": "hard_same_modal"}}, {"text": "Given the logits vector obtained from the matrix-vector multiplication  U _ { \\theta } h _ { l } ( x ) , the probability distribution over the vocabulary is computed using the softmax function:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "A fundamental limitation of static graph retrieval is Hub Bias (or degree centrality bias). In standard", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What mathematical relationship does the formula s_{ij} calculate between the vectors e_p^j and e_t^i?", "query_type": "semantic", "positive": {"text": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "context": "sion, a choice commonly adopted in object detection. Specifically, for each object description [math] , we treat each region proposal [math] as a positive sample if [math] , and negative otherwise. We compute the similarity between the proposal object embedding [math] and the local text embedding [math] of [math] as: s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1} Then, the similarities are optimized via sigmoid focal loss:"}}, "negatives": [{"text": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "context": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "negative_type": "hard_same_modal"}}, {"text": "Looking forward, Lunara-II provides a foundation for several extensions, including expanding contextual factor coverage, increasing variation depth per anchor, and developing standardized evaluation protocols that explicitly score identity consistency, edit specificity, and contextual correctness under controlled transformations. More broadly, relationally structured datasets of this form may help establish clearer empirical signals for when generative models genuinely acquire contextual concepts, rather than relying on superficial correlations.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "A person in a dark military uniform with a peaked cap stands in the foreground on a paved airfield, holding a water bottle. Behind them, a gray single-engine jet fighter with its cockpit canopy open is parked. A yellow ladder leans against the aircraft, and at least two other individuals are visible near the plane. The background consists of a grassy field and a line of trees under an overcast sky.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the variables e_p^j and e_t^i likely represent in this formula, given their superscript notation?", "query_type": "variable", "positive": {"text": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "context": "sion, a choice commonly adopted in object detection. Specifically, for each object description [math] , we treat each region proposal [math] as a positive sample if [math] , and negative otherwise. We compute the similarity between the proposal object embedding [math] and the local text embedding [math] of [math] as: s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1} Then, the similarities are optimized via sigmoid focal loss:"}}, "negatives": [{"text": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "context": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "negative_type": "hard_same_modal"}}, {"text": "We analyze the conditional co-occurrence structure of contextual variation labels in the dataset (see Figure 2). Substantial off-diagonal mass indicates non-trivial dependencies between change types. Illumination–time emerges as a central factor, exhibiting consistently high conditional probabilities with mood–atmosphere, object–composition, viewpoint–camera, and weather–atmosphere changes (approximately 0.53–0.59), suggesting that lighting variations frequently accompany other contextual edits. Scene-level and atmospheric categories show moderate coupling with one another (roughly 0.33–0.48), reflecting coordinated broad-level modifications. Color–tone displays selective coupling—most notably with illumination–time ( 0.49), while remaining more weakly associated with other categories. Viewpoint–camera changes exhibit moderate associations with object–composition and weather–atmosphere, indicating that geometric adjustments often co-occur with broader scene alterations. Overall, these patterns reveal structured, non-uniform dependencies among contextual variations, highlighting the compositional nature of the dataset.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Representing objects as a sequence of embeddings. Following WeDetect-Ref (Fu et al., 2025a), we first use a universal proposal generator, WeDetect-Uni (Fu et al., 2025a), to generate top-N proposals (100 proposals in this work) for each image. Each RoI feature is extracted via RoIAlign and then compressed into a single token via an object projector. These object features will replace the ⟨object⟩ tokens and are organized sequentially before sending to the large language model. Each object will be separated by prompts “Object i: ⟨object⟩” to ensure distinctiveness. By representing objects as a sequence of embeddings, the model can encode all objects simultaneously with high efficiency.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you compute s_{ij} if e_p^j = [3, 4] and e_t^i = [1, 2], and what would be the range of possible values for this result?", "query_type": "application", "positive": {"text": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "context": "sion, a choice commonly adopted in object detection. Specifically, for each object description [math] , we treat each region proposal [math] as a positive sample if [math] , and negative otherwise. We compute the similarity between the proposal object embedding [math] and the local text embedding [math] of [math] as: s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1} Then, the similarities are optimized via sigmoid focal loss:"}}, "negatives": [{"text": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "context": "\\mathcal {L} _ {\\mathrm {L L L}} (s) = - \\log p _ {\\theta} (v \\mid h _ {L} (s)). \\tag {8}", "negative_type": "hard_same_modal"}}, {"text": "targeted attacks. We conduct experiments across six representative open-source VLMs ranging from 1.4B to 72.3B parameters, including UniDiffuser, BLIP-2, InstructBLIP, MiniGPT-4, LLaVA, and LLaVA-NeXT. To ensure a consistent evaluation setting, we employ a unified prompt: “What is the content of the image?” during the inference phase.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Analyze the given image and generate multiple detailed descriptions for the given object (’instance’) found in the image. Each description must be accurate and unique, focusing solely on the information available in the image and annotations. Do not include any information that is not explicitly present in the image or annotation data. The descriptions should ensure that the object can be uniquely identified from a large set of similar images.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the formula L_region calculate in terms of the overall loss function?", "query_type": "semantic", "positive": {"text": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "context": "Then, the similarities are optimized via sigmoid focal loss: \\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2} where [math] if [math] , and 0 otherwise. [math] is the number of annotations and [math] is the number of proposals."}}, "negatives": [{"text": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "context": "We propose to use the [math] as the auxiliary loss in addition to the next token prediction loss [math] of the VLM to optimize the VLM’s parameters for both objectives: L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6} We empirically set [math] to 0.5, which we found effective across all tasks. Like [math] , the proposed [math] is only applied to the last VLM layer, i.e., [math] in Eq. (4). Moreover, after finetuning with LLL, it is also sufficient to apply Logit Lens to the last layer. In contrast, in (Jiang et al., 2025), Logit Lens is computed as the maximum over all layers.", "negative_type": "hard_same_modal"}}, {"text": "Output ONE line per neighbor: ‘ID (Entity Name): Score | (Reasoning if Score  > = ~ 4 \\AA _ { e } )‘", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "\\left. \\nabla _ { h _ { L } ( j ) } L _ { \\mathrm { N T P } } \\right. is capped by  { \\sqrt { 2 } } \\| U \\| for all  j \\in \\mathcal T (ignoring any extra scaling tricks); so it is never exploding. Therefore, the gradient is strongly attenuated, since the attention weights  a _ { j s } are tiny for most visual tokens  s (Esmaeilkhani & Latecki, 2025). Layer-wise visual attention analysis is provided in appendix.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the variables M and N represent in the double summation, and what does each s_ij and y_ij likely correspond to?", "query_type": "variable", "positive": {"text": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "context": "Then, the similarities are optimized via sigmoid focal loss: \\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2} where [math] if [math] , and 0 otherwise. [math] is the number of annotations and [math] is the number of proposals."}}, "negatives": [{"text": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "context": "During VLM training, the standard next-token prediction (NTP) loss is used: L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1} where [math] is the probability of generating the next ground truth token [math] , [math] denotes the model parameters, and [math] is the length of the answer sequence. We observe that the NTP loss does not include any incentives for preserving the image content initially encoded in visual tokens [math] .", "negative_type": "hard_same_modal"}}, {"text": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "context": "\\begin{array}{l} L _ {\\text {L L L}} (\\theta) = \\frac {1}{| \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} ^ {\\prime}} - \\log p _ {\\theta} (v \\mid h _ {l} (s)) \\tag {4} \\\\ + \\frac {1}{| \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime} |} \\sum_ {s \\in \\mathcal {P} \\setminus \\mathcal {P} ^ {\\prime}} - \\log \\big (1 - p _ {\\theta} (v \\mid h _ {l} (s)) \\big). \\\\ \\end{array}", "negative_type": "random"}}, {"text": "From an efficiency perspective, SGAI introduces limited overhead. Anchor features  \\phi ( \\pmb { x } _ { a n c } ^ { k } ) can be computed once and detached, and the iterative optimization only requires computing  \\phi ( { \\pmb x } _ { a d v } ) each iteration. We report the runtime in Table VII.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you apply this region loss formula in practice when training a model that processes spatial data with multiple regions?", "query_type": "application", "positive": {"text": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "context": "Then, the similarities are optimized via sigmoid focal loss: \\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2} where [math] if [math] , and 0 otherwise. [math] is the number of annotations and [math] is the number of proposals."}}, "negatives": [{"text": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3}", "context": "Given the logits vector obtained from the matrix-vector multiplication [math] , the probability distribution over the vocabulary is computed using the softmax function: p _ {\\theta} \\left(v _ {j} \\mid h _ {l} (x)\\right) = \\frac {\\exp \\left(\\log \\mathrm {i t} _ {j}\\right)}{\\sum_ {k = 1} ^ {| \\mathcal {V} |} \\exp \\left(\\log \\mathrm {i t} _ {k}\\right)}, \\quad \\forall v _ {j} \\in \\mathcal {V}. \\tag {3} This yields the model’s predicted token probabilities at layer [math] , where [math] denotes the probability of token [math] conditioned on the latent representation [math] .", "negative_type": "hard_same_modal"}}, {"text": "Figure 3: Distribution of variation types in the image variation dataset. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/9c6ec1937031e091e2186dad2a9714e127a969158ab4e1f5af77d2534670bb5f.jpg", "negative_type": "random", "metadata": {"caption": "Figure 3: Distribution of variation types in the image variation dataset. ", "negative_type": "random"}}, {"text": "We showed that visual embeddings in autoregressive VLMs often lose their localized semantics as they propagate through LLM layers, and that next-token prediction alone does not prevent this drift. To address this, we introduced Logit Lens Loss (LLL), a lightweight auxiliary objective that aligns visual tokens with vocabulary concepts describing their image regions without architectural changes, producing sharper Logit Lens confidence maps and improving performance on vision-centric benchmarks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the IoU loss function L_iou calculate in terms of its relationship to the focal loss components?", "query_type": "semantic", "positive": {"text": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "context": "IoU regression. The IoU loss [math] is also formulated as sigmoid focal loss with ground truth IoUs [math] between proposals and corresponding boxes as labels: \\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3} where [math] is predicted IoU scores. Since not all objects in an image are annotated, [math] is only applied to [math] positive proposals.", "negative_type": "random"}}, "negatives": [{"text": "\\mathcal {L} _ {f e a t} = \\sum_ {l \\in \\mathcal {L}} \\left(\\lambda_ {c l s} \\mathcal {D} (\\mathbf {f} _ {c l s} ^ {l}, \\hat {\\mathbf {f}} _ {c l s} ^ {l}) + \\lambda_ {s p a} \\mathcal {D} (\\overline {{\\mathbf {F}}} _ {s p a} ^ {l}, \\overline {{\\hat {\\mathbf {F}}}} _ {s p a} ^ {l})\\right). \\quad (6)", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {f e a t} = \\sum_ {l \\in \\mathcal {L}} \\left(\\lambda_ {c l s} \\mathcal {D} (\\mathbf {f} _ {c l s} ^ {l}, \\hat {\\mathbf {f}} _ {c l s} ^ {l}) + \\lambda_ {s p a} \\mathcal {D} (\\overline {{\\mathbf {F}}} _ {s p a} ^ {l}, \\overline {{\\hat {\\mathbf {F}}}} _ {s p a} ^ {l})\\right). \\quad (6)", "context": "\\mathcal {L} _ {f e a t} = \\sum_ {l \\in \\mathcal {L}} \\left(\\lambda_ {c l s} \\mathcal {D} (\\mathbf {f} _ {c l s} ^ {l}, \\hat {\\mathbf {f}} _ {c l s} ^ {l}) + \\lambda_ {s p a} \\mathcal {D} (\\overline {{\\mathbf {F}}} _ {s p a} ^ {l}, \\overline {{\\hat {\\mathbf {F}}}} _ {s p a} ^ {l})\\right). \\quad (6)", "negative_type": "hard_same_modal"}}, {"text": "Abstract—Large vision–language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate blackbox VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "This study utilizes four publicly available benchmark datasets, MuSiQue, 2WikiMultiHopQA, HotpotQA, and HoVer, which are standard in the field. These datasets are derived from Wikipedia/Wikidata sources and may therefore contain publicly available information about real people and may incidentally include sensitive topics; however, we did not collect new personal data or interact with human participants. Regarding computational resources and model access, we utilized GPT-4o mini and text-embedding-3-small via the Microsoft Azure API, and accessed Llama-3.3- 70B-Instruct through the OpenRouter API.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the variables û_j and u_j* represent in the focal loss term, and what does N^+ signify in the summation?", "query_type": "variable", "positive": {"text": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "context": "IoU regression. The IoU loss [math] is also formulated as sigmoid focal loss with ground truth IoUs [math] between proposals and corresponding boxes as labels: \\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3} where [math] is predicted IoU scores. Since not all objects in an image are annotated, [math] is only applied to [math] positive proposals.", "negative_type": "random"}}, "negatives": [{"text": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "context": "\\mathcal {F} = \\left\\{p _ {\\theta} (\\cdot \\mid h _ {l} (x)) \\mid x \\in \\mathbb {R} ^ {d} \\right\\}. \\tag {7}", "negative_type": "hard_same_modal"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/20c5a09a141a08df300b7ab68032b28605089ba4ec660c503809113fd331402e.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "formulations like HippoRAG 2, transition probabilities are determined by static structural properties. Consequently, random walks disproportionately converge on high-degree nodes (e.g., generic entities like \"United States\" or \"Song\"), which act as \"topological sinks\". We hypothesize that this structural noise disrupts multi-hop dependency by diverting the retrieval path away from the specific \"bridge\" entities required to connect disjoint facts.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you implement this IoU loss function in practice, specifically regarding when to apply it to positive samples versus negative samples?", "query_type": "application", "positive": {"text": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3}", "context": "IoU regression. The IoU loss [math] is also formulated as sigmoid focal loss with ground truth IoUs [math] between proposals and corresponding boxes as labels: \\mathcal {L} _ {\\mathrm {i o u}} = - \\sum_ {j = 1} ^ {N ^ {+}} \\mathcal {L} _ {\\text {f o c a l}} \\left(\\hat {u} _ {j}, u _ {j} ^ {*}\\right), \\tag {3} where [math] is predicted IoU scores. Since not all objects in an image are annotated, [math] is only applied to [math] positive proposals.", "negative_type": "random"}}, "negatives": [{"text": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "context": "\\mathbf {v} ^ {(k + 1)} = (1 - d) \\cdot \\mathbf {e} _ {s} + d \\cdot \\mathbf {v} ^ {(k)} \\mathbf {T} \\tag {1}", "negative_type": "hard_same_modal"}}, {"text": "In contrast, baseline datasets such as CC3M, LAION-2B-Aesthetic, and WIT exhibit lower mean scores (4.78–5.25) and weaker upper tails, reflecting more heterogeneous and less curated visual content drawn from large-scale web sources.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Figure on page 7", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/250f3af7757927fbbfad394e8407ba9f72c4f256ae814426c1caed33fb03bcd6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the two complementary embeddings that ObjEmbed generates for each region to capture both semantic and spatial aspects of objects?", "query_type": "factual", "positive": {"text": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination. Code is available at https: //github.com/WeChatCV/ObjEmbed.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "As illustrated in Fig. 2, we present SGHA-Attack, a transferbased targeted attack framework designed to craft an adversarial example  \\mathbf { \\boldsymbol { x } } _ { \\mathrm { a d v } } by injecting a human-imperceptible perturbation  \\pmb { \\delta } into a clean image  _ { \\textbf { \\em x } } (i.e.,  { \\pmb x } _ { \\mathrm { a d v } } \\ = \\ { \\pmb x } + \\delta subject to  \\| \\pmb { \\delta } \\| _ { \\infty } \\leq \\epsilon ) , aiming to mislead victim VLMs into generating attacker-specified outputs. The pipeline commences by constructing a visually grounded reference pool  \\mathcal { X } _ { r e f } via a frozen Text-to-Image (T2I) model conditioned on the target text  T _ { \\mathrm { t g t } } , from which the Top-  K most semantically relevant instances are adaptively selected as anchors. Subsequently, we perform iterative optimization on a CLIP surrogate, where the perturbation  \\delta is updated via gradient backpropagation to strictly align the hierarchical features of  \\pmb { x } _ { \\mathrm { a d v } } with these weighted anchor references while synchronizing visual and textual tokens across multiple depths, ultimately yielding the final transferable adversarial image. In the following subsections, we systematically elaborate on the three synergistic components that constitute this framework.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4c4a0fe40380c638454206d543e835e8a75efba86610dce02362d98ad8d42ecc.jpg", "negative_type": "random", "metadata": {"caption": "Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. ", "negative_type": "random"}}, {"text": "We showed that visual embeddings in autoregressive VLMs often lose their localized semantics as they propagate through LLM layers, and that next-token prediction alone does not prevent this drift. To address this, we introduced Logit Lens Loss (LLL), a lightweight auxiliary objective that aligns visual tokens with vocabulary concepts describing their image regions without architectural changes, producing sharper Logit Lens confidence maps and improving performance on vision-centric benchmarks.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does ObjEmbed's approach to combining semantic similarity with predicted IoU address the limitations of existing multimodal embedding models?", "query_type": "conceptual", "positive": {"text": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination. Code is available at https: //github.com/WeChatCV/ObjEmbed.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "Current graph-based RAG models rely on a static transition matrix  T , where transition probabilities are fixed during indexing. We argue that this rigidity induces stochastic drift: without queryspecific guidance, the random walk indiscriminately diffuses probability mass into high-degree \"hub\" nodes that are structurally prominent but semantically irrelevant. To mitigate this, we approximate a query-conditional transition matrix  \\hat { \\hat { T } _ { q } } , concentrating the random walk on edges that maximize information gain. We implement a two-stage coarse-to-fine strategy to dynamically modulate the weights of relation edges  ( E _ { r e l } ) .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Xiaochun Cao is with the School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen 518107, China. (email:caoxiaochun@mail.sysu.edu.cn)", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We integrate Symbolic Anchoring, Dynamic Edge Weighting, and Passage Enhancement to construct a query-adapted graph. Standard PPR (Eq. 1) is executed on this refined structure. The resulting stationary distribution of PPR provides the final passage ranking, prioritizing nodes reachable via semantically relevant reasoning paths.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on ObjEmbed's ability to encode all objects and the full image in a single forward pass while supporting both region-level and image-level tasks, what advantages would this provide for real-world vision-language applications compared to traditional approaches?", "query_type": "inferential", "positive": {"text": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination. Code is available at https: //github.com/WeChatCV/ObjEmbed.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "CLSS imposes cross-modal alignment at intermediate layers: after projection into the shared latent subspace, intermediate adversarial visual features are aligned with the target text features, instead of relying only on final-layer matching. During optimization,  { \\mathcal { L } } _ { m i d } provides cross-modal gradients at multiple depths, which improves targeted transfer when victim VLMs use different projection modules or cross-modal alignment schemes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/7bf14dfb3aadf8c5156bbd5f0013d4764bf2a26082f83220755237dd0aa9cd8c.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/c77dda22663b04389dc142b987ab48fd696f5e21239cbe88f41398e546e79271.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the email addresses of the two correspondence authors mentioned in this research paper?", "query_type": "factual", "positive": {"text": "1School of Computer Science and Engineering, Sun Yat-sen University, China 2Peng Cheng Laboratory, China 3Independent Researcher 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 5Guangdong Province Key Laboratory of Information Security Technology, China 6Pazhou Laboratory (Huangpu), China. Correspondence to: Xiaohua Xie <xiexiaoh6@mail.sysu.edu.cn>, Wei-Shi Zheng <wszheng@ieee.org>.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "The VLM input sequence typically includes visual tokens  \\mathcal { P } from the image, followed by text tokens  \\tau for instructions. These segments are concatenated to form the full input context:  \\mathbf { X } ~ = ~ \\left[ \\mathcal { P } , \\mathcal { T } \\right] . All of the visual and language tokens are passed through the LLM backbone and mixed by the attention layers in a joint embedding space, which is language-dominated (Wang et al., 2024), since LLMs are pretrained on a huge amount of text data. The LLM then generates an answer sequence  \\mathcal { A } = ( t _ { 1 } , \\ldots , t _ { | A | } ) token-bytoken under a left-to-right causal mask.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/23926d8a91fdf9d8f7cff6709a58a8d68de2b60080cf1bc1b189a7cdd05329b8.jpg", "negative_type": "random", "metadata": {"caption": "(a) Query: the player whose name is WANG  (b) Query: the player wearing the number 8 jersey ", "negative_type": "random"}}, {"text": "While the \"Query to Triple\" retrieval in HippoRAG 2 effectively captures implicit semantic cues, we argue that relying solely on dense vector alignment leaves the graph traversal susceptible to semantic drift. Without explicit constraints, the PPR propagation can easily be siphoned into high-degree", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the institutional diversity represented by these affiliations reflect the collaborative nature of modern computational research in China?", "query_type": "conceptual", "positive": {"text": "1School of Computer Science and Engineering, Sun Yat-sen University, China 2Peng Cheng Laboratory, China 3Independent Researcher 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 5Guangdong Province Key Laboratory of Information Security Technology, China 6Pazhou Laboratory (Huangpu), China. Correspondence to: Xiaohua Xie <xiexiaoh6@mail.sysu.edu.cn>, Wei-Shi Zheng <wszheng@ieee.org>.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "Evaluating the semantic relevance of every edge using an LLM is computationally prohibitive. Therefore, we first apply a topological filter to constrain the search space to the most plausible local neighborhoods. We define two hyperparameters: the maximum number of seed entities  N _ { s e e d } and the maximum number of edges per seed  K _ { e d g e } for finegrained alignment. First, we select the top-  . N _ { s e e d } entity nodes based on their initial reset probabilities (derived from the dense retrieval alignment). Let  u be such a selected seed. For the seed phrase  u within top-  . N _ { s e e d } , if the number of outgoing relation edges exceeds a threshold  K _ { e d g e } , we prune its outgoing edges by prioritizing the top-  K _ { e d g e } neighbors based on the vector similarity between the query embedding and fact embeddings of relation edges. Neighbors  v \\not \\in \\mathcal { N } _ { t o p } ( u ) are bypassed by the scoring module and assigned a minimal Weak weight. This step acts as a low-pass structural filter, discarding statistically improbable paths before the intensive semantic scoring.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "(c) Change of illumination time and mood ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/fbdac410498d70ef899a4a35340d89309f279836077f030acdd116283325b314.jpg", "negative_type": "random", "metadata": {"caption": "(c) Change of illumination time and mood ", "negative_type": "random"}}, {"text": "You are a precise, factual image cataloger. Your task is to generate a literal description of the image for a visual database. You should generate a **short caption** and a **long caption** for each image.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the combination of academic institutions and laboratories listed, what can be inferred about the research focus areas of this collaborative work?", "query_type": "inferential", "positive": {"text": "1School of Computer Science and Engineering, Sun Yat-sen University, China 2Peng Cheng Laboratory, China 3Independent Researcher 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 5Guangdong Province Key Laboratory of Information Security Technology, China 6Pazhou Laboratory (Huangpu), China. Correspondence to: Xiaohua Xie <xiexiaoh6@mail.sysu.edu.cn>, Wei-Shi Zheng <wszheng@ieee.org>.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "is first passed through a frozen visual encoder (typically a ViT, CLIP, or DINOv2) to extract image tokens and map them into a language embedding space with a linear projection module. Subsequently, the mapped image tokens, often called visual tokens, are used as part of the LLM input, along with the text input (and special tokens). As visual tokens are transformed through the layers of an LLM, their embeddings change. In particular, the embeddings of visual tokens are mixed with the embeddings of language tokens in the attention layers of the LLM module. However, there is a risk that information carried by visual tokens becomes diluted among the predominantly textual tokens as they propagate through the LLM layers. In particular, the information of visual tokens may be transferred to language tokens, as nicely demonstrated in (Kaduri et al., 2025). In an extreme case, the visual tokens are ignored, and the LLM can even hallucinate an image description for an empty image, as was demonstrated in (Liu et al., 2024; Tong et al., 2024). Further evidence that visual tokens are often ignored among all LLM tokens is the fact that after removing half of the visual tokens, the VLM performance does not decrease (Chen et al., 2024). An unintended consequence is that the connection of visual tokens to the content of corresponding image regions is lost in this process. As pointed out in (Fu et al., 2025), The LLM’s ability to use its vision representations is a limiting factor in VLM performance.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "searches. CatRAG instead introduces a \"one-shot\" context-aware graph modification that dynamically re-weights edges before traversal. Unlike iterative cycles, our approach maintains the efficiency of a single retrieval pass, effectively combining the reasoning precision of adaptive methods with the speed and structural integrity of graph-based retrieval.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Some unintended variability was also observed. In several cases, image composition changed substantially due to prompt phrasing (e.g., camera angle or scene description), and", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What specific capabilities can embedding models achieve when powered by large multimodal models according to the passage?", "query_type": "factual", "positive": {"text": "Multimodal embedding models have emerged as a cornerstone in bridging heterogeneous data modalities, such as vision, language, and audio, into a unified semantic space, enabling rich cross-modal understanding, retrieval, and reasoning. Recent advances in large-scale image-text contrastive learning (Radford et al., 2021; Zhai et al., 2023; Xie et al., 2025b) have led to significant progress in multimodal representation learning, especially in aligning images and corresponding captions. Powered by large multimodal models (Bai et al., 2025b;a), embedding models (Zhang et al., 2024; 2025; Jiang et al., 2025c; Lan et al., 2025) can generate task-specific embeddings following user instructions, generate cross-modal embeddings with arbitrary modality combinations, and even perform high-level summary and deep reasoning through chain-of-thought.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "The architecture of recent autoregressive VLMs for text generation typically involves three main components: a vision encoder to process image inputs, a mapping network to project image features into image embeddings, and an autoregressive language model to process both image and prompt embeddings to generate text. We focus on two recent state-of-the-art VLMs: LLava-v1.5 (Liu et al., 2023) and Qwen2.5-VL (Bai et al., 2025), both used in their 7Bparameter versions. LLava-v1.5 uses a frozen CLIP vision encoder and an MLP projection head trained on visionlanguage data, followed by instruction tuning. In contrast, Qwen2.5-VL employs a jointly trained vision-language architecture in which the vision encoder and projection layers are co-optimized with the language model.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Equipped with this object-centric design, ObjEmbed supports a wide range of downstream applications in a unified framework: (1) Object detection and referring expression comprehension: Class names or natural language expressions are encoded as text embeddings and matched against all object embeddings in the image. The final object matching score combines both semantic similarity (between object and text embeddings) and predicted localization quality (from the IoU embedding), computed as their product, effectively balancing semantic relevance and spatial accuracy. (2) Local image retrieval: When the query describes only a specific region or object, we compute the image-level relevance as the maximum matching score across all detected objects. This strategy enables fine-grained, part-aware retrieval even when the target occupies a small portion of the", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Correlation with Reasoning Completeness. This structural correction directly explains the improvements in reasoning integrity observed in Table 4. While the relative reduction in hub mass  ( 7 \\% ) may appear moderate, it represents a critical redistribution of probability mass away from topological distractors and toward specific bridge entities. This aligns with our results on the HoVer dataset, where avoiding generic associations is crucial for verification; specifically, this structural enhancement enables the  11 \\% relative improvement in JSR. By structurally decoupling prominence from relevance, CatRAG ensures that the retrieved context preserves the complete dependency chain, bridging the gap between partial recall and grounded reasoning.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How do multimodal embedding models function as a bridge between different types of data, and what is the significance of creating a unified semantic space?", "query_type": "conceptual", "positive": {"text": "Multimodal embedding models have emerged as a cornerstone in bridging heterogeneous data modalities, such as vision, language, and audio, into a unified semantic space, enabling rich cross-modal understanding, retrieval, and reasoning. Recent advances in large-scale image-text contrastive learning (Radford et al., 2021; Zhai et al., 2023; Xie et al., 2025b) have led to significant progress in multimodal representation learning, especially in aligning images and corresponding captions. Powered by large multimodal models (Bai et al., 2025b;a), embedding models (Zhang et al., 2024; 2025; Jiang et al., 2025c; Lan et al., 2025) can generate task-specific embeddings following user instructions, generate cross-modal embeddings with arbitrary modality combinations, and even perform high-level summary and deep reasoning through chain-of-thought.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "REIRCOCO (Hao et al., 2025) consists of 4,994 images from the COCO dataset, each annotated with a referring expression describing a specific object. The original evaluation protocol requires both image retrieval and object localization. However, since global embedding models lack localization capabilities, we adapt the protocol to a standard text-to-image retrieval setting, where the goal is to retrieve the correct image containing the described object. We report performance using Recall  @ 1 .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/5baedccab98ab36aceaafb3ddf78c65578a4be4587f766b143e406f4dfc3ec8b.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "While NTP optimizes the model for textual coherence, it does not directly constrain the internal representations of visual tokens, allowing them to drift away from their imagerelated content as attention layers mix visual and textual embeddings. To address this, we introduce a grounding objective that explicitly links each visual token to the textual concepts describing its underlying image content.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the progression described from contrastive learning to large multimodal models, what does this suggest about the evolution of multimodal AI systems' reasoning capabilities?", "query_type": "inferential", "positive": {"text": "Multimodal embedding models have emerged as a cornerstone in bridging heterogeneous data modalities, such as vision, language, and audio, into a unified semantic space, enabling rich cross-modal understanding, retrieval, and reasoning. Recent advances in large-scale image-text contrastive learning (Radford et al., 2021; Zhai et al., 2023; Xie et al., 2025b) have led to significant progress in multimodal representation learning, especially in aligning images and corresponding captions. Powered by large multimodal models (Bai et al., 2025b;a), embedding models (Zhang et al., 2024; 2025; Jiang et al., 2025c; Lan et al., 2025) can generate task-specific embeddings following user instructions, generate cross-modal embeddings with arbitrary modality combinations, and even perform high-level summary and deep reasoning through chain-of-thought.", "modal_type": "text", "image_path": null, "metadata": {}}, "negatives": [{"text": "Representing text queries as embeddings. Finally, we introduce a local text token ⟨local text⟩ and a global text token ⟨global text⟩ to represent text queries. The local text token is used to match object embeddings while the global text token is used to match global image embeddings. The encoding templates are “Find an object that matches the given caption. CAPTION ⟨local text⟩” and “Find an image that matches the given caption. CAPTION ⟨global text⟩”. We use the same model to encode text and visual embeddings.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "ObjEmbed is finetuned from Qwen3-VL-Instruct (Bai et al., 2025a) by first initializing the object projector following WeDetect-Ref (Fu et al., 2025a) and then training under the objective described in Equation (4). The model is trained using 16 GPUs, with a batch size of 2 images per GPU, and an initial learning rate of  2 e ^ { - 5 } . All parameters are updated during training except those in the frozen vision encoder. Training proceeds for two epochs. The loss weights  \\lambda _ { 1 } ,  \\lambda _ { 2 } , and  \\lambda _ { 3 } are set to 1.0, 1.0 and 0.25. Input images are resized such that the number of visual tokens ranges from 900 to 1200, corresponding to  9 0 0 ^ { * } 3 2 ^ { * } 3 2 to  1 2 0 0 ^ { * } 3 2 ^ { * } 3 2 pixels, ensuring adaptive computation based on image content.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(ori) a herd of goats grazing on a rocky hillside ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/d700751fbe00d151ce79aeae7139503bb7007df866147bef003fd9760c4af177.jpg", "negative_type": "random", "metadata": {"caption": "(ori) a herd of goats grazing on a rocky hillside ", "negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What vision module does the LLaVa model use?", "query_type": "factual", "positive": {"text": "TABLE I: Configurations of the victim VLMs. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/a740f5d13e98d5580ae13ade17f2a3c89463912f448705c5030c40e31c963ca5.jpg", "metadata": {"caption": "TABLE I: Configurations of the victim VLMs. "}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/c4ea15c0468161ac2bb368a08cf42689e1cc350c4bf2322d03f8c373a137e1c6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "This anchor-derived target provides a clear intermediate-layer reference based on the anchor images, making the supervision at each layer well-defined.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall  @ 1 . ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/34a03eee5f50cfa07b03dd18d8696f24aa5223be030e5e951fe07321e12a46a6.jpg", "negative_type": "random", "metadata": {"caption": "Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall  @ 1 . ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which open-source model has the largest number of parameters?", "query_type": "comparative", "positive": {"text": "TABLE I: Configurations of the victim VLMs. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/a740f5d13e98d5580ae13ade17f2a3c89463912f448705c5030c40e31c963ca5.jpg", "metadata": {"caption": "TABLE I: Configurations of the victim VLMs. "}}, "negatives": [{"text": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2a608ccbf757c46af78127158327e61aea30e521fd6abe118c6f44c82c9e570b.jpg", "negative_type": "random", "metadata": {"caption": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "negative_type": "hard_same_modal"}}, {"text": "We further distinguish the roles of different ViT tokens. Let  \\mathbf { F } _ { a d v } ^ { l } = \\phi ^ { l } ( \\mathbf { x } _ { a d v } ) \\in \\bar { \\mathbb { R } } ^ { ( N + 1 ) \\times D } be the feature map at layer l, where the first token corresponds to CLS and the remaining  N tokens correspond to spatial patches. We decompose  \\mathbf { F } _ { a d v } ^ { l } and", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Aesthetic I Wang et al. (2026), Conceptual Captions (CC3M) Sharma et al. (2018), a random subset of LAION-2B-Aesthetic Schuhmann et al. (2022b), and the Wikipedia-based Image–Text dataset (WIT) Srinivasan et al. (2021). Table 3 reports full distributional statistics of predicted aesthetic scores.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How many open-source models use the ViT-G/14 vision module?", "query_type": "computational", "positive": {"text": "TABLE I: Configurations of the victim VLMs. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/a740f5d13e98d5580ae13ade17f2a3c89463912f448705c5030c40e31c963ca5.jpg", "metadata": {"caption": "TABLE I: Configurations of the victim VLMs. "}}, "negatives": [{"text": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/7f1eadb1a8e453d26b73bfdb85f4563e972b7d22e1834fd623bb913ba68d5ccf.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Human evaluation results oon identity stability and target attribute realization ", "negative_type": "hard_same_modal"}}, {"text": "(e) Change of viewpoint-camera and weather ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/82ae7dbd09c4b97c839bccd4a200fe1cca2528ba47cf5c14b068f65dbf25e258.jpg", "negative_type": "random", "metadata": {"caption": "(e) Change of viewpoint-camera and weather ", "negative_type": "random"}}, {"text": "Adversarial attacks aim to mislead models via imperceptible input perturbations. Depending on the adversary’s knowledge, they are classified as white-box (full parameter access) [30], [31] or black-box (no access to model parameters or gradients) [32], [33]. Regarding objectives, attacks distinguish between untargeted ones that merely degrade performance and targeted ones that steer outputs toward specific malicious goals, posing greater safety risks. In realistic black-box scenarios, transfer-based attacks [34]–[36] have emerged as the dominant paradigm over query-based methods [37], as the latter are often impractical for large VLMs due to prohibitive inference latency and API costs. Consequently, our work focuses on transfer-based targeted attacks, which optimize perturbations on an accessible surrogate (e.g., CLIP) to compromise unseen victim VLMs via cross-model transferability.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the CLIP Score for the RN50 text encoder when using the COA attack method against the BLIP2 model?", "query_type": "factual", "positive": {"text": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2a608ccbf757c46af78127158327e61aea30e521fd6abe118c6f44c82c9e570b.jpg", "metadata": {"caption": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/1318302589467126045a52f86557a19358e73cb6d536448347f7168445a1aaf3.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "The architecture of recent autoregressive VLMs for text generation typically involves three main components: a vision encoder to process image inputs, a mapping network to project image features into image embeddings, and an autoregressive language model to process both image and prompt embeddings to generate text. We focus on two recent state-of-the-art VLMs: LLava-v1.5 (Liu et al., 2023) and Qwen2.5-VL (Bai et al., 2025), both used in their 7Bparameter versions. LLava-v1.5 uses a frozen CLIP vision encoder and an MLP projection head trained on visionlanguage data, followed by instruction tuning. In contrast, Qwen2.5-VL employs a jointly trained vision-language architecture in which the vision encoder and projection layers are co-optimized with the language model.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "The dataset spans six topic categories: Nature and Landscape comprising 36.7% (1,048 images), City, Building, and Architecture 30.1% (858), Everyday Objects and Daily Life 16.0% (457), Spiritual and Heritage Sites 7.5% (213), Work, Hobby, and Transport 4.6% (131), and Portraits and Human Figures 3.5% (101).", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which VLM model shows the highest ASR_Target performance when attacked using the proposed method ('Ours'), and what is that performance value?", "query_type": "comparative", "positive": {"text": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2a608ccbf757c46af78127158327e61aea30e521fd6abe118c6f44c82c9e570b.jpg", "metadata": {"caption": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 1. The overview of ObjEmbed training dataset. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/120ffad156187adf11f4108680e45a1cf7ee3c11605469ca52e8372d40e6daa1.jpg", "negative_type": "random", "metadata": {"caption": "Table 1. The overview of ObjEmbed training dataset. ", "negative_type": "hard_same_modal"}}, {"text": "We introduced the Lunara Aesthetic II dataset, a public release of 2.8K relational contextual variations designed to support controlled analysis of contextual learning in modern image generation systems. By organizing images into identity-anchored variation sets with targeted contextual transformations, Lunara-II enables evaluation protocols that go beyond i.i.d. sampling, including the assessment of contextual generalization, memorization and shortcut learning, within-identity consistency, and robustness of image editing under controlled transformations. The dataset is released under the Apache 2.0 license to encourage broad reuse, reproducible benchmarking, and small-scale training and adaptation studies.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "SORCE-1K (Liu et al., 2025a) comprises 1,023 carefully curated images with complex backgrounds and textual queries that describe less prominent small objects with minimum surrounding context. The target objects typically occupy less than  10 \\% of the image area, posing significant challenges for global image embedding models that focus on holistic scene understanding. Each query is associated with exactly one positive image, and performance is evaluated using Recall  @ 1 .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the average CLIP Score across all text encoders (RN50, BN101, ViT-H/14/16, A-CLIP, ViT-B/32, ViT-B/14) for the clean image baseline on the UniDiffuser model?", "query_type": "computational", "positive": {"text": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2a608ccbf757c46af78127158327e61aea30e521fd6abe118c6f44c82c9e570b.jpg", "metadata": {"caption": "TABLE II: Quantitative comparison of transfer-based targeted attacks against black-box VLMs. We report the CLIP Score (↑) to measure semantic consistency across various text encoders and their ensemble average, alongside the LLM-based ASR evaluated by GPT-4. In the following tables, highlighted, underlined, and gray-shaded values indicate the best, second-best, and our method’s results, respectively, for each case. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "TABLE VI: Ablation of the three proposed modules (SGAI, HVSA, and CLSS). We report Ens.,  \\mathrm { \\sf A S R _ { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } on multiple VLMs.  \\mathcal { L } _ { t x t } is the baseline, adding  \\mathcal { L } _ { a n c } ,  \\mathcal { L } _ { f e a t } , and  { \\mathcal { L } } _ { m i d } activates SGAI, HVSA, and CLSS, respectively. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0d23de913d4ebd230407101c4eddd9ad171a950cddf82e85c3691d85c5b297d6.jpg", "negative_type": "random", "metadata": {"caption": "TABLE VI: Ablation of the three proposed modules (SGAI, HVSA, and CLSS). We report Ens.,  \\mathrm { \\sf A S R _ { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } on multiple VLMs.  \\mathcal { L } _ { t x t } is the baseline, adding  \\mathcal { L } _ { a n c } ,  \\mathcal { L } _ { f e a t } , and  { \\mathcal { L } } _ { m i d } activates SGAI, HVSA, and CLSS, respectively. ", "negative_type": "hard_same_modal"}}, {"text": "(adv) a wooden tray with meat and rice on it ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/d6c440a23a9d36935df23b5df41edf782a0b265167c59e15db714b172d61b2eb.jpg", "negative_type": "random", "metadata": {"caption": "(adv) a wooden tray with meat and rice on it ", "negative_type": "random"}}, {"text": "For standard retrieval comparisons, we employ several strong and widely used retrieval model,including BM25 (Robertson and Walker,", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the SSIM score for the MF-ii method in the Perturbation Constrained category?", "query_type": "factual", "positive": {"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 4: Reasoning Completeness Evaluation. We evaluation the Full Chain Retrieval (FCR) and Joint Success Rate (JSR) on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/36f7e779793ed3f4786a8804036f42ea2e74c57dac798b2dbabc12cbe5469205.jpg", "negative_type": "random", "metadata": {"caption": "Table 4: Reasoning Completeness Evaluation. We evaluation the Full Chain Retrieval (FCR) and Joint Success Rate (JSR) on multi-hop QA and fact verification datasets. LightRAG is not presented because it do not directly produce passage retrieval results. ", "negative_type": "hard_same_modal"}}, {"text": "\\hat {\\mathbf {F}} _ {a n c} ^ {l} = \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\phi^ {l} \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right). \\tag {5}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\hat {\\mathbf {F}} _ {a n c} ^ {l} = \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\phi^ {l} \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right). \\tag {5}", "context": "Let [math] denote a set of selected transformer layers. Denote by [math] the intermediate feature map extracted at layer [math] . Using the anchor weights [math] from Sec. III-A, we construct a layer-wise weighted anchor target: \\hat {\\mathbf {F}} _ {a n c} ^ {l} = \\sum_ {k = 1} ^ {K} w _ {k} \\cdot \\phi^ {l} \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right). \\tag {5} This anchor-derived target provides a clear intermediate-layer reference based on the anchor images, making the supervision at each layer well-defined.", "negative_type": "random"}}, {"text": "At the final VLM layer, the dependence of a text token  j on a visual token  s arises through a cross-attention block. For such a block, the Jacobian of the hidden state satisfies", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Which method achieves the highest PSNR score among all the Perturbation Constrained methods?", "query_type": "comparative", "positive": {"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table on page 7", "modal_type": "table", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/8f5ccd4c030144c7202b7599927545bc9d8bccd92eaad1329e6395ea232eb9dd.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/1c136aa9e21bfafde8d4c032479d7e3fd81174f1db6b67070c1ba09c46b20579.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "loss, since the language-model head is applied to them. The visual tokens do not get direct gradients from NTP, so they do not contribute to the chain rule, leaving crossattention from text tokens as the only pathway by which NTP’s gradient reaches a visual token.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the average SSIM score for all methods in the Perturbation Constrained category?", "query_type": "computational", "positive": {"text": "TABLE III: Quantitative evaluation of visual quality. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/10c408080fe5e4cfb06697ba8ca7d141c3a28b0d235a423df3f87b8fd55a2d63.jpg", "metadata": {"caption": "TABLE III: Quantitative evaluation of visual quality. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01965/2602_01965/hybrid_auto/images/dd6b9ac6a9a67b9f0c40d35b345feeb0ef3e178cb7ed5ab19250580d4daa9849.jpg", "negative_type": "random", "metadata": {"caption": "Table 6: Hyperparameters for CatRAG. Note that Synonym Edge weights are dynamic, scaled by their vector similarity, whereas the standard HippoRAG 2 framework uses raw vector similarity. ", "negative_type": "hard_same_modal"}}, {"text": "by extracting a bounding box from the map and using it to prompt a frozen segmentation model to generate a mask. The first 4 rows show the segmentation results obtained by thresholding the heat maps of the Logit Lens. We observe a huge improvement when Logit Lens is applied after finetuning with LLL (1st vs 3rd row). Moreover, if the results of the 3rd row are postprocessed with frozen SAM (Kirillov et al., 2023), we obtain results comparable to SOTA (4th row).", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Xiaochun Cao is with the School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen 518107, China. (email:caoxiaochun@mail.sysu.edu.cn)", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the target text used in the SGHA-Attack process shown in the figure?", "query_type": "identification", "positive": {"text": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/eb4f9b51f221cacb01b5fd0359f7bd1957707779bd2c00667245e5e4c84dbd89.jpg", "metadata": {"caption": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 7", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/93563440ca172d385ea20919327b3156b26481b07b860d936db27a779e6811e3.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "We evaluate image aesthetics using the LAION Aesthetics v2 predictor, a CLIP-based model trained to approximate aggregate human judgments of visual appeal. We compare Lunara-AIV against several widely used vision–language datasets, including Lunara Art", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Table 7. Ablation studies on instructions. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/8644dc897bcd0933b4eac5a976db0cd98ae4b4a1d13f7841fb708b2bc0cd6ac6.jpg", "negative_type": "random", "metadata": {"caption": "Table 7. Ablation studies on instructions. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Describe the overall workflow of the SGHA-Attack framework from clean image to attack evaluation.", "query_type": "descriptive", "positive": {"text": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/eb4f9b51f221cacb01b5fd0359f7bd1957707779bd2c00667245e5e4c84dbd89.jpg", "metadata": {"caption": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Figure on page 19", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/57cfe9ddda4a80ac753bf10ef0022c9f936fa550e7fe9418e15db218c4aac6a1.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "hard_same_modal"}}, {"text": "This weak seeding serves a specific regulatory function: it aligns the PPR propagation with the query’s intent. By placing a non-zero probability on the exact named entities mentioned in the query, we create a gravitational pull that resists the diffusion of probability mass into generic graph hubs. Even as the random walk explores the neighborhood defined by the static graph, these weak anchors ensure the traversal recurrently grounded to the specific entities in the query, effectively suppressing semantic drift. As a secondary benefit, this mechanism naturally balance the system’s capability: it retains the triplet-based strength in interpreting implicit clues while ensuring robust coverage for containing explicit entity mentions.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We further distinguish the roles of different ViT tokens. Let  \\mathbf { F } _ { a d v } ^ { l } = \\phi ^ { l } ( \\mathbf { x } _ { a d v } ) \\in \\bar { \\mathbb { R } } ^ { ( N + 1 ) \\times D } be the feature map at layer l, where the first token corresponds to CLS and the remaining  N tokens correspond to spatial patches. We decompose  \\mathbf { F } _ { a d v } ^ { l } and", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why might the adversarial image appear visually identical to the clean image while producing different text outputs from the VLMs?", "query_type": "interpretive", "positive": {"text": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/eb4f9b51f221cacb01b5fd0359f7bd1957707779bd2c00667245e5e4c84dbd89.jpg", "metadata": {"caption": "Fig. 1: Illustration of the proposed SGHA-Attack framework targeting diverse VLMs. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "(ori) The image shows a close up of a car wheel with a white background ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/12b4116103bc22881c42a890428ac93458a96d423e41a8e8b27246d1287057ad.jpg", "negative_type": "random", "metadata": {"caption": "(ori) The image shows a close up of a car wheel with a white background ", "negative_type": "hard_same_modal"}}, {"text": "Human evaluation results indicate that the generated image variations maintain a high degree of identity stability, with a mean Likert score of 4.68 across all evaluated samples. In terms of target attribute realization, performance is consistently strong across contextual dimensions, achieving 93.3% accuracy for illumination changes and 90.5% for weatherrelated variations. Viewpoint changes are correctly realized in 88.2% of cases, while object composition modifications reach 84.0% accuracy. More subtle appearance-related deltas show slightly lower but still robust performance, with 85.8% accuracy for color tone and 81.5% for mood or atmospheric changes. Overall, the results suggest that the model reliably implements a wide range of contextual transformations while preserving the core identity of the scene.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "This study utilizes four publicly available benchmark datasets, MuSiQue, 2WikiMultiHopQA, HotpotQA, and HoVer, which are standard in the field. These datasets are derived from Wikipedia/Wikidata sources and may therefore contain publicly available information about real people and may incidentally include sensitive topics; however, we did not collect new personal data or interact with human participants. Regarding computational resources and model access, we utilized GPT-4o mini and text-embedding-3-small via the Microsoft Azure API, and accessed Llama-3.3- 70B-Instruct through the OpenRouter API.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the three main components that make up the SGHA-Attack framework shown in this figure?", "query_type": "identification", "positive": {"text": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e32d6a90817b410428eebd4faee60ee22f19f87074a780afc0245db94fa102ae.jpg", "metadata": {"caption": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "negative_type": "random"}}, "negatives": [{"text": "Figure 1: Distribution of topics in the image variation dataset. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/6d5110920663b3406642df7eedd1acf010a6386775872a408335bd13f8f3f82e.jpg", "negative_type": "random", "metadata": {"caption": "Figure 1: Distribution of topics in the image variation dataset. ", "negative_type": "hard_same_modal"}}, {"text": "Image resolutions vary by source content and variation construction to reflect realistic image editing scenarios rather than enforcing a fixed canonical size. Original images have a median resolution of 3264  \\times 2448 (max 3456  \\times 4608), while variant (generated) images have a lower median resolution of 896  \\times 1184 for practical purposes.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We identify and address the \"Static Graph Fallacy\" inherent in current structure-aware RAG systems, where fixed transition probabilities predispose retrieval to semantic drift and prevent the recovery of complete evidence chains. We propose CatRAG, a framework that transforms the Knowledge Graph Traversal into a context-aware navigation structure. Experiment across multi-hop benchmarks demonstrate that CatRAG consistently outperforms baselines, including HippoRAG 2, while significantly reducing the bias of high-degree hub nodes. Our analysis reveals that these topological adjustments yield substantial improvements in reasoning completeness, effectively bridging the gap between retrieving partial context and enabling fully grounded, multi-hop reasoning.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Describe the flow of data processing from the clean image input through the various components until the final adversarial image output is generated.", "query_type": "descriptive", "positive": {"text": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e32d6a90817b410428eebd4faee60ee22f19f87074a780afc0245db94fa102ae.jpg", "metadata": {"caption": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "negative_type": "random"}}, "negatives": [{"text": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/e44ceccc00f64484610e3c3299292db91889128d682b9b6f0fcf2dbaea7b1c63.jpg", "negative_type": "random", "metadata": {"caption": "Figure 2. The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨object⟩, ⟨iou⟩, ⟨global⟩, ⟨local text⟩, and ⟨global text⟩) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). ", "negative_type": "hard_same_modal"}}, {"text": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2}", "context": "the context content [math] as: C (v) = \\left\\{ \\begin{array}{l l} \\text {S u m m a r y} (\\mathcal {F} (v)) & \\text {i f} | \\mathcal {F} (v) | > \\tau \\\\ \\text {C o n c a t} (\\mathcal {F} (v)) & \\text {o t h e r w i s e} \\end{array} \\right. \\tag {2} where [math] is a density threshold. For informationdense nodes [math] , we generate a concise summary; for sparse nodes, we use raw triples. This hybrid approach balances context completeness with token efficiency.", "negative_type": "random"}}, {"text": "In contrast, the NTP provides only a weak signal to the majority of visual tokens, as we show now. Since the gradient of NTP loss only flows directly to the previously generated text token, it can only flow to a visual token  s through self-attention. However, most text tokens exhibit only weak attention to visual tokens. More formally, the NTP loss  L _ { \\mathrm { { N T P } } } is applied at the text output position, so the gradient at that position backpropagates through selfattention and cross-attention to  h _ { L } ( s ) . Formally,", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the Semantic-Guided Anchor Injection (SGAI) component utilize the target text and reference pool to influence the adversarial attack generation process?", "query_type": "interpretive", "positive": {"text": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e32d6a90817b410428eebd4faee60ee22f19f87074a780afc0245db94fa102ae.jpg", "metadata": {"caption": "Fig. 2: Overview of the proposed SGHA-Attack framework for generating transferable targeted adversarial examples. The framework consists of three key components: SGAI, HVSA, and CLSS. ", "negative_type": "random"}}, "negatives": [{"text": "Target：a baseball player gets ready to throw a pitch. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0267226f5909578a6dd9935fabf5fe38c7e14d3202f52279e842b413e916242c.jpg", "negative_type": "random", "metadata": {"caption": "Target：a baseball player gets ready to throw a pitch. ", "negative_type": "hard_same_modal"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/a0fba5b03295bd90f717e3cadfa3b27f8e56142a32459cff5df84d96b283b8b7.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "IoU regression. The IoU loss  \\mathcal { L } _ { \\mathrm { i o u } } is also formulated as sigmoid focal loss with ground truth IoUs  \\boldsymbol { u } _ { j } ^ { * } between proposals and corresponding boxes as labels:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What animals are shown in the two rows of images used to demonstrate the adversarial attack methods?", "query_type": "identification", "positive": {"text": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2b95146b666baa981627d602298c523fa3b8fc7c34d72258dc3fadd470663c59.jpg", "metadata": {"caption": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "(ori) a herd of goats grazing on a rocky hillside ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/d700751fbe00d151ce79aeae7139503bb7007df866147bef003fd9760c4af177.jpg", "negative_type": "random", "metadata": {"caption": "(ori) a herd of goats grazing on a rocky hillside ", "negative_type": "hard_same_modal"}}, {"text": "Xiaochun Cao is with the School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen 518107, China. (email:caoxiaochun@mail.sysu.edu.cn)", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "layer, ignoring intermediate features, this can cause overfitting to the surrogate and weaken targeted transfer to instructiontuned or proprietary VLMs.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How do the visual qualities of the adversarial examples compare across different attack methods, particularly noting which methods produce more obvious visual distortions?", "query_type": "descriptive", "positive": {"text": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2b95146b666baa981627d602298c523fa3b8fc7c34d72258dc3fadd470663c59.jpg", "metadata": {"caption": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "Target：a baseball player gets ready to throw a pitch. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/0267226f5909578a6dd9935fabf5fe38c7e14d3202f52279e842b413e916242c.jpg", "negative_type": "random", "metadata": {"caption": "Target：a baseball player gets ready to throw a pitch. ", "negative_type": "hard_same_modal"}}, {"text": "VLMs have evolved from task-specific architectures to more general-purpose, generative foundation models [1], [17], [18]. Prior to the widespread adoption of large language models (LLMs), many VLMs emphasized learning joint visual–textual representations for discriminative objectives, including image– text retrieval [19], [20] and visual question answering [21], [22]. More recently, the emergence of strong LLMs has shifted the dominant paradigm toward leveraging an LLM as a language and reasoning backbone while introducing a visual front-end that enables the model to condition on images [23], [24]. Under this paradigm, a central technical challenge is modality alignment: mapping visual representations into a", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "In contrast to large-scale image–text datasets such as LAION-5B Schuhmann et al. (2022a), which prioritize scale and coverage, and web-harvested caption datasets such as Conceptual Captions and YFCC100M Sharma et al. (2018); Thomee et al. (2016), which lack explicit supervision for identity or contextual consistency, Lunara Aesthetic II is explicitly constructed around anchor-linked, identity-preserving variation. Instructionbased image editing benchmarks, including InstructPix2Pix and MagicBrush Brooks et al. (2023); Zhang et al. (2023), introduce text-guided edits and largely consist of isolated, single-step transformations. Lunara Aesthetic II is designed to unify high aesthetic quality with structured, identity-preserving contextual variation, enabling principled training and evaluation of controlled generalization in image generation and editing models.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the comparison between the 'Clean' images and the various adversarial attack results suggest about the effectiveness and stealth capabilities of different attack methods?", "query_type": "interpretive", "positive": {"text": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/2b95146b666baa981627d602298c523fa3b8fc7c34d72258dc3fadd470663c59.jpg", "metadata": {"caption": "Fig. 3: Visual comparison of adversarial examples generated by various attack methods. ", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "(b) Change of illumination time ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/a30f887e5446dc31bc564675c2d80be69c5a6031881864966d49d1ee3f1914e8.jpg", "negative_type": "random", "metadata": {"caption": "(b) Change of illumination time ", "negative_type": "random"}}, {"text": "TABLE V: Defense-aware black-box attacks against victim VLMs. We evaluate Ens.,  \\mathrm { A S R } _ { \\mathrm { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } under four preprocessing-based defenses: Bit Reduction, JPEG Compression, ComDefend, and DiffPure. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01574/2602_01574/hybrid_auto/images/e4d34d340bba0793995839925b70fd4839d82698d1ffa465b25cb2982bb47853.jpg", "negative_type": "random", "metadata": {"caption": "TABLE V: Defense-aware black-box attacks against victim VLMs. We evaluate Ens.,  \\mathrm { A S R } _ { \\mathrm { f o o l } } , and  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } under four preprocessing-based defenses: Bit Reduction, JPEG Compression, ComDefend, and DiffPure. ", "negative_type": "random"}}, {"text": "(a) Out-of-the-box ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/6f937087b3419008a975d61482b091b1353652a4984895117f41d99f3384e2ab.jpg", "negative_type": "random", "metadata": {"caption": "(a) Out-of-the-box ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does the formula L_txt calculate in terms of the relationship between the adversarial input and target text?", "query_type": "semantic", "positive": {"text": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "context": "matching on a surrogate model. For text–image matching, we align the surrogate embedding of the adversarial image with the target text embedding. Let [math] and [math] denote the surrogate image and text encoders, respectively, and define cosine distance as [math] . Here, [math] produces a global image embedding used for the following matching objectives. A standard objective is \\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1} For image–image matching, the adversarial image is guided toward a reference image by matching their surrogate embeddings, which provides more visually grounded supervision. However, existing methods [13]–[16] often rely on a single reference instance, leading to narrow guidance: one image may capture only one appearance of the target concept, and the optimization can become sensitive to that particular target. This sensitivity makes targeted transfer less reliable when the victim model differs in backbone, alignment module, or processing pipeline.", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\hat {w} _ {u p} = w _ {u p} \\cdot (1 + \\beta \\cdot \\mathbb {I} (u, p \\in \\mathcal {T} _ {\\text {s e e d}})) \\tag {3}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\hat {w} _ {u p} = w _ {u p} \\cdot (1 + \\beta \\cdot \\mathbb {I} (u, p \\in \\mathcal {T} _ {\\text {s e e d}})) \\tag {3}", "context": "\\hat {w} _ {u p} = w _ {u p} \\cdot (1 + \\beta \\cdot \\mathbb {I} (u, p \\in \\mathcal {T} _ {\\text {s e e d}})) \\tag {3}", "negative_type": "hard_same_modal"}}, {"text": "Deep-Layer Uniform Sampling strategy, specifically selecting layers  \\{ 7 , 9 , 1 1 \\} for the base ViT-B/32 model. The balancing weights  \\{ \\lambda _ { f e a t } , \\lambda _ { c l s } , \\lambda _ { s p a } , \\lambda _ { m i d } \\} are scaled according to the backbone architecture, for instance, they are set to 1.5, 1.0, 0.7, 2.5 for ViT-B/32. For larger models like ViT-L and ViT-G, we proportionally increase the feature alignment weights and apply the corresponding layer mappings to accommodate their higher-dimensional feature spaces and deeper architectures.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We evaluate CatRAG against a comprehensive suite of baselines spanning two paradigms: standard RAG with retrieval methods, and structure-aware RAG.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the variables φ(x_adv) and ψ(T_tgt) represent in this text loss function, and how do the functions φ and ψ transform their respective inputs?", "query_type": "variable", "positive": {"text": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "context": "matching on a surrogate model. For text–image matching, we align the surrogate embedding of the adversarial image with the target text embedding. Let [math] and [math] denote the surrogate image and text encoders, respectively, and define cosine distance as [math] . Here, [math] produces a global image embedding used for the following matching objectives. A standard objective is \\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1} For image–image matching, the adversarial image is guided toward a reference image by matching their surrogate embeddings, which provides more visually grounded supervision. However, existing methods [13]–[16] often rely on a single reference instance, leading to narrow guidance: one image may capture only one appearance of the target concept, and the optimization can become sensitive to that particular target. This sensitivity makes targeted transfer less reliable when the victim model differs in backbone, alignment module, or processing pipeline.", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "context": "\\mathcal {L} _ {\\text {r e g i o n}} = \\sum_ {i = 1} ^ {M} \\sum_ {j = 1} ^ {N} \\mathcal {L} _ {\\text {f o c a l}} \\left(s _ {i j}, y _ {i j}\\right), \\tag {2}", "negative_type": "hard_same_modal"}}, {"text": "Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall  @ 1 . ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/34a03eee5f50cfa07b03dd18d8696f24aa5223be030e5e951fe07321e12a46a6.jpg", "negative_type": "random", "metadata": {"caption": "Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall  @ 1 . ", "negative_type": "random"}}, {"text": "Image-level contrastive learning. In line with region-level contrastive learning, we also use sigmoid focal loss (Zhai et al., 2023) for image-level supervision  \\mathcal { L } _ { \\mathrm { i m a g e } } . Specifically, the first global image embedding is only supervised by short captions while the second global image embedding is only supervised by long captions. And we will collect captions from other GPUs to enlarge the negative samples.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you apply this formula to measure the effectiveness of an adversarial attack against a text-based model, and what would constitute optimal values for the distance function D?", "query_type": "application", "positive": {"text": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1}", "context": "matching on a surrogate model. For text–image matching, we align the surrogate embedding of the adversarial image with the target text embedding. Let [math] and [math] denote the surrogate image and text encoders, respectively, and define cosine distance as [math] . Here, [math] produces a global image embedding used for the following matching objectives. A standard objective is \\mathcal {L} _ {t x t} = \\mathcal {D} \\left(\\phi \\left(\\boldsymbol {x} _ {a d v}\\right), \\psi \\left(T _ {t g t}\\right)\\right). \\tag {1} For image–image matching, the adversarial image is guided toward a reference image by matching their surrogate embeddings, which provides more visually grounded supervision. However, existing methods [13]–[16] often rely on a single reference instance, leading to narrow guidance: one image may capture only one appearance of the target concept, and the optimization can become sensitive to that particular target. This sensitivity makes targeted transfer less reliable when the victim model differs in backbone, alignment module, or processing pipeline.", "negative_type": "hard_same_modal"}}, "negatives": [{"text": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "context": "U _ {\\theta} h _ {l} (x) = \\left[ \\operatorname {l o g i t} _ {1}, \\operatorname {l o g i t} _ {2}, \\dots , \\operatorname {l o g i t} _ {| \\mathcal {V} |} \\right], \\tag {2}", "negative_type": "hard_same_modal"}}, {"text": "Large Language Models (LLMs) have demonstrated transformative capabilities across a spectrum of natural language tasks, ranging from creative composition to complex code generation (Joel et al., 2025; Li et al., 2024; Ren et al., 2024; Touvron et al., 2023; Brown et al., 2020). Despite these advances, the widespread deployment of LLMs still remains restricted by hallucinations(Xu et al., 2025; Liu et al., 2024) in response generation, often caused by outdated training data or lack of domain-specific knowledge, resulting in seemingly plausible but actually incorrect content. Retrieval-Augmented Generation (RAG) (Gao et al., 2024; Fan et al., 2024) has emerged as a feasible solution to mitigate the issue, which incorporates external, reliable documents within LLM prompts for response generation.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "1) As shown in Table III, performance varies significantly by attack category. Unrestricted methods like AdvDiffVLM suffer from noticeable structural degradation (SSIM 0.69) due to their diffusion-based generation process. In contrast, perturbation-constrained methods operate under a standard budget  ( \\epsilon = 8 / 2 5 5 ) and consistently", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What does this formula calculate in terms of selecting anchor points from a reference set?", "query_type": "semantic", "positive": {"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, "negatives": [{"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8}", "context": "We integrate the three modules, SGAI, HVSA, and CLSS, into a unified objective function: \\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {t x t}} + \\lambda_ {\\text {a n c}} \\mathcal {L} _ {\\text {a n c}} + \\lambda_ {\\text {f e a t}} \\mathcal {L} _ {\\text {f e a t}} + \\lambda_ {\\text {m i d}} \\mathcal {L} _ {\\text {m i d}}, \\tag {8} where λanc, λf eat, and [math] balance anchor guidance, hierarchical visual alignment, and intermediate cross-modal synchronization. We optimize [math] using projected gradient descent under an [math] budget ϵ. At each iteration, the adversarial image is updated using the sign of the gradient and projected back to the feasible [math] ball around [math] , pixel values are also clipped to the valid range. Algorithm 1 summarizes the complete procedure.", "negative_type": "hard_same_modal"}}, {"text": "1) Our method exhibits stronger effectiveness in both breaking model recognition  ( \\mathrm { { A S R } _ { f o o l } ) } and achieving targeted semantics  ( \\mathrm { A S R } _ { \\mathrm { t a r g e t } } ) . Under the “Enhanced Setting”  ( \\epsilon = 1 6 / 2 5 5 ) ), we achieve a  9 7 \\% \\mathrm { \\bf A S R _ { f o o l } } on GPT-4o (vs.  87 \\% for M-Attack), demonstrating our ability to reliably disrupt the model’s visual understanding. More importantly, we translate this disruption into targeted control, boosting  \\mathrm { A S R } _ { \\mathrm { t a r g e t } } from  47 \\% to  79 \\% . This superiority extends to the robust Claude-3.5, where we surpass M-Attack in both fooling rate  67 \\% vs  52 \\% ) and targeted success  2 6 \\% vs  7 \\% ). Notably, our constrained attack even outperforms the unrestricted AdvDiffVLM (  12 \\% success on GPT-4o) significantly, highlighting the efficiency of our semantic-guided optimization.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "(b) Change of illumination time ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/a30f887e5446dc31bc564675c2d80be69c5a6031881864966d49d1ee3f1914e8.jpg", "negative_type": "random", "metadata": {"caption": "(b) Change of illumination time ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the functions φ(x) and ψ(T_tgt) represent, and how do they relate to the cosine similarity computation?", "query_type": "variable", "positive": {"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, "negatives": [{"text": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "context": "Quantifying Semantic Drift. To assess whether our proposed framework mitigates this drift, we analyzed the topological properties of the top-10 retrieved entity nodes after PPR across 100 randomly sampled queries from MuSiQue. We introduce PPR-Weighted Strength [math] to measure the effective structural prominence of the retrieved context: \\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4} where [math] is the PPR probability mass of node [math] re-normalized over the retrieved set [math] (i.e., [math] ), and Strength [math] is the weighted degree of the node. A higher [math] indicates that the PPR result is more reliant on generic, highconnectivity nodes.", "negative_type": "hard_same_modal"}}, {"text": "where  p _ { \\theta } \\left( t _ { i } \\mid { \\mathcal { A } } _ { a < i } , \\mathbf { X } \\right) = p _ { \\theta } \\left( t _ { i } \\mid h _ { L } ( t _ { i - 1 } ) \\right) is the probability of generating the next ground truth token  t _ { i } \\in \\mathcal V ,  \\theta denotes the model parameters, and  | { \\cal A } | is the length of the answer sequence. We observe that the NTP loss does not include any incentives for preserving the image content initially encoded in visual tokens  \\mathcal { P } .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "sion, a choice commonly adopted in object detection. Specifically, for each object description  C _ { \\mathrm { { o b j } } } ^ { i } , we treat each region proposal  p _ { j } as a positive sample if  \\operatorname { I o U } ( p _ { j } , B ^ { i } ) > 0 . 5 . , and negative otherwise. We compute the similarity between the proposal object embedding  e _ { p } ^ { j } and the local text embedding  e _ { t } ^ { i } of  C _ { \\mathrm { { o b j } } } ^ { i } as:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you use this formula to find the most relevant anchor points when given a target T_tgt and a reference dataset X_ref?", "query_type": "application", "positive": {"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, "negatives": [{"text": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "context": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "negative_type": "hard_same_modal"}}, {"text": "We focus on autoregressive Vision-Language Models (VLMs) likse Llava (Liu et al., 2023), MiniGPT4 (Zhu et al., 2023), and Qwen-VL (Bai et al., 2023). Fig. 1 illustrates their processing flow. The input image (or images)", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "context": "L _ {\\text {t o t a l}} = L _ {\\mathrm {N T P}} + \\lambda L _ {\\mathrm {L L L}}. \\tag {6}", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What type of mathematical operation does this formula compute, and what is the significance of the softmax structure with cosine similarity?", "query_type": "semantic", "positive": {"text": "w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3}", "context": "We then compute importance weights using a temperaturescaled softmax: w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3} Finally, we define an anchor-guided objective that aligns the adversarial embedding with a weighted mixture of anchor embeddings:"}}, "negatives": [{"text": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "context": "\\frac {\\partial}{\\partial z} (- \\log p _ {v}) = p - e _ {v}, \\tag {9}", "negative_type": "hard_same_modal"}}, {"text": "Aesthetic I Wang et al. (2026), Conceptual Captions (CC3M) Sharma et al. (2018), a random subset of LAION-2B-Aesthetic Schuhmann et al. (2022b), and the Wikipedia-based Image–Text dataset (WIT) Srinivasan et al. (2021). Table 3 reports full distributional statistics of predicted aesthetic scores.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Figure on page 6", "modal_type": "figure", "image_path": "data/mineru_output/2602_01530/2602_01530/hybrid_auto/images/de960320e159f0ceddd519d7a6fc58880b2c98ebc264865e0993f5e781d687d6.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What do the functions φ and ψ represent in this formula, and how do they transform the anchor data x_anc^k and target T_tgt respectively?", "query_type": "variable", "positive": {"text": "w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3}", "context": "We then compute importance weights using a temperaturescaled softmax: w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3} Finally, we define an anchor-guided objective that aligns the adversarial embedding with a weighted mixture of anchor embeddings:"}}, "negatives": [{"text": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "context": "L _ {\\mathrm {N T P}} (\\theta) = \\frac {1}{| \\mathcal {A} |} \\sum_ {i = 1} ^ {| \\mathcal {A} |} - \\log p _ {\\theta} \\left(t _ {i} \\mid \\mathcal {A} _ {a <   i}, \\mathbf {X}\\right) \\tag {1}", "negative_type": "hard_same_modal"}}, {"text": "For a target vocabulary token  v and layer-l representation  h _ { l } ( s ) , Logit Lens Loss (LLL) is defined as", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We do not intend to teach VLMs new concepts but to preserve their localized image understanding, and consequently, make Logit Lens visualisation useful in VLM explainability. We focus on finetuning, even though the proposed LLL could be applied in pre-training, it is beyond the scope of this paper.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How would you adjust the temperature parameter τ to control the sharpness of the attention weights, and what would be the effect of setting τ to very small versus very large values?", "query_type": "application", "positive": {"text": "w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3}", "modal_type": "formula", "image_path": null, "metadata": {"latex": "w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3}", "context": "We then compute importance weights using a temperaturescaled softmax: w _ {k} = \\frac {\\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {k}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}{\\sum_ {j = 1} ^ {K} \\exp \\left(\\cos \\left(\\phi \\left(\\boldsymbol {x} _ {a n c} ^ {j}\\right) , \\psi \\left(T _ {t g t}\\right)\\right) / \\tau\\right)}. \\tag {3} Finally, we define an anchor-guided objective that aligns the adversarial embedding with a weighted mixture of anchor embeddings:"}}, "negatives": [{"text": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "context": "s _ {i j} = \\frac {e _ {p} ^ {j} \\cdot e _ {t} ^ {i}}{\\| e _ {p} ^ {j} \\| \\| e _ {t} ^ {i} \\|}, \\tag {1}", "negative_type": "hard_same_modal"}}, {"text": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "context": "\\mathcal {S} _ {p p r} (q) = \\sum_ {v \\in \\mathcal {V} _ {t o p}} \\hat {p} (v) \\cdot \\operatorname {S t r e n g t h} (v) \\tag {4}", "negative_type": "random"}}, {"text": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "modal_type": "table", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/4e1634418e9d611556004f04860f67e8a5360f67d8cff842bead42c27ae133c6.jpg", "negative_type": "random", "metadata": {"caption": "Table 10. Effect of the quality of proposals. ‘mix’ denotes a setting where a portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. ‘AR’ is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. ", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What specific components does SGHA-Attack use to generate and select target references for optimization guidance?", "query_type": "factual", "positive": {"text": "Abstract—Large vision–language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate blackbox VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "Our approach builds on the idea that every deep embedding  h _ { l } ( x ) within the LLM implicitly encodes a distribution over vocabulary tokens via the Logit Lens projection  U _ { \\theta } h _ { l } ( x ) . By treating this distribution as the “semantic fingerprint” of a patch, we define a loss that encourages visual tokens corresponding to object regions to produce vocabulary distributions consistent with their ground-truth labels (e.g., high probability for the word “cat” inside a cat region, and low elsewhere). Meanwhile, the loss discourages patches unrelated to a given object category from developing unrelated connections with that category. In essence, it drives each patch to focus more on objects present within its region and less on those absent. This yields a simple yet effective auxiliary objective, termed Logit Lens Loss, that complements the standard NTP loss by grounding visual embeddings directly in the vocabulary space.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "The primary contribution of Lunara Aesthetic II is its relational structure. Instead of treating images as independent samples, the dataset is organized into contextual anchor groups, where multiple variations are linked to a shared source image. This grouping", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "The evaluation further revealed challenges in defining and applying semantic deltas. Several deltas—particularly color tone, illumination time, mood atmosphere, and weather atmosphere—were often intertwined and not strictly separable, leading to ambiguity in both model behavior and human assessment. Prompts that omitted explicit delta specifications raised additional questions about implicit model inference and its impact on evaluation outcomes. Evaluators recommended introducing clearer delta definitions, clustering overlapping deltas where appropriate, and providing explicit checklists for each variant. They also suggested expanding future evaluations to include greater subject-level variation, especially human-centric attributes such as pose or facial expression, to more rigorously assess identity stability beyond primarily scenic or inanimate content.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How does the concept of hierarchical alignment in SGHA-Attack differ from traditional approaches that focus primarily on final-layer alignment?", "query_type": "conceptual", "positive": {"text": "Abstract—Large vision–language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate blackbox VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "SORCE-1K (Liu et al., 2025a) comprises 1,023 carefully curated images with complex backgrounds and textual queries that describe less prominent small objects with minimum surrounding context. The target objects typically occupy less than  10 \\% of the image area, posing significant challenges for global image embedding models that focus on holistic scene understanding. Each query is associated with exactly one positive image, and performance is evaluated using Recall  @ 1 .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "In this section, we present a comprehensive evaluation of our proposed method against state-of-the-art transfer-based", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Overall, these automated metrics demonstrate that contextual variations are achieved through substantive prompt-level interventions, maintain reasonable axis isolation, and exhibit non-degenerate expressive diversity. Fine-grained instance-level consistency (e.g., unintended object introduction or structural drift) is not directly captured by these metrics and is therefore addressed through targeted human evaluation.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the described methodology, why would SGHA-Attack's use of intermediate visual and textual feature synchronization likely improve transferability across different VLM architectures?", "query_type": "inferential", "positive": {"text": "Abstract—Large vision–language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate blackbox VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "random"}}, "negatives": [{"text": "where scalar  p _ { j } is the  j -th component of  p , corresponding to the probability assigned to vocabulary token  j .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Table 3 presents zero-shot evaluation results on the PixMo-Points subset. All evaluations are performed in pixel space, with coordinates parsed from the model’s textual output using a fixed prompt template. We report the median distance in pixels. Both base models, LLaVA-v1.5 and Qwen2.5- VL, show limited ability to localize object centers without grounding-oriented supervision. Incorporating LLL during fine-tuning substantially improves both models, reducing median localization error. Notably, Qwen2.5-VL with LLL achieves a lower median distance than several proprietary and task-specific baselines, and it outperforms Molmo-7B, which was trained with explicit coordinate-level supervision on PixMo. These results indicate that LLL enables strong transfer of spatial grounding learned from RefCOCO to previously unseen object categories in PixMo", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "Looking forward, Lunara-II provides a foundation for several extensions, including expanding contextual factor coverage, increasing variation depth per anchor, and developing standardized evaluation protocols that explicitly score identity consistency, edit specificity, and contextual correctness under controlled transformations. More broadly, relationally structured datasets of this form may help establish clearer empirical signals for when generative models genuinely acquire contextual concepts, rather than relying on superficial correlations.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are the four index terms listed in this research paper passage?", "query_type": "factual", "positive": {"text": "Index Terms—Adversarial attack, Cross-modal synchronization, Hierarchical alignment, Vision-language models.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "Let  \\mathcal { L } denote a set of selected transformer layers. Denote by  \\phi ^ { l } ( \\cdot ) the intermediate feature map extracted at layer  l \\in \\mathcal L . Using the anchor weights  \\{ w _ { k } \\} from Sec. III-A, we construct a layer-wise weighted anchor target:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Figure on page 18", "modal_type": "figure", "image_path": "data/mineru_output/2602_01753/2602_01753/hybrid_auto/images/1f08b9dbf2064eb2c65438489a4ffed9793bc3d7c8e3b947503513e33d48fec2.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "where  \\mathrm { l o g i t } _ { j } corresponds to token  j in the vocabulary  \\nu . This vector represents the model’s predicted logit distribution over tokens after layer  l .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "How do the concepts of cross-modal synchronization and hierarchical alignment relate to vision-language models in this research context?", "query_type": "conceptual", "positive": {"text": "Index Terms—Adversarial attack, Cross-modal synchronization, Hierarchical alignment, Vision-language models.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "In contrast, the NTP provides only a weak signal to the majority of visual tokens, as we show now. Since the gradient of NTP loss only flows directly to the previously generated text token, it can only flow to a visual token  s through self-attention. However, most text tokens exhibit only weak attention to visual tokens. More formally, the NTP loss  L _ { \\mathrm { { N T P } } } is applied at the text output position, so the gradient at that position backpropagates through selfattention and cross-attention to  h _ { L } ( s ) . Formally,", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Figure on page 1", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/95cdf8052a61aad6d0c7185f2958289b0f8d50a0507fb4913dc15871502e9540.jpg", "negative_type": "random", "metadata": {"caption": "", "negative_type": "random"}}, {"text": "Generate a concise, entity-focused summary that captures the core identity and key relationships of a given entity based on its associated fact triplets.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Based on the combination of index terms presented, what can be inferred about the primary research focus of this paper?", "query_type": "inferential", "positive": {"text": "Index Terms—Adversarial attack, Cross-modal synchronization, Hierarchical alignment, Vision-language models.", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "We further distinguish the roles of different ViT tokens. Let  \\mathbf { F } _ { a d v } ^ { l } = \\phi ^ { l } ( \\mathbf { x } _ { a d v } ) \\in \\bar { \\mathbb { R } } ^ { ( N + 1 ) \\times D } be the feature map at layer l, where the first token corresponds to CLS and the remaining  N tokens correspond to spatial patches. We decompose  \\mathbf { F } _ { a d v } ^ { l } and", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2)", "context": "[math] by sampling from a text-to-image generator conditioned on [math] (e.g., a diffusion model). Given the target text embedding [math] , we select the Top- [math] anchors according to cosine similarity under the surrogate: \\{\\pmb {x} _ {a n c} ^ {k} \\} _ {k = 1} ^ {K} = \\mathrm {T o p K} _ {\\pmb {x} \\in \\mathcal {X} _ {r e f}} \\cos \\big (\\phi (\\pmb {x}), \\psi (T _ {t g t}) \\big). \\qquad (2) We then compute importance weights using a temperaturescaled softmax:", "negative_type": "random"}}, {"text": "Both terms in Eq. (6) are nicely aligned since both are based on increasing the probabilities of generating selected vocabulary tokens, as illustrated in Fig. 1. Formally, since both visual and textual embeddings can be projected into probability distributions over the vocabulary via the Logit Lens, they share a common semantic space. We denote this shared space with  \\mathcal { F } of all probability distribution functions (pdfs)  p _ { \\theta } ( \\cdot \\mid h _ { l } ( x ) ) : \\mathcal { V }  [ 0 , 1 ] over the vocabulary tokens  \\nu , i.e.,", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What are two specific applications mentioned that are powered by Large Vision-Language Models?", "query_type": "factual", "positive": {"text": "T ARGE Vision-Language Models [1] have emerged as L a foundational technology, powering critical applications from autonomous agents to content moderation [2]– [4]. As these systems are increasingly entrusted with highstakes decision-making [5], [6], their inherent vulnerability to adversarial perturbations poses severe security risks [7]. Beyond causing benign classification errors [8], imperceptible perturbations can effectively hijack a model’s semantic understanding, enabling adversaries to circumvent safety guardrails (i.e., jailbreaking) and elicit policy-violating outputs [9]. In the black-box scenario, targeted transfer-based attacks [10] represent a particularly practical threat. By optimizing on an accessible surrogate, attackers can precisely steer proprietary", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "LLL to NTP leads to a threefold increase in object confidence score ratio. This significant result confirms the huge impact on LLL on the object confidence maps shown in Fig. 2. Moreover, the fact that NTP did not improve the ratio confirms the fact that NTP provides only a weak supervisory signal for visual cues during fine-tuning. The results were obtained after finetuning on POPE.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Visualizations of self-annotated data. Figure 5 shows representative examples from our self-annotated dataset. Thanks to carefully designed prompts and the use of frontier MLLMs, the generated captions are both accurate and highly distinctive.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "where  a _ { j s } is the attention weight from text token  j to visual token  s , and  C _ { \\mathrm { { a t t } } } is a constant depending on the operator norms of the projection matrices  ( W _ { Q } , W _ { K } , W _ { V } , W _ { O } ) and the layer normalization. So, the Jacobian  \\partial h _ { L } ( j ) / \\partial h _ { L } ( s ) is dominated by the attention weight  a _ { j s } from token  j to  s , times bounded transformation matrices  C _ { \\mathrm { a t t } } .", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "What is the relationship between adversarial perturbations and jailbreaking in the context of Vision-Language Models?", "query_type": "conceptual", "positive": {"text": "T ARGE Vision-Language Models [1] have emerged as L a foundational technology, powering critical applications from autonomous agents to content moderation [2]– [4]. As these systems are increasingly entrusted with highstakes decision-making [5], [6], their inherent vulnerability to adversarial perturbations poses severe security risks [7]. Beyond causing benign classification errors [8], imperceptible perturbations can effectively hijack a model’s semantic understanding, enabling adversaries to circumvent safety guardrails (i.e., jailbreaking) and elicit policy-violating outputs [9]. In the black-box scenario, targeted transfer-based attacks [10] represent a particularly practical threat. By optimizing on an accessible surrogate, attackers can precisely steer proprietary", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "To evaluate robustness against preprocessing-based defenses, we test our method against Bit Reduction (4-bit) [52], JPEG Compression  \\mathrm { ( Q F = } 7 5 ) [53], ComDefend [54], and DiffPure [55]. As shown in Table V, our method consistently outperforms baselines in maintaining high ASRtarget and Ensemble CLIP Scores. Notably, under quantization and compression, our method significantly surpasses both MF-ii and COA across all victim models. This robustness indicates that our hierarchical alignment strategy encourages robust target-aligned representations features into the deep semantic structure of the image, allowing target semantics to survive input transformations and signal purification mechanisms that typically filter out superficial perturbations.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "Figure 3: Distribution of variation types in the image variation dataset. ", "modal_type": "figure", "image_path": "data/mineru_output/2602_01666/2602_01666/hybrid_auto/images/9c6ec1937031e091e2186dad2a9714e127a969158ab4e1f5af77d2534670bb5f.jpg", "negative_type": "random", "metadata": {"caption": "Figure 3: Distribution of variation types in the image variation dataset. ", "negative_type": "random"}}, {"text": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "modal_type": "formula", "image_path": null, "negative_type": "random", "metadata": {"latex": "\\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4}", "context": "The total training objective is a weighted combination: \\mathcal {L} _ {\\text {t o t a l}} = \\lambda_ {1} \\mathcal {L} _ {\\text {r e g i o n}} + \\lambda_ {2} \\mathcal {L} _ {\\text {i m a g e}} + \\lambda_ {3} \\mathcal {L} _ {\\text {i o u}}, \\tag {4} where [math] , and [math] are hyperparameters.", "negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
{"query": "Why might targeted transfer-based attacks be considered more concerning than other types of adversarial attacks in practical scenarios?", "query_type": "inferential", "positive": {"text": "T ARGE Vision-Language Models [1] have emerged as L a foundational technology, powering critical applications from autonomous agents to content moderation [2]– [4]. As these systems are increasingly entrusted with highstakes decision-making [5], [6], their inherent vulnerability to adversarial perturbations poses severe security risks [7]. Beyond causing benign classification errors [8], imperceptible perturbations can effectively hijack a model’s semantic understanding, enabling adversaries to circumvent safety guardrails (i.e., jailbreaking) and elicit policy-violating outputs [9]. In the black-box scenario, targeted transfer-based attacks [10] represent a particularly practical threat. By optimizing on an accessible surrogate, attackers can precisely steer proprietary", "modal_type": "text", "image_path": null, "metadata": {"negative_type": "hard_same_modal"}}, "negatives": [{"text": "LLL to NTP leads to a threefold increase in object confidence score ratio. This significant result confirms the huge impact on LLL on the object confidence maps shown in Fig. 2. Moreover, the fact that NTP did not improve the ratio confirms the fact that NTP provides only a weak supervisory signal for visual cues during fine-tuning. The results were obtained after finetuning on POPE.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "hard_same_modal"}}, {"text": "\\mathcal { X } _ { r e f } by sampling from a text-to-image generator conditioned on  T _ { t g t } (e.g., a diffusion model). Given the target text embedding  \\psi ( T _ { t g t } ) , we select the Top-  K anchors according to cosine similarity under the surrogate:", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}, {"text": "We introduced the Lunara Aesthetic II dataset, a public release of 2.8K relational contextual variations designed to support controlled analysis of contextual learning in modern image generation systems. By organizing images into identity-anchored variation sets with targeted contextual transformations, Lunara-II enables evaluation protocols that go beyond i.i.d. sampling, including the assessment of contextual generalization, memorization and shortcut learning, within-identity consistency, and robustness of image editing under controlled transformations. The dataset is released under the Apache 2.0 license to encourage broad reuse, reproducible benchmarking, and small-scale training and adaptation studies.", "modal_type": "text", "image_path": null, "negative_type": "random", "metadata": {"negative_type": "random"}}], "difficulty_score": 0.5666666666666667}
