{
  "1104.3913": [
    {
      "doc_id": "1104.3913",
      "figure_id": "1104.3913_fig_1",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg",
      "image_filename": "1104.3913_page0_fig0.jpg",
      "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$",
      "context_before": "In this section, we explore how to implement what may be called fair affirmative action. Indeed, a typical question when we discuss fairness is, “What if we want to ensure statistical parity between two groups S and T but members of S are less likely to be “qualified”? In Section 3, we have seen that when $S$ and $T$ are “similar” then the Lipschitz condition implies statistical parity. Here we consider the complementary case where $S$ and $T$ are very different and imposing statistical parity corresponds to preferential treatment. This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.\n\nFor simplicity, let $T = S ^ { c }$ . Assume $| S | / | T \\cup S | = 1 / 1 0$ , so S is only $10 \\%$ of the population. Suppose that our task-specific metric partitions $S \\cup T$ into two groups, call them $G _ { 0 }$ and $G _ { 1 }$ , where members of $G _ { i }$ are very close to one another and very far from all members of $G _ { 1 - i }$ . Let $S _ { i }$ , respectively $T _ { i }$ , denote the intersection $S \\cap G _ { i }$ , respectively $T \\cap G _ { i }$ , for $i = 0$ 1. Finally, assume $| S _ { 0 } | = | T _ { 0 } | = 9 | S | / 1 0 .$ Thus, $G _ { 0 }$ contains less than $20 \\%$ of the total population, and is equally divided between S and $T$ .\n\nThe Lipschitz condition requires that members of each $G _ { i }$ be treated similarly to one another, but there is no requirement that members of $G _ { 0 }$ be treated similarly to members of $G _ { 1 }$ . The treatment of",
      "context_after": "members of S , on average, may therefore be very different from the treatment, on average, of members of $T$ , since members of S are over-represented in $G _ { 0 }$ and under-represented in $G _ { 1 }$ . Thus the Lipschitz condition says nothing about statistical parity in this case.\n\nSuppose the members of $G _ { i }$ are to be shown an advertisement $\\mathrm { a d } _ { i }$ for a loan offering, where the terms in $\\mathrm { a d } _ { 1 }$ are superior to those in $\\mathrm { a d } _ { 0 }$ . Suppose further that the distance metric has partitioned the population according to (something correlated with) credit score, with those in $G _ { 1 }$ having higher scores than those in $G _ { 0 }$ .\n\nOn the one hand, this seems fair: people with better ability to repay are being shown a more attractive product. Now we ask two questions: “What is the effect of imposing statistical parity?” and “What is the effect of failing to impose statistical parity?”",
      "referring_paragraphs": [
        "In this section, we explore how to implement what may be called fair affirmative action. Indeed, a typical question when we discuss fairness is, “What if we want to ensure statistical parity between two groups S and T but members of S are less likely to be “qualified”? In Section 3, we have seen that when $S$ and $T$ are “similar” then the Lipschitz condition implies statistical parity. Here we consider the complementary case where $S$ and $T$ are very different and imposing statistical parity c",
        "This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.",
        "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$\n\nmembers of S , on average, may therefore be very different from the treatment, on average, of members of $T$ , since members of S are over-represented in $G _ { 0 }$ and under-represented in $G _ { 1 }$ ."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1104.3913_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1104.3913",
      "figure_id": "1104.3913_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig1.jpg",
      "image_filename": "1104.3913_page0_fig1.jpg",
      "caption": "Remark 5.1. If $( V , d )$ is not well-separated, then for every constant $\\epsilon > 0$ it must contain a wellseparated subset $V ^ { \\prime } \\subseteq V$ such that every point $x \\in V$ has a neighbor $x ^ { \\prime } ",
      "context_before": "$$ \\begin{array}{l} \\mathbb{E}_{x\\in V}\\mathbb{E}_{y\\sim \\mathrm{E}(x)}d(x,y)\\leq 1 + \\mathbb{E}_{x\\in V}\\int_{1}^{\\infty}\\frac{re^{-r}}{Z_{x}} |B(x,r)|\\mathrm{d}r \\\\ \\leq 1 + \\mathbb {E} _ {x \\in V} \\int_ {1} ^ {\\infty} r e ^ {- r} | B (x, r) | \\mathrm {d} r \\quad (\\text {s i n c e} Z _ {x} \\geq e ^ {- d (x, x)} = 1) \\\\ = 1 + \\int_ {1} ^ {\\infty} r e ^ {- r} \\mathbb {E} _ {x \\in V} | B (x, r) | \\mathrm {d} r \\\\ \\leq 1 + \\int_ {1} ^ {\\infty} r e ^ {- r} r ^ {k ^ {\\prime}} \\underset {x \\in V} {\\mathbb {E}} | B (x, 1) | \\mathrm {d} r \\quad \\text {(u s i n g (1 8))} \\\\ \\leq 1 + 2 ^ {O (k)} \\int_ {0} ^ {\\infty} r ^ {k ^ {\\prime} + 1} e ^ {- r} d r \\\\ \\leq 1 + 2 ^ {O (k)} \\left(k ^ {\\prime} + 2\\right)! \\\\ \\end{array} $$\n\nAs we assumed that $k = O ( 1 )$ we conclude\n\n$$ \\underset {x \\in V} {\\mathbb {E}} \\underset {y \\sim \\operatorname {E} (x)} {\\mathbb {E}} d (x, y) \\leq 2 ^ {O (k)} (k ^ {\\prime} + 2)! \\leq O (1). $$",
      "context_after": "Remark 5.1. If $( V , d )$ is not well-separated, then for every constant $\\epsilon > 0$ it must contain a wellseparated subset $V ^ { \\prime } \\subseteq V$ such that every point $x \\in V$ has a neighbor $x ^ { \\prime } \\in V ^ { \\prime }$ such that $d ( x , x ^ { \\prime } ) \\leq \\epsilon$ A Lipschitz mapping $M ^ { \\prime }$ defined on $V ^ { \\prime }$ naturally extends to all of $V$ by putting $M ( x ) = M ^ { \\prime } ( x ^ { \\prime } )$\n\nwhere $x ^ { \\prime }$ is the nearest neighbor of $x$ in $V ^ { \\prime }$ It is easy to see that the expected loss of $M$ is only an additive $\\epsilon$ worse than that of $M ^ { \\prime }$ . Similarly, the Lipschitz condition deteriorates by an additive $2 \\epsilon$ ， i.e., $D _ { \\infty } ( M ( x ) , M ( y ) ) \\leq d ( x , y ) + 2 \\epsilon$ Indeed, denoting the nearest neighbors in $V ^ { \\prime }$ of $x , y$ by $x ^ { \\prime } , y ^ { \\prime }$ , yrespectively, we have $D _ { \\infty } ( M ( x ) , M ( y ) ) = D _ { \\infty } ( M ^ { \\prime } ( x ^ { \\prime } ) , M ^ { \\prime } ( y ^ { \\prime } ) ) \\leq d ( x ^ { \\prime } , y ^ { \\prime } ) \\leq d ( x , y ) + d ( x , x ^ { \\prime } ) + d ( y , y ^ { \\prime } ) \\leq \\frac { 1 - \\gamma } { 2 } .$ $d ( x , y ) + 2 \\epsilon$ , y Here, we used the triangle inequality.\n\nThe proof of Theorem 5.2 shows an exponential dependence on the doubling dimension $k$ of the underlying space in the error of the exponential mechanism. The next theorem shows that the loss of any Lipschitz mapping has to scale at least linearly with $k$ The proof follows from a packing argument .similar to that in [HT10]. The argument is slightly complicated by the fact that we need to give a lower bound on the average error (over $x \\in V .$ ) of any mechanism.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1104.3913_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1306.5204": [
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg",
      "image_filename": "1306.5204_page0_fig0.jpg",
      "caption": "(a) Firehose",
      "context_before": "1http://www.alexa.com/topsites\n\n2http://www.nytimes.com/interactive/2012/10/28/nyregion/hurricanesandy.html\n\n3https://dev.twitter.com/docs/streaming-apis",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig1.jpg",
      "image_filename": "1306.5204_page0_fig1.jpg",
      "caption": "(b) Streaming API Figure 1: Tag cloud of top terms from each dataset.",
      "context_before": "",
      "context_after": "Twitter’s Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010), among others. Researchers’ reliance upon this data source is significant, and these examples only provide a cursory glance at the tip of the iceberg. Due to the widespread use of Twitter’s Streaming API in various scientific fields, it is important that we understand how using a sub-sample of the data generated affects these results.\n\nFrom a statistical point of view, the “law of large numbers” (mean of a sample converges to the mean of the entire population) and the Glivenko-Cantelli theorem (the unknown distribution $X$ of an attribute in a population can be approximated with the observed distribution $x$ ) guarantee satisfactory results from sampled data when the randomly selected sub-sample is big enough. From network algorithmic (Wasserman and Faust 1994) perspective the question is more complicated. Previous efforts have delved into the topic of network sampling and how working with a restricted set of data can affect common network measures. The problem was studied earlier in (Granovetter 1976), where the author proposes an algorithm to sample networks in a way that allows one to estimate basic network properties. More recently, (Costenbader and Valente 2003) and (Borgatti, Carley, and Krackhardt 2006) have studied the affect of data error on common network centrality measures by randomly deleting and adding nodes and edges. The authors discover that centrality measures are usually most resilient on dense networks. In (Kossinets 2006), the authors study global properties of simulated random graphs to better understand data error in social networks. (Leskovec and Faloutsos 2006)\n\nproposes a strategy for sampling large graphs to preserve network measures.",
      "referring_paragraphs": [
        "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i",
        "The most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "The number of geotagged tweets is low, with only 16,739 geotagged tweets in the Streaming data $( 3 . 1 7 \\% )$ and 18,579 in the Firehose data $( 1 . 4 5 \\% )$ . We notice that despite the difference in tweets collected on the whole we get $9 0 . 1 0 \\%$ coverage of geotagged tweets. We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1. To better understand the distribut",
        "Figure 1: Tag cloud of top terms from each dataset.",
        "To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
      "image_filename": "1306.5204_page0_fig2.jpg",
      "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
      "context_before": "In this work we compare the datasets by analyzing facets commonly used in the literature. We start by comparing the top hashtags found in the tweets, a feature of the text commonly used for analysis. In (Tsur and Rappoport 2012), the authors try to predict the magnitude of the number of tweets mentioning a particular hashtag. Using a regression model trained with features extracted from the text, the authors find that the content of the idea behind the tag is vital to the count of the tweets employing it. Tweeting a hashtag automatically adds a tweet to a page showing tweets published by other tweeters containing that hashtag. In (Yang et al. 2012), the authors find that this communal property of hashtags along with the meaning of the tag itself drive the adoption of hashtags on Twitter. (De Choudhury et al. 2010) studies the propagation patterns of URLs on sampled Twitter data.\n\nTopic analysis can also be used to better understand the content of tweets. (Kireyev, Palen, and Anderson 2009) drills the problem down to disaster-related tweets, discovering two main types of topics: informational and emotional. Finally, (Yin et al. 2011; Hong et al. 2012; Pozdnoukhov and Kaiser 2011) all study the problem of identifying topics in geographical Twitter datasets, proposing models to extract topics relevant to different geographical areas in the data. (Joseph, Tan, and Carley 2012) studies how the topics users discuss drive their geolocation.\n\nGeolocation has become a prominent area in the study of social media data. In (Wakamiya, Lee, and Sumiya 2011) the authors try to classify towns based upon the content of the geotagged tweets that originate from within the town. (De Longueville, Smith, and Luraschi 2009) studies Twitter’s use as a sensor for disaster information by studying the geographical properties of users tweets. The authors discover that Twitter’s information is accurate in the later stages of a crisis for information dissemination and retrieval.",
      "context_after": "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown in Figure 2. One of the more interesting results in this dataset is that as the data in the Firehose spikes, the Streaming API coverage is reduced. One possible explanation for this phenomenon could be that due to the Western holidays observed at this time, activity on Twitter may have reduced causing the $1 \\%$ threshold to go down.\n\nOne of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the Firehose on any given day. While this is much better than just $1 \\%$ of the tweets promised by the Streaming API, we have no reference point for the data in the tweets we received.\n\nThe most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
      "referring_paragraphs": [
        "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i",
        "different communities with each other or funnel different information sources. Furthermore, we calculate the Potential Reach which counts the number of nodes that are reachable in the network weighted with the path distance. In our Twitter networks this is equivalent to the inverse in-distance of reachable nodes (Sabidussi 1966). This approach results in a metric that finds sources of information (users) that potentially can reach many other nodes on short path distances. Before calculating thes",
        "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
        "The raw counts of tweets we received each day from both sources are shown in Figure 2.",
        "Table 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/5f5fb5eb0ac2d9647748e7eadf75690c81f0da1c4023ae31d2f2cdeec9ba1c6a.jpg",
      "image_filename": "5f5fb5eb0ac2d9647748e7eadf75690c81f0da1c4023ae31d2f2cdeec9ba1c6a.jpg",
      "caption": "Table 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.",
      "context_before": "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown in Figure 2. One of the more interesting results in this dataset is that as the data in the Firehose spikes, the Streaming API coverage is reduced. One possible explanation for this phenomenon could be that due to the Western holidays observed at this time, activity on Twitter may have reduced causing the $1 \\%$ threshold to go down.\n\nOne of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the Firehose on any given day. While this is much better than just $1 \\%$ of the tweets promised by the Streaming API, we have no reference point for the data in the tweets we received.\n\nThe most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
      "context_after": "",
      "referring_paragraphs": [
        "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i",
        "The most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "The number of geotagged tweets is low, with only 16,739 geotagged tweets in the Streaming data $( 3 . 1 7 \\% )$ and 18,579 in the Firehose data $( 1 . 4 5 \\% )$ . We notice that despite the difference in tweets collected on the whole we get $9 0 . 1 0 \\%$ coverage of geotagged tweets. We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1. To better understand the distribut",
        "Figure 1: Tag cloud of top terms from each dataset.",
        "To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg",
      "image_filename": "1306.5204_page0_fig3.jpg",
      "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
      "context_before": "",
      "context_after": "Statistical Measures\n\nWe investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose. We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic. We continue to extract topics from the text, matching topical content and comparing topical distribution to better understand how sampling affects the results of this common process performed on Twitter data. In both cases we compare our streaming data to random datasets obtained by sampling the data obtained through the Firehose.\n\nTop Hashtag Analysis",
      "referring_paragraphs": [
        "One of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the",
        "The most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "To understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)",
        "We complement our node-level analysis by comparing various metrics at the network level. These metrics are reported in Table 3 and are calculated as follows. Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1 \\%$ of the nodes justify our focus on the main component for this data. Therefore, we reduce the networks to their largest comp",
        "the calculations. To describe the structure of the retweet networks we calculate the clustering coefficient, a measure for local density (Watts and Strogatz 1998). We do not take all possible triads of directed networks into account, but treat the networks as undirected when calculating the clustering coefficient. $D _ { i n } > 0$ shows the proportion of nodes in the largest component that are retweeted and $m a x ( D _ { i n } )$ shows the value of the highest unscaled In-Degree value, i.e., n",
        "To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3.",
        "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
        "To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th).",
        "These metrics are reported in Table 3 and are calculated as follows.",
        "After removing these tweets,\n\nTable 3: Comparison of Network-Level Social Network Analysis Metrics."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg",
      "image_filename": "1306.5204_page0_fig4.jpg",
      "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
      "context_before": "Top Hashtag Analysis\n\nHashtags are an important communication device on Twitter. Users employ them to annotate the content they produce, allowing for other users to find their tweets and to facilitate interaction on the platform. Also, adding a hashtag to a tweet is equivalent to joining a community of users discussing the same topic (Yang et al. 2012). In addition, hashtags are also used by Twitter to calculate the trending topics of the day, which encourages the user to post in these communities.\n\nRecently, hashtags have become an important part of Twitter analysis (Efron 2010; Tsur and Rappoport 2012; Recuero and Araujo 2012). For both the purpose of community formation and trend analysis it is important that our Streaming dataset convey the same importance for hashtags as the Firehose data. Here we compare the top hashtags in",
      "context_after": "the two datasets using Kendall’s $\\tau$ rank correlation coefficient (Agresti 2010).\n\nKendall’s $\\tau$ of Top Hashtags Kendall’s $\\tau$ is a statistic which measures the correlation of two ordered lists by analyzing the number of concordant pairs between them. Consider two hashtags, #A and #B. If both lists rank #A higher than #B, then this is considered a concordant pair, otherwise it is counted as a discordant pair. Ties are handled using the $\\tau _ { \\beta }$ statistic as follows:\n\n$$ \\tau_ {\\beta} = \\frac {\\left| P _ {C} \\right| - \\left| P _ {D} \\right|}{\\sqrt {\\left(\\left| P _ {C} \\right| + \\left| P _ {D} \\right| + \\left| T _ {F} \\right|\\right) \\left(\\left| P _ {C} \\right| + \\left| P _ {D} \\right| + \\left| T _ {S} \\right|\\right)}} (1) $$",
      "referring_paragraphs": [
        "To understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)",
        "more than $90 \\%$ of geotagged Tweets from both sources are excluded from the data and the Streaming coverage level is reduced to $3 9 . 1 9 \\%$ . The distribution of tweets by continent is shown in Table 4. Here we see a more even representation of the tweets’ locations in Asia and North America.",
        "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "The results of this experiment are shown in Figure 4.",
        "%</td><td>84.6%</td><td>95.5%</td><td>82.5%</td><td>92.9%</td></tr><tr><td>Clust.Cofef.</td><td>0.029</td><td>0.053</td><td>0.033</td><td>0.050</td></tr><tr><td>DCinCentr.</td><td>0.059</td><td>0.042</td><td>0.085</td><td>0.043</td></tr><tr><td>BCCentr.</td><td>0.010</td><td>0.053</td><td>0.010</td><td>0.050</td></tr><tr><td>PReach Centr.</td><td>0.130</td><td>0.240</td><td>0.156</td><td>0.205</td></tr></table>\n\nTable 4: Geotagged Tweet Location by Continent."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_7",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg",
      "image_filename": "1306.5204_page0_fig5.jpg",
      "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.",
      "context_before": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total. The $\\tau _ { \\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.\n\nTo understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th). The results of this experiment are shown in Figure 4. Here we see mixed results at small values of $n$ , indicating that the Streaming data may not be good for finding the top hashtags. At larger values of $n$ , we see that the Streaming API does a better job of estimating the top hashtags in the Firehose data.\n\nComparison with Random Samples After seeing the results from the previous section, we are left to wonder if the results are an artifact of using the Streaming API or if we",
      "context_after": "could have obtained the same results by any random sampling. Would we obtain the same results with a random sample of equal size from the Firehose data, or does the Streaming API’s filtering mechanism give us an advantage? To answer this question we repeat the experiments for each day in the previous section. This time, instead of using Streaming API data, we select tweets uniformly at random (without replacement) until we have amassed the same number of tweets as we collected from the Streaming API for that day. We repeat this process 100 times and obtain results as shown in Figure 5. Here we see that the levels of coverage in the random and Streaming data have comparable $\\tau _ { \\beta }$ values for large $n$ , however at smaller $n$ we see a much different picture. The random data gets very high $\\tau _ { \\beta }$ scores for $n = 1 0$ , showing a good capacity for finding the top hashtags in the dataset. The Streaming API data does not consistently find the top hashtags, in some cases revealing reverse correlation with the Firehose data at smaller $n$ . This could be indicative of a filtering process in Twitter’s Streaming API which causes a misrepresentation of top hashtags in the data.\n\nTopic models are statistical models which discover topics in a corpus. Topic modeling is especially useful in large data, where it is too cumbersome to extract the topics manually. Due to the large volume of tweets published on Twitter, topic modeling has become central to many content-based studies using Twitter data (Kireyev, Palen, and Anderson 2009; Pozdnoukhov and Kaiser 2011; Hong et al. 2012; Yin et al. 2011; Chae et al. 2012). We compare the topics drawn from the Streaming data with those drawn from the Firehose data using a widely-used topic modeling algorithm, latent Dirichlet allocation (LDA) (Blei, $\\mathrm { N g }$ , and Jordan 2003). Latent Dirichlet allocation is an algorithm for the automated discovery of topics. LDA treats documents as a mixture of topics, and topics as a mixture of words. Each topic discovered\n\nby LDA is represented by a probability distribution which conveys the affinity for a given word to that particular topic. We analyze these distributions to understand the differences between the topics discovered in the two datasets. To get a sense of how the topics found in the Streaming data compare with those found with random samples, we compare with topics found by running LDA on random subsamples of the Firehose data.",
      "referring_paragraphs": [
        "could have obtained the same results by any random sampling. Would we obtain the same results with a random sample of equal size from the Firehose data, or does the Streaming API’s filtering mechanism give us an advantage? To answer this question we repeat the experiments for each day in the previous section. This time, instead of using Streaming API data, we select tweets uniformly at random (without replacement) until we have amassed the same number of tweets as we collected from the Streaming",
        "Figure 5: Random sampling of Firehose data."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig6.jpg",
      "image_filename": "1306.5204_page0_fig6.jpg",
      "caption": "(a) Min. $\\mu = 0 . 0 2 4$ , $\\sigma = 0 . 0 1 9$",
      "context_before": "Comparison with Random Samples In order to get additional perspective on the accuracy of the topics discovered in the Streaming data, we compare the Streaming data with data sampled randomly from the Firehose, as we did earlier to compare the correlation. First, we compute the average of the Jensen-Shannon scores from the Streaming data in Figure 6, $S$ . We then repeat this process for each of the 100 runs with random data, each run called $x _ { i }$ . Next, we use maximum-likelihood estimation (Casella and Berger 2001) to estimate the parameters of the Gaussian distribution from which these points originate, µˆ = 1100 P100i=1 xi, $\\begin{array} { r } { \\hat { \\mu } = \\frac { 1 } { 1 0 0 } \\sum _ { i = 1 } ^ { 1 0 0 } x _ { i } } \\end{array}$ and $\\begin{array} { r } { \\hat { \\sigma } = \\sqrt { \\frac { 1 } { 1 0 0 } \\sum _ { i = 1 } ^ { 1 0 0 } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } } } \\end{array}$ . Finally, we compute the $z$ - Score for $S$ , $\\begin{array} { r } { z = \\frac { S - \\hat { \\mu } } { \\hat { \\sigma } } } \\end{array}$ S−µˆσˆ . This score gives us a concrete measure of the difference between the Streaming API data and the random samples. Results of this experiment, including $z$ -Scores are shown in Figure 7. Nonetheless, we are still able to get topics from the Streaming API that are close to those found in random data with higher levels of coverage. A threshold of 3-sigma is often used in the literature to indicate extreme values (Filliben and Others 2002, Section 6.3.1). With this threshold, we see that overall we are able to get significantly better topics with the random data than with the Streaming API on 4 of the 5 days.\n\nBecause Twitter is a social network, Twitter data can be analyzed with methods from Social Network Analysis (Wasserman and Faust 1994) in addition to statistical measures. Possible 1-mode and 2-mode networks are: $U s e r \\times U s e r$ retweet networks, User × Hashtag content networks, Hashtag × Hashtag co-occurrence networks. For the purpose of this article we focus on $U s e r \\times U s e r$ retweet networks. Users who send tweets within a certain time period are the nodes in the network. Furthermore, users that are retweeted within this time period are also nodes in this network, regardless of the time their original tweet was tweeted. The networks created by this procedure are directed and not symmetric by design, however, bi-directional links are possible in case $a b$ and $b a$ . We ignore line weight created by multiple $a b$ retweets and self-loops (yes, some user retweet themselves). For the network metrics, the comparison is done on both the network, and the node levels. Networks are analyzed using ORA (Carley et al. 2012).\n\nThe node-level comparison is accomplished by calculating measures at the user-level and comparing these results. We calculate three different centrality measures at the node level, two of which—Degree Centrality and Betweenness Centrality—were defined by Freeman as “distinct intuitive conceptions of centrality” (Freeman 1979, p. 215). Degree Centrality counts the number of neighbors in unweighted networks. In particular, we are interested in In-Degree Centrality as this reveals highly respected sources of information in the retweet network (where directed edges point to the source). Betweenness Centrality (Freeman 1979) identifies brokerage positions in the Twitter networks that connect",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig7.jpg",
      "image_filename": "1306.5204_page0_fig7.jpg",
      "caption": "(b) Q1. $\\mu = 0 . 0 1 8$ , $\\sigma = 0 . 0 1 8$ .",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig8.jpg",
      "image_filename": "1306.5204_page0_fig8.jpg",
      "caption": "(c) Median. $\\mu = 0 . 0 1 8$ , σ = 0.020.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg",
      "image_filename": "1306.5204_page0_fig9.jpg",
      "caption": "(d) Q3. $\\mu = 0 . 0 1 4$ , $\\sigma = 0 . 0 1 6$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig10.jpg",
      "image_filename": "1306.5204_page0_fig10.jpg",
      "caption": "(e) Max. µ = 0.016, σ = 0.018.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_13",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig11.jpg",
      "image_filename": "1306.5204_page0_fig11.jpg",
      "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation. (a) Min. $S = 0 . 0 2 4$ , $\\hat { \\mu } = 0 . 0 1 7$ $\\hat { \\sigma } = 0 . 0 0 2$ $z = 3 . 5 0 0$ .",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "where $M = { \\textstyle \\frac { 1 } { 2 } } ( T _ { i } ^ { S } + T _ { j } ^ { F } )$ and $K L$ is the Kullback-Liebler divergence (Cover and Thomas 2006). We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6. We see a trend of higher divergence with lower coverage, and lower divergence with higher coverage. This shows that decreased coverage in the Streaming data causes variance in the discovered topics.",
        "Comparison with Random Samples In order to get additional perspective on the accuracy of the topics discovered in the Streaming data, we compare the Streaming data with data sampled randomly from the Firehose, as we did earlier to compare the correlation. First, we compute the average of the Jensen-Shannon scores from the Streaming data in Figure 6, $S$ . We then repeat this process for each of the 100 runs with random data, each run called $x _ { i }$ . Next, we use maximum-likelihood estimatio",
        "We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6.",
        "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   \n(a) Min. $S = 0 . 0 2 4$ , $\\hat { \\mu } = 0 . 0 1 7$ $\\hat { \\sigma } = 0 . 0 0 2$ $z = 3 . 5 0 0$ ."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig12.jpg",
      "image_filename": "1306.5204_page0_fig12.jpg",
      "caption": "(b) Q1. $S = 0 . 0 1 8$ , $\\hat { \\mu } = 0 . 0 1 2$ $\\hat { \\sigma } = 0 . 0 0 1$ $z = 6 . 0 0 0$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg",
      "image_filename": "1306.5204_page0_fig13.jpg",
      "caption": "(c) Median. $S = 0 . 0 1 8$ $\\hat { \\mu } = 0 . 0 1 3$ , $\\hat { \\sigma } = 0 . 0 0 1$ , $z = 5 . 0 0 0$ .",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig14.jpg",
      "image_filename": "1306.5204_page0_fig14.jpg",
      "caption": "(d) Q3. $S = 0 . 0 1 4$ , $\\hat { \\mu } = 0 . 0 1 3$ $\\hat { \\sigma } = 0 . 0 0 1$ $z = 1 . 0 0 0$ .",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_17",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig15.jpg",
      "image_filename": "1306.5204_page0_fig15.jpg",
      "caption": "(e) Max. $S = 0 . 0 1 6$ , $\\hat { \\mu } = 0 . 0 1 3$ , $\\hat { \\sigma } = 0 . 0 0 1$ , $z = 3 . 0 0 0$ . Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.",
      "context_before": "",
      "context_after": "different communities with each other or funnel different information sources. Furthermore, we calculate the Potential Reach which counts the number of nodes that are reachable in the network weighted with the path distance. In our Twitter networks this is equivalent to the inverse in-distance of reachable nodes (Sabidussi 1966). This approach results in a metric that finds sources of information (users) that potentially can reach many other nodes on short path distances. Before calculating these measures, we extract the main component and delete all other nodes (see next sub-section). In general, centrality measures are used to identify important nodes. Therefore, we calculate the number of top 10 and top 100 nodes that can be correctly identified with the Streaming data. Table 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days.\n\nAlthough, we know from previous studies (Borgatti, Carley, and Krackhardt 2006) that there is a very low likelihood that the ranking will be correct when handling networks with missing data, the accuracy of the daily results is not very satisfying. When we look at the results of the individual days, we can see that the matches have, once again, a broad range as a function of the data coverage rate. In (Borgatti, Carley, and Krackhardt 2006) the authors argue that network measures are stable for denser networks. Twitter data, being very sparse, causes the network metrics’ accuracy to be rather low in the case when the data sub-sample is smaller. However,",
      "referring_paragraphs": [
        "Comparison with Random Samples In order to get additional perspective on the accuracy of the topics discovered in the Streaming data, we compare the Streaming data with data sampled randomly from the Firehose, as we did earlier to compare the correlation. First, we compute the average of the Jensen-Shannon scores from the Streaming data in Figure 6, $S$ . We then repeat this process for each of the 100 runs with random data, each run called $x _ { i }$ . Next, we use maximum-likelihood estimatio",
        "Results of this experiment, including $z$ -Scores are shown in Figure 7.",
        "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_18",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/e69864d03ff9b8c5f645ce56f8988c3275a79f0c5f65c1e48ed22fa97e117fb5.jpg",
      "image_filename": "e69864d03ff9b8c5f645ce56f8988c3275a79f0c5f65c1e48ed22fa97e117fb5.jpg",
      "caption": "Table 2: Average centrality measures for Twitter retweet networks for 28 daily networks. “All” is all 28 days together.",
      "context_before": "different communities with each other or funnel different information sources. Furthermore, we calculate the Potential Reach which counts the number of nodes that are reachable in the network weighted with the path distance. In our Twitter networks this is equivalent to the inverse in-distance of reachable nodes (Sabidussi 1966). This approach results in a metric that finds sources of information (users) that potentially can reach many other nodes on short path distances. Before calculating these measures, we extract the main component and delete all other nodes (see next sub-section). In general, centrality measures are used to identify important nodes. Therefore, we calculate the number of top 10 and top 100 nodes that can be correctly identified with the Streaming data. Table 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days.\n\nAlthough, we know from previous studies (Borgatti, Carley, and Krackhardt 2006) that there is a very low likelihood that the ranking will be correct when handling networks with missing data, the accuracy of the daily results is not very satisfying. When we look at the results of the individual days, we can see that the matches have, once again, a broad range as a function of the data coverage rate. In (Borgatti, Carley, and Krackhardt 2006) the authors argue that network measures are stable for denser networks. Twitter data, being very sparse, causes the network metrics’ accuracy to be rather low in the case when the data sub-sample is smaller. However,",
      "context_after": "identifying ${ \\sim } 5 0 \\%$ key-players correctly for a single day is reasonable, and accuracy can be increased by using longer observation periods. Even more, the Potential Reach metrics are quite stable for some days in the aggregated data.\n\nNetwork-Level Measures\n\nWe complement our node-level analysis by comparing various metrics at the network level. These metrics are reported in Table 3 and are calculated as follows. Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1 \\%$ of the nodes justify our focus on the main component for this data. Therefore, we reduce the networks to their largest component before we proceed with",
      "referring_paragraphs": [
        "From December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown i",
        "different communities with each other or funnel different information sources. Furthermore, we calculate the Potential Reach which counts the number of nodes that are reachable in the network weighted with the path distance. In our Twitter networks this is equivalent to the inverse in-distance of reachable nodes (Sabidussi 1966). This approach results in a metric that finds sources of information (users) that potentially can reach many other nodes on short path distances. Before calculating thes",
        "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
        "The raw counts of tweets we received each day from both sources are shown in Figure 2.",
        "Table 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_19",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/82bef0f01c6a139b10e38358a274c9f8a694895e9eaea8c2f061702ccc2e5472.jpg",
      "image_filename": "82bef0f01c6a139b10e38358a274c9f8a694895e9eaea8c2f061702ccc2e5472.jpg",
      "caption": "Table 3: Comparison of Network-Level Social Network Analysis Metrics.",
      "context_before": "We do not discuss all details of the individual results but focus on the differences between the two data sources. First, the coverage of nodes and links is similar to the coverage of tweets. This is a good indicator that the sub-sample is not biased to the specific Twitter user (e.g. high activity). The smaller proportion of nodes with non-zero In-Degree for the Firehose shows us that the larger number of nodes includes many more peripheral nodes. A low Clustering Coefficient implies that networks are hierarchical rather than interacting communities. Even though the centralization indexes are rather similar, there is one very interesting result when looking at the individual days: The range of values is much higher for the Streaming data as a result of the high coverage fluctuation. Further research will analyze whether we can use network metrics to better estimate how sufficient the sampled Streaming data is.\n\nThe final facet of the Twitter data we compare is the geolocation of the tweets. Geolocation is an important part of a tweet, and the study of the location of content and users is currently an active area of research (Cheng, Caverlee, and Lee 2010; Wakamiya, Lee, and Sumiya 2011). We study how the geographic distribution of the geolocated tweets is affected by the sampling performed by the Streaming API.\n\nThe number of geotagged tweets is low, with only 16,739 geotagged tweets in the Streaming data $( 3 . 1 7 \\% )$ and 18,579 in the Firehose data $( 1 . 4 5 \\% )$ . We notice that despite the difference in tweets collected on the whole we get $9 0 . 1 0 \\%$ coverage of geotagged tweets. We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1. To better understand the distribution of geotagged tweets we repeat the same process, this time excluding tweets originating in the boundary box set in the parameters. After removing these tweets,",
      "context_after": "",
      "referring_paragraphs": [
        "One of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the",
        "The most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "To understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)",
        "We complement our node-level analysis by comparing various metrics at the network level. These metrics are reported in Table 3 and are calculated as follows. Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1 \\%$ of the nodes justify our focus on the main component for this data. Therefore, we reduce the networks to their largest comp",
        "the calculations. To describe the structure of the retweet networks we calculate the clustering coefficient, a measure for local density (Watts and Strogatz 1998). We do not take all possible triads of directed networks into account, but treat the networks as undirected when calculating the clustering coefficient. $D _ { i n } > 0$ shows the proportion of nodes in the largest component that are retweeted and $m a x ( D _ { i n } )$ shows the value of the highest unscaled In-Degree value, i.e., n",
        "To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3.",
        "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
        "To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th).",
        "These metrics are reported in Table 3 and are calculated as follows.",
        "After removing these tweets,\n\nTable 3: Comparison of Network-Level Social Network Analysis Metrics."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_20",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/df59d726dc6e60116c0e4a18a97581f7e62aca7cf10c0e5b313e5c03695300f0.jpg",
      "image_filename": "df59d726dc6e60116c0e4a18a97581f7e62aca7cf10c0e5b313e5c03695300f0.jpg",
      "caption": "Table 4: Geotagged Tweet Location by Continent. Excluding boundary box from parameters.",
      "context_before": "",
      "context_after": "more than $90 \\%$ of geotagged Tweets from both sources are excluded from the data and the Streaming coverage level is reduced to $3 9 . 1 9 \\%$ . The distribution of tweets by continent is shown in Table 4. Here we see a more even representation of the tweets’ locations in Asia and North America.\n\nConclusion and Future Work\n\nIn this work we ask whether data obtained through Twitter’s sampled Streaming API is a sufficient representation of activity on Twitter as a whole. To answer this question we collected data with exactly the same parameters from both the free, but limited, Streaming API and the unlimited, but costly, Firehose. We provide a methodology for comparing the two multifaceted sets of data and results of our analysis.",
      "referring_paragraphs": [
        "To understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th)",
        "more than $90 \\%$ of geotagged Tweets from both sources are excluded from the data and the Streaming coverage level is reduced to $3 9 . 1 9 \\%$ . The distribution of tweets by continent is shown in Table 4. Here we see a more even representation of the tweets’ locations in Asia and North America.",
        "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "The results of this experiment are shown in Figure 4.",
        "%</td><td>84.6%</td><td>95.5%</td><td>82.5%</td><td>92.9%</td></tr><tr><td>Clust.Cofef.</td><td>0.029</td><td>0.053</td><td>0.033</td><td>0.050</td></tr><tr><td>DCinCentr.</td><td>0.059</td><td>0.042</td><td>0.085</td><td>0.043</td></tr><tr><td>BCCentr.</td><td>0.010</td><td>0.053</td><td>0.010</td><td>0.050</td></tr><tr><td>PReach Centr.</td><td>0.130</td><td>0.240</td><td>0.156</td><td>0.205</td></tr></table>\n\nTable 4: Geotagged Tweet Location by Continent."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1306.5204_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1403.7400": [
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg",
      "image_filename": "1403.7400_page0_fig0.jpg",
      "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)",
      "context_before": "The inclusion of hashtags in tweets is a Twitter convention for marking a tweet as part of a particular conversation or topic, and many social media studies rely on them for sample extraction. For example, the Tunisian uprising was associated with the hashtag #sidibouzid while the initial Egyptian protests of January 25, 2011, with #jan25. Facebook’s adoption of hashtags makes the methodological specifics of this convention even more important. While hashtag studies can be a powerful for examining network structure & information flows, all hashtag analyses, by definition, select on a dependent variable, and hence display the concomitant features and weaknesses of this methodological path.\n\n“Selecting on the dependent variable” occurs when inclusion of a case in a sample depends on the very variable being examined. Such samples have specific limits to their analytic power. For example, analyses that only examine revolutions or wars that have occurred will overlook cases where the causes and correlates of revolution and war have been present but in which there have been no resulting wars or revolutions (Geddes, 2010). Thus, selecting on the dependent variable (the occurrence of war or revolution) can help identify necessary conditions, but those may not be sufficient. Selecting on the dependent variable can introduce a range of errors specifics of which depend on the characteristics of the uncorrelated sample.\n\nIn hashtag datasets, a tweet is included because the user chose to use it, a clear act of self-selection. Self-selected samples often will not only have different overall characteristics than the general population, they may also exhibit significantly different correlational tendencies which create thorny issues of confounding variables. Famous examples include the hormone replacement therapy (HRT) controversy in which researchers had, erroneously, believed that HRT conferred health benefits to post-menopausal women based on observational studies of women who self-selected to take HRT. In reality, HRT therapy was adopted by healthier women. Later randomized double-blind studies showed that HRT was, in fact, harmful—so harmful that the researchers stopped the study in its tracks to reverse advice that had been given to women for a decade.",
      "context_after": "Samples drawn using different hashtags can differ in important dimensions, as hashtags are embedded in particular cultural and socio-political frameworks. In some cases, the hashtag is a declaration of particular sympathy. In other cases, there may be warring messages as the hashtag emerges as a contested cultural space. For example, two years of regular monitoring of activity—checking at least for an hour once a week—on the hashtags #jan25 and #Bahrain show their divergent nature. Those who choose to use #jan25 are almost certain to be sympathetic to the Egyptian revolution while #Bahrain tends to be used both by supporters and opponents of the uprising in Bahrain. Data I systematically sampled on three occasions showed that only about 1 in 100 #jan25 tweets were neutral while the rest were all supporting the revolution. Only about 5 out of 100 #Bahrain tweets were neutral, and 15 out of 100 were strongly opposed to the uprising, while the rest, 80 out of 100 were supportive. In contrast, #cairotraffic did not exhibit any overt signs of political preference. Consequently, since the hashtag users are a particular community, thus prone to selection biases, it would be difficult to generalize from their behavior to other samples. Political users may be more prone to retweeting, say, graphic content, whereas non-political users may react with aversion. Hence, questions such as “does graphic content spread quickly on Twitter” or “do angry messages diffuse more quickly” might have quite different answers if the sample is drawn through different hashtags.\n\nHashtag analyses can also be affected by user activity patterns. An analysis of twenty hashtags used during the height of Turkey’s Gezi Park protests in June 2013 (#occupygezi, #occupygeziparki, #direngeziparki, #direnankara, #direngaziparki, etc.) shows a steep rise in activity on May 30th when the protests began, dropping off by June 3rd (Figure 1). Looking at this graph, one might conclude that either the protests had died down, or that people had stopped talking about the protests on Twitter. Both conclusions would be very mistaken, as revealed by the author’s interviews with hundreds of protesters on-the-ground during the protests, online ethnography that followed hundreds of people active in the protests (some of them also interviewed offline), monitoring of Twitter, trending topics, news coverage and the protests themselves.\n\nWhat had happened was that as soon as the protest became the dominant story, large numbers of people continued to discuss them heavily – almost to the point that no other discussion took place on their Twitter feeds – but stopped using the hashtags except to draw attention to a new phenomenon or to engage in “trending topic wars” with ideologically-opposing groups. While the protests continued, and even intensified, the hashtags died down. Interviews revealed two reasons for this. First, once everyone knew the topic, the hashtag was at once superfluous and wasteful on the character-limited Twitter platform. Second, hashtags were seen only as useful for attracting attention to a particular topic, not for talking about it.",
      "referring_paragraphs": [
        "Hashtag analyses can also be affected by user activity patterns. An analysis of twenty hashtags used during the height of Turkey’s Gezi Park protests in June 2013 (#occupygezi, #occupygeziparki, #direngeziparki, #direnankara, #direngaziparki, etc.) shows a steep rise in activity on May 30th when the protests began, dropping off by June 3rd (Figure 1). Looking at this graph, one might conclude that either the protests had died down, or that people had stopped talking about the protests on Twitter",
        "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1403.7400_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig1.jpg",
      "image_filename": "1403.7400_page0_fig1.jpg",
      "caption": "Figure 2: Retweeted widely, but mostly in disgust",
      "context_before": "The question of inference from analyses of social media big data remains underconceptualized and underexamined. What’s a click? What does a retweet mean? In what context? By whom? How do different communities interpret these interactions? As with all human activities, interpreting online imprints engages layers of complexity.\n\n1. What’s in a Retweet? Understanding our Data:\n\nThe same act can have multiple, even contradictory meanings. In many studies, for example, retweets or mentions are used as proxies for influence or agreement. This may hold in some contexts; however, there are many conceptual steps and implicit assumptions embedded in this analysis. It is clear that a retweet is information exposure and/or reaction; however, after that, its meaning could range from affirmation to denunciation to sarcasm to approval to disgust. In fact, many social media acts which are designed as “positive” interactions by the platform engineers, ranging from Retweets on Twitter to even “Likes” on Facebook can carry a range of meanings, some quite negative.",
      "context_after": "As an example, take the recent case of the twitter account of fashion store @celebboutique. On July, 2012, the account tweeted with glee that the word “#aurora” was trending and attributed this to the popularity of a dress named #aurora in its shop. The hashtag was trending, however, because Aurora, Colorado was the site of a movie theatre massacre on that day. There was an expansive backlash against @celebboutique’s crass and insensitive tweet. There were more than 200 mentions and many hundreds of retweets with angry messages in as little as sixty seconds. The tweet itself, too, was retweeted thousands of times (See Figure 2). After about an hour, the company realized its mistake and stepped in. This was followed by more condemnation—a few hundred mentions per minute at a minimum. (For more analysis: (Gilad, 2012)) Hence,\n\nwithout understanding the context, the spike in @celebboutique mentions could easily be misunderstood.\n\nPolarized situations provide other examples of “negative retweets.” For example, during the Gezi protests in Turkey, the mayor of Ankara tweeted personally from his account, often until late hours of the night, engaging Gezi protesters individually in his idiosyncratic style, which involved the use of “ALL CAPS” and colorful language. He became highly visible among supporters as well as opponents of these protests. His visibility, combined with his style, meant that his tweets were widely retweeted—but not always by supporters. Gezi protestors would retweet his messages and then follow the retweet with a negative or mocking message. His messages were also retweeted without comment by people whose own Twitter timelines made clear that their intent was to “expose” or ridicule, rather than agree. A simple aggregation would find that thousands of people were retweeting his tweets, which might be interpreted as influence or agreement.",
      "referring_paragraphs": [
        "As an example, take the recent case of the twitter account of fashion store @celebboutique. On July, 2012, the account tweeted with glee that the word “#aurora” was trending and attributed this to the popularity of a dress named #aurora in its shop. The hashtag was trending, however, because Aurora, Colorado was the site of a movie theatre massacre on that day. There was an expansive backlash against @celebboutique’s crass and insensitive tweet. There were more than 200 mentions and many hundred",
        "Figure 2: Retweeted widely, but mostly in disgust\n\nAs an example, take the recent case of the twitter account of fashion store @celebboutique."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1403.7400_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg",
      "image_filename": "1403.7400_page0_fig2.jpg",
      "caption": "FFigure 3: Two peeople “subtweetting” each other without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.",
      "context_before": "Subtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and consequently to automated data collection—even as the reference remains clear to those “in the know.” This manipulation of visibility can be achieved by referring to a person who has a twitter handle without either “mentioning” this handle, or by inserting a space between the $@$\n\nssign and the hhandle, or by uusing their reggular name or a nnickname ratheer than the hanndle, or even soometimes delibbeerately misspellling the name . In some casees, the referencce ccan only be unnderstood in coontext, as theree is no mentioon oof the target inn any form. Thhese many formms of subtwee tinng come with ddifferent impli cations for bigg data analyticss.\n\nFor examplle, a controvversial article by Egyptiann-AAmerican Monna El Eltahawyy sparked a masssive discussioon inn Egypt’s sociial media. In aa valuable anallysis of this dissccussion, socioloogists Alex Haanna and Marcc Smith extrac teed the tweets wwhich mentioneed her or linkeed to the articlee. TTheir network analysis reveaaled great pol arization in thhe ddiscussion, wit h two distinctlly clustered gr oups. Howeveer, wwhile watchingg this discussioon online, I nooticed that manny oof the high-proofile bloggers and young Eggyptian activistts ddiscussing the article - and g reatly influenccing the converrssation - were iindeed subtweeeting. Later ddiscussions witth thhis communityy revealed thatt this was a deeliberate choicce thhat had been mmade because many people did not want tto ggive Eltahawy attention,” evven as they waanted to discusss thhe topic and hher work.",
      "context_after": "In another exxample drawn from my primmary research oon TTurkey, figure 3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context. Whille eeach person is referring to thhe other, theree are no name s, nnicknames, or hhandles. In adddition, neither follows the othheer on Twitter. It is, howeverr, clearly a dirrect engagemennt aand conversatioon, if a negativve one. A broaad discussion oof thhis “Twitter sppat” on Turkis h Twitter provved people werre aaware of this ass a two-way coonversation. It was so well unndderstood that it was even repoorted in newspaapers.\n\nWhile the truue prevalence of this behavioor is hard to esstaablish, exactlyy because the aactivity is hiddden from largeesscale, machine--led analyses, observations oof Turkish Twiitter during the GGezi protests off June 2013 revvealed that succh ssubtweets weree common. Inn order to gett a sense of itts sscale, I underttook an onlinee ethnographyy in Decembeer, 22013, during wwhich two hunndred Twitter uusers from Turrkkey, assembledd as a purposivve sample inc luding ordinarry uusers as well a s journalists annd pundits, weere followed foor\n\nan houur at a time inn, totaling at leeast 10 hours oof observation deedicated to cattching subtweeets. This resulteed in a collectionn of 100 unmmistakable subttweets; many mmore were undouubtedly missedd because theyy are not alwayys obvious to obsservers. In fact,, the subtweetss were widely uunderstood and reetweeted, whiich increases the importancce of such practicces. Overall, thhe practice apppears commonn enough to be desscribed as routiine, at least in Turkey.",
      "referring_paragraphs": [
        "In another exxample drawn from my primmary research oon TTurkey, figure 3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context. Whille eeach person is referring to thhe other, theree are no name s, nnicknames, or hhandles. In adddition, neither follows the othheer on Twitter. It is, howeverr, clearly a dirrect engagemennt aand conversati",
        "In another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1403.7400_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg",
      "image_filename": "1403.7400_page0_fig3.jpg",
      "caption": "Figuree 4: Algorithmiccally Invisible Enngagement: A coolumnist responds to critics by screeen captures.",
      "context_before": "In another exxample drawn from my primmary research oon TTurkey, figure 3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context. Whille eeach person is referring to thhe other, theree are no name s, nnicknames, or hhandles. In adddition, neither follows the othheer on Twitter. It is, howeverr, clearly a dirrect engagemennt aand conversatioon, if a negativve one. A broaad discussion oof thhis “Twitter sppat” on Turkis h Twitter provved people werre aaware of this ass a two-way coonversation. It was so well unndderstood that it was even repoorted in newspaapers.\n\nWhile the truue prevalence of this behavioor is hard to esstaablish, exactlyy because the aactivity is hiddden from largeesscale, machine--led analyses, observations oof Turkish Twiitter during the GGezi protests off June 2013 revvealed that succh ssubtweets weree common. Inn order to gett a sense of itts sscale, I underttook an onlinee ethnographyy in Decembeer, 22013, during wwhich two hunndred Twitter uusers from Turrkkey, assembledd as a purposivve sample inc luding ordinarry uusers as well a s journalists annd pundits, weere followed foor\n\nan houur at a time inn, totaling at leeast 10 hours oof observation deedicated to cattching subtweeets. This resulteed in a collectionn of 100 unmmistakable subttweets; many mmore were undouubtedly missedd because theyy are not alwayys obvious to obsservers. In fact,, the subtweetss were widely uunderstood and reetweeted, whiich increases the importancce of such practicces. Overall, thhe practice apppears commonn enough to be desscribed as routiine, at least in Turkey.",
      "context_after": "Usiing screen capttures rather thaan quotes is an other practice thhat adds to thhe invisibility of engagemennt to algorithmss. A “caps” is ddone when Twwitter users refeerence each other’s tweets throuugh screen caaptures rather than links, mentioons or quotes . An examplee is shown onn Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”\n\nYett another praactice, colloquuially known as “hatelinkingg,” limits the algorithmic vvisibility of enngagement, althouugh this one i s potentially ttraceable. “Haate-linking” occurss when a user llinks to anotheer user’s tweet rather than mentiooning or quotiing the user. TThis practice, ttoo, would skew analyses baseed on mention s or retweets, though in this caase, it is at leasst possible to loook for such linnks.\n\nSubbtweeters, “capps” users, and hate-linkers arre obviously a smmaller commuunity than tweeeters as a wholle. While it is uncclear how wideespread these ppractices truly are, studying Tuurkish Twitter shows that theey are not unccommon, at least iin that contextt. Other counttries might havve specific",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1403.7400_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg",
      "image_filename": "1403.7400_page0_fig4.jpg",
      "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.",
      "context_before": "4. Fieeld Effects: NNon-Networkks Interactionns\n\nAnothher difference between spaatial or epideemiological netwoorks and humann social netwoorks is that humman social informmation flows ddo not occur onnly through noode-to-node netwoorks but also thhrough field efffects, large-scaale societal eventss which impacct a large grouup of actors coontemporaneouslly. Big events,, weather occuurrences, etc. aall have society-wwide field efffects and ofteen do not difffuse solely througgh interpersonnal interaction (although theey also do greatlyy impact interrpersonal inte raction by afffecting the agendda, mood and ddisposition of inndividuals).\n\nForr example, moost studies agrree that Egypptian social mediaa networks playyed a role in tthe uprising whhich began in Egyypt in January 2011 and weree a key condui t of protest informmation (Lynch,, 2012; Aday et al, 2012; TTufekci and Wilsoon, 2012). Howwever, there waas almost certa inly another impportant informaation diffusionn dynamic. Thee influence of the e Tunisian revoolution itself o n the expectattions of the Egypttian populace was a major turning pointt (Ghonim, 2012; Lynch, 2012) . While analyysis of networkks in Egypt might not have revvealed a majorr difference beetween the secondd and third weeek of Januaryy of 2011, sommething major haad changed in tthe field. To trranslate it into epidemiologicaal language, duue to the Tunnisian revolutioon and the exampple it presentedd, all the “noddes” in the netwwork had a differeent “susceptibiility” and “recceptivity” to innformation about an uprising. TThe downfall oof the Tunisiann president, whichh showed that eeven an enduriing autocracy iin the Middle Eaast was suscepptible to streeet protests, eneergized the oppossition and channged the politiical calculationn in Egypt. This iinformation wwas diffused thhrough multiplle methods and brroadcast mediaa played a keyy role. Thus, thhe communicatioon of the Tuniisia effect to thhe Egyptian neetwork was not neecessarily depeendent on the network struccture of social mmedia.",
      "context_after": "Soccial media itseelf is often inncomprehensibble without referennce to field evvents outside iit. For examplle, take the tweet in Figure 5. The tweets merely statess: “Getting\n\nccrowded underr that bus.” Sttrangely, it haas been tweeteed mmore than sixtty times and favorited morre than 50. Foor thhose followingg in real time, this was an obbvious referencce too New Jersey Governor Chrris Christie’s ppress conferencce inn which he bblamed multiplle aides for thhe closing of a bbridge which caaused massive traffic jams, aallegedly to punnish a mayor whho did not endoorse him. Withhout understanddinng the Chris CChristie press cconference, neeither the tweeet, nnor many retweeets of it are innterpretable.\n\nThe turn to nnetworks as a kkey metaphor iin social scienccees, while fruitfful, should not diminish our attention to thhe mmulti-scale natuure of human ssocial interactioon",
      "referring_paragraphs": [
        "Soccial media itseelf is often inncomprehensibble without referennce to field evvents outside iit. For examplle, take the tweet in Figure 5. The tweets merely statess: “Getting",
        "Figure 5: Cleaar meaning onlyy in context and ttime."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1403.7400_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg",
      "image_filename": "1403.7400_page0_fig5.jpg",
      "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]",
      "context_before": "The turn to nnetworks as a kkey metaphor iin social scienccees, while fruitfful, should not diminish our attention to thhe mmulti-scale natuure of human ssocial interactioon\n\n55. You Namee It, Humans Will Game iit: Reflexivityy aand Humans :\n\nUUnlike disease vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg “trend.” Indeeed, “to trend”” is increasinglly uused as a transsitive verb ammong these useers, as in: “let’s trrend #Hungry44BH”. These eefforts are not mmere blind stabbs aat massive tweeeting; they ofteen display a finne-tuned underrsstanding of Twwitter’s trendiing topics alggorithm, whichh, wwhile not publiic, is widely unnderstood throough reverse ennggineering (Lotaan, 2012). In coordinated caampaigns, Bahhrrain activistss have tre nded hashtaags such aas ##100strikedays , #Bloodyf1, #KillingKhaw aja, #StopTearr-GGasBahrain, annd #F1DontRacceBahrain, amoong others.",
      "context_after": "Campaigns tto trend hashtaags are not limmited to grasssrroots activists. In Figure 6, drawn from mmy primary reessearch in Turkeey, you can seee AKP’s Ankarra mayor, an acctiive figure in TTurkish Twitterr discussed befofore, announcinng thhe hashtag thaat will be “trennded”: #cnnis lying. This waas rretweeted moree than 4000 timmes. He had b een announcinng thhe campaign aand had asked people to be iin front of theeir ddevices at a set time; in these campaiggns, the actuaal hhashtag is ofte n withheld unntil a pre-agreeed time so as tto pproduce a maxximum spike which Twitterr’s algorithm is\n\nsensitiive to. That haashtag indeed trrended worldwwide. Similar cooordinated cammpaigns are commmon in Turkkey and occurredd almost everyy day during thhe contentious protests of June, 22013.\n\nSucch behaviors, aaimed at avoidiing detection, amplifying a signnal, or other go als, by deliberrate gaming of algorithms and mmetrics, should d be expected iin all analysess of human social media. Currenntly, many studdies do take innto account “gamiing” behaviorss such as spamm and bots; hoowever, coordinaated or active aattempts by acctual people too alter metrics orr results, whichh often can onnly be discoverred through qualitaative research, are rarely takeen into accountt.",
      "referring_paragraphs": [
        "Campaigns tto trend hashtaags are not limmited to grasssrroots activists. In Figure 6, drawn from mmy primary reessearch in Turkeey, you can seee AKP’s Ankarra mayor, an acctiive figure in TTurkish Twitterr discussed befofore, announcinng thhe hashtag thaat will be “trennded”: #cnnis lying. This waas rretweeted moree than 4000 timmes. He had b een announcinng thhe campaign aand had asked people to be iin front of theeir ddevices at a set time; in these campaiggns, the actuaal hhashtag is ofte n ",
        "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1403.7400_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1409.0575": [
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/d4ded6bab073da7277a866a06ebb8a636afcf1920147a4504ae4be1288f9bd26.jpg",
      "image_filename": "d4ded6bab073da7277a866a06ebb8a636afcf1920147a4504ae4be1288f9bd26.jpg",
      "caption": "Table 1 Overview of the provided annotations for each of the tasks in ILSVRC.",
      "context_before": "For each image, algorithms produce bounding boxes indicating the position and scale of all instances of all target object categories. The quality of labeling is evaluated by recall, or number of target object instances detected, and precision, or the number of spurious detections produced by the algorithm (see Section 4.3).\n\n[Section: Olga Russakovsky* et al.]\n\n3 In addition, ILSVRC in 2012 also included a taster finegrained classification task, where algorithms would classify dog photographs into one of 120 dog breeds (Khosla et al., 2011). Fine-grained classification has evolved into its own Fine-Grained classification challenge in 2013 (Berg et al., 2013), which is outside the scope of this paper.",
      "context_after": "3 Dataset construction at large scale\n\nOur process of constructing large-scale object recognition image datasets consists of three key steps.\n\nThe first step is defining the set of target object categories. To do this, we select from among the existing ImageNet (Deng et al., 2009) categories. By using WordNet as a backbone (Miller, 1995), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same object category. Since the selection of object categories needs to be done only once per challenge task, we use a combination of automatic heuristics and manual postprocessing to create the list of target categories appropriate for each task. For example, for image classification we may include broader scene categories such as a type of beach, but for single-object localization and object detection we want to focus only on object categories which can be unambiguously localized in images (Sections 3.1.1 and 3.3.1).",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg",
      "image_filename": "1409.0575_page0_fig0.jpg",
      "caption": "Fig. 1 The diversity of data in the ILSVRC image classification and single-object localization tasks. For each of the eight dimensions, we show example object categories along the range of that property. Object scale, number of instances and image clutter for each object category are computed using the metrics defined in Section 3.2.2 and in Appendix B. The other properties were computed by asking human subjects to annotate each of the 1000 object categories (Russakovsky et al., 2013).",
      "context_before": "The 1000 categories used for the image classification task were selected from the ImageNet (Deng et al., 2009) categories. The 1000 synsets are selected such that there is no overlap between synsets: for any synsets $i$ and $j$ , i is not an ancestor of $j$ in the ImageNet hierarchy. These synsets are part of the larger hierarchy and may have children in ImageNet; however, for ILSVRC we do not consider their child subcategories. The synset hierarchy of ILSVRC can be thought of as a “trimmed” version of the complete ImageNet hierarchy. Figure 1 visualizes the diversity of the ILSVRC2012 object categories.\n\nThe exact 1000 synsets used for the image classification and single-object localization tasks have changed over the years. There are 639 synsets which have been used in all five ILSVRC challenges so far. In the first year of the challenge synsets were selected randomly from the available ImageNet synsets at the time, followed by manual filtering to make sure the object categories were not too obscure. With the introduction of\n\n[Section: ImageNet Large Scale Visual Recognition Challenge]",
      "context_after": "[Section: Olga Russakovsky* et al.]\n\nthe object localization challenge in 2011 there were 321 synsets that changed: categories such as “New Zealand beach” which were inherently difficult to localize were removed, and some new categories from ImageNet containing object localization annotations were added. In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.\n\n3.1.2 Collecting candidate images for the image classification dataset",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig1.jpg",
      "image_filename": "1409.0575_page0_fig1.jpg",
      "caption": "Fig. 2 The ILSVRC dataset contains many more fine-grained classes compared to the standard PASCAL VOC benchmark; for example, instead of the PASCAL “dog” category there are 120 different breeds of dogs in ILSVRC2012-2014 classification and single-object localization tasks.",
      "context_before": "3.1.4 Image classification dataset statistics\n\nUsing the image collection and annotation procedure described in previous sections, we collected a largescale dataset used for ILSVRC classification task. There\n\n[Section: ImageNet Large Scale Visual Recognition Challenge]",
      "context_after": "are 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table 2 (top) documents the size of the dataset over the years of the challenge.\n\n3.2 Single-object localization dataset construction\n\nThe single-object localization task evaluates the ability of an algorithm to localize one instance of an object category. It was introduced as a taster task in ILSVRC 2011, and became an official part of ILSVRC in 2012.",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/7de077f802af1e79ff39eb07fd00bc8a84036f471693ad3f82f3b94e45745b51.jpg",
      "image_filename": "7de077f802af1e79ff39eb07fd00bc8a84036f471693ad3f82f3b94e45745b51.jpg",
      "caption": "Image classification annotations (1000 object classes) Additional annotations for single-object localization (1000 object classes)",
      "context_before": "We summarize the crowdsourced bounding box annotation system described in detail in (Su et al., 2012). The goal is to build a system that is fully automated,\n\n[Section: Olga Russakovsky* et al.]\n\n5 Some datasets such as PASCAL VOC (Everingham et al., 2010) and LabelMe (Russell et al., 2007) are able to provide more detailed annotations: for example, marking individual object instances as being truncated. We chose not to provide this level of detail in favor of annotating more images and more object instances.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/5b6a1dae8d157aaaa608103724f0acbd02b8cbfaee57ea527c527f1c07faba74.jpg",
      "image_filename": "5b6a1dae8d157aaaa608103724f0acbd02b8cbfaee57ea527c527f1c07faba74.jpg",
      "caption": "highly accurate, and cost-effective. Given a collection of images where the object of interest has been verified to exist, for each image the system collects a tight bounding box for every instance of the object.",
      "context_before": "",
      "context_after": "highly accurate, and cost-effective. Given a collection of images where the object of interest has been verified to exist, for each image the system collects a tight bounding box for every instance of the object.\n\nThere are two requirements:\n\nThe core challenge of building such a system is effectively controlling the data quality with minimal cost. Our key observation is that drawing a bounding box is significantly more difficult and time consuming than giving answers to multiple choice questions. Thus quality control through additional verification tasks is more cost-effective than consensus-based algorithms. This leads to the following workflow with simple basic subtasks:",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/9eb7bf3ed8872abdc0a6fff6024916e442ce6c52089ae464f7aefdaa7dab8242.jpg",
      "image_filename": "9eb7bf3ed8872abdc0a6fff6024916e442ce6c52089ae464f7aefdaa7dab8242.jpg",
      "caption": "Table 3 Correspondences between the object classes in the PASCAL VOC (Everingham et al., 2010) and the ILSVRC detection task. Object scale is the fraction of image area (reported in percent) occupied by an object instance. It is computed on the validation sets of PASCAL VOC 2012 and of ILSVRC-DET. The average object scale is $2 4 . 1 \\%$ across the 20 PASCAL VOC categories and $2 0 . 3 \\%$ across the 20 corresponding ILSVRC-DET categories. Section 3.3.4 reports additional dataset statistics.",
      "context_before": "3.3 Object detection dataset construction\n\nThe ILSVRC task of object detection evaluates the ability of an algorithm to name and localize all instances of all target objects present in an image. It is much more challenging than object localization because some object instances may be small/occluded/difficult to accurately localize, and the algorithm is expected to locate them all, not just the one it finds easiest.\n\nThere are three key challenges in collecting the object detection dataset. The first challenge is selecting the set of common objects which tend to appear in cluttered photographs and are well-suited for benchmarking object detection performance. Our approach relies on statistics of the object localization dataset and the tradition of the PASCAL VOC challenge (Section 3.3.1).",
      "context_after": "The second challenge is obtaining a much more varied set of scene images than those used for the image classification and single-object localization datasets. Section 3.3.2 describes the procedure for utilizing as much data from the single-object localization dataset as possible and supplementing it with Flickr images queried using hundreds of manually designed high-level queries.\n\nThe third, and biggest, challenge is completely annotating this dataset with all the objects. This is done in two parts. Section 3.3.3 describes the first part: our hierarchical strategy for obtaining the list of all target objects which occur within every image. This is necessary since annotating in a straight-forward way by creating a task for every (image, object class) pair is no longer feasible at this scale. Appendix E describes the second part: annotating the bounding boxes around these objects, using the single-object localization bounding box annotation pipeline of Section 3.2.1 along with extra verification to ensure that every instance of the object is annotated with exactly one bounding box.\n\n[Section: Olga Russakovsky* et al.]",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig2.jpg",
      "image_filename": "1409.0575_page0_fig2.jpg",
      "caption": "Fig. 3 Summary of images collected for the detection task. Images in green (bold) boxes have all instances of all 200 detection object classes fully annotated. Table 4 lists the complete statistics.",
      "context_before": "Staying mindful of the tradition of the PASCAL VOC dataset we also tried to ensure that the set of 200 classes contains as many of the 20 PASCAL VOC classes as possible. Table 3 shows the correspondences. The changes that were done were to ensure more accurate and consistent crowdsourced annotations. The object class with the weakest correspondence is “potted plant” in PASCAL VOC, corresponding to “flower pot” in ILSVRC. “Potted plant” was one of the most challenging object classes to annotate consistently among the PASCAL VOC classes, and in order to obtain accurate annotations using crowdsourcing we had to restrict the definition to a more concrete object.\n\n3.3.2 Collecting images for the object detection dataset\n\nMany images for the detection task were collected differently than the images in ImageNet and the classifica-",
      "context_after": "tion and single-object localization tasks. Figure 3 summarizes the types of images that were collected. Ideally all of these images would be scene images fully annotated with all target categories. However, given budget constraints our goal was to provide as much suitable detection data as possible, even if the images were drawn from a few different sources and distributions.\n\nThe validation and test detection set images come from two sources (percent of images from each source in parentheses). The first source (77%) is images from ILSVRC2012 single-object localization validation and test sets corresponding to the 200 detection classes (or their children in the ImageNet hierarchy). Images where the target object occupied more than 50% of the image area were discarded, since they were unlikely to contain other objects of interest. The second source (23%) is images from Flickr collected specifically for detection task. We queried Flickr using a large set of manually defined queries, such as “kitchenette” or “Australian zoo” to retrieve images of scenes likely to contain several objects of interest. Appendix C contains the full list. We also added pairwise queries, or queries with two target object names such as “tiger lion,” which also often returned cluttered scenes.\n\nFigure 4 shows a random set of both types of validation images. Images were randomly split, with 33% going into the validation set and 67% into the test set.7",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig3.jpg",
      "image_filename": "1409.0575_page0_fig3.jpg",
      "caption": "Fig. 4 Random selection of images in ILSVRC detection validation set. The images in the top 4 rows were taken from ILSVRC2012 single-object localization validation set, and the images in the bottom 4 rows were collected from Flickr using scene-level queries.",
      "context_before": "[Section: ImageNet Large Scale Visual Recognition Challenge]\n\n6 Some of the training objects are actually annotated with more detailed classes: for example, one of the 200 object classes is the category “dog,” and some training instances are annotated with the specific dog breed.\n\n7 The validation/test split is consistent with ILSVRC2012: validation images of ILSVRC2012 remained in the validation set of ILSVRC2013, and ILSVRC2012 test images remained in ILSVRC2013 test set.",
      "context_after": "tage of all the positive examples available. The second source (24%) is negative images which were part of the original ImageNet collection process but voted as negative: for example, some of the images were collected from Flickr and search engines for the ImageNet synset “animals” but during the manual verification step did not collect enough votes to be considered as containing an “animal.” These images were manually re-verified for the detection task to ensure that they did not in fact contain the target objects. The third source (13%)\n\nis images collected from Flickr specifically for the detection task. These images were added for ILSVRC2014 following the same protocol as the second type of images in the validation and test set. This was done to bring the training and testing distributions closer together.\n\n[Section: Olga Russakovsky* et al.]",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig4.jpg",
      "image_filename": "1409.0575_page0_fig4.jpg",
      "caption": "Fig. 5 Consider the problem of binary multi-label annotation. For each input (e.g., image) and each label (e.g., object), the goal is to determine the presence or absense $\\cdot +$ or -) of the label (e.g., decide if the object is present in the image). Multilabel annotation becomes much more efficient when considering real-world structure of data: correlation between labels, hierarchical organization of concepts, and sparsity of labels.",
      "context_before": "tage of all the positive examples available. The second source (24%) is negative images which were part of the original ImageNet collection process but voted as negative: for example, some of the images were collected from Flickr and search engines for the ImageNet synset “animals” but during the manual verification step did not collect enough votes to be considered as containing an “animal.” These images were manually re-verified for the detection task to ensure that they did not in fact contain the target objects. The third source (13%)\n\nis images collected from Flickr specifically for the detection task. These images were added for ILSVRC2014 following the same protocol as the second type of images in the validation and test set. This was done to bring the training and testing distributions closer together.\n\n[Section: Olga Russakovsky* et al.]",
      "context_after": "3.3.3 Complete image-object annotation for the object detection dataset\n\nThe key challenge in annotating images for the object detection task is that all objects in all images need to be labeled. Suppose there are N inputs (images) which need to be annotated with the presence or absence of K labels (objects). A na¨ıve approach would query humans for each combination of input and label, requiring $N K$ queries. However, N and K can be very large and the cost of this exhaustive approach quickly becomes prohibitive. For example, annotating 60, 000 validation and test images with the presence or absence of 200 object classes for the detection task na¨ıvely would take 80 times more effort than annotating $1 5 0 , 0 0 0$ validation and test images with 1 object each for the classification task – and this is not even counting the additional cost of collecting bounding box annotations around each object instance. This quickly becomes infeasible.\n\nIn (Deng et al., 2014) we study strategies for scalable multilabel annotation, or for efficiently acquiring multiple labels from humans for a collection of items. We exploit three key observations for labels in real world applications (illustrated in Figure 5):",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig5.jpg",
      "image_filename": "1409.0575_page0_fig5.jpg",
      "caption": "Fig. 6 Our algorithm dynamically selects the next query to efficiently determine the presence or absence of every object in every image. Green denotes a positive annotation and red denotes a negative annotation. This toy example illustrates a sample progression of the algorithm for one label (cat) on a set of images.",
      "context_before": "Application of the generic multi-class labeling algorithm to our setting. The generic algorithm automatically selects the most informative queries to ask based on object label statistics learned from the training set. In our case of 200 object classes, since obtaining the training set was by itself challenging we chose to design the queries by hand. We created a hierarchy of queries of the type “is there a... in the image?” For example, one of the high-level questions was “is there an animal in the image?” We ask the crowd workers this question about every image we want to label. The children of the “animal” question would correspond to specific examples of animals: for example, “is there a mammal in the image?” or “is there an animal with no legs?” To annotate images efficiently, these questions are asked only on images determined to contain an animal. The 200 leaf node questions correspond to the 200 target objects, e.g., “is there a cat in the image?”. A few sample iterations of the algorithm are shown in Figure 6.\n\nAlgorithm 1 is the formal algorithm for labeling an image with the presence or absence of each target object\n\n[Section: ImageNet Large Scale Visual Recognition Challenge]",
      "context_after": "category. With this algorithm in mind, the hierarchy of questions was constructed following the principle that false positives only add extra cost whereas false negatives can significantly affect the quality of the labeling. Thus, it is always better to stick with more general but less ambiguous questions, such as “is there a mammal in the image?” as opposed to asking overly specific but potentially ambiguous questions, such as “is there an animal that can climb trees?” Constructing this hierarchy was a surprisingly time-consuming process, involving multiple iterations to ensure high accuracy of labeling and avoid question ambiguity. Appendix D shows the constructed hierarchy.\n\nBounding box annotation. Once all images are labeled with the presence or absence of all object categories we use the bounding box system described in Section 3.2.1 along with some additional modifications of Appendix E to annotate the location of every instance of every present object category.\n\n3.3.4 Object detection dataset statistics",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/476f3780bd760a360178d22af023b3d848b58508f488c33f4255792e05036a4b.jpg",
      "image_filename": "476f3780bd760a360178d22af023b3d848b58508f488c33f4255792e05036a4b.jpg",
      "caption": "Object detection annotations (200 object classes)",
      "context_before": "Once the dataset has been collected, we need to define a standardized evaluation procedure for algorithms. Some measures have already been established by datasets such as the Caltech 101 (Fei-Fei et al., 2004) for image classification and PASCAL VOC (Everingham et al., 2012) for both image classification and object detection. To adapt these procedures to the large-scale setting we had to address three key challenges. First, for the image classification and single-object localization tasks only one object category could be labeled in each image due to the scale of the dataset. This created potential ambiguity during evaluation (addressed in Section 4.1). Second, evaluating localization of object instances is inherently difficult in some images which contain a cluster of objects (addressed in Section 4.2). Third, evaluating localization of object instances which occupy few pixels in the image is challenging (addressed in Section 4.3).\n\nIn this section we describe the standardized evaluation criteria for each of the three ILSVRC tasks.\n\n[Section: Olga Russakovsky* et al.]",
      "context_after": "We elaborate further on these and other more minor challenges with large-scale evaluation. Appendix F describes the submission protocol and other details of running the competition itself.\n\n4.1 Image classification\n\nThe scale of ILSVRC classification task (1000 categories and more than a million of images) makes it very expensive to label every instance of every object in every image. Therefore, on this dataset only one object category is labeled in each image. This creates ambiguity in evaluation. For example, an image might be labeled as a “strawberry” but contain both a strawberry and an apple. Then an algorithm would not know which one of the two objects to name. For the image classification task we allowed an algorithm to identify multiple (up to 5) objects in an image and not be penalized as long as one of the objects indeed corresponded to the ground truth label. Figure 7(top row) shows some examples.",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig6.jpg",
      "image_filename": "1409.0575_page0_fig6.jpg",
      "caption": "Image classification Ground truth",
      "context_before": "Here $d ( b _ { i j } , B _ { i k } )$ is the error of localization, defined as 0 if the area of intersection of boxes $b _ { i j }$ and $B _ { i k }$ divided by the areas of their union is greater than 0.5, and 1 otherwise. (Everingham et al., 2010) The error of an algorithm is computed as in Eq. 1.\n\nEvaluating localization is inherently difficult in some images. Consider a picture of a bunch of bananas or a carton of apples. It is easy to classify these images as containing bananas or apples, and even possible to localize a few instances of each fruit. However, in order for evaluation to be accurate every instance of banana or apple needs to be annotated, and that may be impossible. To handle the images where localizing individual object instances is inherently ambiguous we manually discarded 3.5% of images since ILSVRC2012. Some examples of discarded images are shown in Figure 8.\n\n[Section: ImageNet Large Scale Visual Recognition Challenge]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig7.jpg",
      "image_filename": "1409.0575_page0_fig7.jpg",
      "caption": "Accuracy: 1",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig8.jpg",
      "image_filename": "1409.0575_page0_fig8.jpg",
      "caption": "Accuracy: 1",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig9.jpg",
      "image_filename": "1409.0575_page0_fig9.jpg",
      "caption": "Accuracy: 0",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig10.jpg",
      "image_filename": "1409.0575_page0_fig10.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig11.jpg",
      "image_filename": "1409.0575_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig12.jpg",
      "image_filename": "1409.0575_page0_fig12.jpg",
      "caption": "Fig. 7 Tasks in ILSVRC. The first column shows the ground truth labeling on an example image, and the next three show three sample outputs with the corresponding evaluation score. Fig. 8 Images marked as “difficult” in the ILSVRC2012 single-object localization validation set. Please refer to Section 4.2 for details.",
      "context_before": "",
      "context_after": "4.3 Object detection\n\nThe criteria for object detection was adopted from PAS-CAL VOC (Everingham et al., 2010). It is designed to penalize the algorithm for missing object instances, for duplicate detections of one instance, and for false positive detections. Figure 7(bottom row) shows examples.\n\nFor each object class and each image $I _ { i }$ , an algorithm returns predicted detections $( b _ { i j } , s _ { i j } )$ of predicted locations $b _ { i j }$ with confidence scores $s _ { i j }$ . These detections are greedily matched to the ground truth boxes $\\{ B _ { i k } \\}$ using Algorithm 2. For every detection $j$ on image $i$ the algorithm returns $z _ { i j } = 1$ if the detection is",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/e9f666a9da1168e025015b4dbe257e84c44f4e2a49df9a9dc298e29599c4d54a.jpg",
      "image_filename": "e9f666a9da1168e025015b4dbe257e84c44f4e2a49df9a9dc298e29599c4d54a.jpg",
      "caption": "ILSVRC 20 10",
      "context_before": "The winning image classification with provided data team was GoogLeNet, which explored an improved convolutional neural network architecture combining the multi-scale idea with intuitions gained from the Hebbian principle. Additional dimension reduction layers allowed them to increase both the depth and the width\n\n[Section: Olga Russakovsky* et al.]\n\n9 Table 7 omits 4 teams which submitted results but chose not to officially participate in the challenge.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/7cf3dda7925c048a7d6a93d3954442f76e374a3555da2ef5015913970bd8c1da.jpg",
      "image_filename": "7cf3dda7925c048a7d6a93d3954442f76e374a3555da2ef5015913970bd8c1da.jpg",
      "caption": "ILSVRC 20 1 1 ILSVRC 20 1 2",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/3f5c5a02f086d4cc03468e5a30d8bf0000023184e33953489bca3c8a17f2d21b.jpg",
      "image_filename": "3f5c5a02f086d4cc03468e5a30d8bf0000023184e33953489bca3c8a17f2d21b.jpg",
      "caption": "ImageNet Large Scale Visual Recognition Challenge",
      "context_before": "",
      "context_after": "ImageNet Large Scale Visual Recognition Challenge",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/b1f2a0eeee9fea306be14f80c206ec80e1348054662d259a4b82ad22c666f46a.jpg",
      "image_filename": "b1f2a0eeee9fea306be14f80c206ec80e1348054662d259a4b82ad22c666f46a.jpg",
      "caption": "ILSVRC 20 13",
      "context_before": "ImageNet Large Scale Visual Recognition Challenge",
      "context_after": "[Section: ILSVRC 20 13]\n\nOlga Russakovsky* et al.",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/0062726ed7f12b8b637e647e162e8d922565dc3409ce10266103c2d63d26cc7b.jpg",
      "image_filename": "0062726ed7f12b8b637e647e162e8d922565dc3409ce10266103c2d63d26cc7b.jpg",
      "caption": "ILSVRC 20 14",
      "context_before": "[Section: ILSVRC 20 13]\n\nOlga Russakovsky* et al.",
      "context_after": "ImageNet Large Scale Visual Recognition Challenge\n\nof the network significantly without incurring significant computational overhead. In the image classification with external data track, CASIAWS won by using weakly supervised object localization from only classification labels to improve image classification. MCG region proposals (Arbel´aez et al., 2014) pretrained on PASCAL VOC 2012 data are used to extract region proposals, regions are represented using convolutional networks, and a multiple instance learning strategy is used to learn weakly supervised object detectors to represent the image.\n\nIn the single-object localization with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia, 2013). For localization they used per-class bounding box regression similar to OverFeat (Sermanet et al., 2013). In the single-object localization with external data track, Adobe used 2000 additional ImageNet classes to train the classifiers in an integrated convolutional neural network framework for both classification and localization, with bounding box regression. At test time they used k-means to find bounding box clusters and rank the clusters according to the classification scores.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig13.jpg",
      "image_filename": "1409.0575_page0_fig13.jpg",
      "caption": "Fig. 9 Performance of winning entries in the ILSVRC2010- 2014 competitions in each of the three tasks (details about the entries and numerical results are in Section 5.1). There is a steady reduction of error every year in object classification and single-object localization tasks, and a 1.9x improvement in mean average precision in object detection. There are two considerations in making these comparisons. (1) The object categories used in ISLVRC changed between years 2010 and 2011, and between 2011 and 2012. However, the large scale of the data (1000 object categories, 1.2 million training images) has remained the same, making it possible to compare results. Image classification and single-object localization entries shown here use only provided training data. (2) The size of the object detection training data has increased significantly between years 2013 and 2014 (Section 3.3). Section 6.1 discusses the relative effects of training data increase versus algorithmic improvements.",
      "context_before": "6.1.1 Image classification and single-object localization improvement over the years\n\nThere has been a 4.2x reduction in image classification error (from 28.2% to $6 . 7 \\%$ ) and a 1.7x reduction in single-object localization error (from 42.5% to $2 5 . 3 \\%$ ) since the beginning of the challenge. For consistency, here we consider only teams that use the provided training data. Even though the exact object categories have changed (Section 3.1.1), the large scale of the dataset has remained the same (Table 2), making the results comparable across the years. The dataset has not changed since 2012, and there has been a 2.4x reduction in image classification error (from $1 6 . 4 \\%$ to 6.7%) and a 1.3x in\n\n[Section: Olga Russakovsky* et al.]",
      "context_after": "single-object localization error (from $3 3 . 5 \\%$ to $2 5 . 3 \\%$ ) in the past three years.\n\n6.1.2 Object detection improvement over the years\n\nObject detection accuracy as measured by the mean average precision (mAP) has increased 1.9x since the introduction of this task, from 22.6% mAP in ILSVRC2013 to 43.9% mAP in ILSVRC2014. However, these results are not directly comparable for two reasons. First, the size of the object detection training data has increased significantly from 2013 to 2014 (Section 3.3). Second, the 43.9% mAP result was obtained with the addition of the image classification and single-object localization training data. Here we attempt to understand the relative effects of the training set size increase versus algorithmic improvements. All models are evaluated on the same ILSVRC2013-2014 object detection test set.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/6aa0357e198e016aafea2877a60514366fe94019ec013a9ad94646f637d7de8a.jpg",
      "image_filename": "6aa0357e198e016aafea2877a60514366fe94019ec013a9ad94646f637d7de8a.jpg",
      "caption": "Image classification",
      "context_before": "Following the strategy employed by PASCAL VOC (Everingham et al., 2014), for each method we obtain a confidence interval of its score using bootstrap sampling. During each bootstrap round, we sample $N$ images with replacement from all the available $N$ test images and evaluate the performance of the algorithm on those sampled images. This can be done very efficiently by precomputing the accuracy on each image. Given the results of all the bootstrapping rounds we discard the lower and the upper $\\alpha$ fraction. The range of the remaining results represents the $1 - 2 \\alpha$ confi-\n\n[Section: ImageNet Large Scale Visual Recognition Challenge]\n\n10 Personal communication with members of the UvA team.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/e044e461e12ffbf3e6b2aad5dcaf0e3ec9b60612c6ee58397514ef21b3c494be.jpg",
      "image_filename": "e044e461e12ffbf3e6b2aad5dcaf0e3ec9b60612c6ee58397514ef21b3c494be.jpg",
      "caption": "Single-object localization Object detection",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_27",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/472f2396020398c86573b09fe36abb615bfc5297b221a579e37b365b8361c75d.jpg",
      "image_filename": "472f2396020398c86573b09fe36abb615bfc5297b221a579e37b365b8361c75d.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_28",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig14.jpg",
      "image_filename": "1409.0575_page0_fig14.jpg",
      "caption": "Image classification",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_29",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig15.jpg",
      "image_filename": "1409.0575_page0_fig15.jpg",
      "caption": "Single-object localization",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_30",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig16.jpg",
      "image_filename": "1409.0575_page0_fig16.jpg",
      "caption": "Object detection Fig. 10 For each object class, we consider the best performance of any entry submitted to ILSVRC2012-2014, including entries using additional training data. The plots show the distribution of these “optimistic” per-class results. Performance is measured as accuracy for image classification (left) and for single-object localization (middle), and as average precision for object detection (right). While the results are very promising in image classification, the ILSVRC datasets are far from saturated: many object classes continue to be challenging for current algorithms.",
      "context_before": "",
      "context_after": "dence interval. We run a large number of bootstrapping rounds (from 20,000 until convergence). Table 8 shows the results of the top entries to each task of ILSVRC2012-2014. The winning methods are statistically significantly different from the other methods, even at the $9 9 . 9 \\%$ level.\n\n6.3 Current state of categorical object recognition\n\nBesides looking at just the average accuracy across hundreds of object categories and tens of thousands of images, we can also delve deeper to understand where mistakes are being made and where researchers’ efforts should be focused to expedite progress.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_31",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig17.jpg",
      "image_filename": "1409.0575_page0_fig17.jpg",
      "caption": "tiger (100)",
      "context_before": "Image classification\n\nred fox (100) hen-of-the-woods (100)\n\ngoldfinch (10o)flat-coated retriever (100)",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_32",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig18.jpg",
      "image_filename": "1409.0575_page0_fig18.jpg",
      "caption": "hamster(100)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_33",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig19.jpg",
      "image_filename": "1409.0575_page0_fig19.jpg",
      "caption": "porcupine (100)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_34",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig20.jpg",
      "image_filename": "1409.0575_page0_fig20.jpg",
      "caption": "stingray (100)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_35",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig21.jpg",
      "image_filename": "1409.0575_page0_fig21.jpg",
      "caption": "Blenheim spaniel (100)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_36",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig22.jpg",
      "image_filename": "1409.0575_page0_fig22.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_37",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig23.jpg",
      "image_filename": "1409.0575_page0_fig23.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_38",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig24.jpg",
      "image_filename": "1409.0575_page0_fig24.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_39",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig25.jpg",
      "image_filename": "1409.0575_page0_fig25.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_40",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig26.jpg",
      "image_filename": "1409.0575_page0_fig26.jpg",
      "caption": "Hardest classes",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_41",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig27.jpg",
      "image_filename": "1409.0575_page0_fig27.jpg",
      "caption": "hook (66)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_42",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig28.jpg",
      "image_filename": "1409.0575_page0_fig28.jpg",
      "caption": "spotlight (66)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_43",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig29.jpg",
      "image_filename": "1409.0575_page0_fig29.jpg",
      "caption": "ladle (65)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_44",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig30.jpg",
      "image_filename": "1409.0575_page0_fig30.jpg",
      "caption": "restaurant (64)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_45",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig31.jpg",
      "image_filename": "1409.0575_page0_fig31.jpg",
      "caption": "letteropener (59)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_46",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig32.jpg",
      "image_filename": "1409.0575_page0_fig32.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_47",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig33.jpg",
      "image_filename": "1409.0575_page0_fig33.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_48",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig34.jpg",
      "image_filename": "1409.0575_page0_fig34.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_49",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig35.jpg",
      "image_filename": "1409.0575_page0_fig35.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_50",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig36.jpg",
      "image_filename": "1409.0575_page0_fig36.jpg",
      "caption": "Single-object localization",
      "context_before": "",
      "context_after": "Single-object localization\n\ndy turnstone (10o) giant schnauzer (99)",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_51",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig37.jpg",
      "image_filename": "1409.0575_page0_fig37.jpg",
      "caption": "tiger (99)",
      "context_before": "Single-object localization\n\ndy turnstone (10o) giant schnauzer (99)",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_52",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig38.jpg",
      "image_filename": "1409.0575_page0_fig38.jpg",
      "caption": "Maltese dog (99)Japanese spaniel (99)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_53",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig39.jpg",
      "image_filename": "1409.0575_page0_fig39.jpg",
      "caption": "Tibetan mastiff (99)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_54",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig40.jpg",
      "image_filename": "1409.0575_page0_fig40.jpg",
      "caption": "hare (99)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_55",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig41.jpg",
      "image_filename": "1409.0575_page0_fig41.jpg",
      "caption": "African hunting dog (99)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_56",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig42.jpg",
      "image_filename": "1409.0575_page0_fig42.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_57",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig43.jpg",
      "image_filename": "1409.0575_page0_fig43.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_58",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig44.jpg",
      "image_filename": "1409.0575_page0_fig44.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_59",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig45.jpg",
      "image_filename": "1409.0575_page0_fig45.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_60",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig46.jpg",
      "image_filename": "1409.0575_page0_fig46.jpg",
      "caption": "Hardest classes",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_61",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig47.jpg",
      "image_filename": "1409.0575_page0_fig47.jpg",
      "caption": "spotlight (35)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_62",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig48.jpg",
      "image_filename": "1409.0575_page0_fig48.jpg",
      "caption": "wing (35)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_63",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig49.jpg",
      "image_filename": "1409.0575_page0_fig49.jpg",
      "caption": "ladle (28)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_64",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig50.jpg",
      "image_filename": "1409.0575_page0_fig50.jpg",
      "caption": "pole (27)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_65",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig51.jpg",
      "image_filename": "1409.0575_page0_fig51.jpg",
      "caption": "spacebar (23)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_66",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig52.jpg",
      "image_filename": "1409.0575_page0_fig52.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_67",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig53.jpg",
      "image_filename": "1409.0575_page0_fig53.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_68",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig54.jpg",
      "image_filename": "1409.0575_page0_fig54.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_69",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig55.jpg",
      "image_filename": "1409.0575_page0_fig55.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_70",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig56.jpg",
      "image_filename": "1409.0575_page0_fig56.jpg",
      "caption": "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The numbers in parentheses indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with $1 0 0 \\%$ accuracy. Object detection results are shown in Figure 12.",
      "context_before": "",
      "context_after": "[Section: Olga Russakovsky* et al.]",
      "referring_paragraphs": [
        "Object detection results are shown in Figure 12. The easiest classes are living organisms such as “dog” and “tiger”, plus “basketball” and “volleyball” with distinctive shape and color, and a somewhat surprising “snowplow.” The easiest class “butterfly” is not yet perfectly detected but is very close with 92.7% AP. The hardest classes are as expected small thin objects such as “flute” and “nail”, and the highly varied “lamp” and “backpack” classes, with as low as 8.0% AP.",
        "Object detection results are shown in Figure 12.",
        "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The numbers in parentheses indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with $1 0 0 \\%$ accuracy. Object detection results are shown in Figure 12.",
        "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst results. The numbers in parentheses indicate average precision. Image classification and single-object localization results are shown in Figure 11."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_71",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig57.jpg",
      "image_filename": "1409.0575_page0_fig57.jpg",
      "caption": "butterfly (93) basketball (80)",
      "context_before": "[Section: Olga Russakovsky* et al.]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_72",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig58.jpg",
      "image_filename": "1409.0575_page0_fig58.jpg",
      "caption": "dog (84) snowplow (80)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_73",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig59.jpg",
      "image_filename": "1409.0575_page0_fig59.jpg",
      "caption": "volleyball(83) bird (78)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_74",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig60.jpg",
      "image_filename": "1409.0575_page0_fig60.jpg",
      "caption": "rabbit (83) tiger (77)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_75",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig61.jpg",
      "image_filename": "1409.0575_page0_fig61.jpg",
      "caption": "frog (82) zebra (77)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_76",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig62.jpg",
      "image_filename": "1409.0575_page0_fig62.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_77",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig63.jpg",
      "image_filename": "1409.0575_page0_fig63.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_78",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig64.jpg",
      "image_filename": "1409.0575_page0_fig64.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_79",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig65.jpg",
      "image_filename": "1409.0575_page0_fig65.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_80",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig66.jpg",
      "image_filename": "1409.0575_page0_fig66.jpg",
      "caption": "Hardest classes",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_81",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig67.jpg",
      "image_filename": "1409.0575_page0_fig67.jpg",
      "caption": "lamp (15)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_82",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig68.jpg",
      "image_filename": "1409.0575_page0_fig68.jpg",
      "caption": "flute (15) microphone (11) rubber eraser (10)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_83",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig69.jpg",
      "image_filename": "1409.0575_page0_fig69.jpg",
      "caption": "horizontal bar (14)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_84",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig70.jpg",
      "image_filename": "1409.0575_page0_fig70.jpg",
      "caption": "spatula (13)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_85",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig71.jpg",
      "image_filename": "1409.0575_page0_fig71.jpg",
      "caption": "nail (13) backpack (8)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_86",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig72.jpg",
      "image_filename": "1409.0575_page0_fig72.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_87",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig73.jpg",
      "image_filename": "1409.0575_page0_fig73.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_88",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg",
      "image_filename": "1409.0575_page0_fig74.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_89",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig75.jpg",
      "image_filename": "1409.0575_page0_fig75.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_90",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig76.jpg",
      "image_filename": "1409.0575_page0_fig76.jpg",
      "caption": "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst results. The numbers in parentheses indicate average precision. Image classification and single-object localization results are shown in Figure 11.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For image classification, 121 out of 1000 object classes have $1 0 0 \\%$ image classification accuracy according to the optimistic estimate. Figure 11 (top) shows a random set of 10 of them. They contain a variety of classes, such as mammals like “red fox” and animals with distinctive structures like “stingray”. The hardest classes in the image classification task, with accuracy as low as $5 9 . 0 \\%$ , include metallic and see-through man-made objects, such as “hook” and “water bottle,” the mat",
        "Figure 11 (top) shows a random set of 10 of them.",
        "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The numbers in parentheses indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with $1 0 0 \\%$ accuracy. Object detection results are shown in Figure 12.",
        "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst results. The numbers in parentheses indicate average precision. Image classification and single-object localization results are shown in Figure 11."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_91",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig77.jpg",
      "image_filename": "1409.0575_page0_fig77.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_92",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig78.jpg",
      "image_filename": "1409.0575_page0_fig78.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_93",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig79.jpg",
      "image_filename": "1409.0575_page0_fig79.jpg",
      "caption": "Object detection Average scale of object Fig. 13 Performance of the “optimistic” method as a function of object scale in the image, on each task. Each dot corresponds to one object class. Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. “Optimistic” performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014 (including entries with additional training data). The test set has remained the same over these three years. We see that accuracy tends to increase as the objects get bigger in the image. However, it is clear that far from all the variation in accuracy on these classes can be accounted for by scale alone.",
      "context_before": "",
      "context_after": "[Section: ImageNet Large Scale Visual Recognition Challenge]\n\nThe “optimistic” model on each of the three tasks performs statistically significantly better on deformable objects compared to rigid ones. However, this effect disappears when analyzing natural objects separately from man-made objects.\n\n– Amount of texture: none (e.g. punching bag), low (e.g. horse), medium (e.g. sheep) or high (e.g. honeycomb)",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_94",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig80.jpg",
      "image_filename": "1409.0575_page0_fig80.jpg",
      "caption": "Real-world size",
      "context_before": "[Section: Olga Russakovsky* et al.]\n\n11 For rigid versus deformable objects, the average scale in each bin is $3 4 . 1 \\% - 3 4 . 2 \\%$ for classification and localization, and $1 3 . 5 \\% - 1 3 . 7 \\%$ for detection. For texture, the average scale in each of the four bins is $3 1 . 1 \\% - 3 1 . 3 \\%$ for classification and localization, and $1 2 . 7 \\% - 1 2 . 8 \\%$ for detection.\n\nDeformability within instance. In Figure 14(second row) it is clear that the “optimistic” model performs statistically significantly worse on rigid objects than on deformable objects. Image classification accuracy is $9 3 . 2 \\%$",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_95",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig81.jpg",
      "image_filename": "1409.0575_page0_fig81.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_96",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig82.jpg",
      "image_filename": "1409.0575_page0_fig82.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_97",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig83.jpg",
      "image_filename": "1409.0575_page0_fig83.jpg",
      "caption": "Deformability within instance",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_98",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig84.jpg",
      "image_filename": "1409.0575_page0_fig84.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_99",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig85.jpg",
      "image_filename": "1409.0575_page0_fig85.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_100",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig86.jpg",
      "image_filename": "1409.0575_page0_fig86.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_101",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig87.jpg",
      "image_filename": "1409.0575_page0_fig87.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_102",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig88.jpg",
      "image_filename": "1409.0575_page0_fig88.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_103",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig89.jpg",
      "image_filename": "1409.0575_page0_fig89.jpg",
      "caption": "Amount of texture",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_104",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig90.jpg",
      "image_filename": "1409.0575_page0_fig90.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_105",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig91.jpg",
      "image_filename": "1409.0575_page0_fig91.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_106",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg",
      "image_filename": "1409.0575_page0_fig92.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_107",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig93.jpg",
      "image_filename": "1409.0575_page0_fig93.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_108",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig94.jpg",
      "image_filename": "1409.0575_page0_fig94.jpg",
      "caption": "Fig. 14 Performance of the “optimistic” computer vision model as a function of object properties. The x-axis corresponds to object properties annotated by human labelers for each object class (Russakovsky et al., 2013) and illustrated in Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.",
      "context_before": "",
      "context_after": "[Section: ImageNet Large Scale Visual Recognition Challenge]\n\non rigid objects ( $\\mathrm { C I 9 2 . 6 \\% - 9 3 . 8 \\% }$ , much smaller than $9 5 . 7 \\%$ on deformable ones. Single-object localization accuracy is $7 6 . 2 \\%$ on rigid objects (CI $7 4 . 9 \\% - 7 7 . 4 \\%$ ), much smaller than $8 4 . 7 \\%$ on deformable ones. Object detection mAP is $4 0 . 1 \\%$ on rigid objects (CI $3 7 . 2 \\% -$ $4 2 . 9 \\%$ ), much smaller than 44.8% on deformable ones.\n\nWe can further analyze the effects of deformability after separating object classes into “natural” and “man-made” bins based on the ImageNet hierarchy. Deformability is highly correlated with whether the object is natural or man-made: 0.72 correlation for image classification and single-object localization classes, and 0.61 for object detection classes. Figure 14(third row) shows the effect of deformability on performance of the model for man-made and natural objects separately.",
      "referring_paragraphs": [
        "This section provides an overview and history of each of the three tasks. Table 1 shows summary statistics.",
        "The 1000 categories used for the image classification task were selected from the ImageNet (Deng et al., 2009) categories. The 1000 synsets are selected such that there is no overlap between synsets: for any synsets $i$ and $j$ , i is not an ancestor of $j$ in the ImageNet hierarchy. These synsets are part of the larger hierarchy and may have children in ImageNet; however, for ILSVRC we do not consider their child subcategories. The synset hierarchy of ILSVRC can be thought of as a “trimmed” ver",
        "Besides considering image-level properties we can also observe how accuracy changes as a function of intrinsic object properties. We define three properties inspired by human vision: the real-world size of the object, whether it’s deformable within instance, and how textured it is. For each property, the object classes are assigned to one of a few bins (listed below). These properties are illustrated in Figure 1.",
        "Table 1 shows summary statistics.",
        "Table 1 Overview of the provided annotations for each of the tasks in ILSVRC.",
        "Figure 1 visualizes the diversity of the ILSVRC2012 object categories.",
        "These properties are illustrated in Figure 1.",
        "The x-axis corresponds to object properties annotated by human labelers for each object class (Russakovsky et al., 2013) and illustrated in Figure 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_109",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/0d1c0f7184077304fae08b0f612382f3fa8b85acc5e5a80dcc1d7b809de49903.jpg",
      "image_filename": "0d1c0f7184077304fae08b0f612382f3fa8b85acc5e5a80dcc1d7b809de49903.jpg",
      "caption": "Table 9 Human classification results on the ILSVRC2012- 2014 classification test set, for two expert annotators A1 and A2. We report top-5 classification error.",
      "context_before": "The results are reported in Table 9.\n\nAnnotator 1. Annotator A1 evaluated a total of 1500 test set images. The GoogLeNet classification error on this sample was estimated to be 6.8% (recall that the error on full test set of 100,000 images is $6 . 7 \\%$ , as shown in Table 7). The human error was estimated to be ${ \\bf 5 . 1 \\% }$ . Thus, annotator A1 achieves a performance superior to GoogLeNet, by approximately 1.7%. We can analyze the statistical significance of this result under the null hypothesis that they are from the same distribution. In particular, comparing the two proportions with a z-test yields a one-sided $p$ -value of $p = 0 . 0 2 2$ . Thus, we can conclude that this result is statistically significant at the 95% confidence level.\n\nAnnotator 2. Our second annotator (A2) trained on a smaller sample of only 100 images and then labeled 258 test set images. As seen in Table 9, the final classification error is significantly worse, at approximately $1 2 . 0 \\%$ Top-5 error. The majority of these errors (48.8%) can be attributed to the annotator failing to spot and consider the ground truth label as an option.",
      "context_after": "Thus, we conclude that a significant amount of training time is necessary for a human to achieve competitive performance on ILSVRC. However, with a sufficient amount of training, a human annotator is still able to outperform the GoogLeNet result ( $p = 0 . 0 2 2$ ) by approximately 1.7%.\n\nAnnotator comparison. We also compare the prediction accuracy of the two annotators. Of a total of 204 images that both A1 and A2 labeled, 174 (85%) were correctly labeled by both A1 and A2, 19 (9%) were correctly labeled by A1 but not A2, 6 (3%) were correctly labeled by A2 but not A1, and 5 ( $2 \\%$ ) were incorrectly labeled by both. These include 2 images that we consider to be incorrectly labeled in the ground truth.\n\nIn particular, our results suggest that the human annotators do not exhibit strong overlap in their predictions. We can approximate the performance of an “optimistic” human classifier by assuming an image to be correct if at least one of A1 or A2 correctly labeled the image. On this sample of 204 images, we approximate the error rate of an “optimistic” human annotator at $2 . 4 \\%$ , compared to the GoogLeNet error rate of $4 . 9 \\%$ .",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_110",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig95.jpg",
      "image_filename": "1409.0575_page0_fig95.jpg",
      "caption": "Fig. 15 Representative validation images that highlight common sources of error. For each image, we display the ground truth in blue, and top 5 predictions from GoogLeNet follow (red = wrong, green $=$ right). GoogLeNet predictions on the validation set images were graciously provided by members of the GoogLeNet team. From left to right: Images that contain multiple objects, images of extreme closeups and uncharacteristic views, images with filters, images that significantly benefit from the ability to read text, images that contain very small and thin objects, images with abstract representations, and example of a fine-grained image that GoogLeNet correctly identifies but a human would have significant difficulty with.",
      "context_before": "Types of errors in both computer and human annotations:\n\n1. Multiple objects. Both GoogLeNet and humans struggle with images that contain multiple ILSVRC\n\n[Section: ImageNet Large Scale Visual Recognition Challenge]",
      "context_after": "classes (usually many more than five), with little indication of which object is the focus of the image. This error is only present in the Classification setting, since every image is constrained to have exactly one correct label. In total, we attribute 24 (24%) of GoogLeNet errors and 12 (16%) of human errors to this category. It is worth noting that humans can have a slight advantage in this error type, since it can sometimes be easy to identify the most salient object in the image.\n\n2. Incorrect annotations. We found that approximately 5 out of 1500 images (0.3%) were incorrectly annotated in the ground truth. This introduces an approximately equal number of errors for both humans and GoogLeNet.\n\nTypes of errors that the computer is more susceptible to than the human:",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_111",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig96.jpg",
      "image_filename": "1409.0575_page0_fig96.jpg",
      "caption": "1000 classes of ILSVRC2012-2014 single-object localization (dark green) versus 20 classes of PASCAL 2012 (light blue)",
      "context_before": "◦ stringed instrument\n\n[Section: Olga Russakovsky* et al.]\n\n1000 classes of ILSVRC2012-2014 single-object localization (dark green) versus 20 classes of PASCAL 2012 (light blue)",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_112",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig97.jpg",
      "image_filename": "1409.0575_page0_fig97.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_113",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig98.jpg",
      "image_filename": "1409.0575_page0_fig98.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_114",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig99.jpg",
      "image_filename": "1409.0575_page0_fig99.jpg",
      "caption": "200 hardest classes of ILSVRC2012-2014 single-object localization (dark green) versus 20 classes of PASCAL 2012 (light blue)",
      "context_before": "",
      "context_after": "200 hardest classes of ILSVRC2012-2014 single-object localization (dark green) versus 20 classes of PASCAL 2012 (light blue)",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_115",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig100.jpg",
      "image_filename": "1409.0575_page0_fig100.jpg",
      "caption": "200 hardest classes of ILSVRC2012-2014 single-object localization (dark green) versus 20 classes of PASCAL 2012 (light blue)",
      "context_before": "200 hardest classes of ILSVRC2012-2014 single-object localization (dark green) versus 20 classes of PASCAL 2012 (light blue)",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_116",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig101.jpg",
      "image_filename": "1409.0575_page0_fig101.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_117",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig102.jpg",
      "image_filename": "1409.0575_page0_fig102.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_118",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig103.jpg",
      "image_filename": "1409.0575_page0_fig103.jpg",
      "caption": "Fig. 16 Distribution of various measures of localization difficulty on the ILSVRC2012-2014 single-object localization (dark green) and PASCAL VOC 2012 (light blue) validation sets. Object scale is fraction of image area occupied by an average object instance. Chance performance of localization and level of clutter are defined in Appendix B. The plots on top contain the full ILSVRC validation set with 1000 classes; the plots on the bottom contain 200 ILSVRC classes with the lowest chance performance of localization. All plots contain all 20 classes of PASCAL VOC.",
      "context_before": "",
      "context_after": "• food: something you can eat or drink (includes growing fruit, vegetables and mushrooms, but does not include living animals)\n\n◦ food with bread or crust: pretzel, bagel, pizza, hotdog, hamburgers, etc\n\n• items that run on electricity (plugged in or using batteries); including clocks, microphones, traffic lights, computers, etc",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1409.0575_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1412.3756": [
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/e977bbbb1f5d6908f0c64c40216f1a432e8341b0ba65686b0bdfa392b9094f8e.jpg",
      "image_filename": "e977bbbb1f5d6908f0c64c40216f1a432e8341b0ba65686b0bdfa392b9094f8e.jpg",
      "caption": "Tab. 1: A confusion matrix",
      "context_before": "[Section: 3 Disparate Impact and Error Rates]\n\ngender-discrimination scenarios the value 0 would be assigned to “female” and 1 to “male”. We will denote a successful binary classification outcome C (say, a hiring decision) by $C = \\Upsilon \\mathrm { E } S$ and a failure by $C = \\Nu \\mathrm { O }$ . Finally, we will map the majority class to “positive” examples and the minority class to “negative” examples with respect to the classification outcome, all the while reminding the reader that this is merely a convenience to do the mapping, and does not reflect any judgments about the classes. The advantage of this mapping is that it renders our results more intuitive: a classifier with high “error” will also be one that is least biased, because it is unable to distinguish the two classes.\n\nTable 1 describes the confusion matrix for a classification with respect to the above attributes where each entry is the probability of that particular pair of outcomes for data sampled from the input distribution (we use the empirical distribution when referring to a specific data set).",
      "context_after": "The $8 0 \\%$ rule can then be quantified as:\n\n$$ \\frac {c / (a + c)}{d / (b + d)} \\geq 0. 8 $$\n\nNote that the traditional notion of “accuracy” includes terms in the numerator from both columns, and so cannot be directly compared to the $8 0 \\%$ rule. Still, other class-sensitive error metrics are known, and more directly relate to the $8 0 \\%$ rule:",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/662a568e00e1343326d4ffec69a327979ed20d5b28c7c296d381b6a979657128.jpg",
      "image_filename": "662a568e00e1343326d4ffec69a327979ed20d5b28c7c296d381b6a979657128.jpg",
      "caption": "Tab. 2: Confusion matrix for $g$",
      "context_before": "Proof. We will start with the direction showing that disparate impact implies predictability. Suppose that there exists some function $g : Y \\to C$ such that $\\begin{array} { r } { \\dot { \\mathrm { L R } } _ { + } ( g ( \\dot { y } ) , c ) \\dot { \\geq } \\frac { 1 } { \\tau } } \\end{array}$ . We will create a function $\\psi : C \\to X$ such that $\\mathrm { B E R } \\big ( \\psi ( g ( y ) ) , x \\big ) \\ < \\ \\epsilon$ for $( x , y ) \\in D$ . Thus the combined predictor $f = \\psi \\circ g$ satisfies the definition of predictability.\n\nConsider the confusion matrix associated with $g .$ , depicted in Table 2. Set $\\alpha \\triangleq { \\frac { b } { b + d } }$\n\n[Section: 4 Computational Fairness]",
      "context_after": "$\\textstyle { \\beta \\triangleq { \\frac { c } { a + c } } }$ . Then we can write $\\begin{array} { r } { \\operatorname { L R } _ { + } ( g ( y ) , X ) = \\frac { 1 - \\alpha } { \\beta } } \\end{array}$ and $\\begin{array} { r } { \\mathsf { D } \\mathsf { I } ( g ) = \\frac { \\beta } { 1 - \\alpha } } \\end{array}$ .\n\nWe define the purely biased mapping $\\psi \\colon C \\to X$ as $\\psi ( \\mathrm { Y E S } ) = 1$ and $\\psi ( \\mathrm { N O } ) = 0$ . Finally, let $\\phi \\colon Y X = \\psi \\circ g$ . The confusion matrix for $\\phi$ is depicted in Table 3. Note that the confusion matrix for $\\phi$ is identical to the matrix for $g$ .",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/d0ff56f4b59918a5562ee229c709b52a66c48d199445c376d56eef6de6a519e1.jpg",
      "image_filename": "d0ff56f4b59918a5562ee229c709b52a66c48d199445c376d56eef6de6a519e1.jpg",
      "caption": "Tab. 3: Confusion matrix for $\\phi$",
      "context_before": "$\\textstyle { \\beta \\triangleq { \\frac { c } { a + c } } }$ . Then we can write $\\begin{array} { r } { \\operatorname { L R } _ { + } ( g ( y ) , X ) = \\frac { 1 - \\alpha } { \\beta } } \\end{array}$ and $\\begin{array} { r } { \\mathsf { D } \\mathsf { I } ( g ) = \\frac { \\beta } { 1 - \\alpha } } \\end{array}$ .\n\nWe define the purely biased mapping $\\psi \\colon C \\to X$ as $\\psi ( \\mathrm { Y E S } ) = 1$ and $\\psi ( \\mathrm { N O } ) = 0$ . Finally, let $\\phi \\colon Y X = \\psi \\circ g$ . The confusion matrix for $\\phi$ is depicted in Table 3. Note that the confusion matrix for $\\phi$ is identical to the matrix for $g$ .",
      "context_after": "We can now express $\\operatorname { B E R } { \\big ( } \\phi { \\big ) }$ in terms of this matrix. Specifically, $\\begin{array} { r } { \\mathtt { B E R } ( \\phi ) = \\frac { \\alpha + \\beta } { 2 } } \\end{array}$\n\nRepresentations. We can now express contours of the DI and BER functions as curves in the unit square $[ 0 , 1 ] ^ { 2 }$ . Reparametrizing $\\pi _ { 1 } = 1 - \\alpha$ and $\\pi _ { 0 } = \\beta ,$ we can express the error measures as $\\begin{array} { r } { \\mathsf { D I } ( g ) = \\frac { \\pi _ { 0 } } { \\pi _ { 1 } } } \\end{array}$ and $\\begin{array} { r } { \\mathtt { B E R } ( \\phi ) = \\frac { 1 + \\pi _ { 0 } - \\pi _ { 1 } } { 2 } } \\end{array}$ 2\n\nAs a consequence, any classifier $g$ with $\\mathsf { D } \\mathsf { I } ( g ) = \\delta$ can be represented in the $[ 0 , 1 ] ^ { 2 }$ unit square as the line $\\pi _ { 1 } = \\pi _ { 0 } / \\delta$ . Any classifier $\\phi$ with $\\mathtt { B E R } ( \\phi ) = \\epsilon$ can be written as the function $\\pi _ { 1 } = \\pi _ { 0 } + 1 - 2 \\epsilon$ .",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg",
      "image_filename": "1412.3756_page0_fig0.jpg",
      "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.",
      "context_before": "Proof. In order to show that $\\bar { D }$ strongly preserves rank, recall that we would like to show that $F _ { x } ( y ) = F _ { x } ( \\bar { y } )$ for all $x \\in X , \\bar { y } \\in \\bar { Y } _ { x } ,$ , and $y \\in Y _ { x }$ . Since, by definition of our algorithm, $\\bar { y } = F _ { A } ^ { - 1 } ( F _ { x } ( y ) ) .$ , we know that $F _ { x } ( \\bar { y } ) = F _ { x } ( F _ { A } ^ { - 1 } ( F _ { x } ( y ) ) )$ , so we would like to show that $F _ { x } ( F _ { A } ^ { - 1 } ( z ) ) = z$ for all $z \\in [ 0 , 1 ]$ and for all $x$ . Recall that $F _ { A } ^ { - 1 } ( z ) = \\mathrm { m e d i a n } _ { x \\in X } F _ { x } ^ { - 1 } ( z )$ .\n\nSuppose the above claim is not true. Then there are two values $z _ { 1 } < z _ { 2 }$ and some value $x$ such that $F _ { x } ( F _ { A } ^ { - 1 } ( z _ { 1 } ) ) > F _ { x } ( F _ { A } ^ { - 1 } ( z _ { 2 } ) )$ . That is, there is some $x$ and two elements $y _ { 1 } = F _ { A } ^ { - 1 } ( z _ { 1 } ) , y _ { 2 } = F _ { A } ^ { - \\bar { 1 } } \\bar { ( z _ { 2 } ) }$ such that $y _ { 1 } > y _ { 2 }$ . Now we know that y1 = median $_ { x \\in X } F _ { x } ^ { - 1 } \\left( z _ { 1 } \\right)$\n\n[Section: 5 Removing disparate impact]",
      "context_after": "Therefore, if $y _ { 1 } > y _ { 2 }$ it must be that there are strictly less than $| X | / 2$ elements of the set $\\{ F _ { x } ^ { - 1 } ( z _ { 1 } ) | x \\in X \\}$ below $y _ { 2 }$ . But by the assumption that $z _ { 1 } < z _ { 2 } ,$ we know that each element of $\\{ F _ { x } ^ { - 1 } ( z _ { 1 } ) | x \\in X \\}$ is above the corresponding element of $\\{ F _ { x } ^ { - 1 } ( z _ { 2 } ) | x \\in X \\}$ and there are $| X | / 2$ elements of this latter set below $y _ { 2 }$ by definition. Hence we have a contradiction and so a flip cannot occur, which means that the claim is true.\n\nNote that the resulting $\\hat { Y } _ { x }$ distributions are the same for all $x \\in X ,$ , so there is no way for Bob to differentiate between the protected attributes. Hence the algorithm is 1-fair.\n\nThis repair has the effect that if you consider the $\\bar { Y }$ values at some rank $z ,$ , the probability of the occurrence of a data item with attribute $x \\in X$ is the same as the probability of the occurrence of $x$ in the full population. This informal observation gives the intuitive backing for the lack of predictability of X from $\\bar { Y }$ and, hence, the lack of disparate impact in the repaired version of the data.",
      "referring_paragraphs": [
        "Table 1 describes the confusion matrix for a classification with respect to the above attributes where each entry is the probability of that particular pair of outcomes for data sampled from the input distribution (we use the empirical distribution when referring to a specific data set).",
        "Algorithm. Our repair algorithm creates $\\bar { Y } _ { \\cdot }$ , such that for all $y \\in Y _ { x } ,$ the corresponding $\\bar { y } = F _ { A } ^ { - 1 } ( F _ { x } ( y ) )$ . The resulting $\\bar { D } = ( X , \\bar { Y } , C )$ changes only Y while the protected attribute and class remain the same as in the original data, thus preserving the ability to predict the class. See Figure 1 for an example.",
        "See Figure 1 for an example."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg",
      "image_filename": "1412.3756_page0_fig1.jpg",
      "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.",
      "context_before": "6.2 Fairness / Utility Tradeoff\n\nThe goal in this section is to determine how much the partial repair procedure degrades utility. Using the same data sets as described above, we will examine how the utility (see Definition 5.3) changes DI (measuring fairness) increases. Utility will be defined with respect to the data labels. Note that this may itself be faulty data, in that the labels may not themselves provide the best possible utility based on the underlying, but perhaps\n\n[Section: 6 Experiments]",
      "context_after": "[Section: 6 Experiments]",
      "referring_paragraphs": [
        "Consider the confusion matrix associated with $g .$ , depicted in Table 2. Set $\\alpha \\triangleq { \\frac { b } { b + d } }$",
        "In Figure 2 we can see that there are no data points greater than the BER threshold and also much below $\\tau = 0 . 8$ , the threshold for legal disparate impact. The only false positives are a few points very close to the line. This is likely because the $\\beta$ value, as measured from the data, has some error. We can also see, from the points close to the BER threshold line on its left but below τ that while we chose the threshold conservatively, we were not overly conservative. Still, using a",
        "Consider the confusion matrix associated with $g .$ , depicted in Table 2.",
        "In Figure 2 we can see that there are no data points greater than the BER threshold and also much below $\\tau = 0 ."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "image_filename": "1412.3756_page0_fig2.jpg",
      "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "context_before": "[Section: 6 Experiments]",
      "context_after": "unobservable, desired outcomes. For example, the results on the test from the Ricci data may not perfectly measure a firefighter’s ability and so outcomes based on that test may not correctly predict who should be promoted. Still, in the absence of knowledge of more precise data, we will use these labels to measure utility. For the Ricci data, which is unlabeled, we will assume that the true labels are those provided by the simple threshold classifier used on the non-repaired version of the Ricci data, i.e. that anyone with a score of at least $7 0 \\%$ should pass the exam. Disparate impact (DI) for all data sets is measured with respect to the predicted outcomes on the test set as differentiated by protected attribute. The SVM described above is used to classify on the Adult Income and German Credit data sets while the Ricci data uses the simple threshold classifier. The utility (1 − BER) shown is based on the confusion matrix of the original labels versus the labels predicted by these classifiers.\n\nThe results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases. Each unrepaired data set begins with $\\mathsf { D } \\mathsf { I } < 0 . 8 ,$ , i.e., it would fail the $8 0 \\%$ rule, and we are able to repair it to a legal value. For the Adult Income data set, repairing the data fully only results in a utility loss from about $7 4 \\%$ to $7 2 \\%$ , while for the German\n\n[Section: 6 Experiments]",
      "referring_paragraphs": [
        "We define the purely biased mapping $\\psi \\colon C \\to X$ as $\\psi ( \\mathrm { Y E S } ) = 1$ and $\\psi ( \\mathrm { N O } ) = 0$ . Finally, let $\\phi \\colon Y X = \\psi \\circ g$ . The confusion matrix for $\\phi$ is depicted in Table 3. Note that the confusion matrix for $\\phi$ is identical to the matrix for $g$ .",
        "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases. Each unrepaired data set begins with $\\mathsf { D } \\mathsf { I } < 0 . 8 ,$ , i.e., it would fail the $8 0 \\%$ rule, and we are able to repair it to a legal value. For the Adult Income data set, repairing the data fully only results in a utility loss from about $7 4 \\%$ to $7 2 \\%$ , while for the German",
        "Figure 3 also shows that combinatorial and geometric repairs have similar DI and utility values for all partial repair data sets. This means that either repair can be used.",
        "The confusion matrix for $\\phi$ is depicted in Table 3.",
        "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg",
      "image_filename": "1412.3756_page0_fig3.jpg",
      "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.",
      "context_before": "6.3 Comparison to previous work\n\nHere, we compare our results to related work on the German credit data and Adult income data sets. Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.’s solution [26]. All comparison data is taken from Zemel et al.’s implementations [26]. Zemel et al. define discrimination as $( 1 - \\alpha ) - \\beta$ . So that increasing Zemel scores mean that fairness has increased, as is the case with DI, we will look at the Zemel fairness score which we define as $1 - \\left( \\left( 1 - \\alpha \\right) - \\beta \\right) = 2$ · BER. Accuracy is the usual rate of successful classification. Unlike the compared works, we do not choose a single partial repair point. Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.\n\n[Section: 6 Experiments]",
      "context_after": "Figure 5 shows that our method can be flexible with respect to the chosen classifier. Since the repair is done over the data, we can choose a classification algorithm appropriate to the data set. For example, on the Adult Income data set the repairs based on Na¨ıve Bayes have better accuracy at high values of fairness than the repairs based on Logistic Regression. On the German and Adult data sets our results show that for any fairness value a partially repaired data set at that value can be chosen and a classifier applied to achieve accuracy that is better than competing methods.\n\nSince the charts in Figure 5 include unrepaired data, we can also separate the effects of our classifier choices from the effects of the repair. In each classifier repair series, the data point with the lowest Zemel fairness (furthest to the left) is the original data. Comparing the original data point when the LR classifier was used to the LR classifier used by Zemel et al. as a comparison baseline, we see a large jump in both fairness and accuracy. Configuring the classifier to weight classes equally may have accounted for this improvement.\n\n7 Limitations and Future Work",
      "referring_paragraphs": [
        "The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own. In other words, the loss does not compound. These good results are likely due in part to the size of the data set allowing each subgroup to still be large enough. On such data sets, allowing all protected attributes to be repaired appears reasonable.",
        "The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg",
      "image_filename": "1412.3756_page0_fig4.jpg",
      "caption": "0.65acc RLR0.40.65 0.80 0.85 0.90 0.95 1.00Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.",
      "context_before": "7 Limitations and Future Work\n\nOur experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.\n\n[Section: 7 Limitations and Future Work]",
      "context_after": "[Section: 7 Limitations and Future Work]\n\nIn addition, our discussion of disparate impact is necessarily tied to the legal framework as defined in United States law. It would be valuable in future work to collect the legal frameworks of different jurisdictions, and investigate whether a single unifying formulation is possible.\n\nFinally, we note that the algorithm we present operates only on numerical attributes. Although we are satisfied with its performance, we chose this setting mostly for its relative theoretical simplicity. A natural avenue for future work is to investigate generalizations of our repair procedures for datasets with different attribute types, such as categorical data, vector-valued attributes, etc.",
      "referring_paragraphs": [
        "Here, we compare our results to related work on the German credit data and Adult income data sets. Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.’s solution [26]. All comparison data is taken from Zemel et al.’s implementations [26]. Zemel et al. define discrimination as $( 1 - \\alpha ) - \\beta$ . S",
        "Figure 5 shows that our method can be flexible with respect to the chosen classifier. Since the repair is done over the data, we can choose a classification algorithm appropriate to the data set. For example, on the Adult Income data set the repairs based on Na¨ıve Bayes have better accuracy at high values of fairness than the repairs based on Logistic Regression. On the German and Adult data sets our results show that for any fairness value a partially repaired data set at that value can be cho",
        "Since the charts in Figure 5 include unrepaired data, we can also separate the effects of our classifier choices from the effects of the repair. In each classifier repair series, the data point with the lowest Zemel fairness (furthest to the left) is the original data. Comparing the original data point when the LR classifier was used to the LR classifier used by Zemel et al. as a comparison baseline, we see a large jump in both fairness and accuracy. Configuring the classifier to weight classes ",
        "Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.",
        "Figure 5 shows that our method can be flexible with respect to the chosen classifier."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1412.3756_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1511.00830": [
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "image_filename": "1511.00830_page0_fig0.jpg",
      "caption": "Figure 1: Unsupervised model",
      "context_before": "[Section: Published as a conference paper at ICLR 2016]\n\narXiv:1511.00830v6 [stat.ML] 10 Aug 2017\n\n2 LEARNING INVARIANT REPRESENTATIONS",
      "context_after": "",
      "referring_paragraphs": [
        "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also obser",
        "is concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) (Ganin et al., 2015). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture.",
        "Figure 1: Unsupervised model",
        "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg",
      "image_filename": "1511.00830_page0_fig1.jpg",
      "caption": "Figure 2: Semi-supervised model",
      "context_before": "",
      "context_after": "2.1 UNSUPERVISED MODEL\n\nFactoring out undesired variations from the data can be easily formulated as a general probabilistic model which admits two distinct (independent) “sources”; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information. This generative process can be formally defined as:\n\n$$ \\mathbf {z} \\sim p (\\mathbf {z}); \\qquad \\mathbf {x} \\sim p _ {\\theta} (\\mathbf {x} | \\mathbf {z}, \\mathbf {s}) $$",
      "referring_paragraphs": [
        "Figure 2: Semi-supervised model",
        "Table 2: Results on the Extended Yale B dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig2.jpg",
      "image_filename": "1511.00830_page0_fig2.jpg",
      "caption": "Published as a conference paper at ICLR 2016",
      "context_before": "As for the Health dataset; this dataset is extremely imbalanced, with only $15 \\%$ of the patients being admitted to a hospital. Therefore, each of the classifiers seems to predict the majority class as the label y for every point. For the invariance against s however, the results were more interesting. On the one hand, the VAE model on this dataset did maintain some sensitive information, which could be identified both linearly and non-linearly. On the other hand, VFAE and the LFR methods were able to retain less information in their latent representation, since only Random Forest was able to achieve higher than random chance accuracy. This further justifies our choice for including the MMD penalty in the lower bound of the VAE. .\n\nIn order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation $\\mathbf { z } _ { 1 }$ that does not have the MMD penalty and the independence properties between $\\mathbf { z } _ { 1 }$ and s in the prior. By\n\n[Section: Published as a conference paper at ICLR 2016]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig3.jpg",
      "image_filename": "1511.00830_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig4.jpg",
      "image_filename": "1511.00830_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig5.jpg",
      "image_filename": "1511.00830_page0_fig5.jpg",
      "caption": "(a) Adult dataset",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig6.jpg",
      "image_filename": "1511.00830_page0_fig6.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig7.jpg",
      "image_filename": "1511.00830_page0_fig7.jpg",
      "caption": "(b) German dataset",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg",
      "image_filename": "1511.00830_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig9.jpg",
      "image_filename": "1511.00830_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_11",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig10.jpg",
      "image_filename": "1511.00830_page0_fig10.jpg",
      "caption": "(c) Health dataset Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.",
      "context_before": "",
      "context_after": "introducing these independence properties as well as the MMD penalty the nuisance variable groups become practically indistinguishable.",
      "referring_paragraphs": [
        "The results for all three datasets can be seen in Figure 3. Since we are dealing with the “fair” classification scenario here, low accuracy and discrimination against s is more important than the accuracy on y (as long as we do not produce degenerate representations).",
        "The results for all three datasets can be seen in Figure 3.",
        "Figure 3: Fair classification results."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig11.jpg",
      "image_filename": "1511.00830_page0_fig11.jpg",
      "caption": "(a)",
      "context_before": "introducing these independence properties as well as the MMD penalty the nuisance variable groups become practically indistinguishable.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg",
      "image_filename": "1511.00830_page0_fig12.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig13.jpg",
      "image_filename": "1511.00830_page0_fig13.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_15",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig14.jpg",
      "image_filename": "1511.00830_page0_fig14.jpg",
      "caption": "(d) Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.",
      "context_before": "",
      "context_after": "3.3.2 DOMAIN ADAPTATION\n\nAs for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also observed by Ganin et al. (2015) and Chen et al. (2012). As far as the accuracy on y\n\n[Section: Published as a conference paper at ICLR 2016]",
      "referring_paragraphs": [
        "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation $\\mathbf { z } _ { 1 }$ that does not have the MMD penalty and the independence properties between $\\ma",
        "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4.",
        "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_16",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/fdf6c7c916149f26dac10569971359e954c5a0d7c91f2cd6d863483a273ca7f7.jpg",
      "image_filename": "fdf6c7c916149f26dac10569971359e954c5a0d7c91f2cd6d863483a273ca7f7.jpg",
      "caption": "Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).",
      "context_before": "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also observed by Ganin et al. (2015) and Chen et al. (2012). As far as the accuracy on y\n\n[Section: Published as a conference paper at ICLR 2016]\n\nis concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) (Ganin et al., 2015). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture.",
      "context_after": "3.4 LEARNING INVARIANT REPRESENTATIONS\n\nRegarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions. As soon as we utilize our VFAE model we simultaneously decrease the accuracy on s, from $96 \\%$ to about $50 \\%$ , and increase our accuracy on y, from $78 \\%$ to about $85 \\%$ . This effect can also be seen in Figure 5b: the images are now mostly clustered according to the person ID (the label y). It is clear that in this scenario the information about s is purely “nuisance” with respect to the labels y. Therefore, by using our VFAE model we are able to obtain improved generalization and classification performance by effectively removing s from our representations.",
      "referring_paragraphs": [
        "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also obser",
        "is concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) (Ganin et al., 2015). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture.",
        "Figure 1: Unsupervised model",
        "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_17",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/445f1437429b794f8f4956a8323d1965c0e90d46c90f124eb2948385f2b41d36.jpg",
      "image_filename": "445f1437429b794f8f4956a8323d1965c0e90d46c90f124eb2948385f2b41d36.jpg",
      "caption": "Table 2: Results on the Extended Yale B dataset. We also included the best result from Li et al. (2014) under the $\\mathrm { N N } + \\mathrm { M M D }$ row.",
      "context_before": "3.4 LEARNING INVARIANT REPRESENTATIONS\n\nRegarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions. As soon as we utilize our VFAE model we simultaneously decrease the accuracy on s, from $96 \\%$ to about $50 \\%$ , and increase our accuracy on y, from $78 \\%$ to about $85 \\%$ . This effect can also be seen in Figure 5b: the images are now mostly clustered according to the person ID (the label y). It is clear that in this scenario the information about s is purely “nuisance” with respect to the labels y. Therefore, by using our VFAE model we are able to obtain improved generalization and classification performance by effectively removing s from our representations.",
      "context_after": "Most related to our “fair” representations view is the work from Zemel et al. (2013). They proposed a neural network based semi-supervised clustering model for learning fair representations. The idea is to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster gets assigned roughly equal proportions of data from each group in s. Although their approach was successfully applied on several datasets, the restriction to clustering means that it cannot leverage the representational power of a distributed representation. Furthermore, this penalty does not account for higher order moments in the latent distribution. For example, if $p ( z _ { k } ^ { - } = 1 | x _ { i } , s = 0 )$ always\n\n[Section: Published as a conference paper at ICLR 2016]",
      "referring_paragraphs": [
        "Figure 2: Semi-supervised model",
        "Table 2: Results on the Extended Yale B dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg",
      "image_filename": "1511.00830_page0_fig15.jpg",
      "caption": "(a)",
      "context_before": "Most related to our “fair” representations view is the work from Zemel et al. (2013). They proposed a neural network based semi-supervised clustering model for learning fair representations. The idea is to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster gets assigned roughly equal proportions of data from each group in s. Although their approach was successfully applied on several datasets, the restriction to clustering means that it cannot leverage the representational power of a distributed representation. Furthermore, this penalty does not account for higher order moments in the latent distribution. For example, if $p ( z _ { k } ^ { - } = 1 | x _ { i } , s = 0 )$ always\n\n[Section: Published as a conference paper at ICLR 2016]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_19",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig16.jpg",
      "image_filename": "1511.00830_page0_fig16.jpg",
      "caption": "(b) Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
      "context_before": "",
      "context_after": "returns 1 or 0, while $p ( z _ { k } = 1 | x _ { i } , s = 1 )$ returns values between values 0 and 1, then the penalty could still be satisfied, but information could still leak through. We addressed both of these issues in this paper.\n\nDomain adaptation can also be cast as learning representations that are “invariant” with respect to a discrete variable s, the domain. Most similar to our work are neural network approaches which try to match the feature distributions between the domains. This was performed in an unsupervised way with mSDA (Chen et al., 2012) by training denoising autoencoders jointly on all domains, thus implicitly obtaining a representation general enough to explain both the domain and the data. This is in contrast to our approach where we instead try to learn representations that explicitly remove domain information during the learning process. For the latter we find more similarities with “domain-regularized” supervised approaches that simultaneously try to predict the label for a data point and remove domain specific information. This is done with either MMD (Long & Wang, 2015; Tzeng et al., 2014) or adversarial (Ganin et al., 2015) penalties at the hidden layers of the network. In our model however the main “domain-regularizer” stems from the independence properties of the prior over the domain and latent representations. We also employ MMD on our model but from a different perspective since we consider a slightly more difficult case where the domain s and label y are correlated; we need to ensure that we remain as “invariant” as possible since $q _ { \\phi } ( \\mathbf { y } | \\mathbf { z } _ { 1 } )$ might ‘leak’ information about s.\n\nWe introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible. We further use a Maximum Mean Discrepancy regularizer in order to further promote invariance in the posterior distribution over latent variables. We apply this model to tasks involving developing fair classifiers that are invariant to sensitive demographic information and show that it produces a better tradeoff with respect to accuracy and invariance. As a second application, we consider the task of domain adaptation, where the goal is to improve classification by training a classifier that is invariant to the domain. We find that our model is competitive with recently proposed adversarial approaches. Finally, we also consider the more general task of learning invariant representations. We can observe that our model provides a clear improvement against a neural network that incorporates a Maximum Mean Discrepancy penalty.",
      "referring_paragraphs": [
        "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg",
      "image_filename": "1511.00830_page0_fig17.jpg",
      "caption": "It is clear that low PAD scores correspond to low discrimination of the source and target domain examples from the classifier.",
      "context_before": "Similarly to Ganin et al. (2015), we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the $\\mathbf { z } _ { 1 }$ representations of VFAE. Briefly, Proxy Adistance is an approximation to the $\\mathcal { H }$ -divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010). To compute it we first need to train a learning algorithm on the task of discriminating examples from the source and target domain. Afterwards we can use the test error $\\epsilon$ of that algorithm in the following formula:\n\n$$ \\mathrm {P A D} (\\epsilon) = 2 (1 - 2 \\epsilon) $$\n\nIt is clear that low PAD scores correspond to low discrimination of the source and target domain examples from the classifier. To obtain $\\epsilon$ for our model we used Logistic Regression. The resulting plot can be seen in Figure 6, where we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference. It can be seen that our VFAE model can factor out the information about s better, since the PAD scores on our new representation are, overall, lower than the ones obtained from the DANN architecture.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_21",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig18.jpg",
      "image_filename": "1511.00830_page0_fig18.jpg",
      "caption": "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))",
      "context_before": "",
      "context_after": "[Section: Published as a conference paper at ICLR 2016]",
      "referring_paragraphs": [
        "It is clear that low PAD scores correspond to low discrimination of the source and target domain examples from the classifier. To obtain $\\epsilon$ for our model we used Logistic Regression. The resulting plot can be seen in Figure 6, where we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference. It can be seen that our VFAE model can factor out the information about s better, since the PAD scores on our new r",
        "The resulting plot can be seen in Figure 6, where we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference.",
        "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1511.00830_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1602.05352": [
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg",
      "image_filename": "1602.05352_page0_fig0.jpg",
      "caption": "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings.",
      "context_before": "Propensity-based approaches have been widely used in causal inference from observational studies (Imbens & Rubin, 2015), as well as in complete-case analysis for missing data (Little & Rubin, 2002; Seaman & White, 2013) and in survey sampling (Thompson, 2012). However, their use in matrix completion is new to our knowledge. Weighting approaches are also widely used in domain adaptation and covariate shift, where data from one source is used to train for a different problem (e.g., Huang et al., 2006; Bickel et al., 2009; Sugiyama & Kawanabe, 2012). We will draw upon this work, especially the learning theory of weighting approaches in (Cortes et al., 2008; 2010).\n\n3. Unbiased Performance Estimation for Recommendation\n\nConsider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings. Denote with $u ~ \\in ~ \\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }$ for our toy example, where a sub-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig1.jpg",
      "image_filename": "1602.05352_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig2.jpg",
      "image_filename": "1602.05352_page0_fig2.jpg",
      "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
      "context_before": "",
      "context_after": "set of users are “horror lovers” who rate all horror movies 5 and all romance movies 1. Similarly, there is a subset of “romance lovers” who rate just the opposite way. However, both groups rate dramas as 3. The binary matrix ${ \\cal O } \\in \\{ 0 , 1 \\} ^ { \\bar { U } \\times \\bar { I } }$ in Figure 1 shows for which movies the users provided their rating to the system, $\\left[ O _ { u , i } \\right. = 1 ] \\Leftrightarrow$ $[ Y _ { u , i }$ observed]. Our toy example shows a strong correlation between liking and rating a movie, and the matrix $P$ describes the marginal probabilities $P _ { u , i } = P ( O _ { u , i } = 1 )$ with which each rating is revealed. For this data, consider the following two evaluation tasks.\n\n3.1. Task 1: Estimating Rating Prediction Accuracy\n\nFor the first task, we want to evaluate how well a predicted rating matrix $\\hat { Y }$ reflects the true ratings in $Y$ . Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as:",
      "referring_paragraphs": [
        "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings. Denote with $u ~ \\in ~ \\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }$ for our toy example, where a sub-",
        "set of users are “horror lovers” who rate all horror movies 5 and all romance movies 1. Similarly, there is a subset of “romance lovers” who rate just the opposite way. However, both groups rate dramas as 3. The binary matrix ${ \\cal O } \\in \\{ 0 , 1 \\} ^ { \\bar { U } \\times \\bar { I } }$ in Figure 1 shows for which movies the users provided their rating to the system, $\\left[ O _ { u , i } \\right. = 1 ] \\Leftrightarrow$ $[ Y _ { u , i }$ observed]. Our toy example shows a strong correlation bet",
        "We call this the naive estimator, and its naivety leads to a gross misjudgment for the $\\hat { Y } _ { 1 }$ and $\\hat { Y } _ { 2 }$ given in Figure 1. Even though $\\hat { Y _ { 1 } }$ is clearly better than $\\hat { Y } _ { 2 }$ by any reasonable measure of performance, $\\hat { R } _ { n a i v e } ( \\hat { Y } )$ will reliably claim that $\\hat { Y } _ { 2 }$ has better MAE than $\\hat { Y } _ { 1 }$ . This error is due to selection bias, since 1-star ratings are under-represented in the",
        "Instead of evaluating the accuracy of predicted ratings, we may want to more directly evaluate the quality of a particular recommendation. To this effect, let’s redefine $\\hat { Y }$ to now encode recommendations as a binary matrix analogous to $O$ , where $[ \\hat { Y } _ { u , i } = 1 ] \\Leftrightarrow [ i$ is recommended to $u ]$ , limited to a budget of $k$ recommendations per user. An example is $\\hat { Y } _ { 3 }$ in Figure 1. A reasonable way to measure the quality of a recommendation is ",
        "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ . Let $| Y = r |$ be the number of $r$ -star ratings in $Y$ .",
        "Rankings for $\\operatorname { D C G } @ 5 0$ were created by sorting items according to $\\hat { Y _ { i } }$ for each user. In Table 1, we report the average and standard deviation of estimates over 50 samples of $O$ from $P$ . We see that the mean IPS estimate perfectly matches the true performance for both MAE and DCG as expected. The bias of SNIPS is negligible as well. The naive estimator is severely biased and its estimated MAE incorrectly ranks the prediction matrices $\\hat { Y } _ { i }$ ",
        "Results. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 . 2 5$ . Next, we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) each estimator predicts the true MSE and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices $\\hat { Y _ { i } }$ given in Sect",
        "Figure 1.",
        "We call this the naive estimator, and its naivety leads to a gross misjudgment for the $\\hat { Y } _ { 1 }$ and $\\hat { Y } _ { 2 }$ given in Figure 1.",
        "An example is $\\hat { Y } _ { 3 }$ in Figure 1.",
        "Note that the IPS estimator only requires the marginal probabilities $P _ { u , i }$ and unbiased-ness is not affected by dependencies within $O$ :\n\nTable 1.",
        "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .",
        "Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/6a23148d20bdf053655c55091ed67546c43bc3309a9ba0257852a4f6561e3706.jpg",
      "image_filename": "6a23148d20bdf053655c55091ed67546c43bc3309a9ba0257852a4f6561e3706.jpg",
      "caption": "Table 1. Mean and standard deviation of the Naive, IPS, and SNIPS estimators compared to true MAE and DCG@50 on ML100K.",
      "context_before": "Unlike the naive estimator $\\hat { R } _ { n a i v e } ( \\hat { Y } )$ , the IPS estimator is unbiased for any probabilistic assignment mechanism. Note that the IPS estimator only requires the marginal probabilities $P _ { u , i }$ and unbiased-ness is not affected by dependencies within $O$ :\n\n[Section: Recommendations as Treatments: Debiasing Learning and Evaluation]\n\n2More realistically, $Y$ would contain quality scores derived from indicators like “clicked” and “watched to the end”.",
      "context_after": "$$ \\begin{array}{l} \\mathbb {E} _ {O} \\Big [ \\hat {R} _ {I P S} (\\hat {Y} | P) \\Big ] = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\mathbb {E} _ {O _ {u, i}} \\bigg [ \\frac {\\delta_ {u , i} (Y , \\hat {Y})}{P _ {u , i}} O _ {u, i} \\bigg ] \\\\ = \\frac {1}{U \\cdot I} \\sum_ {u} \\sum_ {i} \\delta_ {u, i} (Y, \\hat {Y}) = R (\\hat {Y}). \\\\ \\end{array} $$\n\nTo characterize the variability of the IPS estimator, however, we assume that observations are independent given $P$ , which corresponds to a multivariate Bernoulli model where each $O _ { u , i }$ is a biased coin flip with probability $P _ { u , i }$ . The following proposition (proof in appendix) provides some intuition about how the accuracy of the IPS estimator changes as the propensities become more “non-uniform”.\n\nProposition 3.1 (Tail Bound for IPS Estimator). Let P be the independent Bernoulli probabilities of observing each entry. For any given $\\hat { Y }$ and $Y$ , with probability $1 - \\eta ,$ , the IPS estimator $\\check { \\hat { R } } _ { I P S } ( \\hat { Y } | P )$ does not deviate from the true $R ( { \\hat { Y } } )$ by more than:",
      "referring_paragraphs": [
        "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings. Denote with $u ~ \\in ~ \\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }$ for our toy example, where a sub-",
        "set of users are “horror lovers” who rate all horror movies 5 and all romance movies 1. Similarly, there is a subset of “romance lovers” who rate just the opposite way. However, both groups rate dramas as 3. The binary matrix ${ \\cal O } \\in \\{ 0 , 1 \\} ^ { \\bar { U } \\times \\bar { I } }$ in Figure 1 shows for which movies the users provided their rating to the system, $\\left[ O _ { u , i } \\right. = 1 ] \\Leftrightarrow$ $[ Y _ { u , i }$ observed]. Our toy example shows a strong correlation bet",
        "We call this the naive estimator, and its naivety leads to a gross misjudgment for the $\\hat { Y } _ { 1 }$ and $\\hat { Y } _ { 2 }$ given in Figure 1. Even though $\\hat { Y _ { 1 } }$ is clearly better than $\\hat { Y } _ { 2 }$ by any reasonable measure of performance, $\\hat { R } _ { n a i v e } ( \\hat { Y } )$ will reliably claim that $\\hat { Y } _ { 2 }$ has better MAE than $\\hat { Y } _ { 1 }$ . This error is due to selection bias, since 1-star ratings are under-represented in the",
        "Instead of evaluating the accuracy of predicted ratings, we may want to more directly evaluate the quality of a particular recommendation. To this effect, let’s redefine $\\hat { Y }$ to now encode recommendations as a binary matrix analogous to $O$ , where $[ \\hat { Y } _ { u , i } = 1 ] \\Leftrightarrow [ i$ is recommended to $u ]$ , limited to a budget of $k$ recommendations per user. An example is $\\hat { Y } _ { 3 }$ in Figure 1. A reasonable way to measure the quality of a recommendation is ",
        "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ . Let $| Y = r |$ be the number of $r$ -star ratings in $Y$ .",
        "Rankings for $\\operatorname { D C G } @ 5 0$ were created by sorting items according to $\\hat { Y _ { i } }$ for each user. In Table 1, we report the average and standard deviation of estimates over 50 samples of $O$ from $P$ . We see that the mean IPS estimate perfectly matches the true performance for both MAE and DCG as expected. The bias of SNIPS is negligible as well. The naive estimator is severely biased and its estimated MAE incorrectly ranks the prediction matrices $\\hat { Y } _ { i }$ ",
        "Results. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 . 2 5$ . Next, we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) each estimator predicts the true MSE and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices $\\hat { Y _ { i } }$ given in Sect",
        "Figure 1.",
        "We call this the naive estimator, and its naivety leads to a gross misjudgment for the $\\hat { Y } _ { 1 }$ and $\\hat { Y } _ { 2 }$ given in Figure 1.",
        "An example is $\\hat { Y } _ { 3 }$ in Figure 1.",
        "Note that the IPS estimator only requires the marginal probabilities $P _ { u , i }$ and unbiased-ness is not affected by dependencies within $O$ :\n\nTable 1.",
        "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .",
        "Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg",
      "image_filename": "1602.05352_page0_fig3.jpg",
      "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
      "context_before": "ML100K Dataset. The ML100K dataset4 provides 100K MNAR ratings for 1683 movies by 944 users. To allow ground-truth evaluation against a fully known rating matrix, we complete these partial ratings using standard matrix factorization. The completed matrix, however, gives\n\n[Section: Recommendations as Treatments: Debiasing Learning and Evaluation]\n\n4http://grouplens.org/datasets/movielens/",
      "context_after": "unrealistically high ratings to almost all movies. We therefore adjust ratings for the final $Y$ to match a more realistic rating distribution $[ p _ { 1 } , p _ { 2 } , p _ { 3 } , p _ { 4 } , p _ { 5 } ]$ for ratings 1 to 5 as given in Marlin & Zemel (2009) as follows: we assign the bottom $p _ { 1 }$ fraction of the entries by value in the completed matrix a rating of 1, and the next $p _ { 2 }$ fraction of entries by value a rating of 2, and so on. Hyper-parameters (rank $d$ and L2 regularization $\\lambda$ ) were chosen by using a 90-10 train-test split of the 100K ratings, and maximizing the 0/1 accuracy of the completed matrix on the test set.\n\nML100K Observation Model. If the underlying rating is 4 or 5, the propensity for observing the rating is equal to $k$ . For ratings $r \\ < \\ 4$ , the corresponding propensity is $k \\alpha ^ { 4 - r }$ . For each $\\alpha$ , $k$ is set so that the expected number of ratings we observe is $5 \\%$ of the entire matrix. By varying $\\alpha \\ > \\ 0$ , we vary the MNAR effect: $\\alpha \\ = \\ 1$ is missing uniformly at random (MCAR), while $\\alpha 0$ only reveals 4 and 5 rated items. Note that $\\alpha \\ = \\ 0 . 2 5$ gives a marginal distribution of observed ratings that reasonably matches the observed MNAR rating marginals on ML100K ([0.06, 0.11, 0.27, 0.35, 0.21] in the real data vs. $[ 0 . 0 6 , 0 . 1 0 , 0 . 2 5 , 0 . 4 2 , 0 . 1 7 ]$ in our model).\n\nResults. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 . 2 5$ . Next, we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) each estimator predicts the true MSE and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices $\\hat { Y _ { i } }$ given in Section 3.4 and across 50 trials. Shaded regions indicate a $9 5 \\%$ confidence interval.",
      "referring_paragraphs": [
        "Results. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 . 2 5$ . Next, we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) each estimator predicts the true MSE and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices $\\hat { Y _ { i } }$ given in Sect",
        "Results. Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation",
        "Figure 2.",
        "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation\n\nTable 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg",
      "image_filename": "1602.05352_page0_fig4.jpg",
      "caption": "Over most of the range of $\\alpha$ , in particular for the realistic value of $\\alpha = 0 .",
      "context_before": "ML100K Observation Model. If the underlying rating is 4 or 5, the propensity for observing the rating is equal to $k$ . For ratings $r \\ < \\ 4$ , the corresponding propensity is $k \\alpha ^ { 4 - r }$ . For each $\\alpha$ , $k$ is set so that the expected number of ratings we observe is $5 \\%$ of the entire matrix. By varying $\\alpha \\ > \\ 0$ , we vary the MNAR effect: $\\alpha \\ = \\ 1$ is missing uniformly at random (MCAR), while $\\alpha 0$ only reveals 4 and 5 rated items. Note that $\\alpha \\ = \\ 0 . 2 5$ gives a marginal distribution of observed ratings that reasonably matches the observed MNAR rating marginals on ML100K ([0.06, 0.11, 0.27, 0.35, 0.21] in the real data vs. $[ 0 . 0 6 , 0 . 1 0 , 0 . 2 5 , 0 . 4 2 , 0 . 1 7 ]$ in our model).\n\nResults. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 . 2 5$ . Next, we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) each estimator predicts the true MSE and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices $\\hat { Y _ { i } }$ given in Section 3.4 and across 50 trials. Shaded regions indicate a $9 5 \\%$ confidence interval.\n\nOver most of the range of $\\alpha$ , in particular for the realistic value of $\\alpha = 0 . 2 5$ , the IPS and SNIPS estimators are orders-of-magnitude more accurate than the Naive estimator. Even for severely low choices of $\\alpha$ , the gain due to bias reduction of IPS and SNIPS still outweighs the added variability compared to Naive. When $\\alpha = 1$ (MCAR), SNIPS is algebraically equivalent to Naive, while IPS pays a small penalty due to increased variability from propensity weighting. For MSE, SNIPS consistently reduces estimation error over IPS while both are tied for DCG.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig5.jpg",
      "image_filename": "1602.05352_page0_fig5.jpg",
      "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).",
      "context_before": "",
      "context_after": "6.3. How does sampling bias severity affect learning?\n\nNow we explore whether these gains in risk estimation accuracy translate into improved learning via ERM, again in the Experimental Setting. Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model with separate $\\lambda$ selected via cross-validation and $d = 2 0$ . The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials. The propensity-weighted matrix factorization MF-IPS consistently outperforms conventional matrix factorization in terms of MSE. We also conducted experiments for MAE, with similar results.\n\n6.4. How robust is evaluation and learning to inaccurately learned propensities?",
      "referring_paragraphs": [
        "Now we explore whether these gains in risk estimation accuracy translate into improved learning via ERM, again in the Experimental Setting. Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model with separate $\\lambda$ selected via cross-validation and $d = 2 0$ . The results are plotted in Figure 3 (left), where shaded regi",
        "Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3. We compare the MSE prediction error of MF-IPS-NB with estimated propensities to that of MF-Naive and MF-IPS with known propensities. The shaded area shows the $9 5 \\%$ confidence interval over 30 trials. Again, we see that MF-IPS-NB outperforms MF-Naive even for",
        "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).",
        "The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials.",
        "Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg",
      "image_filename": "1602.05352_page0_fig6.jpg",
      "caption": "Recommendations as Treatments: Debiasing Learning and Evaluation",
      "context_before": "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perform worse than Naive. Interestingly, IPS-NB with estimated propensities can perform even better than IPS-KNOWN with known propensities, as can be seen for MSE. This is a known effect, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).\n\nFigure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3. We compare the MSE prediction error of MF-IPS-NB with estimated propensities to that of MF-Naive and MF-IPS with known propensities. The shaded area shows the $9 5 \\%$ confidence interval over 30 trials. Again, we see that MF-IPS-NB outperforms MF-Naive even for\n\n[Section: Recommendations as Treatments: Debiasing Learning and Evaluation]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig7.jpg",
      "image_filename": "1602.05352_page0_fig7.jpg",
      "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.",
      "context_before": "",
      "context_after": "severely degraded propensity estimates, demonstrating the robustness of the approach.\n\n6.5. Performance on Real-World Data\n\nOur final experiment studies performance on real-world datasets. We use the following two datasets, which both have a separate test set where users were asked to rate a uniformly drawn sample of items.",
      "referring_paragraphs": [
        "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perform worse than Naive. Interestingly, IPS-NB with estimated propensities can perform even better than IPS-KNOWN with known propensities, as can be seen for MSE. This is a known effect, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).",
        "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2.",
        "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.\n\nseverely degraded propensity estimates, demonstrating the robustness of the approach."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_10",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1efd2efeef564cec02953da4278f2598f1363cd1c27830c6d717517d9b0322e9.jpg",
      "image_filename": "1efd2efeef564cec02953da4278f2598f1363cd1c27830c6d717517d9b0322e9.jpg",
      "caption": "Table 2. Test set MAE and MSE on the Yahoo and Coat datasets.",
      "context_before": "Yahoo! R3 Dataset. This dataset5 (Marlin & Zemel, 2009) contains user-song ratings. The MNAR training set provides over 300K ratings for songs that were selfselected by 15400 users. The test set contains ratings by a subset of 5400 users who were asked to rate 10 randomly chosen songs. For this data, we estimate propensities via Naive Bayes. As a MCAR sample for eliciting the marginal rating distribution, we set aside $5 \\%$ of the test set and only report results on the remaining $9 5 \\%$ of the test set.\n\nCoat Shopping Dataset. We collected a new dataset6 simulating MNAR data of customers shopping for a coat in an online store. The training data was generated by giving Amazon Mechanical Turkers a simple web-shop interface with facets and paging. They were asked to find the coat in the store that they wanted to buy the most. Afterwards, they had to rate 24 of the coats they explored (self-selected) and 16 randomly picked ones on a five-point scale. The dataset contains ratings from 290 Turkers on an inventory of 300 items. The self-selected ratings are the training set and the uniformly selected ratings are the test set. We learn propensities via logistic regression based on user covariates (gender, age group, location, and fashion-awareness) and item covariates (gender, coat type, color, and was it promoted). A standard regularized logistic regression (Pedregosa et al., 2011) was trained using all pairs of user and item covariates as features and cross-validated to optimize log-likelihood of the self-selected observations.\n\nResults. Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation",
      "context_after": "models from (Hernandez-Lobato et al. ´ , 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, $p < 0 . 0 0 1$ for all). This holds for both MAE and MSE. Furthermore, the performance of MF-IPS beats the best published results for Yahoo in terms of MSE (1.115) and is close in terms of MAE (0.770) (the CTP-v model of (Marlin & Zemel, 2009) as reported in the supplementary material of Hernandez- ´ Lobato et al. (2014)). For MF-IPS and MF-Naive all hyperparameters (i.e., $\\lambda \\in \\{ 1 0 ^ { - 6 } , . . . , 1 \\}$ and $d \\in \\{ 5 , 1 0 , 2 0 , 4 0 \\} )$ were chosen by cross-validation. For the HL baselines, we explored $d \\in \\{ 5 , 1 0 , 2 0 , 4 0 \\}$ using software provided by the authors7 and report the best performance on the test set for efficiency reasons. Note that our performance numbers for HL on Yahoo closely match the values reported in (Hernandez-Lobato et al.´ , 2014).\n\nCompared to the complex generative HL models, we conclude that our discriminative MF-IPS performs robustly and efficiently on real-world data. We conjecture that this strength is a result of not requiring any generative assumptions about the validity of the rating model. Furthermore, note that there are several promising directions for further improving performance, like propensity clipping (Strehl et al., 2010), doubly-robust estimation (Dud´ık et al., 2011), and the use of improved methods for propensity estimation (McCaffrey et al., 2004).\n\nWe proposed an effective and robust approach to handle selection bias in the evaluation and training of recommender systems based on propensity scoring. The approach is a discriminative alternative to existing joint-likelihood methods which are generative. It therefore inherits many of the advantages (e.g., efficiency, predictive performance, no need for latent variables, fewer modeling assumptions) of discriminative methods. The modularity of the approach— separating the estimation of the assignment model from the rating model—also makes it very practical. In particular, any conditional probability estimation method can be plugged in as the propensity estimator, and we conjecture that many existing rating models can be retrofit with propensity weighting without sacrificing scalability.",
      "referring_paragraphs": [
        "Results. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 . 2 5$ . Next, we vary the severity of the sampling bias by changing $\\alpha \\in ( 0 , 1 ]$ . Figure 2 reports how accurately (in terms of root mean squared estimation error (RMSE)) each estimator predicts the true MSE and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices $\\hat { Y _ { i } }$ given in Sect",
        "Results. Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation",
        "Figure 2.",
        "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation\n\nTable 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1602.05352_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1603.07025": [
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg",
      "image_filename": "1603.07025_page0_fig0.jpg",
      "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
      "context_before": "We choose Reddit as our target community for a number of reasons. It has existed since 2005, meaning that there has been ample time for the community to evolve and for differences in user cohorts to appear. Second, it is one of the most popular online communities, allowing different types of contributions—comments and original submissions—across many different subreddits. Third, a number of Reddit users believe that it is, in fact, getting worse over time [1, 14, 20, 46, 50, 67]. Finally, Reddit data are publicly available through an API.\n\n1There is more to say about Reddit itself (see [55]).\n\n2 [57] provides more statistics about Reddit.",
      "context_after": "",
      "referring_paragraphs": [
        "Each submission can be imagined as the root of a threaded comment tree, in which Redditors can comment on submissions or each other’s comments. Redditors can also vote on both submissions and comments; these votes affect the order in which submissions and comments are displayed and also form the basis of “karma”, a reputation system that tracks how often people upvote a given Redditor’s comments and submissions. We can observe these elements in Figure 1.",
        "Table 1 provides some clues to what might be going on. When we move down the rows, we observe an increasing tendency in each cohort column. It means that the average comment length increases for these users. However, when we move right through the columns, people in later cohorts tend to write less per comment. If we were to average each row, we would still get an overall increasing comment length per year, but that is not what we see in the overall column. What happens here is that the latter c",
        "We can observe these elements in Figure 1.",
        "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
        "Table 1: Evolution of the average throughout the years for each cohort."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg",
      "image_filename": "1603.07025_page0_fig1.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig2.jpg",
      "image_filename": "1603.07025_page0_fig2.jpg",
      "caption": "(b) Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.",
      "context_before": "",
      "context_after": "Redditor Stuck In The Matrix used Reddit’s API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable. He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions. These datasets contain the JSON data objects returned by Reddit’s API for comments and submissions3; for our purposes, the main items of interest were the UTC creation date, the username, the subreddit, and for comments, the comment text.\n\nWe focus on submissions and comments in the dataset because they have timestamps and can be tied to specific users and subreddits, allowing us to perform time-based analyses. In some analyses, we look only at comments; in some, we combine comments and submissions, calling them “posts”. We would also like to have looked at voting behavior as a measure of user activity4, but individual votes with timestamps and usernames are not available through the API, only the aggregate number of votes that posts receive.\n\n3A full description of the JSON objects is available at [56].",
      "referring_paragraphs": [
        "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg",
      "image_filename": "1603.07025_page0_fig3.jpg",
      "caption": "(a)",
      "context_before": "We focus on submissions and comments in the dataset because they have timestamps and can be tied to specific users and subreddits, allowing us to perform time-based analyses. In some analyses, we look only at comments; in some, we combine comments and submissions, calling them “posts”. We would also like to have looked at voting behavior as a measure of user activity4, but individual votes with timestamps and usernames are not available through the API, only the aggregate number of votes that posts receive.\n\n3A full description of the JSON objects is available at [56].\n\n4This would also give us more insight than usual into lurkers’ behavior; we’ll return to this in the discussion.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig4.jpg",
      "image_filename": "1603.07025_page0_fig4.jpg",
      "caption": "(b) Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.",
      "context_before": "",
      "context_after": "3.3 Preprocessing the dataset\n\nTo analyze the data, we used Google BigQuery [21], a big data processing tool. Redditor fhoffa imported the comments into BigQuery and made them publicly available [16]. We uploaded the submission data ourselves using Google’s SDK.\n\nFor the analysis in the paper, we did light preprocessing to filter out posts by deleted users, posts with no creation time, and posts by authors with bot-like names5.",
      "referring_paragraphs": [
        "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg",
      "image_filename": "1603.07025_page0_fig5.jpg",
      "caption": "(a)",
      "context_before": "In many cases, we will look at the evolution of these cohorts. Since users can be created at any time during their cohort year, and our dataset ends in 2014, we are likely to have a variation on the data available for each user of up to one year, even though they are in the same cohort. To deal with this, some of our cohorted analyses will consider only the overlapping time window for which we collect data for all users in a cohort. This means that we are normally not going to include the 2014 cohort in our analyses.\n\nOur data starts in October 2007, but Reddit existed before that. That means that, not only do we have incomplete data for the 2007 year (which compromises this cohort), but there might also be users and subreddits\n\n5Ending with “ bot” or “Bot”; or containing “transcriber” or “automoderator”.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig6.jpg",
      "image_filename": "1603.07025_page0_fig6.jpg",
      "caption": "(b) Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.",
      "context_before": "",
      "context_after": "that show up in 2007 that were actually created in the previous years. Since we can not control for these, we will also omit 2007 cohort. We will, however, include 2007 in the overall analyses over time (the non cohorted ones) for two reasons: first, it does not have any direct impact on the results; second, we often compare the cohorted approach with a naive approach based on aggregation, and we would not expect a naive approach to do such filtering.\n\n4 Average posts per user\n\nOne common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks. In this section, we use this measure to address our first research question (RQ1): how does the amount of users’ activity change over time?",
      "referring_paragraphs": [
        "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg",
      "image_filename": "1603.07025_page0_fig7.jpg",
      "caption": "(a) 2010 cohort",
      "context_before": "This average view hides several important aspects of users’ activity dynamics. Previous work has looked into behavior relative to the user creation time. It has been shown that edge creation time in a social network relative to the user creation follows an exponential distribution [36]. User lifetime, however, does not follow a exponential distribution and some types of user content generation follow a stretched exponential distribution [24]. Throw-away accounts are one example of very short-lived users in Reddit [6], for example.\n\nTo address these characteristics, Figure 3b shows a view that emphasizes the trajectory over a user’s lifespan rather than the community’s. To do this, we scale the x-axis not by clock time, as in Figure 3a, but by time since the user’s first post: “1” on the x-axis refers to one year since the user’s account first post, and so on. We call this the time in the user referential. One caution about interpreting graphs with time in the user referential is that the amount of data available rapidly decreases over time as users leave the community, meaning that values toward the right side of an individual data series are more subject to individual variation.\n\nThe evidence at this point supports the tempting hypothesis that the longer a user survives, the more posts they make (H1). This hypothesis, however, is incorrect; we will present a more nuanced description of what is happening informed by cohort-based analyses.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig8.jpg",
      "image_filename": "1603.07025_page0_fig8.jpg",
      "caption": "(b) 2011 cohort",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_10",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig9.jpg",
      "image_filename": "1603.07025_page0_fig9.jpg",
      "caption": "(c) 2012 cohort Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
      "context_before": "",
      "context_after": "4.2 New cohorts do not catch up\n\nFigure 3b suggests that older users are more active than newer ones, raising the question of whether new users eventually follow in older users’ footsteps (RQ1a).\n\nAnalyzing users’ behavior by cohort is a reasonable way to address this question, and Figure 4a shows a first attempt at this analysis. We can already observe a significant cohort effect: users from later cohorts appear to level off at significantly lower posting averages than users from earlier ones. It suggests that newer users likely will never be as active as older ones on average. It also shows that surviving users are significantly more active than the overall average (the black line in the figure) would suggest.",
      "referring_paragraphs": [
        "Figure 5 shows this analysis for the 2010, 2011 and 2012 cohorts6. Across all cohorts and yearly survival sub-cohorts, users who leave earlier come in with a lower initial posting rate. Thus, the rise in average posts per active user is driven by the fact that users who have high posting averages throughout their lifespan are the ones who are more likely to survive. As the less active users leave the system, the average per active user increases. In other words, the correct interpretation of Fig",
        "Combining Figure 5’s insight that the main reason why these curves increase is because the low posting users are dying sooner with the earlier observation that the stable activity level is lower for newer cohorts suggests that low-activity users from later cohorts tend to survive longer than those from earlier cohorts. That is, people joining later in the community’s life are less likely to be either committed users or leave than those from earlier on: they are more likely to be “casual” users t",
        "Second, and unlike the case for average post length, surviving users’ behavior changes over time. For post length, Figure 5 shows that even the most active users come in at a certain activity level and stay there, perhaps even slowly declining over time. Here, Figures 7c-f show that the ratio of comments to submissions increases over time. Combined with the observation that overall activity stays steady, this suggests that the ratio is changing because people substitute making their own submissi",
        "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
        "Figure 5 shows this analysis for the 2010, 2011 and 2012 cohorts6.",
        "Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network.",
        "Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived.",
        "For post length, Figure 5 shows that even the most active users come in at a certain activity level and stay there, perhaps even slowly declining over time."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig10.jpg",
      "image_filename": "1603.07025_page0_fig10.jpg",
      "caption": "(a)",
      "context_before": "Combining Figure 5’s insight that the main reason why these curves increase is because the low posting users are dying sooner with the earlier observation that the stable activity level is lower for newer cohorts suggests that low-activity users from later cohorts tend to survive longer than those from earlier cohorts. That is, people joining later in the community’s life are less likely to be either committed users or leave than those from earlier on: they are more likely to be “casual” users that stick around.\n\nActivity as measured by the average number of posts per user is one proxy for user effort. Comment length can also be considered as a proxy for user effort in the network. Users that type more put more of their time in the network, contribute with more content, and might create stronger ties with the community. Thus, we put forward the following question (RQ2): how does comment length change in the community over time, both overall and by cohort?\n\n6We only show these figures for the sake of saving space, but the same trends are observed in the other cohorts.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig11.jpg",
      "image_filename": "1603.07025_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg",
      "image_filename": "1603.07025_page0_fig12.jpg",
      "caption": "(c) 2010 cohort",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig13.jpg",
      "image_filename": "1603.07025_page0_fig13.jpg",
      "caption": "(d) 2011 cohort",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_15",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig14.jpg",
      "image_filename": "1603.07025_page0_fig14.jpg",
      "caption": "(e) 2012 cohort Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
      "context_before": "",
      "context_after": "5.1 Comment length drops over time\n\nFigure 6a shows the overall comment length in Reddit over time (the darker line) and the overall length per cohort. Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users’ commitment to the network is decreasing over time (H3), or that there is some community-wide norm toward shorter commenting (H4).\n\nHowever, this might not be the best way to interpret this information. Figure 6b shows the comment length per cohort in the user referential time. An important observation here is that younger users start from a lower baseline comment length than older ones. Considering the fact that Reddit has experienced exponential growth, the overall average for Figures 6a and 6b is heavily influenced by the ever-growing younger generations, who are more numerous than older survivors and who post shorter comments.",
      "referring_paragraphs": [
        "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_16",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/d2650899dadd8b1b66ac80641fc94d692a8f74840e1f041e562ad455e4e411b2.jpg",
      "image_filename": "d2650899dadd8b1b66ac80641fc94d692a8f74840e1f041e562ad455e4e411b2.jpg",
      "caption": "Table 1: Evolution of the average throughout the years for each cohort. Each column here is one cohort and each line is one year in time. Cohorts start generating data in their cohort year, therefore the upper diagonal is blank. On the right column we see the overall average for all users.",
      "context_before": "5.2 Simpson’s Paradox: the length also rises\n\nLet us go back to Figure 6a, which shows the overall average comment length on Reddit over time. We see a clear trend towards declining length of comments in the overall line (the black line that averages across all users). This could be a warning sign for Reddit community managers, assuming longer comments are associated with more involved users and healthier discussions. A data analyst looking at these numbers might think about ways to promote longer comments on Reddit.\n\nHowever, Figure 6b shows that average comment length increases over time for every cohort. While later cohorts start at smaller comment length, after an initial drop, on average all cohorts write longer comments over time. This is puzzling: when each of the cohorts exhibits a steady increase in their average comment length, how can the overall mean comment length decrease? This anomaly is an instance of the Simpson’s paradox [63], and occurs because we fail to properly condition on different cohorts when computing mean comment length.",
      "context_after": "Table 1 provides some clues to what might be going on. When we move down the rows, we observe an increasing tendency in each cohort column. It means that the average comment length increases for these users. However, when we move right through the columns, people in later cohorts tend to write less per comment. If we were to average each row, we would still get an overall increasing comment length per year, but that is not what we see in the overall column. What happens here is that the latter cohorts have many more users than earlier ones. Since their numbers increase year by year, we have a much larger contribution from them towards comments, compared to users of earlier cohorts. This uneven contribution leads to the paradox we observed in Figure 6a.\n\nWithout the decision to condition on cohorts, one would have gathered an entirely wrong conclusion. People are not writing less as they survive, contra (H3). Rather, those who tend to write less are joining the community in much larger numbers. Why later users write less is an open question we speculate about later in the discussion and future work section.\n\n5.3 New users burn brighter",
      "referring_paragraphs": [
        "Each submission can be imagined as the root of a threaded comment tree, in which Redditors can comment on submissions or each other’s comments. Redditors can also vote on both submissions and comments; these votes affect the order in which submissions and comments are displayed and also form the basis of “karma”, a reputation system that tracks how often people upvote a given Redditor’s comments and submissions. We can observe these elements in Figure 1.",
        "Table 1 provides some clues to what might be going on. When we move down the rows, we observe an increasing tendency in each cohort column. It means that the average comment length increases for these users. However, when we move right through the columns, people in later cohorts tend to write less per comment. If we were to average each row, we would still get an overall increasing comment length per year, but that is not what we see in the overall column. What happens here is that the latter c",
        "We can observe these elements in Figure 1.",
        "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
        "Table 1: Evolution of the average throughout the years for each cohort."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig15.jpg",
      "image_filename": "1603.07025_page0_fig15.jpg",
      "caption": "(a)",
      "context_before": "Consider the case of Usenet: people who never start threads and only respond play the role of answerer, while there are other roles that include fostering discussion [69]. These might naturally map onto people who primarily comment and who primarily submit in Reddit, respectively. Submissions can be considered new content that an author generates, while comments can be considered as contributions toward existing content from another author.\n\nSince the total number of comments always surpasses the number of submissions, we compute a user’s ratio of comments per submission as a rough measure of the kinds of contributions they make. Figure 7a shows the overall and cohorted evolution of comments per submission from 2008 to 2013. Users who most prefer commenting to submitting come from 2009 to 2011, while over time the average ratio of comments to submissions increases both overall and per-cohort for active users.\n\nAgain, we analyze our data from the user-time referential, as seen in Figure 7b. It shows a clear pattern for users in earlier cohorts to have a lower comment per submission ratio than users in later cohorts, given that they both survived the same amount of time. Surviving users from later cohorts also exhibit a more rapid increase in comments per submission than those from earlier cohorts. In particular, the 2008 and 2009 cohorts increase",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig16.jpg",
      "image_filename": "1603.07025_page0_fig16.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig17.jpg",
      "image_filename": "1603.07025_page0_fig17.jpg",
      "caption": "(c) 2008 cohort",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg",
      "image_filename": "1603.07025_page0_fig18.jpg",
      "caption": "(d) 2009 cohort",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig19.jpg",
      "image_filename": "1603.07025_page0_fig19.jpg",
      "caption": "(e) 2010 cohort",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_22",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig20.jpg",
      "image_filename": "1603.07025_page0_fig20.jpg",
      "caption": "(f) 2011 cohort Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).",
      "context_before": "",
      "context_after": "much more slowly over time than those from 2010 onwards; later cohorts are more similar (although the 2012 and 2013 cohorts may level off lower than 2011 based on the limited data we have).\n\n6.2 Comment early, comment often\n\nFigures 7c-f shows the cohorts from 2008 to 2011 segmented by surviving year. Three interesting observations arise from these data. First, we see that just as in the analysis of average posts per user, the users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-tosubmission ratio relative to users in their cohort who abandon Reddit more quickly. This suggests that both the count of posts and the propensity to comment might be a useful early predictor of user survival.",
      "referring_paragraphs": [
        "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1603.07025_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1607.06520": [
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/73201de53a180a40f2a4b737f38ce57de01d90e2984ac750aae271ceb82d08ab.jpg",
      "image_filename": "73201de53a180a40f2a4b737f38ce57de01d90e2984ac750aae271ceb82d08ab.jpg",
      "caption": "Gender stereotype she-he analogies.",
      "context_before": "12. guidance counselor\n\nExtreme he occupations\n\nGender stereotype she-he analogies.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/2657e3c68f696abbf01560aefe4d344ce56cf79d7501b1ad0f5136820db4aef7.jpg",
      "image_filename": "2657e3c68f696abbf01560aefe4d344ce56cf79d7501b1ad0f5136820db4aef7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/b757bebef4a20484076babc1d710d6c11eeb4ae993d310eade366e85ecf1afd9.jpg",
      "image_filename": "b757bebef4a20484076babc1d710d6c11eeb4ae993d310eade366e85ecf1afd9.jpg",
      "caption": "Gender appropriate she-he analogies.",
      "context_before": "",
      "context_after": "Gender appropriate she-he analogies.",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/08ebf58898020ff95c18eb88dc69b04f31e483365a57efcc40dacff89d988369.jpg",
      "image_filename": "08ebf58898020ff95c18eb88dc69b04f31e483365a57efcc40dacff89d988369.jpg",
      "caption": "Gender appropriate she-he analogies.",
      "context_before": "Gender appropriate she-he analogies.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/6112f43719c95fe80f86ab5008a3b2c2cae246c530b64ada54d2cf752e5560c2.jpg",
      "image_filename": "6112f43719c95fe80f86ab5008a3b2c2cae246c530b64ada54d2cf752e5560c2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/f365bedd7564b5067599ca4d77ed38d39d47d80017ea69ccd64bbc1b408422ec.jpg",
      "image_filename": "f365bedd7564b5067599ca4d77ed38d39d47d80017ea69ccd64bbc1b408422ec.jpg",
      "caption": "Figure 2: Analogy examples. Examples of automatically generated analogies for the pair she-he using the procedure described in text. For example, the first analogy is interpreted as she:sewing :: he:carpentry in the orig",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2: Analogy examples. Examples of automatically generated analogies for the pair she-he using the procedure described in text. For example, the first analogy is interpreted as she:sewing :: he:carpentry in the original w2vNEWS embedding. Each automatically generated analogy is evaluated by 10 crowd-workers are to whether or not it reflects gender stereotype. Top: illustrative gender stereotypic analogies automatically generated from w2vNEWS, as rated by at least 5 of the 10 crowd-workers. ",
        "Since analogies, stereotypes, and biases are heavily influenced by culture, we employed U.S. based crowdworkers to evaluate the analogies output by the analogy generating algorithm described above. For each analogy, we asked the workers two yes/no questions: (a) whether the pairing makes sense as an analogy, and (b) whether it reflects a gender stereotype. Every analogy is judged by 10 workers, and we used the number of workers that rated this pair as stereotyped to quantify the degree of bias o",
        "<table><tr><td>queen-king</td></tr><tr><td>waitress-waiter</td></tr></table>\n\n<table><tr><td>sister-brother</td></tr><tr><td>ovarian cancer-prostate cancer</td></tr></table>\n\n<table><tr><td>mother-father</td></tr><tr><td>convent-monastery</td></tr></table>\n\nFigure 2: Analogy examples.",
        "Examples of analogies generated from w2vNEWS that were rated as stereotypical are shown at the top of Figure 2, and examples of analogies that make sense and are rated as gender-appropriate are shown at the bottom of Figure 2."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/33ba740c5edb7f9d35a6b31ab6427400f5f95f36328c81cdb7bb98e01f8860b3.jpg",
      "image_filename": "33ba740c5edb7f9d35a6b31ab6427400f5f95f36328c81cdb7bb98e01f8860b3.jpg",
      "caption": "Figure 3: Example of indirect bias. The five most extreme occupations on the softball-football axis, which indirectly captures gender bias. For each occupation, the degree to which the association represents a gender bia",
      "context_before": "",
      "context_after": "In other words, the same system that solved the above reasonable analogies will offensively answer “man is to computer programmer as woman is to $x$ ” with $_ { x }$ =homemaker. Similarly, it outputs that a father is to a doctor as a mother is to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec [24, 25] embedding trained on a corpus of Google News texts consisting of 3 million English words and terms into 300 dimensions, which we refer to here as the w2vNEWS. One might have hoped that the Google News embedding would exhibit little gender bias because many of its authors are professional journalists. We also analyze other publicly available embeddings trained via other algorithms and find similar biases.\n\nIn this paper, we will quantitatively demonstrate that word-embeddings contain biases in their geometry that reflect gender stereotypes present in broader society. Due to their wide-spread usage as basic features, word embeddings not only reflect such stereotypes but can also amplify them. This poses a significant risk and challenge for machine learning and its applications.\n\nTo illustrate bias amplification, consider bias present in the task of retrieving relevant web pages for a given query. In web search, one recent project has shown that, when carefully combined with existing approaches, word vectors have the potential to improve web page relevance results [27]. As an example, suppose the search query is cmu computer science phd student for a computer science Ph.D. student at Carnegie Mellon University. Now, the directory1 offers 127 nearly identical web pages for students — these pages differ only in the names of the students. A word embedding’s semantic knowledge can improve relevance by identifying, for examples, that the terms graduate research assistant and phd student are related. However, word embeddings also rank terms related to computer science closer to male names than female names (e.g., the embeddings give John:computer programmer :: Mary:homemaker). The consequence is that, between two pages that differ only in the names Mary and John, the word embedding would influence the search engine to rank John’s web page higher than Mary. In this hypothetical example, the usage of word embedding makes it even harder for women to be recognized as computer scientists and would contribute to widening the existing gender gap in computer science. While we focus on gender bias, specifically Female-Male (F-M) bias, the approach may be applied to other types of bias.",
      "referring_paragraphs": [
        "Figure 3: Example of indirect bias. The five most extreme occupations on the softball-football axis, which indirectly captures gender bias. For each occupation, the degree to which the association represents a gender bias is shown, as described in Section 5.3.",
        "Indirect gender bias. The direct bias analyzed above manifests in the relative similarities between genderspecific words and gender neutral words. Gender bias could also affect the relative geometry between gender neutral words themselves. To test this indirect gender bias, we take pairs of words that are gender-neutral, for example softball and football. We project all the occupation words onto the softball − football direction and looked at the extremes words, which are listed in Figure 3. For",
        "Unfortunately, the above definitions still do not capture indirect bias. To see this, imagine completely removing from the embedding both words in gender pairs (as well as words such as beard or uterus that are arguably gender-specific but which cannot be paired). There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3). As discussed in the Introduction, it can be subtle to obtain the g",
        "In Figure 3, as a case study, we examine the most extreme words on the softball $-$ football direction. The five most extreme words (i.e. words with the highest positive or the lowest negative projections onto",
        "Indirect bias. We also investigated how the strict debiasing algorithm affects indirect gender bias. Because we do not have the ground truth on the indirect effects of gender bias, it is challenging to quantify the performance of the algorithm in this regard. However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example. After applying the strict debias algorithm, we repeated the experiment and show the most extreme words in the $\\overrightarrow { \\",
        "In the main text, we focused on the results from a cleaned version of w2vNEWS consisting of 26,377 lower-case words. We have also applied our hard debiasing algorithm to the full w2vNEWS dataset. Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).",
        "midfielder</td></tr></table>\n\nFigure 3: Example of indirect bias.",
        "We project all the occupation words onto the  softball − football direction and looked at the extremes words, which are listed in Figure 3.",
        "There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3).",
        "In Figure 3, as a case study, we examine the most extreme words on the  softball $-$ football direction.",
        "However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example.",
        "Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_8",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg",
      "image_filename": "1607.06520_page0_fig0.jpg",
      "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
      "context_before": "4 Gender stereotypes in word embeddings\n\nOur first task is to understand the biases present in the word-embedding (i.e. which words are closer to she than to he, etc.) and the extent to which these geometric biases agree with human notion of gender stereotypes. We use two simple methods to approach this problem: 1) evaluate whether the embedding has\n\n4https://code.google.com/archive/p/word2vec/",
      "context_after": "stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged to reflect stereotypes by humans. The exploratory analysis of this section will motivate the more rigorous metrics used in the next two sections.\n\nOccupational stereotypes. Figure 1 lists the occupations that are closest to she and to he in the w2vNEWS embeddings. We asked the crowdworkers to evaluate whether an occupation is considered femalestereotypic, male-stereotypic, or neutral. Each occupation word was evaluated by ten crowd-workers as to whether or not it reflects gender stereotype. Hence, for each word we had a integer rating, on a scale of 0-10, of stereotypicality. The projection of the occupation words onto the she-he axis is strongly correlated with the stereotypicality estimates of these words (Spearman $\\rho = 0 . 5 1$ ), suggesting that the geometric biases of embedding vectors is aligned with crowd judgment of gender stereotypes. We used occupation words here because they are easily interpretable by humans and often capture common gender stereotypes. Other word sets could be used for this task. Also note that we could have used other words, e.g. woman and man, as the gender-pair in the task. We chose she and he because they are frequent and do not have fewer alternative word senses (e.g., man can also refer to mankind).\n\nWe projected each of the occupations onto the she-he direction in the w2vNEWS embedding as well as a different embedding generated by the GloVe algorithm on a web-crawl corpus [30]. The results are highly consistent (Figure 4), suggesting that gender stereotypes is prevalent across different embeddings and is not an artifact of the particular training corpus or methodology of word2vec.",
      "referring_paragraphs": [
        "We projected each of the occupations onto the she-he direction in the w2vNEWS embedding as well as a different embedding generated by the GloVe algorithm on a web-crawl corpus [30]. The results are highly consistent (Figure 4), suggesting that gender stereotypes is prevalent across different embeddings and is not an artifact of the particular training corpus or methodology of word2vec.",
        "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_9",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/5fffe11f54319b8a82e5fe3d6445383995804ada42456ee8c791e3d41e8d0b63.jpg",
      "image_filename": "5fffe11f54319b8a82e5fe3d6445383995804ada42456ee8c791e3d41e8d0b63.jpg",
      "caption": "Figure 5: Ten possible word pairs to define gender, ordered by word frequency, along with agreement with two sets of 100 words solicited from the crowd, one with definitional and and one with stereotypical gender associa",
      "context_before": "In this section, we study the bias present in the embedding geometrically, identifying the gender direction and quantifying the bias independent of the extent to which it is aligned with the crowd bias. We develop metrics of direct and indirect bias that more rigorously quantify the observations of the previous section.\n\n5.1 Identifying the gender subspace\n\nLanguage use is “messy” and therefore individual word pairs do not always behave as expected. For instance, the word man has several different usages: it may be used as an exclamation as in oh man! or to refer to people of either gender or as a verb, e.g., man the station. To more robustly estimate bias, we shall aggregate across multiple paired comparisons. By combining several directions, such as $\\overrightarrow { \\mathrm { s h e } } - \\overrightarrow { \\mathrm { h e } }$ and $\\overrightarrow { \\mathrm { w o m a n } } - \\overrightarrow { \\mathrm { m a n } }$ , we identify a gender direction $g \\in \\mathbb { R } ^ { d }$ that largely captures gender in the embedding. This direction helps us to quantify direct and indirect biases in words and associations.",
      "context_after": "In English as in many languages, there are numerous gender pair terms, and for each we can consider the difference between their embeddings. Before looking at the data, one might imagine that they all had roughly the same vector differences, as in the following caricature:\n\n$$ \\overrightarrow {\\mathrm {g r a n d m o t h e r}} = \\overrightarrow {\\mathrm {w i s e}} + \\overrightarrow {\\mathrm {g a l}} $$\n\n$$ \\overrightarrow {\\text {g r a n d f a t h e r}} = \\overrightarrow {\\text {w i s e}} + \\overrightarrow {\\text {g u y}} $$",
      "referring_paragraphs": [
        "Figure 5: Ten possible word pairs to define gender, ordered by word frequency, along with agreement with two sets of 100 words solicited from the crowd, one with definitional and and one with stereotypical gender associations. For each set of words, comprised of the most frequent 50 female and 50 male crowd suggestions, the accuracy is shown for the corresponding gender classifier based on which word is closer to a target word, e.g., the she-he classifier predicts a word is female if it is close",
        "However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences. Figure 5 illustrates ten possible gender pairs, $\\left\\{ \\left( x _ { i } , y _ { i } \\right) \\right\\} _ { i = 1 }",
        "We experimentally verified that the pairs of vectors corresponding to these words do agree with the crowd concept of gender. On Amazon Mechanical Turk, we asked crowdworkers to generate two lists of words: one list corresponding to words that they think are gendered by definition (waitress, menswear) and a separate list corresponding to words that they believe captures gender stereotypes (e.g., sewing, football). From this we generated the most frequently suggested 50 male and 50 female words fo",
        "<table><tr><td></td><td>def.</td><td>stereo.</td><td></td><td>def.</td><td>stereo.</td></tr><tr><td>\\( \\overrightarrow{\\mathrm{{she}}} -\\overrightarrow{\\mathrm{{he}}} \\)</td><td>92%</td><td>89%</td><td>\\( \\overrightarrow{\\text{daughter}} -\\overrightarrow{\\text{son}} \\)</td><td>93%</td><td>91%</td></tr><tr><td>\\( \\overrightarrow{\\mathrm{{her}}} -\\overrightarrow{\\mathrm{{his}}} \\)</td><td>84%</td><td>87%</td><td>\\( \\overrightarrow{\\text{mother}} -\\overrightarrow{\\text{father}} \\)</td><td>91%</td><td>85%</td></tr><tr><td>\\( \\overrightarrow{\\mathrm{{woman}}} -\\overrightarrow{\\mathrm{{man}}} \\)</td><td>90%</td><td>83%</td><td>\\( \\overrightarrow{\\text{gal}} -\\overrightarrow{\\text{guy}} \\)</td><td>85%</td><td>85%</td></tr><tr><td>\\( \\overrightarrow{\\mathrm{{Mary}}} -\\overrightarrow{\\mathrm{{John}}} \\)</td><td>75%</td><td>87%</td><td>\\( \\overrightarrow{\\text{girl}} -\\overrightarrow{\\text{boy}} \\)</td><td>90%</td><td>86%</td></tr><tr><td>\\( \\overrightarrow{\\mathrm{{herself}}} -\\overrightarrow{\\mathrm{{himself}}} \\)</td><td>93%</td><td>89%</td><td>\\( \\overrightarrow{\\text{female}} -\\overrightarrow{\\text{male}} \\)</td><td>84%</td><td>75%</td></tr></table>\n\nFigure 5: Ten possible word pairs to define gender, ordered by word frequency, along with agreement with two sets of 100 words solicited from the crowd, one with definitional and and one with stereotypical gender associations.",
        "Figure 5 illustrates ten possible gender pairs, $\\left\\{ \\left( x _ { i } , y _ { i } \\right) \\right\\} _ { i = 1 } ^ { 1 0 }$\n\nWe experimentally verified that the pairs of vectors corresponding to these words do agree with the crowd concept of gender."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg",
      "image_filename": "1607.06520_page0_fig1.jpg",
      "caption": "To measure direct bias, we first identify words that should be gender-neutral for the application in question.",
      "context_before": "We experimentally verified that the pairs of vectors corresponding to these words do agree with the crowd concept of gender. On Amazon Mechanical Turk, we asked crowdworkers to generate two lists of words: one list corresponding to words that they think are gendered by definition (waitress, menswear) and a separate list corresponding to words that they believe captures gender stereotypes (e.g., sewing, football). From this we generated the most frequently suggested 50 male and 50 female words for each list to be used for a classification task. For each candidate pair, for example she, he, we say that it accurately classifies a crowd suggested female definition (or stereotype) word if that word vector is closer to she than to he. Table 5 reports the classification accuracy for definition and stereotype words for each gender pair. The accuracies are high, indicating that these pairs capture the intuitive notion of gender.\n\nTo identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest. Note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues. However, as also illustrated in 6, the decrease one observes due to random sampling is much more gradual and uniform. Therefore we hypothesize that the top PC, denoted by the unit vector $g$ , captures the gender subspace. In general, the gender subspace could be higher dimensional and all of our analysis and algorithms (described below) work with general subspaces.\n\nTo measure direct bias, we first identify words that should be gender-neutral for the application in question. How to generate this set of gender-neutral words is described in Section 7. Given the gender neutral words, denoted by $N$ , and the gender direction learned from above, $g$ , we define the direct gender bias of an",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_11",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig2.jpg",
      "image_filename": "1607.06520_page0_fig2.jpg",
      "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
      "context_before": "",
      "context_after": "$$ \\operatorname {D i r e c t B i a s} _ {c} = \\frac {1}{| N |} \\sum_ {w \\in N} | \\cos (\\vec {w}, g) | ^ {c} $$\n\nwhere $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1. Such strict measurement of bias might be desirable in settings such as the college admissions example from the Introduction, where it would be unacceptable for the embedding to introduce a slight preference for one candidate over another by gender. A more gradual bias would be setting $c = 1$ . The presentation we have chosen favors simplicity – it would be natural to extend our definitions to weight words by frequency. For example, in w2vNEWS, if we take $N$ to be the set of 327 occupations, then DirectBias1 = 0.08, which confirms that many occupation words have substantial component along the gender direction.\n\nUnfortunately, the above definitions still do not capture indirect bias. To see this, imagine completely removing from the embedding both words in gender pairs (as well as words such as beard or uterus that are arguably gender-specific but which cannot be paired). There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3). As discussed in the Introduction, it can be subtle to obtain the ground truth of the extent to which such similarities is due to gender.",
      "referring_paragraphs": [
        "To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest. Note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues. However, as also illustrated in 6, the decrease one observes due to random sampling is much more gra",
        "As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors.",
        "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).\n\nembedding to be"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_12",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg",
      "image_filename": "1607.06520_page0_fig3.jpg",
      "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
      "context_before": "$$ \\beta (w, v) = \\left(w \\cdot v - \\frac {w _ {\\perp} \\cdot v _ {\\perp}}{\\| w _ {\\perp} \\| _ {2} \\| v _ {\\perp} \\| _ {2}}\\right) \\Bigg / w \\cdot v. $$\n\nThe intuition behind this metric is as follow: $\\frac { \\textbf { { w } } _ { \\perp } \\cdot \\boldsymbol { v } _ { \\perp } } { \\| \\boldsymbol { w } _ { \\perp } \\| _ { 2 } \\| \\boldsymbol { v } _ { \\perp } \\| _ { 2 } }$ is the inner product between the two vectors if we project out the gender subspace and renormalize the vectors to be of unit length. The metric quantifies how much this inner product changes (as a fraction of the original inner product value) due to this operation of removing the gender subspace. Because of noise in the data, every vector has some non-zero component $w _ { \\perp }$ and $\\beta$ is well-defined. Note that $\\beta ( w , w ) = 0$ , which is reasonable since the similarity of a word to itself should not depend on gender contribution. If $w _ { g } = 0 = v _ { g }$ , then $\\beta ( w , v ) = 0$ ; and if $w _ { \\perp } = 0 = v _ { \\perp }$ , then $\\beta ( w , v ) = 1$ .\n\nIn Figure 3, as a case study, we examine the most extreme words on the softball $-$ football direction. The five most extreme words (i.e. words with the highest positive or the lowest negative projections onto",
      "context_after": "−−−−→ softball − −−−−−→ football) are shown in the table. Words such as receptionist, waitress and homemaker are closer to softball than football, and the $\\beta$ ’s between these words and softball is substantial (67%, 35%, 38%, respectively). This suggests that the apparent similarity in the embeddings of these words to softball can be largely explained by gender biases in the embedding. Similarly, businessman and maestro are closer to football and this can also be attributed largely to indirect gender bias, with $\\beta$ ’s of $3 1 \\%$ and 42%, respectively.\n\n6 Debiasing algorithms\n\nThe debiasing algorithms are defined in terms of sets of words rather than just pairs, for generality, so that we can consider other biases such as racial or religious biases. We also assume that we have a set of words to neutralize, which can come from a list or from the embedding as described in Section 7. (In many cases it may be easier to list the gender specific words not to neutralize as this set can be much smaller.)",
      "referring_paragraphs": [
        "Figure 7 illustrates the results of the classifier for separating gender-specific words from gender-neutral words. To make the figure legible, we show a subset of the words. The $x$ -axis correspond to projection of words onto the $\\overrightarrow { \\mathrm { s h e } } - \\overrightarrow { \\mathrm { h e } }$ direction and the $y$ -axis corresponds to the distance from the decision boundary of the trained SVM.",
        "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line.",
        "Figure 7 illustrates the results of the classifier for separating gender-specific words from gender-neutral words."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1521c469efb769ac5c325c60d800e4bf1cc84bb6fe2720ca9c637a031503a94d.jpg",
      "image_filename": "1521c469efb769ac5c325c60d800e4bf1cc84bb6fe2720ca9c637a031503a94d.jpg",
      "caption": "Direct Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-w",
      "context_before": "Using 10-fold cross-validation to evaluate the accuracy of this process, we find an $F$ -score of . $6 2 7 \\pm . 1 0 2$ based on stratified 10-fold cross-validation. The binary accuracy is well over 99% due to the imbalanced nature of the classes. For another test of how accurately the embedding agrees with our base set of 218 words, we evaluate the class-balanced error by re-weighting the examples so that the positive and negative examples have equal weights, i.e., weighting each class inverse proportionally to the number of samples from that class. Here again, we use stratified 10-fold cross validation to evaluate the error. Within each fold, the regularization parameter was also chosen by 10-fold (nested) cross validation. The average (balanced) accuracy of the linear classifiers, across folds, was $9 5 . 1 2 \\% \\pm 1 . 4 6 \\%$ with 95% confidence.\n\nFigure 7 illustrates the results of the classifier for separating gender-specific words from gender-neutral words. To make the figure legible, we show a subset of the words. The $x$ -axis correspond to projection of words onto the $\\overrightarrow { \\mathrm { s h e } } - \\overrightarrow { \\mathrm { h e } }$ direction and the $y$ -axis corresponds to the distance from the decision boundary of the trained SVM.\n\nWe evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases.",
      "context_after": "Direct Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-workers to evaluate whether these pairs reflect gender stereotypes. Figure 8 shows the results. On the initial w2vNEWS embedding, 19% of the top 150 analogies were judged as showing gender stereotypes by a majority of the ten workers. After applying our hard debiasing algorithm, only 6% of the new embedding were judged as stereotypical. As an example, consider the analogy puzzle, he to doctor is as she to $X$ . The original embedding returns $X = n u r s e$ while the hard-debiased embedding finds X = physician. Moreover the hard-debiasing algorithm preserved gender appropriate analogies such as she to ovarian cancer is as he to prostate cancer. This demonstrates that the hard-debiasing has effectively reduced the gender stereotypes in the word embedding. Figure 8 also shows that the number of appropriate analogies remains similar as in the original embedding after executing hard-debiasing. This demonstrates that that the quality of the embeddings is preserved. The details results are in Appendix G. Soft-debiasing was less effective in removing gender bias.\n\nTo further confirms the quality of embeddings after debiasing, we tested the debiased embedding on several standard benchmarks that measure whether related words have similar embeddings as well as how well the embedding performs in analogy tasks. Table 1 shows the results on the original and the new embeddings and the transformation does not negatively impact the performance.\n\nIndirect bias. We also investigated how the strict debiasing algorithm affects indirect gender bias. Because we do not have the ground truth on the indirect effects of gender bias, it is challenging to quantify the performance of the algorithm in this regard. However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example. After applying the strict debias algorithm, we repeated the experiment and show the most extreme words in the $\\overrightarrow { \\mathrm { s o f t b a l l } } - \\overrightarrow { \\mathrm { f o o t b a l l } }$ direction. The most extreme words closer to softball are now infielder and major leaguer in addition to pitcher, which are more relevant and do not exhibit gender bias. Gender stereotypic associations such are receptionist, waitress and homemaker are moved down the list. Similarly, words that clearly show male bias, e.g. businessman, are also no longer at the top of the list. Note that the two most extreme words in the softball − football direction are pitcher and footballer. The similarities between pitcher and softball and between footballer and football comes from the actual functions of these words and hence have little gender contribution. These two words are essentially unchanged by the debiasing algorithm.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg",
      "image_filename": "1607.06520_page0_fig4.jpg",
      "caption": "To reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing",
      "context_before": "Indirect bias. We also investigated how the strict debiasing algorithm affects indirect gender bias. Because we do not have the ground truth on the indirect effects of gender bias, it is challenging to quantify the performance of the algorithm in this regard. However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example. After applying the strict debias algorithm, we repeated the experiment and show the most extreme words in the $\\overrightarrow { \\mathrm { s o f t b a l l } } - \\overrightarrow { \\mathrm { f o o t b a l l } }$ direction. The most extreme words closer to softball are now infielder and major leaguer in addition to pitcher, which are more relevant and do not exhibit gender bias. Gender stereotypic associations such are receptionist, waitress and homemaker are moved down the list. Similarly, words that clearly show male bias, e.g. businessman, are also no longer at the top of the list. Note that the two most extreme words in the softball − football direction are pitcher and footballer. The similarities between pitcher and softball and between footballer and football comes from the actual functions of these words and hence have little gender contribution. These two words are essentially unchanged by the debiasing algorithm.\n\nWord embeddings help us further our understanding of bias in language. We find a single direction that largely captures gender, that helps us capture associations between gender neutral words and gender as well as indirect inequality.The projection of gender neutral words on this direction enables us to quantify their degree of female- or male-bias.\n\nTo reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_15",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig5.jpg",
      "image_filename": "1607.06520_page0_fig5.jpg",
      "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.",
      "context_before": "",
      "context_after": "their gender associations. For instance, nurse is moved to to be equally male and female in the direction $g$ . In addition, we find that gender-specific words have additional biases beyond $g$ . For instance, grandmother and grandfather are both closer to wisdom than gal and guy are, which does not reflect a gender difference. On the other hand, the fact that babysit is so much closer to grandmother than grandfather (more than for other gender pairs) is a gender bias specific to grandmother. By equating grandmother and grandfather outside of gender, and since we’ve removed $g$ from babysit, both grandmother and grandfather and equally close to babysit after debiasing. By retaining the gender component for gender-specific words, we maintain analogies such as she:grandmother :: he:grandfather. Through empirical evaluations, we show that our hard-debiasing algorithm significantly reduces both direct and indirect gender bias while preserving the utility of the embedding. We have also developed a soft-embedding algorithm which balances reducing bias with preserving the original distances, and could be appropriate in specific settings.\n\nOne perspective on bias in word embeddings is that it merely reflects bias in society, and therefore one should attempt to debias society rather than word embeddings. However, by reducing the bias in today’s computer systems (or at least not amplifying the bias), which is increasingly reliant on word embeddings, in a small way debiased word embeddings can hopefully contribute to reducing gender bias in society. At the very least, machine learning should not be used to inadvertently amplify these biases, as we have seen can naturally happen.\n\nIn specific applications, one might argue that gender biases in the embedding (e.g. computer programmer is closer to he) could capture useful statistics and that, in these special cases, the original biased embeddings could be used. However given the potential risk of having machine learning algorithms that amplify gender stereotypes and discriminations, we recommend that we should err on the side of neutrality and use the debiased embeddings provided here as much as possible.",
      "referring_paragraphs": [
        "Since analogies, stereotypes, and biases are heavily influenced by culture, we employed U.S. based crowdworkers to evaluate the analogies output by the analogy generating algorithm described above. For each analogy, we asked the workers two yes/no questions: (a) whether the pairing makes sense as an analogy, and (b) whether it reflects a gender stereotype. Every analogy is judged by 10 workers, and we used the number of workers that rated this pair as stereotyped to quantify the degree of bias o",
        "Direct Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-workers to evaluate whether these pairs reflect gender stereotypes. Figure 8 shows the results. On the initial w2vNEWS embedding, 19% of the top 150 analogies were judged as showing gender stereotypes by a majority of the ten workers. After applying our hard debiasing algorithm, o",
        "Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8).",
        "Figure 8 shows the results.",
        "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/0bdaa6c0af122bf758a22c8c95f611ca35512df7d317fd91ed0d8482a4d99df7.jpg",
      "image_filename": "0bdaa6c0af122bf758a22c8c95f611ca35512df7d317fd91ed0d8482a4d99df7.jpg",
      "caption": "The first term ensures that the pairwise inner products are preserved and the second term induces the biases of gender neutral words onto the gender subspace to be small.",
      "context_before": "Let $X = T ^ { T } T$ , then this is equivalent to the following semi-definite programming problem\n\n$$ \\min _ {X} \\| W ^ {T} X W - W ^ {T} W \\| _ {F} ^ {2} + \\lambda \\| N ^ {T} X B \\| _ {F} ^ {2} \\quad \\mathrm {s . t .} X \\succeq 0. \\tag {3} $$\n\nThe first term ensures that the pairwise inner products are preserved and the second term induces the biases of gender neutral words onto the gender subspace to be small. The user-specified parameter $\\lambda$ balances the two terms.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_17",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/c201f5f4d0cbeb5bc93d7b44b3f44cf0fa7530a8d554a23397e97a2a8d0c4609.jpg",
      "image_filename": "c201f5f4d0cbeb5bc93d7b44b3f44cf0fa7530a8d554a23397e97a2a8d0c4609.jpg",
      "caption": "Figure 9: First 10 different she-he analogies generated using the parallelogram approach and our approach, from the top 100 she-he analogies not containing gender specific words. Most of the analogies on the left seem to",
      "context_before": "",
      "context_after": "Directly solving this SDP optimization problem is challenging. In practice, the dimension of matrix $W$ is in the scale of $3 0 0 \\times 4 0 0 , 0 0 0$ . The dimensions of the matrices $W ^ { T } X W$ and $W ^ { T } W$ are $4 0 0 , 0 0 0 \\times 4 0 0 , 0 0 0$ causing computational and memory issues. We perform singular value decomposition on $W$ , such that $W = U \\Sigma V ^ { T }$ , where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix.\n\n$$ \\begin{array}{l} \\| W ^ {T} X W - W ^ {T} W \\| _ {F} ^ {2} = \\| W ^ {T} (X - I) W \\| _ {F} ^ {2} \\\\ = \\left\\| V \\Sigma U ^ {T} (X - I) U \\Sigma V ^ {T} \\right\\| _ {F} ^ {2} \\tag {4} \\\\ = \\| \\Sigma U ^ {T} (X - I) U \\Sigma \\| _ {F} ^ {2}. \\\\ \\end{array} $$\n\nThe last equality follows the fact that $V$ is an orthogonal matrix and $\\begin{array} { r } { \\langle \\| V Y V ^ { T } \\| _ { F } ^ { 2 } = t r ( V Y ^ { T } V ^ { T } V Y V ^ { T } ) = } \\end{array}$ $t r ( V Y ^ { T } Y V ^ { T ^ { \\prime } } ) = t r ( Y ^ { T } Y V ^ { T ^ { \\prime } } V ) = t r ( Y ^ { T } Y ) = \\| Y \\| _ { F } ^ { 2 }$ .)",
      "referring_paragraphs": [
        "Finally, Figure 9 highlights differences between analogies generated from our approach and the corresponding analogies generated by the first approach mentioned above, namely minimizing:",
        "Figure 9: First 10 different she-he analogies generated using the parallelogram approach and our approach, from the top 100 she-he analogies not containing gender specific words. Most of the analogies on the left seem to have little connection to gender.",
        "(1)</td></tr><tr><td>petite-lanky</td></tr><tr><td>volleyball-football</td></tr><tr><td>interior designer-architect</td></tr><tr><td>bitch-bastard</td></tr><tr><td>bra-pants</td></tr><tr><td>nurse-surgeon</td></tr><tr><td>feminine-manly</td></tr><tr><td>glamorous-flashy</td></tr><tr><td>registered nurse-physician</td></tr><tr><td>cupcakes-pizzas</td></tr></table>\n\nFigure 9: First 10 different she-he analogies generated using the parallelogram approach and our approach, from the top 100 she-he analogies not containing gender specific words."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/9f59bd79aac9a1fc985de1e35cb2afadfaf88a913f518288f5690316931a4d0a.jpg",
      "image_filename": "9f59bd79aac9a1fc985de1e35cb2afadfaf88a913f518288f5690316931a4d0a.jpg",
      "caption": "G Analogies Generated by Word Embeddings",
      "context_before": "Check the analogies that are nonsensical\n\nAny suggestions or comments on the hit? Optional feedback\n\nG Analogies Generated by Word Embeddings",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/cc5c8ba8bf2a3b8493341b1283d4fd5431b44d245c794685e815d24475ece01f.jpg",
      "image_filename": "cc5c8ba8bf2a3b8493341b1283d4fd5431b44d245c794685e815d24475ece01f.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/b1dad87e5d9fb56a60fa1ec3e94825358b3aa9d6f693f08da61371b81bc9e1c8.jpg",
      "image_filename": "b1dad87e5d9fb56a60fa1ec3e94825358b3aa9d6f693f08da61371b81bc9e1c8.jpg",
      "caption": "daughters:sons 10",
      "context_before": "",
      "context_after": "spokeswoman:spokesman 10\n\npolitician:statesman 1\n\nsuitor:takeover_bid 8",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/6d6aad9f27406faef40cedfd25dd74eb751fb8b7fe25147eb952426de079dd81.jpg",
      "image_filename": "6d6aad9f27406faef40cedfd25dd74eb751fb8b7fe25147eb952426de079dd81.jpg",
      "caption": "H Debiasing the full w2vNEWS embedding.",
      "context_before": "waitress:waiter 10 1\n\nhusband:younger_brother 3\n\nteenage_girls:youths 0",
      "context_after": "H Debiasing the full w2vNEWS embedding.\n\nIn the main text, we focused on the results from a cleaned version of w2vNEWS consisting of 26,377 lower-case words. We have also applied our hard debiasing algorithm to the full w2vNEWS dataset. Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_22",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/d4ce383fe73dd438d80f253d60f5d7eeecca8bf1c8e6c6d9cc54fe16c7988dd4.jpg",
      "image_filename": "d4ce383fe73dd438d80f253d60f5d7eeecca8bf1c8e6c6d9cc54fe16c7988dd4.jpg",
      "caption": "Table 3: The columns show the performance of the original, complete w2vNEWS embedding (“before”) and the debiased w2vNEWS on the standard evaluation metrics measuring coherence and analogy-solving abilities: RG [32], WS [12], MSR-analogy [26]. Higher is better. The results show that the performance does not degrade after debiasing.",
      "context_before": "H Debiasing the full w2vNEWS embedding.\n\nIn the main text, we focused on the results from a cleaned version of w2vNEWS consisting of 26,377 lower-case words. We have also applied our hard debiasing algorithm to the full w2vNEWS dataset. Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3: Example of indirect bias. The five most extreme occupations on the softball-football axis, which indirectly captures gender bias. For each occupation, the degree to which the association represents a gender bias is shown, as described in Section 5.3.",
        "Indirect gender bias. The direct bias analyzed above manifests in the relative similarities between genderspecific words and gender neutral words. Gender bias could also affect the relative geometry between gender neutral words themselves. To test this indirect gender bias, we take pairs of words that are gender-neutral, for example softball and football. We project all the occupation words onto the softball − football direction and looked at the extremes words, which are listed in Figure 3. For",
        "Unfortunately, the above definitions still do not capture indirect bias. To see this, imagine completely removing from the embedding both words in gender pairs (as well as words such as beard or uterus that are arguably gender-specific but which cannot be paired). There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3). As discussed in the Introduction, it can be subtle to obtain the g",
        "In Figure 3, as a case study, we examine the most extreme words on the softball $-$ football direction. The five most extreme words (i.e. words with the highest positive or the lowest negative projections onto",
        "Indirect bias. We also investigated how the strict debiasing algorithm affects indirect gender bias. Because we do not have the ground truth on the indirect effects of gender bias, it is challenging to quantify the performance of the algorithm in this regard. However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example. After applying the strict debias algorithm, we repeated the experiment and show the most extreme words in the $\\overrightarrow { \\",
        "In the main text, we focused on the results from a cleaned version of w2vNEWS consisting of 26,377 lower-case words. We have also applied our hard debiasing algorithm to the full w2vNEWS dataset. Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3).",
        "midfielder</td></tr></table>\n\nFigure 3: Example of indirect bias.",
        "We project all the occupation words onto the  softball − football direction and looked at the extremes words, which are listed in Figure 3.",
        "There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3).",
        "In Figure 3, as a case study, we examine the most extreme words on the  softball $-$ football direction.",
        "However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example.",
        "Evalution based on the standard metrics shows that the debiasing does not degrade the utility of the embedding (Table 3)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1607.06520_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1608.07187": [
    {
      "doc_id": "1608.07187",
      "figure_id": "1608.07187_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg",
      "image_filename": "1608.07187_page0_fig0.jpg",
      "caption": "Figure 1. Occupation-gender association",
      "context_before": "Stimuli: We use the gender stimuli found in Nosek et al. (2002a, p. 114) along with the occupation attributes we derived from labor statistics.\n\n[Section: Outdated draft. See published version at http://opus.bath.ac.uk/55288/.]\n\n3http://www.bls.gov/cps/cpsaat11.htm",
      "context_after": "Comparison to Real-World Data: Androgynous Names\n\nSimilarly, we looked at the veridical association of gender to androgynous names, that is, names sometimes used by either gender. In this case, the most recent information we were able to find was the 1990 census name and gender statistics. Perhaps because of the age of our name data, our correlation was weaker than for the 2015 occupation statistics, but still strikingly significant.",
      "referring_paragraphs": [
        "Original Data: The x-axis of Figure 1 is derived from the 2015 U.S. Bureau of Labor Statistics3, which provides information about occupational categories and the percentage of women that have certain occupations under these categories. We generated single word occupation names (as explained in the Methods section) based on the available data and calculated the percentage of women for the set of single word occupation names.",
        "Next, consider statistical machine translation (SMT). Unsurprisingly, today’s SMT systems reflect existing gender stereotypes. Translations to English from many gender-neutral languages such as Finnish, Estonian, Hungarian, Persian, and Turkish lead to gender-stereotyped sentences. For example, Google Translate converts these Turkish sentences with genderless pronouns: “O bir doktor. O bir hems¸ire.” to these English sentences: “He is a doctor. She is a nurse.” A test of the 50 occupation words ",
        "Original Data: The x-axis of Figure 1 is derived from the 2015 U.S.",
        "She is a nurse.” A test of the 50 occupation words used in the results presented in Figure 1 shows that the pronoun is translated to “he” in the majority of cases and “she” in about a quarter of cases; tellingly, we found that the gender association of the word vectors almost perfectly predicts which pronoun will appear in the translation."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1608.07187_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1608.07187",
      "figure_id": "1608.07187_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig1.jpg",
      "image_filename": "1608.07187_page0_fig1.jpg",
      "caption": "Figure 2. People with androgynous names",
      "context_before": "Comparison to Real-World Data: Androgynous Names\n\nSimilarly, we looked at the veridical association of gender to androgynous names, that is, names sometimes used by either gender. In this case, the most recent information we were able to find was the 1990 census name and gender statistics. Perhaps because of the age of our name data, our correlation was weaker than for the 2015 occupation statistics, but still strikingly significant.",
      "context_after": "Original Data: The x-axis of Figure 2 is derived from the 1990 U.S. census data4 that provides first name and gender information in population.\n\nOur Finding: The y-axis reflects our calculation of the bias for how male or female each of the names is. By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson’s correlation coefficient of $\\rho = 0 . 8 4$\n\n[Section: Outdated draft. See published version at http://opus.bath.ac.uk/55288/.]",
      "referring_paragraphs": [
        "Original Data: The x-axis of Figure 2 is derived from the 1990 U.S. census data4 that provides first name and gender information in population.",
        "Similarly, we looked at the veridical association of gender to androgynous names, that is, names sometimes used by either gender. In this case, the most recent information we were able to find was the 1990 census name and gender statistics. Perhaps because of the age of our name data, our correlation was weaker than for the 2015 occupation statistics, but still strikingly significant.\n\nFigure 2. People with androgynous names",
        "Pearson’s correlation coefficient $\\rho = 0 . 8 4$ with $p$ -value $< 1 0 ^ { - 1 3 }$ .\n\nOriginal Data: The x-axis of Figure 2 is derived from the 1990 U.S. census data4 that provides first name and gender information in population.\n\nOur Finding: The y-axis reflects our calculation of the bias for how male or female each of the names is. By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson’s correlation coefficient of $\\rho = 0 . 8 4$"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1608.07187_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1608.07187",
      "figure_id": "1608.07187_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg",
      "image_filename": "1608.07187_page0_fig2.jpg",
      "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.",
      "context_before": "Our Finding: The y-axis reflects our calculation of the bias for how male or female each of the names is. By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson’s correlation coefficient of $\\rho = 0 . 8 4$\n\n[Section: Outdated draft. See published version at http://opus.bath.ac.uk/55288/.]\n\n4http://www.census.gov/main/www/cen1990.html",
      "context_after": "with $p$ -value $< 1 0 ^ { - 1 3 }$ .\n\nStimuli: We use the gender stimuli found in Nosek et al. (2002a, p. 114) along with the most popular androgynous names from 1990’s public census data as targets.\n\nA word embedding is a representation of words as points in a vector space. Loosely, embeddings satisfy the property that vectors that are close to each other represent semantically “similar” words. Word embeddings derive their power from the discovery that vector spaces with around 300 dimensions suffice to capture most aspects of similarity, enabling a computationally tractable representation of all or most words in large corpora of text (Bengio et al., 2003; Lowe, 1997). Starting in 2013, the word2vec family of word embedding techniques has gained popularity due to a new set of computational techniques for generating word embeddings from large training corpora of text, with superior speed and predictive performance in various natural-language processing tasks (Mikolov et al., 2013; Mikolov and Dean, 2013).",
      "referring_paragraphs": [
        "Most famously, word embeddings excel at solving “word analogy” tasks because the algebraic relationships between vectors capture syntactic and semantic relationships between words (Figure 3). In addition, word embeddings have found use in natural-language processing tasks such as web search and document classification. They have also found use in cognitive science for understanding human memory and recall (Zaromb et al., 2006; McDonald and Lowe, 1998).",
        "Figure 3.",
        "Most famously, word embeddings excel at solving “word analogy” tasks because the algebraic relationships between vectors capture syntactic and semantic relationships between words (Figure 3)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1608.07187_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1609.05807": [
    {
      "doc_id": "1609.05807",
      "figure_id": "1609.05807_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1609.05807/1609.05807/hybrid_auto/images/1609.05807_page0_fig0.jpg",
      "image_filename": "1609.05807_page0_fig0.jpg",
      "caption": "Now, we can prove that the integral assignment problem is NP-hard.",
      "context_before": "Lemma 5.1 For any $z _ { 1 } , \\ldots , z _ { k } \\in \\mathbb { R } ,$ ,\n\n$$ \\sum_ {i = 1} ^ {k} z _ {i} ^ {2} - \\frac {1}{k} \\left(\\sum_ {i = 1} ^ {m} z _ {i}\\right) ^ {2} = \\frac {1}{k} \\sum_ {i < j} ^ {k} (z _ {i} - z _ {j}) ^ {2} $$\n\n$$ \\begin{array}{l} \\sum_ {i = 1} ^ {k} z _ {i} ^ {2} - \\frac {1}{k} \\left(\\sum_ {i = 1} ^ {m} z _ {i}\\right) ^ {2} = \\sum_ {i = 1} ^ {k} z _ {i} ^ {2} - \\frac {1}{k} \\left(\\sum_ {i = 1} ^ {k} z _ {i} ^ {2} + 2 \\sum_ {i < j} ^ {k} z _ {i} z _ {j}\\right) \\\\ = \\frac {k - 1}{k} \\sum_ {i = 1} ^ {k} z _ {i} ^ {2} - \\frac {2}{k} \\sum_ {i < j} ^ {k} z _ {i} z _ {j} \\\\ = \\frac {1}{k} \\sum_ {i < j} ^ {k} (z _ {i} ^ {2} + z _ {j} ^ {2}) - \\frac {2}{k} \\sum_ {i < j} ^ {k} z _ {i} z _ {j} \\\\ = \\frac {1}{k} \\sum_ {i < j} ^ {k} z _ {i} ^ {2} - 2 z _ {i} z _ {j} + z _ {j} ^ {2} \\\\ = \\frac {1}{k} \\sum_ {i < j} ^ {k} (z _ {i} - z _ {j}) ^ {2} \\\\ \\end{array} $$",
      "context_after": "Now, we can prove that the integral assignment problem is NP-hard.\n\nProof: First, we observe that for any nontrivial solution to the integral assignment instance, there must be two bins $b \\ne b ^ { \\prime }$ such that $X _ { \\sigma _ { 2 m + 1 } , b } = 1$ and $X _ { \\sigma _ { 2 m + 2 } , b ^ { \\prime } } = 1$ . In other words, the people with $\\sigma _ { 2 m + 1 }$ and $\\sigma _ { 2 m + 2 }$ must be split up. If not, then all the people of group 1 would be in the same bin, meaning that bin must be labeled with the base rate $\\rho _ { 1 } = 1 / 2$ . In order to maintain fairness, the same would have to be done for all the people of group 2, resulting in the trivial solution. Moreover, $b$ and $b ^ { \\prime }$ must be labeled $( 1 \\pm \\sqrt { 2 \\gamma - 1 } ) / 2$ respectively because those are the fraction of people of group 1 in those bins who belong to the positive class.\n\nThis means that $\\gamma _ { 1 } = 1 / \\rho \\cdot ( a _ { 1 , \\sigma _ { 2 m + 1 } } p _ { \\sigma _ { 2 m + 1 } } ^ { 2 } + a _ { 1 , \\sigma _ { 2 m + 2 } } p _ { \\sigma _ { 2 m + 2 } } ^ { 2 } ) = p _ { \\sigma _ { 2 m + 1 } } ^ { 2 } + p _ { \\sigma _ { 2 m + 2 } } ^ { 2 } = \\gamma$ as defined above. We know that a well-calibrated assignment is fair only if $\\gamma _ { 1 } = \\gamma _ { 2 }$ , so we know $\\gamma _ { 2 } = \\gamma$ .",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1609.05807_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1610.02413": [
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg",
      "image_filename": "1610.02413_page0_fig0.jpg",
      "caption": "Proof. Since a derived predictor $\\widetilde { Y }$ can only depend on $( { \\widehat { Y } } , A )$ and these variables are binary, the predictor $\\widetilde { Y }$ is completely described by four parameters in [0,1] cor",
      "context_before": "Our next lemma shows that $P _ { 0 } ( \\widehat { Y } )$ and $P _ { 1 } ( \\widehat { Y } )$ characterize exactly the trade-offs between false positives and true positives that we can achieve with any derived classifier. The polytopes are visualized in Figure 1.\n\nLemma 4.3. A predictor $\\widetilde { Y }$ is derived if and only if for all $a \\in \\{ 0 , 1 \\}$ , we have $\\gamma _ { a } ( \\widetilde { Y } ) \\in P _ { a } ( \\widehat { Y } )$ .\n\nProof. Since a derived predictor $\\widetilde { Y }$ can only depend on $( { \\widehat { Y } } , A )$ and these variables are binary, the predictor $\\widetilde { Y }$ is completely described by four parameters in [0,1] corresponding to the probabilities $\\operatorname* { P r } \\left\\{ { \\widetilde { Y } } = 1 \\mid { \\widehat { \\overline { { Y } } } } = { \\widehat { y } } , { \\dot { A } } = a \\right\\}$ for $\\widehat { y , a } \\in \\{ 0 , 1 \\}$ . Each of these parameter choices leads to one of the points in $P _ { a } ( \\widehat { Y } )$ and every point in the convex hull can be achieved by some parameter setting. □",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig1.jpg",
      "image_filename": "1610.02413_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig2.jpg",
      "image_filename": "1610.02413_page0_fig2.jpg",
      "caption": "Figure 1 gives a simple geometric picture for the solution of the linear program whose guarantees are summarized next.",
      "context_before": "",
      "context_after": "Combining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds:\n\n$$ \\min _ {\\widetilde {Y}} \\mathbb {E} \\ell (\\widetilde {Y}, Y) \\tag {4.3} $$\n\n$$ \\text {s . t .} \\quad \\forall a \\in \\{0, 1 \\}: \\gamma_ {a} (\\widetilde {Y}) \\in P _ {a} (\\widehat {Y}) \\quad \\left(\\text {d e r i v e d}\\right) $$",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "image_filename": "1610.02413_page0_fig3.jpg",
      "caption": "$$ D _ {a} \\stackrel {\\text {d e f}} {=} \\operatorname {c o n v h u l l} \\left\\{C _ {a} (t): t \\in [ 0, 1 ] \\right\\} \\tag {4.4} $$",
      "context_before": "When the ROC curves do not agree, we might choose different thresholds $t _ { a }$ for the different protected groups. This yields different points on each $A$ -conditional ROC curve. For the resulting predictor to satisfy equalized odds, these must be at the same point in the false/truepositive plane. This is possible only at points where all $A$ -conditional ROC curves intersect. But the ROC curves might not all intersect except at the trivial endpoints, and even if they do, their point of intersection might represent a poor tradeoff between false positive and false negatives.\n\nAs with the case of correcting a binary predictor, we can use randomization to fill the span of possible derived predictors and allow for significant intersection in the false/true-positive plane. In particular, for every protected group $a$ , consider the convex hull of the image of the conditional ROC curve:\n\n$$ D _ {a} \\stackrel {\\text {d e f}} {=} \\operatorname {c o n v h u l l} \\left\\{C _ {a} (t): t \\in [ 0, 1 ] \\right\\} \\tag {4.4} $$",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig4.jpg",
      "image_filename": "1610.02413_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig5.jpg",
      "image_filename": "1610.02413_page0_fig5.jpg",
      "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "context_before": "",
      "context_after": "The definition of $D _ { a }$ is analogous to the polytope $P _ { a }$ in the previous section, except that here we do not consider points below the main diagonal (line from $( 0 , 0 )$ to (1, 1)), which are worse than “random guessing” and hence never desirable for any reasonable loss function.\n\nDeriving an optimal equalized odds threshold predictor. Any point in the convex hull $D _ { a }$ represents the false/true positive rates, conditioned on $A = a$ , of a randomized derived predictor based on R. In particular, since the space is only two-dimensional, such a predictor $\\widetilde { Y }$ can always be taken to be a mixture of two threshold predictors (corresponding to the convex hull of two points on the ROC curve). Conditional on $A = a$ , the predictor $\\widetilde { Y }$ behaves as\n\n$$ \\widetilde {Y} = \\mathbb {I} \\left\\{R > T _ {a} \\right\\}, $$",
      "referring_paragraphs": [
        "The feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal (see Figure 2). Since for any loss function the optimal false/true-positive rate will always be on the upper-left boundary of this feasible set, this is effectively the ROC curve of the equalized odds predictors. This ROC curve is the pointwise minimum of all $A$ -conditional ROC curves. The performance of an ",
        "Deriving an optimal equal opportunity threshold predictor. The construction follows the same approach except that there is one fewer constraint. We only need to find points on the conditional ROC curves that have the same true positive rates in both groups. Assuming continuity of the conditional ROC curves, this means we can always find points on the boundary of the conditional ROC curves. In this case, no randomization is necessary. The optimal solution corresponds to two deterministic threshol",
        "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right).",
        "$\\widetilde { Y } = \\overset { \\cdots } { 1 }$ ${ \\underline { { p } } } _ { a }$\n\nThe feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal (see Figure 2).",
        "Here we use, as Figure 2 illustrates, that the cost of the best solution is convex as a function of its true positive rate."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg",
      "image_filename": "1610.02413_page0_fig6.jpg",
      "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.",
      "context_before": "The Bayes optimal classifier, for any proper loss, is then a threshold predictor of $R$ , where the threshold depends on the loss function (see, e.g., [Was10]). We will extend this result to the case where we additionally ask the classifier to satisfy an oblivious property, such as our non-discrimination properties.\n\nProposition 5.2. For any source distribution over $( Y , X , A )$ with Bayes optimal regressor $R ( X , A )$ , any loss function, and any oblivious property C, there exists a predictor $Y ^ { * } ( R , A )$ such that:\n\nProof. Consider an arbitrary classifier $\\widehat { Y }$ on the attributes $( X , A )$ , defined by a (possibly randomized) function ${ \\widehat { Y } } = f ( X , A )$ . Given $R = r , A = a$ ), we can draw a fresh $X ^ { \\prime }$ from the distribution $( X \\mid R = r , A = a )$ , and set $Y ^ { * } = f ( X ^ { \\prime } , a )$ . This satisfies (2). Moreover, since Y is binary with expectation R, Y is independent of $X$ conditioned on $( R , A )$ . Hence $( Y , X , R , A )$ and $( Y , X ^ { \\prime } , R , A )$ have identical distributions, so $( Y ^ { * } , A , Y )$ and $( \\widehat { Y } , A , Y )$ also have identical distributions. This implies $Y ^ { \\ast }$ satisfies (1) as desired. □",
      "context_after": "Corollary 5.3 (Optimality characterization). An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor.\n\nWe can furthermore show that if we can approximate the (unconstrained) Bayes optimal regressor well enough, then we can also construct a nearly optimal non-discriminating classifier.\n\nTo state the result, we introduce the following distance measure on random variables.",
      "referring_paragraphs": [
        "Figure 3: Graphical model for the proof of Proposition 5.2.\n\nCorollary 5.3 (Optimality characterization). An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_8",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig7.jpg",
      "image_filename": "1610.02413_page0_fig7.jpg",
      "caption": "Figure 4: Graphical model for Scenario I.",
      "context_before": "6 Oblivious identifiability of discrimination\n\nBefore turning to analyzing data, we pause to consider to what extent “black box” oblivious tests like ours can identify discriminatory predictions. To shed light on this issue, we introduce two possible scenarios for the dependency structure of the score, the target and the protected attribute. We will argue that while these two scenarios can have fundamentally different interpretations from the point of view of fairness, they can be indistinguishable from their joint distribution. In particular, no oblivious test can resolve which of the two scenarios applies.\n\nScenario I Consider the dependency structure depicted in Figure 4. Here, $X _ { 1 }$ is a feature highly (even deterministically) correlated with the protected attribute $A$ , but independent of the target Y given A. For example, $X _ { 1 }$ might be “languages spoken at home” or “great great grandfather’s profession”. The target Y has a statistical correlation with the protected attribute. There’s a second real-valued feature $X _ { 2 }$ correlated with Y , but only related to A through Y . For example, $X _ { 2 }$",
      "context_after": "might capture an applicant’s driving record if applying for insurance, financial activity if applying for a loan, or criminal history in criminal justice situations. An intuitively “fair” predictor here is to use only\n\nthe feature $X _ { 2 }$ through the score $\\widetilde { R } = X _ { 2 } ^ { { \\mathbf { \\Upsilon } } }$ . The score $\\widetilde { R }$ satisfies equalized odds, since $X _ { 2 }$ and $A$ are independent conditional on Y . Because of the statistical correlation between $A$ and $Y$ , a better statistical predictor, with greater power, can be obtained by taking into account also the protected attribute $A$ , or perhaps its surrogate $X _ { 1 }$ . The statistically optimal predictor would have the form $R ^ { * } = r _ { I } ^ { * } ( X _ { 2 } , X _ { 1 } )$ , biasing the score according to the protected attribute $A$ . The score $R ^ { * }$ does not satisfy equalized odds, and in a sense seems to be “profiling” based on $A$ .\n\nScenario II Now consider the dependency structure depicted in Figure 5. Here $X _ { 3 }$ is a feature, e.g. “wealth” or “annual income”, correlated with the protected attribute $A$ and directly predictive of the target Y . That is, in this model, the probability of paying back of a loan is just a function of an individual’s wealth, independent of their race. Using $X _ { 3 }$ on its own as a predictor, e.g. using the score $R ^ { * } = X _ { 3 }$ , does not naturally seem directly discriminatory. However, as can be seen from the dependency structure, this score does not satisfy equalized odds. We can correct it to satisfy equalized odds and consider the optimal non-discriminating predictor $\\widetilde { R } = \\widetilde { r _ { I I } } ( X _ { 3 } , A )$",
      "referring_paragraphs": [
        "Scenario I Consider the dependency structure depicted in Figure 4. Here, $X _ { 1 }$ is a feature highly (even deterministically) correlated with the protected attribute $A$ , but independent of the target Y given A. For example, $X _ { 1 }$ might be “languages spoken at home” or “great great grandfather’s profession”. The target Y has a statistical correlation with the protected attribute. There’s a second real-valued feature $X _ { 2 }$ correlated with Y , but only related to A through Y . For",
        "Scenario I Consider the dependency structure depicted in Figure 4.",
        "Figure 4: Graphical model for Scenario I."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_9",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg",
      "image_filename": "1610.02413_page0_fig8.jpg",
      "caption": "Figure 5: Graphical model for Scenario II.",
      "context_before": "the feature $X _ { 2 }$ through the score $\\widetilde { R } = X _ { 2 } ^ { { \\mathbf { \\Upsilon } } }$ . The score $\\widetilde { R }$ satisfies equalized odds, since $X _ { 2 }$ and $A$ are independent conditional on Y . Because of the statistical correlation between $A$ and $Y$ , a better statistical predictor, with greater power, can be obtained by taking into account also the protected attribute $A$ , or perhaps its surrogate $X _ { 1 }$ . The statistically optimal predictor would have the form $R ^ { * } = r _ { I } ^ { * } ( X _ { 2 } , X _ { 1 } )$ , biasing the score according to the protected attribute $A$ . The score $R ^ { * }$ does not satisfy equalized odds, and in a sense seems to be “profiling” based on $A$ .\n\nScenario II Now consider the dependency structure depicted in Figure 5. Here $X _ { 3 }$ is a feature, e.g. “wealth” or “annual income”, correlated with the protected attribute $A$ and directly predictive of the target Y . That is, in this model, the probability of paying back of a loan is just a function of an individual’s wealth, independent of their race. Using $X _ { 3 }$ on its own as a predictor, e.g. using the score $R ^ { * } = X _ { 3 }$ , does not naturally seem directly discriminatory. However, as can be seen from the dependency structure, this score does not satisfy equalized odds. We can correct it to satisfy equalized odds and consider the optimal non-discriminating predictor $\\widetilde { R } = \\widetilde { r _ { I I } } ( X _ { 3 } , A )$\n\nthat does satisfy equalized odds. If $A$ and $X _ { 3 }$ , and thus $A$ and $Y$ , are positively correlated, then $\\widetilde { R }$ would depend inversely on $A$ (see numerical construction below), introducing a form of “corrective discrimination”, so as to make $\\widetilde { R }$ is independent of $A$ given Y (as is required by equalized odds).",
      "context_after": "6.1 Unidentifiability\n\nThe above two scenarios seem rather different. The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protected attribute and the score would give different indications for the optimal score $R ^ { * }$ in the two scenarios. If it were judged unfair in one scenario, it would also be judged unfair in the other.\n\nWe will show this by constructing specific instantiations of the two scenarios where the joint distributions over $( Y , \\overbrace { A } , R ^ { * } , \\widetilde { R } )$ are identical. The scenarios are thus unidentifiable based only on these joint distributions.",
      "referring_paragraphs": [
        "Scenario II Now consider the dependency structure depicted in Figure 5. Here $X _ { 3 }$ is a feature, e.g. “wealth” or “annual income”, correlated with the protected attribute $A$ and directly predictive of the target Y . That is, in this model, the probability of paying back of a loan is just a function of an individual’s wealth, independent of their race. Using $X _ { 3 }$ on its own as a predictor, e.g. using the score $R ^ { * } = X _ { 3 }$ , does not naturally seem directly discriminatory",
        "Scenario II Now consider the dependency structure depicted in Figure 5.",
        "Figure 5: Graphical model for Scenario II."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg",
      "image_filename": "1610.02413_page0_fig9.jpg",
      "caption": "We will consider binary targets and protected attributes taking values in $A , Y \\in \\{ - 1 , 1 \\}$ and real valued features.",
      "context_before": "The above two scenarios seem rather different. The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protected attribute and the score would give different indications for the optimal score $R ^ { * }$ in the two scenarios. If it were judged unfair in one scenario, it would also be judged unfair in the other.\n\nWe will show this by constructing specific instantiations of the two scenarios where the joint distributions over $( Y , \\overbrace { A } , R ^ { * } , \\widetilde { R } )$ are identical. The scenarios are thus unidentifiable based only on these joint distributions.\n\nWe will consider binary targets and protected attributes taking values in $A , Y \\in \\{ - 1 , 1 \\}$ and real valued features. We deviate from our convention of {0,1}-values only to simplify the resulting expressions. In Scenario I, let:",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_11",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig10.jpg",
      "image_filename": "1610.02413_page0_fig10.jpg",
      "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.",
      "context_before": "",
      "context_after": "In Scenario II, let:\n\nThe following proposition establishes the equivalence between the scenarios and the optimality of the scores (proof at end of section):\n\nProposition 6.1. The joint distributions of $( Y , A , R ^ { * } , \\widetilde { R } )$ are identical in the above two scenarios. Moreover, $R ^ { * }$ and $\\widetilde { R }$ are optimal unconstrained and equalized odds scores respectively, in that their ROC curves are optimal and for any loss function an optimal (unconstrained or equalized odds) classifier can be derived from them by thresholding.",
      "referring_paragraphs": [
        "Not only can an oblivious test (based only on $( Y , A , R ) )$ not distinguish between the two scenarios, but even having access to the features is not of much help. Suppose we have access to all three feature, i.e. to a joint distribution over $( Y , A , X _ { 1 } , X _ { 2 } , X _ { 3 } )$ —since the distributions over $( Y , A , R ^ { * } , \\widetilde { R } )$ agree, we can construct such a joint distribution with $X _ { 2 } = \\widetilde { R }$ and $X _ { 3 } = \\widetilde { R }$ . The featur",
        "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg",
      "image_filename": "1610.02413_page0_fig11.jpg",
      "caption": "in figures), Hispanic, and black.",
      "context_before": "7 Case study: FICO scores\n\nWe examine various fairness measures in the context of FICO scores with the protected attribute of race. FICO scores are a proprietary classifier widely used in the United States to predict credit worthiness. Our FICO data is based on a sample of 301536 TransUnion TransRisk scores from 2003 [Res07]. These scores, ranging from 300 to 850, try to predict credit risk; they form our score R. People were labeled as in default if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; this gives an outcome Y . Our protected attribute $A$ is race, which is restricted to four values: Asian, white non-Hispanic (labeled “white”\n\nin figures), Hispanic, and black. FICO scores are complicated proprietary classifiers based on features, like number of bank accounts kept, that could interact with culture—and hence race—in unfair ways. A credit score cutoff of 620 is commonly used for prime-rate loans1,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_13",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig12.jpg",
      "image_filename": "1610.02413_page0_fig12.jpg",
      "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.",
      "context_before": "",
      "context_after": "which corresponds to an any-account default rate of $1 8 \\%$ . Note that this measures default on any account TransUnion was aware of; it corresponds to a much lower $( \\approx 2 \\% )$ chance of default on individual new loans. To illustrate the concepts, we use any-account default as our target Y —a higher positive rate better illustrates the difference between equalized odds and equal opportunity.\n\nWe therefore consider the behavior of a lender who makes money on default rates below this, i.e., for whom whom false positives (giving loans to people that default on any account) is 82/18 as expensive as false negatives (not giving a loan to people that don’t default). The lender thus wants to construct a predictor $\\widehat { Y }$ that is optimal with respect to this asymmetric loss. A typical classifier will pick a threshold per group and set $\\widehat { Y } = \\overset { \\bullet } { 1 }$ for people with FICO scores above the threshold for their group. Given the marginal distributions for each group (Figure 7), we can study the optimal profit-maximizing classifier under five different constraints on allowed predictors:\n\n1http://www.creditscoring.com/pages/bar.htm (Accessed: 2016-09-20)",
      "referring_paragraphs": [
        "We therefore consider the behavior of a lender who makes money on default rates below this, i.e., for whom whom false positives (giving loans to people that default on any account) is 82/18 as expensive as false negatives (not giving a loan to people that don’t default). The lender thus wants to construct a predictor $\\widehat { Y }$ that is optimal with respect to this asymmetric loss. A typical classifier will pick a threshold per group and set $\\widehat { Y } = \\overset { \\bullet } { 1 }$ for",
        "Figure 7: These two marginals, and the number of people per group, constitute our input data."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg",
      "image_filename": "1610.02413_page0_fig13.jpg",
      "caption": "1http://www.creditscoring.com/pages/bar.htm (Accessed: 2016-09-20)",
      "context_before": "which corresponds to an any-account default rate of $1 8 \\%$ . Note that this measures default on any account TransUnion was aware of; it corresponds to a much lower $( \\approx 2 \\% )$ chance of default on individual new loans. To illustrate the concepts, we use any-account default as our target Y —a higher positive rate better illustrates the difference between equalized odds and equal opportunity.\n\nWe therefore consider the behavior of a lender who makes money on default rates below this, i.e., for whom whom false positives (giving loans to people that default on any account) is 82/18 as expensive as false negatives (not giving a loan to people that don’t default). The lender thus wants to construct a predictor $\\widehat { Y }$ that is optimal with respect to this asymmetric loss. A typical classifier will pick a threshold per group and set $\\widehat { Y } = \\overset { \\bullet } { 1 }$ for people with FICO scores above the threshold for their group. Given the marginal distributions for each group (Figure 7), we can study the optimal profit-maximizing classifier under five different constraints on allowed predictors:\n\n1http://www.creditscoring.com/pages/bar.htm (Accessed: 2016-09-20)",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_15",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig14.jpg",
      "image_filename": "1610.02413_page0_fig14.jpg",
      "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.",
      "context_before": "",
      "context_after": "• Equalized odds requires both the fraction of non-defaulters that qualify for loans and the fraction of defaulters that qualify for loans to be constant across groups. This cannot be achieved with a single threshold for each group, but requires randomization. There are many ways to do it; here, we pick two thresholds for each group, so above both thresholds people always qualify and between the thresholds people qualify with some probability.\n\nWe could generalize the above constraints to allow non-threshold classifiers, but we can show that each profit-maximizing classifier will use thresholds. As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9. Our proposed fairness definitions give thresholds between those of max-profit/race-blind thresholds and of demographic parity. Figure 10 plots the ROC curves for each group. It should be emphasized that differences in the ROC curve do not indicate differences in default behavior but rather differences in prediction accuracy—lower curves indicate FICO scores are less predictive for those populations. This demonstrates, as one should expect, that the majority (white) group is classified more accurately than minority groups, even over-represented minority groups like Asians.\n\nThe left side of Figure 11 shows the fraction of people that wouldn’t default that would qualify for loans by the various metrics. Under max-profit and race-blind thresholds, we find that black people that would not default have a significantly harder time qualifying for loans than others. Under demographic parity, the situation is reversed.",
      "referring_paragraphs": [
        "Hence it will pick the single threshold at which $8 2 \\%$ of people do not default overall, shown in Figure 8.",
        "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg",
      "image_filename": "1610.02413_page0_fig15.jpg",
      "caption": "The right side of Figure 11 gives the profit achieved by each method, as a fraction of the max profit achievable.",
      "context_before": "We could generalize the above constraints to allow non-threshold classifiers, but we can show that each profit-maximizing classifier will use thresholds. As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9. Our proposed fairness definitions give thresholds between those of max-profit/race-blind thresholds and of demographic parity. Figure 10 plots the ROC curves for each group. It should be emphasized that differences in the ROC curve do not indicate differences in default behavior but rather differences in prediction accuracy—lower curves indicate FICO scores are less predictive for those populations. This demonstrates, as one should expect, that the majority (white) group is classified more accurately than minority groups, even over-represented minority groups like Asians.\n\nThe left side of Figure 11 shows the fraction of people that wouldn’t default that would qualify for loans by the various metrics. Under max-profit and race-blind thresholds, we find that black people that would not default have a significantly harder time qualifying for loans than others. Under demographic parity, the situation is reversed.\n\nThe right side of Figure 11 gives the profit achieved by each method, as a fraction of the max profit achievable. We show this as a function of the non-default rate above which loans are profitable (i.e. $8 2 \\%$ in the other figures). At $8 2 \\%$ , we find that a race blind threshold gets $9 9 . 3 \\%$ of the maximal profit, equal opportunity gets $9 2 . 8 \\%$ , equalized odds gets $8 0 . 2 \\%$ , and demographic parity gets $6 9 . 8 \\%$ . So equal opportunity fairness costs less than a quarter what demographic parity costs—and if the classifier improves, this would reduce further.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_17",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig16.jpg",
      "image_filename": "1610.02413_page0_fig16.jpg",
      "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.",
      "context_before": "",
      "context_after": "The difference between equal odds and equal opportunity is that under equal opportunity, the classifier can make use of its better accuracy among whites. Under equal odds this is viewed as unfair, since it means that white people who wouldn’t pay their loans have a harder time getting them than minorities who wouldn’t pay their loans. An equal odds classifier must classify everyone as poorly as the hardest group, which is why it costs over twice as much in this case. This also leads to more conservative lending, so it is slightly harder for non-defaulters of all groups to get loans.\n\nThe equal opportunity classifier does make it easier for defaulters to get loans if they are minorities, but the incentives are aligned properly. Under max profit, a small group may not be worth figuring out how to classify and so be treated poorly, since the classifier can’t identify the qualified individuals. Under equal opportunity, such poorly-classified groups are instead treated better than well-classified groups. The cost is thus born by the company using the classifier, which can decide to invest in better classification, rather than the classified group, which cannot. Equalized odds gives a similar, but much stronger, incentive since the cost for a small group is not proportional to its size.\n\nWhile race blindness achieves high profit, the fairness guarantee is quite weak. As with max profit, small groups may be classified poorly and so treated poorly, and the company has little incentive to improve the accuracy. Furthermore, when race is redundantly encoded, race blindness degenerates into max profit.",
      "referring_paragraphs": [
        "We could generalize the above constraints to allow non-threshold classifiers, but we can show that each profit-maximizing classifier will use thresholds. As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9. Our proposed fairness definitions give thresholds between those of max-profit/race-blind thresholds and of demographic parity. Figure 10 plots the ROC curves for each group. It should be emphasized that differences in the ROC curve do n",
        "As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9.",
        "Figure 9: FICO thresholds for various definitions of fairness."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg",
      "image_filename": "1610.02413_page0_fig17.jpg",
      "caption": "We proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion. Second, it is fully",
      "context_before": "The equal opportunity classifier does make it easier for defaulters to get loans if they are minorities, but the incentives are aligned properly. Under max profit, a small group may not be worth figuring out how to classify and so be treated poorly, since the classifier can’t identify the qualified individuals. Under equal opportunity, such poorly-classified groups are instead treated better than well-classified groups. The cost is thus born by the company using the classifier, which can decide to invest in better classification, rather than the classified group, which cannot. Equalized odds gives a similar, but much stronger, incentive since the cost for a small group is not proportional to its size.\n\nWhile race blindness achieves high profit, the fairness guarantee is quite weak. As with max profit, small groups may be classified poorly and so treated poorly, and the company has little incentive to improve the accuracy. Furthermore, when race is redundantly encoded, race blindness degenerates into max profit.\n\nWe proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion. Second, it is fully",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_19",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig18.jpg",
      "image_filename": "1610.02413_page0_fig18.jpg",
      "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We could generalize the above constraints to allow non-threshold classifiers, but we can show that each profit-maximizing classifier will use thresholds. As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9. Our proposed fairness definitions give thresholds between those of max-profit/race-blind thresholds and of demographic parity. Figure 10 plots the ROC curves for each group. It should be emphasized that differences in the ROC curve do n",
        "Figure 10 plots the ROC curves for each group.",
        "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg",
      "image_filename": "1610.02413_page0_fig19.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_21",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig20.jpg",
      "image_filename": "1610.02413_page0_fig20.jpg",
      "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.",
      "context_before": "",
      "context_after": "aligned with the central goal of supervised machine learning, that is, to build higher accuracy classifiers. In light of our results, we draw several conclusions aimed to help interpret and apply our framework effectively.\n\nChoose reliable target variables. Our notion requires access to observed outcomes such as default rates in the loan setting. This is precisely the same requirement that supervised learning\n\ngenerally has. The broad success of supervised learning demonstrates that this requirement is met in many important applications. That said, having access to reliable “labeled data” is not always possible. Moreover, the measurement of the target variable might in itself be unreliable or biased. Domain-specific scrutiny is required in defining and collecting a reliable target variable.",
      "referring_paragraphs": [
        "The left side of Figure 11 shows the fraction of people that wouldn’t default that would qualify for loans by the various metrics. Under max-profit and race-blind thresholds, we find that black people that would not default have a significantly harder time qualifying for loans than others. Under demographic parity, the situation is reversed.",
        "The right side of Figure 11 gives the profit achieved by each method, as a fraction of the max profit achievable. We show this as a function of the non-default rate above which loans are profitable (i.e. $8 2 \\%$ in the other figures). At $8 2 \\%$ , we find that a race blind threshold gets $9 9 . 3 \\%$ of the maximal profit, equal opportunity gets $9 2 . 8 \\%$ , equalized odds gets $8 0 . 2 \\%$ , and demographic parity gets $6 9 . 8 \\%$ . So equal opportunity fairness costs less than a quarter w",
        "The left side of Figure 11 shows the fraction of people that wouldn’t default that would qualify for loans by the various metrics.",
        "Figure 11: On the left, we see the fraction of non-defaulters that would get loans."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.02413_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1610.07524": [
    {
      "doc_id": "1610.07524",
      "figure_id": "1610.07524_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg",
      "image_filename": "1610.07524_page0_fig0.jpg",
      "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
      "context_before": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\n\n2.1 Implied constraints on the false positive and false negative rates\n\nTo facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , which is obtained",
      "context_after": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a confusion matrix, as shown below.",
      "referring_paragraphs": [
        "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.",
        "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.\n\nby thresholding $S$ at some cutoff $s H R$"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.07524_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.07524",
      "figure_id": "1610.07524_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/09498987ca3b129ebf2c38faa64e216d91beb00ee6d9dfb69e971601979d1f6c.jpg",
      "image_filename": "09498987ca3b129ebf2c38faa64e216d91beb00ee6d9dfb69e971601979d1f6c.jpg",
      "caption": "It is easily verified that test fairness of $S$ implies that the positive predictive value of the coarsened score $S _ { c }$ does not depend on $R$ . More precisely, it implies that that the quantity",
      "context_before": "by thresholding $S$ at some cutoff $s H R$\n\n$$ S _ {c} (x) \\equiv \\left\\{ \\begin{array}{l l} \\mathrm {H R} & \\text {i f} S (x) > s _ {H R} \\\\ \\mathrm {L R} & \\text {i f} S (x) \\leq s _ {H R} \\end{array} \\right. \\tag {2.2} $$\n\nThe coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a confusion matrix, as shown below.",
      "context_after": "It is easily verified that test fairness of $S$ implies that the positive predictive value of the coarsened score $S _ { c }$ does not depend on $R$ . More precisely, it implies that that the quantity\n\n$$ \\operatorname {P P V} \\left(S _ {c} \\mid R = r\\right) \\equiv \\mathbb {P} (Y = 1 \\mid S _ {c} = \\mathrm {H R}, R = r) \\tag {2.3} $$\n\ndoes not depend on $r$ . Equation (2.3) thus forms a necessary condition for the test fairness of $S$ . We can think of this as a constraint on the values of the confusion matrix. A second constraint—one that we have no direct control over—is the recidivism prevalence within groups, which we denote here by $p _ { r } \\equiv \\mathbb { P } ( Y = 1 \\mid R = r )$ .",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.07524_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.07524",
      "figure_id": "1610.07524_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
      "image_filename": "1610.07524_page0_fig1.jpg",
      "caption": "The $\\%$ non-overlap of two distributions is generally calculated assuming both distributions are normal, and thus has a one-to-one correspondence to Cohen’s $d$ [12].6 Figure 3 shows that the COMPAS decile score is far ",
      "context_before": "3.1 Connections to measures of effect size\n\nA natural question to ask is whether the level of disparate impact, $\\Delta$ , is related to some measures of effect size commonly used in scientific reporting. With a small generalization of the $\\%$ non-overlap measure, we can answer this question in the affirmative.\n\nThe $\\%$ non-overlap of two distributions is generally calculated assuming both distributions are normal, and thus has a one-to-one correspondence to Cohen’s $d$ [12].6 Figure 3 shows that the COMPAS decile score is far from being normally distributed. A more reasonable way to calculate $\\%$ non-overlap is to note that in the Gaussian case $\\%$ non-overlap",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.07524_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.07524",
      "figure_id": "1610.07524_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig2.jpg",
      "image_filename": "1610.07524_page0_fig2.jpg",
      "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals. Figure 3: COMPAS decile score histograms for Black and White defendants. Cohen’s $d = 0 . 6 0$ , non-overlap $d _ { \\mathrm { T V } } ( f _ { b } , f _ { w } ) = 2 4 . 5 \\%$ .",
      "context_before": "",
      "context_after": "is equivalent to the total variation distance. Letting $f _ { r , y } ( s )$ denote the score distribution for race $r$ and recidivism outcome $y$ , one can establish the following sharp bound on $\\Delta$ .\n\nProposition 3.2 (Percent overlap bound). Under the MinMax policy,\n\n$$ \\Delta \\leq (t _ {H} - t _ {L}) d _ {\\mathrm {T V}} (f _ {b, y}, f _ {w, y}). $$",
      "referring_paragraphs": [
        "One might expect that differences in false positive rates are largely attributable to the subset of defendants who are charged with more serious offenses and who have a larger number of prior arrests/convictions. While it is true that the false positive rates within both racial groups are higher for defendants with worse criminal histories, considerable between-group differences in these error rates persist across low prior count subgroups. Figure 2 shows a plot of false positive rates across di",
        "Our analysis indicates that there are risk assessment use cases in which it is desirable to balance error rates across different groups, even though this will generally result in risk assessments that are not free from predictive bias. However, balancing error rates overall may not be sufficient, as this does not guarantee balance at finer levels of granularity. That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior rec",
        "Figure 2 shows a plot of false positive rates across different ranges of prior count for defendants charged with a misdemeanor offense, which is the lowest severity criminal offense category.",
        "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense.",
        "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.07524_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1610.08452": [
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg",
      "image_filename": "1610.08452_page0_fig0.jpg",
      "caption": "to her sensitive attribute information, and there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups.",
      "context_before": "The emergence and widespread usage of automated datadriven decision making systems in a wide variety of applications, ranging from content recommendations to pretrial risk assessment, has raised concerns about their potential unfairness towards people with certain traits [8, 22, 24, 27]. Anti-discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits, also called sensitive attributes (e.g., gender, race). These laws typically distinguish between two different notions of unfairness [5] namely, disparate treatment and disparate impact. More specifically, there is disparate treatment when the decisions an individual user receives change with changes\n\nAn open-source code implementation of our scheme is available at: http://fate-computing.mpi-sws.org/\n\n$\\textcircled { \\mathrm { c } } 2 0 1 7$ International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW 2017, April 3–7, 2017, Perth, Australia. ACM 978-1-4503-4913-0/17/04. http://dx.doi.org/10.1145/3038912.3052660",
      "context_after": "to her sensitive attribute information, and there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups. A number of recent studies [10, 21, 29], including our own prior work [28], have focused on designing decision making systems that avoid one or both of these types of unfairness.\n\nThese prior designs have attempted to tackle unfairness in decision making scenarios where the historical decisions in the training data are biased (i.e., groups of people with certain sensitive attributes may have historically received unfair treatment) and there is no ground truth about the correctness of the historical decisions (i.e., one cannot tell whether a historical decision used during the training phase was right or wrong). However, when the ground truth for historical decisions is available, disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by means of the ground truth. Therefore, disparate impact would not be a suitable notion of unfairness in such scenarios.\n\nIn this paper, we propose an alternative notion of unfairness, disparate mistreatment , especially well-suited for scenarios where ground truth is available for historical decisions used during the training phase. We call a decision making process to be suffering from disparate mistreatment with respect to a given sensitive attribute (e.g., race) if the misclassification rates differ for groups of people having different values of that sensitive attribute (e.g., blacks and whites). For example, in the case of the NYPD Stopquestion-and-frisk program (SQF) [1], where pedestrians are stopped on the suspicion of possessing an illegal weapon [12], having different weapon discovery rates for different races would constitute a case of disparate mistreatment.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/2981d6b9081dfd2aa714b04d5c166a6692b2665ab3f4dff9d6856f82ac2d389c.jpg",
      "image_filename": "2981d6b9081dfd2aa714b04d5c166a6692b2665ab3f4dff9d6856f82ac2d389c.jpg",
      "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a ",
      "context_before": "In addition to all misclassifications in general, depending on the application scenario, one might want to measure disparate mistreatment with respect to different kinds of misclassifications. For example, in pretrial risk assessments, the decision making process might only be required to ensure that the false positive rates are equal for all groups, since it may be more acceptable to let a guilty person go, rather than incarcerate an innocent person. 1 On the other hand, in loan approval systems, one might instead favor a decision making process in which the false negative rates are equal, to ensure that deserving (positive class) people with a certain sensitive attribute value are not denied (negative class) loans disproportionately. Similarly, depending on the application\n\narXiv:1610.08452v2 [stat.ML] 8 Mar 2017\n\n1 “It is better that ten guilty persons escape than that one innocent suffer”—William Blackstone",
      "context_after": "scenario at hand, and the cost of the type of misclassification, one may choose to measure disparate mistreatment using false discovery and false omission rates, instead of false positive and false negative rates (see Table 1).\n\nIn the remainder of the paper, we first formalize disparate treatment, disparate impact and disparate mistreatment in the context of (binary) classification. Then, we introduce intuitive measures of disparate mistreatment for decision boundary-based classifiers and show that, for a wide variety of linear and nonlinear classifiers, these measures can be incorporated into their formulation as convex-concave constraints. The resulting formulation can be solved efficiently using recent advances in convex-concave programming [26]. Finally, we experiment with synthetic as well as real world datasets and show that our methodology can be effectively used to avoid disparate mistreatment.\n\n2. BACKGROUND AND RELATED WORK",
      "referring_paragraphs": [
        "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.",
        "scenario at hand, and the cost of the type of misclassification, one may choose to measure disparate mistreatment using false discovery and false omission rates, instead of false positive and false negative rates (see Table 1).",
        "Figure 1 provides an example of decision making systems (classifiers) with and without disparate mistreatment. In all cases, the classifiers need to decide whether to stop a pedestrian—on the suspicion of possessing an illegal weapon— using a set of features such as bulge in clothing and proximity to a crime scene. The “ground truth” on whether a pedestrian actually possesses an illegal weapon is also shown. We show decisions made by three different classifiers $\\mathbf { C _ { 1 } }$ , $\\mathbf",
        "In Figure 1, we deem $\\mathbf { C _ { 2 } }$ and ${ \\bf C 3 }$ to be unfair due to disparate treatment since C2’s (C3’s) decisions for M ale 1 and F emale 1 (Male 2 and F emale 2) are different even though they have the same values of non-sensitive attributes. Here, disparate treatment corresponds to the very intuitive notion of fairness: two otherwise similar persons should not be treated differently solely because of a difference in gender.",
        "In Figure 1, assuming that a pedestrian benefits from a decision of not being stopped, we deem $\\mathbf { C _ { 1 } }$ as unfair due to disparate impact because the fraction of males and females that were stopped are different (1.0 and 0.66, respectively).",
        "New notion 3: Avoiding disparate mistreatment. A binary classifier does not suffer from disparate mistreatment if the misclassification rates for different groups of people having different values of the sensitive feature $z$ are the same. Table 1 describes various ways of measuring misclassification rates. Specifically, misclassification rates can be measured as fractions over the class distribution in the ground truth labels, i.e., as false positive and false negative rates, or over the class ",
        "Some recent works [7, 18] have investigated the impossibility of simultaneously satisfying multiple notions of fairness. Chouldechova [7] and Kleinberg et al. [18], show that, when the fraction of users with positive class labels differ between members of different sensitive attribute value groups, it is impossible to construct classifiers that are equally wellcalibrated (where well-calibration essentially measures the false discovery and false omission rates of a classifier) and also satisfy th",
        "Crime</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C1</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>Male 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>0</td><td>C2</td><td>✓</td><td>X</td><td>✓</td></tr><tr><td>Male 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Male 3</td><td>0</td><td>1</td><td>✓</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 3</td><td>0</td><td>0</td><td>✓</td><td>0</td><td>1</td><td>0</td><td></td><td></td><td></td><td></td></tr></table>\n\nFigure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon.",
        "Figure 1 provides an example of decision making systems (classifiers) with and without disparate mistreatment.",
        "Table 1: In addition to the overall misclassification rate, error rates can be measured in two different ways: false negative rate and false positive rate are defined as fractions over the class distribution in the ground truth labels, or true labels.",
        "Table 1 describes various ways of measuring misclassification rates.",
        "These results suggest that satisfying all five criterion of disparate mistreatment (Table 1) simultaneously is impossible when the underlying distribution of data is different for different groups."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/e0de09b580c7c517ccffa597e2121a1874d929ec4ae4b2383c7a8159c6c4a552.jpg",
      "image_filename": "e0de09b580c7c517ccffa597e2121a1874d929ec4ae4b2383c7a8159c6c4a552.jpg",
      "caption": "Table 1: In addition to the overall misclassification rate, error rates can be measured in two different ways: false negative rate and false positive rate are defined as fractions over the class distribution in the ground truth labels, or true labels. On the other hand, false discovery rate and false omission rate are defined as fractions over the class distribution in the predicted labels.",
      "context_before": "Given the above terminology, we can formally express the absence of disparate treatment, disparate impact and disparate mistreatment as follows:\n\nExisting notion 1: Avoiding disparate treatment. A binary classifier does not suffer from disparate treatment if:\n\n$$ P (\\hat {y} | \\mathbf {x}, z) = P (\\hat {y} | \\mathbf {x}), \\tag {1} $$",
      "context_after": "i.e., if the probability that the classifier outputs a specific value of $\\hat { y }$ given a feature vector x does not change after observing the sensitive feature $z$ , there is no disparate treatment.\n\nExisting notion 2: Avoiding disparate impact. A binary classifier does not suffer from disparate impact if:\n\n$$ P (\\hat {y} = 1 | z = 0) = P (\\hat {y} = 1 | z = 1), \\tag {2} $$",
      "referring_paragraphs": [
        "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.",
        "scenario at hand, and the cost of the type of misclassification, one may choose to measure disparate mistreatment using false discovery and false omission rates, instead of false positive and false negative rates (see Table 1).",
        "Figure 1 provides an example of decision making systems (classifiers) with and without disparate mistreatment. In all cases, the classifiers need to decide whether to stop a pedestrian—on the suspicion of possessing an illegal weapon— using a set of features such as bulge in clothing and proximity to a crime scene. The “ground truth” on whether a pedestrian actually possesses an illegal weapon is also shown. We show decisions made by three different classifiers $\\mathbf { C _ { 1 } }$ , $\\mathbf",
        "In Figure 1, we deem $\\mathbf { C _ { 2 } }$ and ${ \\bf C 3 }$ to be unfair due to disparate treatment since C2’s (C3’s) decisions for M ale 1 and F emale 1 (Male 2 and F emale 2) are different even though they have the same values of non-sensitive attributes. Here, disparate treatment corresponds to the very intuitive notion of fairness: two otherwise similar persons should not be treated differently solely because of a difference in gender.",
        "In Figure 1, assuming that a pedestrian benefits from a decision of not being stopped, we deem $\\mathbf { C _ { 1 } }$ as unfair due to disparate impact because the fraction of males and females that were stopped are different (1.0 and 0.66, respectively).",
        "New notion 3: Avoiding disparate mistreatment. A binary classifier does not suffer from disparate mistreatment if the misclassification rates for different groups of people having different values of the sensitive feature $z$ are the same. Table 1 describes various ways of measuring misclassification rates. Specifically, misclassification rates can be measured as fractions over the class distribution in the ground truth labels, i.e., as false positive and false negative rates, or over the class ",
        "Some recent works [7, 18] have investigated the impossibility of simultaneously satisfying multiple notions of fairness. Chouldechova [7] and Kleinberg et al. [18], show that, when the fraction of users with positive class labels differ between members of different sensitive attribute value groups, it is impossible to construct classifiers that are equally wellcalibrated (where well-calibration essentially measures the false discovery and false omission rates of a classifier) and also satisfy th",
        "Crime</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C1</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>Male 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>0</td><td>C2</td><td>✓</td><td>X</td><td>✓</td></tr><tr><td>Male 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Male 3</td><td>0</td><td>1</td><td>✓</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 3</td><td>0</td><td>0</td><td>✓</td><td>0</td><td>1</td><td>0</td><td></td><td></td><td></td><td></td></tr></table>\n\nFigure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon.",
        "Figure 1 provides an example of decision making systems (classifiers) with and without disparate mistreatment.",
        "Table 1: In addition to the overall misclassification rate, error rates can be measured in two different ways: false negative rate and false positive rate are defined as fractions over the class distribution in the ground truth labels, or true labels.",
        "Table 1 describes various ways of measuring misclassification rates.",
        "These results suggest that satisfying all five criterion of disparate mistreatment (Table 1) simultaneously is impossible when the underlying distribution of data is different for different groups."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg",
      "image_filename": "1610.08452_page0_fig1.jpg",
      "caption": "(a) Cov. vs FPR",
      "context_before": "We then train several logistic regression classifiers on the same training data subject to fairness constraints on false positive rate, i.e., we train a logistic regressor by solving problem (18), where $g _ { \\boldsymbol { \\theta } } ( y , \\mathbf { x } )$ is given by Eq. (11). Each classifier constrains the false positive rate covariance (c) with a multiplicative factor $\\left( m \\in \\left[ 0 , 1 \\right] \\right)$ ) of the covariance of the unconstrained classifier (c∗), that is, $c = m c ^ { * }$ . Ideally, a smaller $m$ , and hence a smaller $c$ , would result in more fair outcomes.\n\nResults. Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier\n\n4 Our fairness constraints can be easily incorporated to other boundary-based classifiers such as (non)linear SVMs.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig2.jpg",
      "image_filename": "1610.08452_page0_fig2.jpg",
      "caption": "(b) Fairness vs. Acc.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig3.jpg",
      "image_filename": "1610.08452_page0_fig3.jpg",
      "caption": "(c) Boundaries Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
      "context_before": "",
      "context_after": "(dashed). In this figure, we observe that: i) as the fairness constraint value $c = m c ^ { * }$ goes to zero, the false positive rates for both groups $z = 0$ and $z = 1$ ) converge, and hence, the outcomes of the classifier become more fair, i.e., $D _ { F P R } 0$ , while $D _ { F N R }$ remains close to zero (the invariance of $D _ { F N R }$ may however change depending on the underlying distribution of the data); ii) ensuring lower values of disparate mistreatment leads to a larger drop in accuracy.\n\n5.1.2 Disparate mistreatment on both false positive rate and false negative rate\n\nIn this section, we consider a more complex scenario, where the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive rate and false negative rate, i.e., both $D _ { F P R }$ and $D _ { F N R }$ are non-zero. This scenario can in turn be split into two cases:",
      "referring_paragraphs": [
        "Results. Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier",
        "Comparison results. Table 2 shows the performance comparison for all the methods on the three synthetic datasets described above. We can observe that, while all four methods mostly achieve similar levels of fairness, they do it at different costs in terms of accuracy. Both Our methodsen and Hardt et al.—which use sensitive feature information while making decisions—present the best performance in terms of accuracy (due to the additional information available to them). However, as explained earli",
        "Results. Table 2 (last block) summarizes the results by showing the trade-off between fairness and accuracy achieved by our method, the method by Hardt et al., and the baseline. Similarly to the results in Section 5.1.2, we observe that for all three mehtods, controlling for disparate mistreatment on false positive rate (false negative rate) also helps decrease disparate mistreatment on false negative rate (false positive rate). Moreover, all three methods are able to achieve similar accuracy fo",
        "and hence, a better performance from our method. On the other hand, the method by Hardt et al. is able to achieve both zero $D _ { F P R }$ and $D _ { F N R }$ while controlling for disparate mistreatment on both false positive and false negative rates (Table 2)—albeit at a considerable drop in terms of accuracy. Since this method operates on a data of much smaller dimensionality (the final classifier probability estimates), it is not expected to suffer as much from the small size of the dataset",
        "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier",
        "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar.",
        "Table 2 shows the performance comparison for all the methods on the three synthetic datasets described above.",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig4.jpg",
      "image_filename": "1610.08452_page0_fig4.jpg",
      "caption": "(a) FPR constraints",
      "context_before": "$$ p (\\mathbf {x} | z = 1, y = - 1) = \\mathcal {N} ([ - 5, 0 ], [ 5, 1; 1, 5 ]) $$\n\nThen, we train an unconstrained logistic regression classifier on this dataset. It attains an accuracy of 0.80 but leads to $D _ { F P R } = 0 . 3 3 - 0 . 0 8 = 0 . 2 5$ and $D _ { F N R } = 0 . 2 6 - 0 . 1 2 = 0 . 1 4$ , resulting in disparate mistreatment in terms of both false positive and negative rates. Then, similarly to the previous scenario, we train three different kind of constrained classifiers to remove disparate mistreatment on (i) false positive rate, (ii) false negatives rate, and (iii) both.\n\nResults. Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment with respect to false positive rate, false negative rate and both, respectively. We",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg",
      "image_filename": "1610.08452_page0_fig5.jpg",
      "caption": "(b) FNR constraints",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig6.jpg",
      "image_filename": "1610.08452_page0_fig6.jpg",
      "caption": "(c) Both constraints",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_10",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig7.jpg",
      "image_filename": "1610.08452_page0_fig7.jpg",
      "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results. (a) FPR constraints",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl",
        "In this analysis, for simplicity, we only consider a subset of offenders whose race was either black or white. Recidivism rates for the two groups are shown in Table 3.",
        "Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.",
        "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   \n(a) FPR constraints",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "image_filename": "1610.08452_page0_fig8.jpg",
      "caption": "(b) FNR constraints",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_12",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig9.jpg",
      "image_filename": "1610.08452_page0_fig9.jpg",
      "caption": "(c) Both constraints Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "context_before": "",
      "context_after": "observe several interesting patterns. First, controlling disparate mistreatment for only false positive rate (false negative rate), leads to a minor drop in accuracy, but can exacerbate the disparate mistreatment on false negative rate (false positive rate). For example, while the decision boundary is moved to control for disparate mistreatment on false negative rate, that is, to ensure that more examples with $z = 0$ are well-classified in the positive class (reducing false negative rate), it also moves previously well-classified negative examples into the positive class, hence increasing the false positive rate. A similar phenomenon occur when controlling disparate mistreatment with respect to only false positive rate. As a consequence, controlling for both types of disparate mistreatment simultaneously brings $D _ { F P R }$ and $D _ { F N R }$ close to zero, but causes a large drop in accuracy.\n\n5.1.3 Performance Comparison\n\nIn this section, we compare the performance of our scheme with two different methods on the synthetic datasets described above. In particular, we compare the performance of the following approaches:",
      "referring_paragraphs": [
        "Results. Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment with respect to false positive rate, false negative rate and both, respectively. We",
        "Using this ground truth, we build an unconstrained logistic regression classifier to predict whether an offender will (positive class) or will not (negative class) recidivate within two years. The set of features used in the classification task are described in Table 4. 6, 7",
        "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment with respect to false positive rate, false negative rate and both, respectively.",
        "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign.",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/87166449bda9f81cc5d7585fea03ab59f8734136363011f75a45caacb0c89b29.jpg",
      "image_filename": "87166449bda9f81cc5d7585fea03ab59f8734136363011f75a45caacb0c89b29.jpg",
      "caption": "5 COMPAS tries to predict the recidivism risk (on a scale of 1–10) of a criminal offender by analyzing answers to 137 questions pertaining to the offender’s criminal history and behavioral patterns [2].",
      "context_before": "7 Since race is one of the features in the learnable set, we additionally assume that all the methods have access to the sensitive attributes while making decisions.\n\n2, 639 examples in the training set.\n\n5 COMPAS tries to predict the recidivism risk (on a scale of 1–10) of a criminal offender by analyzing answers to 137 questions pertaining to the offender’s criminal history and behavioral patterns [2].",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/355979e3f76b36240e6f8e0059dd614840bd4700ec1ed0c9ccef7b27851f7938.jpg",
      "image_filename": "355979e3f76b36240e6f8e0059dd614840bd4700ec1ed0c9ccef7b27851f7938.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/5dd2af74ee86e0baf2b7d13d1f2215c85123ed41aa85ee876622f4a34c8471b0.jpg",
      "image_filename": "5dd2af74ee86e0baf2b7d13d1f2215c85123ed41aa85ee876622f4a34c8471b0.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/5a4444a3cb9a00cf8852f635b7bd010c13590be199c152e00fbbc5a930a4568c.jpg",
      "image_filename": "5a4444a3cb9a00cf8852f635b7bd010c13590be199c152e00fbbc5a930a4568c.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/f803b3e90fd8e0a754441284c123bb8831bbabee059e752b029316f9c92da2ae.jpg",
      "image_filename": "f803b3e90fd8e0a754441284c123bb8831bbabee059e752b029316f9c92da2ae.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/be1ca83fda5b851fe02cb330ce44fd6e3ea06dc898f866defe262920233c5ab8.jpg",
      "image_filename": "be1ca83fda5b851fe02cb330ce44fd6e3ea06dc898f866defe262920233c5ab8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/6760939c497257caf8c488100c4f891680a946c7f1a0770594c9d3f70761399c.jpg",
      "image_filename": "6760939c497257caf8c488100c4f891680a946c7f1a0770594c9d3f70761399c.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/4419ce00c33482b1d24fdc73398e9cba704c050e84752d6da4eeacedc71ff0f6.jpg",
      "image_filename": "4419ce00c33482b1d24fdc73398e9cba704c050e84752d6da4eeacedc71ff0f6.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_21",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/8a1c20185e7441e1887d712ed58fd47c4e719d3a989512597c1c13d1952e0d7c.jpg",
      "image_filename": "8a1c20185e7441e1887d712ed58fd47c4e719d3a989512597c1c13d1952e0d7c.jpg",
      "caption": "Table 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Results. Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier",
        "Comparison results. Table 2 shows the performance comparison for all the methods on the three synthetic datasets described above. We can observe that, while all four methods mostly achieve similar levels of fairness, they do it at different costs in terms of accuracy. Both Our methodsen and Hardt et al.—which use sensitive feature information while making decisions—present the best performance in terms of accuracy (due to the additional information available to them). However, as explained earli",
        "Results. Table 2 (last block) summarizes the results by showing the trade-off between fairness and accuracy achieved by our method, the method by Hardt et al., and the baseline. Similarly to the results in Section 5.1.2, we observe that for all three mehtods, controlling for disparate mistreatment on false positive rate (false negative rate) also helps decrease disparate mistreatment on false negative rate (false positive rate). Moreover, all three methods are able to achieve similar accuracy fo",
        "and hence, a better performance from our method. On the other hand, the method by Hardt et al. is able to achieve both zero $D _ { F P R }$ and $D _ { F N R }$ while controlling for disparate mistreatment on both false positive and false negative rates (Table 2)—albeit at a considerable drop in terms of accuracy. Since this method operates on a data of much smaller dimensionality (the final classifier probability estimates), it is not expected to suffer as much from the small size of the dataset",
        "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier",
        "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar.",
        "Table 2 shows the performance comparison for all the methods on the three synthetic datasets described above.",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_22",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/0f6b9336e99bb68f8d1513846a95d0aaabaed0b5a0261aca4fc29f6f8844e9f8.jpg",
      "image_filename": "0f6b9336e99bb68f8d1513846a95d0aaabaed0b5a0261aca4fc29f6f8844e9f8.jpg",
      "caption": "Table 3: Recidivism rates in ProPublica COMPAS data for both races.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Results. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previousl",
        "In this analysis, for simplicity, we only consider a subset of offenders whose race was either black or white. Recidivism rates for the two groups are shown in Table 3.",
        "Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.",
        "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   \n(a) FPR constraints",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_23",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/b552bcd7b68dcb83e63a36f7b08a042de108a11f990805b90d1e2346d6abb749.jpg",
      "image_filename": "b552bcd7b68dcb83e63a36f7b08a042de108a11f990805b90d1e2346d6abb749.jpg",
      "caption": "Table 4: Description of features used from ProPublica COMPAS data.",
      "context_before": "",
      "context_after": "and hence, a better performance from our method. On the other hand, the method by Hardt et al. is able to achieve both zero $D _ { F P R }$ and $D _ { F N R }$ while controlling for disparate mistreatment on both false positive and false negative rates (Table 2)—albeit at a considerable drop in terms of accuracy. Since this method operates on a data of much smaller dimensionality (the final classifier probability estimates), it is not expected to suffer as much from the small size of the dataset as compared to our method or the baseline (which depend on the misclassification covariance computed on the feature set).\n\n6. DISCUSSION AND FUTURE WORK\n\nAs shown in Section 5, the method proposed in this paper provides a flexible tradeoff between disparate mistreatmentbased fairness and accuracy. It also allows to avoid disparate mistreatment and disparate treatment simultaneously. This feature might be specially useful in scenarios when the sensitive attribute information is not available (e.g., due to privacy reasons) or is prohibited from being used due to disparate treatment laws [5].",
      "referring_paragraphs": [
        "Results. Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment with respect to false positive rate, false negative rate and both, respectively. We",
        "Using this ground truth, we build an unconstrained logistic regression classifier to predict whether an offender will (positive class) or will not (negative class) recidivate within two years. The set of features used in the classification task are described in Table 4. 6, 7",
        "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment with respect to false positive rate, false negative rate and both, respectively.",
        "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign.",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1610.08452_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1611.07438": [
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/125d9c1213112fd86b7b711a5de0fbb87973b1b695431ae92b73629bd5b46374.jpg",
      "image_filename": "125d9c1213112fd86b7b711a5de0fbb87973b1b695431ae92b73629bd5b46374.jpg",
      "caption": "Table 1: Summary statistics of Example 1.",
      "context_before": "Achieving Non-Discrimination in Data Release\n\nDiscrimination discovery and prevention/removal are increasingly important tasks in data mining. Discrimination discovery aims to unveil discriminatory practices on the protected attribute (e.g., gender) by analyzing the dataset of historical decision records, and discrimination prevention aims to remove discrimination by modifying the biased data before conducting predictive analysis. In this paper, we show that the key to discrimination discovery and prevention is to find the meaningful partitions that can be used to provide quantitative evidences for the judgment of discrimination. With the support of the causal graph, we present a graphical condition for identifying a meaningful partition. Based on that, we develop a simple criterion for the claim of non-discrimination, and propose discrimination removal algorithms which accurately remove discrimination while retaining good data utility. Experiments using real datasets show the effectiveness of our approaches.\n\nDiscrimination discovery and prevention/removal has been an active research area recently [11, 13, 25, 24, 8]. Discrimination discovery is the data mining problem of unveiling evidence of discriminatory practices by analyzing the dataset of historical decision records, and discrimination prevention aims to ensure non-discrimination by modifying the biased data before conducting predictive analysis (e.g., building classifiers). Discrimination refers to unjustified distinctions of individuals based on their membership in a certain group. Federal Laws and regulations (e.g., Fair Credit Reporting Act or Equal Credit Opportunity Act) prohibit discrimination on several grounds, such as gender, age, marital status, sexual orientation, race, religion or belief, and disability or illness, which are referred to as the protected attributes. Different types of discrimination have been introduced, which can be generally categorized as direct and indirect discrimination [8]. Direct discrimination occurs when individuals are treated less favorably in comparable situations explicitly due to their membership in a protected group; indirect discrimination refers to an apparently neutral practice which results in an unfair treatment of a protected group [11, 24]. In this paper, we focus on the problem of discrimination discovery and prevention on direct discrimination. In the following, we simply say discrimination for direct discrimination.",
      "context_after": "",
      "referring_paragraphs": [
        "tected attribute, and admission is the decision. We assume there is no correlation between gender and test score. The summary statistics of the admission rate is shown in Table 1. It can be observed that the average admission rate is $37 \\%$ for females and $46 \\%$ for males. It is already known that the judgment of discrimination cannot be made simply based on the average admission rates in the whole population and further partitioning is needed. If we partition the data conditioning on test sc",
        "We use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examin",
        "Table 1: Summary statistics of Example 1.",
        "Figure 1: Causal graph of an example university admission system.",
        "The causal graph of the examples is shown in Figure 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/7f7397e73e6c5668f821f78d526c1b92de2e41da8da2858fa7326406964ecc6a.jpg",
      "image_filename": "7f7397e73e6c5668f821f78d526c1b92de2e41da8da2858fa7326406964ecc6a.jpg",
      "caption": "Table 2: Summary statistics of Example 2.",
      "context_before": "",
      "context_after": "For a quantitative measurement of discrimination, a general legal principle is to measure the difference in the proportion of positive decisions between the protected group and non-protected group [24]. Discovering and preventing discrimination is not trivial. As shown by Zliobait ˇ e et ˙ al. [28], simply considering the difference measured in the whole population fails to take into account the part of differences that are explainable by other attributes, and removing all the differences will result in reverse discrimination. They proposed to partition the data by conditioning on a certain attribute, and remove the difference within each produced subpopulation. Their method, although in the right direction, has two significant limitations: 1) it does not distinguish whether a partition is meaningful for measuring and removing discrimination; 2) it does not consider the situation where there exist multiple meaningful partitions.\n\nTypically, given a population, a partition is determined by a subset of non-protected attributes and a subpopulation is specified by a value assignment to the attributes. In our study we find that, the key to answering whether a population contains discrimination is to finding the meaningful partitions that can be used to provide quantitative evidences for the judgment of discrimination. We have two observations. First, not all partitions are meaningful and using inappropriate partitions will result in misleading conclusions. For example, consider a toy model for a university admission system that contains four attributes: gender, major, test score, and admission, where gender is the pro-\n\narXiv:1611.07438v1 [cs.LG] 22 Nov 2016",
      "referring_paragraphs": [
        "Second, when there are multiple meaningful partitions, examining one partition showing no bias does not guarantee no bias based on other partitions. Consider a different example on the same toy model shown in Table 2. The average admission rate now becomes $43 \\%$ equally for both females and males. Further conditioning on major still shows that females and males have the same chance to be admitted in the two subpopulations. However, when partitioning the data based on the combination {major, te",
        "We use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examin",
        "education is defined in the second tier, and all other attributes are defined in the third tier. The constructed causal graph is shown in Figure 2. We treat sex (female and male) as the protected attribute and income (low income and high income) as the decision. An arc pointing from sex to income is observed. We first find set Q of income, which contains all the non-protected attributes. There are 512 subpopulations specified by Q, and 376 subpopulations with non-zero number of tuples. Then, we ",
        "applicants</td><td>450</td><td>150</td><td>150</td><td>450</td><td>300</td><td>100</td><td>100</td><td>300</td></tr><tr><td>admission rate</td><td>20%</td><td>40%</td><td>20%</td><td>40%</td><td>50%</td><td>70%</td><td>50%</td><td>70%</td></tr><tr><td></td><td colspan=\"2\">25%</td><td colspan=\"2\">35%</td><td colspan=\"2\">55%</td><td colspan=\"2\">65%</td></tr></table>\n\nTable 2: Summary statistics of Example 2.",
        "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 .",
        "The constructed causal graph is shown in Figure 2.",
        "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg",
      "image_filename": "1611.07438_page0_fig0.jpg",
      "caption": "Figure 1: Causal graph of an example university admission system.",
      "context_before": "We make use of the above connection to identify the direct causal effect of $C$ on $E$ . We construct a new DAG $\\scriptstyle { G ^ { \\prime } }$ by deleting the arc $C E$ from $\\mathcal { G }$ and keeping everything else unchanged. Thus, the possible difference between the causal relationships represented by $\\scriptstyle { G ^ { \\prime } }$ and $\\mathcal { G }$ lies merely in the presence of the direct causal effect of $C$ on $E$ . We consider a node set B such that $( E \\mathrm { ~ \\bf ~ \\underline { ~ } { ~ \\underline { ~ } { ~ \\bf ~ U ~ } ~ } ~ } C \\mathrm { ~ \\bf ~ \\underline { ~ } { ~ \\bf ~ \\mathsf { ~ B ~ } ~ } ~ } ) _ { \\mathcal { G } ^ { \\prime } }$ , and use B to examine the conditional independence relations in $\\mathcal { D }$ . If there is no direct causal effect of $C$ on $E$ in $\\mathcal { G }$ , we should also obtain $( E ~ \\texttt { l l } C \\texttt { l } \\mathbf { B } ) _ { \\mathcal { G } }$ , which entails $( E \\perp \\perp C \\mid \\mathbf { B } ) _ { \\mathcal { D } }$ , i.e., $\\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) = \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } ) = \\operatorname* { P r } ( e ^ { + } | \\mathbf { b } )$ for each value assignment $\\mathbf { b }$ , ,of B. However, if we observe $\\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) \\neq \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } )$ , the difference must be due to the , ,existence of the direct causal effect of $C$ on $E$ . Therefore, $\\Delta P | _ { \\mathbf { b } } = \\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) - \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } )$ can be used to measure the ,direct causal effect of $C$ on $E$ ,. On the other hand, if a node set S does not satisfy $( E \\perp \\perp C \\mid \\mathbf { S } ) _ { \\mathcal { G } ^ { \\prime } }$ , then this conditional dependence between $C$ and $E$ given S that is not caused by the direct causal effect will also exist in $\\Delta P \\vert _ { \\mathbf { s } }$ . As a result, $\\Delta P \\vert _ { \\mathbf { s } }$ cannot accurately measure the direct causal effect.\n\nIt must be noted that, for $\\Delta P \\vert _ { \\mathbf { b } }$ to correctly measure the direct causal effect, set B cannot contain any descendant of $E$ even when it satisfies the requirement $( E \\perp \\perp C \\mid \\mathbf { B } ) _ { \\mathcal { G } ^ { \\prime } }$ . This is because when conditioning on $E$ ’s descendants, part of the knowledge of $E$ is already given since the consequences caused by $E$ is known.\n\nBased on the above analysis, we measure the direct causal effect of $C$ on $E$ using $\\Delta P \\vert _ { \\mathbf { b } }$ where the node set B satisfies the following requirements: (1) $( C \\parallel E \\mid \\mathbf { B } ) _ { \\mathcal { G } ^ { \\prime } }$ holds; (2) B contains none of $E$ ’s decedents. We call such set B a block set. By treating the direct causal effect of $C$ one $E$ as the effect of direct discrimination, $\\Delta P \\vert _ { \\mathbf { b } }$ is considered as a correct measure for direct discrimination if and only if B is a block set. Thus, if $| \\Delta P | _ { \\mathbf { b } } | \\ge \\tau$ , it is a quantitative evidence τof discrimination against either $c ^ { - }$ or $c ^ { + }$ for subpopulation b. As can be seen, each block set B forms a meaningful partition on the dataset where the direct causal effect of $C$ on $E$ within each subpopulation b can be correctly measured by $\\Delta P \\vert _ { \\mathbf { b } }$ . On the other hand, for any partition that is not defined by a block set, the measured differences, which either contain spurious influences or have been explained by the consequences of the decisions, cannot accurately measure the direct causal effect and hence cannot be used to prove discrimination or non-discrimination. Therefore, we",
      "context_after": "give the following theorem. The proof follows the above analysis.\n\nTheorem 2.1. A node set B forms a meaningful partition for measuring discrimination $i f$ and only if B is a block set, i.e., B satisfies: (1) $( C \\texttt { \\ \" } \\bot { E } | \\texttt { B } ) _ { \\mathcal { G } ^ { \\prime } }$ holds; (2) B contains none of $E$ ’s decedents, where $\\scriptstyle { G ^ { \\prime } }$ is the graph constructed by deleting arc $C \\ \\ E$ from $\\mathcal { G }$ . Discriminatory effect is considered to present for subpopulation b if $| \\Delta P | _ { \\mathbf { b } } | ~ \\ge ~ \\tau ,$ , where $\\Delta P | _ { \\mathbf { b } } = \\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) - \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } )$ .\n\n3 Discrimination Discovery and Prevention",
      "referring_paragraphs": [
        "tected attribute, and admission is the decision. We assume there is no correlation between gender and test score. The summary statistics of the admission rate is shown in Table 1. It can be observed that the average admission rate is $37 \\%$ for females and $46 \\%$ for males. It is already known that the judgment of discrimination cannot be made simply based on the average admission rates in the whole population and further partitioning is needed. If we partition the data conditioning on test sc",
        "We use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examin",
        "Table 1: Summary statistics of Example 1.",
        "Figure 1: Causal graph of an example university admission system.",
        "The causal graph of the examples is shown in Figure 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/70ad0c655da5ed8499636b5de3224e6deb769b0861f3a77939cc7f55a89a4053.jpg",
      "image_filename": "70ad0c655da5ed8499636b5de3224e6deb769b0861f3a77939cc7f55a89a4053.jpg",
      "caption": "Table 3: Contingency table within subpopulation q.",
      "context_before": "3.3.2 Discrimination Removal by Modifying Dataset The second strategy directly modifies the decisions of selected tuples from the dataset to meet the non-discrimination criterion. For each value assignment q, if $\\Delta P | _ { \\mathbf { q } } \\ge \\tau$ , we randomly select a number of tuples with $C = c ^ { - }$ τand $E = e ^ { - }$ , and change their $E$ values from $e ^ { - }$ to $e ^ { + }$ . If $\\Delta P | _ { \\mathbf { q } } \\le - \\tau$ , we\n\nselect tuples similarly and change their $E$ values from $e ^ { + }$ to $e ^ { - }$ . As result, we ensure that for each q we have $| \\Delta P | _ { \\mathbf { q } } | \\le \\tau$ .\n\nFor any $E$ ’s non-decedent $X$ τ, according to the Markov condition, $X$ is independent of $E$ in each subpopulation specified by $E$ ’s parents, i.e., $C$ and Q. Since the modified tuples are randomly selected in the subpopulation specified by $C$ and Q, $X$ would still be independent of $E$ after the modification. Thus, all $E$ ’s non-decedents would be conditionally independent of $E$ given $C$ and Q, implying that Q is still the parent set of $E$ excluding $C$ after the modification. According to Theorem 3.2, the modified dataset satisfies the non-discrimination criterion.",
      "context_after": "To calculate the number of tuples to be modified within each subpopulation q, we express $\\Delta P \\vert _ { \\mathbf { q } }$ as $n _ { \\mathbf { q } } ^ { c ^ { + } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { + } } -$ $n _ { \\mathbf { q } } ^ { c ^ { - } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { - } }$ /. Please refer to Table 3 for the meaning of the /notations. For subpopulations with $\\Delta P | _ { \\mathbf { q } } \\ge \\tau$ , by selecting $\\lceil n _ { \\mathbf { q } } ^ { c ^ { - } } \\cdot ( \\lvert \\Delta P \\rvert _ { \\mathbf { q } } \\rvert - \\tau ) \\rceil$ tuples with $C = c ^ { - }$ and $E = e ^ { - }$ , and changing their $E$ τvalues from $e ^ { - }$ to $e ^ { + }$ , the value of $\\Delta P \\vert _ { \\mathbf { q } }$ would decrease by $\\lceil n _ { \\mathbf { q } } ^ { c ^ { - } } \\cdot ( | \\Delta P | _ { \\mathbf { q } } | - \\tau ) \\rceil / n _ { \\mathbf { q } } ^ { c ^ { - } } \\geq \\Delta P | _ { \\mathbf { q } } - \\tau$ . Therefore, we have $\\Delta P | _ { \\mathbf { q } } < \\tau$ τ / τafter the modification. The result is similar when $\\Delta P | _ { \\mathbf { q } } \\le - \\tau$ . The pseudo-code of the algorithm is τshown in Algorithm 3.\n\nThe complexity of Algorithm 3 includes the complexity of finding Q. Similar to Algorithm 1, we can identify $E$ ’s parents without building the whole network. Therefore, local discovery algorithms can be employed to improve the efficiency of algorithm. The complexity from Line 5 to 14 is bounded by the size of the original dataset, i.e., $O ( | \\mathcal { D } | )$ .\n\n4 Relaxed Non-Discrimination Criterion",
      "referring_paragraphs": [
        "To calculate the number of tuples to be modified within each subpopulation q, we express $\\Delta P \\vert _ { \\mathbf { q } }$ as $n _ { \\mathbf { q } } ^ { c ^ { + } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { + } } -$ $n _ { \\mathbf { q } } ^ { c ^ { - } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { - } }$ /. Please refer to Table 3 for the meaning of the /notations. For subpopulations with $\\Delta P | _ { \\mathbf { q } } \\ge \\tau$ , by selecting $\\lceil n _ { \\mathbf { q } } ^ { c ^ { - } } ",
        "αAnother dataset Dutch census consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, education level is defined in the second tire, and all other attributes are defined in the third tire. The constructed causal graph is shown in Figure 3. We treat sex (female and male) as the protected attribute and o",
        "Table 3: Contingency table within subpopulation q.",
        "The constructed causal graph is shown in Figure 3.",
        "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg",
      "image_filename": "1611.07438_page0_fig1.jpg",
      "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.",
      "context_before": "αAnother dataset Dutch census consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, education level is defined in the second tire, and all other attributes are defined in the third tire. The constructed causal graph is shown in Figure 3. We treat sex (female and male) as the protected attribute and occupation (occupation w low income, occupation w high income) as the decision. An arc from sex to occupation is observed in the causal graph. Set Q of occupation is $\\mathbf { Q } =$ {edu level age}. The value of $\\Delta P \\vert _ { \\mathbf { q } }$ ranges from 0 062 , .to 0 435 across all the 12 subpopulations specified by Q. .Thus, discrimination against females is detected in the Dutch dataset based on the non-discrimination criterion. Moreover, the mean and the standard variance of $\\Delta P \\vert _ { \\mathbf { q } }$ are 0 222 and 0 125, which has small $\\mathrm { P r } ( | \\Delta P | _ { \\mathbf { q } } | < \\tau )$ . based on the Cheby-.shev’s inequality, e.g., $\\mathrm { P r } ( | \\Delta P | _ { \\mathbf { q } } | < 0 . 3 0 ) \\ge 2 7 . 9 4 \\%$ . Hence, < . .the Dutch census dataset still contains discrimination based on the relaxed $\\alpha$ -non-discrimination criterion.\n\nαOur current implementation uses the PC algorithm to construct the complete causal graph. In our experiment, the PC algorithm with the default significance threshold 0 01 .takes 51.59 seconds to build the graph for the binarized Adult dataset and 139.96 seconds for the binarized Dutch census dataset. We also run the PC algorithm on the original Adult dataset, which incurs 4492.36 seconds. In our future work, we will explore the use of the local causal discovery algorithms to improve the efficiency.\n\n5.2 Discrimination Removal The performance of our two proposed discrimination removal algorithms, MGraph",
      "context_after": "",
      "referring_paragraphs": [
        "Second, when there are multiple meaningful partitions, examining one partition showing no bias does not guarantee no bias based on other partitions. Consider a different example on the same toy model shown in Table 2. The average admission rate now becomes $43 \\%$ equally for both females and males. Further conditioning on major still shows that females and males have the same chance to be admitted in the two subpopulations. However, when partitioning the data based on the combination {major, te",
        "We use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examin",
        "education is defined in the second tier, and all other attributes are defined in the third tier. The constructed causal graph is shown in Figure 2. We treat sex (female and male) as the protected attribute and income (low income and high income) as the decision. An arc pointing from sex to income is observed. We first find set Q of income, which contains all the non-protected attributes. There are 512 subpopulations specified by Q, and 376 subpopulations with non-zero number of tuples. Then, we ",
        "applicants</td><td>450</td><td>150</td><td>150</td><td>450</td><td>300</td><td>100</td><td>100</td><td>300</td></tr><tr><td>admission rate</td><td>20%</td><td>40%</td><td>20%</td><td>40%</td><td>50%</td><td>70%</td><td>50%</td><td>70%</td></tr><tr><td></td><td colspan=\"2\">25%</td><td colspan=\"2\">35%</td><td colspan=\"2\">55%</td><td colspan=\"2\">65%</td></tr></table>\n\nTable 2: Summary statistics of Example 2.",
        "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 .",
        "The constructed causal graph is shown in Figure 2.",
        "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/5361a1fc4e66988905c68ad46f73c660ad29cf966147db0254913c0d759abe87.jpg",
      "image_filename": "5361a1fc4e66988905c68ad46f73c660ad29cf966147db0254913c0d759abe87.jpg",
      "caption": "Table 4: Comparison of MGraph, MData, Naive, and two conditional discrimination removal algorithms (LM and LPS) on Adult and Dutch Census.",
      "context_before": "",
      "context_after": "and MData, in terms of the utility of the modified data is shown in Table 4. We also report the results from the Naive method used in [8] in which we completely reshuffle the gender information. We measure the utility by three metrics: the Euclidean distance $( d \\mathbf { \\theta } )$ , the number of modified tuples $( n _ { T } )$ , and the utility loss $( \\chi ^ { 2 } )$ . We can observe from χTable 4 that the MGraph algorithm retains the highest utility. Both MGraph and MData algorithms significantly outperform the Naive method. We also examine how utility in terms of three metrics vary with different $\\tau$ values for our τMGraph and MData algorithms. We can see from Table 5 that both discrimination removal algorithms incur less utility loss with larger $\\tau$ values. This observation validates our analysis of non-discrimination model.\n\nWe measure the execution times of our removal algo-",
      "referring_paragraphs": [
        "and MData, in terms of the utility of the modified data is shown in Table 4. We also report the results from the Naive method used in [8] in which we completely reshuffle the gender information. We measure the utility by three metrics: the Euclidean distance $( d \\mathbf { \\theta } )$ , the number of modified tuples $( n _ { T } )$ , and the utility loss $( \\chi ^ { 2 } )$ . We can observe from χTable 4 that the MGraph algorithm retains the highest utility. Both MGraph and MData algorithms signi",
        "the unexplainable (bad) discrimination when one of the attributes is considered to be explanatory for the discrimination. However, their methods do not distinguish whether a partition is meaningful or not. Therefore, they cannot find the correct partitions to measure the direct discriminatory effects. Our experiments show that, their methods cannot completely remove discrimination conditioning on any single attribute. The results are skipped due to space limitation. In addition, even if we remov",
        "Table 4: Comparison of MGraph, MData, Naive, and two conditional discrimination removal algorithms (LM and LPS) on Adult and Dutch Census.",
        "Furthermore, their methods incur much larger utility loss than our algorithms, as shown in the last two columns of Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_7",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/d6b85fd58267293011ffc95bc7fcc7b8662a00f66725b14e1ec9bfba96566c8a.jpg",
      "image_filename": "d6b85fd58267293011ffc95bc7fcc7b8662a00f66725b14e1ec9bfba96566c8a.jpg",
      "caption": "Table 5: Comparison of utility with varied $\\tau$ values for MGraph and MData.",
      "context_before": "and MData, in terms of the utility of the modified data is shown in Table 4. We also report the results from the Naive method used in [8] in which we completely reshuffle the gender information. We measure the utility by three metrics: the Euclidean distance $( d \\mathbf { \\theta } )$ , the number of modified tuples $( n _ { T } )$ , and the utility loss $( \\chi ^ { 2 } )$ . We can observe from χTable 4 that the MGraph algorithm retains the highest utility. Both MGraph and MData algorithms significantly outperform the Naive method. We also examine how utility in terms of three metrics vary with different $\\tau$ values for our τMGraph and MData algorithms. We can see from Table 5 that both discrimination removal algorithms incur less utility loss with larger $\\tau$ values. This observation validates our analysis of non-discrimination model.\n\nWe measure the execution times of our removal algo-",
      "context_after": "rithms. As expected, MGraph takes longer time than MData since the former requires quadratic programming and data generation based on the whole modified graph while the latter only requires the information of Q. For the Adult dataset with $\\tau \\ = \\ 0 . 0 5$ , MGraph takes 20.86s while MData takes τ .11.43s. For the Dutch dataset the difference is even larger, i.e., 735.83s for MGraph and 0.20s for MData, since the size of Q of Dutch census is much smaller.\n\n5.3 Comparison with conditional discrimination methods In [28], the authors measured the “bad” discrimination i.e., the effect that can be explained by conditioning on one attribute. They developed two methods, local massaging (LM) and local preferential sampling (LPS), to remove",
      "referring_paragraphs": [
        "and MData, in terms of the utility of the modified data is shown in Table 4. We also report the results from the Naive method used in [8] in which we completely reshuffle the gender information. We measure the utility by three metrics: the Euclidean distance $( d \\mathbf { \\theta } )$ , the number of modified tuples $( n _ { T } )$ , and the utility loss $( \\chi ^ { 2 } )$ . We can observe from χTable 4 that the MGraph algorithm retains the highest utility. Both MGraph and MData algorithms signi",
        "We can see from Table 5 that both discrimination removal algorithms incur less utility loss with larger $\\tau$ values."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg",
      "image_filename": "1611.07438_page0_fig2.jpg",
      "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.",
      "context_before": "rithms. As expected, MGraph takes longer time than MData since the former requires quadratic programming and data generation based on the whole modified graph while the latter only requires the information of Q. For the Adult dataset with $\\tau \\ = \\ 0 . 0 5$ , MGraph takes 20.86s while MData takes τ .11.43s. For the Dutch dataset the difference is even larger, i.e., 735.83s for MGraph and 0.20s for MData, since the size of Q of Dutch census is much smaller.\n\n5.3 Comparison with conditional discrimination methods In [28], the authors measured the “bad” discrimination i.e., the effect that can be explained by conditioning on one attribute. They developed two methods, local massaging (LM) and local preferential sampling (LPS), to remove",
      "context_after": "the unexplainable (bad) discrimination when one of the attributes is considered to be explanatory for the discrimination. However, their methods do not distinguish whether a partition is meaningful or not. Therefore, they cannot find the correct partitions to measure the direct discriminatory effects. Our experiments show that, their methods cannot completely remove discrimination conditioning on any single attribute. The results are skipped due to space limitation. In addition, even if we remove “bad” discrimination using their methods by conditioning on each attribute one by one, a significant amount of discriminatory effects still exist. After running the local massaging (LM) method, there are still 97 subpopultions (out of 376) with discrimination for Adult and 4 subpopulations (out of 12) with discrimination for Dutch census. The local preferential sampling (LPS) method performs even worse — there are 108 subpopultions with discrimination for Adult and 8 subpopulations with discrimination for Dutch census. This is because for both datasets, any single attribute is not a block set and hence does not form a meaningful partition. Even assuming each attribute forms a meaningful partition, removing discrimination for each partition one by one does not guarantee to remove discrimination since the modification under one partition may change the distributions under other partitions. Differently, our approaches remove discrimination based on block set Q and ensure that the causal structure is not changed after the modification. Thus, Theorem 3.2 can prove non-discrimination for our approaches. Furthermore, their methods incur much larger utility loss than our algorithms, as shown in the last two columns of Table 4.\n\nA number of data mining techniques have been proposed to discover discrimination in the literature. Classification rule-based methods such as elift [23] and belift [20] were proposed to represent certain discrimination patterns. In [19, 29], the authors dealt with the individual discrimination by finding a group of similar individuals. Zliobaitˇ e et˙ al. [28] proposed conditional discrimination. However, their approaches cannot determine whether a partition is meaningful and hence cannot achieve non-discrimination guarantee. Our work showed that the causal discriminatory effect through $C E$ can only be correctly measured under the partition specified by the block set. Recently, the authors in [3] proposed a framework based on the Suppes-Bayes causal network and developed several random-walk-based methods to detect different types of discrimination. However, the construction of the Suppes-Bayes causal network is impractical with the large number of attribute-value pairs. In addition, it is unclear how the number of random walks is related to practical discrimination metrics, e.g., the difference in positive decision rates.\n\nProposed methods for discrimination removal are either based on data preprocessing [13, 28] or algorithm tweaking [14, 4, 15]. The authors [7] addressed the problem of fair classification that achieves both group fairness, i.e., the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole, and individual fairness, i.e., similar individuals should be treated similarly. A recent work [8] studies how to remove disparate impact, i.e., indirect discrimination, from",
      "referring_paragraphs": [
        "To calculate the number of tuples to be modified within each subpopulation q, we express $\\Delta P \\vert _ { \\mathbf { q } }$ as $n _ { \\mathbf { q } } ^ { c ^ { + } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { + } } -$ $n _ { \\mathbf { q } } ^ { c ^ { - } e ^ { + } } / n _ { \\mathbf { q } } ^ { c ^ { - } }$ /. Please refer to Table 3 for the meaning of the /notations. For subpopulations with $\\Delta P | _ { \\mathbf { q } } \\ge \\tau$ , by selecting $\\lceil n _ { \\mathbf { q } } ^ { c ^ { - } } ",
        "αAnother dataset Dutch census consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, education level is defined in the second tire, and all other attributes are defined in the third tire. The constructed causal graph is shown in Figure 3. We treat sex (female and male) as the protected attribute and o",
        "Table 3: Contingency table within subpopulation q.",
        "The constructed causal graph is shown in Figure 3.",
        "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07438_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1611.07509": [
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg",
      "image_filename": "1611.07509_page0_fig0.jpg",
      "caption": "Figure 1: The toy model.",
      "context_before": "The causal modeling based discrimination detection has been proposed most recently (Bonchi et al. 2015; Zhang,\n\nCopyright $©$ 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\narXiv:1611.07509v1 [cs.LG] 22 Nov 2016",
      "context_after": "Wu, and Wu 2016b; 2016a) for improving the correlation based approaches. In this paper, we develop a framework for discovering and removing both direct and indirect discrimination based on the causal network. A causal network is a directed acyclic graph (DAG) widely used for causal representation, reasoning and inference (Pearl 2009), where causal effects are carried by the paths that trace arrows pointing from the cause to the effect which are referred to as the causal paths. Using this model, direct and indirect discrimination can be captured by the causal effects of the protected attribute on the decision transmitted along different paths. Direct discrimination is modeled by the causal effect transmitted along the direct path from the protected attribute to the decision. Indirect discrimination, on the other hand, is modeled by the causal effect transmitted along other causal paths that contain any unjustified attribute. Consider a toy model of a loan application system shown in Figure 1 for example. Assume that we treat Race as the protected attribute, Loan as the decision, and Zip code as the unjustified attribute that causes redlining. Direct discrimination is then modeled by path $\\mathtt { R a c e } \\to \\mathtt { L o a n }$ , and indirect discrimination is modeled by path Race → Zip code Loan. Assume that the use of Income can be objectively justified as it is reasonable to deny a loan if the applicant has low income. In this case, path Race → Income Loan is explainable, which means that the difference in loan issuance across different race groups can be explained by the fact that some race groups in the dataset tend to be under-paid.\n\nTo measure the causal effect transmitted along a certain causal path, we employ the formulation of the path-specific effect (Avin, Shpitser, and Pearl 2005; Shpitser 2013). We define direct and indirect discrimination as different pathspecific effects and show how to accurately measure them using the observational data. Based on that, we propose an effective algorithm for discovering direct and indirect discrimination, as well as an algorithm for precisely removing both types of discrimination while retaining good data utility. Our approaches can ensure that the predictive models built from the modified data are not subject to any type of discrimination. The experiments using two real datasets show that our approaches are effective in discovering and removing discrimination.\n\nPreliminary Concepts",
      "referring_paragraphs": [
        "Wu, and Wu 2016b; 2016a) for improving the correlation based approaches. In this paper, we develop a framework for discovering and removing both direct and indirect discrimination based on the causal network. A causal network is a directed acyclic graph (DAG) widely used for causal representation, reasoning and inference (Pearl 2009), where causal effects are carried by the paths that trace arrows pointing from the cause to the effect which are referred to as the causal paths. Using this model, ",
        "The physical meaning of $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } )$ is the expected π ,change in decisions (in term of the probability of $E = e ^ { + }$ ) of individuals from protected group $c ^ { - }$ , if it is told that these individuals were from the other group $c ^ { + }$ and everything else remains unchanged. When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is in",
        "represents the expected change in decisions of individuals from protected group $c ^ { - }$ , if the profiles of these individuals along path $\\pi _ { i }$ were changed as if they were from the other group $c ^ { + }$ π. When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group. Thus, the $\\pi _ { i }$ -specific effect is πappropriate for measuring the discrimin",
        "χThe results are shown in Table 1. As shown in the column “PSE-DD”, both the modified training data and the predictions for the testing data contain no direct and indirect discrimination. In addition, PSE-DD produces relatively small data utility loss in term of $\\chi ^ { 2 }$ and good prediction accuχracy. For comparison, we include algorithms from previous works: LMSG, LPS and DI. For LMSG and LPS, discrimination is not removed even from the training data, and hence also exists in the predicti",
        "Figure 1: The toy model.",
        "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).",
        "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.",
        "χThe results are shown in Table 1.",
        "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07509_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg",
      "image_filename": "1611.07509_page0_fig1.jpg",
      "caption": "Figure 2: An example with the recanting witness criterion satisfied.",
      "context_before": "The path-specific effect is an extension to the total causal effect in the sense that the effect of the intervention is transmitted only along a subset of causal paths from $X$ to Y. Denote a subset of causal paths by $\\pi$ . The $\\pi$ -specific effect conπ πsiders a counterfactual situation where the effect of $X$ on $Y$ with the intervention is transmitted along $\\pi$ , while the effect of $X$ πon Y without the intervention is transmitted along paths not in $\\pi$ . We denote by $P ( y \\mid d o ( x _ { 2 } | \\pi ) )$ the distribution of $Y$ πafter an intervention of changing $X$ πfrom $x _ { 1 }$ to $x _ { 2 }$ with the effect transmitted along $\\pi$ . Then, the $\\pi$ -specific effect of $X$ π πon Y is defined as follows (Avin, Shpitser, and Pearl 2005).\n\nDefinition 2 (Path-specific effect) Given a path set $\\pi$ , the $\\pi$ -specific effect of the value change of $X$ from $x _ { 1 }$ to $x _ { 2 }$ on $Y = y$ is given by\n\n$$ S E _ {\\pi} \\left(x _ {2}, x _ {1}\\right) = P (y \\mid d o \\left(x _ {2} \\mid_ {\\pi}\\right)) - P (y \\mid d o \\left(x _ {1}\\right)). $$",
      "context_after": "The authors in (Avin, Shpitser, and Pearl 2005) have given the condition under which the path-specific effect can be estimated from the observational data, known as the recanting witness criterion.\n\nDefinition 3 (Recanting witness criterion) Given a path set , let Z be a node in $\\mathcal { G }$ such that: 1) there exists a path πfrom X to Z which is a segment of a path in ; 2) there exπists a path from Z to Y which is a segment of a path in ; 3) πthere exists another path from Z to Y which is not a segment of any path in . Then, the recanting witness criterion for the $\\pi$ π-specific effect is satisfied with Z as a witness.\n\nTheorem 1 (Identifiability) The -specific effect can be esπtimated from the observational data if and only if the recanting witness criterion for the -specific effect is not satisfied.",
      "referring_paragraphs": [
        "Note that the above computation requires $\\mathbf { S } _ { \\pi } \\cap \\bar { \\mathbf { S } } _ { \\pi } = \\varnothing$ . Theorem 1 is reflected in that: $\\mathbf { S } _ { \\pi } \\cap \\bar { \\mathbf { S } } _ { \\pi } \\overset { . } { \\neq } \\varnothing$ π π if and only if the πrecanting witness criterion for the $\\pi$ π-specific effect is satisπfied. Figure 2 shows an example with the recanting witness criterion satisfied, where $\\pi = \\{ X \\to Z _ { 1 } \\to Z _ { 2 } \\to Y \\}$ . According to the",
        "Figure 2: An example with the recanting witness criterion satisfied."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07509_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig2.jpg",
      "image_filename": "1611.07509_page0_fig2.jpg",
      "caption": "Experiments",
      "context_before": "The computational complexity of PSE-DR depends on the complexity of solving the quadratic programming problem. It can be easily shown that, the coefficients of the quadratic terms in the objective function form a positive definite matrix. According to (Kozlov, Tarasov, and Khachiyan 1980), the quadratic programming can be solved in polynomial time. Finally, it is also worth noting that our approach can be easily extended to handle the situation where either direct or indirect discrimination needs to be removed.\n\nDealing with Unidentifiable Situation\n\nAs stated in Theorem 1, when the recanting witness criterion is satisfied, the $\\pi _ { i }$ -specific effect cannot be estimated from πthe observational data. However, the structure of the recanting witness criterion implies indirect discrimination as there exist causal paths from $C$ to $E$ passing through the redlining attributes. From the data owners’ perspective, they may want to ensure non-discrimination even though the discriminatory effect cannot be accurately measured. In this case, we remove discrimination by adapting Algorithm 2 as follows. Recall that $\\mathbf { S } _ { \\pi _ { i } } \\cap \\bar { \\mathbf { S } } _ { \\pi _ { i } } \\neq \\boldsymbol { \\varnothing }$ if and only if the recanting witness π πcriterion is satisfied. For each node $S \\in { \\bf S } _ { \\pi _ { i } } \\cap \\bar { \\bf S } _ { \\pi _ { i } }$ , we cut off all the causal paths from S to $E$ π πthat pass through R, so that S would not belong to $\\mathbf { S } _ { \\pi _ { i } }$ any more. Then, we must have $\\mathbf { S } _ { \\pi _ { i } } \\cap \\bar { \\mathbf { S } } _ { \\pi _ { i } } = \\varnothing$ πafter the modification. To cut off the paths, we π πfocus on the arc from $E$ ’s each parent $Q$ , i.e., $Q E$ . If these exists a path from S to $Q$ passing through R, then arc $Q E$ is removed from the network. The pseudo-code of this procedure is shown below, which can be added before line 1 in Algorithm 2 to deal with this situation.",
      "context_after": "In this section, we conduct experiments using two real datasets: the Adult dataset (Lichman 2013) and the Dutch Census of 2001 (Netherlands 2001). We compare our algorithms with the local massaging (LMSG) and local preferential sampling (LPS) algorithms proposed in (Zliobaitˇ e,˙ Kamiran, and Calders 2011) and disparate impact removal algorithm (DI) proposed in (Feldman et al. 2015; Adler et al. 2016). The causal networks are constructed and presented by utilizing an open-source software TETRAD (Glymour and others 2004). We employ the original PC algorithm (Spirtes, Glymour, and Scheines 2000) and set the significance threshold 0 01 for conditional independence testing in .causal network construction. The quadratic programming is solved using CVXOPT (Dahl and Vandenberghe 2006).",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07509_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg",
      "image_filename": "1611.07509_page0_fig3.jpg",
      "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
      "context_before": "In this section, we conduct experiments using two real datasets: the Adult dataset (Lichman 2013) and the Dutch Census of 2001 (Netherlands 2001). We compare our algorithms with the local massaging (LMSG) and local preferential sampling (LPS) algorithms proposed in (Zliobaitˇ e,˙ Kamiran, and Calders 2011) and disparate impact removal algorithm (DI) proposed in (Feldman et al. 2015; Adler et al. 2016). The causal networks are constructed and presented by utilizing an open-source software TETRAD (Glymour and others 2004). We employ the original PC algorithm (Spirtes, Glymour, and Scheines 2000) and set the significance threshold 0 01 for conditional independence testing in .causal network construction. The quadratic programming is solved using CVXOPT (Dahl and Vandenberghe 2006).",
      "context_after": "Discrimination Discovery\n\nThe Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc. Since the computational complexity of the PC algorithm is an exponential function of the number of attributes and their domain sizes, for computational feasibility we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the first tier, edu level and marital status are defined in the second tier, and all other attributes are defined in the third tier. The causal network is shown in Figure 3. We treat sex as the protected attribute, income as the decision, and marital status as the redlining attribute. The green path represents the direct path from sex to income, and the blue paths represent the indirect paths passing through marital status. We set the discrimination threshold $\\tau$ as τ0.05. By computing the path-specific effects, we obtain that $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) = 0 . 0 2 5$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) = 0 . 1 7 5$ , which indiπ , . π , .cate no direct discrimination but significant indirect discrimination against females according to our criterion.\n\nThe Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the decision, and marital status as the redlining attribute. For this dataset, $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) =$ 0 220 and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 0 1$ π ,, indicating significant di-. π , .rect discrimination but no indirect discrimination against females.",
      "referring_paragraphs": [
        "The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc. Since the computational complexity of the PC algorithm is an exponential function of the number of attributes and their domain sizes, for computational feasibility we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the firs",
        "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
        "The causal network is shown in Figure 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07509_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg",
      "image_filename": "1611.07509_page0_fig4.jpg",
      "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
      "context_before": "We run the removal algorithm PSE-DR to remove discrimination from the Adult and Dutch datasets. Then, we run the discovery algorithm PSE-DD to further examine whether discrimination is truly removed in the modified datasets. For the modified Adult dataset we have $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) = 0 . 0 1 3$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 4 9$ π , ., and for the modified Dutch π ,dataset we have $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 5 0$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) =$ π , . π ,0 001. The results show that the modified datasets contain .no direct and indirect discrimination.\n\nDiscrimination in predictive models. We aim to examine whether the predictive models built from the modified dataset incur discrimination in decision making. We use the Adult dataset where indirect discrimination is detected, and divide the original dataset into the training and testing datasets. First, we remove discrimination from the training dataset to obtain the modified training dataset. Then, we build the predictive models from the modified training dataset, and use them to make predictive decisions over the testing data. Two classifiers, SVM and Decision Tree, are used for prediction with five-fold cross-validation. Finally, we run PSE-DD to examine whether the predictions for the testing data contain discrimination. We also examine the data utility $( \\chi ^ { 2 } )$ and the prediction accuracy.\n\nχThe results are shown in Table 1. As shown in the column “PSE-DD”, both the modified training data and the predictions for the testing data contain no direct and indirect discrimination. In addition, PSE-DD produces relatively small data utility loss in term of $\\chi ^ { 2 }$ and good prediction accuχracy. For comparison, we include algorithms from previous works: LMSG, LPS and DI. For LMSG and LPS, discrimination is not removed even from the training data, and hence also exists in the predictions. The $D I$ algorithm provides a parameter to indicate the amount of discrimination to be λremoved, where $\\lambda = 0$ represents no modification and $\\lambda = 1$",
      "context_after": "",
      "referring_paragraphs": [
        "The Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the deci",
        "The causal graph is shown in Figure 4.",
        "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07509_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_6",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/fdd1004c513835da188353f3f94f92476f68856a2ce4d146ecfe558bdc171fea.jpg",
      "image_filename": "fdd1004c513835da188353f3f94f92476f68856a2ce4d146ecfe558bdc171fea.jpg",
      "caption": "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data. Values violating the discrimination criterion are marked in bold.",
      "context_before": "",
      "context_after": "represents full discrimination removal. However, has no λdirect connection with the threshold . In our experiments, we execute $D I$ τmultiple times with different s and report the one that is closest to achieve $\\tau \\ : = \\ : 0 . 0 5$ λ. As shown in τ .the column “DI”, it indeed removes direct and indirect discrimination from the training data. However, as indicated by the bold values 0.167/0.168, significant amount of indirect discrimination exists in the predictions of both classifiers. In addition, its data utility is far more worse than PSE-DR, implying that it removes many information unrelated to discrimination.\n\nA number of techniques have been proposed to discover discrimination in the literature. Classification rule-based methods such as elift (Pedreshi, Ruggieri, and Turini 2008) and belift (Mancuhan and Clifton 2014) were proposed to represent certain discrimination patterns. (Luong, Ruggieri, and\n\nTurini 2011; Zhang, Wu, and Wu 2016b) dealt with the individual discrimination by finding a group of similar individuals. (Zliobait ˇ e, Kamiran, and Calders 2011) proposed ˙ conditional discrimination which considers some part of the discrimination may be explainable by certain attributes. None of these work explicitly identifies direct discrimination, indirect discrimination, and explainable effects. In (Bonchi et al. 2015), the authors proposed a framework based on the Suppes-Bayes causal network and developed several random-walk-based methods to detect different types of discrimination. However, the construction of the Suppes-Bayes causal network is impractical with the large number of attribute-value pairs. In addition, it is unclear how the number of random walks is related to practical discrimination metrics, e.g., the difference in positive decision rates.",
      "referring_paragraphs": [
        "Wu, and Wu 2016b; 2016a) for improving the correlation based approaches. In this paper, we develop a framework for discovering and removing both direct and indirect discrimination based on the causal network. A causal network is a directed acyclic graph (DAG) widely used for causal representation, reasoning and inference (Pearl 2009), where causal effects are carried by the paths that trace arrows pointing from the cause to the effect which are referred to as the causal paths. Using this model, ",
        "The physical meaning of $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } )$ is the expected π ,change in decisions (in term of the probability of $E = e ^ { + }$ ) of individuals from protected group $c ^ { - }$ , if it is told that these individuals were from the other group $c ^ { + }$ and everything else remains unchanged. When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is in",
        "represents the expected change in decisions of individuals from protected group $c ^ { - }$ , if the profiles of these individuals along path $\\pi _ { i }$ were changed as if they were from the other group $c ^ { + }$ π. When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group. Thus, the $\\pi _ { i }$ -specific effect is πappropriate for measuring the discrimin",
        "χThe results are shown in Table 1. As shown in the column “PSE-DD”, both the modified training data and the predictions for the testing data contain no direct and indirect discrimination. In addition, PSE-DD produces relatively small data utility loss in term of $\\chi ^ { 2 }$ and good prediction accuχracy. For comparison, we include algorithms from previous works: LMSG, LPS and DI. For LMSG and LPS, discrimination is not removed even from the training data, and hence also exists in the predicti",
        "Figure 1: The toy model.",
        "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).",
        "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.",
        "χThe results are shown in Table 1.",
        "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1611.07509_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1701.08230": [
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1f4572e64663f787874618af9f17e4c4996cb864282415c743c3aab762053005.jpg",
      "image_filename": "1f4572e64663f787874618af9f17e4c4996cb864282415c743c3aab762053005.jpg",
      "caption": "Table 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety. For each fairness constraint, we estimate the increase in violent crime committed by released defendants, relative to a rule that optimizes for public safety alone; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety).",
      "context_before": "We use data from Broward County, Florida originally compiled by ProPublica [30]. Following their analysis, we only consider black and white defendants who were assigned COMPAS risk scores within 30 days of their arrest, and were not arrested for an ordinary trac crime. We further restrict to only those defendants who spent at least two years (aer their COMPAS evaluation) outside a correctional facility without being arrested for a violent crime, or were arrested for a violent crime within this two-year period. Following standard practice, we use this two-year violent recidivism metric to approximate the benet $y _ { i }$ of detention: we set $y _ { i } = 1$ for those who reoended, and $y _ { i } = 0$ i i for those who did not. For the yi3,377 defendants satisfying these criteria, the dataset includes race, age, sex, number of prior convictions, and COMPAS violent crime risk score (a discrete score between 1 and 10).\n\ne COMPAS scores may not be the most accurate estimates of risk, both because the scores are discretized and because they are not trained specically for Broward County. erefore, to estimate ${ p / X | X }$ we re-train a risk assessment model that predicts two-year Y Xviolent recidivism using $L ^ { 1 }$ -regularized logistic regression followed Lby Pla scaling [35]. e model is based on all available features for each defendant, excluding race. Our risk scores achieve higher AUC on a held-out set of defendants than the COMPAS scores (0.75 vs. 0.73). We note that adding race to this model does not improve performance, as measured by AUC on the test set.\n\nWe investigate the three past fairness denitions previously discussed: statistical parity, conditional statistical parity, and predictive equality. For each denition, we nd the set of thresholds that produce a decision rule that: (1) satises the fairness denition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public",
      "context_after": "safety subject to (1) and (2). e proportion of defendants detained is chosen to match the fraction of defendants classied as medium or high risk by COMPAS (scoring 5 or greater). Conditional statistical parity requires that one dene the “legitimate” factors $\\ell ( X )$ , and ` Xthis choice signicantly impacts results. For example, if all variables are deemed legitimate, then this fairness condition imposes no constraint on the algorithm. In our application, we consider only a defendant’s number of prior convictions to be legitimate; to deal with sparsity in the data, we partition prior convictions into four bins: 0, 1–2, 3–4, and 5 or more.\n\nWe estimate two quantities for each decision rule: the increase in violent crime commied by released defendants, relative to a rule that optimizes for public safety alone, ignoring formal fairness requirements; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety). We compute these numbers on 100 random train-test splits of the data. On each iteration, we train the risk score model and nd the optimal thresholds using $7 0 \\%$ of the data, and then calculate the two statistics on the remaining $3 0 \\%$ . Ties are broken randomly when they occur, and we report results averaged over all runs.\n\nFor each fairness constraint, Table 1 shows that violent recidivism increases while low risk defendants are detained. For example, when we enforce statistical parity, $1 7 \\%$ of detained defendants are relatively low risk. An equal number of high-risk defendants are thus released (because we hold xed the number of individuals detained), leading to an estimated $9 \\%$ increase in violent recidivism among released defendants. ere are thus tangible costs to satisfying popular notions of algorithmic fairness.",
      "referring_paragraphs": [
        "For each fairness constraint, Table 1 shows that violent recidivism increases while low risk defendants are detained. For example, when we enforce statistical parity, $1 7 \\%$ of detained defendants are relatively low risk. An equal number of high-risk defendants are thus released (because we hold xed the number of individuals detained), leading to an estimated $9 \\%$ increase in violent recidivism among released defendants. ere are thus tangible costs to satisfying popular notions of algorith",
        "e reason for these disparities is that white and black defendants in Broward County have dierent distributions of risk, $\\ p _ { Y \\vert X }$ , pY Xas shown in Figure 1. In particular, a greater fraction of black defendants have relatively high risk scores, in part because black defendants are more likely to have prior arrests, which is a strong indicator of reoending. Importantly, while an algorithm designer can choose dierent decision rules based on these risk scores, the algorithm cannot ",
        "Kleinberg et al. [29] establish the incompatibility of dierent fairness measures when the overall risk $\\operatorname* { P r } ( Y = 1 \\mid g ( X ) = g _ { i } )$ differs between groups $g _ { i }$ i. However, the tension we identify between imaximizing public safety and satisfying various notions of algorithmic fairness typically persists even if groups have the same overall risk. To demonstrate this phenomenon, Figure 1 shows risk score distributions for two hypothetical populations with equa",
        "34, 37]. In that work, taste-based discrimination [6] is equated with applying decision thresholds that dier by race. eir seing is human, not algorithmic, decision making, and so one cannot directly observe the thresholds being applied; the goal is thus to infer the thresholds from observable statistics. ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so c",
        "For each denition, we nd the set of thresholds that produce a decision rule that: (1) satises the fairness denition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public\n\nTable 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety.",
        "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right).",
        "ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical (as shown in Figure 1)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig0.jpg",
      "image_filename": "1701.08230_page0_fig0.jpg",
      "caption": "9One can construct examples in which the group-specic thresholds coincide, leading to a single threshold, but it is unlikely for the thresholds to be exactly equal in practice.",
      "context_before": "5 THE COST OF PUBLIC SAFETY\n\nA decision rule constrained to satisfy statistical parity, conditional statistical parity, or predictive equality reduces public safety. However, a single-threshold rule that maximizes public safety generally violates all of these fairness denitions. For example, in the Broward County data, optimally detaining $3 0 \\%$ of defendants with a singlethreshold rule means that $4 0 \\%$ of black defendants are detained, compared to $1 8 \\%$ of white defendants, violating statistical parity. And among defendants who ultimately do not go on to commit a violent crime, $1 4 \\%$ of whites are detained compared to $3 2 \\%$ of blacks, violating predictive equality.\n\n9One can construct examples in which the group-specic thresholds coincide, leading to a single threshold, but it is unlikely for the thresholds to be exactly equal in practice. We discuss this possibility further in Section 5.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
      "image_filename": "1701.08230_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig2.jpg",
      "image_filename": "1701.08230_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_5",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig3.jpg",
      "image_filename": "1701.08230_page0_fig3.jpg",
      "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le ), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
      "context_before": "",
      "context_after": "e reason for these disparities is that white and black defendants in Broward County have dierent distributions of risk, $\\ p _ { Y \\vert X }$ , pY Xas shown in Figure 1. In particular, a greater fraction of black defendants have relatively high risk scores, in part because black defendants are more likely to have prior arrests, which is a strong indicator of reoending. Importantly, while an algorithm designer can choose dierent decision rules based on these risk scores, the algorithm cannot alter the risk scores themselves, which reect underlying features of the population of Broward County.\n\nOnce a decision threshold is specied, these risk distributions determine the statistical properties of the decision rule, including the group-specic detention and false positive rates. In theory, it is possible that these distributions line up in a way that achieves statistical parity or predictive equality, but in practice that is unlikely. Consequently, any decision rule that guarantees these various fairness criteria are met will in practice deviate from the unconstrained optimum.\n\nKleinberg et al. [29] establish the incompatibility of dierent fairness measures when the overall risk $\\operatorname* { P r } ( Y = 1 \\mid g ( X ) = g _ { i } )$ differs between groups $g _ { i }$ i. However, the tension we identify between imaximizing public safety and satisfying various notions of algorithmic fairness typically persists even if groups have the same overall risk. To demonstrate this phenomenon, Figure 1 shows risk score distributions for two hypothetical populations with equal average risk. Even though their means are the same, the tail of the red distribution is heavier than the tail of the blue distribution, resulting in higher detention and false positive rates in the red group.",
      "referring_paragraphs": [
        "For each fairness constraint, Table 1 shows that violent recidivism increases while low risk defendants are detained. For example, when we enforce statistical parity, $1 7 \\%$ of detained defendants are relatively low risk. An equal number of high-risk defendants are thus released (because we hold xed the number of individuals detained), leading to an estimated $9 \\%$ increase in violent recidivism among released defendants. ere are thus tangible costs to satisfying popular notions of algorith",
        "e reason for these disparities is that white and black defendants in Broward County have dierent distributions of risk, $\\ p _ { Y \\vert X }$ , pY Xas shown in Figure 1. In particular, a greater fraction of black defendants have relatively high risk scores, in part because black defendants are more likely to have prior arrests, which is a strong indicator of reoending. Importantly, while an algorithm designer can choose dierent decision rules based on these risk scores, the algorithm cannot ",
        "Kleinberg et al. [29] establish the incompatibility of dierent fairness measures when the overall risk $\\operatorname* { P r } ( Y = 1 \\mid g ( X ) = g _ { i } )$ differs between groups $g _ { i }$ i. However, the tension we identify between imaximizing public safety and satisfying various notions of algorithmic fairness typically persists even if groups have the same overall risk. To demonstrate this phenomenon, Figure 1 shows risk score distributions for two hypothetical populations with equa",
        "34, 37]. In that work, taste-based discrimination [6] is equated with applying decision thresholds that dier by race. eir seing is human, not algorithmic, decision making, and so one cannot directly observe the thresholds being applied; the goal is thus to infer the thresholds from observable statistics. ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so c",
        "For each denition, we nd the set of thresholds that produce a decision rule that: (1) satises the fairness denition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public\n\nTable 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety.",
        "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right).",
        "ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical (as shown in Figure 1)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg",
      "image_filename": "1701.08230_page0_fig4.jpg",
      "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.",
      "context_before": "Once a decision threshold is specied, these risk distributions determine the statistical properties of the decision rule, including the group-specic detention and false positive rates. In theory, it is possible that these distributions line up in a way that achieves statistical parity or predictive equality, but in practice that is unlikely. Consequently, any decision rule that guarantees these various fairness criteria are met will in practice deviate from the unconstrained optimum.\n\nKleinberg et al. [29] establish the incompatibility of dierent fairness measures when the overall risk $\\operatorname* { P r } ( Y = 1 \\mid g ( X ) = g _ { i } )$ differs between groups $g _ { i }$ i. However, the tension we identify between imaximizing public safety and satisfying various notions of algorithmic fairness typically persists even if groups have the same overall risk. To demonstrate this phenomenon, Figure 1 shows risk score distributions for two hypothetical populations with equal average risk. Even though their means are the same, the tail of the red distribution is heavier than the tail of the blue distribution, resulting in higher detention and false positive rates in the red group.\n\nat a single decision threshold can, and generally does, result in racial disparities is closely related to the notion of infra-marginality in the econometric literature on taste-based discrimination [3, 4,",
      "context_after": "34, 37]. In that work, taste-based discrimination [6] is equated with applying decision thresholds that dier by race. eir seing is human, not algorithmic, decision making, and so one cannot directly observe the thresholds being applied; the goal is thus to infer the thresholds from observable statistics. ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical (as shown in Figure 1). In the algorithmic seing, past fairness measures notably focus on these infra-marginal statistics, even though the thresholds themselves are directly observable.\n\n6 DETECTING DISCRIMINATION\n\ne algorithms we have thus far considered output a decision $d ( x )$ d xfor each individual. In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant’s risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).",
      "referring_paragraphs": [
        "In some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory. To address this concern, researchers oen examine whether scores are calibrated [29], as dened by Eq. (4).10 Since the true probabilities ${ p / X | X }$ are necessarily calipY Xbrated, it is reasonable to expect risk estimates that approximate these probabilities to be calibrated as well. Figure 2 shows that the COMPAS scores indeed satisf",
        "Figure 2: Recidivism rate by COMPAS risk score and race.",
        "Figure 2 shows that the COMPAS scores indeed satisfy this property."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg",
      "image_filename": "1701.08230_page0_fig5.jpg",
      "caption": "Maximizing public safety requires detaining all individuals deemed suciently likely to commit a violent crime, regardless of race.",
      "context_before": "ese examples relate to the historical practice of redlining, in which lending decisions were intentionally based only on coarse information—usually neighborhood—in order to deny loans to wellqualied minorities [11]. Since even creditworthy minorities oen resided in neighborhoods with low average income, lenders could deny their applications by adhering to a facially neutral policy of not serving low-income areas. In the case of redlining, one discriminates by ignoring information about the disfavored group; in the pretrial seing, one ignores information about the favored group. Both strategies, however, operate under the same general principle.\n\nere is no evidence to suggest that organizations have intentionally ignored relevant information when constructing risk scores. Similar eects, however, may also arise through negligence or unintentional oversights. Indeed, we found in Section 4 that we could improve the predictive power of the Broward County COMPAS scores with a standard statistical model. To ensure an algorithm is equitable, it is thus important to inspect the algorithm itself and not just the decisions it produces.\n\nMaximizing public safety requires detaining all individuals deemed suciently likely to commit a violent crime, regardless of race.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig6.jpg",
      "image_filename": "1701.08230_page0_fig6.jpg",
      "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).",
      "context_before": "",
      "context_after": "However, to satisfy common metrics of fairness, one must set multiple, race-specic thresholds. ere is thus an inherent tension between minimizing expected violent crime and satisfying common notions of fairness. is tension is real: by analyzing data from Broward County, we nd that optimizing for public safety yields stark racial disparities; conversely, satisfying past fairness denitions means releasing more high-risk defendants, adversely aecting public safety.\n\nPolicymakers face a dicult and consequential choice, and it is ultimately unclear what course of action is best in any given situation. We note, however, one important consideration: with race-specic thresholds, a black defendant may be released while an equally risky white defendant is detained. Such racial classications would likely trigger strict scrutiny [18], the most stringent standard of judicial review used by U.S. courts under the Equal Protection Clause of the Fourteenth Amendment. A single-threshold rule thus maximizes public safety while satisfying a core constitutional law rule, bolstering the case in its favor.\n\nTo some extent, concerns embodied by past fairness denitions can be addressed while still adopting a single-threshold rule. For example, by collecting more data and accordingly increasing the accuracy of risk estimates, one can lower error rates. Further, one could raise the threshold for detaining defendants, reducing the number of people erroneously detained from all race groups. Finally, one could change the decision such that classication errors are less costly. For example, rather than being held in jail, risky defendants might be required to participate in community supervision programs.",
      "referring_paragraphs": [
        "Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates. We start by adding noise to the true scores (black curve) of the group that we wish to treat favorably—in the gure we use $\\mathrm { N } ( 0 , 0 . 5 )$ noise. We then , .use the perturbed scores to predict the outcomes $y _ { i }$ via a logistic yiregression model. e resulting model predictions (red curve) are more tightly clustered around their mean, since adding noise removes informati",
        "Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates.",
        "Figure 3: Calibration is insucient to assess discrimination."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1701.08230_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1703.06856": [
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig0.jpg",
      "image_filename": "1703.06856_page0_fig0.jpg",
      "caption": "(a)",
      "context_before": "for all y and for any value $a ^ { \\prime }$ attainable by $A$ .\n\nThis notion is closely related to actual causes [13], or token causality in the sense that, to be fair, $A$ should not be a cause of $\\hat { Y }$ in any individual instance. In other words, changing $A$ while holding things which are not causally dependent on $A$ constant will not change the distribution of $\\hat { Y }$ . We also emphasize that counterfactual fairness is an individual-level definition. This is substantially different from comparing different individuals that happen to share the same “treatment” $A = a$ and coincide on the values of $X$ , as discussed in Section 4.3.1 of [29] and the Supplementary Material. Differences between $X _ { a }$ and $X _ { a ^ { \\prime } }$ must be caused by variations on $A$ only. Notice also that this definition is agnostic with respect to how good a predictor $\\hat { Y }$ is, which we discuss in Section 4.\n\nRelation to individual fairness. IF is agnostic with respect to its notion of similarity metric, which is both a strength (generality) and a weakness (no unified way of defining similarity). Counterfactuals and similarities are related, as in the classical notion of distances between “worlds” corresponding to different counterfactuals [23]. If $\\hat { Y }$ is a deterministic function of $W \\subset A \\cup X \\cup U$ , as in several of",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg",
      "image_filename": "1703.06856_page0_fig1.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig2.jpg",
      "image_filename": "1703.06856_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig3.jpg",
      "image_filename": "1703.06856_page0_fig3.jpg",
      "caption": "(e) Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
      "context_before": "",
      "context_after": "our examples to follow, then IF can be defined by treating equally two individuals with the same $W$ in a way that is also counterfactually fair.\n\nRelation to Pearl et al. [29]. In Example 4.4.4 of [29], the authors condition instead on $X , A$ , and the observed realization of $\\hat { Y }$ , and calculate the probability of the counterfactual realization $\\hat { Y } _ { A a ^ { \\prime } }$ differing from the factual. This example conflates the predictor $\\hat { Y }$ with the outcome $Y$ , of which we remain agnostic in our definition but which is used in the construction of $\\hat { Y }$ as in Section 4. Our framing makes the connection to machine learning more explicit.\n\nTo provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of these examples with more detailed insights.",
      "referring_paragraphs": [
        "To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of these examples with more detailed insights.",
        "Scenario 1: The Red Car. A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$ . They assume there is an unobserved factor corresponding to aggressive driving $U$ , that (a) causes drivers to be more likely have an accident, and (b) causes individuals to prefer red cars (the observed variable $X$ ). Moreover, individuals belonging to a certain race $A$ are more likely to drive red cars. However, these individuals are no more likely to be aggressiv",
        "Scenario 2: High Crime Regions. A city government wants to estimate crime rates by neighborhood to allocate policing resources. Its analyst constructed training data by merging (1) a registry of residents containing their neighborhood $X$ and race $A$ , with (2) police records of arrests, giving each resident a binary label with $Y = 1$ indicating a criminal arrest record. Due to historically segregated housing, the location $X$ depends on $A$ . Locations $X$ with more police resources have larg",
        "Dealing with historical biases and an existing fairness paradox. The explicit difference between $\\hat { Y }$ and $Y$ allows us to tackle historical biases. For instance, let $Y$ be an indicator of whether a client defaults on a loan, while $\\hat { Y }$ is the actual decision of giving the loan. Consider the DAG $A Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables. $Y$ is the objectively ideal measure for decision making, the binary indicator of",
        "Accuracy. We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1. The Full model achieves the lowest RMSE as it uses race and sex to more accurately reconstruct FYA. Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3. The (also unfair) Unaware model still uses the unfair variables GPA and LSAT, but because it does not use race and sex it cannot match th",
        "Lemma 3. Consider a linear model with the structure in Figure 1(a). Fitting a linear predictor to $X$ only is not counterfactually fair, while the same algorithm will produce a fair predictor using both $A$ and $X$ .",
        "Proof. As in the definition, we will consider the population case, where the joint distribution is known. Consider the case where the equations described by the model in Figure 1(a) are deterministic and linear:",
        "Note that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .",
        "The above lemma holds in a more general case for the structure given in Figure 1(a): any non-constant estimator that depends only on $X$ is not counterfactually fair as changing $A$ always alters $X$ .",
        "Scenario 3: University Success. A university wants to know if students will be successful postgraduation $Y$ . They have information such as: grade point average (GPA), advanced placement (AP) exams results, and other academic features $X$ . The university believes however, that an individual’s gender $A$ may influence these features and their post-graduation success $Y$ due to social discrimination. They also believe that independently, an individual’s latent talent $U$ causes $X$ and $Y$ . The",
        "As discussed by [27], there are some counterfactual manipulations usable in a causal definition of fairness that can be performed by exploiting only independence constraints among the counterfactuals: that is, without requiring the explicit description of structural equations or other models for latent variables. A contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figur",
        "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios.",
        "Each of these correspond to one of the two causal graphs in Figure 1(a),(b).",
        "Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.",
        "This model is shown\n\nTable 1: Prediction results using logistic regression.",
        "We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1.",
        "Consider a linear model with the structure in Figure 1(a).",
        "□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .",
        "A contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figure 1(d), where many of the mediators themselves are considered to be unfairly affected by the protected attribute, and independence constraints among counterfactuals alone are less likely to be useful in identifying constraints for the fitting of a fair predictor."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig4.jpg",
      "image_filename": "1703.06856_page0_fig4.jpg",
      "caption": "Level 2",
      "context_before": "$$ \\mathrm {F Y A} \\sim \\mathcal {N} \\left(w _ {F} ^ {K} K + w _ {F} ^ {R} R + w _ {F} ^ {S} S, 1\\right), $$\n\n$$ \\mathrm {K} \\sim \\mathcal {N} (0, 1) $$\n\nWe perform inference on this model using an observed training set to estimate the posterior distribution of $K$ . We use the probabilistic programming language Stan [34] to learn $K$ . We call the predictor constructed using $K$ , Fair $K$ .",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig5.jpg",
      "image_filename": "1703.06856_page0_fig5.jpg",
      "caption": "Level 3",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_7",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg",
      "image_filename": "1703.06856_page0_fig6.jpg",
      "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .",
      "context_before": "",
      "context_after": "In Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex (that may in turn be correlated with one-another). This model is shown",
      "referring_paragraphs": [
        "In Level 2, we postulate that a latent variable: a student’s knowledge (K), affects GPA, LSAT, and FYA scores. The causal graph corresponding to this model is shown in Figure 2, (Level 2). This is a short-hand for the distributions:",
        "in Figure 2, (Level 3), and is expressed by:",
        "Accuracy. We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1. The Full model achieves the lowest RMSE as it uses race and sex to more accurately reconstruct FYA. Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3. The (also unfair) Unaware model still uses the unfair variables GPA and LSAT, but because it does not use race and sex it cannot match th",
        "Counterfactual fairness. We would like to empirically test whether the baseline methods are counterfactually fair. To do so we will assume the true model of the world is given by Figure 2, (Level 2). We can fit the parameters of this model using the observed data and evaluate counterfactual fairness by sampling from it. Specifically, we will generate samples from the model given either the observed race and sex, or counterfactual race and sex variables. We will fit models to both the original an",
        "The causal graph corresponding to this model is shown in Figure 2, (Level 2).",
        "Figure 2: Left: A causal model for the problem of predicting law school success fairly.",
        "Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_8",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/08fd2af4f3f4e209bae332b4b0dae6871b3687d4b71fedd1b9f90aaf83211b6f.jpg",
      "image_filename": "08fd2af4f3f4e209bae332b4b0dae6871b3687d4b71fedd1b9f90aaf83211b6f.jpg",
      "caption": "Table 1: Prediction results using logistic regression. Note that we must sacrifice a small amount of accuracy to ensuring counterfactually fair prediction (Fair $K$ , Fair Add), versus the models that use unfair features: GPA, LSAT, race, sex (Full, Unaware).",
      "context_before": "In Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex (that may in turn be correlated with one-another). This model is shown",
      "context_after": "in Figure 2, (Level 3), and is expressed by:\n\n$$ \\mathbf {G P A} = b _ {G} + w _ {G} ^ {R} R + w _ {G} ^ {S} S + \\epsilon_ {G}, \\epsilon_ {G} \\sim p (\\epsilon_ {G}) $$\n\n$$ \\mathrm {L S A T} = b _ {L} + w _ {L} ^ {R} R + w _ {L} ^ {S} S + \\epsilon_ {L}, \\quad \\epsilon_ {L} \\sim p (\\epsilon_ {L}) $$",
      "referring_paragraphs": [
        "To provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of these examples with more detailed insights.",
        "Scenario 1: The Red Car. A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$ . They assume there is an unobserved factor corresponding to aggressive driving $U$ , that (a) causes drivers to be more likely have an accident, and (b) causes individuals to prefer red cars (the observed variable $X$ ). Moreover, individuals belonging to a certain race $A$ are more likely to drive red cars. However, these individuals are no more likely to be aggressiv",
        "Scenario 2: High Crime Regions. A city government wants to estimate crime rates by neighborhood to allocate policing resources. Its analyst constructed training data by merging (1) a registry of residents containing their neighborhood $X$ and race $A$ , with (2) police records of arrests, giving each resident a binary label with $Y = 1$ indicating a criminal arrest record. Due to historically segregated housing, the location $X$ depends on $A$ . Locations $X$ with more police resources have larg",
        "Dealing with historical biases and an existing fairness paradox. The explicit difference between $\\hat { Y }$ and $Y$ allows us to tackle historical biases. For instance, let $Y$ be an indicator of whether a client defaults on a loan, while $\\hat { Y }$ is the actual decision of giving the loan. Consider the DAG $A Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables. $Y$ is the objectively ideal measure for decision making, the binary indicator of",
        "Accuracy. We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1. The Full model achieves the lowest RMSE as it uses race and sex to more accurately reconstruct FYA. Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3. The (also unfair) Unaware model still uses the unfair variables GPA and LSAT, but because it does not use race and sex it cannot match th",
        "Lemma 3. Consider a linear model with the structure in Figure 1(a). Fitting a linear predictor to $X$ only is not counterfactually fair, while the same algorithm will produce a fair predictor using both $A$ and $X$ .",
        "Proof. As in the definition, we will consider the population case, where the joint distribution is known. Consider the case where the equations described by the model in Figure 1(a) are deterministic and linear:",
        "Note that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .",
        "The above lemma holds in a more general case for the structure given in Figure 1(a): any non-constant estimator that depends only on $X$ is not counterfactually fair as changing $A$ always alters $X$ .",
        "Scenario 3: University Success. A university wants to know if students will be successful postgraduation $Y$ . They have information such as: grade point average (GPA), advanced placement (AP) exams results, and other academic features $X$ . The university believes however, that an individual’s gender $A$ may influence these features and their post-graduation success $Y$ due to social discrimination. They also believe that independently, an individual’s latent talent $U$ causes $X$ and $Y$ . The",
        "As discussed by [27], there are some counterfactual manipulations usable in a causal definition of fairness that can be performed by exploiting only independence constraints among the counterfactuals: that is, without requiring the explicit description of structural equations or other models for latent variables. A contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figur",
        "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios.",
        "Each of these correspond to one of the two causal graphs in Figure 1(a),(b).",
        "Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.",
        "This model is shown\n\nTable 1: Prediction results using logistic regression.",
        "We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1.",
        "Consider a linear model with the structure in Figure 1(a).",
        "□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .",
        "A contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figure 1(d), where many of the mediators themselves are considered to be unfairly affected by the protected attribute, and independence constraints among counterfactuals alone are less likely to be useful in identifying constraints for the fitting of a fair predictor."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_9",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg",
      "image_filename": "1703.06856_page0_fig7.jpg",
      "caption": "Figure 3: A causal model for the stop and frisk dataset.",
      "context_before": "S5 The Multifaceted Dynamics of Fairness\n\nOne particularly interesting question was raised by one of the reviewers: what is the effect of continuing discrimination after fair decisions are made? For instance, consider the case where banks enforce a fair allocation of loans for business owners regardless of, say, gender. This does not mean such businesses will thrive at a balanced rate if customers continue to avoid female owned business at a disproportionate rate for unfair reasons. Is there anything useful that can be said about this issue from a causal perspective?\n\nThe work here proposed regards only what we can influence by changing how machine learningaided decision making takes place at specific problems. It cannot change directly how society as a whole carry on with their biases. Ironically, it may sound unfair to banks to enforce the allocation of resources to businesses at a rate that does not correspond to the probability of their respective success, even if the owners of the corresponding businesses are not to be blamed by that. One way of conciliating the different perspectives is by modeling how a fair allocation of loans, even if it does not come without a cost, can nevertheless increase the proportion of successful female businesses",
      "context_after": "compared to the current baseline. This change can by itself have an indirect effect on the culture and behavior of a society, leading to diminishing continuing discrimination by a feedback mechanism, as in affirmative action. We believe that in the long run isolated acts of fairness are beneficial even if we do not have direct control on all sources of unfairness in any specific problem. Causal modeling can help on creating arguments about the long run impact of individual contributions as e.g. a type of macroeconomic assessment. There are many challenges, and we should not pretend that precise answers can be obtained, but in theory we should aim at educated quantitative assessments validating how a systemic improvement in society can emerge from localized ways of addressing fairness.\n\nS6 Case Study: NYC Stop-and-Frisk Data\n\nSince 2002, the New York Police Department (NYPD) has recorded information about every time a police officer has stopped someone. The officer records information such as if the person was searched or frisked, if a weapon was found, their appearance, whether an arrest was made or a summons issued, if force was used, etc. We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking at just males stopped as this accounts for more than $9 0 \\%$ of the data. We fit a model which postulates that police interactions is caused by race and a single latent factor labeled Criminality that is meant to index other aspects of the individual that have been used by the police and which are independent of race. We do not claim that this model has a solid theoretical basis, we use it below as an illustration on how to carry on an analysis of counterfactually fair decisions. We also describe a spatial analysis of the estimated latent factors.",
      "referring_paragraphs": [
        "Model. We model this stop-and-frisk data using the graph in Figure 3. Specifically, we posit main causes for the observations: Arrest (if an individual was arrested), Force (some sort of force was used during the stop), Frisked, and Searched. The first cause of these observations is some measure of an individual’s latent Criminality, which we do not observe. We believe that Criminality also directly affects Weapon (an individual was found to be carrying a weapon). For all of the features previou",
        "Figure 3: A causal model for the stop and frisk dataset.",
        "We model this stop-and-frisk data using the graph in Figure 3."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_10",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg",
      "image_filename": "1703.06856_page0_fig8.jpg",
      "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.",
      "context_before": "Model. We model this stop-and-frisk data using the graph in Figure 3. Specifically, we posit main causes for the observations: Arrest (if an individual was arrested), Force (some sort of force was used during the stop), Frisked, and Searched. The first cause of these observations is some measure of an individual’s latent Criminality, which we do not observe. We believe that Criminality also directly affects Weapon (an individual was found to be carrying a weapon). For all of the features previously mentioned we believe there is an additional cause, an individual’s Race which we do observe. This factor is introduced as we believe that these observations may be biased based on an officer’s perception of whether an individual is likely a criminal or not, affected by an individual’s Race. Thus note that, in this model, Criminality is counterfactually fair for the prediction of any characteristic of the individual for problems where Race is a protected attribute.\n\nVisualization on a map of New York City. Each of the stops can be mapped to longitude and latitude points for where the stop occurred7. This allows us to visualize the distribution of two distinct populations: the stops of White and Black Hispanic individuals, shown in Figure 4. We note that there are more White individuals stopped (4492) than Black Hispanic individuals (2414). However, if we look at the arrest distribution (visualized geographically in the second plot) the rate of arrest for White individuals is lower $( 1 2 . 1 \\% )$ than for Black Hispanic individuals $( 1 9 . 8 \\%$ , the highest rate for any race in the dataset). Given our model we can ask: “If every individual had been White,\n\n7https://github.com/stablemarkets/StopAndFrisk",
      "context_after": "would they have been arrested?”. The answer to this is in the third plot. We see that the overall number of arrests decreases (from 5659 to 3722). What if every individual had been Black Hispanic? The fourth plot shows an increase in the number of arrests had individuals been Black Hispanic, according to the model (from 5659 to 6439). The yellow and purple circles show two regions where the difference in counterfactual arrest rates is particularly striking. Thus, the model indicates that, even when everything else in the model is held constant, race has a differential affect on arrest rate under the (strong) assumptions of the model.",
      "referring_paragraphs": [
        "Visualization on a map of New York City. Each of the stops can be mapped to longitude and latitude points for where the stop occurred7. This allows us to visualize the distribution of two distinct populations: the stops of White and Black Hispanic individuals, shown in Figure 4. We note that there are more White individuals stopped (4492) than Black Hispanic individuals (2414). However, if we look at the arrest distribution (visualized geographically in the second plot) the rate of arrest for Wh",
        "This allows us to visualize the distribution of two distinct populations: the stops of White and Black Hispanic individuals, shown in Figure 4.",
        "Figure 4: How race affects arrest."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.06856_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1703.09207": [
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/b05f4fe19bafb63fa143f3a1e0e3aa96c02314ae395846cac34f5187d5fa6fba.jpg",
      "image_filename": "b05f4fe19bafb63fa143f3a1e0e3aa96c02314ae395846cac34f5187d5fa6fba.jpg",
      "caption": "Table 1: A Cross-Tabulation of The Actual Outcome by The Predicted Outcome When The Prediction Algorithm Is Applied To A Dataset",
      "context_before": "2 Confusion Tables, Accuracy, and Fairness\n\nFor ease of exposition and with no important loss of generality, $Y$ is the response variable, henceforth assumed to be binary, and there are two protected group categories: men and women. We begin by introducing by example some key ideas needed later to define fairness and accuracy. We build on the simple structure of a 2 by 2 cross-tabulation (Berk, 2016b; Chouldechova, 2016; Hardt et al., 2016). Illustrations follow shortly.\n\n2 An algorithm is not a model. An algorithm is simply a sequential set of instructions for performing some task. When you balance your checkbook, you are applying an algorithm. A model is an algebraic statement of how the world works. In statistics, often it represents how the data were generated.",
      "context_after": "Table 1 is a cross-tabulation of the actual binary outcome $Y$ by the predicted binary outcome $\\hat { Y }$ . Such tables are in machine learning often called a “confusion table” (also “confusion matrix”). $\\hat { Y }$ is the fitted values that result when an algorithmic procedure is applied in the data. A “failure” is called a “positive” because it motivates the risk assessment; a positive might be an arrest for a violent crime. A “success” is a “negative,” such as completing a probation sentence without any arrests. These designations are arbitrary but allow for a less abstract discussion.3\n\nThe left margin of the table shows the actual outcome classes. The top margin of the table shows the predicted outcome classes. Cell counts internal to the table are denoted by letters. For example, “ $u$ ” is the number of observations in the upper-left cell. All counts in a particular cell have the same observed outcome class and the same predicted outcome class. For example, “ $\\boldsymbol { a }$ ” is the number of observations for which the observed response class is a failure, and the predicted response class is a failure. It is a true positive. Starting at the upper left cell and moving clockwise around the table are true positives, false negatives, true negatives, and false positives.\n\nThe cell counts and computed values on the margins of the table can be interpreted as descriptive statistics for the observed values and fitted values in the data on hand. Also common is to interpret the computed values on the margins of the table as estimates of the corresponding probabilities in a population. We turn to that later.",
      "referring_paragraphs": [
        "Table 1 is a cross-tabulation of the actual binary outcome $Y$ by the predicted binary outcome $\\hat { Y }$ . Such tables are in machine learning often called a “confusion table” (also “confusion matrix”). $\\hat { Y }$ is the fitted values that result when an algorithmic procedure is applied in the data. A “failure” is called a “positive” because it motivates the risk assessment; a positive might be an arrest for a violent crime. A “success” is a “negative,” such as completing a probation senten",
        "The discussion of fairness to follow uses all of these features of Table 1, although the particular features employed will vary with the kind of fairness. We will see, in addition, that the different kinds of fairness can be related to one another and to accuracy. But before getting into a more formal discussion, some common fairness issues will be illustrated with three hypothetical confusion tables.",
        "In order to provide clear definitions of algorithmic fairness, we will proceed for now as if $\\hat { f } ( L , S )$ provides estimates that are the same as the corresponding population features. In this way, we do not conflate a discussion of fairness with a discussion of estimation accuracy. The estimation accuracy is addressed later. We draw heavily on our earlier discussion of confusion tables, but to be consistent with the fairness literature, we emphasize accuracy rather than error. Neverth",
        "Alternatively, $\\hat { f } ( L , S )$ can also be seen estimating a response surface in the population that is an acknowledged approximation of the true response surface. In the population, the approximation has the same form as $\\hat { f } ( L , S )$ . Therefore, the estimates of probabilities from Table 1 can be estimates of the corresponding probabilities from a $Y$ by $\\hat { Y }$ cross-tabulation if $h ( L , S )$ were applied in the population. Thanks to the IID nature of the data, these es",
        "Table 1: A Cross-Tabulation of The Actual Outcome by The Predicted Outcome When The Prediction Algorithm Is Applied To A Dataset   \n\n<table><tr><td></td><td>Failure Predicted</td><td>Success Predicted</td><td>Conditional Procedure Error</td></tr><tr><td>Failure - A Positive</td><td>aTrue Positives</td><td>bFalse Negatives</td><td>b/(a+b)False Negative Rate</td></tr><tr><td>Success - A Negative</td><td>cFalse Positives</td><td>dTrue Negatives</td><td>c/(c+d)False Positive Rate</td></tr><tr><td>Conditional Use Error</td><td>c/(a+c)Failure Prediction Error</td><td>b/(b+d)Success Prediction Error</td><td>(c+b)(a+b+c+d)Overall Procedure Error</td></tr></table>\n\nTable 1 is a cross-tabulation of the actual binary outcome $Y$ by the predicted binary outcome $\\hat { Y }$ .",
        "Nevertheless, the notation is drawn from Table 1.",
        "Therefore, the estimates of probabilities from Table 1 can be estimates of the corresponding probabilities from a $Y$ by $\\hat { Y }$ cross-tabulation if $h ( L , S )$ were applied in the population."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/cc3ac197dbb180baeb8b932459363f78742c4147e646d20c78b2db44a986a932.jpg",
      "image_filename": "cc3ac197dbb180baeb8b932459363f78742c4147e646d20c78b2db44a986a932.jpg",
      "caption": "Table 2: FEMALES: FAIL(f ) OR SUCCEED(s) ON PAROLE (Success Base Rate = 500/1000 = .50, Cost ratio = 200/200 = 1:1, Predicted to Succeed $5 0 0 / 1 0 0 0 = . 5 0 \\AA$",
      "context_before": "4 We proceed in this manner because there will be clear links to fairness. There are many other measures from such a table for which this is far less true. Powers (2011) provides an excellent review.\n\nThe discussion of fairness to follow uses all of these features of Table 1, although the particular features employed will vary with the kind of fairness. We will see, in addition, that the different kinds of fairness can be related to one another and to accuracy. But before getting into a more formal discussion, some common fairness issues will be illustrated with three hypothetical confusion tables.\n\n5 There seems to be less naming consistency for these kinds errors compared to false negatives and false positives. Discussions in statistics about generalization error (Hastie et al., 2009: Section 7.2), can provide one set of terms whereas concerns about errors from statistical tests can provide another. In neither case, moreover, is the application to confusion tables necessarily natural. Terms like the “false discover rate” and the “false omission rate,” or “Type II” and “Type I” errors can be instructive for interpreting statistical tests but build in content that is not relevant for prediction errors. There is no null hypothesis being tested.",
      "context_after": "Table 2 is a confusion table for a hypothetical set of women released on parole. Gender is the protected individual attribute. As failure on parole is a “positive,” and a success on parole is a “negative.” For ease of exposition, the counts are meant to produce a very simple set of results.\n\nThe base rate for success is .50 because half of the women are not rearrested. The algorithm correctly predicts that the proportion who succeed on parole is .50. This is a favorable initial indication of the algorithm’s performance because the marginal distribution of $Y$ and $\\hat { Y }$ is the same.\n\nThe false negative rate and false positive rate of .40 is the same for successes and failures. When the outcome is known, the algorithm can correctly identify it 60% of the time. The cost ratio is, therefore, 1 to 1.",
      "referring_paragraphs": [
        "Table 2 is a confusion table for a hypothetical set of women released on parole. Gender is the protected individual attribute. As failure on parole is a “positive,” and a success on parole is a “negative.” For ease of exposition, the counts are meant to produce a very simple set of results.",
        "The prediction error of .40 is the same for predicted successes and predicted failures. When the outcome is predicted, the prediction is correct 60% of the time. There is no consideration of fairness because Table 2 shows only the results for women.",
        "parole is changed from .50 to .33. Men are substantially less likely to succeed on parole than women. The base rate was changed by multiplying the top row of cell counts in Table 2 by 2.0. That is the only change made to the cell counts. The bottom row of cell counts are unchanged.",
        "Whereas in Table 2, .50 of the women are predicted to succeed, in Table 3, .47 of the men are predicted to succeed. This is a small difference in practice, but it favors women. Some would call this unfair, but it is a different kind of unfairness than disparate prediction errors by gender.",
        "Finally, the cost ratio in Table 2 for women makes false positives and false negatives equally costly (1 to 1). In Table 3, false positives are twice as costly as false negatives. Incorrectly classifying a success on parole as failure is twice as costly for men (2 to 1). This too can be seen as unfair. Put",
        "We will see later that there are a number of proposals that try to correct for various kinds of unfairness, including those illustrated in the comparisons between Table 2 and Table 3. For example, it is sometimes possible to tune classification procedures to reduce or even eliminate some forms of unfairness.",
        "Table 2: FEMALES: FAIL(f ) OR SUCCEED(s) ON PAROLE (Success Base Rate = 500/1000 = .50, Cost ratio = 200/200 = 1:1, Predicted to Succeed $5 0 0 / 1 0 0 0 = ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/4e8221b838dff14030dab65b3a150b0e053e32844ca97ed2d04fd16cc1b2dc80.jpg",
      "image_filename": "4e8221b838dff14030dab65b3a150b0e053e32844ca97ed2d04fd16cc1b2dc80.jpg",
      "caption": "Table 3: MALES: FAIL(f ) OR SUCCEED(s) ON PAROLE (Success Base Rate = 500/1500 = .33, Cost ratio 400/200 = 2:1, Predicted to Succeed 700/1500 = .47)",
      "context_before": "The base rate for success is .50 because half of the women are not rearrested. The algorithm correctly predicts that the proportion who succeed on parole is .50. This is a favorable initial indication of the algorithm’s performance because the marginal distribution of $Y$ and $\\hat { Y }$ is the same.\n\nThe false negative rate and false positive rate of .40 is the same for successes and failures. When the outcome is known, the algorithm can correctly identify it 60% of the time. The cost ratio is, therefore, 1 to 1.\n\nThe prediction error of .40 is the same for predicted successes and predicted failures. When the outcome is predicted, the prediction is correct 60% of the time. There is no consideration of fairness because Table 2 shows only the results for women.",
      "context_after": "Table 3 is a confusion table for a hypothetical set of men released on parole. To help illustrate fairness concerns, the base rate for success on\n\nparole is changed from .50 to .33. Men are substantially less likely to succeed on parole than women. The base rate was changed by multiplying the top row of cell counts in Table 2 by 2.0. That is the only change made to the cell counts. The bottom row of cell counts are unchanged.\n\nThe false negative and false positive rates are the same and unchanged at .40. Just as for women, when the outcome is known, the algorithm can correctly identify it $6 0 \\%$ of the time. We will see later that the important comparison is across the two tables. Having a false positive and false negative rate within a table the same, does not figure in definitions of fairness. What matters is whether the false negative rate varies across tables and whether the false positive rate varies across tables.",
      "referring_paragraphs": [
        "Table 3 is a confusion table for a hypothetical set of men released on parole. To help illustrate fairness concerns, the base rate for success on",
        "Whereas in Table 2, .50 of the women are predicted to succeed, in Table 3, .47 of the men are predicted to succeed. This is a small difference in practice, but it favors women. Some would call this unfair, but it is a different kind of unfairness than disparate prediction errors by gender.",
        "Finally, the cost ratio in Table 2 for women makes false positives and false negatives equally costly (1 to 1). In Table 3, false positives are twice as costly as false negatives. Incorrectly classifying a success on parole as failure is twice as costly for men (2 to 1). This too can be seen as unfair. Put",
        "We will see later that there are a number of proposals that try to correct for various kinds of unfairness, including those illustrated in the comparisons between Table 2 and Table 3. For example, it is sometimes possible to tune classification procedures to reduce or even eliminate some forms of unfairness.",
        "Table 3: MALES: FAIL(f ) OR SUCCEED(s) ON PAROLE (Success Base Rate = 500/1500 = .33, Cost ratio 400/200 = 2:1, Predicted to Succeed 700/1500 = .47)   \n\n<table><tr><td></td><td>Yf</td><td>Ys</td><td>Conditional Procedure Error</td></tr><tr><td rowspan=\"2\">Yf - Positive</td><td>600</td><td>400</td><td>.40</td></tr><tr><td>True Positives</td><td>False Negatives</td><td>False Negative Rate</td></tr><tr><td rowspan=\"2\">Ys - Negatives</td><td>200</td><td>300</td><td>.40</td></tr><tr><td>False Positives</td><td>True Negative</td><td>False Positive Rate</td></tr><tr><td rowspan=\"2\">Conditional Use Error</td><td>.25</td><td>.57</td><td></td></tr><tr><td>Failure Prediction Error</td><td>Success Prediction error</td><td></td></tr></table>\n\nTable 3 is a confusion table for a hypothetical set of men released on parole."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/80a325dd57a30183fe382cdd9666200b4252bab1aa11bd46a077789a70c950a7.jpg",
      "image_filename": "80a325dd57a30183fe382cdd9666200b4252bab1aa11bd46a077789a70c950a7.jpg",
      "caption": "Table 4: MALES TUNED: FAIL(f ) OR SUCCEED(s) ON PAROLE (Success Base Rate = 500/1500 = .33, Cost ratio = 200/200 = 1:1, Predicted to succeed 500/1500 = .33)",
      "context_before": "Finally, the cost ratio in Table 2 for women makes false positives and false negatives equally costly (1 to 1). In Table 3, false positives are twice as costly as false negatives. Incorrectly classifying a success on parole as failure is twice as costly for men (2 to 1). This too can be seen as unfair. Put\n\nanother way, individuals who succeed on parole but who would be predicted to fail, are of greater relative concern when the individual is a man.\n\nNote, that all of these potential unfairness and accuracy problems surface solely by changing the base rate even when the false negative rate and false positive rates are unaffected. Base rates can matter a great deal, which is a theme to which we will return.",
      "context_after": "We will see later that there are a number of proposals that try to correct for various kinds of unfairness, including those illustrated in the comparisons between Table 2 and Table 3. For example, it is sometimes possible to tune classification procedures to reduce or even eliminate some forms of unfairness.\n\nIn Table 4, the success base rate for men is still .33, but the cost ratio for men is again 1 to 1. Now, when success on parole is predicted, it is incorrect 40 times out of 100 and corresponds to .40 success prediction error for women. When predicting success on parole, we have equal accuracy for men and women. One kind of unfairness has been eliminated. Moreover, the fraction of men predicted to succeed on parole now equals the actual fraction of men who succeed on parole. Some measure of credibility has been restored to the predictions for men.\n\nHowever, the false negative rate for men is now .20, not .40, as it is for women. In trade, therefore, when men actually fail on parole, the algorithm is more likely than for women to correctly identify it. By this measure, the algorithm performs better for men. Tradeoffs of these kinds are endemic in classification procedures that try to correct for unfairness. Some tradeoffs are inevitable and some are simply common. This too is a theme to which we will return.",
      "referring_paragraphs": [
        "In Table 4, the success base rate for men is still .33, but the cost ratio for men is again 1 to 1. Now, when success on parole is predicted, it is incorrect 40 times out of 100 and corresponds to .40 success prediction error for women. When predicting success on parole, we have equal accuracy for men and women. One kind of unfairness has been eliminated. Moreover, the fraction of men predicted to succeed on parole now equals the actual fraction of men who succeed on parole. Some measure of cred",
        "Table 4: MALES TUNED: FAIL(f ) OR SUCCEED(s) ON PAROLE (Success Base Rate = 500/1500 = .33, Cost ratio = 200/200 = 1:1, Predicted to succeed 500/1500 = .33)   \n\n<table><tr><td></td><td>Yf</td><td>Ys</td><td>Conditional Procedure Error</td></tr><tr><td rowspan=\"2\">Yf - Positive</td><td>800</td><td>200</td><td>.20</td></tr><tr><td>True Positives</td><td>False Negatives</td><td>False Negative Rate</td></tr><tr><td rowspan=\"2\">Ys - Negative</td><td>200</td><td>300</td><td>.40</td></tr><tr><td>False Positives</td><td>True Negatives</td><td>False Positive Rate</td></tr><tr><td rowspan=\"2\">Conditional Use Error</td><td>.20</td><td>.40</td><td></td></tr><tr><td>Failure Prediction Error</td><td>Success Prediction error</td><td></td></tr></table>\n\nWe will see later that there are a number of proposals that try to correct for various kinds of unfairness, including those illustrated in the comparisons between Table 2 and Table 3."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/8b580cac11be4bfa4cf7b80e16e6c83b1fccabe25fcaf612b1b4ef6b6eb76dba.jpg",
      "image_filename": "8b580cac11be4bfa4cf7b80e16e6c83b1fccabe25fcaf612b1b4ef6b6eb76dba.jpg",
      "caption": "Table 5: Males: A Cross-Tabulation When All Cases Are Assigned The Outcome Of Failure (Base Rate = .80, N = 500)",
      "context_before": "Tables 5 and 6 provide an example when the base rates are the same for men and women. There are 500 men and 50 women, but the relative representation of men and women does not matter materially in what follows. Failures are coded 1 and successes are coded 0, much as they might be in practice. Each case is assigned failure (i.e., $\\hat { Y } = 1$ ), but the same lessons would be learned if each case is assigned a success (i.e., $\\hat { Y } = 0$ ). A base rate of .80 for failures is imposed on both tables.\n\nIn practice, this approach makes no sense. Predictors are not being ex-\n\n17 This impossibility theorem is formulated a little differently by Kleinberg and his colleagues and by Chouldechova. Kleinberg et al. (2016) impose calibration and make explicit use of a risk scores from the algorithm. There is no formal transition to outcome classes. Chouldechova (2016), does not impose calibration in the same sense, and moves quickly from risk scores to outcome classes. But both sets of results are for our purposes effectively the same and consistent with our statement.",
      "context_after": "",
      "referring_paragraphs": [
        "If one allows the base rates for men and women differ, there is immediately a fairness price. Suppose in Table 5, 500 men fail instead of 400. The false positive and false negative rates are unchanged. But because the base rate for men is now larger than the base rate for women (i.e., .83 v. .80), conditional use accuracy is now higher for men, and a lower proportion of men will be incorrectly predicted to fail. This is the sort of result that would likely trigger charges of gender bias. Even in",
        "Predictors are not being ex-\n\nTable 5: Males: A Cross-Tabulation When All Cases Are Assigned The Outcome Of Failure (Base Rate = .80, N = 500)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>400</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>100</td><td>0</td><td>0.0</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>-</td><td></td></tr></table>\n\nTable 6: Females: A Cross-Tabulation When All Cases Are Assigned The Outcome of Failure (Base Rate = .80, N = 50)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>40</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>10</td><td>0</td><td>0.0</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>-</td><td></td></tr></table>\n\nploited."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/a4775e1cd995a5b4c2775f5d9a659c716540671d34cf581998689c9f2fe3dbea.jpg",
      "image_filename": "a4775e1cd995a5b4c2775f5d9a659c716540671d34cf581998689c9f2fe3dbea.jpg",
      "caption": "Table 6: Females: A Cross-Tabulation When All Cases Are Assigned The Outcome of Failure (Base Rate = .80, N = 50)",
      "context_before": "",
      "context_after": "ploited. But, one can see that there is conditional procedure accuracy equality, conditional use accuracy equality and overall accuracy equality. The false negative and false positive rates are the same for men and women as well at 0.0 and 1.0. There is also statistical parity. One does very well on fairness for a risk tool that cannot help decision-makers address risk in a useful manner. Accuracy has been given a very distant backseat. There is a dramatic tradeoff between accuracy and fairness.\n\nIf one allows the base rates for men and women differ, there is immediately a fairness price. Suppose in Table 5, 500 men fail instead of 400. The false positive and false negative rates are unchanged. But because the base rate for men is now larger than the base rate for women (i.e., .83 v. .80), conditional use accuracy is now higher for men, and a lower proportion of men will be incorrectly predicted to fail. This is the sort of result that would likely trigger charges of gender bias. Even in this “trivial” case, base rates matter.18\n\n18 When base rates are the same in this example, one perhaps could not achieve perfect fairness while also getting perfect accuracy. The example doesn’t have enough information to conclude that the populations aren’t separable. But that is not the point we are trying to make.",
      "referring_paragraphs": [
        "Predictors are not being ex-\n\nTable 5: Males: A Cross-Tabulation When All Cases Are Assigned The Outcome Of Failure (Base Rate = .80, N = 500)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>400</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>100</td><td>0</td><td>0.0</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>-</td><td></td></tr></table>\n\nTable 6: Females: A Cross-Tabulation When All Cases Are Assigned The Outcome of Failure (Base Rate = .80, N = 50)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>40</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>10</td><td>0</td><td>0.0</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>-</td><td></td></tr></table>\n\nploited."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_7",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/a348ee56f45efabd27ddb603018a3c7925ec6526f30109d749c723cb94afea7b.jpg",
      "image_filename": "a348ee56f45efabd27ddb603018a3c7925ec6526f30109d749c723cb94afea7b.jpg",
      "caption": "Table 7: Males: A Cross-Tabulation With Failure Assigned To All With A Probability of .30 (Base Rate = .80, N = 500)",
      "context_before": "6.1.2 Trivial Case #2: Assigning the Classes Using the Same Probability for All\n\nSuppose each case is assigned to an outcome class with the same probability. As in Trivial Case #1, no use made of predictors, so that accuracy does not figure into the fitting process.\n\nFor Tables 7 and 8, the assignment probability for failure is .30 for all, and therefore, the assignment probability for success is .70 for all. Nothing important changes should some other probability be used.19 The base rates for men and women are the same. For both, the proportions that fail are .80.",
      "context_after": "",
      "referring_paragraphs": [
        "Table 7: Males: A Cross-Tabulation With Failure Assigned To All With A Probability of .30 (Base Rate = .80, N = 500)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>120</td><td>280</td><td>.30</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>30</td><td>70</td><td>.70</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>.20</td><td></td></tr></table>\n\nTable 8: Females: A Cross-Tabulation With Failure Assigned To All With A Probability of .30 (Base Rate = .80, N = 50)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>12</td><td>28</td><td>.30</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>3</td><td>7</td><td>.70</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>.20</td><td></td></tr></table>\n\nIn Tables 7 and 8, we have the same fairness results we had in Tables 5 and 6, again with accuracy sacrificed."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_8",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/568a8c479b40cf09b3befdff94662244ac4141ef01c644189dba8f9959fdadac.jpg",
      "image_filename": "568a8c479b40cf09b3befdff94662244ac4141ef01c644189dba8f9959fdadac.jpg",
      "caption": "Table 8: Females: A Cross-Tabulation With Failure Assigned To All With A Probability of .30 (Base Rate = .80, N = 50)",
      "context_before": "",
      "context_after": "In Tables 7 and 8, we have the same fairness results we had in Tables 5 and 6, again with accuracy sacrificed. But suppose the second row of entries in Table 8 were 30 and 70 rather than 3 and 7. Now the failure base rate for women is .29, not .80. Conditional procedure accuracy equality remains from which it follows that the false negative and false positive rates are the\n\n19 The numbers in each cell assume for arithmetic simplicity that the counts come out exactly as they would in a limitless number of realizations. In practice, an assignment probability of .30 does not require exact cell counts of 30%.\n\nsame as well. But conditional use accuracy equality is lost. The probabilities of correct predictions for men are again .80 for failures, and .20 for successes. But for women, the corresponding probabilities are .29 and .71. Base rates really matter.",
      "referring_paragraphs": [
        "In Tables 7 and 8, we have the same fairness results we had in Tables 5 and 6, again with accuracy sacrificed. But suppose the second row of entries in Table 8 were 30 and 70 rather than 3 and 7. Now the failure base rate for women is .29, not .80. Conditional procedure accuracy equality remains from which it follows that the false negative and false positive rates are the",
        "Table 7: Males: A Cross-Tabulation With Failure Assigned To All With A Probability of .30 (Base Rate = .80, N = 500)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>120</td><td>280</td><td>.30</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>30</td><td>70</td><td>.70</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>.20</td><td></td></tr></table>\n\nTable 8: Females: A Cross-Tabulation With Failure Assigned To All With A Probability of .30 (Base Rate = .80, N = 50)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>12</td><td>28</td><td>.30</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>3</td><td>7</td><td>.70</td></tr><tr><td>Conditional Use Accuracy</td><td>.80</td><td>.20</td><td></td></tr></table>\n\nIn Tables 7 and 8, we have the same fairness results we had in Tables 5 and 6, again with accuracy sacrificed."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_9",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/53115b04773fdc65e1066f33fbab11fb9151cdc6789022909e78e44362a91446.jpg",
      "image_filename": "53115b04773fdc65e1066f33fbab11fb9151cdc6789022909e78e44362a91446.jpg",
      "caption": "Table 9: Males: A Cross-Tabulation With Separation and Perfect Prediction (Base Rate = .80, N = 500)",
      "context_before": "same as well. But conditional use accuracy equality is lost. The probabilities of correct predictions for men are again .80 for failures, and .20 for successes. But for women, the corresponding probabilities are .29 and .71. Base rates really matter.\n\n6.1.3 Perfect Separation\n\nWe now turn to an $h ( L , S )$ that is not trivial, but also very unlikely in practice. In a population, the observations are separable. In Tables 9 and 10, there is perfect separation, and $h ( L , S )$ finds it. Base rates are the same for men and women: .80 fail.",
      "context_after": "",
      "referring_paragraphs": [
        "Table 9: Males: A Cross-Tabulation With Separation and Perfect Prediction (Base Rate = .80, N = 500)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>400</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>0</td><td>100</td><td>1.0</td></tr><tr><td>Conditional Use Accuracy</td><td>1.0</td><td>1.0</td><td></td></tr></table>\n\nTable 10: Females: A Cross-Tabulation With Separation and Perfect Prediction (Base Rate = .80, N = 50)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>40</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>0</td><td>10</td><td>1.0</td></tr><tr><td>Conditional Use Accuracy</td><td>1.0</td><td>1.0</td><td></td></tr></table>\n\nThere are no false positives or false negatives, so the false positive rate and the false negative rate for both men and women are 0.0."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_10",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/4ac151614433093cada8c6fd09ecb0c802b714b8c537dd695d181520fe05b1b9.jpg",
      "image_filename": "4ac151614433093cada8c6fd09ecb0c802b714b8c537dd695d181520fe05b1b9.jpg",
      "caption": "Table 10: Females: A Cross-Tabulation With Separation and Perfect Prediction (Base Rate = .80, N = 50)",
      "context_before": "",
      "context_after": "There are no false positives or false negatives, so the false positive rate and the false negative rate for both men and women are 0.0. There is conditional procedure accuracy equality and conditional use accuracy equality because conditional procedure accuracy and conditional use accuracy are both perfect. This is the ideal, but fanciful, setting in which we can have it all.\n\nSuppose for women in Table 10, there are 20 women who do not fail rather than 10. Their failure base rate for females is now .67 rather than\n\n.80. But because of separation, conditional procedure accuracy equality and conditional use accuracy equality remain, and the false positive and false negative rates for men and women are still 0.0. Separation saves the day.20",
      "referring_paragraphs": [
        "Suppose for women in Table 10, there are 20 women who do not fail rather than 10. Their failure base rate for females is now .67 rather than",
        "Table 9: Males: A Cross-Tabulation With Separation and Perfect Prediction (Base Rate = .80, N = 500)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>400</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>0</td><td>100</td><td>1.0</td></tr><tr><td>Conditional Use Accuracy</td><td>1.0</td><td>1.0</td><td></td></tr></table>\n\nTable 10: Females: A Cross-Tabulation With Separation and Perfect Prediction (Base Rate = .80, N = 50)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>40</td><td>0</td><td>1.0</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>0</td><td>10</td><td>1.0</td></tr><tr><td>Conditional Use Accuracy</td><td>1.0</td><td>1.0</td><td></td></tr></table>\n\nThere are no false positives or false negatives, so the false positive rate and the false negative rate for both men and women are 0.0."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_11",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/f8d71983d184e0cc5cbadc85eff503c951583e40d327e024849170dce69a9f4b.jpg",
      "image_filename": "f8d71983d184e0cc5cbadc85eff503c951583e40d327e024849170dce69a9f4b.jpg",
      "caption": "Table 11: Females: A Cross-Tabulation Without Separation (Base Rate = .56, N = 900)",
      "context_before": "6.1.4 Closer To Real Life\n\nThere will virtually never be separation in the real data even if there there happens to be separation in the joint probability distribution responsible for the data. The fitting procedure $h ( L , S )$ may be overmatched because important predictors are not available or because the algorithm arrives at a suboptimal result. Nevertheless, some types of fairness can sometimes be achieved if base rates are cooperative.\n\nIf the base rates are the same and $h ( L , S , )$ finds that, there can be lots of good news. Tables 11 and 12 illustrate. Conditional procedure accuracy equality, conditional use accuracy equality, overall procedure accuracy hold, and the false negative rate and the false positive rate are the same for men and women. Results like those shown in Tables 11 and 12 can occur in real data, but would be rare in criminal justice applications for the common protected groups. Base rates will not be the same.",
      "context_after": "Suppose there is separation but the base rates are not the same. We are back to Tables 9 and 10, but with a lower base rate. Suppose there is no separation, but the base rates are the same. We are back to Tables 11 and 12.\n\nFrom Tables 13 and 14, one can see that when there is no separation and different base rates, there can still be conditional procedure accuracy equality. From conditional procedure accuracy equality, the false negative\n\n20 Although statistical parity has not figured in these illustrations, changing the base rate negates it.",
      "referring_paragraphs": [
        "Table 11: Females: A Cross-Tabulation Without Separation (Base Rate = .56, N = 900)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>300</td><td>200</td><td>.60</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>200</td><td>200</td><td>.50</td></tr><tr><td>Conditional Use Accuracy</td><td>.60</td><td>.50</td><td></td></tr></table>\n\nSuppose there is separation but the base rates are not the same."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_12",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/d68c8a71d11b544ea35501526dcd50fa6d1ae9d110599865543c98ef2559dfc7.jpg",
      "image_filename": "d68c8a71d11b544ea35501526dcd50fa6d1ae9d110599865543c98ef2559dfc7.jpg",
      "caption": "Table 12: Males: Confusion Table Without Separation (Base Rate is = .56, $\\mathrm { N } = 1 4 0 0$ )",
      "context_before": "Suppose there is separation but the base rates are not the same. We are back to Tables 9 and 10, but with a lower base rate. Suppose there is no separation, but the base rates are the same. We are back to Tables 11 and 12.\n\nFrom Tables 13 and 14, one can see that when there is no separation and different base rates, there can still be conditional procedure accuracy equality. From conditional procedure accuracy equality, the false negative\n\n20 Although statistical parity has not figured in these illustrations, changing the base rate negates it.",
      "context_after": "",
      "referring_paragraphs": [
        "From conditional procedure accuracy equality, the false negative\n\nTable 12: Males: Confusion Table Without Separation (Base Rate is = .56, $\\mathrm { N } = 1 4 0 0$ )   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>600</td><td>400</td><td>.60</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>400</td><td>400</td><td>.50</td></tr><tr><td>Conditional Use Accuracy</td><td>.60</td><td>.50</td><td></td></tr></table>\n\nTable 13: Confusion Table For Females With No Separation And A Different Base Rate Compared to Males (Female Base Rate Is 500/900 = .56)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>300</td><td>200</td><td>.60</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>200</td><td>200</td><td>.50</td></tr><tr><td>Conditional Use Accuracy</td><td>.60</td><td>.50</td><td></td></tr></table>\n\nrate and false positive rate, though different from one another, are the same across men and women."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_13",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/86ed3931d799334045739e69cbfbfc06008e83e13d9f780611552e5fad3bac22.jpg",
      "image_filename": "86ed3931d799334045739e69cbfbfc06008e83e13d9f780611552e5fad3bac22.jpg",
      "caption": "Table 13: Confusion Table For Females With No Separation And A Different Base Rate Compared to Males (Female Base Rate Is 500/900 = .56)",
      "context_before": "",
      "context_after": "rate and false positive rate, though different from one another, are the same across men and women. This is a start. But treatment equality is gone from which it follows that conditional use accuracy equality has been sacrificed. There is greater conditional use accuracy for women.",
      "referring_paragraphs": [
        "From conditional procedure accuracy equality, the false negative\n\nTable 12: Males: Confusion Table Without Separation (Base Rate is = .56, $\\mathrm { N } = 1 4 0 0$ )   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>600</td><td>400</td><td>.60</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>400</td><td>400</td><td>.50</td></tr><tr><td>Conditional Use Accuracy</td><td>.60</td><td>.50</td><td></td></tr></table>\n\nTable 13: Confusion Table For Females With No Separation And A Different Base Rate Compared to Males (Female Base Rate Is 500/900 = .56)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>300</td><td>200</td><td>.60</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>200</td><td>200</td><td>.50</td></tr><tr><td>Conditional Use Accuracy</td><td>.60</td><td>.50</td><td></td></tr></table>\n\nrate and false positive rate, though different from one another, are the same across men and women."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_14",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/0abd04a8e84875991ee7c4057c02f4f81dd8dd7dad31eaf7b8cbe9f70a6a3852.jpg",
      "image_filename": "0abd04a8e84875991ee7c4057c02f4f81dd8dd7dad31eaf7b8cbe9f70a6a3852.jpg",
      "caption": "Table 14: Confusion Table for Males With No Separation And A different Base Rate Compared to Females (Male Base Rate Is 1000/2200 = .45)",
      "context_before": "rate and false positive rate, though different from one another, are the same across men and women. This is a start. But treatment equality is gone from which it follows that conditional use accuracy equality has been sacrificed. There is greater conditional use accuracy for women.",
      "context_after": "Of the lessons that can be taken from the sets of tables just analyzed, perhaps the most important for policy is that when there is a lack of separation and different base rates across protected group categories, a key tradeoff will be between the false positive and false negative rates on one hand and conditional use accuracy equality on the other. Different base rates across\n\nprotected group categories would seem to require a thumb on the scale if conditional use accuracy equality is to be achieved. To see if this is true, we now consider corrections that have been proposed to improve algorithmic fairness.\n\n7 Potential Solutions",
      "referring_paragraphs": [
        "Table 14: Confusion Table for Males With No Separation And A different Base Rate Compared to Females (Male Base Rate Is 1000/2200 = .45)   \n\n<table><tr><td>Truth</td><td>Ŷ = 1</td><td>Ŷ = 0</td><td>Conditional Procedure Accuracy</td></tr><tr><td>Y = 1 (a positive - Fail)</td><td>600</td><td>400</td><td>.60</td></tr><tr><td>Y = 0 (a negative - Not Fail)</td><td>600</td><td>600</td><td>.50</td></tr><tr><td>Conditional Use Accuracy</td><td>.50</td><td>.40</td><td></td></tr></table>\n\nOf the lessons that can be taken from the sets of tables just analyzed, perhaps the most important for policy is that when there is a lack of separation and different base rates across protected group categories, a key tradeoff will be between the false positive and false negative rates on one hand and conditional use accuracy equality on the other."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1703.09207",
      "figure_id": "1703.09207_fig_15",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.09207/1703.09207/hybrid_auto/images/2eea5e26f26a669c7187f8efae5ec764ee265229ce0a8adb70ad5826571db3bd.jpg",
      "image_filename": "2eea5e26f26a669c7187f8efae5ec764ee265229ce0a8adb70ad5826571db3bd.jpg",
      "caption": "Table 15: Fairness Analysis for Black and White Offenders at Arraignment Using As An Outcome An Absence of Any Subsequent Arrest for A Crime of Violence (13,396 Blacks; 6604 Whites)",
      "context_before": "Two outcome classes are used for this illustration: within 21 months of arraignment, an arrest for a crime of violence or no arrest for a crime of violence. We use these two categories because should a crime of violence be predicted at arraignment, an offender would likely be detained. For other kinds of predicted arrests, an offender might well be freed or diverted into a treatment program. A prediction of no arrest probably could readily lead to a release.24 A 21 month follow up may seem inordinately lengthy, but in this\n\n23 Because of racial residential patterns, zip code can be a strong proxy for race. In this jurisdiction, stakeholders decided that race and zip code should not be included as predictors. Moreover, because of separate analyses for Whites and Blacks, race is a constant within each analysis.\n\n24 Actually, the decision is more complicated because a magistrate must also anticipate whether an offender will report to court when required to do so. There are machine learning forecasts being developed for failures to appear (FTAs), but a discussion of that work is well beyond the scope of this paper.",
      "context_after": "jurisdiction, it can take that long for a case to be resolved.25\n\nTable 15 provides the output that can be used to consider the kinds of fairness commonly addressed in the existing criminal justice literature. Success base rates are reported on the far left of the table, separately for Blacks and Whites: .89 and .94 respectively. For both, the vast majority of offenders are not arrested for a violent crime, but Blacks are more likely to be arrested for a crime of violence after a release. It follows that the White re-arrest rate is .06, and the black re-arrest rate is .11, nearly a 2 to 1 difference.\n\nFor this application, we focus on the probability that when the absence of an arrest for a violent crime is forecasted, the forecast is correct. The two different applications of random forests were tuned so that the probabilities are virtually the same: .93 and .94. There is conditional use accuracy equality, which some assert is a necessary feature of fairness.",
      "referring_paragraphs": [
        "Table 15 provides the output that can be used to consider the kinds of fairness commonly addressed in the existing criminal justice literature. Success base rates are reported on the far left of the table, separately for Blacks and Whites: .89 and .94 respectively. For both, the vast majority of offenders are not arrested for a violent crime, but Blacks are more likely to be arrested for a crime of violence after a release. It follows that the White re-arrest rate is .06, and the black re-arrest",
        "In summary, Table 15 illustrates well the formal results discussed earlier. There are different kinds of fairness that in practice are incompatible. There is no technical solution without some price being paid. How the tradeoffs should be made is a political decision.",
        "A prediction of no arrest probably could readily lead to a release.24 A 21 month follow up may seem inordinately lengthy, but in this\n\nTable 15: Fairness Analysis for Black and White Offenders at Arraignment Using As An Outcome An Absence of Any Subsequent Arrest for A Crime of Violence (13,396 Blacks; 6604 Whites)   \n\n<table><tr><td>Race</td><td>Base Rate</td><td>Conditional Use Accuracy</td><td>False Negative Rate</td><td>False Positive Rate</td></tr><tr><td>Black</td><td>.89</td><td>.93</td><td>.49</td><td>.24</td></tr><tr><td>White</td><td>.94</td><td>.94</td><td>.93</td><td>.02</td></tr></table>\n\njurisdiction, it can take that long for a case to be resolved.25\n\nTable 15 provides the output that can be used to consider the kinds of fairness commonly addressed in the existing criminal justice literature."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1703.09207_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1705.10378": [
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
      "image_filename": "1705.10378_page0_fig0.jpg",
      "caption": "(a)",
      "context_before": "Another standard assumption is known as conditional ignorability. This assumption states that conditional on a set of factors $\\mathbf { C } \\subseteq \\mathbf { X }$ , $A$ is independent of any counterfactual outcome, i.e. $Y ( a ) ~ \\bot \\bot ~ A | \\mathbf { C } , \\forall a \\ \\in \\ \\mathfrak { X } _ { A }$ , where (. ⊥⊥ .|.) represents conditional independence. Given these assumptions, we can show that $\\begin{array} { r } { p ( \\bar { Y } ( a ) ) = \\sum _ { \\mathbf { C } } p ( Y \\mid a , \\mathbf { C } ) p ( \\bar { \\mathbf { C } } ) } \\end{array}$ , known as the adjustment formula, the backdoor formula, or stratification. Intuitively, the set $\\mathbf { C }$ acts as a set of observed confounders, such that adjusting for their influence suffices to remove all non-causal dependence of $A$ and $Y$ , leaving only the part of the dependence that corresponds to the causal effect. A general characterization of identifiable functionals of causal effects exists (Tian and Pearl 2002; Shpitser and Pearl 2008).\n\nCausal relationships are often represented by graphical causal models (Spirtes, Glymour, and Scheines 2001; Pearl 2009). Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables (Richardson and Robins 2013). In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships. As an example, in Fig. 1 (a), $C$ is potentially a direct cause of $A$ , while $M$ mediates a part of the causal influence of $A$ on $Y$ , represented by all directed paths from $A$ to $Y$ .\n\nA natural step in causal inference is understanding the mechanism by which $A$ influences $Y$ . A simple form of understanding mechanisms is via mediation analysis, where the causal influence of $A$ on $Y$ , as quantified by the ACE,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1705.10378_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig1.jpg",
      "image_filename": "1705.10378_page0_fig1.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1705.10378_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig2.jpg",
      "image_filename": "1705.10378_page0_fig2.jpg",
      "caption": "(c) Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.",
      "context_before": "",
      "context_after": "is decomposed into the direct effect, and the indirect effect mediated by a mediator variable $M$ . In typical mediation settings, X is partitioned into a treatment $A$ , a single mediator $M$ , an outcome $Y$ , and a set of baseline factors $\\mathbf { C } = \\mathbf { X } \\setminus \\{ A , M , Y \\}$ .\n\nMediation is encoded via a counterfactual contrast using a nested potential outcome of the form $Y ( a , M ( a ^ { \\prime } ) )$ , for $a , a ^ { \\prime } \\in \\bar { \\mathfrak { X } _ { A } } . Y ( a , M ( a ^ { \\prime } ) )$ reads as “the outcome $Y$ if $A$ were set to $a$ , while $M$ were set to whatever value it would have attained had $A$ been set to $a ^ { \\prime }$ . An intuitive interpretation for this counterfactual occurs in cases where a treatment can be decomposed into two disjoint parts, one of which acts on $Y$ but not $M$ , and another acts on $M$ but not $Y$ . For instance, smoking can be decomposed into smoke and nicotine. Then if $M$ is a mediator affected by smoke, but not nicotine (for instance lung cancer), and $Y$ is a composite health outcome, then $Y ( a , { \\bar { M } } ( a ^ { \\prime } ) )$ corresponds to the response of $Y$ to an intervention that sets the nicotine exposure (the part of the treatment associated with $Y$ ) to what it would be in smokers, and the smoke exposure (the part of the treatment associated with $M$ ) to what it would be in non-smokers. An example of such an intervention would be a nicotine patch.\n\nGiven $Y ( a , M ( a ^ { \\prime } ) )$ , we define the following effects on the mean difference scale: the natural direct effect (NDE) as $\\mathbb { E } [ Y ( a , M ( a ^ { \\prime } ) ) ] ~ - ~ \\mathbb { E } [ Y ( a ^ { \\prime } ) ]$ , and the natural indirect effect (NIE) as $\\mathbb { E } [ \\bar { Y } ( a ) ] \\ \\bar { - } \\ \\mathbb { E } [ Y ( a , M ( a ^ { \\prime } ) ) ]$ (Robins and Greenland 1992). Intuitively, the NDE compares the mean outcome affected only by the part of the treatment that acts on it, and the mean outcome under placebo treatment. Similarly, the NIE compares the outcome affected by all treatment, and the outcome where the part of the treatment that acts on the mediator is “turned off.” A telescoping sum argument implies that $\\mathrm { A C E } = \\mathrm { N D E } + \\mathrm { N I E }$ .",
      "referring_paragraphs": [
        "Causal relationships are often represented by graphical causal models (Spirtes, Glymour, and Scheines 2001; Pearl 2009). Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables (Richardson and Robins 2013). In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships. As ",
        "In general, we may be interested in decomposing the ACE into effects along particular causal pathways. For example in Fig. 1 (b), we may wish to decompose the effect of $A$ on $Y$ into the contribution of the path $A W Y$ , and the path bundle $A Y$ and $A \\to M \\to W \\to Y .$ Effects along paths, such as an effect along the path $A W Y$ , are known as path-specific effects (Pearl 2001). Just as the NDE and NIE, path-specific effects (PSEs) can be formulated as nested counterfactuals (Shpitser 2",
        "We now illustrate the relationship between the choice of W and the choice of $g$ by considering three of the four consistent estimators of the NDE (assuming the model shown in Fig. 1 (a) is correct) presented in (Tchetgen and Shpitser 2012b). The first estimator is the MLE plug in estimator for (1), given by",
        "bear children in the family. However, gender also likely influences the subject’s performance on the entrance test, and requiring that certain requirements of strength and fitness is reasonable in a job like construction. The situation is represented by Fig. 1 (b), with a hidden common cause of $M$ and $Y$ added since it does not influence the subsequent analysis.",
        "In this case, the PSE that must be minimized for the purposes of making the hiring decision is given by (2), and is identified, given a causal model in Fig. 1 (b), by (3). If we use the analogue of (5), we would maximize $\\mathcal { L } ( \\mathcal { D } ; \\boldsymbol { \\alpha } )$ subject to",
        "Generally, the optimization problem in (4) involves complex non-linear constraints on the parameter space. However, in certain cases the optimization problem is significantly simpler, and involves box constraints on individual parameters of the likelihood. In such cases, standard optimization software such as the optim function in the R programming language can be used to directly solve (4). We describe two such cases here. First, if we assume a linear regression model for the outcome $Y = w _ {",
        "Suppose our problem entailed the causal model in Fig. 1 (b), or Fig. 1 (c) where in both cases only the NDE of $A$ on $Y$ is discriminatory. Existing identification results for PSEs (Shpitser 2013) imply that the NDE is not identified in either model. This means estimation of the NDE from observed data is not possible as the NDE is not a function of the observed data distribution in either model.",
        "In such cases, three approaches are possible. In both cases, the unobserved confounders $U$ are responsible for the lack of identification. If it were possible to obtain data on these variables, or obtain reliable proxies for them, the NDE becomes identifiable in both cases. If measuring $U$ is not possible, a second alternative is to consider a PSE that is identified, and that includes the paths in the PSE of interest and other paths. For example, in Fig. 1 (b), while the NDE of $A$ on $Y$ , wh",
        "If we are using the PSE on the mean difference scale, the magnitude of the effect which includes more paths than the PSE we are interested in must be an upper bound on the magnitude of the PSE of interest in order for the bounds we impose to actually limit discrimination. This is only possible if, for instance, all causal influence of $A$ on $Y$ along paths involved in the PSE are of the same sign. In Fig. 1 (b), this would mean assuming that if we expect the NDE of $A$ on $Y$ to be negative (du",
        "Figure 1: (a) A causal graph with a single mediator."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1705.10378_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/2cd613e34ad3d319a2a253594f183b84fd72e045957e19f9c3bb70a20819b1a2.jpg",
      "image_filename": "2cd613e34ad3d319a2a253594f183b84fd72e045957e19f9c3bb70a20819b1a2.jpg",
      "caption": "That is, gender is randomly assigned at birth, the people in the cohort are very likely to have prior convictions (with men having more), and $p ( H | C , G )$ specifies a certain hiring rule for the cohort.",
      "context_before": "A common class of approaches for fair inference is to quantify fairness via an associative (rather than causal) relationship between the sensitive feature $S$ and the outcome $Y$ . For instance, (Feldman et al. 2015) adopted the $80 \\%$ rule, for comparing selection rates based on sensitive features. This is a guideline (not a legal test) advocated by the Equal Employment Opportunity Commission (EEOC. 1979) as a way of suggesting possible discrimination. Rate of selection here is defined as the conditional probability of selection given the sensitive feature, or $p ( \\boldsymbol { Y } | \\boldsymbol { S } )$ . (Feldman et al. 2015) proposed methods for removing disparities based on this rule via a link to classification accuracy. A White House report on “equal opportunity by design\" (Executive Office of the President May 2016) prompted (Hardt, Price, and Srebro 2016) to propose a fairness criterion, called equalized odds, that ensures that true\n\nand false positive rates are equal across all groups. This criterion is also associative.\n\nThe issue with these approaches is they do not give intuitive results in cases where the sensitive feature is not randomly assigned (as gender is at conception), but instead exhibits spurious correlations with the outcome via another, possibly unobserved, feature. We illustrate the difficulty with the following hypothetical example. Certain states in the US prohibit discrimination based on past conviction history. Prior convictions are influenced by other variables, such as gender (men have more prior convictions than women, on average). Consider a hypothetical dataset (consisting mostly of people with prior convictions) with two features – prior conviction $( C )$ and gender $( G )$ as well as the hiring outcome $( H )$ . The values are coded as follows: male is 1, female is 0, prior conviction and hiring are 1, lack of prior conviction and no hiring is 0. Assume the dataset is drawn from the joint density specified as follows: $p ( G = 1 ) = 0 . 5$ , and",
      "context_after": "That is, gender is randomly assigned at birth, the people in the cohort are very likely to have prior convictions (with men having more), and $p ( H | C , G )$ specifies a certain hiring rule for the cohort. For simplicity, we assume no other features of people in the cohort are relevant for either the prior conviction or the hiring decision. It’s easy to show that\n\n$$ p (H = 1 | C = 1) = 0. 0 5 9 5 \\approx 0. 0 5 1 5 = p (H = 1 | C = 0). $$\n\nHowever, intuitively we would consider a hiring rule in this example fair if, in a hypothetical randomized trial that assigned convictions randomly (conviction to the case group, no conviction to the control group), the rule would yield equal hiring probabilities to cases and controls. In our example, this implies comparing counterfactual probabilities $p ( H ( C = 1 ) )$ ) and $p ( H ( C = 0 ) )$ . Since we posited no other relevant features for assigning $C$ and $H$ than $A$ , these probabilities are identified, via the adjustment formula described earlier, yielding $p ( H ( C ~ = ~ 1 ) ) \\bar { ~ } = ~ 0 . 0 3 5$ , and $p ( H ( C \\ =$ $0 ) ) \\ = \\ \\mathrm { { \\dot { 0 } } } . 1 2 5$ . That is, any method relying on associative measures of discrimination will likely conclude no discrimination here, yet the intuitively compelling test of discrimination will reveal a strong preference to hiring people without prior convictions. The large difference between $p ( H ( C =$ 0)) and $p ( H \\mid C = 0 )$ has to do with extreme probabilities $p ( C | G )$ in our example. Even in less extreme examples, any approach that relies on associative measures of association will be led astray due to failing to properly model sources of confounding for the relationship of the sensitive feature and the outcome. One might imagine that a simple repair in this example would be to also include $G$ as a feature. The reason this does not work in general is not all features are possible to measure, and in general counterfactual probabilities are complex functions of the observed data, not just conditional densities (Shpitser and Pearl 2006).",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1705.10378_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg",
      "image_filename": "1705.10378_page0_fig3.jpg",
      "caption": "(a)",
      "context_before": "Correctional Offender Management Profiling for Alternative Sanctions, or COMPAS, is a risk assessment tool, created by the company Northpointe, that is being used across the US to determine whether to release or detain a defendant before his or her trial. Each pretrial defendant receives several COMPAS scores based on factors including but not limited to demographics, criminal history, family history, and social status. Among these scores, we are primarily interested in “Risk of Recidivism\". Propublica (Angwin et al. 2016) has obtained two years worth of COMPAS scores from the Broward County Sheriff’s Office in Florida that contains scores for over 11000 people who were assessed at the pretrial stage and scored in 2013 and 2014. COMPAS score for each defendant ranges from 1 to 10, with 10 being the highest risk. Besides the COMPAS score, the data also includes records on defendant’s age, gender, race, prior convictions, and whether or not recidivism occurred in a span of two years. We limited our attention to the cohort consisting of African-Americans and Caucasians.\n\nWe are interested in predicting whether a defendant would reoffend using the COMPAS data. For illustration, we assume the use of prior convictions, possibly influenced by race, is fair for determining recidivism. Thus, we defined discrimination as effect along the direct path from race to the recidivism prediction outcome. The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism. The “disallowed\" path in this problem is drawn in green in Figure 2(a). The effect along this path is the NDE. The objective is to learn a fair model for $Y$ . i.e. a model where NDE is minimized.\n\nWe obtained the posterior sample representation of $\\mathbb { E } [ Y | A , M , \\mathbf { C } ]$ via both regular and constrained BART. Under the unconstrained posterior, the NDE (on the odds ratio scale) was equal to 1.3. This number is interpreted to mean that the odds of recidivism would have been 1.3 times higher",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1705.10378_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig4.jpg",
      "image_filename": "1705.10378_page0_fig4.jpg",
      "caption": "(b) Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
      "context_before": "",
      "context_after": "had we changed race from Caucasian to African-American. In our experiment we restricted NDE to lie between 0.95 and 1.05. Using unconstrained BART, our prediction accuracy on the test set was $6 7 . 8 \\%$ , removing treatment from the outcome model dropped the accuracy to $6 4 . 0 \\%$ , and using constrained BART lead to the accuracy of $6 6 . 4 \\%$ . As expected, dropping race, an informative feature, led to a greater decrease in accuracy, compared to simply constraining the outcome model to obey the constraint on the NDE.\n\nIn addition to our approach to removing discrimination, we are also interested in assessing the extent to which the existing recidivism classifier used by Northpointe is biased. Unfortunately, we do not have access to the exact model which generated COMPAS scores, since it is proprietary, nor all the input features used. Instead, we used our dataset to predict a binarized COMPAS score by fitting the model $\\tilde { p } ( \\bar { Y } | M , \\mathbf { C } )$ using BART. We dropped race, as we know Northpointe’s model does not use that feature. Discrimination, as we defined it, may still be present even if we drop race. To assess discrimination, we estimate the NDE, our measure of discrimination, in the semiparametric model of $p ( Y , M , A , { \\bf C } )$ , where the only constraint is that $p ( Y | M , \\mathbf { C } )$ is equal to $\\tilde { p }$ above. This model corresponds to (our approximation of) the “world” used by Northpointe. Measuring the NDE on the ratio scale using this model yielded 2.1, which is far from 1 (the null effect value). In other words, assuming the defendant is Caucasian, then the odds of recidivism for him would be 2.1 times higher had he been, contrary to fact, African-American. Thus, our best guess on Northpointe’s model is that it is severely discriminatory.\n\nThe “adult” dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income that is recorded as a binary variable denoting whether individuals have income above or below $5 0 k$ – high vs low income. The objective is to learn a statistical model that predicts the class of income for a given individual. Suppose banks are interested in using this model to identify reliable candidates for loan application. Raw use of data might construct models that are biased towards females who are perceived to have lower income in general compared to males. The causal model for this dataset is drawn in Figure 2(b). Gender is the sensitive variable in this example denoted by $A$ in figure 2(b) and income class is denoted by Y . M denotes the marital",
      "referring_paragraphs": [
        "We are interested in predicting whether a defendant would reoffend using the COMPAS data. For illustration, we assume the use of prior convictions, possibly influenced by race, is fair for determining recidivism. Thus, we defined discrimination as effect along the direct path from race to the recidivism prediction outcome. The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age a",
        "The “adult” dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income that is recorded as a binary variable denoting whether individuals have income above or below $5 0 k$ – high vs low income. The objective is to learn a statistical model that predicts the class of income for a given individual. Suppose banks are interested in using this ",
        "Here, besides the direct effect $A Y$ ), we would like to remove the effect of gender on income through marital status $( A \\to M \\to \\ldots \\to Y )$ ). The “disallowed\" paths are drawn in green in Figure 2(b). The PSE along the green paths is identifiable via the recanting district criterion in (Shpitser 2013), and can be computed by calculating odds ratio or contrast comparison of the counterfactual variable $Y ( a , M ( a ) , L ( a ^ { \\prime } , M ( a ) ) , { \\bf R } ( a ^ { \\prime } , M ( a",
        "The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism.",
        "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
        "The causal model for this dataset is drawn in Figure 2(b)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1705.10378_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1706.02409": [
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/834287260aa1d3dd87e59dcd6cda5d48342d6c4f5fdb2b7c9a35846e53a127fd.jpg",
      "image_filename": "834287260aa1d3dd87e59dcd6cda5d48342d6c4f5fdb2b7c9a35846e53a127fd.jpg",
      "caption": "Table 1: Summary of datasets. Type indicates whether regression is logistic or linear; $n$ is total number of data points; $d$ is dimensionality; Minority $n$ is the number of data points in the smaller population; Protected indicates which feature is protected or fairness-sensitive.",
      "context_before": "5We only used the data in Adult.data in our experiments.\n\n6http://www2.law.ucla.edu/sander/Systemic/Data.htm\n\ngoal is to predict the sentence length given by the judge based on factors such as previous criminal records and the crimes for which the conviction was obtained. The protected attribute is gender.",
      "context_after": "4.1 Accuracy-Fairness Efficient Frontiers\n\nWe begin by examining the efficient frontier of accuracy vs. fairness for the six datasets. These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function. For the logistic regression cases, we extract probabilities from the learned model w as ${ \\mathrm { P r } } [ y _ { i } = 1 ] =$ $\\exp ( \\mathbf { w } \\cdot x _ { i } ) / ( 1 + \\exp ( \\mathbf { w } \\cdot x _ { i } ) )$ and evaluate these probabilities as predictions for the binary labels using MSE. 7 In all of the datasets, as $\\lambda$ increases, the models converge to the best constant predictor, which minimizes the fairness penalties.\n\nPerhaps the most striking aspect of Figure 1 is the great diversity of tradeoffs across different datasets and different fairness regularizers. For instance, if we examine the individual fairness regularizer, on four of the datasets (Adult, Communities and Crime, Law School and Sentencing), the curvature is relatively mild and constant — there is an approximately fixed rate at which fairness can be traded for accuracy. In contrast, on COMPAS and Default, fairness loss can be reduced almost for “free” until some small threshold value, at which point the accuracy cost increases dramatically. Similar comments can be made regarding hybrid fairness in the logistic regression cases.",
      "referring_paragraphs": [
        "The datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protected” feature (race or gender) that thus defines the subgroups across which we apply our fairness criteria (see Appendix A.3 for more details). The datasets vary considerably in the number of observations, their dimensionality, and the relative size of the minority subgroup.",
        "We begin by examining the efficient frontier of accuracy vs. fairness for the six datasets. These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function. For the logistic regression cases, we extract probabilities from the learned model w as ${ \\mathrm { P r } } [ y _ { i } = 1 ] =$ $\\exp ( \\mathbf { w } \\cdot x _ { i } ) / ( 1 + \\exp ( \\",
        "Perhaps the most striking aspect of Figure 1 is the great diversity of tradeoffs across different datasets and different fairness regularizers. For instance, if we examine the individual fairness regularizer, on four of the datasets (Adult, Communities and Crime, Law School and Sentencing), the curvature is relatively mild and constant — there is an approximately fixed rate at which fairness can be traded for accuracy. In contrast, on COMPAS and Default, fairness loss can be reduced almost for “",
        "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy. However, they are difficult to compare quantitatively, because the scale of the fairness loss differs substantially from data set to data set. In this section, we give a cross-dataset comparison using a measure (which we call Price of Fairness) which has the eff",
        "For a given data set and regression type (linear or logistic), let w∗ be the optimal model absent any fairness penalty (i.e. the empirical risk minimizer when the fairness “regularization” weight $\\lambda = 0$ ). This model will suffer some fairness penalty: it represents the “maximally unfair” point on the fairness/accuracy frontiers from Figure 1. For each dataset, we will fix a normalization such that this fairness penalty is rescaled to be 1, and ask for the cost (in terms of the relative in",
        "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case. We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1. For some datasets (e.g. COMPAS and Sentencing), increasing the fairness constraint by decreasing $\\alpha$ has only a mild cost in terms of error. For others (e.g. Communities and Crime, and La",
        "While the fairness losses 1, 2, and 3 are defined using all the $n _ { 1 } \\times n _ { 2 }$ cross pairs in the dataset, in our experiments we only used $2 \\times$ Minority $n$ random cross pairs where Minority $n = \\operatorname* { m i n } \\{ n _ { 1 } , n _ { 2 } \\}$ (see Table 1). This is because: (1) using more cross pairs did not substantially improve the efficiency curves in Figure 1, (2) the CVXPY solver for binary-valued problems would become unstable when using individual fairness if we",
        "The datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protected” feature (race or gender) that thus defines the subgroups across which we apply our fairness criteria (see Appendix A.3 for more details).",
        "These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function.",
        "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
        "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g.",
        "We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1.",
        "This is because: (1) using more cross pairs did not substantially improve the efficiency curves in Figure 1, (2) the CVXPY solver for binary-valued problems would become unstable when using individual fairness if we increase the number of cross pairs significantly."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig0.jpg",
      "image_filename": "1706.02409_page0_fig0.jpg",
      "caption": "improve the fairness/accuracy trade-off for any of these notions of fairness.",
      "context_before": "Note that in theory group fairness is strictly less costly than individual fairness for any particular model w (by Jensen’s inequality), and using separate models (one for each group) should strictly\n\n7Note that assessing the MSE of these probabilities, interpreted as predictions, is a sensible choice. Since squared error is a proper scoring rule, if the labels are indeed generated according to a logistic regression model, minimizing the squared error of a predictor using mean squared error will elicit the true model as its minimizer.\n\nimprove the fairness/accuracy trade-off for any of these notions of fairness. However, both group fairness and separate models are more prone to overfitting (than individual fairness and single model), and hence a larger $\\ell _ { 2 }$ -regularization parameter $\\gamma$ tends to be selected in cross validation in these settings (see Appendix A.1 for more details). This is a surprising interaction between the strength of the fairness penalty and the generalization ability of the model, and results in group fairness sometimes having a more severe tradeoff with accuracy when compared to individual fairness, and separate models having little benefit out of sample, although they can appear to have a large effect in-sample (because of the effects of over-fitting).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig1.jpg",
      "image_filename": "1706.02409_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig2.jpg",
      "image_filename": "1706.02409_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg",
      "image_filename": "1706.02409_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig4.jpg",
      "image_filename": "1706.02409_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_7",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig5.jpg",
      "image_filename": "1706.02409_page0_fig5.jpg",
      "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
      "context_before": "",
      "context_after": "4.2 Price of Fairness\n\nThe efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy. However, they are difficult to compare quantitatively, because the scale of the fairness loss differs substantially from data set to data set. In this section, we give a cross-dataset comparison using a measure (which we call Price of Fairness) which has the effect of normalizing the fairness loss across data sets to lie on the same scale.\n\nFor a given data set and regression type (linear or logistic), let w∗ be the optimal model absent any fairness penalty (i.e. the empirical risk minimizer when the fairness “regularization” weight $\\lambda = 0$ ). This model will suffer some fairness penalty: it represents the “maximally unfair” point on the fairness/accuracy frontiers from Figure 1. For each dataset, we will fix a normalization such that this fairness penalty is rescaled to be 1, and ask for the cost (in terms of the relative increase in mean squared error) of constraining our predictor to have fairness penalty $\\alpha \\leq 1$ . Equivalently, this is measuring the relative increase in MSE that results from constraining a predictor to have fairness",
      "referring_paragraphs": [
        "The datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protected” feature (race or gender) that thus defines the subgroups across which we apply our fairness criteria (see Appendix A.3 for more details). The datasets vary considerably in the number of observations, their dimensionality, and the relative size of the minority subgroup.",
        "We begin by examining the efficient frontier of accuracy vs. fairness for the six datasets. These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function. For the logistic regression cases, we extract probabilities from the learned model w as ${ \\mathrm { P r } } [ y _ { i } = 1 ] =$ $\\exp ( \\mathbf { w } \\cdot x _ { i } ) / ( 1 + \\exp ( \\",
        "Perhaps the most striking aspect of Figure 1 is the great diversity of tradeoffs across different datasets and different fairness regularizers. For instance, if we examine the individual fairness regularizer, on four of the datasets (Adult, Communities and Crime, Law School and Sentencing), the curvature is relatively mild and constant — there is an approximately fixed rate at which fairness can be traded for accuracy. In contrast, on COMPAS and Default, fairness loss can be reduced almost for “",
        "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy. However, they are difficult to compare quantitatively, because the scale of the fairness loss differs substantially from data set to data set. In this section, we give a cross-dataset comparison using a measure (which we call Price of Fairness) which has the eff",
        "For a given data set and regression type (linear or logistic), let w∗ be the optimal model absent any fairness penalty (i.e. the empirical risk minimizer when the fairness “regularization” weight $\\lambda = 0$ ). This model will suffer some fairness penalty: it represents the “maximally unfair” point on the fairness/accuracy frontiers from Figure 1. For each dataset, we will fix a normalization such that this fairness penalty is rescaled to be 1, and ask for the cost (in terms of the relative in",
        "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case. We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1. For some datasets (e.g. COMPAS and Sentencing), increasing the fairness constraint by decreasing $\\alpha$ has only a mild cost in terms of error. For others (e.g. Communities and Crime, and La",
        "While the fairness losses 1, 2, and 3 are defined using all the $n _ { 1 } \\times n _ { 2 }$ cross pairs in the dataset, in our experiments we only used $2 \\times$ Minority $n$ random cross pairs where Minority $n = \\operatorname* { m i n } \\{ n _ { 1 } , n _ { 2 } \\}$ (see Table 1). This is because: (1) using more cross pairs did not substantially improve the efficiency curves in Figure 1, (2) the CVXPY solver for binary-valued problems would become unstable when using individual fairness if we",
        "The datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protected” feature (race or gender) that thus defines the subgroups across which we apply our fairness criteria (see Appendix A.3 for more details).",
        "These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function.",
        "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
        "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g.",
        "We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1.",
        "This is because: (1) using more cross pairs did not substantially improve the efficiency curves in Figure 1, (2) the CVXPY solver for binary-valued problems would become unstable when using individual fairness if we increase the number of cross pairs significantly."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig6.jpg",
      "image_filename": "1706.02409_page0_fig6.jpg",
      "caption": "In contrast, in this work we have studied a variety of fairness regularizers for regression problems, and applied them to data sets in which fairness is not subservient to generalization, but is instead a first-order con",
      "context_before": "Finally, this normalization allows us to observe variation across fairness penalties in the rate of change in the PoF as $\\alpha$ is decreased. In some datasets (e.g. Communities and Crime, and Sentencing), the PoF changes in lock-step across all measures of unfairness. However, for others (e.g. Default), the PoF increases substantially with $\\alpha$ when we consider group or hybrid fairness measures, but is much more stable for individual fairness.\n\nThe use of a complexity regularizer to control overfitting is both standard and well-understood in machine learning. While the use of such a regularizer introduces a trade-off — goodness of fit vs. model complexity — it does not introduce a tension, because complexity regularization is always in service of improving generalization, and is usually not a goal in its own right.\n\nIn contrast, in this work we have studied a variety of fairness regularizers for regression problems, and applied them to data sets in which fairness is not subservient to generalization, but is instead a first-order consideration. Our empirical study has demonstrated that the choice of fairness regularizer (group, individual, hybrid, or other) and the particular data set can have qualitative effects on the trade-off between accuracy and fairness. Combined with recent theoretical results [7, 12, 22] that also highlight the incompatibility of various fairness measures, our results highlight the care that must be taken by practitioners in defining the type of fairness they care about for their particular application, and in determining the appropriate balance between predictive accuracy and fairness.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig7.jpg",
      "image_filename": "1706.02409_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig8.jpg",
      "image_filename": "1706.02409_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
      "image_filename": "1706.02409_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig10.jpg",
      "image_filename": "1706.02409_page0_fig10.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_13",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig11.jpg",
      "image_filename": "1706.02409_page0_fig11.jpg",
      "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
      "context_before": "",
      "context_after": "A Missing Details from the Experiments\n\nA.1 Cross Validation for Picking $\\gamma$\n\nIn this section we show how we used cross validation in our experiments to find $\\gamma$ . For each dataset $S$ , our framework requires that we solve optimization problems of the form $\\mathrm { m i n } _ { \\mathbf { w } } \\ell ( \\mathbf { w } , S ) + \\lambda f ( \\mathbf { w } , S ) +$ $\\gamma | | \\mathbf { w } | | _ { 2 }$ for variable values of $\\lambda$ , where $\\ell ( \\mathbf { w } , S )$ is either MSE (linear regression) or the logistic regression loss. For each $\\lambda$ we picked $\\gamma$ as a function of this $\\lambda$ as follows:",
      "referring_paragraphs": [
        "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case. We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1. For some datasets (e.g. COMPAS and Sentencing), increasing the fairness constraint by decreasing $\\alpha$ has only a mild cost in terms of error. For others (e.g. Communities and Crime, and La",
        "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case.",
        "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02409_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1706.02744": [
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg",
      "image_filename": "1706.02744_page0_fig0.jpg",
      "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .",
      "context_before": "A structural equation model is a set of equations $V _ { i } ~ = ~ f _ { i } ( p a ( V _ { i } ) , N _ { i } )$ , for $i \\in \\{ 1 , \\ldots , n \\}$ , where $p a ( V _ { i } )$ are the parents of $V _ { i }$ , i.e. its direct causes, and the $N _ { i }$ are independent noise variables. We interpret these equations as assignments. Because we assume acyclicity, starting from the roots of the graph, we can recursively compute the other variables, given the noise variables. This leads us to view the structural equation model and its corresponding graph as a data generating model. The predictor $R$ maps inputs, e.g., the features $X$ , to a predicted output. Hence we model it as a childless node, whose parents are its input variables. Finally, note that given the noise variables, a structural equation model entails a unique joint distribution; however, the same joint distribution can usually be entailed by multiple structural equation models corresponding to distinct causal structures.\n\n2 Unresolved discrimination and limitations of observational criteria\n\nTo bear out the limitations of observational criteria, we turn to Pearl’s commentary on claimed gender discrimination in Berkeley college admissions [11, Section 4.5.3]. Bickel [20] had shown earlier that a lower college-wide admission rate for women than for men was explained by the fact that women applied in more competitive departments. When adjusted for department choice, women experienced a slightly higher acceptance rate compared with men. From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$ , see Figure 1. We shall use the term resolving variable for any variable in the causal graph that is influenced by $A$ in a manner that we accept as nondiscriminatory. With this convention, the criterion can be stated as follows.",
      "context_after": "Definition 1 (Unresolved discrimination). A variable $V$ in a causal graph exhibits unresolved discrimination if there exists a directed path from $A$ to $V$ that is not blocked by a resolving variable and $V$ itself is non-resolving.\n\nPearl’s commentary is consistent with what we call the skeptic viewpoint. All paths from the protected attribute $A$ to $R$ are problematic, unless they are justified by a resolving variable. The presence of unresolved discrimination in the predictor $R$ is worrisome and demands further scrutiny. In practice, $R$ is not a priori part of a given graph. Instead it is our objective to construct it as a function of the features $X$ , some of which might be resolving. Hence we should first look for unresolved discrimination in the features. A canonical way to avoid unresolved discrimination in $R$ is to only input the set of features that do not exhibit unresolved discrimination. However, the remaining\n\n1As it is not needed in our work, we do not discuss the graph-theoretic notion of d-separation.",
      "referring_paragraphs": [
        "To bear out the limitations of observational criteria, we turn to Pearl’s commentary on claimed gender discrimination in Berkeley college admissions [11, Section 4.5.3]. Bickel [20] had shown earlier that a lower college-wide admission rate for women than for men was explained by the fact that women applied in more competitive departments. When adjusted for department choice, women experienced a slightly higher acceptance rate compared with men. From the causal point of view, what matters is the",
        "From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$ , see Figure 1.",
        "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg",
      "image_filename": "1706.02744_page0_fig1.jpg",
      "caption": "The definition of unresolved discrimination in a predictor has some interesting special cases worth highlighting.",
      "context_before": "1As it is not needed in our work, we do not discuss the graph-theoretic notion of d-separation.\n\nfeatures might be affected by non-resolving and resolving variables. In Section 4 we investigate whether one can exclusively remove unresolved discrimination from such features. A related notion of “explanatory features” in a non-causal setting was introduced in [21].\n\nThe definition of unresolved discrimination in a predictor has some interesting special cases worth highlighting. If we take the set of resolving variables to be empty, we intuitively get a causal analog of demographic parity. No directed paths from $A$ to $R$ are allowed, but $A$ and $R$ can still be statistically dependent. Similarly, if we choose the set of resolving variables to be the singleton set $\\{ Y \\}$ containing the true outcome, we obtain a causal analog of equalized odds where strict independence is not necessary. The causal intuition implied by “the protected attribute should not affect the prediction”, and “the protected attribute can only affect the prediction when the information comes through the true label”, is neglected by (con-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig2.jpg",
      "image_filename": "1706.02744_page0_fig2.jpg",
      "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
      "context_before": "",
      "context_after": "ditional) statistical independences $A \\perp \\perp R$ , and $A \\bot \\bot R \\vert Y$ , but well captured by only considering dependences mitigated along directed causal paths.\n\nWe will next show that observational criteria are fundamentally unable to determine whether a predictor exhibits unresolved discrimination or not. This is true even if the predictor is Bayes optimal. In passing, we also note that fairness criteria such as equalized odds may or may not exhibit unresolved discrimination, but this is again something an observational criterion cannot determine.\n\nTheorem 1. Given a joint distribution over the protected attribute A, the true label Y , and some features $X _ { 1 } , \\ldots , X _ { n }$ , in which we have already specified the resolving variables, no observational criterion can generally determine whether the Bayes optimal unconstrained predictor or the Bayes optimal equal odds predictor exhibit unresolved discrimination.",
      "referring_paragraphs": [
        "The two graphs in Figure 2 are taken from [2], which we here reinterpret in the causal context to prove Theorem 1. We point out that there is an established set of conditions under which unresolved discrimination can, in fact, be determined from observational data. Note that the two graphs are not Markov equivalent. Therefore, to obtain the same joint distribution we must violate a condition called faithfulness.2 We later argue that violation of faithfulness is by no means pathological, but emer",
        "Proof. Let us consider the two graphs in Figure 2. First, we show that these graphs can generate the same joint distribution $\\mathbb { P } ( A , Y , X _ { 1 } , X _ { 2 } , R ^ { * } )$ for the Bayes optimal unconstrained predictor $R ^ { * }$ .",
        "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ .",
        "Let us consider the two graphs in Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg",
      "image_filename": "1706.02744_page0_fig3.jpg",
      "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P R$ to cancel the influence along $P $ $X R$ in the intervened graph.",
      "context_before": "We now work out a formal procedure to solve this task under specific assumptions and simultaneously illustrate it in a fully linear example, i.e. the structural equations are given by\n\n$$ P = \\alpha_ {P} A + N _ {P}, \\qquad X = \\alpha_ {X} A + \\beta P + N _ {X}, \\qquad R _ {\\theta} = \\lambda_ {P} P + \\lambda_ {X} X. $$\n\nNote that we choose linear functions parameterized by $\\theta \\ = \\ ( \\lambda _ { P } , \\lambda _ { X } )$ as the hypothesis class for $R _ { \\theta } ( P , X )$ .",
      "context_after": "",
      "referring_paragraphs": [
        "While presenting the general procedure, we illustrate each step in the example shown in Figure 3. A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent. Finally, the proxy also has an effect on the features $X$ and the predictor $R$ is a function of $P$ and $X$ . Given labeled training data, our task is to find a good predictor that exhibi",
        "1. Intervene on $P$ by removing all incoming arrows and replacing the structural equation for $P$ by $P = p$ . For the example in Figure 3,",
        "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations",
        "Motivated by the algorithm to avoid proxy discrimination, we discuss some natural variants of the notion in this section that connect our interventional approach to individual fairness and other proposed criteria. We consider a generic graph structure as shown on the left in Figure 5. The proxy $P$ and the features $X$ could be multidimensional. The empty circle in the middle represents any number of variables forming a DAG that respects the drawn arrows. Figure 3 is an example thereof. All dash",
        "Note that in general $\\mathbb { E } [ X | d o ( P ) ] \\neq \\mathbb { E } [ X | P ]$ . Since in practice we only have observational data from $\\tilde { \\mathcal { G } }$ , one cannot simply build a predictor based on the “regressed out features” ${ \\tilde { X } } : = X -$ $\\mathbb { E } [ X | P ]$ to avoid proxy discrimination. In the scenario of Figure 3, the direct effect of $P$ on $X$ along the arrow $P X$ in the left graph cannot be estimated by $\\mathbb { E } [ X | P ]$ , because of the comm",
        "While presenting the general procedure, we illustrate each step in the example shown in Figure 3.",
        "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.",
        "For the example in Figure 3,",
        "Figure 3 is an example thereof.",
        "In the scenario of Figure 3, the direct effect of $P$ on $X$ along the arrow $P  X$ in the left graph cannot be estimated by $\\mathbb { E } [ X | P ]$ , because of the common confounder $A$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg",
      "image_filename": "1706.02744_page0_fig4.jpg",
      "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .",
      "context_before": "",
      "context_after": "We will refer to the terminal ancestors of a node $V$ in a causal graph $\\mathcal { D }$ , denoted by $t a ^ { \\mathcal { D } } ( V )$ , which are those ancestors of $V$ that are also root nodes of $\\mathcal { D }$ . Moreover, in the procedure we clarify the notion of expressibility, which is an assumption about the relation of the given structural equations and the hypothesis class we choose for $R _ { \\theta }$ .\n\nProposition 2. If there is a choice of parameters $\\theta _ { 0 }$ such that $R _ { \\theta _ { 0 } } ( P , X )$ is constant with respect to its first argument and the structural equations are expressible, the following procedure returns a predictor from the given hypothesis class that exhibits no proxy discrimination and is non-trivial in the sense that it can make use of features that exhibit potential proxy discrimination.\n\n1. Intervene on $P$ by removing all incoming arrows and replacing the structural equation for $P$ by $P = p$ . For the example in Figure 3,",
      "referring_paragraphs": [
        "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations",
        "1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$ , see Figure 4. In the example we find",
        "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right).",
        "Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination.\n\n1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$ , see Figure 4. In the example we find"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg",
      "image_filename": "1706.02744_page0_fig5.jpg",
      "caption": "In general, the non-discrimination constraint (4) is by construction just $\\mathbb { P } ( R | d o ( P ~ = ~ p ) ) ~ =$ $\\mathbb { P } ( \\boldsymbol { \\bar { R } } | d o ( \\boldsymbol { P } = \\boldsymbol { p ^ { \\prime }",
      "context_before": "$$ R _ {\\theta} = - \\lambda_ {X} \\beta P + \\lambda_ {X} X = \\lambda_ {X} (X - \\beta P), $$\n\nwith the free parameter $\\lambda _ { X } \\in \\mathbb { R }$ .\n\nIn general, the non-discrimination constraint (4) is by construction just $\\mathbb { P } ( R | d o ( P ~ = ~ p ) ) ~ =$ $\\mathbb { P } ( \\boldsymbol { \\bar { R } } | d o ( \\boldsymbol { P } = \\boldsymbol { p ^ { \\prime } } ) )$ , coinciding with Definition 3. Thus Proposition 2 holds by construction of the procedure. The choice of $\\theta _ { 0 }$ strongly influences the non-discrimination constraint. However, as the example shows, it allows $R _ { \\theta }$ to exploit features that exhibit potential proxy discrimination.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_7",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig6.jpg",
      "image_filename": "1706.02744_page0_fig6.jpg",
      "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
      "context_before": "",
      "context_after": "4.2 Avoiding unresolved discrimination\n\nWe proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations\n\n$$ E = \\alpha_ {E} A + N _ {E}, \\qquad X = \\alpha_ {X} A + \\beta E + N _ {X}, \\qquad R _ {\\theta} = \\lambda_ {E} E + \\lambda_ {X} X. $$",
      "referring_paragraphs": [
        "Motivated by the algorithm to avoid proxy discrimination, we discuss some natural variants of the notion in this section that connect our interventional approach to individual fairness and other proposed criteria. We consider a generic graph structure as shown on the left in Figure 5. The proxy $P$ and the features $X$ could be multidimensional. The empty circle in the middle represents any number of variables forming a DAG that respects the drawn arrows. Figure 3 is an example thereof. All dash",
        "For an analysis of proxy discrimination, we need the structural equations for $P , X , R$ in Figure 5",
        "For convenience, we will use the notation $t a _ { P } ^ { \\mathcal { G } } ( X ) \\ : = \\ t a ^ { \\mathcal { G } } ( X ) \\setminus \\{ P \\}$ . We can find $f _ { X } , f _ { R }$ from $\\hat { f } _ { X } , \\hat { f } _ { R }$ by first rewriting the functions in terms of root nodes of the intervened graph, shown on the right side of Figure 5, and then assigning the overall dependence on $P$ to the first argument.",
        "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
        "We consider a generic graph structure as shown on the left in Figure 5.",
        "Individual proxy discrimination aims at comparing examples with the same features $X$ , for different values of $P$ . Note that this can be individuals with different values for the unobserved non-feature variables. A true individual-level comparison of the form “What would have happened to me, if I had always belonged to another group” is captured by counterfactuals and discussed in [15, 19].\n\nFor an analysis of proxy discrimination, we need the structural equations for $P , X , R$ in Figure 5",
        "We can find $f _ { X } , f _ { R }$ from $\\hat { f } _ { X } , \\hat { f } _ { R }$ by first rewriting the functions in terms of root nodes of the intervened graph, shown on the right side of Figure 5, and then assigning the overall dependence on $P$ to the first argument."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig7.jpg",
      "image_filename": "1706.02744_page0_fig7.jpg",
      "caption": "Proof of Corollary 1",
      "context_before": "$$ \\begin{array}{l} \\mathbb {E} [ X \\mid d o (P) ] = \\mathbb {E} \\left[ g _ {X} \\left(t a _ {P} ^ {\\mathcal {G}} (X)\\right) + \\mu_ {X} P \\mid d o (P) \\right] \\\\ = \\underbrace {\\mathbb {E} [ g _ {X} (t a _ {P} ^ {\\mathcal {G}} (X)) \\mid d o (P) ]} _ {= 0} + \\mathbb {E} [ \\mu_ {X} P \\mid d o (P) ] \\\\ = \\mu_ {X} P. \\\\ \\end{array} $$\n\n$$ X - \\mathbb {E} [ X \\mid d o (P) ] = g _ {X} \\left(t a _ {P} ^ {\\mathcal {G}} (X)\\right) $$\n\nis clearly constant w.r.t. to $P$ .",
      "context_after": "Proof of Corollary 1\n\nCorollary. Under the assumptions of Theorem 2, if all directed paths from any ancestor of $P$ to $X$ in the graph $\\mathcal { G }$ are blocked by $P$ , then any predictor based on the adjusted features ${ \\tilde { X } } : = X - \\mathbb { E } [ X | P ]$ exhibits no proxy discrimination and can be learned from the observational distribution $\\mathbb { P } ( P , X , Y )$ when target labels $Y$ are available.\n\nProof. Let $Z$ denote the set of ancestors of $P$ . Under the given assumptions $Z \\cap t a ^ { \\mathcal { G } } ( X ) = \\varnothing$ , because in $\\mathcal { G }$ all arrows into $P$ are removed, which breaks all directed paths from any variable in $Z$ to $X$ by assumption. Hence the distribution of $X$ under an intervention on $P$ in $\\tilde { \\mathcal { G } }$ , where the influence of potential ancestors of $P$ on $X$ that does not go through $P$ would not be affected, is the same as simply conditioning on $P$ . Therefore $\\mathbb { E } [ X | d o ( \\bar { P } ) ] = \\bar { \\mathbb { E } } [ X | P ]$ , which can be computed from the joint observational distribution, since we observe $X$ and $P$ as generated by $\\tilde { \\mathcal { G } }$ . □",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig8.jpg",
      "image_filename": "1706.02744_page0_fig8.jpg",
      "caption": "Additional statements",
      "context_before": "Proof. We directly test the definition of proxy discrimination in expectation using the linearity of the expectation\n\n$$ \\begin{array}{l} \\mathbb {E} [ R \\mid d o (P = p) ] = \\mathbb {E} [ \\lambda (X - \\mathbb {E} [ X \\mid d o (P) ]) + c \\mid d o (P = p) ] \\\\ = \\lambda (\\mathbb {E} [ X \\mid d o (P = p) ] - \\mathbb {E} [ X \\mid d o (P = p) ]) + c \\\\ = c. \\\\ \\end{array} $$\n\nThis holds for any $p$ , hence proxy discrimination in expectation is achieved.",
      "context_after": "Additional statements\n\nHere we provide an additional statement that is a first step towards the “opposite direction” of Theorem 2, i.e. whether we can infer information about the structural equations, when we are given a predictor of a special form that does not exhibit proxy discrimination.\n\nTheorem. Let the influence of $P$ on $X$ be additive and linear and let the influence of $P$ on the argument of R be additive linear, i.e.",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig9.jpg",
      "image_filename": "1706.02744_page0_fig9.jpg",
      "caption": "$$ R = X + \\mu_ {R} P = X - \\mathbb {E} [ X \\mid d o (P) ]. $$",
      "context_before": "$$ \\mathbb {E} [ X \\mid d o (P) ] = \\mu_ {X} P, $$\n\nunder the given assumptions any predictor that avoids proxy discrimination is simply\n\n$$ R = X + \\mu_ {R} P = X - \\mathbb {E} [ X \\mid d o (P) ]. $$",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1706.02744_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1707.00574": [
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig0.jpg",
      "image_filename": "1707.00574_page0_fig0.jpg",
      "caption": "(a) (c)",
      "context_before": "The model has two parameters: $\\beta$ regulates the importance of popularity over quality and thus represents the popularity bias of the algorithm. When $\\beta = 0$ , choices are entirely driven by quality (no popularity bias). When $\\beta = 1$ , only popularity choices are allowed, yielding a type of Polya urn model [22]. The parameter $\\alpha$ can be thought of as an exploration cost. A large $\\alpha$ implies that users are likely to consider only one or a few most popular items, whereas a small $\\alpha$ allows users to explore less popular choices. In the limit $\\alpha 0$ , the selection no longer depends on popularity, yielding the uniform probability across the discrete set of $N$ items.\n\nWe vary $\\beta$ systematically in $\\lfloor 0 , 1 \\rfloor$ and consider different values of $\\alpha$ between 0 and 3. We simulate 1,000 realizations for each parameter configuration. In each realization we perform $T = 1 0 ^ { 6 }$ selections using Eqs. 1 and 2 and store the final popularity values.\n\nWe characterize two properties of the final distribution of popularity $\\{ p _ { i } \\} _ { i = 1 } ^ { N }$ with respect to the intrinsic quality distribution $\\left\\{ q _ { i } \\right\\} _ { i = 1 } ^ { N }$ . For brevity, we pose $p _ { i } ~ = ~ p _ { i } ( T )$ The first quantity we meand the second property ure is the average quality is the faithfulness of the a $\\bar { q } \\ =$ $\\textstyle \\sum _ { i = 1 } ^ { N } p _ { i } q _ { i } / \\sum _ { i = 1 } ^ { N } p _ { i }$ $\\tau$ faithfulness using Kendall’s rank correlation between popularity and quality [17]. We can derive the values of both properties in the extreme cases of maximum or no popularity bias. When $\\beta = 0$ , selections are made exclusively on the basis of quality and therefore one expects $p _ { i } \\to q _ { i }$ as $T \\to \\infty$ . The rankings by quality and popularity are therefore perfectly aligned, and $\\tau = 1$ . In the limit of large $N$ we can make a continuous approximation $\\begin{array} { r } { \\bar { q } \\int _ { 0 } ^ { 1 } q ^ { 2 } d q / \\int _ { 0 } ^ { 1 } q d q = 2 / 3 } \\end{array}$ . When $\\beta = 1$ , quality never enters the picture and any permutation of the items is an equally likely popularity ranking, which translates into $\\tau = 0$ . Also $p _ { i } \\to 1 / N$ and in the continuous approximation $\\begin{array} { r } { \\bar { q } \\int _ { 0 } ^ { 1 } q d q = 1 / 2 } \\end{array}$ . What happens for intermediate values of popularity bias is harder to predict. The question we ask is whether it is possible to leverage some popularity bias to obtain a higher",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg",
      "image_filename": "1707.00574_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig2.jpg",
      "image_filename": "1707.00574_page0_fig2.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig3.jpg",
      "image_filename": "1707.00574_page0_fig3.jpg",
      "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.",
      "context_before": "",
      "context_after": "average quality, even at the cost of decreasing the algorithm’s faithfulness.\n\nThe dependence of the average quality $q$ on the popularity bias $\\beta$ and exploration cost $\\alpha$ is shown in Fig. 1(a,b). We observe that if $\\alpha$ is small, popularity bias only hinders quality; the best average quality is obtained for $\\beta = 0$ . However, if $\\alpha$ is sufficiently large, an optimal value of $q$ is attained for $\\beta > 0$ . The location of the maximum, $\\ddot { \\beta } = \\arg \\operatorname* { m a x } _ { \\beta } \\bar { q } ( \\beta )$ , depends non-trivially on the exploration cost $\\alpha$ . When popularity-based choices are strongly focused on the top-ranked items ( $\\alpha > 1$ ), $\\hat { \\beta }$ is a decreasing function of $\\alpha$ . Overall, the highest value of $q$ is observed for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ .\n\nIn Fig. 1(c,d) we show the behavior of faithfulness $\\tau$ as a function of $\\alpha$ and $\\beta$ . We observe that popularity bias always hinders the algorithm’s faithfulness, however the effect is small for small $\\beta$ . This suggests that in the regime where popularity bias improves quality on average, there is a small price to be paid in terms of over-represented low-quality items and under-represented higherquality items. If these mistakes occur in the low-quality range, they will not affect the average quality significantly. In general, the algorithm can retain",
      "referring_paragraphs": [
        "The dependence of the average quality $q$ on the popularity bias $\\beta$ and exploration cost $\\alpha$ is shown in Fig. 1(a,b). We observe that if $\\alpha$ is small, popularity bias only hinders quality; the best average quality is obtained for $\\beta = 0$ . However, if $\\alpha$ is sufficiently large, an optimal value of $q$ is attained for $\\beta > 0$ . The location of the maximum, $\\ddot { \\beta } = \\arg \\operatorname* { m a x } _ { \\beta } \\bar { q } ( \\beta )$ , depends non-trivially on the ",
        "In Fig. 1(c,d) we show the behavior of faithfulness $\\tau$ as a function of $\\alpha$ and $\\beta$ . We observe that popularity bias always hinders the algorithm’s faithfulness, however the effect is small for small $\\beta$ . This suggests that in the regime where popularity bias improves quality on average, there is a small price to be paid in terms of over-represented low-quality items and under-represented higherquality items. If these mistakes occur in the low-quality range, they will not affe",
        "Figure 1: Effects of popularity bias on average quality and faithfulness.."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg",
      "image_filename": "1707.00574_page0_fig4.jpg",
      "caption": "In Fig. 1(c,d) we show the behavior of faithfulness $\\tau$ as a function of $\\alpha$ and $\\beta$ . We observe that popularity bias always hinders the algorithm’s faithfulness, however the effect is small for small $\\beta",
      "context_before": "average quality, even at the cost of decreasing the algorithm’s faithfulness.\n\nThe dependence of the average quality $q$ on the popularity bias $\\beta$ and exploration cost $\\alpha$ is shown in Fig. 1(a,b). We observe that if $\\alpha$ is small, popularity bias only hinders quality; the best average quality is obtained for $\\beta = 0$ . However, if $\\alpha$ is sufficiently large, an optimal value of $q$ is attained for $\\beta > 0$ . The location of the maximum, $\\ddot { \\beta } = \\arg \\operatorname* { m a x } _ { \\beta } \\bar { q } ( \\beta )$ , depends non-trivially on the exploration cost $\\alpha$ . When popularity-based choices are strongly focused on the top-ranked items ( $\\alpha > 1$ ), $\\hat { \\beta }$ is a decreasing function of $\\alpha$ . Overall, the highest value of $q$ is observed for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ .\n\nIn Fig. 1(c,d) we show the behavior of faithfulness $\\tau$ as a function of $\\alpha$ and $\\beta$ . We observe that popularity bias always hinders the algorithm’s faithfulness, however the effect is small for small $\\beta$ . This suggests that in the regime where popularity bias improves quality on average, there is a small price to be paid in terms of over-represented low-quality items and under-represented higherquality items. If these mistakes occur in the low-quality range, they will not affect the average quality significantly. In general, the algorithm can retain",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig5.jpg",
      "image_filename": "1707.00574_page0_fig5.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_7",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig6.jpg",
      "image_filename": "1707.00574_page0_fig6.jpg",
      "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.",
      "context_before": "",
      "context_after": "faithfulness in the presence of moderate popularity bias, even when the average quality is poor. When $\\alpha$ is large, $\\tau$ remains high over a wide range of popularity bias values. In this regime, the preference for popular items is so strong that the vast majority of items (those that do not make the top of the ranking early on) are chosen only by quality-based choice, and therefore their relative ranking perfectly reflects quality. The average quality is however hindered by the topranked items, which are selected via popularity irrespective of low quality.\n\nIn summary, our results show that some popularity bias, together with a mild exploration cost, can produce excellent average quality with minimal loss in faithfulness. Optimizing the average quality of consumed items requires a careful balancing of quality- and popularity-based choices as well as a fine tuning of the focus on the most popular items. For a given value of $\\beta$ , if $\\alpha$ is too low, the popularity bias hinders quality because it fails to enhance the signal provided by the quality-based choices. To understand why quality is also hindered by the popularity bias when $\\alpha$ is too high, consider the evolution of the average quality in simulations of the model for different values of $\\alpha$ and $\\beta$ , shown in Fig. 2. By focusing only on the top ranked items ( $\\alpha = 2$ ), the system converges prematurely to a sub-optimal ranking, producing lower quality on average. In other words, with insufficient exploration the popularity bias risks enhancing initial noise rather than the quality-based signal. With more exploration ( $\\alpha = 1$ ), $q$ continues to grow. The premature convergence to sub-optimal ranking caused by excessive popularity bias is also reflected in the increased variance of the average quality across runs of the model for larger values of both $\\alpha$ and $\\beta$ . This is consistent with the increase in variance of outcomes observed in other studies [29, 16].\n\nCultural markets like social media and the music and fashion industry account for multi-billion businesses with worldwide social and economic impact [25]. Success in these markets may strongly depend on structural or subjective features, like competition for limited attention [34, 26]. The inherent quality of cultural products is often difficult to establish, therefore relying on measurable quantitative features like the popularity of an item is hugely advantageous in terms of cognitive processing and scalability.",
      "referring_paragraphs": [
        "In summary, our results show that some popularity bias, together with a mild exploration cost, can produce excellent average quality with minimal loss in faithfulness. Optimizing the average quality of consumed items requires a careful balancing of quality- and popularity-based choices as well as a fine tuning of the focus on the most popular items. For a given value of $\\beta$ , if $\\alpha$ is too low, the popularity bias hinders quality because it fails to enhance the signal provided by the qu",
        "Figure 2: Temporal evolution of average quality."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.00574_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1707.09457": [
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg",
      "image_filename": "1707.09457_page0_fig0.jpg",
      "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
      "context_before": "Our analysis reveals that over $45 \\%$ and $37 \\%$ of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1. For example, as seen in Figure 1, the cooking activity in imSitu is a heavily biased verb. Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by $5 . 0 \\%$ for vSRL, and $3 . 6 \\%$ in MLC.\n\narXiv:1707.09457v1 [cs.AI] 29 Jul 2017\n\n1To simplify our analysis, we only consider a gender binary as perceived by annotators in the datasets. We recognize that a more fine-grained analysis would be needed for deployment in a production system. Also, note that the proposed approach can be applied to other NLP tasks and other variables such as identification with a racial or ethnic group.",
      "context_after": "To mitigate the role of bias amplification when training models on biased corpora, we propose a novel constrained inference framework, called RBA, for Reducing Bias Amplification in predictions. Our method introduces corpus-level constraints so that gender indicators co-occur no more often together with elements of the prediction task than in the original training distribution. For example, as seen in Figure 1, we would like noun man to occur in the agent role of the cooking as often as it occurs in the imSitu training set when evaluating on a development set. We combine our calibration constraint with the original structured predictor and use Lagrangian relaxation (Korte and Vygen, 2008; Rush and Collins, 2012) to reweigh bias creating factors in the original model.\n\nWe evaluate our calibration method on imSitu vSRL and COCO MLC and find that in both instances, our models substantially reduce bias amplification. For vSRL, we reduce the average magnitude of bias amplification by $40 . 5 \\%$ . For MLC, we are able to reduce the average magnitude of bias amplification by $4 7 . 5 \\%$ . Overall, our calibration methods do not affect the performance of the underlying visual system, while substantially reducing the reliance of the system on socially biased correlations2.\n\nAs intelligence systems start playing important roles in our daily life, ethics in artificial intelligence research has attracted significant interest. It is known that big-data technologies sometimes inadvertently worsen discrimination due to implicit biases in data (Podesta et al., 2014). Such issues have been demonstrated in various learning systems, including online advertisement systems (Sweeney, 2013), word embedding models (Bolukbasi et al., 2016; Caliskan et al., 2017), online news (Ross and Carter, 2011), web search (Kay et al., 2015), and credit score (Hardt et al., 2016). Data collection biases have been discussed in the context of creating image corpus (Misra et al., 2016; van Miltenburg, 2016) and text corpus (Gordon and Van Durme, 2013; Van Durme, 2010). In contrast, we show that given a gender biased corpus, structured models such as conditional random fields, amplify the bias.",
      "referring_paragraphs": [
        "tics from images and require large quantities of labeled data, predominantly retrieved from the web. Methods often combine structured prediction and deep learning to model correlations between labels and images to make judgments that otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking. Yet such methods run the risk of discovering and exploiting societal bia",
        "Our analysis reveals that over $45 \\%$ and $37 \\%$ of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1. For example, as seen in Figure 1, the cooking activity in imSitu is a heavily biased verb. Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by $5 . 0 \\%$ for vSRL, and $3 . 6 \\%$ in MLC.",
        "To mitigate the role of bias amplification when training models on biased corpora, we propose a novel constrained inference framework, called RBA, for Reducing Bias Amplification in predictions. Our method introduces corpus-level constraints so that gender indicators co-occur no more often together with elements of the prediction task than in the original training distribution. For example, as seen in Figure 1, we would like noun man to occur in the agent role of the cooking as often as it occur",
        "role in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in Table 1. We also provide setup details for our calibration method.",
        "For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking.",
        "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset.",
        "For example, one can represent the problem as an\n\nTable 1: Statistics for the two recognition problems.",
        "In this section, we provide details about the two visual recognition tasks we evaluated for bias: visual semantic role labeling (vSRL), and multi-label classification (MLC). We focus on gender, defining $G = \\{ \\mathrm { m a n } , \\mathrm { w o m a n } \\}$ and focus on the agent\n\nrole in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in Table 1. We also provide setup details for our calibration method."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/ec39e21fb44a922a487a63da2ae6305d9fa5edf16eb972b8a32abe434971542b.jpg",
      "image_filename": "ec39e21fb44a922a487a63da2ae6305d9fa5edf16eb972b8a32abe434971542b.jpg",
      "caption": "Table 1: Statistics for the two recognition problems. In vSRL, we consider gender bias relating to verbs, while in MLC we consider the gender bias related to objects.",
      "context_before": "3A sufficiently large sample of test instances must be used so that bias statistics can be estimated. In this work we use the entire test set for each respective problem.\n\n4We use $r$ to refer to a combination of role and noun. For example, one possible value indicates an agent is a woman.\n\n5For the sake of simplicity, we abuse the notations and use $_ { i }$ to represent both input and data index.",
      "context_after": "integer linear program and solve it using an offthe-shelf solver (e.g., Gurobi (Gurobi Optimization, 2016)). However, Eq. (3) involves all test instances. Solving a constrained optimization problem on such a scale is difficult. Therefore, we consider relaxing the constraints and solve Eq. (3) using a Lagrangian relaxation technique (Rush and Collins, 2012). We introduce a Lagrangian multiplier $\\lambda _ { j } \\geq 0$ for each corpus-level constraint. The Lagrangian is\n\n$$ \\begin{array}{l} L (\\lambda , \\{y ^ {i} \\}) = \\\\ \\sum_ {i} f _ {\\theta} \\left(y ^ {i}\\right) - \\sum_ {j = 1} ^ {l} \\lambda_ {j} \\left(A _ {j} \\sum_ {i} y ^ {i} - b _ {j}\\right), \\tag {4} \\\\ \\end{array} $$\n\nwhere all the $\\lambda _ { j } \\ge 0 , \\forall j \\in \\left\\{ 1 , \\ldots , l \\right\\}$ . The solution of Eq. (3) can be obtained by the following iterative procedure:",
      "referring_paragraphs": [
        "tics from images and require large quantities of labeled data, predominantly retrieved from the web. Methods often combine structured prediction and deep learning to model correlations between labels and images to make judgments that otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking. Yet such methods run the risk of discovering and exploiting societal bia",
        "Our analysis reveals that over $45 \\%$ and $37 \\%$ of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1. For example, as seen in Figure 1, the cooking activity in imSitu is a heavily biased verb. Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by $5 . 0 \\%$ for vSRL, and $3 . 6 \\%$ in MLC.",
        "To mitigate the role of bias amplification when training models on biased corpora, we propose a novel constrained inference framework, called RBA, for Reducing Bias Amplification in predictions. Our method introduces corpus-level constraints so that gender indicators co-occur no more often together with elements of the prediction task than in the original training distribution. For example, as seen in Figure 1, we would like noun man to occur in the agent role of the cooking as often as it occur",
        "role in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in Table 1. We also provide setup details for our calibration method.",
        "For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking.",
        "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset.",
        "For example, one can represent the problem as an\n\nTable 1: Statistics for the two recognition problems.",
        "In this section, we provide details about the two visual recognition tasks we evaluated for bias: visual semantic role labeling (vSRL), and multi-label classification (MLC). We focus on gender, defining $G = \\{ \\mathrm { m a n } , \\mathrm { w o m a n } \\}$ and focus on the agent\n\nrole in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in Table 1. We also provide setup details for our calibration method."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg",
      "image_filename": "1707.09457_page0_fig1.jpg",
      "caption": "(a) Bias analysis on imSitu vSRL",
      "context_before": "6.2 Multilabel Classification\n\nMS-COCO is gender biased In Figure 2(b) along the x-axis, similarly to imSitu, we analyze bias of objects in MS-COCO with respect to males. MS-COCO is even more heavily biased toward men than imSitu, with $8 6 . 6 \\%$ of objects biased toward men, but with smaller average magnitude, 0.65. One third of the nouns are extremely biased toward males, $3 7 . 9 \\%$ of nouns favor men with a bias of at least 0.7. Some problematic examples include kitchen objects such as knife, fork, or spoon being more biased toward woman. Outdoor recreation related objects such tennis racket, snowboard and boat tend to be more biased toward men.\n\n6In this gender binary, bias toward woman is $1 -$ the bias toward man",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig2.jpg",
      "image_filename": "1707.09457_page0_fig2.jpg",
      "caption": "(b) Bias analysis on MS-COCO MLC Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.",
      "context_before": "",
      "context_after": "Training on MS-COCO amplifies bias In Figure 2(b), along the y-axis, we show the ratio of man $\\%$ of both gender) in predictions on an unseen development set. The mean bias amplification across all objects is 0.036, with $6 5 . 6 7 \\%$ of nouns exhibiting amplification. Larger training bias again tended to indicate higher bias amplification: biased objects with training bias over 0.7 had mean amplification of 0.081. Again, several problematic biases have now been amplified. For example, kitchen categories already biased toward females such as knife, fork and spoon have all been amplified. Technology oriented categories initially biased toward men such as keyboard and mouse have each increased their bias toward males by over 0.100.\n\nWe confirmed our hypothesis that (a) both the im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased and that (b) models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data. Furthermore, across both datasets, we showed that the degree of bias amplification was related to the size of the initial bias, with highly biased object and verb categories exhibiting more bias amplification. Our results demonstrate that care needs be taken in deploying such uncalibrated systems otherwise they could not only reinforce existing social bias but actually make them worse.\n\n7 Calibration Results",
      "referring_paragraphs": [
        "imSitu is gender biased In Figure 2(a), along the x-axis, we show the male favoring bias of im-Situ verbs. Overall, the dataset is heavily biased toward male agents, with $6 4 . 6 \\%$ of verbs favoring a male agent by an average bias of 0.707 (roughly 3:1 male). Nearly half of verbs are extremely biased in the male or female direction: $4 6 . 9 5 \\%$ of verbs favor a gender with a bias of at least 0.7.6 Figure 2(a) contains several activity labels revealing problematic biases. For example, shopp",
        "Training on imSitu amplifies bias In Figure 2(a), along the y-axis, we show the ratio of male agents $\\%$ of total people) in predictions on an unseen development set. The mean bias amplification in the development set is high, 0.050 on average, with $4 5 . 7 5 \\%$ of verbs exhibiting amplification. Biased verbs tend to have stronger amplification: verbs with training bias over 0.7 in either the male or female direction have a mean amplification of 0.072. Several already problematic biases have ",
        "MS-COCO is gender biased In Figure 2(b) along the x-axis, similarly to imSitu, we analyze bias of objects in MS-COCO with respect to males. MS-COCO is even more heavily biased toward men than imSitu, with $8 6 . 6 \\%$ of objects biased toward men, but with smaller average magnitude, 0.65. One third of the nouns are extremely biased toward males, $3 7 . 9 \\%$ of nouns favor men with a bias of at least 0.7. Some problematic examples include kitchen objects such as knife, fork, or spoon being more ",
        "Training on MS-COCO amplifies bias In Figure 2(b), along the y-axis, we show the ratio of man $\\%$ of both gender) in predictions on an unseen development set. The mean bias amplification across all objects is 0.036, with $6 5 . 6 7 \\%$ of nouns exhibiting amplification. Larger training bias again tended to indicate higher bias amplification: biased objects with training bias over 0.7 had mean amplification of 0.081. Again, several problematic biases have now been amplified. For example, kitchen",
        "We test our methods for reducing bias amplification in two problem settings: visual semantic role labeling in the imSitu dataset (vSRL) and multilabel image classification in MS-COCO (MLC). In all settings we derive corpus constraints using the training set and then run our calibration method in batch on either the development or testing set. Our results are summarized in Table 2 and Figure 3.",
        "Our quantitative results are summarized in the first two sections of Table 2. On the development set, the number of verbs whose bias exceed the original bias by over $5 \\%$ decreases $3 0 . 5 \\%$ (Viol.). Overall, we are able to significantly reduce bias amplification in vSRL by $52 \\%$ on the development set (Amp. bias). We evaluate the underlying recognition performance using the standard measure in vSRL: top-1 semantic role accuracy, which tests how often the correct verb was predicted and th",
        "Our quantitative results on MS-COCO RBA are summarized in the last two sections of Table 2. Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.). Bias amplification was reduced by $3 1 . 3 \\%$ on the development set (Amp. bias). The underlying recognition system was evaluated by the standard measure: top-1 mean average precision, the precision averaged across object categories. Our calibration method results i",
        "imSitu is gender biased In Figure 2(a), along the x-axis, we show the male favoring bias of im-Situ verbs.",
        "MS-COCO is gender biased In Figure 2(b) along the x-axis, similarly to imSitu, we analyze bias of objects in MS-COCO with respect to males.",
        "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC.",
        "Our quantitative results are summarized in the first two sections of Table 2.",
        "(%)</td></tr><tr><td colspan=\"4\">vSRL: Development Set</td></tr><tr><td>CRF</td><td>154</td><td>0.050</td><td>24.07</td></tr><tr><td>CRF + RBA</td><td>107</td><td>0.024</td><td>23.97</td></tr><tr><td colspan=\"4\">vSRL: Test Set</td></tr><tr><td>CRF</td><td>149</td><td>0.042</td><td>24.14</td></tr><tr><td>CRF + RBA</td><td>102</td><td>0.025</td><td>24.01</td></tr><tr><td colspan=\"4\">MLC: Development Set</td></tr><tr><td>CRF</td><td>40</td><td>0.032</td><td>45.27</td></tr><tr><td>CRF + RBA</td><td>24</td><td>0.022</td><td>45.19</td></tr><tr><td colspan=\"4\">MLC: Test Set</td></tr><tr><td>CRF</td><td>38</td><td>0.040</td><td>45.40</td></tr><tr><td>CRF + RBA</td><td>16</td><td>0.021</td><td>45.38</td></tr></table>\n\nTable 2: Number of violated constraints, mean amplified bias, and test performance before and after calibration using RBA.",
        "Our quantitative results on MS-COCO RBA are summarized in the last two sections of Table 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig3.jpg",
      "image_filename": "1707.09457_page0_fig3.jpg",
      "caption": "(a) Bias analysis on imSitu vSRL without RBA",
      "context_before": "7.1 Visual Semantic Role Labeling\n\nOur quantitative results are summarized in the first two sections of Table 2. On the development set, the number of verbs whose bias exceed the original bias by over $5 \\%$ decreases $3 0 . 5 \\%$ (Viol.). Overall, we are able to significantly reduce bias amplification in vSRL by $52 \\%$ on the development set (Amp. bias). We evaluate the underlying recognition performance using the standard measure in vSRL: top-1 semantic role accuracy, which tests how often the correct verb was predicted and the noun value was correctly assigned to a semantic role. Our calibration method results in a negligible decrease in performance (Perf.). In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\%$ .\n\nFigure 3(e) demonstrates that across all initial training bias, RBA is able to reduce bias amplification. In general, RBA struggles to remove bias amplification in areas of low initial training bias,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig4.jpg",
      "image_filename": "1707.09457_page0_fig4.jpg",
      "caption": "(b) Bias analysis on MS-COCO MLC without RBA",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig5.jpg",
      "image_filename": "1707.09457_page0_fig5.jpg",
      "caption": "(c) Bias analysis on imSitu vSRL with RBA",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg",
      "image_filename": "1707.09457_page0_fig6.jpg",
      "caption": "(d) Bias analysis on MS-COCO MLC with RBA",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig7.jpg",
      "image_filename": "1707.09457_page0_fig7.jpg",
      "caption": "(e) Bias in vSRL with (blue) / without (red) RBA",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_10",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig8.jpg",
      "image_filename": "1707.09457_page0_fig8.jpg",
      "caption": "(f) Bias in MLC with (blue) / without (red) RBA Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We test our methods for reducing bias amplification in two problem settings: visual semantic role labeling in the imSitu dataset (vSRL) and multilabel image classification in MS-COCO (MLC). In all settings we derive corpus constraints using the training set and then run our calibration method in batch on either the development or testing set. Our results are summarized in Table 2 and Figure 3.",
        "Our quantitative results are summarized in the first two sections of Table 2. On the development set, the number of verbs whose bias exceed the original bias by over $5 \\%$ decreases $3 0 . 5 \\%$ (Viol.). Overall, we are able to significantly reduce bias amplification in vSRL by $52 \\%$ on the development set (Amp. bias). We evaluate the underlying recognition performance using the standard measure in vSRL: top-1 semantic role accuracy, which tests how often the correct verb was predicted and th",
        "Figure 3(e) demonstrates that across all initial training bias, RBA is able to reduce bias amplification. In general, RBA struggles to remove bias amplification in areas of low initial training bias,",
        "Our quantitative results on MS-COCO RBA are summarized in the last two sections of Table 2. Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.). Bias amplification was reduced by $3 1 . 3 \\%$ on the development set (Amp. bias). The underlying recognition system was evaluated by the standard measure: top-1 mean average precision, the precision averaged across object categories. Our calibration method results i",
        "In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\%$ .",
        "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC.",
        "In Figure 3(d), we demonstrate that we substantially reduce the distance between training bias and bias in the development set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/e81fc29b304c19c7c42406ad3f777b808693e2c30b910e5b23659c5bcd0c997a.jpg",
      "image_filename": "e81fc29b304c19c7c42406ad3f777b808693e2c30b910e5b23659c5bcd0c997a.jpg",
      "caption": "likely because bias is encoded in image statistics and cannot be removed as effectively with an image agnostic adjustment.",
      "context_before": "",
      "context_after": "likely because bias is encoded in image statistics and cannot be removed as effectively with an image agnostic adjustment. Results on the test set support our development set results: we decrease bias amplification by $40 . 5 \\%$ (Amp. bias).\n\n7.2 Multilabel Classification\n\nOur quantitative results on MS-COCO RBA are summarized in the last two sections of Table 2. Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.). Bias amplification was reduced by $3 1 . 3 \\%$ on the development set (Amp. bias). The underlying recognition system was evaluated by the standard measure: top-1 mean average precision, the precision averaged across object categories. Our calibration method results in a negligible loss in performance. In Figure 3(d), we demonstrate that we substantially reduce the distance between training bias and bias in the development set. Finally, in Figure 3(f) we demonstrate that we decrease bias amplification for all initial training bias settings. Results on the test set support our development results: we decrease bias amplification by $4 7 . 5 \\%$ (Amp. bias).",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1707.09457_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    }
  ],
  "1709.02012": [
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig0.jpg",
      "image_filename": "1709.02012_page0_fig0.jpg",
      "caption": "(a) Possible cal. classifiers $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ (blue/red).",
      "context_before": "$$ c _ {f n} \\left(h _ {t}\\right) = \\left(1 - \\mu_ {t}\\right) / \\mu_ {t} c _ {f p} \\left(h _ {t}\\right). \\tag {1} $$\n\n2 In practice, $h _ { 1 }$ and $h _ { 2 }$ can be trained jointly (i.e. they are the same classifier).\n\n3 Throughout this work we will treat the calibration constraint as holding exactly; however, our results generalize to approximate settings as well. See the Supplementary Materials for more details.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig1.jpg",
      "image_filename": "1709.02012_page0_fig1.jpg",
      "caption": "(b) Satisfying cal. and equal F.P. rates.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg",
      "image_filename": "1709.02012_page0_fig2.jpg",
      "caption": "(c) Satisfying cal. and equal F.N. rates.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig3.jpg",
      "image_filename": "1709.02012_page0_fig3.jpg",
      "caption": "(d) Satisfying cal. and a general constraint.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_5",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig4.jpg",
      "image_filename": "1709.02012_page0_fig4.jpg",
      "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers. (a) Level-order curves of cost. Low cost implies low error rates.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   \n(a) Level-order curves of cost. Low cost implies low error rates."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg",
      "image_filename": "1709.02012_page0_fig5.jpg",
      "caption": "(b) Usually, there is a calibrated classifier $\\tilde { h } _ { 2 }$ with the same cost of $h _ { 1 }$ .",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig6.jpg",
      "image_filename": "1709.02012_page0_fig6.jpg",
      "caption": "(c) Cal. and equal-cost are incompatible if $h _ { 1 }$ has high error.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_8",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig7.jpg",
      "image_filename": "1709.02012_page0_fig7.jpg",
      "caption": "(d) Possible cal. classifiers for $G _ { 2 }$ (bold red) by mixing $h _ { 2 }$ and $h ^ { \\mu _ { 2 } }$ . Figure 2: Calibration-Preserving Parity through interpolation.",
      "context_before": "",
      "context_after": "In other words, $h _ { 1 }$ lies on a line with slope $( 1 - \\mu _ { 1 } ) / \\mu _ { 1 }$ and $h _ { 2 }$ lies on a line with slope $( 1 - \\mu _ { 2 } ) / \\mu _ { 2 }$ (Figure 1a). The lower endpoint of each line is the perfect classifier, which assigns the correct prediction with complete certainty to every input. The upper endpoint is a trivial classifier, as no calibrated classifier can perform “worse than random” (see Lemma 3 in Section S2). The only trivial classifier that satisfies the calibration condition for a group $G _ { t }$ is the one that outputs the base rate $\\mu _ { t }$ . We will refer to $h ^ { \\mu _ { 1 } }$ and $h ^ { \\mu _ { 2 } }$ as the trivial classifiers, calibrated for groups $G _ { 1 }$ and $G _ { 2 }$ respectively. It follows from the definitions that $c _ { f p } ( h ^ { \\mu _ { 1 } } ) = \\mu _ { 1 }$ and $c _ { f n } ( h ^ { \\mu _ { 1 } } ) = \\bar { 1 } - \\bar { \\mu _ { 1 } }$ , and likewise for $h ^ { \\mu _ { 2 } }$ .\n\nFinally, it is worth noting that for calibrated classifiers, a lower false-positive rate necessarily corresponds to a lower false-negative rate and vice-versa. In other words, for a given base rate, a “better” calibrated classifier lies closer to the origin on the line of calibrated classifiers.\n\nImpossibility of Equalized Odds with Calibration. With this geometric intuition, we can provide a simplified proof of the main impossibility result from [26]:",
      "referring_paragraphs": [
        "Figure 2: Calibration-Preserving Parity through interpolation."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg",
      "image_filename": "1709.02012_page0_fig8.jpg",
      "caption": "(a) Income Prediction.",
      "context_before": "Impossibility of Satisfying Multiple Equal-Cost Constraints. It is natural to argue there might be multiple cost functions that we would like to equalize across groups. However, satisfying more than one distinct equal-cost constraint (i.e. different curves in the F.P./F.N. plane) is infeasible.\n\nTheorem 1 (Generalized impossibility result). Let $h _ { 1 }$ and $h _ { 2 }$ be calibrated classifiers for $G _ { 1 }$ and $G _ { 2 }$ with equal cost with respect to $g _ { t }$ . If $\\mu _ { 1 } \\neq \\mu _ { 2 }$ , and if $h _ { 1 }$ and $h _ { 2 }$ also have equal cost with respect to a different cost function $g _ { t } ^ { \\prime }$ , then $h _ { 1 }$ and $h _ { 2 }$ must be perfect classifiers.\n\n(Proof in Section S5). Note that this is a generalization of the impossibility result of [26]. Furthermore, we show in Theorem 9 (in Section S5) that this holds in an approximate sense: if calibration and multiple distinct equal-cost constraints are approximately achieved by some classifier, then that classifier must have approximately zero generalized false-positive and false-negative rates.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig9.jpg",
      "image_filename": "1709.02012_page0_fig9.jpg",
      "caption": "(b) Health Prediction.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_11",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig10.jpg",
      "image_filename": "1709.02012_page0_fig10.jpg",
      "caption": "(c) Recidivism Prediction. Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.",
      "context_before": "",
      "context_after": "In light of these findings, our goal is to understand the impact of imposing calibration and an equalcost constraint on real-world datasets. We will empirically show that, in many cases, this will result in performance degradation, while simultaneously increasing other notions of disparity. We perform experiments on three datasets: an income-prediction, a health-prediction, and a criminal recidivism dataset. For each task, we choose a cost function within our framework that is appropriate for the given scenario. We begin with two calibrated classifiers $h _ { 1 }$ and $h _ { 2 }$ for groups $G _ { 1 }$ and $G _ { 2 }$ . We assume that these classifiers cannot be significantly improved without more training data or features. We then derive $\\tilde { h } _ { 2 }$ to equalize the costs while maintaining calibration. The original classifiers are trained on a portion of the data, and then the new classifiers are derived using a separate holdout set. To compare against the (uncalibrated) Equalized Odds framework, we derive F.P./F.N. matching classifiers using the post-processing method of [19] (EO-Derived). On the criminal recidivism dataset, we additionally learn classifiers that directly encode the Equalized Odds constraints, using the methods of [37] (EO-Trained). (See Section S6 for detailed training and post-processing procedures.) We visualize model error rates on the generalized F.P. and F.N. plane. Additionally, we plot the calibrated classifier lines for $G _ { 1 }$ and $G _ { 2 }$ to visualize model calibration.\n\nIncome Prediction. The Adult Dataset from UCI Machine Learning Repository [28] contains 14 demographic and occupational features for various people, with the goal of predicting whether a person’s income is above $\\$ 50$ , 000. In this scenario, we seek to achieve predictions with equalized cost across genders ( $G _ { 1 }$ represents women and $G _ { 2 }$ represents men). We model a scenario where the primary concern is ensuring equal generalized F.N. rates across genders, which would, for example, help job recruiters prevent gender discrimination in the form of underestimated salaries. Thus, we choose our cost constraint to require equal generalized F.N. rates across groups. In Figure 3a, we see that the original classifiers $h _ { 1 }$ and $h _ { 2 }$ approximately lie on the line of calibrated classifiers. In the left plot (EO-Derived), we see that it is possible to (approximately) match both error rates of the classifiers at the cost of $h _ { 1 } ^ { e o }$ deviating from the set of calibrated classifiers. In the right plot, we see that it is feasible to equalize the generalized F.N. rates while maintaining calibration. $h _ { 1 }$ and $\\tilde { h } _ { 2 }$ lie on the same level-order curve of $g _ { t }$ (represented by the dashed-gray line), and simultaneously remain on the “line” of calibrated classifiers. It is worth noting that achieving either notion of non-discrimination requires some cost to at least one of the groups. However, maintaining calibration further increases the difference in F.P. rates between groups. In some sense, the calibrated framework trades off one notion of disparity for another while simultaneously increasing the overall error rates.\n\nHealth Prediction. The Heart Dataset from the UCI Machine Learning Repository contains 14 processed features from 906 adults in 4 geographical locations. The goal of this dataset is to accurately predict whether or not an individual has a heart condition. In this scenario, we would like to reduce disparity between middle-aged adults $( G _ { 1 } )$ and seniors $\\left( G _ { 2 } \\right)$ . In this scenario, we consider F.P. and F.N. to both be undesirable. A false prediction of a heart condition could result in unnecessary medical attention, while false negatives incur cost from delayed treatment. We therefore utilize the following cost function $g _ { t } ( h _ { t } ) = \\bar { r _ { f p } } h _ { t } ( \\mathbf { x } ) \\left( 1 - y \\right) + r _ { f n } \\left( 1 - h _ { t } ( \\mathbf { x } ) \\right) y$ , which essentially assigns a weight to both F.N. and F.P. predictions. In our experiments, we set $r _ { f p } = 1$ and $r _ { f n } = 3$ . In the right plot of Figure 3b, we can see that the level-order curves of the cost function form a curved line in the generalized F.P./F.N. plane. Because our original classifiers lie approximately on the same level-order curve, little change is required to equalize the costs of $h _ { 1 }$ and $\\tilde { h } _ { 2 }$ while maintaining calibration. This is the only experiment in which the calibrated framework incurs little additional cost, and therefore could be considered a viable option. However, it is worth noting that, in this example, the equal-cost constraint does not explicitly match either of the error types, and therefore the two groups will in expectation experience different types of errors. In the left plot of Figure 3b (EO-Derived), we see that it is alternatively feasible to explicitly match both the F.P. and F.N. rates while sacrificing calibration.",
      "referring_paragraphs": [
        "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig11.jpg",
      "image_filename": "1709.02012_page0_fig11.jpg",
      "caption": "Corollary 2.1. Let $\\mathcal { H } _ { t }$ be the set of perfectly calibrated classifiers for group $G _ { t }$ — i.e. for any $h _ { t } ^ { * } \\in \\mathcal { H } _ { T }$ , we have $\\epsilon ( h _ { t } ^ { * } ) = 0",
      "context_before": "We can get a similar lower bound for $c _ { f n } ( h _ { t } )$ as\n\n$$ \\begin{array}{l} c _ {f n} \\left(h _ {t}\\right) \\geq \\frac {1}{\\mu_ {t}} \\left(\\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) \\right] - \\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) ^ {2} \\right] - \\delta_ {c a l}\\right) \\\\ \\geq \\frac {1}{\\mu_ {t}} \\left(\\left(1 - \\mu_ {t}\\right) c _ {f p} \\left(h _ {t}\\right) - 2 \\delta_ {c a l}\\right) \\\\ = \\frac {1 - \\mu_ {t}}{\\mu_ {t}} c _ {f p} (h _ {t}) - \\frac {2 \\delta_ {c a l}}{\\mu_ {t}} \\\\ \\end{array} $$\n\nMultiplying these inequalities by $\\mu _ { t }$ completes this proof.",
      "context_after": "Corollary 2.1. Let $\\mathcal { H } _ { t }$ be the set of perfectly calibrated classifiers for group $G _ { t }$ — i.e. for any $h _ { t } ^ { * } \\in \\mathcal { H } _ { T }$ , we have $\\epsilon ( h _ { t } ^ { * } ) = 0$ . The generalized false-positive and false-negative rates of $h _ { t } ^ { * }$ are given by\n\n$$ c _ {f p} \\left(h _ {t} ^ {*}\\right) = \\frac {1}{1 - \\mu_ {t}} \\left(\\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) \\right] - \\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) ^ {2} \\right]\\right) \\tag {S5} $$\n\n$$ c _ {f n} \\left(h _ {t} ^ {*}\\right) = \\frac {1}{\\mu_ {t}} \\left(\\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) \\right] - \\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) ^ {2} \\right]\\right) \\tag {S6} $$",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig12.jpg",
      "image_filename": "1709.02012_page0_fig12.jpg",
      "caption": "Corollary 2.2. For a group $G _ { t }$ , any perfectly calibrated classifier $h _ { t } ^ { * }$ satisfies",
      "context_before": "$$ c _ {f p} \\left(h _ {t} ^ {*}\\right) = \\frac {1}{1 - \\mu_ {t}} \\left(\\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) \\right] - \\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) ^ {2} \\right]\\right) \\tag {S5} $$\n\n$$ c _ {f n} \\left(h _ {t} ^ {*}\\right) = \\frac {1}{\\mu_ {t}} \\left(\\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) \\right] - \\underset {G _ {t}} {\\mathbb {E}} \\left[ h _ {t} (\\mathbf {x}) ^ {2} \\right]\\right) \\tag {S6} $$\n\nProof. This is a direct consequence of (S3) and (S4).",
      "context_after": "Corollary 2.2. For a group $G _ { t }$ , any perfectly calibrated classifier $h _ { t } ^ { * }$ satisfies\n\n$$ c _ {f n} \\left(h _ {t} ^ {*}\\right) = \\frac {1 - \\mu_ {t}}{\\mu_ {t}} c _ {f p} \\left(h _ {t}\\right). \\tag {S7} $$\n\nIn other words, all perfectly calibrated classifiers $h _ { t } ^ { * } \\in \\mathcal { H } _ { t }$ for group $G _ { t }$ lie on a line in the generalized false-positive/false-negative plane, where the slope of the line is uniquely determined by the group’s base-rate $\\mu _ { t }$ .",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig13.jpg",
      "image_filename": "1709.02012_page0_fig13.jpg",
      "caption": "Next, we show that $g _ { t }$ is linear under randomized interpolations.",
      "context_before": "$$ \\begin{array}{l} h _ {t} ^ {\\max } = \\underset {h \\in \\mathcal {H} _ {t} ^ {*}} {\\operatorname {a r g m a x}} \\left[ \\left(\\frac {a _ {t}}{1 - \\mu_ {t}} + \\frac {b _ {t}}{\\mu_ {t}}\\right) \\left(\\mu_ {t} - \\underset {G _ {t}} {\\mathbb {E}} [ h (x) ^ {2} ]\\right) \\right] \\\\ = \\operatorname * {a r g m a x} _ {h \\in \\mathcal {H} _ {t} ^ {*}} \\left[ - \\underset {G _ {t}} {\\mathbb {E}} \\left[ h (x) ^ {2} \\right] \\right] \\\\ = \\operatorname * {a r g m i n} _ {h \\in \\mathcal {H} _ {t} ^ {*}} \\left[ \\underset {G _ {t}} {\\mathbb {E}} \\left[ h (x) ^ {2} \\right] \\right] \\\\ = \\operatorname * {a r g m i n} _ {h \\in \\mathcal {H} _ {t} ^ {*}} \\left[ \\underset {G _ {t}} {\\mathbb {E}} \\left[ h (x) ^ {2} \\right] - \\mu_ {t} ^ {2} \\right] \\\\ \\end{array} $$\n\nThus, the calibrated classifier with minimum variance will have the highest cost. This translates to a classifier that outputs the same probability for every sample. By the calibration constraint, this constant must be equal to $\\mu _ { t }$ , so this classifier must be the trivial classifier $h ^ { \\mu _ { t } }$ — i.e. for all x\n\n$$ h _ {t} ^ {\\max } (\\mathbf {x}) = h ^ {\\mu_ {t}} (\\mathbf {x}) = \\mu_ {t}. $$",
      "context_after": "Next, we show that $g _ { t }$ is linear under randomized interpolations.\n\nLemma 4. Let $\\tilde { h } _ { 2 }$ be the classifier derived from (3) with interpolation parameter $\\alpha \\in [ 0 , 1 ]$ . The cost of $\\tilde { h } _ { 2 }$ is given by\n\n$$ g _ {2} (\\tilde {h} _ {2}) = (1 - \\alpha) g _ {2} (h _ {2}) + \\alpha g _ {2} \\left(h ^ {\\mu_ {2}}\\right) $$",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig14.jpg",
      "image_filename": "1709.02012_page0_fig14.jpg",
      "caption": "Next, we observe that by Lemma 5, for any classifiers $h _ { 1 } ^ { \\prime }$ and $h _ { 2 } ^ { \\prime }$ with $\\epsilon ( h _ { 1 } ^ { \\prime } ) \\leq \\delta _ { c a l }$ and $\\epsilon ( h _ { 2 } ^ { \\prime } ) \\leq",
      "context_before": "$$ \\begin{array}{l} \\underset {G _ {2}} {\\mathrm {P}} [ y = 1 \\mid \\tilde {h} _ {2} (\\mathbf {x}) = p ] = (1 - \\beta) \\underset {G _ {2}} {\\mathrm {P}} [ y = 1 \\mid h _ {2} (\\mathbf {x}) = p ] + \\beta \\underset {G _ {2}} {\\mathrm {P}} [ y = 1 \\mid h ^ {\\mu_ {2}} (\\mathbf {x}) = p ] \\\\ = (1 - \\beta) \\underset {G _ {2}} {\\mathrm {P}} [ y = 1 \\mid h _ {2} (\\mathbf {x}) = p ] + \\beta p \\\\ \\end{array} $$\n\nbecause $h ^ { \\mu _ { 2 } }$ is perfectly calibrated. Moreover, note that $\\mathrm { P } _ { G _ { 2 } } \\big [ \\tilde { h } _ { 2 } ( \\mathbf { x } ) { = } p \\big ] = \\mathrm { P } _ { G _ { 2 } } \\big [ h _ { 2 } ( \\mathbf { x } ) { = } p \\big ] / ( 1 - \\beta )$ Using this, we have $| \\mathrm { P } _ { G _ { 2 } } \\big [ y = 1 \\mid \\tilde { h } _ { 2 } ( { \\bf x } ) = p \\big ] - p | \\mathrm { P } _ { G _ { 2 } } \\big [ \\tilde { h } _ { 2 } ( { \\bf x } ) = p \\big ] = | \\mathrm { P } _ { G _ { 2 } } \\big [ y = 1 \\mid h _ { 2 } ( { \\bf x } ) =$ $p \\big ] - p | \\operatorname* { P } { } \\quad G _ { 2 } \\big [ h _ { 2 } ( \\mathbf { x } ) = p \\big ]$ . Thus,\n\n$$ \\begin{array}{l} \\epsilon (\\tilde {h} _ {2}) = \\int_ {0} ^ {1} \\left| \\mathrm {P} _ {G _ {2}} [ y = 1 | \\tilde {h} _ {2} (\\mathbf {x}) = p ] - p \\right| \\mathrm {P} _ {G _ {2}} [ \\tilde {h} _ {2} (\\mathbf {x}) = p ] d p \\\\ \\leq \\int_ {0} ^ {1} \\left| \\mathrm {P} _ {G _ {2}} [ y = 1 \\mid h _ {2} (\\mathbf {x}) = p ] - p \\right| \\mathrm {P} _ {G _ {2}} [ h _ {2} (\\mathbf {x}) = p ] d p \\\\ = \\epsilon (h _ {2}) \\\\ \\end{array} $$",
      "context_after": "Next, we observe that by Lemma 5, for any classifiers $h _ { 1 } ^ { \\prime }$ and $h _ { 2 } ^ { \\prime }$ with $\\epsilon ( h _ { 1 } ^ { \\prime } ) \\leq \\delta _ { c a l }$ and $\\epsilon ( h _ { 2 } ^ { \\prime } ) \\leq \\delta _ { c a l }$ satisfying the equal-cost constraint, it must be the case that $\\begin{array} { r } { c _ { f p } ( h _ { t } ^ { \\prime } ) \\geq c _ { f p } ( \\tilde { h } _ { t } ) - \\frac { 4 \\delta _ { c a l } } { 1 - \\mu _ { t } } } \\end{array}$ 4δcal1−µ and cf n(h0t) ≥ 1-μt $c _ { f n } { \\left( h _ { t } ^ { \\prime } \\right) } \\geq$ $\\begin{array} { r } { c _ { f p } ( \\tilde { h } _ { t } ) - \\frac { 4 \\delta _ { c a l } } { \\mu _ { t } } } \\end{array}$ for $t = 1 , 2$ .\n\nThus, approximately calibrated classifiers will be approximately optimal. From this result, it is easy to derive the optimality result for perfectly-calibrated classifiers.\n\nTheorem 8 (Exact Optimality of Algorithm 1). Algorithm 1 produces the classifiers $h _ { 1 }$ and $\\tilde { h } _ { 2 }$ that satisfy both perfect calibration and the equal-cost constraint with the lowest possible generalized false positive and false negative rates.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig15.jpg",
      "image_filename": "1709.02012_page0_fig15.jpg",
      "caption": "Note that Theorem 9 is not intended to be a tight bound. It simply shows that impossibility result degrades smoothly for approximate constraints.",
      "context_before": "$$ \\| \\widehat {A} ^ {- 1} \\| _ {\\infty} \\leq \\max _ {j} \\sum_ {i = 1} ^ {4} | d _ {i j} | \\leq 1 6 M ^ {3} D ^ {4} = L $$\n\n$$ \\| \\tilde {q} \\| _ {\\infty} \\leq L \\| \\nu \\| _ {\\infty} $$\n\nwhich proves the claim.",
      "context_after": "Note that Theorem 9 is not intended to be a tight bound. It simply shows that impossibility result degrades smoothly for approximate constraints.\n\nS6 Details on Experiments\n\nPost-processing for Equalized Odds To derive classifiers that satisfy the Equalized Odds notion of fairness, we use the method introduced by Hardt et al. [19]. Essentially, the false-positive and false-negative constraints are satisfied by randomly flipping some of the predictions of the original classifiers. Let q(t)n2p $q _ { \\mathrm { n 2 p } } ^ { ( t ) }$ be the probability for group $G _ { t }$ of “flipping” a negative prediction to positive, and $q _ { \\mathrm { p 2 n } } ^ { ( t ) }$ be that of flipping a positive prediction to negative. The derived classifiers $h _ { 1 } ^ { e o }$ and $h _ { 2 } ^ { e o }$ essentially flip predictions according to these probabilities:",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1709.02012_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1710.08615": [
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg",
      "image_filename": "1710.08615_page0_fig0.jpg",
      "caption": "Kristina Lerman",
      "context_before": "Simpson’s paradox [4, 19] is one important phenomenon confounding analysis of heterogeneous social data. According to the paradox, an association observed in data that has been aggregated over an entire population may be quite different from, and even opposite to, associations found in the underlying subgroups. A notorious example of Simpson’s paradox comes from the gender bias lawsuit against UC Berkeley [3]. Analysis of graduate school admissions data seemingly revealed a statistically significant bias against women: a smaller fraction of female applicants were admitted for graduate studies. However, when admissions data was disaggregated by department, women had parity and even a slight advantage over men in some departments. The paradox arose because departments preferred by female applicants have lower admissions rates for both genders.\n\nSimpson’s paradox also affects analysis of trends. When measuring how an outcome variable changes as a function of some independent variable, the characteristics of the population over which the trend is measured may change with the independent variable. As a result, the data may appear to exhibit a trend, which disappears or reverses when the data is disaggregated by subgroups [1]. Vaupel and Yashin [23] give several illustrations of this effect. For example, a study of recidivism among convicts released from prison showed that the rate at which they return to prison declines over time. From this, policy makers concluded that age has a pacifying effect, with older convicts less likely to commit crimes. In reality, this is not the case. Instead, the population of ex-convicts is composed of two subgroups with nearly constant, but very different recidivism rates. The first subgroup—the “reformed”—will never commit a crime once released from prison. The other subgroup—the “incorrigibles”—are highly likely commit a crime. Over time, as “incorrigibles” commit offenses and return to prison, there are fewer of them left in the population. Survivor bias changes the composition of the population, creating an illusion of an overall decline in recidivism. As Vaupel and Yashin warn, “unsuspecting researchers who are not wary of heterogeneity’s ruses may fallaciously assume that observed patterns for the population as a whole also hold on the sub-population or individual level.”\n\n[Section: Kristina Lerman]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig1.jpg",
      "image_filename": "1710.08615_page0_fig1.jpg",
      "caption": "(a) (b) Fig. 1 Exposure response in social media. The probability to retweet some information as a function of the number of friends who previously tweeted it has a nonmonotonic trend when averaged over all users (a), but increases monotonically when users are separated according to the number of friends they follow (b). This suggests that additional exposures increase retweet likelihood, instead of suppressing it.",
      "context_before": "",
      "context_after": "To highlight the perils of ignoring Simpson’s paradox, I describe several studies of online behavior in which the trends discovered in aggregate data lead to wrong conclusions about behavior. For decision makers and platform designers seeking to use research findings to inform policy, incorrect interpretation can lead to counterproductive choices where a policy thought to enhance some behavior instead suppresses it, or vice-versa. To identify such cases, I present a simple method researchers can use to test for the presence of the paradox in their data. When paradox is confirmed, analysis should be performed on the stratified data that has been disaggregated by subgroups [1, 19]. Testing and controlling for Simpson’s paradox should be part of every computational social scientist’s toolbox.\n\n2 Examples of Simpson’s Paradox\n\nMultiple examples of Simpson’s paradox have been identified in empirical studies of online behavior. For example, a study of Reddit [2] found that average comment length decreased over time. However, when data was disaggregated by cohorts based on the year the user joined Reddit, comment length within each cohort increases. Additional examples of Simpson’s paradox are described below.",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig2.jpg",
      "image_filename": "1710.08615_page0_fig2.jpg",
      "caption": "(a)",
      "context_before": "Multiple examples of Simpson’s paradox have been identified in empirical studies of online behavior. For example, a study of Reddit [2] found that average comment length decreased over time. However, when data was disaggregated by cohorts based on the year the user joined Reddit, comment length within each cohort increases. Additional examples of Simpson’s paradox are described below.\n\nExposure Response in Social Media. When examining how users spread information on a social media site Twitter, it may appear that repeated exposures to hashtags or links to online content make an individual less likely to use the hashtag himself or herself (Figure 1 of [21]) or share the links with followers [9] (Fig. 1 (a)). From this, one may conclude the additional exposures “inoculate” the user and suppress the sharing of information. In fact, the opposite is true: additional exposures monotonically increase the user’s likelihood to share information with followers [17]. The paradox arises because those users who follow many others— and are likely to be exposed to information or a hashtag multiple times—are less responsive overall (Fig. 1 (b)), simply because they are overloaded with a large volume of information they receive [10]. Calculating response as a function of the number of exposures in the aggregate data falls prey to survivor bias: the more\n\n[Section: Computational Social Scientist Beware: Simpson’s Paradox in Behavioral Data]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig3.jpg",
      "image_filename": "1710.08615_page0_fig3.jpg",
      "caption": "(b) Fig. 2 Rate of content consumption during a session. Average time spent viewing each item in a social feed appears to increase over the course of a session when looking at all the data (a) but decreases within sessions of the same length (b). This indicates that users speed up near the end of the session, taking less and less time to view each item.",
      "context_before": "",
      "context_after": "responsive users (with fewer friends) quickly drop out of analysis (since they are generally exposed fewer times), leaving only the highly connected, but less responsive users behind. Their reduced susceptibility biases aggregate response, leading to wrong conclusions about individual behavior. Once data is disaggregated based on the volume of information individuals receive [20], a clearer pattern of response emerges, one that is more predictive of behavior [11].\n\nContent Consumption in Social Media. A study of content consumption on a popular social networking site Facebook examined the time users devote to viewing each item in their social feed [15]. The study segmented each user’s activity into sessions, defined as sequences of activity without a prolonged break (see Fig. 4 for an explanation). At a population level, it looks as if users slow down over the course of a session, taking more and more time to view each item (Fig. 2 (a)). However, when looking at user activity within sessions of the same length, e.g., sessions that are 30 minutes long, it appears that individuals speed up instead (Fig. 2 (b)). As the session progresses, they spend less and less time viewing each item, which suggests that they begin to skim posts.\n\nThe difference in trends arises because users who have longer sessions also tend to spend more time viewing each item in their feed. When calculating how long users view items as a function of time, the faster users drop out of analysis of aggregate data, leaving the slower, users who tend to have longer sessions. Therefore, stratifying data by session length removes the confounding factor and allows us to study behavior within a similar cohort.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig4.jpg",
      "image_filename": "1710.08615_page0_fig4.jpg",
      "caption": "Kristina Lerman",
      "context_before": "The difference in trends arises because users who have longer sessions also tend to spend more time viewing each item in their feed. When calculating how long users view items as a function of time, the faster users drop out of analysis of aggregate data, leaving the slower, users who tend to have longer sessions. Therefore, stratifying data by session length removes the confounding factor and allows us to study behavior within a similar cohort.\n\nAnswer Quality on Stack Exchange. Stack Exchange is a popular question-answering platform where users ask and answer questions. Askers can also “accept” an answer as the best answer to their question. A study of dynamics of user performance on Stack Exchange found that answer quality, as measured by the probability that it will be accepted by the asker as the best answer, declines steadily over the course of a session, with each successive answer written by a user ever less likely to get accepted [7]. However, this trend is seen only when comparing sessions of the same length, for example, sessions where exactly four answers were written (Fig. 3 (b)). When calculating answer acceptance probability over all the data, it looks as though answers written later in a session are more likely to get accepted\n\n[Section: Kristina Lerman]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig5.jpg",
      "image_filename": "1710.08615_page0_fig5.jpg",
      "caption": "(a) (b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig6.jpg",
      "image_filename": "1710.08615_page0_fig6.jpg",
      "caption": "Fig. 3 Quality of answers on Stack Exchange. Probability that an answer is accepted as the best answer to a question increases as a function of its position within the session in the aggregated data (a) but decreases within sessions of the same length (b). This suggests that the quality of answers written by users deteriorates over the course of a session. Note that each line in the right panel represents sessions of a given length. Only sessions with five or fewer answers are shown. Fig. 4 Data randomization for the shuffle test. The top row shows the original stream of user actions $C _ { 1 } , \\ldots , C _ { 4 }$ . A session is a sequence of actions without an extended break, e.g., 60 minutes. Here, user actions $C _ { 1 }$ through $C _ { 3 }$ are assigned to one session, while $C _ { 4 }$ is assigned to a new session. The middle row shows data randomization strategy that shuffles time intervals between actions while preserving their order. This tends to change the definition of sessions. The bottom row shows the second randomization strategy, which shuffles the order of actions within sessions, while preserving the time intervals between actions. (Fig. 3 (a)). Here, the length of the session confounds analysis: users who have longer sessions write answers that are more likely to be accepted.",
      "context_before": "",
      "context_after": "3 Testing Data for Simpson’s Paradox\n\nWhen can a cautious researcher accept results of analysis? I describe a simple test that can help ascertain whether a pattern observed in data is robust or potentially a manifestation of Simpson’s paradox. The test creates a randomized version of the data by shuffling it with respect to the attribute for which the trend is measured. Shuffling preserves the distribution of features, but destroys correlation between the outcome variable and that attribute. As a result, any trends with respect to\n\n[Section: Computational Social Scientist Beware: Simpson’s Paradox in Behavioral Data]",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig7.jpg",
      "image_filename": "1710.08615_page0_fig7.jpg",
      "caption": "Computational Social Scientist Beware: Simpson’s Paradox in Behavioral Data",
      "context_before": "3 Testing Data for Simpson’s Paradox\n\nWhen can a cautious researcher accept results of analysis? I describe a simple test that can help ascertain whether a pattern observed in data is robust or potentially a manifestation of Simpson’s paradox. The test creates a randomized version of the data by shuffling it with respect to the attribute for which the trend is measured. Shuffling preserves the distribution of features, but destroys correlation between the outcome variable and that attribute. As a result, any trends with respect to\n\n[Section: Computational Social Scientist Beware: Simpson’s Paradox in Behavioral Data]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig8.jpg",
      "image_filename": "1710.08615_page0_fig8.jpg",
      "caption": "(a) (b) Fig. 5 Online shopping. Relationship between purchase price and time to next purchase in data (red line) and in the shuffled data (blue line), in which the purchase prices of items were randomly shuffled. The positive trend seen in the aggregate data (a) still persists when data is shuffled. However, when data is disaggregated by the number of purchases, specifically, users who made exactly five purchases (b), the trend disappears in the shuffled data.",
      "context_before": "",
      "context_after": "that attribute should disappear. This suggests a rule of thumb: if the trend persists in the aggregate data, but disappears when the shuffled data is disaggregated, then Simpson’s paradox may be present.\n\nIn the analyses described above, the independent variable was time, or a proxy of it, such as the point within a session when the action takes place. There are at least two different randomization strategies with respect to time. The first strategy creates randomized session data by preserving the temporal order of actions, but shuffling the time intervals between them, as shown in Fig. 4 (middle row). Since session break is defined as a sufficiently long time interval between actions, shuffling time intervals will merge sessions and break up longer sessions, while preserving the sequence of actions. The second strategy creates a randomized index data by shuffling the order of actions within a session, e.g., exchanging $C _ { 1 }$ by $C _ { 3 }$ in Fig. 4 (bottom row).\n\nBelow I illustrate the shuffle test with real-world examples. I show that when the data is shuffled, the trend still persists in the aggregate data, but disappears, as expected, when the shuffled data is disaggregated.",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig9.jpg",
      "image_filename": "1710.08615_page0_fig9.jpg",
      "caption": "Kristina Lerman",
      "context_before": "Online Shopping. A study of online shopping examined whether individual purchasing decisions are constrained by finances. The study looked at the relationship between purchase price of an item and the time interval since last purchase [14]. Budgetary constraints would force a user to wait after making a purchase to accumulate enough money for another purchase. Figure 5 (a) reports (normalized) purchase price of an item as a function of the time since last purchase (red line). The longer the delay, the larger the fraction of the budget users spend on a purchase, which appears to support the hypothesis.\n\nTo test the robustness of this finding, the data was shuffled by randomly swapping the prices of products purchased by users, which destroys the correlation between the time between purchases and purchase price. Surprisingly, the trend remains (blue line). This is due to heterogeneity of the underlying population: the population represents a mix of users with different purchasing habits. The frequent buyers purchase cheaper items more frequently, and they are systematically overrepresented on the left side of the plot, even in shuffled data.\n\n[Section: Kristina Lerman]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig10.jpg",
      "image_filename": "1710.08615_page0_fig10.jpg",
      "caption": "(a) (b) Fig. 6 Answer’s acceptance probability as a function of its session index in the randomized Stack Exchange data. The left panel shows that the upward trend seen in Fig. 3 is preserved in the aggregate shuffled data. However, when shuffled data is disaggregated by session length (b), the trends largely disappear.",
      "context_before": "",
      "context_after": "To stratify data, buyers were grouped by the number of purchases they make, for example, those making exactly five purchases (Fig. 5 (b)). The positive trend between the normalized purchase price and time seen in the disaggregated data (red line) disappears in the shuffled data (blue line), giving unbiased support for the limited budget hypothesis.\n\nStack Exchange. To test robustness of trends shown in Figure 3, which reports how acceptance probability of an answer posted on Stack Exchange changes over the course of a session, we randomize data by shuffling the time intervals between answers posted by each user, while preserving other features, including the temporal order of answers. The randomization procedure changes sessions by breaking up longer sessions and concatenating shorter ones. By changing which sequence of answers is considered to belong to a session, we expect randomization to change the observed trends in acceptance probability.\n\nThe upward trend in acceptance probability seen in aggregate data still exists in the randomized data (Fig. 6 (a)), even though the trends in randomized data disappear, as expected, when data is disaggregated by session length (Fig. 6 (b)). This confirms the need for stratifying data by session length in analysis.",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig11.jpg",
      "image_filename": "1710.08615_page0_fig11.jpg",
      "caption": "Fig. 7 Deterioration in comment quality on Reddit. When data is disaggregated by length of the session (different color lines), the quantitative proxies of comment quality decline over the course of a session. The x-axis represents index of the comment within a session, and the y-axis gives the average value of the proxy measure (with error bars). The declines observed in original Reddit data (top row) mostly disappear when data is randomized (bottom row).",
      "context_before": "The upward trend in acceptance probability seen in aggregate data still exists in the randomized data (Fig. 6 (a)), even though the trends in randomized data disappear, as expected, when data is disaggregated by session length (Fig. 6 (b)). This confirms the need for stratifying data by session length in analysis.\n\nReddit Comments. A similar quality deterioration effect was observed for comments posted on Reddit. Regardless of what measure is used as a proxy of quality— comment length, the number of responses or upvotes from others it receives, its textual complexity—the quality of each successive comment written by a Reddit user decreases over the course of a session [22]. To test the robustness of this finding, Singer et al. randomized Reddit activity data. Figure 7 compares the trends for the proxies of comment quality in the original data to those in the randomized data. Both data sets have been disaggregated by session length. The decreasing trends observed in the original Reddit data (top row) largely disappear in the randomized data (bottom row). Where the trends still exist, the deterioration effect is much reduced. This suggests that most of data heterogeneity is captured by session length.\n\n[Section: Computational Social Scientist Beware: Simpson’s Paradox in Behavioral Data]",
      "context_after": "Simpson’s paradox can indicate that interesting patterns exist in data [6], but it can also skew analysis. The paradox suggests that data comes from subgroups that differ systematically in their behavior, and that these differences are large enough to affect analysis of aggregate data. In this case, the trends discovered in disaggregated data are more likely to describe—and predict—individual behavior than the trends found in aggregate data. Thus, to build more robust models of behavior, computational social scientists need to identify confounding variables which could affect observed trends. The shuffle test described in this paper provides a framework for determining whether Simpson’s paradox is affecting conclusions.\n\nMany people have contributed along the way to identifying the problem of Simpson’s paradox in data analysis, investigating it empirically, as well as devising methods to mitigate its effects. These people include Nathan Hodas, Farshad Kooti, Keith Burghardt, Philipp Singer, Emilio Ferrara, Peter Fennell, Nazanin Alipourfard. This work was funded, in part, by Army Research Office under contract W911NF-15-1-0142.\n\n[Section: Kristina Lerman]",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.08615_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1710.11214": [
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg",
      "image_filename": "1710.11214_page0_fig0.jpg",
      "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.",
      "context_before": "Recommendation systems; algorithmic confounding.\n\nRecommendation systems are designed to help people make decisions. These systems are commonly used on online platforms for video, music, and product purchases through service providers such as Netflix, Pandora, and Amazon. Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1). While the broad notion of confounding from the data collection process has been studied extensively, we seek to characterize the impact of this feedback loop in the context of recommendation systems, demonstrating the unintended consequences of algorithmic confounding. As recommendation systems become increasingly important in decision-making, we have an ethical responsibility to understand the idiosyncrasies of these systems and consider their implications for individual and societal welfare [18].\n\nIndividual decisions can aggregate to have broad political and economic consequences. Recommendation systems can influence how users perceive the world by filtering access to media; pushing political dialog towards extremes [59] or filtering out contrary opinions [25]. Even more gravely, these systems impact crucial decision-making processes, such as loan approvals, criminal profiling, and medical interventions. As recommendation systems shape access to goods and resources, issues of fairness and transparency need to be considered. For example, if a distinct group of minoritypreference users exists, a system may deferentially undermine the",
      "context_after": "needs or preferences of the minority in favor of optimizing the utility of the majority group.\n\nMany researchers and practitioners still focus on evaluating recommendation systems in terms of held-out accuracy, which cannot capture the full effects of the feedback loop. Even with accuracy as a primary concern, algorithmic confounding can play a crucial role; for instance, when a recommendation system is evaluated using confounded held-out data, results are biased toward recommendation systems similar to the confounding algorithm. Thus, the choice of data can considerably impact held-out evaluation and subsequent conclusions. Averaged accuracy metrics, however, are only one approach to evaluating recommendation systems, and do not detect disparate impact across users. It is our hope that this work will help motivate researchers and practitioners to 1) actively assess systems with objectives such as diversity, serendipity, novelty, and coverage [31]; 2) apply causal reasoning techniques to counter the effects of algorithmic confounding; and 3) evaluate the distribution of impact across all users, instead of exclusively reporting averages.\n\nWe begin with a summary of our claims (section 2) and then situate this work among related lines of inquiry (section 3). To provide evidence for our claims, we introduce a model for users interacting with recommendations (section 4); this allows us to analyze the impact of algorithmic confounding on simulated communities (section 5). We find that algorithmic confounding amplifies the homogenization of user behavior (section 5.2) without corresponding gains in utility (section 5.3) and also amplifies the impact of recommendation systems on the distribution of item consumption (section 5.4). We briefly discuss weighting approaches to counter these effects (section 6) before we conclude (section 7). For our simulations and analysis, we develop a general framework for recommendation systems (appendix A) which highlights commonalities between seemingly distinct approaches.",
      "referring_paragraphs": [
        "Recommendation systems are designed to help people make decisions. These systems are commonly used on online platforms for video, music, and product purchases through service providers such as Netflix, Pandora, and Amazon. Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1). While the broad notion of confounding from the data collection process has been studied extensively, we seek to ",
        "Real-world recommendation systems are often part of a feedback loop (figure 1): the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system. We attempt to characterize the impact of this feedback loop through three claims.",
        "Bias, confounding, and estimands. Schnabel, et al. [52] note that users introduce selection bias; this occurs during the interaction component of the feedback loop shown in figure 1. They consider a mechanism for interaction in which users first select an item and then rate it. Other work also considers similar notions of missingness in rating data [43, 62]. However, many platforms exist where users express their preferences implicitly by viewing or reading content, as opposed to explicitly rati",
        "We consider two cases of observing user interactions with items: a simple case where each recommendation algorithm is trained once, and a more complicated case of repeated training; this allows us to compare a single cycle of the feedback loop (figure 1) to multiple cycles. In the simple paradigm, we run 50 iterations of “start-up” (new items only each iteration), train the algorithms, and then observe 50 iterations of confounded behavior. In the second paradigm, we have ten iterations of “start",
        "to ideal. Figure 3 shows these results for both the single training and the repeated training cases. In the single training case, users became slightly homogenized after training, but then returned to the ideal homogenization. With repeated training, all recommendation systems (except random), homogenize user behavior beyond what was needed to achieve ideal utility. As the number of cycles in the feedback loop (figure 1) increases, we observe homogenization effects continue to increase without c",
        "Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1).",
        "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems.",
        "Real-world recommendation systems are often part of a feedback loop (figure 1): the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system.",
        "[52] note that users introduce selection bias; this occurs during the interaction component of the feedback loop shown in figure 1.",
        "We consider two cases of observing user interactions with items: a simple case where each recommendation algorithm is trained once, and a more complicated case of repeated training; this allows us to compare a single cycle of the feedback loop (figure 1) to multiple cycles.",
        "As the number of cycles in the feedback loop (figure 1) increases, we observe homogenization effects continue to increase without corresponding increases in utility."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.11214_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg",
      "image_filename": "1710.11214_page0_fig1.jpg",
      "caption": "Figure 2: Example true utility matrix for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.",
      "context_before": "For our simulations, all of the six approaches recommend from the set of items that exist in the system at the time of training; random recommends these items in random order. Ideal recommends items for each user $u$ based on the user’s true utility $V _ { u i }$ for those u Vuiitems. Comparison with these two approaches minimizes the impact of the interaction model assumptions (section 4) on our results.\n\nIn all of our simulations, a community consists of 100 users and is run for 1,000 time intervals with ten new items being introduced at each interval; each simulation is repeated with ten random seeds and all our results are averages over these ten “worlds.” We generate the distributions of user preference and item attribute popularity, as used in equation (3), in $K = 2 0$ dimensions; we generate uneven user preferences, but approximately even item attributes. The user preference parameter is generated as follows: ˜ ∼ Dirichlet(1) and $\\mu _ { \\rho } = 1 0 \\cdot \\tilde { \\mu } _ { \\rho }$ ρ. This mirrors the real world where preferences are ρ ρunevenly distributed, which allows us to expose properties of the recommendation algorithms. Item attribute popularity is encouraged to be more even in aggregate, but still be sparse for individual items; we draw: ˜ ∼ Dirichlet(100) and $\\mu _ { \\alpha } = 0 . 1 \\cdot \\tilde { \\mu } _ { \\alpha }$ . While item µα µα . µαattributes are not evenly distributed in the real world, this ensures that all users will be able to find items that match their preferences. With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior. We generate social networks (used by social filtering only) using the covariance matrix of user preferences; we impose that each user must have at least one network connection and binarize the covariance matrix using this criteria. This procedure enforces homophily between connected users, which is generally (but not always) true in the real world.\n\nWe consider two cases of observing user interactions with items: a simple case where each recommendation algorithm is trained once, and a more complicated case of repeated training; this allows us to compare a single cycle of the feedback loop (figure 1) to multiple cycles. In the simple paradigm, we run 50 iterations of “start-up” (new items only each iteration), train the algorithms, and then observe 50 iterations of confounded behavior. In the second paradigm, we have ten iterations of “start-up,” then train the algorithms every iteration for the remaining 90 iterations using all previous data.",
      "context_after": "5.2 Homogenization Effects\n\nRecommendation systems may not change the underlying preferences of user (especially not when used on short time scales), but they do impact user behavior, or the collection of items with which users interact. Recommendation algorithms encourage similar users to interact with the same set of items, therefore homogenizing their behavior, relative to the same platform without recommended content. For example, Popularity-based systems represent all users in the same way; this homogenizes all users, as seen in previous work [11, 58]. Social recommendation systems homogenize connected users or within cliques, and matrix factorization homogenizes users along learned latent factors.\n\nHomogenizing effects are not inherently bad as they indicate that the models are learning patterns from the data, as intended; when achieving optimum utility, users will have some degree of homogenization. However, homogenization of user behavior does not correspond directly with an increase in utility: we can observe an increase in homogenization without a corresponding increase in utility. This is related to the explore/exploit paradigm, where we wish to exploit the user representation to maximize utility, but not to homogenize users more than necessary. When a representation of users is over-exploited, users are being pushed to be have more similar behaviors than their underlying preferences would optimally dictate. This suggests that the “tyranny of majority” and niche “echo chamber” effects may both be manifestations of the same problem: over-exploitation of recommendation models. While concerns about maximizing utility are not new to the recommendation system literature, there are also grave social consequences from homogenization that have received less consideration.",
      "referring_paragraphs": [
        "In all of our simulations, a community consists of 100 users and is run for 1,000 time intervals with ten new items being introduced at each interval; each simulation is repeated with ten random seeds and all our results are averages over these ten “worlds.” We generate the distributions of user preference and item attribute popularity, as used in equation (3), in $K = 2 0$ dimensions; we generate uneven user preferences, but approximately even item attributes. The user preference parameter is g",
        "With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior.",
        "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.11214_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig2.jpg",
      "image_filename": "1710.11214_page0_fig2.jpg",
      "caption": "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.",
      "context_before": "We compared the Jaccard index for paired users against the Jaccard index of the same users exposed to ideal recommendations; this difference captures how much the behavior has homogenized relative\n\n[Section: RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada]\n\n[Section: Allison J.B. Chaney, Brandon M. Stewart, and Barbara E. Engelhardt]",
      "context_after": "",
      "referring_paragraphs": [
        "to ideal. Figure 3 shows these results for both the single training and the repeated training cases. In the single training case, users became slightly homogenized after training, but then returned to the ideal homogenization. With repeated training, all recommendation systems (except random), homogenize user behavior beyond what was needed to achieve ideal utility. As the number of cycles in the feedback loop (figure 1) increases, we observe homogenization effects continue to increase without c",
        "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.",
        "Figure 3 shows these results for both the single training and the repeated training cases."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.11214_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg",
      "image_filename": "1710.11214_page0_fig3.jpg",
      "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.",
      "context_before": "",
      "context_after": "to ideal. Figure 3 shows these results for both the single training and the repeated training cases. In the single training case, users became slightly homogenized after training, but then returned to the ideal homogenization. With repeated training, all recommendation systems (except random), homogenize user behavior beyond what was needed to achieve ideal utility. As the number of cycles in the feedback loop (figure 1) increases, we observe homogenization effects continue to increase without corresponding increases in utility.\n\nWe can consider global homogenization to reveal the impact of the feedback loop at the population level; instead of comparing to paired users based on $\\theta _ { u }$ , we compare users matched randomly (figθuure 4). In this setting, all recommendation systems (except random) increased global homogeneity of user behavior. The popularity system increased homogeneity the most; after that, matrix factorization and social filtering homogenized users comparably, and content filtering homogenized users least of all, but still more than ideal.\n\nWe have shown that when practitioners update their models without considering the feedback loop of recommendation and interaction, they encourage users to consume a more narrow range of items, both in terms of local niche behavior and global behavior.",
      "referring_paragraphs": [
        "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.11214_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig4.jpg",
      "image_filename": "1710.11214_page0_fig4.jpg",
      "caption": "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization.",
      "context_before": "We found that the feedback loop amplifies the impact of recommendation systems on the distribution of item consumption,\n\n[Section: Algorithmic Confounding in Recommendation Systems Increases Homogeneity RecSys ’18, October 2–7, 2018, Vancouver, BC, Canada]\n\n$^ 5 \\mathrm { R P } ( i , t )$ is the rank of item based on how many people have consumed it, relative i, t ito all other items (up to time ).",
      "context_after": "",
      "referring_paragraphs": [
        "Changes in utility due to these effects are not necessarily born equally across all users. For example, users whose true preferences are not captured well by the low dimensional representation of user preferences may be disproportionately impacted. These minority users may see lesser improvements or even decreases in utility when homogenization occurs. Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorithms, we find that users who exper",
        "Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor.",
        "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.11214_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig5.jpg",
      "image_filename": "1710.11214_page0_fig5.jpg",
      "caption": "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items). Each point is a single simulation. Similar homogenization can result in different item consumption distributions.",
      "context_before": "",
      "context_after": "irrespective of homogenization effects. Specifically, two recommendation systems can produce similar amounts of user homogenization with different distributions of item consumption (figure 6). For example, matrix factorization (MF) and content filtering have comparable homogenizing effects, but MF creates a more unequal distribution of item consumption.\n\nAs a community, we do not fully understand the ways in which these systems change the popularity of items. Differential item consumption may ultimately change item production from strategic actors, such as companies like Amazon and Netflix which are now producing content based on their consumers’ behavior data. Thus, recommendation systems change not only what users see first, but can fundamentally alter the collection of content from which users can choose.\n\n6 ACCOUNTING FOR CONFOUNDING",
      "referring_paragraphs": [
        "irrespective of homogenization effects. Specifically, two recommendation systems can produce similar amounts of user homogenization with different distributions of item consumption (figure 6). For example, matrix factorization (MF) and content filtering have comparable homogenizing effects, but MF creates a more unequal distribution of item consumption.",
        "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1710.11214_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1711.05144": [
    {
      "doc_id": "1711.05144",
      "figure_id": "1711.05144_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg",
      "image_filename": "1711.05144_page0_fig0.jpg",
      "caption": "(a)",
      "context_before": "Recall that in addition to the choices of protected attributes and model classes for Learner and Auditor, FairFictPlay has a parameter C, which is a bound on the norm of the dual variables for Auditor (the dual player). While the theory does not provide an explicit bound or guide for choosing C, it needs to be large enough to permit the dual player to force the minmax value of the game. For our experiments we chose $C = 1 0$ , which despite being a relatively small value seems to suffice for (approximate) convergence.\n\nThe other and more meaningful parameter of the algorithm is the bound $\\gamma$ in the Fair ERM optimization problem implemented by the game, which controls the amount of unfairness permitted. If on a given round the subgroup disparity found by the Auditor is greater than $\\gamma$ , the Learner must react by adding a fairness penalty for this subgroup to its objective function; if it is smaller than $\\gamma$ , the Learner can ignore it and continue to optimize its previous objective function. Ideally, and as we shall see, varying γ allows us to trace out a menu of trade-offs between accuracy and fairness.\n\nParticularly in light of the gaps between the idealized theory and the actual implementation, the most basic questions about FairFictPlay are whether it converges at all, and if so, whether it converges to “interesting” models — that is, models with both nontrivial classification error (much better than the $3 0 \\%$ or 0.3 baserate), and nontrivial subgroup fairness (much better than ignoring fairness altogether). We shall see that at least for the C&C dataset, the answers to these questions is strongly affirmative.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.05144_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.05144",
      "figure_id": "1711.05144_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig1.jpg",
      "image_filename": "1711.05144_page0_fig1.jpg",
      "caption": "(b) Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.",
      "context_before": "",
      "context_after": "We begin by examining the evolution of the error and unfairness of Learner’s model. In the left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of γ ranging from 0 to 0.029. Several comments are in order.\n\nFirst, after an initial period in which there is a fair amount of oscillatory behavior, by 6000 iterations most of the curves have largely flattened out, and by 8,000 iterations it appears most but not all have reached approximate convergence. Second, while the top-to-bottom ordering of these error curves is approximately aligned with decreasing γ — so larger γ generally results in lower error, as expected — there are many violations of this for small $t$ , and even a few at large t. Third, and as we will examine more closely shortly, the converged values at large t do indeed exhibit a range of errors.\n\nIn the right panel of Figure 1, we show the corresponding unfairness $\\gamma _ { t }$ of the subgroup found by the Auditor at each iteration $t$ for the same runs and values of the parameter $\\gamma$ (indicated by horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is generally reversed — larger values of $\\gamma$ generally lead to higher $\\gamma _ { t }$ curves, since the fairness constraint on the Learner is weaker. We again see a great deal of early oscillatory behavior, with most $\\gamma _ { t }$ curves then eventually settling at or near their corresponding input γ value, as Learner and Auditor engage in a back-and-forth struggle for lower error for Learner and $\\gamma$ -subgroup fairness for Auditor.",
      "referring_paragraphs": [
        "We begin by examining the evolution of the error and unfairness of Learner’s model. In the left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of γ ranging from 0 to 0.029. Several comments are in order.",
        "In the right panel of Figure 1, we show the corresponding unfairness $\\gamma _ { t }$ of the subgroup found by the Auditor at each iteration $t$ for the same runs and values of the parameter $\\gamma$ (indicated by horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is generally reversed — larger values of $\\gamma$ generally lead to higher $\\gamma _ { t }$ curves, since the fairness constraint on the Learner is weaker. We again see a great deal of early os",
        "For any choice of the parameter $\\gamma$ , and each iteration $t$ , the two panels of Figure 1 yield a pair of realized values $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ from the experiment, corresponding to a Learner model whose error is $\\varepsilon _ { t } .$ , and for which the worst subgroup the Auditor was able to find had unfairness $\\gamma _ { t }$ . The set of all $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all runs or $\\gamma$ values thus represents ",
        "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ.",
        "For any choice of the parameter $\\gamma$ , and each iteration $t$ , the two panels of Figure 1 yield a pair of realized values $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ from the experiment, corresponding to a Learner model whose error is $\\varepsilon _ { t } .$ , and for which the worst subgroup the Auditor was able to find had unfairness $\\gamma _ { t }$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.05144_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.05144",
      "figure_id": "1711.05144_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "image_filename": "1711.05144_page0_fig2.jpg",
      "caption": "(a)",
      "context_before": "We begin by examining the evolution of the error and unfairness of Learner’s model. In the left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of γ ranging from 0 to 0.029. Several comments are in order.\n\nFirst, after an initial period in which there is a fair amount of oscillatory behavior, by 6000 iterations most of the curves have largely flattened out, and by 8,000 iterations it appears most but not all have reached approximate convergence. Second, while the top-to-bottom ordering of these error curves is approximately aligned with decreasing γ — so larger γ generally results in lower error, as expected — there are many violations of this for small $t$ , and even a few at large t. Third, and as we will examine more closely shortly, the converged values at large t do indeed exhibit a range of errors.\n\nIn the right panel of Figure 1, we show the corresponding unfairness $\\gamma _ { t }$ of the subgroup found by the Auditor at each iteration $t$ for the same runs and values of the parameter $\\gamma$ (indicated by horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is generally reversed — larger values of $\\gamma$ generally lead to higher $\\gamma _ { t }$ curves, since the fairness constraint on the Learner is weaker. We again see a great deal of early oscillatory behavior, with most $\\gamma _ { t }$ curves then eventually settling at or near their corresponding input γ value, as Learner and Auditor engage in a back-and-forth struggle for lower error for Learner and $\\gamma$ -subgroup fairness for Auditor.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.05144_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.05144",
      "figure_id": "1711.05144_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig3.jpg",
      "image_filename": "1711.05144_page0_fig3.jpg",
      "caption": "(b) Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "context_before": "",
      "context_after": "For any choice of the parameter $\\gamma$ , and each iteration $t$ , the two panels of Figure 1 yield a pair of realized values $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ from the experiment, corresponding to a Learner model whose error is $\\varepsilon _ { t } .$ , and for which the worst subgroup the Auditor was able to find had unfairness $\\gamma _ { t }$ . The set of all $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all runs or $\\gamma$ values thus represents the different trade-offs between error and unfairness found by our algorithm on the data. Most of these pairs are of course Pareto-dominated by other pairs, so we are primarily interested in the undominated frontier.\n\nIn the left panel of Figure 2, for each value of $\\gamma$ we show the Pareto-optimal pairs, color-coded for the value of $\\gamma$ . Each value of $\\gamma$ yields a set or cloud of undominated pairs that are usually fairly\n\nclose to each other, and as expected, as γ is increased, these clouds generally move leftwards and upwards (lower error and higher unfairness).",
      "referring_paragraphs": [
        "In the left panel of Figure 2, for each value of $\\gamma$ we show the Pareto-optimal pairs, color-coded for the value of $\\gamma$ . Each value of $\\gamma$ yields a set or cloud of undominated pairs that are usually fairly",
        "We anticipate that the practical use of our algorithm would, as we have done, explore many values of $\\gamma$ and then pick a model corresponding to a point on the aggregated Pareto frontier across all $\\gamma$ , which represents the collection of all undominated models and the overall errorunfairness trade-off. This aggregate frontier is shown in the right panel of Figure 2, and shows a relatively smooth menu of options, ranging from error about 0.21 and no unfairness at one extreme, to error a",
        "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.05144_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1711.07076": [
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg",
      "image_filename": "1711.07076_page0_fig0.jpg",
      "caption": "32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.",
      "context_before": "Recently, owing to the increased use of machine learning (ML) to assist in consequential decisions, the topic of quantifying and mitigating ML-based discrimination has attracted interest in both policy and ML. However, while the existing legal doctrine offers qualitative ideas, intervention in an MLbased system requires more concrete formalism. Inspired by the relevant legal concepts, technical papers have proposed several criteria to quantify discrimination. One criterion requires that the fraction given a positive decision be equal across different groups. Another criterion states that a\n\narXiv:1711.07076v3 [stat.ML] 11 Jan 2019\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig1.jpg",
      "image_filename": "1711.07076_page0_fig1.jpg",
      "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
      "context_before": "",
      "context_after": "classifier should be blind to the protected characteristic. Within the technical literature, these criteria are commonly referred to as disparate impact and disparate treatment, respectively.\n\nIn this paper, we call these technical criteria impact parity and treatment parity to distinguish them from their legal antecedents. The distinction between technical and legal terminology is important to maintain. While impact and treatment parity are inspired by legal concepts, technical approaches that achieve these criteria may fail to satisfy the underlying legal and ethical desiderata.\n\nWe demonstrate one such disconnect through DLPs, a class of algorithms designed to simultaneously satisfy treatment- and impact-parity criteria [3–5]. DLPs operate according to the following principle: The protected characteristic may be used during training, but is not available to the model at prediction time. In the earliest such approach the protected characteristic is used to winnow the set of acceptable rules from an expert system [3]. Others incorporate the protected characteristic as either a regularizer, a constraint, or to preprocess the training data [5–7].",
      "referring_paragraphs": [
        "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women. We apply the DLP proposed by Zafar et al. [5], using code available from the authors.2 While the DLP nearly equalizes hiring rates (satisfying a $10 5 \\mathrm { - } \\%$ rule), it does so through a problematic within-class discrimination mechanism. The DLP rule advantages individuals with longer hair over those with shorter hair and considerably longer work experi",
        "Finally, for reproducibility, we repeat our experiments from Section 4.2 on a variety of public datasets (code and data will be released at publication time). Again we compare applying our simple thresholding scheme against the fairness constraint of [5], considering a binary outcome and a single protected feature. Basic info about these datasets (including the prediction target and protected feature) is shown in Table 1.",
        "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1).",
        "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women.",
        "Starting from the\n\nTable 1: Statistics of public datasets.",
        "Basic info about these datasets (including the prediction target and protected feature) is shown in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg",
      "image_filename": "1711.07076_page0_fig2.jpg",
      "caption": "2https://github.com/mbilalzafar/fair-classification/",
      "context_before": "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women. We apply the DLP proposed by Zafar et al. [5], using code available from the authors.2 While the DLP nearly equalizes hiring rates (satisfying a $10 5 \\mathrm { - } \\%$ rule), it does so through a problematic within-class discrimination mechanism. The DLP rule advantages individuals with longer hair over those with shorter hair and considerably longer work experience. We find that several women who would have been hired under historical practices, owing to their $^ { 1 2 + }$ years of work experience, would not be hired under the DLP due to their short hair (i.e., their male-like characteristics captured in x). Similarly, several men, who would not have been hired based on work experience alone, are advantaged by the DLP due to their longer hair (i.e., their ‘female-like’ characteristics in $\\mathbf { x }$ ). The DLP violates rational ordering, and harms some of the most qualified individuals in the protected group. Group parity is achieved at the cost of individual unfairness.\n\nGranted, we might not expect factors such as hair length to knowingly be used as inputs to a typical hiring algorithm. We construct this toy example to illustrate a more general point: since DLPs do not have direct access to the protected feature, they must infer from the other features which people are most likely to belong to each subgroup. Using the protected feature directly can yield more\n\n2https://github.com/mbilalzafar/fair-classification/",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig3.jpg",
      "image_filename": "1711.07076_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig4.jpg",
      "image_filename": "1711.07076_page0_fig4.jpg",
      "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
      "context_before": "",
      "context_after": "reasonable policies: For example, by applying per-group thresholds, we could hire the highest rated individuals in each group, rather than distorting rankings within groups based on how female/male individuals appear to be from their other features.\n\n4.2 Case study: Gender bias in CS graduate admissions\n\nFor our next example, we demonstrate a similar result but this time by analyzing real data with synthetic discrimination, to empirically demonstrate our arguments. We consider a sample of ${ \\sim } 9 { , } 0 0 0$ students considered for admission to the MS program of a large US university over an 11-year period. Half of the examples are withheld for testing. Available attributes include basic information, such as country of origin, interest areas, and gender, as well as quantitative fields such as GRE scores. Our data also includes a label in the form of an ‘above-the-bar’ decision provided by faculty reviewers. Admission rates for male and female applicants were observed to be within $1 \\%$ of each other. So, to demonstrate the effects of DLPs, we corrupt the data with synthetic discrimination. Of all women who were admitted, i.e., $z _ { i } = b , y _ { i } = 1$ , we flip $2 5 \\%$ of those labels to 0: giving noisy labels $\\bar { y } _ { i } = y _ { i } \\cdot \\eta$ , for $\\eta \\sim B e r n o u l l i ( . 2 5 )$ . This simulates historical bias in the training data.",
      "referring_paragraphs": [
        "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary. Furthermore, students predicted to be male $\\mathbf { \\dot { X } }$ -axis) tend to be flipped to the negative class (left half of plot) while students predicted to be female tend to be flipped to the positive class (right",
        "The protocol we follow is the same as in Section 4.2. Each of these datasets exhibits a certain degree of bias w.r.t. the protected characteristic (Table 2), so no synthetic discrimination is applied. In Table 2, we compare (1) The $p \\%$ rule obtained using the classifier of [5] compared to that of a naïve classifier (column k vs. column h); and (2) The $p \\mathrm { - } \\%$ rule obtained when applying our thresholding strategy from Section 4.2. As before, half of the data are withheld for testi",
        "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data.",
        "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary.",
        "the protected characteristic (Table 2), so no synthetic discrimination is applied."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_6",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/73253ae8394dedc7c7f2c1c9f3baecda271905c3ae24a045329e78298413e562.jpg",
      "image_filename": "73253ae8394dedc7c7f2c1c9f3baecda271905c3ae24a045329e78298413e562.jpg",
      "caption": "Table 1: Statistics of public datasets.",
      "context_before": "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary. Furthermore, students predicted to be male $\\mathbf { \\dot { X } }$ -axis) tend to be flipped to the negative class (left half of plot) while students predicted to be female tend to be flipped to the positive class (right half of plot). This is shown in detail in Figure 2 (center and right). Of the 43 students whose decisions are flipped to ‘non-admit,’ 5 are female, each of whom has ‘male-like’ characteristics according to their other features as demonstrated in our synthetic hair-length example. Demonstrated here with real-world data, the DLP both disrupts the within-group ordering and violates the do no harm principle by disadvantaging some women who, but for the DLP, would have been admitted.\n\nComparison with Treatment Disparity. To demonstrate the better performance of per-group thresholding, we implement a simple decision scheme and compare its performance to the DLP.\n\nOur thresholding rule for maximizing accuracy subject to a $p { - } \\%$ rule works as follows: Recall that the $p { - } \\%$ rule requires that $q _ { b } / q _ { a } > p / 1 0 0$ , which can be written as $\\begin{array} { r } { \\frac { p } { 1 0 0 } q _ { a } - q _ { b } < 0 } \\end{array}$ . We denote the quantity $\\frac { p } { 1 0 0 } q _ { a } - q _ { b }$ as the $p$ -gap. To maximize accuracy subject to satisfying the $p { - } \\%$ rule, we construct a score that quantifies reduction in $p$ -gap per reduction in accuracy. Starting from the",
      "context_after": "accuracy-maximizing classifications $\\hat { y }$ (thresholding at .5), we then flip those predictions which close the gap fastest:\n\n1. Assign each example with $\\{ \\tilde { y } _ { i } = 0 , z _ { i } = b \\}$ or $\\{ \\tilde { y } _ { i } = 1 , z _ { i } = a \\}$ , a score $c _ { i }$ equal to the reduction in the p-gap divided by the reduction in accuracy:\n\n2. Flip examples in descending order according to this score until the desired CV-score is reached.",
      "referring_paragraphs": [
        "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women. We apply the DLP proposed by Zafar et al. [5], using code available from the authors.2 While the DLP nearly equalizes hiring rates (satisfying a $10 5 \\mathrm { - } \\%$ rule), it does so through a problematic within-class discrimination mechanism. The DLP rule advantages individuals with longer hair over those with shorter hair and considerably longer work experi",
        "Finally, for reproducibility, we repeat our experiments from Section 4.2 on a variety of public datasets (code and data will be released at publication time). Again we compare applying our simple thresholding scheme against the fairness constraint of [5], considering a binary outcome and a single protected feature. Basic info about these datasets (including the prediction target and protected feature) is shown in Table 1.",
        "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1).",
        "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women.",
        "Starting from the\n\nTable 1: Statistics of public datasets.",
        "Basic info about these datasets (including the prediction target and protected feature) is shown in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_7",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/0fbfcf39dc88d96ca575ba08fe2933a6560752dae4588585fbe2e534623dfe81.jpg",
      "image_filename": "0fbfcf39dc88d96ca575ba08fe2933a6560752dae4588585fbe2e534623dfe81.jpg",
      "caption": "Table 2: Comparison between unconstrained classification, DLPs, and thresholding schemes. Note that the $p \\%$ rules from [5] were the strongest that could be obtained with their method; on complex datasets $p \\%$ rules of $100 \\%$ are rarely obtained in practice, due to their specific approximation scheme. Employee and Customer datasets are from IBM, the others are UCI datasets.",
      "context_before": "The protocol we follow is the same as in Section 4.2. Each of these datasets exhibits a certain degree of bias w.r.t. the protected characteristic (Table 2), so no synthetic discrimination is applied. In Table 2, we compare (1) The $p \\%$ rule obtained using the classifier of [5] compared to that of a naïve classifier (column k vs. column h); and (2) The $p \\mathrm { - } \\%$ rule obtained when applying our thresholding strategy from Section 4.2. As before, half of the data are withheld for testing.\n\nFirst, we note that in most cases, the method of [5] increases the $p \\mathrm { - } \\%$ rule (column k vs. h), while maintaining an accuracy similar to that of unconstrained classification (column i vs. f). One exception is the UCI-Credit dataset, in which both the accuracy and the $p \\mathrm { - } \\%$ rule simultaneously decrease; although this is against our expectations, note that the optimization technique of [5] is an approximation scheme and does not offer accuracy guarantees in practice (nor can it in general achieve a $p \\%$ rule of $100 \\%$ ). However these details are implementation-specific and not the focus of this paper. Second, as in Section 4.2, we note that the optimal thresholding strategy is able to offer a strictly larger $p \\mathrm { - } \\%$ rule (column l vs. k) at a given accuracy (in this case, the accuracy from column i). In most cases, we can obtain a $p \\%$ rule of (close to) $100 \\%$ at the given accuracy.\n\nWe emphasize that the goal of our experiments is not to ‘beat’ the method of [5], or even to comment on any specific discrimination-aware classification scheme. Rather, we emphasize that any DLP is fundamentally upper-bounded (in terms of the $p \\mathrm { - } \\%$ rule/accuracy trade-off) by simple schemes that explicitly consider the protected feature. Our experiments validate this claim, and reveal that the two schemes make strikingly different decisions. While concealing the protected feature from the classifier may be conceptually desirable, practitioners should be aware of the consequences.",
      "context_after": "Coming to terms with treatment disparity. Legal considerations aside, treatment disparity approaches have three advantages over DLPs: they optimally trade accuracy for representativeness, preserve rankings among members of each group, and do no harm to members of the disadvantaged group. In addition, treatment disparity has another advantage: by setting class-dependent thresholds, it’s easier to understand how treatment disparity impacts individuals. It seems plausible that policy-makers could reason about thresholds to decide on the right trade-off between group equality and individual fairness. By contrast the tuning parameters of DLPs may be harder to reason about from a policy standpoint. Several key challenges remain. Our theoretical arguments demonstrate that thresholding approaches are optimal in the setting where we assume complete knowledge of the data-generating distribution. It is not always clear how best to realize these gains in practice, where imbalanced or unrepresentative datasets can pose a significant obstacle to accurate estimation.\n\nSeparating estimation from decision-making. In the context of algorithmic, or algorithmsupported decision-making, it’s often useful to obtain not just a classification, but also an accurate probability estimate. These estimates could then be incorporated into the decision-theoretic part of the pipeline where appropriate measures could be taken to align decisions with social values. By intervening at the modeling phase, DLPs distort the predicted probabilities themselves. It’s not clear what the outputs of the resulting classifiers actually signify. In unconstrained learning approaches, even if the label itself may reflect historical prejudice, one at least knows what is being estimated. This leaves open the possibility of intervening at decision time to promote more equal outcomes.\n\nFairness beyond disparate impact How best to quantify discrimination and unfairness remains an important open question. The CV scores and $p - \\%$ rules offer one set of definitions, but there are many other parity criteria to which our results do not directly apply, e.g., equality of opportunity [13]. Other notions of fairness and the trade-offs between them have been studied [14, 26–29]. In a recent paper, Zafar et al. [30] depart from parity-based definitions and propose instead a preference-based notion of fairness. Dwork et al. [11] address the problem of how best to incorporate information about protected characteristics for several of these other fairness criteria.",
      "referring_paragraphs": [
        "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary. Furthermore, students predicted to be male $\\mathbf { \\dot { X } }$ -axis) tend to be flipped to the negative class (left half of plot) while students predicted to be female tend to be flipped to the positive class (right",
        "The protocol we follow is the same as in Section 4.2. Each of these datasets exhibits a certain degree of bias w.r.t. the protected characteristic (Table 2), so no synthetic discrimination is applied. In Table 2, we compare (1) The $p \\%$ rule obtained using the classifier of [5] compared to that of a naïve classifier (column k vs. column h); and (2) The $p \\mathrm { - } \\%$ rule obtained when applying our thresholding strategy from Section 4.2. As before, half of the data are withheld for testi",
        "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data.",
        "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary.",
        "the protected characteristic (Table 2), so no synthetic discrimination is applied."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.07076_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1711.08536": [
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg",
      "image_filename": "1711.08536_page0_fig0.jpg",
      "caption": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
      "context_before": "The first version of the ImageNet data set was released in 2009 by Deng et al. [2]. An updated 2012 release [4], used to train the model in this paper, consisted of approximately 1.2 million image thumbnails and URLs from 1000 categories. Each image in the data set is associated with a humanverified single label. The Open Images data set, released in 2016 by Krasin et al. [3], contains about 9 million URLs to Creative Commons licensed images. There are 6012 image-level human-verified labels, and each image can be associated with multiple labels.\n\narXiv:1711.08536v1 [stat.ML] 22 Nov 2017\n\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig1.jpg",
      "image_filename": "1711.08536_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig2.jpg",
      "image_filename": "1711.08536_page0_fig2.jpg",
      "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes. Figure 2: Distribution of the geographically identifiable images in the Open Images data set, by country. Almost a third of the data in our sample was US-based, and $60 \\%$ of the data was from the six most represented countries across North America and Europe.",
      "context_before": "",
      "context_after": "Pretrained image classification models trained on both ImageNet and Open Images are publicly available on the Tensorflow [1] $\\mathrm { S l i m } ^ { 1 }$ and Open Images Github2 pages, respectively. For each data set, we use publicly released pretrained models with the Inception V3 [6] architecture, which gives competitive performance across standard benchmarks.\n\n3 Analyzing Geo-Diversity\n\nOur first goal was to assess the geo-diversity of the images in the open source data sets. It is naturally difficult to identify the geo-location of every image in previously released open source image data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.",
      "referring_paragraphs": [
        "Geo-Diversity of ImageNet. For the 14 million images in the fall 2011 release of the ImageNet data set,3 we similarly acquired country-level geo-location data. We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1. Around $45 \\%$ of the data in our sample was US-based. Here, China and India were represented with $1 \\%$ and $2 . 1 \\%$ of the images, respectively.",
        "Figure 1: Fraction of Open Images and ImageNet images from each country.",
        "We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg",
      "image_filename": "1711.08536_page0_fig3.jpg",
      "caption": "2https://github.com/openimages/dataset",
      "context_before": "Geo-Diversity of Open Images. Of the 9 million images in the Open Images data set, we were able to acquire country-level geo-location for roughly 2 million. This is a large (but potentially non-uniform) subset of the overall data. Geo-location data is shown in Figures 1 and 2. Overall, more than $32 \\%$ of the sample data was US-based and $60 \\%$ of the data was from the six most represented\n\n1https://github.com/tensorflow/models/tree/master/research/slim\n\n2https://github.com/openimages/dataset",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig4.jpg",
      "image_filename": "1711.08536_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig5.jpg",
      "image_filename": "1711.08536_page0_fig5.jpg",
      "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.",
      "context_before": "",
      "context_after": "countries across North America and Europe. Meanwhile, China and India – the two most populous countries in the world – were represented with only $1 \\%$ and $2 \\%$ of the images, respectively. Despite our expectation that there would be some skew, we were surprised to find this level of imbalance.\n\nGeo-Diversity of ImageNet. For the 14 million images in the fall 2011 release of the ImageNet data set,3 we similarly acquired country-level geo-location data. We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1. Around $45 \\%$ of the data in our sample was US-based. Here, China and India were represented with $1 \\%$ and $2 . 1 \\%$ of the images, respectively.\n\n4 Analyzing Classification Behavior Based on Geo-Location",
      "referring_paragraphs": [
        "Figure 3 shows some categories that showed noticeable differences in performance. These differences appear in both classifiers, suggesting that this problem is not particular to a single data set. Using the geolocated images from the web, we compare performance between countries (Figure 4). Some classes of images have similar distributions of predictions across countries, indicating that the training data set is better-represented in such classes.",
        "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets.",
        "Figure 3 shows some categories that showed noticeable differences in performance."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg",
      "image_filename": "1711.08536_page0_fig6.jpg",
      "caption": "3http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz",
      "context_before": "Geo-located web images. While the raters in Hyderabad gave us one source of location-specific image data, we needed another approach to find data from a wider range of countries. To this end, we first identified 15 countries to target and joined the per-country location proxy described above with inferred labels from a classifier similar to Google Cloud Vision API, across a large data store of images from the web. For analysis, we focused on labels related to “people”, such as bridegroom, police officer, and greengrocer.\n\nOne limitation of this work is that even our geographically diverse images were collected from the internet using tools that rely (at least partially) on image classifiers themselves. The human raters used web search to find images that depicted people from their communities. Similarly, when building a data set from underrepresented countries using geo-located web images to stress-test a classifier, an image classifier was used to filter for relevant images.\n\n3http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig7.jpg",
      "image_filename": "1711.08536_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig8.jpg",
      "image_filename": "1711.08536_page0_fig8.jpg",
      "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.",
      "context_before": "",
      "context_after": "Geo-Dependent Mis-Classifications. Looking over groom, bridegroom images supplied by the Hyderabad raters, we found that the classifier trained on ImageNet data was likely to misclassify these images as chain mail, a kind of armor. Other images were misclassified as focusing on cloth, academic gown, or vestment. Using a method similar to SmoothGrad [5], we looked at saliency maps to determine which parts of the images were most depended on by the model when making these classifications. Surprisingly, in all cases that we looked at, the human face in the image was highlighted rather than the attire, despite the fact that the majority of misclassifications assigned an attire-based label.\n\nClassifier performance on localized data. We use two pretrained models, one trained on ImageNet and another trained on Open Images to test the difference in classifiers’ performances between data drawn from the standard evaluation data split in ImageNet and Open Images and rater-supplied images.\n\nFigure 3 shows some categories that showed noticeable differences in performance. These differences appear in both classifiers, suggesting that this problem is not particular to a single data set. Using the geolocated images from the web, we compare performance between countries (Figure 4). Some classes of images have similar distributions of predictions across countries, indicating that the training data set is better-represented in such classes.",
      "referring_paragraphs": [
        "Figure 3 shows some categories that showed noticeable differences in performance. These differences appear in both classifiers, suggesting that this problem is not particular to a single data set. Using the geolocated images from the web, we compare performance between countries (Figure 4). Some classes of images have similar distributions of predictions across countries, indicating that the training data set is better-represented in such classes.",
        "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_10",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg",
      "image_filename": "1711.08536_page0_fig9.jpg",
      "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.",
      "context_before": "We focused on labels relating to humans in this work, but noticeable distributional differences between developed and developing countries can occur other areas as well, including sports, transportation, and wildlife.\n\nIt is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world. This is not too surprising, as these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems.\n\nThis study highlights the importance of assessing the appropriateness of a given data set before using it to learn models for use in the developing world. Equally, this work emphasizes the importance of creating new data sets that prioritize broad geo-representation as first class goals, in order to aid ML in the developing world.",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 5 plots images of groom, bridegroom images from different countries by log likelihood. The US-based images are clustered to the far right, showing high confidence, while images from Ethiopia and Pakistan are much more uniformly distributed, showing poorer classifier performance. We confirmed this trend across several other countries in different regions of the world.",
        "Figure 5 plots images of groom, bridegroom images from different countries by log likelihood.",
        "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1711.08536_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1801.04385": [
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/7f42ddc2f4b37f32cafbcc5e1409bb37f84145852e4aca3b348893549be9fe45.jpg",
      "image_filename": "7f42ddc2f4b37f32cafbcc5e1409bb37f84145852e4aca3b348893549be9fe45.jpg",
      "caption": "Table 1: Examples of Simpson’s paradox in Stack Exchange data. For these variables, the trend in the outcome variable (answer acceptance) as a function of $X _ { p }$ in the aggregate data Xpreverses when the data disaggregated on $X _ { c }$ .",
      "context_before": "Words: Number of words in the answer.\n\nLines of codes: Number of lines of codes in the answer.\n\n1hps://archive.org/details/stackexchange",
      "context_after": "URLs: Number of hyperlinks in the answer.\n\nReadability: Answer’s Flesch Reading Ease [14] score.\n\n4.2 Simpson’s Paradoxes on Stack Exchange",
      "referring_paragraphs": [
        "e eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.",
        "Our approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its position (or",
        "index) within a session. According to Fig. 1a, which reports aggregate acceptance probability, answers wrien later in a session are more likely to be accepted than earlier answers. However, once the same data is disaggregated by session length, the trend reverses (Fig. 1b): each successive answer within the same session is less likely to be accepted than the previous answer. For example, for sessions during which ve answers were wrien, the rst answer is more likely to be accepted than the se",
        "Table 1: Examples of Simpson’s paradox in Stack Exchange data.",
        "Figure 1: Simpson’s paradox in Stack Exchange data."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/02ad1593a0b35c44205ae0bca93de6f06af65434d4129f16cfe5dbd6b648abf5.jpg",
      "image_filename": "02ad1593a0b35c44205ae0bca93de6f06af65434d4129f16cfe5dbd6b648abf5.jpg",
      "caption": "Table 2: Number of data points in each group",
      "context_before": "Our method uncovers a novel Simpson’s paradox for user experience variables Reputation and Number of Answers. In the aggregate data, acceptance probability increases as a function of the Number of Answers (Fig. 2a). is is consistent with our expectations that the more experienced users—who have wrien more answers over their tenure on Stack Exchange—produce higher quality answers. However, when data is conditioned on Reputation, the trend reverses (Fig. 2b). In other words, focusing on groups of users with the same reputation, those who have wrien more answers over their tenure are less likely to have a new answer accepted than the less active answerers.\n\n4.3 e Origins of Simpson’s paradox\n\nTo understand why Simpson’s paradox occurs in Stack Exchange data, we illustrate the mathematical explanation of Section 3.2 with examples from our study. Consider the paradox for Answer Position–Session Length Simpson’s pair, illustrated in Fig. 1a. In the disaggregated data, trend lines of acceptance probability for sessions of dierent length are stacked (Fig. 1b): answers produced during longer sessions are more likely to be accepted than answers",
      "context_after": "",
      "referring_paragraphs": [
        "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.",
        "1b): answers produced during longer sessions are more likely to be accepted than answers\n\nTable 2: Number of data points in each group   \n\n<table><tr><td>Session Length</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Data points</td><td>7.2M</td><td>2.6M</td><td>1.3M</td><td>0.7M</td><td>0.4M</td><td>0.3M</td><td>0.2M</td><td>0.1M</td></tr></table>",
        "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg",
      "image_filename": "1801.04385_page0_fig0.jpg",
      "caption": "(a) Aggregated Data",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig1.jpg",
      "image_filename": "1801.04385_page0_fig1.jpg",
      "caption": "(b) Disaggregated Data",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_5",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig2.jpg",
      "image_filename": "1801.04385_page0_fig2.jpg",
      "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers. (a) Aggregated Data",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "e eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.",
        "Our approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its position (or",
        "index) within a session. According to Fig. 1a, which reports aggregate acceptance probability, answers wrien later in a session are more likely to be accepted than earlier answers. However, once the same data is disaggregated by session length, the trend reverses (Fig. 1b): each successive answer within the same session is less likely to be accepted than the previous answer. For example, for sessions during which ve answers were wrien, the rst answer is more likely to be accepted than the se",
        "Table 1: Examples of Simpson’s paradox in Stack Exchange data.",
        "Figure 1: Simpson’s paradox in Stack Exchange data."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg",
      "image_filename": "1801.04385_page0_fig3.jpg",
      "caption": "(b) Disaggregated Data Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
      "context_before": "",
      "context_after": "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.\n\nWhat happens to the trend in the aggregated data? When calculating acceptance probability as a function of answer position, all sessions contribute to acceptance probability for the rst answer of a session. Sessions of length one dominate the average. When calculating acceptance probability for answers in the second position, sessions of length one do not contribute, and acceptance probability",
      "referring_paragraphs": [
        "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.",
        "1b): answers produced during longer sessions are more likely to be accepted than answers\n\nTable 2: Number of data points in each group   \n\n<table><tr><td>Session Length</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Data points</td><td>7.2M</td><td>2.6M</td><td>1.3M</td><td>0.7M</td><td>0.4M</td><td>0.3M</td><td>0.2M</td><td>0.1M</td></tr></table>",
        "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
      "image_filename": "1801.04385_page0_fig4.jpg",
      "caption": "(a) Disaggregated data",
      "context_before": "produced during shorter sessions. In addition, there are many more shorter sessions than longer ones. Table 2 reports the number of sessions of dierent length. By far, the most common session has length one: users write only one answer during these sessions. Each longer session is about half as common as a session that is one answer shorter.\n\nWhat happens to the trend in the aggregated data? When calculating acceptance probability as a function of answer position, all sessions contribute to acceptance probability for the rst answer of a session. Sessions of length one dominate the average. When calculating acceptance probability for answers in the second position, sessions of length one do not contribute, and acceptance probability",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig5.jpg",
      "image_filename": "1801.04385_page0_fig5.jpg",
      "caption": "(b) Joint distribution of $X _ { c }$ and $X _ { p }$ Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "context_before": "",
      "context_after": "is dominated by data from sessions of length two. Similarly, acceptance probability of answers in the third position is dominated by sessions of length three. Survivor bias excludes data from shorter sessions, which also have lower acceptance probability, creating an upward trend in acceptance probability.\n\nWe back up this intuitive explanation with mathematical analysis of Section 3.2. Although acceptance probability is decreasing as a function of Answer Position for each value of Session Length (Fig. 1b), the probability mass of Session Length is constantly moving towards larger values as Answer Position increases. Notice that as Answer Position increments from to $a + 1$ , sessions of length $a$ are no a alonger included (as the minimum session length is now $a + 1$ ). us, while Session Length has probability mass $\\mathrm { P r } ( X _ { c } = a | X _ { \\mathcal { P } } = a )$ when $X _ { p } = a$ , it has probability $\\operatorname* { P r } ( X _ { c } = a | X _ { p } = a + 1 ) = 0$ Xp at $X _ { p } = a + 1$ :\n\n$$ \\frac {d}{d x _ {p}} \\Pr (X _ {c} = a | X _ {p} = x _ {p}) \\| _ {x _ {p} = a} = - \\Pr (X _ {c} = a | X _ {p} = a). \\tag {11} $$",
      "referring_paragraphs": [
        "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg",
      "image_filename": "1801.04385_page0_fig6.jpg",
      "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.",
      "context_before": "In the real world this means that users, who have wrien more answers are not more likely to have a new answer they write accepted. In fact, among users with same Reputation, those who earned this reputation with fewer answers are more likely to have a new answer they write accepted as best answer. is suggests that such users are simply beer at answering questions, and that this can be detected early in their tenure on Stack Exchange (while they still have low reputation). Note, however, that an exception to the trend reversal occurs for users with very high reputation. In Stack Exchange, users can gain reputation by “Answer is marked accepted”, “Answer is voted up”, “estion is voted up”, etc. It seems that, high reputation users and low reputation users are dierent: for high reputation users, experience (number of wrien answers) is important, while for low reputation users the quality of answers, which may lead to votes, is more important. Analysis of this behavior is beyond the scope of this paper.\n\n4.4 Discussion and Implications\n\nPresence of a Simpson’s paradox in data can indicate interesting or surprising paerns [8], and for trends in social data, important behavioral dierences within a population. Since social data is oen generated by a mixture of subgroups, existence of Simpson’s",
      "context_after": "paradox suggests that these subgroups dier systematically and signicantly in their behavior. By isolating important subgroups in social data, our method can yield insights into their behaviors.\n\nFor example, our method identies Session Length as a conditioning variable for disaggregating data when studying trends in acceptance probability as a function of answer’s position within a session. In fact, prior work has identied session length as an important parameter in studies of online performance [1, 9, 15, 19]. Unless activity data is disaggregated into individual sessions—sequences of activity without an extended break—important paerns are obscured. A pervasive paern in online platforms is user performance deterioration, whereby the quality of a user’s contribution decreases over the course of a single session. is deterioration was observed for the quality of answers wrien on Stack Exchange [9], comments posted on Reddit [19], and the time spent reading posts on Facebook [15]. Our method automatically identies position of an action within a session and session length as an important pair of variables describing Stack Exchange.\n\nWe examine in detail one novel paradox discovered by our method for the Reputation–Number of Answers variables. e trends in Fig. 2b suggest that both variables jointly aect acceptance probability. Inspired by this observation, we construct a new variable— Reputation / Number of Answers—i.e., Reputation Rate. Figure 4 shows how acceptance probability changes with respect to Reputation Rate for dierent groups of users. ere is an strong upward trend, suggesting that answers provided by users with higher Reputation Rate are more likely to be accepted. Moreover, while the lines span reputations of an extremely broad range—from one to $^ { 1 0 0 , 0 0 0 - }$ they collapse onto a single curve. is suggests that Reputation Rate is a good proxy of user performance. e remaining paradoxes uncovered by our method could yield similarly interesting insights into user behavior on Stack Exchange.",
      "referring_paragraphs": [
        "We examine in detail one novel paradox discovered by our method for the Reputation–Number of Answers variables. e trends in Fig. 2b suggest that both variables jointly aect acceptance probability. Inspired by this observation, we construct a new variable— Reputation / Number of Answers—i.e., Reputation Rate. Figure 4 shows how acceptance probability changes with respect to Reputation Rate for dierent groups of users. ere is an strong upward trend, suggesting that answers provided by users wi",
        "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg",
      "image_filename": "1801.04385_page0_fig7.jpg",
      "caption": "(a) Disaggregated data",
      "context_before": "xcSecondly, our method of aggregating by simple averaging of the linear coecient signs of the subgroups means that trends within each subgroup are weighted equally regardless of how many datapoints are in that subgroup. is is contrary to multivariate linear models, which t the model parameters based on each datapoint (and so weigh heavily towards values of $X _ { c }$ with many datapoints). XcTo illustrate, we show that our algorithm nds Time Since Previous Answer - Answer Position as a Simpson’s pair, which a multivariate logistic regression does not. e variable Answer Position is the index of the answer a user has completed without an extended $( { \\romannumeral 1 0 0 }$ minute) break, and so Answer $= 1$ if Time Since Previous Answer $\\geq 1 0 0$ minutes and Answer $> 1$ if Time Since Previous Answer $< 1 0 0$ Position > minutes. Fig. (5a) shows that, for Answer Position $= 1$ <, the acceptance probability decreases as a function Positionof Time Since Previous Answer, possibly because beer users take shorter breaks. On the other hand, for other Answer Positions the trend is reversed, and acceptance probability increases with Time Since Previous Answer, suggesting that in short term, users who take more time to answer questions or take short breaks between questions write answers of higher quality.\n\nClearly, Time Since Previous Answer - Answer Position is an important Simpson’s pair, illustrating that time has a benecial eect on answer quality ar short time scales. even though it is detrimental on the aggregate level. Multivariate logistic regression does not capture this behaviour, as $6 5 \\%$ of the probability mass of Time Since Previous Answer is for values larger than 100 minutes, so when ing $f _ { p , c } ( \\alpha + \\beta X _ { p } + \\beta _ { c } X _ { c } )$ to the data, it tries to t a hyperplane, p,c p c cwhich describes the majority of the data as best as possible, in this case the decreasing trend corresponding to Answer $= 1$ .\n\nWe presented a method for systematically uncovering instances of Simpson’s paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data. Our mathematical analysis suggests that Simspon’s paradox",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_11",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig8.jpg",
      "image_filename": "1801.04385_page0_fig8.jpg",
      "caption": "(b) Joint distribution of $X _ { c }$ and $X _ { p }$ Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "context_before": "",
      "context_after": "is caused by both correlations between independent variables in data (Figs. 3b and 5b), as well as diering behaviour of the outcome variable within subgroups, illustrated here by the stacked curves of Figs. 1b and 2b. Failure to account for this eect can lead analysis to wrong conclusions about typical behavior of individuals.\n\nWe applied our method to real-world data from the questionanswering site Stack Exchange. We were specically interested in uncovering features aecting the probability that an answer wrien by a user will be accepted by the asker as the best answer to his or her question. We identied eleven relevant features of answers and users. Not only did the method conrm an existing paradox, but it also uncovered new instances of Simpson’s paradox.\n\nOur work opens several directions for future work. e proposed algorithm could benet from a more principled method to bin continuous data and more sophisticated techniques for re-aggregating the intercepts of the curves ed to disaggregated data. Also, while it appears that conditioning on $X _ { c }$ disaggregates the population into Xcmore homogeneous subgroups, we have not used formal methods, such as goodness of t, to test for beer t of regression models to data. Goodness of t may also be used to guide data disaggregation strategies. In addition, our method applies to explicitly declared variables, and not to latent variables that may aect data. While these and similar questions remain, our proposed method oers a promising tool for the analysis of heterogeneous social data.",
      "referring_paragraphs": [
        "Figure 5: A pair which multivariate logistic regression cannot nd in the data."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.04385_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1801.07593": [
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg",
      "image_filename": "1801.07593_page0_fig0.jpg",
      "caption": "Figure 1: The architecture of the adversarial network.",
      "context_before": "There has been significant work done in the area of debiasing various specific types of data or predictor.\n\nDebiasing word embeddings: Bolukbasi et al. (2016) devises a method to remove gender bias from word embeddings. The method relies on a lot of human input; namely, it needs a large “training set” of gender-specific words.\n\nSimple models: Lum and Johndrow (2016) demonstrate that removing the protected variable from the training data fails to yield a debiased model (since other variables can be highly correlated with the protected variable), and devise a method for learning fair predictive models in cases when the learning model is simple (e.g. linear regression). Hardt et al.",
      "context_after": "(2016) discuss the shortcomings of focusing solely on DE-MOGRAPHIC PARITY, present alternate definitions of fairness, and devise a method for deriving an unbiased predictor from a biased one, in cases when both the output variable and the protected variable are discrete.\n\nAdversarial training: Goodfellow et al. (2014) pioneered the technique of using multiple networks with competing goals to force the first network to “deceive” the second network, applying this method to the problem of creating reallife-like pictures. Beutel et al. (2017) apply an adversarial training method to achieve EQUALITY OF OPPORTUNITY in cases when the output variable is discrete. They also discuss the ability of the adversary to be powerful enough to enforce a fairness constraint even when it has access to a very small training sample.\n\n3 Adversarial Debiasing",
      "referring_paragraphs": [
        "We begin with a model, which we call the predictor, trained to accomplish the task of predicting $Y$ given $X$ . As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.",
        "Figure 1: The architecture of the adversarial network.",
        "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.",
        "(2016), the\n\nTable 1: Completions for he : she :: doctor : ?"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg",
      "image_filename": "1801.07593_page0_fig1.jpg",
      "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
      "context_before": "We update $U$ to minimize $L _ { A }$ at each training time step, according to the gradient $\\nabla _ { U } L _ { A }$ . We modify $W$ according\n\n2Achieving equality of odds and demographic parity are generally incongruent goals. See also Kleinberg, Mullainathan, and Raghavan (2016) for incongruency between calibration and equalized odds.\n\n3This last technique of restricting the training set is discussed at length by Beutel et al. (2017), so we only mention it here.",
      "context_after": "$$ \\nabla_ {W} L _ {P} - \\operatorname {p r o j} _ {\\nabla_ {W} L _ {A}} \\nabla_ {W} L _ {P} - \\alpha \\nabla_ {W} L _ {A} \\tag {1} $$\n\nwhere $\\alpha$ is a tuneable hyperparameter that can vary at each time step and we define $\\mathrm { p r o j } _ { v } x = 0$ if $v = 0$ .\n\nThe middle term p $\\mathbf { r o j } _ { \\nabla _ { W } L _ { A } } \\nabla _ { W } L _ { P }$ prevents the predictor from moving in a direction that helps the adversary decrease its loss while the last term, $\\alpha \\nabla _ { W } L _ { A }$ , attempts to increase the adversary’s loss. Without the projection term, it is possible for the predictor to end up helping the adversary (see Fig. 2). Without the last term, the predictor will never try to hurt the adversary, and, due to the stochastic nature of many gradient-based methods, will likely end up helping the adversary anyway. The result is that when training is completed the desired definition of equality should be satisfied.",
      "referring_paragraphs": [
        "The middle term p $\\mathbf { r o j } _ { \\nabla _ { W } L _ { A } } \\nabla _ { W } L _ { P }$ prevents the predictor from moving in a direction that helps the adversary decrease its loss while the last term, $\\alpha \\nabla _ { W } L _ { A }$ , attempts to increase the adversary’s loss. Without the projection term, it is possible for the predictor to end up helping the adversary (see Fig. 2). Without the last term, the predictor will never try to hurt the adversary, and, due to the stochastic nat",
        "Details on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception to the fnlwgt feature, which we discard. We convert the remaining columns into tensors where the categorical columns are sparse tensors, age is bucketized at boundaries [18, 25, 30, 35, 40, 45, 50, 55, 60, 65], and the rest of the continuous columns are real-valued.",
        "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.\n\nto the expression:",
        "Table 2: Features in the UCI dataset per individual.",
        "Details on the features that the dataset provides are available in Table 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig2.jpg",
      "image_filename": "1801.07593_page0_fig2.jpg",
      "caption": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for e",
      "context_before": "Proof. Since the adversary converges, $L _ { A } ( W ^ { * } , U ^ { * } ) \\ \\leq$ $L _ { A } ( W ^ { * } , U _ { 0 } )$ : otherwise, since $L _ { A }$ is convex in $U$ , the adversary’s weights would move toward $U _ { 0 }$ . In other words, the adversary’s minimum is the point at which the adversary gains an advantage from using $\\hat { Y }$ . Similarly, since the predictor converges, $L _ { A } ( W ^ { * } , U ^ { * } ) \\ge L _ { A } ( W _ { 0 } , \\bar { U } ^ { * } )$ : Otherwise, the predictor would be able to increase the adversary’s loss by moving toward $W _ { 0 }$ , and the projection term and negative weight on $\\nabla _ { W } L _ { A }$ in Eqn. 1 would push the predictor to move towards 0. Then:\n\n$$ \\begin{array}{l} L _ {A} \\left(W ^ {*}, U _ {0}\\right) \\geq L _ {A} \\left(W ^ {*}, U ^ {*}\\right) \\quad (\\text {a s}) \\\\ \\geq L _ {A} \\left(W _ {0}, U ^ {*}\\right) \\quad (\\text {a s}) \\\\ \\geq L _ {A} \\left(W _ {0}, U _ {0}\\right) \\quad (\\text {b y}) \\\\ = L _ {A} \\left(W ^ {*}, U _ {0}\\right) \\quad (\\text {b y}) \\\\ \\end{array} $$\n\nso we must have $L _ { A } ( W ^ { * } , U ^ { * } ) = L _ { A } ( W ^ { * } , U _ { 0 } )$ .",
      "context_after": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for equality of odds, it can be given both $\\hat { Y }$ and $Y$ .\n\nWe will show in the next propositions that the adversary gaining no advantage from information about $\\hat { Y }$ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.\n\nProposition 2. Let the training data be comprised of triples $( X , { \\bar { Y } } , Z )$ drawn according to some distribution $D$ . Suppose:",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/a9cae1e5ef3a65c7111c90a7f5a24a9469357bff317b2c966c9bb2effe311d98.jpg",
      "image_filename": "a9cae1e5ef3a65c7111c90a7f5a24a9469357bff317b2c966c9bb2effe311d98.jpg",
      "caption": "Table 1: Completions for he : she :: doctor : ?",
      "context_before": "If one trains generically a logistic regression model to predict $y$ given $x$ , it outputs something like $y = \\sigma ( 0 . 7 u + 0 . \\bar { 7 } r )$ , which is a reasonable model, but heavily incorporates the protected variable $r$ . To debias, We now train a model that achieves DEMOGRAPHIC PARITY. Note that removing the variable $r$ from the training data is insuffucient for debiasing: the model will still learn to use $u$ to predict $y$ , and $u$ is correlated with $r$ . If we use the described technique and add in another logistic model that tries to predict $z$ given $y$ , we find that the predictor model outputs something like $y = \\sigma ( 0 . 6 u - 0 . 6 r \\ ' + 0 . 6 )$ . Notice that not only is $r$ not included with a positive weight anymore, the model actually learns to use a negative weight on $r$ in order to balance out the effect of $r$ on $u$ Notice that $u - r \\sim N ( 0 , 2 )$ ; i.e., it is not dependent on $r$ , so we have successfully trained a model to predict $y$ independently of $r$ .\n\nWe train a model to perform the analogy task (i.e., fill in the blank: man : woman :: he : ?).\n\nIt is known that word embeddings reflect or amplify problematic biases from the data they are trained on, for example, gender (Bolukbasi et al. 2016). We seek to train a model that can still solve analogies well, but is less prone to these gender biases. We first calculate a “gender direction” $g$ using a method based on Bolukbasi et al. (2016) which gives a method for defining the protected variable. We will use this technique in the context of defining gender for word embeddings, but, as discussed in Bolukbasi et al. (2016), the",
      "context_after": "technique generalizes to other protected variables and other forms of embeddings. Following Bolukbasi et al. (2016), we pick 10 (male, female) word pairs, and define the and define the bias subspace to be the space spanned by the top $k$ principal components of the differences, where $k$ is a tuneable parameter. In our experiments, we find that $k = 1$ gives reasonable results, so we did not experiment further.\n\nWe use embeddings trained from Wikipedia to generate input data from the Google analogy data set (Mikolov et al. 2013). For each analogy in the dataset, we let $x =$ $\\left( { { x } _ { 1 } } , { { x } _ { 2 } } , { { x } _ { 3 } } \\right) \\in { \\mathbb { R } } ^ { 3 d }$ comprise the word vectors for the first three words, $y$ be the word vector of the fourth word, and $z$ be $\\operatorname { p r o j } _ { g } y$ $_ { g } y$ . It is worth noting that these word vectors computed from the original embeddings are never updated nor is there projection onto the bias subspace and therefore the original word embeddings are never modified. What is learned is a tranform from a biased embedding space to a debiased embedding space.\n\nAs a model, we use the following: let $v = x _ { 2 } + x _ { 3 } - x _ { 1 }$ , and output $\\boldsymbol { \\hat { y } } \\ = \\ v \\ - \\ w w ^ { T } \\boldsymbol { v }$ , where our model parameter is $w$ . Intuitively, $v$ is the “generic” analogy vector as is commonly5 used for the analogy task. If left to its own devices (i.e., if not told to be unbiased on anything), the model should either learn $w = 0$ or else learn $w$ as a useless vector.",
      "referring_paragraphs": [
        "We begin with a model, which we call the predictor, trained to accomplish the task of predicting $Y$ given $X$ . As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.",
        "Figure 1: The architecture of the adversarial network.",
        "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.",
        "(2016), the\n\nTable 1: Completions for he : she :: doctor : ?"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/24ed65306e56dfae5a76f82264e0d84dfadd62bf9e63acafad5c5b29d78a1bb9.jpg",
      "image_filename": "24ed65306e56dfae5a76f82264e0d84dfadd62bf9e63acafad5c5b29d78a1bb9.jpg",
      "caption": "Table 2: Features in the UCI dataset per individual. Features are either continuous (Cont) or Categorical (Cat). Categorical features are converted to sparse tensors for the model.",
      "context_before": "We use embeddings trained from Wikipedia to generate input data from the Google analogy data set (Mikolov et al. 2013). For each analogy in the dataset, we let $x =$ $\\left( { { x } _ { 1 } } , { { x } _ { 2 } } , { { x } _ { 3 } } \\right) \\in { \\mathbb { R } } ^ { 3 d }$ comprise the word vectors for the first three words, $y$ be the word vector of the fourth word, and $z$ be $\\operatorname { p r o j } _ { g } y$ $_ { g } y$ . It is worth noting that these word vectors computed from the original embeddings are never updated nor is there projection onto the bias subspace and therefore the original word embeddings are never modified. What is learned is a tranform from a biased embedding space to a debiased embedding space.\n\nAs a model, we use the following: let $v = x _ { 2 } + x _ { 3 } - x _ { 1 }$ , and output $\\boldsymbol { \\hat { y } } \\ = \\ v \\ - \\ w w ^ { T } \\boldsymbol { v }$ , where our model parameter is $w$ . Intuitively, $v$ is the “generic” analogy vector as is commonly5 used for the analogy task. If left to its own devices (i.e., if not told to be unbiased on anything), the model should either learn $w = 0$ or else learn $w$ as a useless vector.\n\nBy contrast, if we add the adversarial discriminator network (here, simply $\\hat { z } = w _ { 2 } ^ { T } \\hat { y } )$ , we expect the debiased prediction model to learn that $w$ should be something close to $g$ $( \\ o \\mathrm { r } - g )$ , so that the discriminator cannot predict $z = p r o j _ { g } y$ . Indeed, both of these expectations hold: Without debiasing, the trained vector $w$ is approximately a unit vector nearly perpendicular to $g : w ^ { T } g \\stackrel { \\cdot \\cdot } { = } 0 . 0 8 , | | w | | = 0 . 8 2$ ; with debiasing, $w$ is approximately a unit vector pointing in a direction highly correlated with $\\begin{array} { r } { g : w ^ { T } g = 0 . 5 5 , \\lvert \\lvert w \\rvert \\rvert = 0 . 9 6 } \\end{array}$ . Even after debiasing, gendered analogies such as man : woman :: he : she are still preserved; however, many biased analogies go away, suggesting that the adversarial training process was indeed successful. An example of the kinds of changes in analogy completions observed after debiasing are illustrated in Table $1 ^ { 6 }$ .",
      "context_after": "To better align with the work in Beutel et al. (2017), we attempt to enforce EQUALITY OF ODDS on a model for the task of predicting the income of a person – in particular, predicting whether the income is $> { \\mathfrak { S } } 5 0 k$ – given various attributes about the person, as made available in the UCI Adult dataset (Asuncion and Newman 2007).\n\nDetails on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception to the fnlwgt feature, which we discard. We convert the remaining columns into tensors where the categorical columns are sparse tensors, age is bucketized at boundaries [18, 25, 30, 35, 40, 45, 50, 55, 60, 65], and the rest of the continuous columns are real-valued.\n\nAs discussed before, to enforce equality of odds, we give the adversary access to the true label $y$ . The adversary will learn the relationship between $y$ and $z$ regardless of what the predictor does; further, if the predictor’s predictions $\\hat { y }$ give more information about $z$ than is already contained in $y$ , the adversary will be able to improve its loss. Thus, the predictor, in attempting to fool the adversary, will move toward making sure that $\\hat { y }$ does not give such additional information; in other words, toward equality of odds.",
      "referring_paragraphs": [
        "The middle term p $\\mathbf { r o j } _ { \\nabla _ { W } L _ { A } } \\nabla _ { W } L _ { P }$ prevents the predictor from moving in a direction that helps the adversary decrease its loss while the last term, $\\alpha \\nabla _ { W } L _ { A }$ , attempts to increase the adversary’s loss. Without the projection term, it is possible for the predictor to end up helping the adversary (see Fig. 2). Without the last term, the predictor will never try to hurt the adversary, and, due to the stochastic nat",
        "Details on the features that the dataset provides are available in Table 2. We use both categorical and continuous columns as given, with exception to the fnlwgt feature, which we discard. We convert the remaining columns into tensors where the categorical columns are sparse tensors, age is bucketized at boundaries [18, 25, 30, 35, 40, 45, 50, 55, 60, 65], and the rest of the continuous columns are real-valued.",
        "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.\n\nto the expression:",
        "Table 2: Features in the UCI dataset per individual.",
        "Details on the features that the dataset provides are available in Table 2."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/f8a5da271202e5818f0db76045e8dc38022f67693a17833977d88ae3c6d87026.jpg",
      "image_filename": "f8a5da271202e5818f0db76045e8dc38022f67693a17833977d88ae3c6d87026.jpg",
      "caption": "Table 3: Confusion matrices on the UCI Adult dataset, with and without equality of odds enforcement.",
      "context_before": "where $c$ and $b$ are learnable scalars, $w _ { 2 }$ is a learnable vector, and $\\sigma ^ { - 1 }$ is the inverse of the sigmoid function (logit function) $\\sigma ^ { - 1 } ( t ) = \\log t - \\log ( 1 - t ) $ . Intuitively, we want our adversary to be able to learn functions of the form $\\hat { z } = f ( y , [ \\hat { y } > 0 . 5 ] )$ (i.e. dependent only on the boolean predicted value $[ \\hat { y } > 0 . 5 ]$ ), and thus enforce equality of odds. Here, the adversary would learn such a function by making c extremely large. We add 1 to $| c |$ to make sure the adversary never tries to ignore $\\hat { y }$ by setting $c = 0$ , which could be a difficult local minimum for the adversary to escape7. This adversary is both general enough to be used whenever $y$ and $z$ are both discrete8, and powerful enough that deviation from true equality of odds should cause the adversary to be able to decrease its loss.\n\nWithout tweaking, this algorithm ran into issues with local minima, and the resulting models were often closer to demographic parity than equality of odds. We implemented a technique that helped: by increasing the hyperparameter $\\alpha$ in Eqn. 1 over time, the predictor had a much easier time learning to deceive the adversary and therefore more strictly enforce equality of odds. We set $\\alpha = { \\sqrt { t } }$ (where $t$ is the step counter), and to avoid divergence we set the predictor’s step size to $\\eta \\propto 1 / t$ , so that $\\alpha \\eta \\to 0$ as is preferred for stochastic gradient-based methods such as Adam.\n\nWe train the model twice, once with debiasing and once without, and present side-by-side confusion matrices on the test set for income bracket with respect to the protected variable values Male and Female, shown in Table 3, and we present the false positive rates (FPR) and false negative rates (FNR) in Table 4. Note that false negative rate is equal to $1 -$ true positive rate, so the trade-offs are directly comparable to the $( x , y )$ values of an ROC curve.",
      "context_after": "We notice that debiasing has only a small effect on overall accuracy $( 8 6 . 0 \\%$ vs $8 4 . 5 \\%$ ), and that the debiased model indeed (nearly) obeys equality of odds: as shown in Table 4, with debiasing, the FNR and FPR values are approximately equal across sex subgroups: $0 . 0 6 4 7 \\approx 0 . 0 7 0 1$ and $0 . 4 4 5 8 \\approx 0 . 4 3 4 9$ .\n\nAlthough the values don’t exactly reach equality, neither difference is statistically significant: a two-proportion twotail large sample $z$ -test yields $p$ -values 0.25 for $y = 0$ and",
      "referring_paragraphs": [
        "We train the model twice, once with debiasing and once without, and present side-by-side confusion matrices on the test set for income bracket with respect to the protected variable values Male and Female, shown in Table 3, and we present the false positive rates (FPR) and false negative rates (FNR) in Table 4. Note that false negative rate is equal to $1 -$ true positive rate, so the trade-offs are directly comparable to the $( x , y )$ values of an ROC curve.",
        "We train the model twice, once with debiasing and once without, and present side-by-side confusion matrices on the test set for income bracket with respect to the protected variable values Male and Female, shown in Table 3, and we present the false positive rates (FPR) and false negative rates (FNR) in Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/a670a34fb79175675f388e5348270f7122f116ea7add225c3a76b1008f6e0d0b.jpg",
      "image_filename": "a670a34fb79175675f388e5348270f7122f116ea7add225c3a76b1008f6e0d0b.jpg",
      "caption": "Table 4: False Positive Rate (FPR) and False Negative Rate (FNR) for income bracket predictions for the two sex subgroups, with and without adversarial debiasing.",
      "context_before": "We notice that debiasing has only a small effect on overall accuracy $( 8 6 . 0 \\%$ vs $8 4 . 5 \\%$ ), and that the debiased model indeed (nearly) obeys equality of odds: as shown in Table 4, with debiasing, the FNR and FPR values are approximately equal across sex subgroups: $0 . 0 6 4 7 \\approx 0 . 0 7 0 1$ and $0 . 4 4 5 8 \\approx 0 . 4 3 4 9$ .\n\nAlthough the values don’t exactly reach equality, neither difference is statistically significant: a two-proportion twotail large sample $z$ -test yields $p$ -values 0.25 for $y = 0$ and",
      "context_after": "In this work, we demonstrate a general and powerful method for training unbiased machine learning models. We state and prove theoretical guarantees for our method under reasonable assumptions, demonstrating in theory that the method can enforce the constraints that we claim, across multiple definitions of fairness, regardless of the complexity of the predictor’s model, or the nature (discrete or continuous) of the predicted and protected variables in question. We apply the method in practice to two very different scenarios: a standard supervised learning task, and the task of debiasing word embeddings while still maintaining ability to perform a certain task (analogies). We demonstrate in both cases the ability to train a model that is demonstrably less biased than the original one, and yet still performs extremely well on the task at hand. We discuss difficulties in getting these models to converge. We propose, in the common case of discrete output and protected variables, a simple adversary that is usable regardless of the complexity of the underlying model.\n\nThis process yields many questions that require further work to answer.\n\n7This value added to $| c |$ is an adjustable hyperparameter; we found reasonable results using the value 1 and thus not feel the need to experiment further.",
      "referring_paragraphs": [
        "We train the model twice, once with debiasing and once without, and present side-by-side confusion matrices on the test set for income bracket with respect to the protected variable values Male and Female, shown in Table 3, and we present the false positive rates (FPR) and false negative rates (FNR) in Table 4. Note that false negative rate is equal to $1 -$ true positive rate, so the trade-offs are directly comparable to the $( x , y )$ values of an ROC curve.",
        "We notice that debiasing has only a small effect on overall accuracy $( 8 6 . 0 \\%$ vs $8 4 . 5 \\%$ ), and that the debiased model indeed (nearly) obeys equality of odds: as shown in Table 4, with debiasing, the FNR and FPR values are approximately equal across sex subgroups: $0 . 0 6 4 7 \\approx 0 . 0 7 0 1$ and $0 . 4 4 5 8 \\approx 0 . 4 3 4 9$ .",
        "We train the model twice, once with debiasing and once without, and present side-by-side confusion matrices on the test set for income bracket with respect to the protected variable values Male and Female, shown in Table 3, and we present the false positive rates (FPR) and false negative rates (FNR) in Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1801.07593_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1802.08139": [
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg",
      "image_filename": "1802.08139_page0_fig0.jpg",
      "caption": "(a)",
      "context_before": "Kusner et al. (2017) recently introduced a causal definition of fairness, called counterfactual fairness, which states that a decision is fair toward an individual if it coincides with the one that would have been taken in a counterfactual world in which the sensitive attribute were different, and suggested a general algorithm to achieve this notion. This definition considers the entire effect of the sensitive attribute on the decision as problematic. However, in many practical scenarios this is not the case. For example, in the Berkeley alleged sex bias case, female applicants were rejected more often than male applicants as they were more often applying to departments with lower admission rates. Such an effect of\n\narXiv:1802.08139v1 [stat.ML] 22 Feb 2018\n\n1DeepMind, London, UK. Correspondence to: Silvia Chiappa <csilvia@google.com>.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig1.jpg",
      "image_filename": "1802.08139_page0_fig1.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig2.jpg",
      "image_filename": "1802.08139_page0_fig2.jpg",
      "caption": "(c) Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .",
      "context_before": "",
      "context_after": "gender through department choice is not unfair.\n\nTo deal with such scenarios, we propose a novel fairness definition called path-specific counterfactual fairness, which states that a decision is fair toward an individual if it coincides with the one that would have been taken in a counterfactual world in which the sensitive attribute along the unfair pathways were different.\n\nIn order to achieve path-specific counterfactual fairness, a decision system needs to be able to discern the causal effect of the sensitive attribute on the decision along the fair and unfair pathways, and to disregard the effect along the latter pathways. Kilbertus et al. (2017) and Nabi & Shpitser (2018) propose to constrain the learning of the model parameters such that the unfair effect is eliminated or reduced. However, this approach has several limitations and restrictions:",
      "referring_paragraphs": [
        "It follows that the causal effect of $A$ on $Y$ can be seen as the information that $A$ sends to $Y$ through causal paths, i.e. directed paths, or as the conditional distribution of $Y$ given $A$ restricted to those paths. This implies that the causal effect of $A$ on $Y$ coincides with $p ( Y | A )$ only if there are no open noncausal, i.e. undirected, paths between $A$ and $Y$ . An example of an open undirected path from $A$ to $Y$ is given by $A \\left. C \\right. Y$ in Fig. 1(a): the variable ",
        "If confounders are present, then the causal effect can be retrieved by intervening on $A$ , which replaces the conditional distribution of $A$ with, in the case considered in this paper, a fixed value $a$ . For the model in Fig. 1(a), intervening on $A$ by setting it to the fixed value $a$ would correspond to replacing $p ( A | C )$ with a delta distribution $\\delta _ { A = a }$ , thereby removing the link from $C$ to $A$ and leaving the remaining conditional distributions $p ( { \\boldsymbol { Y",
        "Suppose that the GCM contains only one indirect path through a variable $M$ , as in Fig. 1(b). We define $Y _ { a } ( M ( a ^ { \\prime } ) )$ to be the counterfactual random variable that results from the intervention $A = a$ along $A Y$ and the intervention $A = a ^ { \\prime }$ along $A M Y$ . The average direct effect (ADE) and the average indirect effect (AIE) are given by1",
        "If the GCM contains a confounder for the effect of either $A$ or $M$ on $Y$ , e.g. $C$ in Fig. 1(c), then $p ( Y _ { a , m } ) \\neq p ( Y | A =$ $a , M = m$ ). In this case, by following similar arguments as used in Eq. (1) but conditioning on $C$ (and therefore assuming $Y _ { a , m } \\perp \\perp M _ { a ^ { \\prime } } | C )$ , we obtain",
        "In Table 1, we show the unfair and fair accuracy on the test set at different stages of the training, together with the corresponding MMD values for $H _ { m } , H _ { l }$ and $H _ { r }$ . As we can see, the MMD value for $H _ { m }$ is drastically reduced from 5,000 to 8,000 and 15,000 training steps, without drastic loss in accuracy. After 20,000 training steps, the fair accuracy reduces to that of a dummy classifier. These results were obtained by performing counterfactual correction for bo",
        "Figure 1.",
        "Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg",
      "image_filename": "1802.08139_page0_fig3.jpg",
      "caption": "(a)",
      "context_before": "To gain insights into the problem of path-specific fairness, consider the following linear model\n\n$$ \\begin{array}{l} A = \\operatorname {B e r n o u l l i} (\\pi), C = \\epsilon_ {c}, \\\\ M = \\theta^ {m} + \\theta_ {a} ^ {m} A + \\theta_ {c} ^ {m} C + \\epsilon_ {m}, \\\\ L = \\theta^ {l} + \\theta_ {a} ^ {l} A + \\theta_ {c} ^ {l} C + \\theta_ {m} ^ {l} M + \\epsilon_ {l}, \\\\ Y = \\theta^ {y} + \\theta_ {a} ^ {y} A + \\theta_ {c} ^ {y} C + \\theta_ {m} ^ {y} M + \\theta_ {l} ^ {y} L + \\epsilon_ {y}. \\tag {3} \\\\ \\end{array} $$\n\nThe variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms with variance $\\sigma _ { a } ^ { 2 } , \\sigma _ { c } ^ { 2 } , \\sigma _ { m } ^ { 2 } , \\sigma _ { l } ^ { 2 }$ and $\\sigma _ { y } ^ { 2 }$ . The GCM corresponding to this model is depicted in Fig. 2(c).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig4.jpg",
      "image_filename": "1802.08139_page0_fig4.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig5.jpg",
      "image_filename": "1802.08139_page0_fig5.jpg",
      "caption": "(c) Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).",
      "context_before": "",
      "context_after": "We want to learn to predict $Y$ from $A , C , M$ and $L$ . However, $A$ is a sensitive attribute, and its direct effect on $Y$ and effect through $M$ is considered unfair. Therefore, to obtain a fair decision system, we need to disregard the PSE of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ $I , A \\to M \\to , \\dots , \\to Y$ , namely along the green and dashed green-black links of Fig. 2(c). Notice that the dashed green-black links differ fundamentally from the green links; they contain unfairness only as a consequence of $A M$ , corresponding to the parameter $\\theta _ { a } ^ { m }$ , being unfair.\n\nAssume $a ^ { \\prime } = 0$ is the baseline value of $A$ . Using the recursive rule described in $\\ S 3 . 2$ , we can deduce that the counterfactual variable required to estimate the desired PSE is $Y _ { a } ( M ( a ) , L ( a ^ { \\prime } , M ( a ) ) )$ and has distribution\n\n$$ \\int_ {C, M, L} p (Y | a, C, M, L) p \\left(L \\mid a ^ {\\prime}, C, M\\right) p (M | a, C) p (C). $$",
      "referring_paragraphs": [
        "For example, the required counterfactual variable for the effect along the path $A \\ \\ W \\ \\ Y$ in Fig. 2(a) is $Y _ { a ^ { \\prime } } ( M ( a ^ { \\prime } ) , W ( a , M ( a ^ { \\prime } ) ) )$ . Indeed, in the first iteration, as $A$ and $M$ are direct causes of $Y$ along black arrows, whilst $W$ is a direct cause of $Y$ along a green arrow, we obtain $Y _ { a ^ { \\prime } } ( M ( a ^ { \\prime } ) , \\gamma _ { W } )$ . In the second iteration, as $M$ is a direct cause of $W$ along a black arro",
        "For the path $A Y$ in Fig. 2(b), we would need instead $p ( Y _ { a } ( M ( a ^ { \\prime } ) , W ( a ^ { \\prime } ) ) )$ . Under the assumption $Y _ { a , m , w } ~ \\bot$ $\\{ M _ { a ^ { \\prime } } , W _ { a ^ { \\prime } } \\}$ , we would obtain $p ( Y _ { a } ( M ( a ^ { \\prime } ) , W ( a ^ { \\prime } ) ) ) \\ =$ $\\begin{array} { r l } { \\int _ { m , w } p ( Y _ { a , m , w } ) p ( M _ { a ^ { \\prime } } , W _ { a ^ { \\prime } } ) } \\end{array}$ . However, $p ( Y _ { a , m , w } ) \\quad \\neq \\qq",
        "The variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms with variance $\\sigma _ { a } ^ { 2 } , \\sigma _ { c } ^ { 2 } , \\sigma _ { m } ^ { 2 } , \\sigma _ { l } ^ { 2 }$ and $\\sigma _ { y } ^ { 2 }$ . The GCM corresponding to this model is depicted in Fig. 2(c).",
        "We want to learn to predict $Y$ from $A , C , M$ and $L$ . However, $A$ is a sensitive attribute, and its direct effect on $Y$ and effect through $M$ is considered unfair. Therefore, to obtain a fair decision system, we need to disregard the PSE of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ $I , A \\to M \\to , \\dots , \\to Y$ , namely along the green and dashed green-black links of Fig. 2(c). Notice that the dashed green-black links differ fundamentally from the green",
        "For addressing a more general data-generation process mismatch than the one considered above, we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m }$ and $H _ { l }$ are",
        "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff",
        "Figure 2.",
        "Table 2."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg",
      "image_filename": "1802.08139_page0_fig6.jpg",
      "caption": "(a)",
      "context_before": "$$ \\begin{array}{l} m _ {\\text {f a i r}} ^ {n} = \\theta^ {m} + \\theta_ {a} ^ {p \\prime} + \\theta_ {c} ^ {m} c ^ {n} + \\epsilon_ {m} ^ {n}, \\\\ l _ {\\mathrm {f a i r}} ^ {n} = \\theta^ {l} + \\theta_ {a} ^ {l} + \\theta_ {c} ^ {l} c ^ {n} + \\theta_ {m} ^ {l} m _ {\\mathrm {f a i r}} ^ {n} + \\epsilon_ {l} ^ {n}, \\\\ \\hat {y} _ {\\text {f a i r}} ^ {n} = \\theta^ {y} + \\theta_ {a} ^ {y} + \\theta_ {c} ^ {y} c ^ {n} + \\theta_ {m} ^ {y} m _ {\\text {f a i r}} ^ {n} + \\theta_ {l} ^ {y} l _ {\\text {f a i r}} ^ {n} \\\\ = \\theta^ {y} + \\theta_ {a} ^ {y} - \\theta_ {a} ^ {y} + \\theta_ {c} ^ {y} c ^ {n} + \\theta_ {m} ^ {y} \\left(m ^ {n} - \\theta_ {a} ^ {m}\\right) \\\\ + \\theta_ {l} ^ {y} \\left(l ^ {n} - \\theta_ {m} ^ {l} \\theta_ {a} ^ {m}\\right). \\tag {4} \\\\ \\end{array} $$\n\nIn other words, we compute a fair prediction by intervening on $A$ , setting it to the baseline value along the links that create unfairness $A M$ and $A Y$ . This is effectively an extension of the procedure for performing counterfactual reasoning in structural equation models (Pearl, 2000), where the counterfactual correction is only restricted to the problematic links $A M$ and $A Y$ .\n\nModel-Observations Mismatch. Whilst this approach provides us with an elegant and straightforward way to",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig7.jpg",
      "image_filename": "1802.08139_page0_fig7.jpg",
      "caption": "(b) Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .",
      "context_before": "",
      "context_after": "impose path-specific counterfactual fairness, if there is a mismatch between the data-generation processes assumed by the learned model and underlying the observations, fairness is most likely not achieved.\n\nConsider, for example, the case in which we assume the data-generation process of Eq. (3), but the observed $m ^ { n }$ , $n = 1 , \\ldots , N$ , are generated from a modified version of Eq. (3) containing an extra non-linear term $f ( A , C )$ . The learned $\\theta$ would not be able to describe this non-linear term, which would therefore be absorbed into the noise values $\\epsilon _ { m } ^ { n }$ , making the noise and $A$ dependent, as shown in Fig. 3(a).\n\nTo solve this issue, we propose to introduce an explicit latent variable $H _ { m }$ for the generation of $M$ , i.e. $M = \\theta ^ { m } + \\theta _ { a } ^ { m } A +$ $\\theta _ { c } ^ { m } C + H _ { m } + \\epsilon _ { m }$ , obtaining the GCM of Fig. 3(b). Define",
      "referring_paragraphs": [
        "Consider, for example, the case in which we assume the data-generation process of Eq. (3), but the observed $m ^ { n }$ , $n = 1 , \\ldots , N$ , are generated from a modified version of Eq. (3) containing an extra non-linear term $f ( A , C )$ . The learned $\\theta$ would not be able to describe this non-linear term, which would therefore be absorbed into the noise values $\\epsilon _ { m } ^ { n }$ , making the noise and $A$ dependent, as shown in Fig. 3(a).",
        "To solve this issue, we propose to introduce an explicit latent variable $H _ { m }$ for the generation of $M$ , i.e. $M = \\theta ^ { m } + \\theta _ { a } ^ { m } A +$ $\\theta _ { c } ^ { m } C + H _ { m } + \\epsilon _ { m }$ , obtaining the GCM of Fig. 3(b). Define",
        "Kusner et al. (2017), who also use a latent-variable approach, do not enforce small dependence. To demonstrate that this is necessary, we learned the parameters of the modified model with Gaussian distribution $p ( H _ { m } )$ , using an expectation maximization approach. $\\tilde { p } ( H _ { m } | A )$ is shown in Fig. 3(a). As we can see, the extra term $f ( A , C )$ is absorbed by the latent variable. In other words, even if $p ( H _ { m } | A ) = p ( H _ { m } )$ , the mismatch between the",
        "Consider the GCM in Fig. 3(a), corresponding to Eq. (3) with the addition of a Gaussian latent variable $H _ { m } \\sim \\mathcal { N } ( \\theta ^ { h } , \\sigma _ { h } ^ { 2 } )$ in the equation for $M$ . The joint distribution $p ( Z = \\{ Y , L , M , C , H _ { m } \\} | A )$ is Gaussian with exponent proportional to $- { \\textstyle \\frac { 1 } { 2 } } \\big ( Z ^ { \\prime } N Z - 2 n \\big )$ with",
        "Figure 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg",
      "image_filename": "1802.08139_page0_fig8.jpg",
      "caption": "In our case, rather than $q ( H | V )$ , we use $q ( H | V ^ { * } \\equiv V \\setminus Y )$ . Our approach is therefore to learn simultaneously the latent embedding and predictive distributions in Eq. (5).",
      "context_before": "If we group the observed and latent variables as $V =$ $\\{ A , C , M , L , Y \\}$ and $H \\ = \\ \\{ H _ { m } , H _ { l } \\}$ respectively, the variational approximation $q _ { \\phi } ( H | V )$ to the intractable posterior $p _ { \\theta } ( H | V )$ is obtained by finding the variational parameters $\\phi$ that minimize the Kullback-Leibler divergence $K L ( q _ { \\phi } ( H | V ) | | p _ { \\theta } ( H | V ) )$ . This is equivalent to maximizing a lower bound $\\mathcal { F } _ { \\theta , \\phi }$ on the log marginal likelihood $\\log p _ { \\theta } ( V ) \\geq \\mathcal { F } _ { \\theta , \\phi }$ with\n\n$$ \\mathcal {F} _ {\\theta , \\phi} = - \\langle \\log q (H | V) \\rangle_ {q (H | V)} + \\langle \\log p (V, H) \\rangle_ {q (H | V)}. $$\n\nIn our case, rather than $q ( H | V )$ , we use $q ( H | V ^ { * } \\equiv V \\setminus Y )$ . Our approach is therefore to learn simultaneously the latent embedding and predictive distributions in Eq. (5).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_10",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig9.jpg",
      "image_filename": "1802.08139_page0_fig9.jpg",
      "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
      "context_before": "",
      "context_after": "This could be preferable to other causal latent variable approaches such as the FairLearning algorithm proposed in Kusner et al. (2017), which separately learns a predictor of $Y$ using samples from the previously inferred latent variables and from the non-descendants of $A$ .\n\nIn order for $\\mathcal { F } _ { \\theta , \\phi }$ to be tractable conjugacy is required, which heavily restricts the family of models that can be used. This issue can be addressed with a Monte-Carlo approximation recently introduced in Kingma & Welling (2014) and Rezende et al. (2014). This approach represents $H$ as a non-linear transformation $H = f _ { \\phi } ( { \\mathcal { E } } )$ of a random variable $\\mathcal { E }$ from a parameter free distribution $q _ { \\epsilon }$ . As we choose $q$ to be Gaussian, $H = \\mu _ { \\phi } + \\sigma _ { \\phi } \\mathcal { E }$ with $q _ { \\epsilon } = \\mathcal { N } ( 0 , 1 )$ for the univariate case. This enables us to rewrite the bound as\n\n$$ \\mathcal {F} _ {\\theta , \\phi} = - \\left\\langle \\log q (H = f _ {\\phi} (\\mathcal {E})) + \\log p (V, H = f _ {\\phi} (\\mathcal {E})) \\right\\rangle_ {q _ {\\epsilon}}. $$",
      "referring_paragraphs": [
        "The German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans. Each applicant is classified as a good or bad credit risk, i.e. as likely or not likely to repay the loan. We assume the GCM in Fig. 4(b), where $A$ corresponds to the protected attribute sex, $C$ to age, $S$ to the triple status of checking account, savings, and housing, and $R$ the duple credit amount and repayment duration. The attributes age, credit amount, and repayment duration",
        "Figure 4."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_11",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/7aba8c4fe5f535c8fe5ad4312414bb3fed9b54ea616ef261d8df2528fab733e4.jpg",
      "image_filename": "7aba8c4fe5f535c8fe5ad4312414bb3fed9b54ea616ef261d8df2528fab733e4.jpg",
      "caption": "Table 1. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { m }$ , $H _ { l }$ , and $H _ { r }$ $( \\times \\ 1 0 { , } 0 0 0 )$ for the UCI Adult dataset. Rows represent values after 5,000, 8,000, 15,000, and 20,000 training steps.",
      "context_before": "In order to provide a test-case for our methodology, we consider the Berkeley Admission dataset, which contains sex $A$ ,\n\n[Section: Path-Specific Counterfactual Fairness]\n\n5.2. The UCI Adult Dataset",
      "context_after": "week are continuous, whilst sex, nationality, marital status, working class, occupation, and income are categorical. Besides the direct effect $A Y$ , we would like to remove the effect of $A$ on $Y$ through marital status, namely along the paths $A \\to M \\to , . . . , \\to Y$ . This GCM is similar to the one analyzed in $\\ S 3 . 2$ and, except for the latent variables, is the same as the one used in Nabi & Shpitser (2018).\n\nNabi & Shpitser (2018) assume that all variables, except $A$ and $Y$ are continuous, and linearly related, except $Y$ for which $p ( Y = 1 | \\mathrm { p a r } ( Y ) ) = \\pi = \\sigma ( \\theta ^ { y } + \\sum _ { X _ { i } \\in \\mathrm { p a r } ( Y ) } \\theta _ { x _ { i } } ^ { y } X _ { i } )$ where $\\sigma ( \\cdot )$ is the sigmoid function. With the encoding $A \\ \\in \\ \\{ 0 , 1 \\}$ , where 0 indicates the male baseline value, and under the approximation $\\log ( \\pi / ( 1 - \\pi ) ) \\ \\approx \\ \\log \\pi$ , we can write the PSE in the odds ratio scale as $\\mathrm { P S E } \\approx$ $\\exp ( \\theta _ { a } ^ { y } + \\theta _ { m } ^ { y } \\theta _ { a } ^ { m } + \\theta _ { l } ^ { y } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m } + \\theta _ { r } ^ { y } \\big ( \\theta _ { m } ^ { r } \\theta _ { a } ^ { m } + \\theta _ { l } ^ { r } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m } \\big ) )$ . A new instance from the test set $\\{ a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } , r ^ { n } \\}$ is classified by using $\\begin{array} { r } { p ( Y _ { a ^ { n } } = 1 | c ^ { n } ) = \\int _ { M , L , R } p ( Y | a ^ { n } , c ^ { n } , M , L , R ) \\times } \\end{array}$ $p ( R | a ^ { n } , c ^ { n } , M , L ) p ( L | a ^ { n } , c ^ { n } , M ) p ( M | a ^ { n } , c ^ { n } )$ . The test accuracy obtained by constraining the PSE to lie between 0.95 and 1.05 is $72 \\%$ , compared to $82 \\%$ of the unconstrained case.\n\nIn our method, for the MMD penalization we used a two stage approach, where a factor $\\beta = 0$ (no penalization) was used for the first 5,000 training steps, and a factor $\\beta = 1 0 0 0$ was used for the remaining training steps. For the Monte-Carlo approximation in Eq. (5), we used $I = 5 0 0$ . These values were chosen based on accuracy/computational cost on the training set.",
      "referring_paragraphs": [
        "It follows that the causal effect of $A$ on $Y$ can be seen as the information that $A$ sends to $Y$ through causal paths, i.e. directed paths, or as the conditional distribution of $Y$ given $A$ restricted to those paths. This implies that the causal effect of $A$ on $Y$ coincides with $p ( Y | A )$ only if there are no open noncausal, i.e. undirected, paths between $A$ and $Y$ . An example of an open undirected path from $A$ to $Y$ is given by $A \\left. C \\right. Y$ in Fig. 1(a): the variable ",
        "If confounders are present, then the causal effect can be retrieved by intervening on $A$ , which replaces the conditional distribution of $A$ with, in the case considered in this paper, a fixed value $a$ . For the model in Fig. 1(a), intervening on $A$ by setting it to the fixed value $a$ would correspond to replacing $p ( A | C )$ with a delta distribution $\\delta _ { A = a }$ , thereby removing the link from $C$ to $A$ and leaving the remaining conditional distributions $p ( { \\boldsymbol { Y",
        "Suppose that the GCM contains only one indirect path through a variable $M$ , as in Fig. 1(b). We define $Y _ { a } ( M ( a ^ { \\prime } ) )$ to be the counterfactual random variable that results from the intervention $A = a$ along $A Y$ and the intervention $A = a ^ { \\prime }$ along $A M Y$ . The average direct effect (ADE) and the average indirect effect (AIE) are given by1",
        "If the GCM contains a confounder for the effect of either $A$ or $M$ on $Y$ , e.g. $C$ in Fig. 1(c), then $p ( Y _ { a , m } ) \\neq p ( Y | A =$ $a , M = m$ ). In this case, by following similar arguments as used in Eq. (1) but conditioning on $C$ (and therefore assuming $Y _ { a , m } \\perp \\perp M _ { a ^ { \\prime } } | C )$ , we obtain",
        "In Table 1, we show the unfair and fair accuracy on the test set at different stages of the training, together with the corresponding MMD values for $H _ { m } , H _ { l }$ and $H _ { r }$ . As we can see, the MMD value for $H _ { m }$ is drastically reduced from 5,000 to 8,000 and 15,000 training steps, without drastic loss in accuracy. After 20,000 training steps, the fair accuracy reduces to that of a dummy classifier. These results were obtained by performing counterfactual correction for bo",
        "Figure 1.",
        "Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig10.jpg",
      "image_filename": "1802.08139_page0_fig10.jpg",
      "caption": "2We omit race, and capital gain and loss (although including capital gain and loss would increase test accuracy from $8 2 . 9 \\%$ to $8 4 . 7 \\%$ ) to use the same attributes as in Nabi & Shpitser (2018).",
      "context_before": "In Fig. 5, we show histograms of $\\tilde { q } ( H _ { m } | A )$ separately for\n\n[Section: Path-Specific Counterfactual Fairness]\n\n2We omit race, and capital gain and loss (although including capital gain and loss would increase test accuracy from $8 2 . 9 \\%$ to $8 4 . 7 \\%$ ) to use the same attributes as in Nabi & Shpitser (2018).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg",
      "image_filename": "1802.08139_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig12.jpg",
      "image_filename": "1802.08139_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_15",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig13.jpg",
      "image_filename": "1802.08139_page0_fig13.jpg",
      "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff",
        "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg",
      "image_filename": "1802.08139_page0_fig14.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_17",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig15.jpg",
      "image_filename": "1802.08139_page0_fig15.jpg",
      "caption": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.",
      "context_before": "",
      "context_after": "males and females for increasing numbers of training steps. The remaining variables are shown in the Appendix. As can be seen, the addition of the MMD penalization to the variational bound for more training steps has the effect of reducing the number of modes in the posterior. From the evidence available, it is unclear if the shape changes are a necessary consequence of enforcing them to be similar, or if a simplification of the latent space is a more fundamental drawback of the MMD method. We leave any further investigations into such constraints for future work.\n\n5.3. The UCI German Credit Dataset\n\nThe German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans. Each applicant is classified as a good or bad credit risk, i.e. as likely or not likely to repay the loan. We assume the GCM in Fig. 4(b), where $A$ corresponds to the protected attribute sex, $C$ to age, $S$ to the triple status of checking account, savings, and housing, and $R$ the duple credit amount and repayment duration. The attributes age, credit amount, and repayment duration are continuous, whilst checking account, savings, and housing are categorical. Besides the direct effect $A Y$ , we would like to remove the effect of $A$ on $Y$ through $S$ . We only need to introduce a hidden variable $H _ { s }$ for $S$ , as $R$ does not need to be corrected.",
      "referring_paragraphs": [
        "In Fig. 5, we show histograms of $\\tilde { q } ( H _ { m } | A )$ separately for",
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_18",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/b3fb2ac1c0dbc2e93cde0695817bae19a24c605c323fd094e8312e08228f5035.jpg",
      "image_filename": "b3fb2ac1c0dbc2e93cde0695817bae19a24c605c323fd094e8312e08228f5035.jpg",
      "caption": "Table 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.",
      "context_before": "5.3. The UCI German Credit Dataset\n\nThe German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans. Each applicant is classified as a good or bad credit risk, i.e. as likely or not likely to repay the loan. We assume the GCM in Fig. 4(b), where $A$ corresponds to the protected attribute sex, $C$ to age, $S$ to the triple status of checking account, savings, and housing, and $R$ the duple credit amount and repayment duration. The attributes age, credit amount, and repayment duration are continuous, whilst checking account, savings, and housing are categorical. Besides the direct effect $A Y$ , we would like to remove the effect of $A$ on $Y$ through $S$ . We only need to introduce a hidden variable $H _ { s }$ for $S$ , as $R$ does not need to be corrected.\n\nWe divided the dataset into training and test sets of sizes 700 and 300 respectively. We used $\\beta = 0$ for the first 2,000 training steps, and $\\beta = 1 0 0$ afterward. For the Monte-Carlo approximation in Eq. (5), we used $I = 1 0 0 0$ . Counterfactual correction was performed for both males and females.",
      "context_after": "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not differ significantly for females and males. In Fig. 6, we show $\\widetilde { q } ( H _ { s } | A )$ for one dimension of the variable housing, which shows the most significant difference between females and males. The remaining variables are shown in the Appendix.\n\nWe have introduced a latent inference-projection method to achieve path-specific counterfactual fairness which simplifies, generalizes and outperforms previous literature. A fair decision is achieved by correcting the variables that are descendants of the protected attribute along unfair pathways, rather than by imposing constraints on the model parameters. This enables us to retain fair information contained in the problematic descendants and to leave unaltered the underlying data-generation mechanism. In the future, we plan to investigate alternative techniques to MMD for enforcing independence between the latent space and the sensitive attribute.\n\n[Section: Path-Specific Counterfactual Fairness]",
      "referring_paragraphs": [
        "For example, the required counterfactual variable for the effect along the path $A \\ \\ W \\ \\ Y$ in Fig. 2(a) is $Y _ { a ^ { \\prime } } ( M ( a ^ { \\prime } ) , W ( a , M ( a ^ { \\prime } ) ) )$ . Indeed, in the first iteration, as $A$ and $M$ are direct causes of $Y$ along black arrows, whilst $W$ is a direct cause of $Y$ along a green arrow, we obtain $Y _ { a ^ { \\prime } } ( M ( a ^ { \\prime } ) , \\gamma _ { W } )$ . In the second iteration, as $M$ is a direct cause of $W$ along a black arro",
        "For the path $A Y$ in Fig. 2(b), we would need instead $p ( Y _ { a } ( M ( a ^ { \\prime } ) , W ( a ^ { \\prime } ) ) )$ . Under the assumption $Y _ { a , m , w } ~ \\bot$ $\\{ M _ { a ^ { \\prime } } , W _ { a ^ { \\prime } } \\}$ , we would obtain $p ( Y _ { a } ( M ( a ^ { \\prime } ) , W ( a ^ { \\prime } ) ) ) \\ =$ $\\begin{array} { r l } { \\int _ { m , w } p ( Y _ { a , m , w } ) p ( M _ { a ^ { \\prime } } , W _ { a ^ { \\prime } } ) } \\end{array}$ . However, $p ( Y _ { a , m , w } ) \\quad \\neq \\qq",
        "The variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms with variance $\\sigma _ { a } ^ { 2 } , \\sigma _ { c } ^ { 2 } , \\sigma _ { m } ^ { 2 } , \\sigma _ { l } ^ { 2 }$ and $\\sigma _ { y } ^ { 2 }$ . The GCM corresponding to this model is depicted in Fig. 2(c).",
        "We want to learn to predict $Y$ from $A , C , M$ and $L$ . However, $A$ is a sensitive attribute, and its direct effect on $Y$ and effect through $M$ is considered unfair. Therefore, to obtain a fair decision system, we need to disregard the PSE of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ $I , A \\to M \\to , \\dots , \\to Y$ , namely along the green and dashed green-black links of Fig. 2(c). Notice that the dashed green-black links differ fundamentally from the green",
        "For addressing a more general data-generation process mismatch than the one considered above, we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m }$ and $H _ { l }$ are",
        "In Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not diff",
        "Figure 2.",
        "Table 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
      "image_filename": "1802.08139_page0_fig16.jpg",
      "caption": "(a)",
      "context_before": "[Section: Path-Specific Counterfactual Fairness]\n\n[Section: Path-Specific Counterfactual Fairness]\n\n[Section: Path-Specific Counterfactual Fairness]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_20",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig17.jpg",
      "image_filename": "1802.08139_page0_fig17.jpg",
      "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A Y$ cannot be identified by only using observed variables.",
      "context_before": "",
      "context_after": "A. Identifiability of PSE\n\nWe summarize the method described in Shpitser (2013) to graphically establish whether a PSE is identifiable.\n\nAcyclic Directed Mixed Graph (ADMG): An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause. The ADMG corresponding to Fig. 7(a) is given by Fig. 7(b).",
      "referring_paragraphs": [
        "Acyclic Directed Mixed Graph (ADMG): An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause. The ADMG corresponding to Fig. 7(a) is given by Fig. 7(b).",
        "District: The set of nodes in an ADMG that are reachable from $A$ through bidirected paths is called the district of $A$ . For example, the district of $Y$ in Fig. 7(b) is $\\{ M , Y \\}$ .",
        "For example, the set $\\nu$ in Fig. 7(b) is $\\{ M , W , Y \\}$ . The districts in $\\mathcal { G } _ { \\nu }$ are $\\{ M , Y \\}$ . This district is recanting for the effect along $A Y$ , as $A \\to Y \\in \\pi$ , whilst $A \\to M \\to Y \\notin \\pi$ . (This district is not recanting for the effect along $A \\to W \\to Y .$ .)",
        "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig18.jpg",
      "image_filename": "1802.08139_page0_fig18.jpg",
      "caption": "Path-Specific Counterfactual Fairness",
      "context_before": "$$ n = \\left( \\begin{array}{c} {\\frac {\\theta^ {y} + \\theta_ {a} ^ {y} A}{\\sigma_ {y} ^ {2}}} \\\\ {- \\frac {\\theta_ {l} ^ {y} (\\theta^ {y} + \\theta_ {a} ^ {y} A)}{\\sigma_ {y} ^ {2}} + \\frac {\\theta^ {l} + \\theta_ {a} ^ {l} A}{\\sigma_ {l} ^ {2}}} \\\\ {- \\frac {\\theta_ {m} ^ {y} (\\theta^ {y} + \\theta_ {a} ^ {y} A)}{\\sigma_ {y} ^ {2}} - \\frac {\\theta_ {m} ^ {l} (\\theta^ {l} + \\theta_ {a} ^ {l} A)}{\\sigma_ {l} ^ {2}} + \\frac {\\theta^ {m} + \\theta_ {a} ^ {m} A}{\\sigma_ {m} ^ {2}}} \\\\ {- \\frac {\\theta_ {c} ^ {y} (\\theta^ {y} + \\theta_ {a} ^ {y} A)}{\\sigma_ {y} ^ {2}} - \\frac {\\theta_ {c} ^ {l} (\\theta^ {l} + \\theta_ {a} ^ {l} A)}{\\sigma_ {l} ^ {2}} - \\frac {\\theta_ {c} ^ {m} (\\theta^ {m} + \\theta_ {a} ^ {m} A)}{\\sigma_ {m} ^ {2}} + \\frac {\\theta^ {c}}{\\sigma_ {c} ^ {2}}} \\\\ {- \\frac {\\theta_ {h} ^ {m} (\\theta^ {m} + \\theta_ {a} ^ {m} A)}{\\sigma_ {m} ^ {2}} + \\frac {\\theta^ {h}}{\\sigma_ {h} ^ {2}}} \\end{array} \\right). $$\n\nThe Gaussian conditional $p ( H _ { m } | A , C , M , L )$ can be computed through the formulas for Gaussian marginalization and conditioning.\n\n[Section: Path-Specific Counterfactual Fairness]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig19.jpg",
      "image_filename": "1802.08139_page0_fig19.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig20.jpg",
      "image_filename": "1802.08139_page0_fig20.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig21.jpg",
      "image_filename": "1802.08139_page0_fig21.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig22.jpg",
      "image_filename": "1802.08139_page0_fig22.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig23.jpg",
      "image_filename": "1802.08139_page0_fig23.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_27",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig24.jpg",
      "image_filename": "1802.08139_page0_fig24.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_28",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig25.jpg",
      "image_filename": "1802.08139_page0_fig25.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_29",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig26.jpg",
      "image_filename": "1802.08139_page0_fig26.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_30",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig27.jpg",
      "image_filename": "1802.08139_page0_fig27.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_31",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig28.jpg",
      "image_filename": "1802.08139_page0_fig28.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_32",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig29.jpg",
      "image_filename": "1802.08139_page0_fig29.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_33",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig30.jpg",
      "image_filename": "1802.08139_page0_fig30.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_34",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig31.jpg",
      "image_filename": "1802.08139_page0_fig31.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_35",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig32.jpg",
      "image_filename": "1802.08139_page0_fig32.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_36",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig33.jpg",
      "image_filename": "1802.08139_page0_fig33.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_37",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig34.jpg",
      "image_filename": "1802.08139_page0_fig34.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_38",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg",
      "image_filename": "1802.08139_page0_fig35.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_39",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig36.jpg",
      "image_filename": "1802.08139_page0_fig36.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_40",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig37.jpg",
      "image_filename": "1802.08139_page0_fig37.jpg",
      "caption": "(b) Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.",
      "context_before": "",
      "context_after": "C. Experimental Details\n\nFor all datasets, as the prior distribution $p$ for each latent variable we used a mixture of two-dimensional Gaussians with ten mixture components and diagonal covariances. As the variational posterior distribution $q$ we used a two-dimensional Gaussian with diagonal covariance, with means and log variances obtained as the outputs of a neural network with two linear layers of size 20 and tanh activation, followed by a linear layer. In the conditional distributions, $f _ { \\theta }$ was a neural network with one linear layer of size 100 with tanh activation, followed by a linear layer. The outputs were Gaussian means for continuous variables and logits for categorical variables. We used the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.01, mini-batch size 128, and default values $\\beta _ { 1 } = 0 . 9$ , $\\beta _ { 2 } = 0 . 9 9 9$ , and $\\epsilon = 1 e { - } 8$ .\n\nC.1. UCI Adult Dataset",
      "referring_paragraphs": [
        "In Fig. 8 we show histograms for prior and posterior distributions in the latent space.",
        "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_41",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig38.jpg",
      "image_filename": "1802.08139_page0_fig38.jpg",
      "caption": "Path-Specific Counterfactual Fairness",
      "context_before": "C.1. UCI Adult Dataset\n\nIn Fig. 8 we show histograms for prior and posterior distributions in the latent space.\n\n[Section: Path-Specific Counterfactual Fairness]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_42",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig39.jpg",
      "image_filename": "1802.08139_page0_fig39.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_43",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig40.jpg",
      "image_filename": "1802.08139_page0_fig40.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_44",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig41.jpg",
      "image_filename": "1802.08139_page0_fig41.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_45",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig42.jpg",
      "image_filename": "1802.08139_page0_fig42.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_46",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig43.jpg",
      "image_filename": "1802.08139_page0_fig43.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_47",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig44.jpg",
      "image_filename": "1802.08139_page0_fig44.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_48",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg",
      "image_filename": "1802.08139_page0_fig45.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_49",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig46.jpg",
      "image_filename": "1802.08139_page0_fig46.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_50",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig47.jpg",
      "image_filename": "1802.08139_page0_fig47.jpg",
      "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).",
      "context_before": "",
      "context_after": "C.2. UCI German Credit Dataset\n\nIn Fig. 9 we show histograms for posterior distributions in the latent space.\n\n[Section: Path-Specific Counterfactual Fairness]",
      "referring_paragraphs": [
        "In Fig. 9 we show histograms for posterior distributions in the latent space.",
        "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1802.08139_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1803.04383": [
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig0.jpg",
      "image_filename": "1803.04383_page0_fig0.jpg",
      "caption": "OUTCOME CURVE",
      "context_before": "2.1 The Outcome Curve\n\nWe now introduce important outcome regimes, stated in terms of the change in average group score. A policy $( \\tau _ { \\mathsf { A } } , \\tau _ { \\mathsf { B } } )$ is said to cause active harm to group j if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) < 0$ , stagnation if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) = 0$ , and improvement if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) > 0$ . Under our model, MaxUtil policies can be chosen in a standard fashion which applies the same threshold $\\tau ^ { \\mathrm { M a x U t i 1 } }$ for both groups, and is agnostic to the distributions $\\pi _ { \\mathsf { A } }$ and $\\pi _ { \\mathsf { B } }$ . Hence, if we define\n\n$$ \\Delta \\boldsymbol {\\mu} _ {j} ^ {\\text {M a x U t i l}} := \\Delta \\boldsymbol {\\mu} _ {j} \\left(\\tau^ {\\text {M a x U t i l}}\\right) \\tag {3} $$",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg",
      "image_filename": "1803.04383_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig2.jpg",
      "image_filename": "1803.04383_page0_fig2.jpg",
      "caption": "Figure 1: The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.",
      "context_before": "",
      "context_after": "we say that a policy causes relative harm to group j if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) < \\Delta \\mu _ { \\mathrm { j } } ^ { \\mathrm { n a x } \\cup \\mathrm { t } _ { 1 } }$ , and relative improvement if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) > \\Delta \\mu _ { \\mathrm { j } } ^ { \\mathrm { M a x U t i 1 } }$ . In particular, we focus on these outcomes for a disadvantaged group, and consider whether imposing a fairness constraint improves their outcomes relative to the MaxUtil strategy. From this point forward, we take A to be disadvantaged or protected group.\n\nFigure 1 displays the important outcome regimes in terms of selection rates $\\begin{array} { r } { \\beta _ { \\mathrm { j } } : = \\sum _ { x \\in \\mathcal { X } } \\pi _ { \\mathrm { j } } ( x ) \\pmb { \\tau } _ { \\mathrm { j } } ( x ) } \\end{array}$ This succinct characterization is possible when considering decision rules based on (possibly randomized) score thresholding, in which all individuals with scores above a threshold are selected. In Section 5, we justify the restriction to such threshold policies by showing it preserves optimality. In Section 5.1, we show that the outcome curve is concave, thus implying that it takes the shape depicted in Figure 1. To explicitly connect selection rates to decision policies, we define the rate function $r _ { \\pi } ( \\tau _ { \\mathrm { j } } )$ which returns the proportion of group j selected by the policy. We show that this function is invertible for a suitable class of threshold policies, and in fact the outcome curve is precisely the graph of the map from selection rate to outcome $\\beta \\mapsto \\Delta \\mu _ { \\mathsf { A } } ( r _ { \\pi _ { \\mathsf { A } } } ^ { - 1 } ( \\beta ) )$ . Next, we define the values of $\\beta$ that mark boundaries of the outcome regions.\n\nDefinition 2.1 (Selection rates of interest). Given the protected group A, the following selection rates are of interest in distinguishing between qualitatively different classes of outcomes (Figure 1). We define $\\beta ^ { \\mathrm { M a x U t i 1 } }$ as the selection rate for A under MaxUtil; $\\beta _ { 0 }$ as the harm threshold, such that $\\Delta \\mu _ { \\mathsf { A } } ( r _ { \\pi _ { \\mathsf { A } } } ^ { - 1 } ( \\beta _ { 0 } ) ) ~ = ~ 0$ ; β∗ as the selection rate such that $\\Delta \\pmb { \\mu } _ { \\mathsf { A } }$ is maximized; $\\overline { { \\beta } }$ as the outcomecomplement of the MaxUtil selection rate, $\\Delta \\mu _ { \\mathsf { A } } r _ { \\pi _ { \\mathsf { A } } } ^ { - 1 } ( \\beta ) ) \\ = \\ \\Delta \\mu _ { \\mathsf { A } } ( r _ { \\pi _ { \\mathsf { A } } } ^ { - 1 } ( \\beta ^ { \\mathsf { M a x U t i 1 } } ) )$ with $\\overline { { \\beta } } ~ > ~ \\beta ^ { \\mathrm { M a x U t i 1 } }$ .",
      "referring_paragraphs": [
        "2 If we consider functions $\\Delta _ { p } ( x ) : \\mathcal { X } \\to \\mathbb { R }$ and $\\Delta _ { n } ( x ) : \\mathcal { X } \\to \\mathbb { R }$ to represent the average effect of selection and non-selection respectively, then ∆µj(τ ) := Px∈X πj(x) (τ j(x)∆p(x) + (1 − τ j(x))∆n(x)). This model corresponds to replacing $\\Delta ( x )$ in the original outcome definition with $\\Delta _ { p } ( x ) - \\Delta _ { n } ( x )$ , and adding a offset $\\begin{array} { r } { \\sum _ { x \\in \\mathcal { X } } ",
        "Figure 1 displays the important outcome regimes in terms of selection rates $\\begin{array} { r } { \\beta _ { \\mathrm { j } } : = \\sum _ { x \\in \\mathcal { X } } \\pi _ { \\mathrm { j } } ( x ) \\pmb { \\tau } _ { \\mathrm { j } } ( x ) } \\end{array}$ This succinct characterization is possible when considering decision rules based on (possibly randomized) score thresholding, in which all individuals with scores above a threshold are selected. In Section 5, we justify the restriction to such threshold ",
        "Definition 2.1 (Selection rates of interest). Given the protected group A, the following selection rates are of interest in distinguishing between qualitatively different classes of outcomes (Figure 1). We define $\\beta ^ { \\mathrm { M a x U t i 1 } }$ as the selection rate for A under MaxUtil; $\\beta _ { 0 }$ as the harm threshold, such that $\\Delta \\mu _ { \\mathsf { A } } ( r _ { \\pi _ { \\mathsf { A } } } ^ { - 1 } ( \\beta _ { 0 } ) ) ~ = ~ 0$ ; β∗ as the selection rate such that $\\Delta \\pmb ",
        "Because fairness criteria encourage a higher selection rate for disadvantaged groups (Corollary 3.2), systematic underestimation widens the regime of their applicability. Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes. Thus the potential for measurement error should be a factor when motivating these criteria.",
        "We introduce the notion of an outcome curve (Figure 1) which succinctly describes the different regimes in which one criterion is preferable over the others.",
        "Figure 1: The above figure shows the outcome curve.",
        "Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg",
      "image_filename": "1803.04383_page0_fig3.jpg",
      "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.",
      "context_before": "We direct the reader to Appendix C for the proof of the above proposition, and all subsequent results presented in this section. The results are corollaries to theorems presented in Section 6.\n\n3.1 Prospects and Pitfalls of Fairness Criteria\n\nWe begin by characterizing general settings under which fairness criteria act to improve outcomes over unconstrained MaxUtil strategies. For this result, we will assume that group A is disadvantaged",
      "context_after": "in the sense that the MaxUtil acceptance rate for $\\textsf { B }$ is large compared to relevant acceptance rates for A.\n\nCorollary 3.2 (Fairness Criteria can cause Relative Improvement). (a) Under the assumption that $\\beta _ { \\mathsf { A } } ^ { \\mathsf { M a x U t i 1 } } < \\overline { { \\beta } }$ and $\\beta _ { \\mathsf { B } } ^ { \\mathsf { M a x U t i 1 } } > \\beta _ { \\mathsf { A } } ^ { \\mathsf { M a x U t i 1 } }$ , there exist population proportions $g _ { 0 } < g _ { 1 } < 1$ such that, for all $g _ { \\mathsf { A } } \\in [ g _ { 0 } , g _ { 1 } ]$ , $\\beta _ { \\mathsf { A } } ^ { \\mathsf { M a x U t i 1 } } < \\beta _ { \\mathsf { A } } ^ { \\mathsf { D e m P a r i t y } } < \\overline { { \\beta } }$ < < βDemParityA < β. That is, DemParity causes relative improvement.\n\nA (b) Under the assumption that there exist $\\beta _ { \\mathsf { A } } ^ { \\mathtt { M a x U t i 1 } } ~ < ~ \\beta ~ < ~ \\beta ^ { \\prime } ~ < ~ \\overline { { \\beta } }$ such that $\\beta _ { \\mathsf { B } } ^ { \\mathsf { M a x U t i 1 } } >$ [g2, g3], βMaxUtilA < $\\ [ g _ { 2 } , g _ { 3 } ] , \\ \\beta _ { \\mathsf { A } } ^ { \\mathsf { M a x U t i 1 } } < \\beta _ { \\mathsf { A } } ^ { \\mathsf { E q O p t } } < \\overline { { \\beta } }$ $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) , G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ^ { \\prime } )$ , there exist population proportions 2 3 . That is, EqOpt causes relative improvement. $g _ { 2 } \\ < \\ g _ { 3 } \\ < \\ 1$ such that, for all $g _ { \\mathsf { A } } ~ \\in$",
      "referring_paragraphs": [
        "Just as the expected outcome $\\Delta \\mu$ can be expressed in terms of selection rate for threshold policies, so can the total utility $\\mathcal { U }$ . In the unconstrained cause, $\\boldsymbol { \\mathcal { U } }$ varies independently over the selection rates for group A and B; however, in the presence of fairness constraints the selection rate for one group determines the allowable selection rate for the other. The selection rates must be equal for DemParity, but for EqOpt we can define a tran",
        "This result gives the conditions under which we can guarantee the existence of settings in which fairness criteria cause improvement relative to MaxUtil. Relying on machinery proved in Section 6, the result follows from comparing the position of optima on the utility curve to the outcome curve. Figure 2 displays a illustrative example of both the outcome curve and the institutions’ utility $\\boldsymbol { u }$ as a function of the selection rates in group A. In the utility function (1), the contr",
        "This idea is expressed in Figure 2, and underpins the results to follow.",
        "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group.",
        "The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig4.jpg",
      "image_filename": "1803.04383_page0_fig4.jpg",
      "caption": "5.1 Quantiles and Concavity of the Outcome Curve",
      "context_before": "On the other hand, assume that $\\pmb { \\tau } _ { \\mathrm { j } } \\cong _ { \\pmb { \\pi } _ { \\mathrm { j } } } r _ { \\pmb { \\pi } _ { \\mathrm { j } } } ^ { - 1 } \\left( r _ { \\pmb { \\pi } _ { \\mathrm { j } } } ( \\pmb { \\tau } _ { \\mathrm { j } } ) \\right)$ . We show that $r _ { \\pi _ { \\mathrm { j } } } ^ { - 1 } ( r _ { \\pi _ { \\mathrm { j } } } ( \\tau _ { \\mathrm { j } } ) )$ is a maximizer; which will imply that $\\tau _ { \\mathrm { j } }$ is a maximizer since ${ \\pmb \\tau } _ { \\mathrm { j } } \\cong _ { { \\pmb \\pi } _ { \\mathrm { j } } } r _ { { \\pmb \\pi } _ { \\mathrm { j } } } ^ { - 1 } ( r _ { { \\pmb \\pi } _ { \\mathrm { j } } } ( { \\pmb \\tau } _ { \\mathrm { j } } ) )$ implies that $\\mathcal { U } _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) = \\tau _ { \\mathrm { j } } \\cong _ { \\pi _ { \\mathrm { j } } }$ $r _ { \\pi _ { \\mathrm { j } } } ^ { - 1 } ( r _ { \\pi _ { \\mathrm { j } } } ( \\tau _ { \\mathrm { j } } ) )$ . By Lemma 5.2 there exists a maximizer $\\tau _ { \\mathrm { j } } ^ { * } \\in \\mathcal { T } _ { \\mathrm { t h r e s h } } ( \\boldsymbol { \\pi } )$ , which means that $\\tau _ { \\mathrm { j } } ^ { \\ast } =$ $r _ { \\pi _ { \\mathrm { j } } } ^ { - 1 } ( r _ { \\pi _ { \\mathrm { j } } } ( \\tau _ { \\mathrm { j } } ^ { * } ) )$ . Since $\\tau _ { \\mathrm { j } } ^ { \\ast }$ is feasible, we must have $r _ { \\pi _ { \\mathrm { j } } } ( \\tau _ { \\mathrm { j } } ^ { * } ) = r _ { \\pi _ { \\mathrm { j } } } ( \\tau _ { \\mathrm { j } } )$ , and thus $\\tau _ { \\mathrm { j } } ^ { * } = r _ { \\pi _ { \\mathrm { j } } } ^ { - 1 } ( r _ { \\pi _ { \\mathrm { j } } } ( \\pmb { \\tau } _ { \\mathrm { j } } ) )$ , as needed. The same argument follows verbatim if we instead choose ${ \\pmb v } ( x ) = { \\pmb \\Delta } ( x )$ , and compute $\\langle v , \\tau \\rangle = \\Delta \\mu _ { \\mathrm { j } } ( \\tau )$ . □\n\nWe now argue Proposition 5.2 for MaxUtil, as it is a straightforward application of Lemma 5.2. We will prove Proposition 5.2 for DemParity and EqOpt separately in Sections 6.1 and 6.2.\n\nProof of Proposition 5.2 for MaxUtil. MaxUtil follows from lemma 5.2 with $\\pmb { v } ( \\boldsymbol { x } ) = \\pmb { u } ( \\boldsymbol { x } )$ , and $t = 0$ and $\\mathbf { \\nabla } w = \\mathbf { 0 }$ .",
      "context_after": "5.1 Quantiles and Concavity of the Outcome Curve\n\nTo further our analysis, we now introduce left and right quantile functions, allowing us to specify thresholds in terms of both selection rate and score cutoffs.\n\nDefinition 5.2 (Upper quantile function). Define Q to be the upper quantile function corresponding to $\\pi$ , i.e.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg",
      "image_filename": "1803.04383_page0_fig5.jpg",
      "caption": "Utility Contour Plot Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).",
      "context_before": "Observe that $\\Delta \\mu ( r _ { \\pi } ^ { - 1 } ( \\beta ) ) = \\langle \\Delta , \\pi \\circ r _ { \\pi } ^ { - 1 } ( \\beta ) \\rangle$ . By Lemma 5.3, $\\pi \\circ r _ { \\pi } ^ { - 1 } ( \\beta )$ has right and left derivatives $e _ { \\mathrm { Q } \\left( \\beta \\right) }$ and $e _ { \\mathrm { Q } ^ { + } \\left( \\beta \\right) }$ . Hence, we have that\n\n$$ \\partial_ {+} \\Delta \\boldsymbol {\\mu} \\left(\\beta_ {\\mathrm {B}}\\right) = \\Delta \\left(\\mathrm {Q} \\left(\\beta_ {\\mathrm {B}}\\right)\\right) \\quad \\text {a n d} \\quad \\partial_ {-} \\Delta \\boldsymbol {\\mu} \\left(\\beta_ {\\mathrm {B}}\\right) = \\Delta \\left(\\mathrm {Q} ^ {+} \\left(\\beta_ {\\mathrm {B}}\\right)\\right). \\tag {16} $$\n\nUsing the fact that $\\Delta ( x )$ is monotone, and that $\\mathrm { Q \\leq Q ^ { + } }$ , we see that $\\partial _ { + } \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) ) \\leq \\partial _ { - } \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) )$ , and that $\\partial \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) )$ and $\\partial _ { + } \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) )$ are non-increasing, from which it follows that $\\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( { \\boldsymbol \\beta } _ { \\mathsf { B } } ) )$ is concave. The general concavity result holds by replacing $\\Delta ( x )$ with $w ( x )$ .",
      "context_after": "6 Proofs of Main Theorems\n\nWe are now ready to present and prove theorems that characterize the selection rates under fairness constraints, namely DemParity and EqOpt. These characterizations are crucial for proving the results in Section 3. Our computations also generalize readily to other linear constraints, in a way that will become clear in Section 6.2.\n\n6.1 A Characterization Theorem for DemParity",
      "referring_paragraphs": [
        "Let us introduce the auxiliary variable $\\beta : = \\langle \\pi _ { \\mathsf { A } } , \\tau _ { \\mathsf { A } } \\rangle = \\langle \\pi _ { \\mathsf { B } } , \\tau _ { \\mathsf { B } } \\rangle$ corresponding to the selection rate which is held constant across groups, so that all feasible solutions lie on the green DP line in Figure 3. We can then express the following equivalent linear program:",
        "where $\\begin{array} { r } { \\textstyle t _ { \\operatorname* { m a x } } = \\operatorname* { m i n } _ { \\mathrm { j } \\in \\{ \\mathsf { A } , \\mathsf { B } \\} } \\{ \\langle \\pi _ { \\mathrm { j } } , w _ { \\mathrm { j } } \\rangle \\} } \\end{array}$ is the largest possible TPR. The magenta EO curve in Figure 3 illustrates that feasible solutions to this optimization problem lie on a curve parametrized by $t$ . Note that the objective function decouples for $\\mathsf { j } \\in \\{ \\mathsf { A } , \\maths",
        "Utility Contour Plot   \nFigure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves.",
        "The magenta EO curve in Figure 3 illustrates that feasible solutions to this optimization problem lie on a curve parametrized by $t$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig6.jpg",
      "image_filename": "1803.04383_page0_fig6.jpg",
      "caption": "Hence, any optimal policy is equivalent to the threshold policy $\\pmb { \\tau } = ( r _ { \\pmb { \\pi } _ { \\mathsf { A } } } ^ { - 1 } ( \\beta ) , r _ { \\pmb { \\pi } _ { \\mathsf { B } } } ^ { - 1 } ( \\beta ) )$ , where $\\",
      "context_before": "$$ \\max _ {\\boldsymbol {\\tau} = (\\boldsymbol {\\tau} _ {\\mathrm {A}}, \\boldsymbol {\\tau} _ {\\mathrm {B}}) \\in [ 0, 1 ] ^ {2 C}, \\beta \\in [ 0, 1 ]} \\mathcal {U} (\\boldsymbol {\\tau}) \\quad \\mathrm {s . t .} \\quad \\beta = \\langle \\boldsymbol {\\pi} _ {\\mathrm {j}}, \\boldsymbol {\\tau} _ {\\mathrm {j}} \\rangle , \\mathrm {j} \\in \\{\\mathrm {A}, \\mathrm {B} \\}. $$\n\nThis is equivalent because, for a given $\\beta$ , Proposition 5.2 says that the utility maximizing policies are of the form $\\tau _ { \\mathrm { j } } = r _ { \\pi _ { \\mathrm { j } } } ^ { - 1 } ( \\beta )$ . We now prove this:\n\nProof of Proposition 5.2 for DemParity. Noting that $r _ { \\pi _ { \\mathrm { j } } } ( \\pmb { \\tau } _ { \\mathrm { j } } ) = \\langle \\pmb { \\pi } _ { \\mathrm { j } } , \\pmb { \\tau } _ { \\mathrm { j } } \\rangle$ , we see that, by Lemma 5.2, under the special case where $\\pmb { v } ( \\boldsymbol { x } ) = \\pmb { u } ( \\boldsymbol { x } )$ and $\\pmb { w } ( x ) = 1$ , the optimal solution $( \\tau _ { \\mathsf { A } } ^ { * } ( \\beta ) , \\tau _ { \\mathsf { B } } ^ { * } ( \\beta ) )$ for fixed $r _ { \\pi _ { \\mathsf { A } } } ( \\tau _ { \\mathsf { A } } ) = r _ { \\pi _ { \\mathsf { B } } } ( \\tau _ { \\mathsf { B } } ) = \\beta$ can be chosen to coincide with the threshold policies. Optimizing over $\\beta$ , the global optimal must coincide with thresholds.",
      "context_after": "Hence, any optimal policy is equivalent to the threshold policy $\\pmb { \\tau } = ( r _ { \\pmb { \\pi } _ { \\mathsf { A } } } ^ { - 1 } ( \\beta ) , r _ { \\pmb { \\pi } _ { \\mathsf { B } } } ^ { - 1 } ( \\beta ) )$ , where $\\beta$ solves the following optimization:\n\n$$ \\max _ {\\beta \\in [ 0, 1 ]} \\mathcal {U} \\left(\\left(r _ {\\pi_ {A}} ^ {- 1} (\\beta), r _ {\\pi_ {B}} ^ {- 1} (\\beta)\\right)\\right). \\tag {17} $$\n\nWe shall show that the above expression is in fact a concave function in $\\beta$ , and hence the set of optimal selection rates can be characterized by first order conditions. This is presented formally in the following theorem:",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig7.jpg",
      "image_filename": "1803.04383_page0_fig7.jpg",
      "caption": "With a result of the above form, we can now easily prove statements such as that in Corollary 3.3 (see appendix C for proofs), by fixing a selection rate of interest (e.g. $\\beta _ { 0 }$ ) and inverting the",
      "context_before": "The same argument shows that\n\n$$ \\partial_ {-} \\mathcal {U} \\left(\\left(r _ {\\pi_ {A}} ^ {- 1} (\\beta), r _ {\\pi_ {B}} ^ {- 1} (\\beta)\\right)\\right) = g _ {A} u \\left(Q _ {A} ^ {+} (\\beta)\\right) + g _ {B} u \\left(Q _ {B} ^ {+} (\\beta)\\right). $$\n\nBy concavity of $\\mathcal { U } \\left( \\left( r _ { \\pi _ { \\mathsf { A } } } ^ { - 1 } ( \\beta ) , r _ { \\pi _ { \\mathsf { B } } } ^ { - 1 } ( \\beta ) \\right) \\right)$ , a positive right derivative at $\\beta$ implies that $\\beta < \\beta ^ { * }$ for all $\\beta ^ { * }$ satisfying (17), and similarly, a negative left derivative at $\\beta$ implies that $\\beta > \\beta ^ { * }$ for all $\\beta ^ { * }$ satisfying (17).",
      "context_after": "With a result of the above form, we can now easily prove statements such as that in Corollary 3.3 (see appendix C for proofs), by fixing a selection rate of interest (e.g. $\\beta _ { 0 }$ ) and inverting the\n\ninequalities in Theorem 6.1 to find the exact population proportions under which, for example, DemParity results in a higher selection rate than $\\beta _ { 0 }$ .\n\n6.2 EqOpt and General Constraints",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig8.jpg",
      "image_filename": "1803.04383_page0_fig8.jpg",
      "caption": "Repay Probability by Group",
      "context_before": "By concavity, a positive right derivative at $t$ implies that $t ~ < ~ t ^ { * }$ for all $t ^ { * }$ satisfying (21), and similarly, a negative left derivative at $t$ implies that $t > t ^ { * }$ for all $t ^ { * }$ satisfying (21).\n\nFinally, by Lemma 6.1, this interval in $t$ uniquely characterizes an interval of acceptance rates. Thus we translate directly into a statement about the selection rates $\\beta$ for group A by seeing that $T _ { \\mathsf { A } , \\mathsf { w } _ { \\mathsf { A } } } ^ { - 1 } ( t ) = \\beta$ and $T _ { \\mathsf { B } , \\boldsymbol { w } _ { \\mathsf { B } } } ^ { - 1 } ( t ) = G _ { \\boldsymbol { w } } ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta )$ . □\n\nLastly, we remark that the results derived in this section go through verbatim for any linear constraint of the form $\\langle \\pmb { w } , \\pmb { \\pi } _ { \\mathsf { A } } \\circ \\pmb { \\tau } _ { \\mathsf { A } } \\rangle = \\langle \\pmb { w } , \\pmb { \\pi } _ { \\mathsf { B } } \\circ \\pmb { \\tau } _ { \\mathsf { B } } \\rangle$ , as long as $\\pmb { u } ( \\boldsymbol { x } ) / \\pmb { w } ( \\boldsymbol { x } )$ is increasing in $x$ , and $w ( x ) > 0$ .",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_10",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg",
      "image_filename": "1803.04383_page0_fig9.jpg",
      "caption": "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.",
      "context_before": "",
      "context_after": "We examine the outcomes induced by fairness constraints in the context of FICO scores for two race groups. FICO scores are a proprietary classifier widely used in the United States to predict credit worthiness. Our FICO data is based on a sample of 301,536 TransUnion TransRisk scores from 2003 [US Federal Reserve, 2007], preprocessed by Hardt et al. [2016]. These scores, corresponding to $x$ in our model, range from 300 to 850 and are meant to predict credit risk. Empirical data labeled by race allows us to estimate the distributions $\\pi _ { \\mathrm { j } }$ , where j represents race, which is restricted to two values: white non-Hispanic (labeled “white” in figures), and black. Using national demographic data, we set the population proportions to be 18% and $8 2 \\%$ .\n\nIndividuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empirical data (see Figure 4). Our outcome curve framework allows for this relaxation; however, this discrepancy can also be attributed to group-dependent mismeasurement of score, and adjusting the scores accordingly would allow for a single $\\rho ( x )$ . We use the success probabilities to define the affine utility and score change functions defined in Example 2.1. We model individual penalties as a score drop of $c _ { - } = - 1 5 0$ in the case of a default, and in increase of $c _ { + } = 7 5$ in the case of successful repayment.\n\nIn Figure 5, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities. In the case that the bank experiences a loss/profit ratio of $\\frac { u _ { - } } { u _ { + } } = - 1 0$ , no fairness criteria surpass the active harm rate $\\beta _ { 0 }$ ; however, in the case of $\\frac { u _ { - } } { u _ { + } } = - 4$ , DemParity overloans, in line with the statement in Corollary 3.3. u+",
      "referring_paragraphs": [
        "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empirical data (see Figure 4). Our outcome curve framework allows for this relaxation; however, this discrepancy can also be attributed to group-dependent mismeasurement of score, and adjusting the scores ac",
        "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.",
        "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empirical data (see Figure 4)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig10.jpg",
      "image_filename": "1803.04383_page0_fig10.jpg",
      "caption": "Loaning Decisions",
      "context_before": "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empirical data (see Figure 4). Our outcome curve framework allows for this relaxation; however, this discrepancy can also be attributed to group-dependent mismeasurement of score, and adjusting the scores accordingly would allow for a single $\\rho ( x )$ . We use the success probabilities to define the affine utility and score change functions defined in Example 2.1. We model individual penalties as a score drop of $c _ { - } = - 1 5 0$ in the case of a default, and in increase of $c _ { + } = 7 5$ in the case of successful repayment.\n\nIn Figure 5, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities. In the case that the bank experiences a loss/profit ratio of $\\frac { u _ { - } } { u _ { + } } = - 1 0$ , no fairness criteria surpass the active harm rate $\\beta _ { 0 }$ ; however, in the case of $\\frac { u _ { - } } { u _ { + } } = - 4$ , DemParity overloans, in line with the statement in Corollary 3.3. u+\n\nThese results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group. To plot the MaxUtil utility curves, the group that is not on display has selection rate fixed at $\\beta ^ { \\mathrm { M a x U t i 1 } }$ . In this figure, the top panel corresponds to the average change in credit scores for each group under different loaning rates $\\beta$ ; the bottom panels shows the corresponding total utility $\\boldsymbol { u }$ (summed over both groups and weighted by group population sizes) for the bank.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_12",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg",
      "image_filename": "1803.04383_page0_fig11.jpg",
      "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.",
      "context_before": "",
      "context_after": "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates. In this specific instance, the utility and change ratios are fairly close, u− $\\frac { u _ { - } } { u _ { + } } = - 4$ u+ , and $\\frac { c _ { - } } { c _ { + } } = - 2$ , meaning that the bank’s profit motivations align with individual outcomes to some extent. Here, we can see that EqOpt loans much closer to optimal than DemParity, similar to the setting suggested by Corollary 3.2.\n\nAlthough one might hope for decisions made under fairness constraints to positively affect the black group, we observe the opposite behavior. The MaxUtil policy (solid orange line) and the EqOpt policy result in similar expected credit score change for the black group. However, DemParity (dashed green line) causes a negative expected credit score change in the black group, corresponding to active harm. For the white group, the bank utility curve has almost the same shape under the fairness criteria as it does under MaxUtil, the main difference being that fairness criteria lowers the total expected profit from this group.\n\nThis behavior stems from a discrepancy in the outcome and profit curves for each population. While incentives for the bank and positive results for individuals are somewhat aligned for the majority group, under fairness constraints, they are more heavily misaligned in the minority group, as seen in graphs (left) in Figure 6. We remark that in other settings where the unconstrained profit maximization is misaligned with individual outcomes (e.g., when u−u = −10), fairness criteria may $\\frac { u _ { - } } { u _ { + } } = - 1 0 $ perform more favorably for the minority group by pulling the utility curve into a shape consistent with the outcome curve.",
      "referring_paragraphs": [
        "In Figure 5, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities. In the case that the bank experiences a loss/profit ratio of $\\frac { u _ { - } } { u _ { + } } = - 1 0$ , no fairness criteria surpass the active harm rate $\\beta _ { 0 }$ ; however, in the case of $\\frac { u _ { - } } { u _ { + } } = - 4$ , DemParity overloans, in line with the statement in Corollary 3.3. u+",
        "In Figure 5, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities.",
        "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig12.jpg",
      "image_filename": "1803.04383_page0_fig12.jpg",
      "caption": "Outcome Curves",
      "context_before": "Although one might hope for decisions made under fairness constraints to positively affect the black group, we observe the opposite behavior. The MaxUtil policy (solid orange line) and the EqOpt policy result in similar expected credit score change for the black group. However, DemParity (dashed green line) causes a negative expected credit score change in the black group, corresponding to active harm. For the white group, the bank utility curve has almost the same shape under the fairness criteria as it does under MaxUtil, the main difference being that fairness criteria lowers the total expected profit from this group.\n\nThis behavior stems from a discrepancy in the outcome and profit curves for each population. While incentives for the bank and positive results for individuals are somewhat aligned for the majority group, under fairness constraints, they are more heavily misaligned in the minority group, as seen in graphs (left) in Figure 6. We remark that in other settings where the unconstrained profit maximization is misaligned with individual outcomes (e.g., when u−u = −10), fairness criteria may $\\frac { u _ { - } } { u _ { + } } = - 1 0 $ perform more favorably for the minority group by pulling the utility curve into a shape consistent with the outcome curve.\n\nBy analyzing the resulting affects of MaxUtil, DemParity, and EqOpt on actual credit score lending data, we show the applicability of our model to real-world applications. In particular, some results shown in Section 3 hold empirically for the FICO TransUnion TransRisk scores.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig13.jpg",
      "image_filename": "1803.04383_page0_fig13.jpg",
      "caption": "Utility Curves",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig14.jpg",
      "image_filename": "1803.04383_page0_fig14.jpg",
      "caption": "selection rate",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_16",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg",
      "image_filename": "1803.04383_page0_fig15.jpg",
      "caption": "selection rate Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
      "context_before": "",
      "context_after": "8 Conclusion and Future Work\n\nWe argue that without a careful model of delayed outcomes, we cannot foresee the impact a fairness criterion would have if enforced as a constraint on a classification system. However, if such an accurate outcome model is available, we show that there are more direct ways to optimize for positive outcomes than via existing fairness criteria.\n\nOur formal framework exposes a concise, yet expressive way to model outcomes via the expected change in a variable of interest caused by an institutional decision. This leads to the natural concept of an outcome curve that allows us to interpret and compare solutions effectively. In essence, the formalism we propose requires us to understand the two-variable causal mechanism that translates decisions to outcomes. Depending on the application, such an understanding might necessitate greater domain knowledge and additional research into the specifics of the application. This is consistent with much scholarship that points to the context-sensitive nature of fairness in machine learning.",
      "referring_paragraphs": [
        "These results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group. To plot the MaxUtil utility curves, the group that is not on display has selection rate fixed at $\\beta ^ { \\mathrm { M a x U t i 1 } }$ . In this figure, the top panel corresponds to the average change in credit scores for each group under different loaning rates $\\beta$ ; the bottom panels shows the corresponding total utility $\\boldsymbol ",
        "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates. In this specific instance, the utility and change ratios are fairly close, u− $\\frac { u _ { - } } { u _ { + } } = - 4$ u+ , and $\\frac { c _ { - } } { c _ { + } } = - 2$ , meaning that the bank’s profit motivations align with individual outcomes to some extent. Here, we can see that EqOpt loans much closer to optimal than DemParity, similar to the setting suggested by Corollary ",
        "This behavior stems from a discrepancy in the outcome and profit curves for each population. While incentives for the bank and positive results for individuals are somewhat aligned for the majority group, under fairness constraints, they are more heavily misaligned in the minority group, as seen in graphs (left) in Figure 6. We remark that in other settings where the unconstrained profit maximization is misaligned with individual outcomes (e.g., when u−u = −10), fairness criteria may $\\frac { u ",
        "u+\n\nThese results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.",
        "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.",
        "selection rate   \nFigure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig16.jpg",
      "image_filename": "1803.04383_page0_fig16.jpg",
      "caption": "We use the following technical lemma in the proof of the above lemma.",
      "context_before": "Proof. If we have β > Px>µ $\\beta > \\sum _ { x > \\mu _ { \\mathsf { A } } } \\pi _ { \\mathsf { A } }$ , by lemma C.3, we must also have earity of expectation and linearity of µB < $\\frac { \\mu _ { \\mathsf { B } } } { \\mu _ { \\mathsf { A } } } < \\frac { \\mathrm { Q } _ { \\mathsf { B } } ( \\beta _ { 0 } ) } { \\mathrm { Q } _ { \\mathsf { A } } ( \\beta _ { 0 } ) }$ . This impliese, $\\begin{array} { r } { \\kappa = \\frac { \\sum _ { x } \\pi _ { \\mathsf { B } } ( x ) \\rho ( x ) } { \\sum _ { x } \\pi _ { \\mathsf { A } } ( x ) \\rho ( x ) } < \\frac { \\rho ( \\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) ) } { \\rho ( \\mathrm { Q } _ { \\mathsf { A } } ( \\beta _ { 0 } ) ) } } \\end{array}$ $\\rho$\n\n$$ \\kappa \\cdot \\frac {\\rho \\left(\\mathrm {Q} _ {\\mathrm {A}} (\\beta)\\right)}{\\rho \\left(\\mathrm {Q} _ {\\mathrm {B}} \\left(\\beta_ {0}\\right)\\right)} < 1 \\tag {31} $$\n\nFurther, using $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) > \\beta$ from lemma C.3 and the fact that $\\frac { \\pmb { u } ( x ) } { \\pmb { \\rho } ( x ) }$ is increasing in $x$ , we have $\\begin{array} { r } { \\frac { \\boldsymbol { u } ( \\mathrm { Q } _ { \\mathsf { B } } ( G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) ) ) } { \\rho ( \\mathrm { Q } _ { \\mathsf { B } } ( G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) ) ) } < \\frac { \\boldsymbol { u } ( \\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) ) } { \\rho ( \\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) ) } } \\end{array}$ . Therefore, $\\begin{array} { r } { u ( \\mathrm { Q } _ { \\mathtt { B } } ( G ^ { ( \\mathsf { A } \\mathsf { B } ) } ( \\beta ) ) ) \\cdot \\kappa \\cdot \\frac { \\rho ( \\mathrm { Q } _ { \\mathtt { A } } ( \\beta _ { 0 } ) ) } { \\rho ( \\mathrm { Q } _ { \\mathtt { B } } ( G ^ { ( \\mathsf { A } \\mathsf { B } ) } ( \\beta _ { 0 } ) ) ) } < \\kappa \\cdot \\frac { u ( \\mathrm { Q } _ { \\mathtt { B } } ( \\beta ) ) } { \\rho ( \\mathrm { Q } _ { \\mathtt { B } } ( \\beta ) ) } } \\end{array}$ $\\rho ( \\mathrm { Q } _ { \\mathsf { A } } ( \\beta ) ) < { \\pmb u } ( \\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) )$ where the last inequality follows from (31).",
      "context_after": "We use the following technical lemma in the proof of the above lemma.\n\nLemma C.3. If $\\pi _ { \\mathsf { A } } , \\pi _ { \\mathsf { B } }$ that are identical up to a translation with $\\mu _ { \\mathsf { A } } < \\mu _ { \\mathsf { B } }$ , then\n\n$$ G (\\beta) > \\beta \\quad \\forall \\beta , \\tag {32} $$",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1803.04383_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1804.06876": [
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig0.jpg",
      "image_filename": "1804.06876_page0_fig0.jpg",
      "caption": "Type 1",
      "context_before": "We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are avialable at http://winobias.org.\n\nCoreference resolution is a task aimed at identifying phrases (mentions) referring to the same entity. Various approaches, including rule-based (Raghunathan et al., 2010), feature-based (Durrett and Klein, 2013; Peng et al., 2015a), and neuralnetwork based (Clark and Manning, 2016; Lee et al., 2017) have been proposed. While significant advances have been made, systems carry the risk of relying on societal stereotypes present in training data that could significantly impact their performance for some demographic groups.\n\nIn this work, we test the hypothesis that coreference systems exhibit gender bias by creating a new challenge corpus, WinoBias.This dataset follows the winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015b), and contains references to people using a vocabulary of 40 occupations. It contains two types of challenge sentences that require linking gendered pro-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig1.jpg",
      "image_filename": "1804.06876_page0_fig1.jpg",
      "caption": "Type 2",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg",
      "image_filename": "1804.06876_page0_fig2.jpg",
      "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
      "context_before": "",
      "context_after": "nouns to either male or female stereotypical occupations (see the illustrative examples in Figure 1). None of the examples can be disambiguated by the gender of the pronoun but this cue can potentially distract the model. We consider a system to be gender biased if it links pronouns to occupations dominated by the gender of the pronoun (pro-stereotyped condition) more accurately than occupations not dominated by the gender of the pronoun (anti-stereotyped condition). The corpus can be used to certify a system has gender bias.1\n\nWe use three different systems as prototypical examples: the Stanford Deterministic Coreference System (Raghunathan et al., 2010), the\n\n1Note that the counter argument (i.e., systems are gender bias free) may not hold.",
      "referring_paragraphs": [
        "nouns to either male or female stereotypical occupations (see the illustrative examples in Figure 1). None of the examples can be disambiguated by the gender of the pronoun but this cue can potentially distract the model. We consider a system to be gender biased if it links pronouns to occupations dominated by the gender of the pronoun (pro-stereotyped condition) more accurately than occupations not dominated by the gender of the pronoun (anti-stereotyped condition). The corpus can be used to ce",
        "To better identify gender bias in coreference resolution systems, we build a new dataset centered on people entities referred by their occupations from a vocabulary of 40 occupations gathered from the US Department of Labor, shown in Table 1.3 We use the associated occupation statistics to determine what constitutes gender stereotypical roles (e.g. $90 \\%$ of nurses are women in this survey). Entities referred by different occupations are paired and used to construct test case scenarios. Sentenc",
        "Type 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun] [circumstances]. Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1). Such examples are challenging because they contain no syntactic cues.",
        "Type 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]. These tests can be resolved using syntactic information and understanding of the pronoun (Figure 1; Type 2). We expect systems to do well on such cases because both semantic and syntactic cues help disambiguation.",
        "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset.",
        "Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/89b88d06df9094145068b49a5780a0e3991fd584bd525a9c805e78d1df1a638b.jpg",
      "image_filename": "89b88d06df9094145068b49a5780a0e3991fd584bd525a9c805e78d1df1a638b.jpg",
      "caption": "Table 1: Occupations statistics used in WinoBias dataset, organized by the percent of people in the occupation who are reported as female. When woman dominate profession, we call linking the noun phrase referring to the job with female and male pronoun as ‘pro-stereotypical‘, and ‘anti-stereotypical‘, respectively. Similarly, if the occupation is male dominated, linking the noun phrase with the male and female pronoun is called, ‘pro-stereotypical‘ and ‘anti-steretypical‘, respectively.",
      "context_before": "Berkeley Coreference Resolution System (Durrett and Klein, 2013) and the current best published system: the UW End-to-end Neural Coreference Resolution System (Lee et al., 2017). Despite qualitatively different approaches, all systems exhibit gender bias, showing an average difference in performance between pro-stereotypical and antistereotyped conditions of 21.1 in F1 score. Finally we show that given sufficiently strong alternative cues, systems can ignore their bias.\n\nIn order to study the source of this bias, we analyze the training corpus used by these systems, Ontonotes 5.0 (Weischedel et al., 2012).2 Our analysis shows that female entities are significantly underrepresented in this corpus. To reduce the impact of such dataset bias, we propose to generate an auxiliary dataset where all male entities are replaced by female entities, and vice versa, using a rule-based approach. Methods can then be trained on the union of the original and auxiliary dataset. In combination with methods that remove bias from fixed resources such as word embeddings (Bolukbasi et al., 2016), our data augmentation approach completely eliminates bias when evaluating on WinoBias , without significantly affecting overall coreference accuracy.\n\nTo better identify gender bias in coreference resolution systems, we build a new dataset centered on people entities referred by their occupations from a vocabulary of 40 occupations gathered from the US Department of Labor, shown in Table 1.3 We use the associated occupation statistics to determine what constitutes gender stereotypical roles (e.g. $90 \\%$ of nurses are women in this survey). Entities referred by different occupations are paired and used to construct test case scenarios. Sentences are duplicated using male and female pronouns, and contain equal numbers of correct coreference decisions for all occupations. In total, the dataset contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging",
      "context_after": "and designed to cover cases requiring semantics and syntax separately.4\n\nType 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun] [circumstances]. Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1). Such examples are challenging because they contain no syntactic cues.\n\nType 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]. These tests can be resolved using syntactic information and understanding of the pronoun (Figure 1; Type 2). We expect systems to do well on such cases because both semantic and syntactic cues help disambiguation.",
      "referring_paragraphs": [
        "nouns to either male or female stereotypical occupations (see the illustrative examples in Figure 1). None of the examples can be disambiguated by the gender of the pronoun but this cue can potentially distract the model. We consider a system to be gender biased if it links pronouns to occupations dominated by the gender of the pronoun (pro-stereotyped condition) more accurately than occupations not dominated by the gender of the pronoun (anti-stereotyped condition). The corpus can be used to ce",
        "To better identify gender bias in coreference resolution systems, we build a new dataset centered on people entities referred by their occupations from a vocabulary of 40 occupations gathered from the US Department of Labor, shown in Table 1.3 We use the associated occupation statistics to determine what constitutes gender stereotypical roles (e.g. $90 \\%$ of nurses are women in this survey). Entities referred by different occupations are paired and used to construct test case scenarios. Sentenc",
        "Type 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun] [circumstances]. Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1). Such examples are challenging because they contain no syntactic cues.",
        "Type 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]. These tests can be resolved using syntactic information and understanding of the pronoun (Figure 1; Type 2). We expect systems to do well on such cases because both semantic and syntactic cues help disambiguation.",
        "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset.",
        "Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/14715b189046abc095c05c18f6900e980695ab4f74ce1d266d9fb0d5a253675b.jpg",
      "image_filename": "14715b189046abc095c05c18f6900e980695ab4f74ce1d266d9fb0d5a253675b.jpg",
      "caption": "7Five turkers were presented with anonymized spans and asked to mark if it indicated male, female, or neither, and if male or female, rewrite it so it refers to the other gender.",
      "context_before": "5To exclude mentions such as “his mother”, we use Collins head finder (Collins, 2003) to identify the head word of each mention, and only consider the mentions whose head word is gender pronoun.\n\n6We pick more than 900 job titles from a gazetteer.\n\n7Five turkers were presented with anonymized spans and asked to mark if it indicated male, female, or neither, and if male or female, rewrite it so it refers to the other gender.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/8004c62a0561732f089ceb940f14aae3eb0346b3bed6f1e8ac0d529b1b546d40.jpg",
      "image_filename": "8004c62a0561732f089ceb940f14aae3eb0346b3bed6f1e8ac0d529b1b546d40.jpg",
      "caption": "Table 2: F1 on OntoNotes and WinoBias development set. WinoBias results are split between Type-1 and Type-2 and in pro/anti-stereotypical conditions. * indicates the difference between pro/anti stereotypical conditions is significant $( p < . 0 5 )$ under an approximate randomized test (Graham et al., 2014). Our methods eliminate the difference between pro-stereotypical and anti-stereotypical conditions (Diff), with little loss in performance (OntoNotes and Avg). Table 3: F1 on OntoNotes and Winobias test sets. Methods were run once, supporting development set conclusions.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "WinoBias Reveals Gender Bias Table 2 summarizes development set evaluations using all three systems. Systems were evaluated on both types of sentences in WinoBias (T1 and T2), separately in pro-stereotyped and anti-stereotyped conditions ( T1-p vs. T1-a, T2-p vs T2-a). We evaluate the effect of named-entity anonymization (Anon.), debiasing supporting resources8 (Re-",
        "WinoBias Reveals Gender Bias Table 2 summarizes development set evaluations using all three systems."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/4116309fff724c75092bd25db0a87d2fb95556f27d414bf76dc10bffa5226f8b.jpg",
      "image_filename": "4116309fff724c75092bd25db0a87d2fb95556f27d414bf76dc10bffa5226f8b.jpg",
      "caption": "Table 4: Performance on the original and the genderreversed developments dataset (anonymized).",
      "context_before": "",
      "context_after": "sour.) and using data-augmentation through gender swapping (Aug.). E2E and Feature were retrained in each condition using default hyperparameters while Rule was not debiased because it is untrainable. We evaluate using the coreference scorer v8.01 (Pradhan et al., 2014) and compute the average (Avg) and absolute difference (Diff) between pro-stereotyped and antistereotyped conditions in WinoBias.\n\nAll initial systems demonstrate severe disparity between pro-stereotyped and anti-stereotyped conditions. Overall, the rule based system is most biased, followed by the neural approach and feature rich approach. Across all conditions, anonymization impacts E2E the most, while all other debiasing methods result in insignificant loss in performance on the OntoNotes dataset. Removing biased resources and data-augmentation reduce bias independently and more so in combination, allowing both E2E and Feature to pass WinoBias without significantly impacting performance on either OntoNotes or WinoBias . Qualitatively, the neural system is easiest to de-bias and our approaches could be applied to future end-to-\n\nend systems. Systems were evaluated once on test sets, Table 3, supporting our conclusions.",
      "referring_paragraphs": [
        "Table 4 summarizes our results. The E2E system does not demonstrate significant degradation in performance, while Feature loses roughly 1.0- F1.10 This demonstrates that given sufficient alternative signal, systems often do ignore gender biased cues. On the other hand, WinoBias provides an analysis of system bias in an adversarial setup, showing, when examples are challenging, systems are likely to make gender biased predictions.",
        "<table><tr><td>Method</td><td>Anon.</td><td>Resour.</td><td>Aug.</td><td>OntoNotes</td><td>T1-p</td><td>T1-a</td><td>Avg</td><td>|Diff|</td><td>T2-p</td><td>T2-a</td><td>Avg</td><td>|Diff|</td></tr><tr><td>E2E</td><td rowspan=\"2\">✓</td><td rowspan=\"2\">✓</td><td rowspan=\"2\">✓</td><td>67.2</td><td>74.9</td><td>47.7</td><td>61.3</td><td>27.2*</td><td>88.6</td><td>77.3</td><td>82.9</td><td>11.3*</td></tr><tr><td>E2E</td><td>66.5</td><td>62.4</td><td>60.3</td><td>61.3</td><td>2.1</td><td>78.4</td><td>78.0</td><td>78.2</td><td>0.4</td></tr><tr><td>Feature</td><td rowspan=\"2\">✓</td><td rowspan=\"2\">✓</td><td rowspan=\"2\">✓</td><td>64.0</td><td>62.9</td><td>58.3</td><td>60.6</td><td>4.6*</td><td>68.5</td><td>57.8</td><td>63.1</td><td>10.7*</td></tr><tr><td>Feature</td><td>63.6</td><td>62.2</td><td>60.6</td><td>61.4</td><td>1.7</td><td>70.0</td><td>69.5</td><td>69.7</td><td>0.6</td></tr><tr><td>Rule</td><td></td><td></td><td></td><td>58.7</td><td>72.0</td><td>37.5</td><td>54.8</td><td>34.5*</td><td>47.8</td><td>26.6</td><td>37.2</td><td>21.2*</td></tr></table>\n\nTable 4: Performance on the original and the genderreversed developments dataset (anonymized)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.06876_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1804.09301": [
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg",
      "image_filename": "1804.09301_page0_fig0.jpg",
      "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
      "context_before": "That a majority of people are reportedly unable to solve this riddle1 is taken as evidence of underlying implicit gender bias (Wapman and Belle, 2014): many first-time listeners have difficulty assigning both the role of “mother” and “surgeon” to the same entity.\n\nAs the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.\n\nThere are many ways one could approach this question; here we focus on gender bias with respect to occupations, for which we have corresponding U.S. employment statistics. Our approach is to construct a challenge dataset in",
      "context_after": "the style of Winograd schemas, wherein a pronoun must be resolved to one of two previouslymentioned entities in a sentence designed to be easy for humans to interpret, but challenging for data-driven systems (Levesque et al., 2011). In our setting, one of these mentions is a person referred to by their occupation; by varying only the pronoun’s gender, we are able to test the impact of gender on resolution. With these “Winogender schemas,” we demonstrate the presence of systematic gender bias in multiple publiclyavailable coreference resolution systems, and that occupation-specific bias is correlated with employment statistics. We release these test sentences to the public.2\n\nIn our experiments, we represent gender as a categorical variable with either two or three possible values: female, male, and (in some cases) neutral. These choices reflect limitations of the textual and real-world datasets we use.\n\n2 Coreference Systems",
      "referring_paragraphs": [
        "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.",
        "Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1. To this end, we create a specialized evaluation set consisting of 120 hand-written sentence templates, in the style of the Winograd Schemas (Levesque et al., 2011). Each sentence contains three referring expressions of interest:",
        "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende",
        "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1).",
        "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
        "Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1.",
        "Table 1: Correlation values for Figures 3 and 4.",
        "Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg",
      "image_filename": "1804.09301_page0_fig1.jpg",
      "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.",
      "context_before": "Validation Like Winograd schemas, each sentence template is written with one intended correct answer (here, either OCCUPATION or PAR-\n\n3This data was distributed in the CoNLL 2011 and 2012 shared tasks on coreference resolution. (Pradhan et al., 2011, 2012)\n\n450 are from the supplement of Caliskan et al. (2017), an additional 7 from personal communication with the authors, and three that we added: doctor, firefighter, and secretary.",
      "context_after": "TICIPANT).5 We aimed to write sentences where (1) pronoun resolution was as unambiguous for humans as possible (in the absence of additional context), and (2) the resolution would not be affected by changing pronoun gender. (See Figure 2.) Nonetheless, to ensure that our own judgments are shared by other English speakers, we validated all 720 sentences on Mechanical Turk, with 10-way redundancy. Each MTurk task included 5 sentences from our dataset, and 5 sentences from the Winograd Schema Challenge (Levesque et al., 2011)6, though this additional validation step turned out to be unnecessary.7 Out of 7200 binary-choice worker annotations (720 sentences $\\times 1 0$ -way redundancy), $9 4 . 9 \\%$ of responses agree with our intended answers. With simple majority voting on each sentence, worker responses agree with our intended answers for 718 of 720 sentences $( 9 9 . 7 \\% )$ . The two sentences with low agreement have neutral gender (“they”), and are not reflected in any binary (female-male) analysis.",
      "referring_paragraphs": [
        "We use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresponding gender percentages available from the U.S. Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2). For each sentence template, there are three PRO-NOUN instantiations (female, male, or neutral), and two PARTICIPANT instantiations (",
        "TICIPANT).5 We aimed to write sentences where (1) pronoun resolution was as unambiguous for humans as possible (in the absence of additional context), and (2) the resolution would not be affected by changing pronoun gender. (See Figure 2.) Nonetheless, to ensure that our own judgments are shared by other English speakers, we validated all 720 sentences on Mechanical Turk, with 10-way redundancy. Each MTurk task included 5 sentences from our dataset, and 5 sentences from the Winograd Schema Chall",
        "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende",
        "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).",
        "Figure 2: A “Winogender” schema for the occupation paramedic.",
        "We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about which mentions are coreferent, percentage-wise differences in real-world statistics may translate into absolute differences in system predictions."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1f7c7ec514efeaeb6fe748921e08e0547aff05d496fb0a8573b2f718083c6998.jpg",
      "image_filename": "1f7c7ec514efeaeb6fe748921e08e0547aff05d496fb0a8573b2f718083c6998.jpg",
      "caption": "Table 1: Correlation values for Figures 3 and 4.",
      "context_before": "TICIPANT).5 We aimed to write sentences where (1) pronoun resolution was as unambiguous for humans as possible (in the absence of additional context), and (2) the resolution would not be affected by changing pronoun gender. (See Figure 2.) Nonetheless, to ensure that our own judgments are shared by other English speakers, we validated all 720 sentences on Mechanical Turk, with 10-way redundancy. Each MTurk task included 5 sentences from our dataset, and 5 sentences from the Winograd Schema Challenge (Levesque et al., 2011)6, though this additional validation step turned out to be unnecessary.7 Out of 7200 binary-choice worker annotations (720 sentences $\\times 1 0$ -way redundancy), $9 4 . 9 \\%$ of responses agree with our intended answers. With simple majority voting on each sentence, worker responses agree with our intended answers for 718 of 720 sentences $( 9 9 . 7 \\% )$ . The two sentences with low agreement have neutral gender (“they”), and are not reflected in any binary (female-male) analysis.",
      "context_after": "",
      "referring_paragraphs": [
        "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.",
        "Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1. To this end, we create a specialized evaluation set consisting of 120 hand-written sentence templates, in the style of the Winograd Schemas (Levesque et al., 2011). Each sentence contains three referring expressions of interest:",
        "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende",
        "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1).",
        "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
        "Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1.",
        "Table 1: Correlation values for Figures 3 and 4.",
        "Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg",
      "image_filename": "1804.09301_page0_fig2.jpg",
      "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
      "context_before": "",
      "context_after": "4 Results and Discussion\n\nWe evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).\n\nBy multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems. First we observe that these systems do not behave in a gender-neutral fashion. That is to say, we have designed test sentences where correct pronoun resolution is not a function of gender (as validated by human annotators), but system predictions do exhibit sensitivity to pronoun gender: $68 \\%$ of male-female minimal pair test sentences are resolved differently by the RULE system; $28 \\%$ for STAT; and $13 \\%$ for NEURAL.",
      "referring_paragraphs": [
        "no managers are predicted to be female. This illustrates two related phenomena: first, that datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline, and second, that although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3).",
        "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
        "This illustrates two related phenomena: first, that datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline, and second, that although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg",
      "image_filename": "1804.09301_page0_fig3.jpg",
      "caption": "7In the end, we did not use the Winograd schemas to filter annotators, as raw agreement on the Winogender schemas was much higher to begin with $9 4 . 9 \\%$ Winogender vs. $8 6 . 5 \\%$ Winograd).",
      "context_before": "5Unlike Winograd schemas, we are not primarily concerned with whether these sentences are “hard” to solve, e.g., because they would require certain types of human knowledge or could not be easily solved with word co-occurrence statistics.\n\n6We used the publicly-available examples from https://cs.nyu.edu/faculty/davise/ papers/WinogradSchemas/WSCollection.html\n\n7In the end, we did not use the Winograd schemas to filter annotators, as raw agreement on the Winogender schemas was much higher to begin with $9 4 . 9 \\%$ Winogender vs. $8 6 . 5 \\%$ Winograd).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig4.jpg",
      "image_filename": "1804.09301_page0_fig4.jpg",
      "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
      "context_before": "",
      "context_after": "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about which mentions are coreferent, percentage-wise differences in real-world statistics may translate into absolute differences in system predictions. For example, the occupation “manager” is $3 8 . 5 \\%$ female in the U.S. according to real-world statistics (BLS); mentions of “manager” in text are only $5 . 1 8 \\%$ female (B&L resource); and finally, as viewed through the behavior of the three coreference systems we tested,\n\nno managers are predicted to be female. This illustrates two related phenomena: first, that datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline, and second, that although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3).",
      "referring_paragraphs": [
        "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende",
        "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_7",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/d21b28b9a0e5f04b69a329b6b24cbf7f1edd5af127e6790983b6a9569dad2a79.jpg",
      "image_filename": "d21b28b9a0e5f04b69a329b6b24cbf7f1edd5af127e6790983b6a9569dad2a79.jpg",
      "caption": "Table 2: System accuracy $( \\% )$ bucketed by gender and difficulty (so-called “gotchas,” shaded in purple). For female pronouns, a “gotcha” sentence is one where either (1) the correct answer is OCCUPATION but the occupation is $< 5 0 \\%$ female (according to BLS); or (2) the occupation is $\\geq 5 0 \\%$ female but the correct answer is PARTICIPANT; this is reversed for male pronouns. Systems do uniformly worse on “gotchas.”",
      "context_before": "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about which mentions are coreferent, percentage-wise differences in real-world statistics may translate into absolute differences in system predictions. For example, the occupation “manager” is $3 8 . 5 \\%$ female in the U.S. according to real-world statistics (BLS); mentions of “manager” in text are only $5 . 1 8 \\%$ female (B&L resource); and finally, as viewed through the behavior of the three coreference systems we tested,\n\nno managers are predicted to be female. This illustrates two related phenomena: first, that datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline, and second, that although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3).",
      "context_after": "Here we give a brief (and non-exhaustive) overview of prior work on gender bias in NLP systems and datasets. A number of papers explore (gender) bias in English word embeddings:\n\n8“ The librarian helped the child pick out a book because he liked to encourage reading.” is an example of a “gotcha” sentence; librarians are $> 5 0 \\%$ female (BLS).\n\nhow they capture implicit human biases in modern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukbasi et al., 2016). Further work on debiasing models with adversarial learning is explored by Beutel et al. (2017) and Zhang et al. (2018).",
      "referring_paragraphs": [
        "We use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresponding gender percentages available from the U.S. Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2). For each sentence template, there are three PRO-NOUN instantiations (female, male, or neutral), and two PARTICIPANT instantiations (",
        "TICIPANT).5 We aimed to write sentences where (1) pronoun resolution was as unambiguous for humans as possible (in the absence of additional context), and (2) the resolution would not be affected by changing pronoun gender. (See Figure 2.) Nonetheless, to ensure that our own judgments are shared by other English speakers, we validated all 720 sentences on Mechanical Turk, with 10-way redundancy. Each MTurk task included 5 sentences from our dataset, and 5 sentences from the Winograd Schema Chall",
        "When these systems’ predictions diverge based on pronoun gender, they do so in ways that reinforce and magnify real-world occupational gender disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gende",
        "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).",
        "Figure 2: A “Winogender” schema for the occupation paramedic.",
        "We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about which mentions are coreferent, percentage-wise differences in real-world statistics may translate into absolute differences in system predictions."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1804.09301_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1805.03094": [
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/8b918a9fb2bde89fe1ee51f0bfd3c145b309679ad9c2ebf0a652c051648d9492.jpg",
      "image_filename": "8b918a9fb2bde89fe1ee51f0bfd3c145b309679ad9c2ebf0a652c051648d9492.jpg",
      "caption": "Table 1: Variables defining important disaggregations of Stack Exchange data, along with their pseudo- $\\bar { . } R ^ { 2 }$ scores.",
      "context_before": "First, we study answerer performance on Stack Exchange (SE). Launched in 2008 as a forum for asking computer programming questions, Stack Exchange has grown to encompass a variety of technical and non-technical topics. Any user can ask a question, which others may answer. Users can vote for answers they find helpful, but only the asker can accept one of the answers as the best answer to the question. We used anonymized data representing all answers to questions posted on Stack Exchange from August 2008 until September 2014.2 Approximately half of the 9.6M questions had an accepted answer, and we included in the study questions that received two or more answers.\n\nTo understand factors affecting user performance on SE, we study the relationship between the various features extracted from data and the outcome, here a binary attribute\n\n2https://archive.org/details/stackexchange",
      "context_after": "denoting whether the answer written by a user is accepted by the asker as best answer to his or her question. To this end, for each answer written by a user, we create a list of features describing the answer and the user. Features include the numbers of words, hyperlinks, and lines of code the answer contains, and its Flesch readability score (Kincaid et al. 1975). Features describing answerers are their reputation, tenure on SE (in seconds and in terms of percentile rank) and the total number of answers written during their tenure. These features relate to user experience. We also use activity-related features, including time since previous answer written by the user, session length, giving the number of answers user writes during the session, and answer position within that session. We define a session as a period of activity without a break of 100 minutes of longer.\n\nOf the 110 potential disaggregations of SE data arising from all possible pairs of covariates, our method identified 8 as significant. Table 1 ranks these disaggregations along with their pseudo- $R ^ { 2 }$ scores. Note that user experience, either in terms of the reputation or the number of answers written by the user over his or her tenure, comes up as an important conditioning variable in several disaggregations. Features related to user activity, such as answer position within a session, session length, and time since previous answer, appear as important dimensions of performance. This suggests that answerer behavior over the course of a session changes significantly, and these changes are different across different sub-populations.\n\nFigure 1 visualizes the data, disaggregated on the number of answers. Each horizontal band in the heatmap in Fig. 1(a) is a different bin of the conditioning variable number of answers, and it corresponds to a distinct subgroup within the data. The first bin ranges in value from one to eleven answers, the second bin from 12 to over 50 answers, etc. Within each bin, the color shows the relationship between the outcome—the probability the answer is accepted—and answer’s position within a session. Dark blue corresponds to the lowest acceptance probability, and dark red to the highest. Within each bin, the color changes from lighter blue to darker blue (for the bottom-most bins), indicating a lower acceptance probability for answers written later in the session. For the top-most bins, the acceptance probability is overall higher, but also decreases, e.g., from pink to white to blue. Note that data is noisy, as manifested by color flipping, where there are few data points (Fig. 1(b)).",
      "referring_paragraphs": [
        "Of the 110 potential disaggregations of SE data arising from all possible pairs of covariates, our method identified 8 as significant. Table 1 ranks these disaggregations along with their pseudo- $R ^ { 2 }$ scores. Note that user experience, either in terms of the reputation or the number of answers written by the user over his or her tenure, comes up as an important conditioning variable in several disaggregations. Features related to user activity, such as answer position within a session, se",
        "Figure 1 visualizes the data, disaggregated on the number of answers. Each horizontal band in the heatmap in Fig. 1(a) is a different bin of the conditioning variable number of answers, and it corresponds to a distinct subgroup within the data. The first bin ranges in value from one to eleven answers, the second bin from 12 to over 50 answers, etc. Within each bin, the color shows the relationship between the outcome—the probability the answer is accepted—and answer’s position within a session. ",
        "The trends corresponding to these empirical observations are captured in Fig. 1(c). Note that the decreasing trends are in contrast to the trend in aggregate data (Fig. 1(d)), which shows performance increasing with answer position within the session. This suggests that user experience, as captured by the number of answers, is an important factor differentiating the behavior of users.",
        "Figure 1 visualizes the data, disaggregated on the number of answers.",
        "Figure 1: Disaggregation of Stack Exchange data."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig0.jpg",
      "image_filename": "1805.03094_page0_fig0.jpg",
      "caption": "3https://www.khanacademy.org",
      "context_before": "As an outcome variable in this data, we take student performance on a problem, a binary variable equal to one when the student solved the problem correctly on the first try, and zero otherwise (either did not solve it correctly, or used hints). To study factors affecting performance, we extracted the features of problems and users. These included the overall solving time during user activity, total solve time and the number of attempts made to solve the problem, time since the previous problem (tspp), the number of sessions prior to the current one, all sessions user contributed to, the session length in terms of the number of problems solved, problem position within the session (session index of the problem), the timestamp of the attempt, including the month, day of week, type of weekday ( whether it is weekend or not) and hour the student attempted to solve the problem, the month the student joined KA, his or her tenure, the number of all attempts on all problems solved since joining, and how many of the problems were solved correctly on the first attempts. As a proxy of skill or some background knowledge the student brings, we use how many problems were correctly solved during the student’s five first attempts to solve problems. For example, the least prepared students answered few of the five problems they attempted to solve, but best students would have solved all five correctly.\n\nOur method identified 32 significant disaggregations of KA data, out of 342 potential disaggregations. Some of these are presented in Table 2. The table lists conditioning variables for selected covariates, sorted by their pseudo- $R ^ { \\breve { 2 } }$\n\n3https://www.khanacademy.org",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg",
      "image_filename": "1805.03094_page0_fig1.jpg",
      "caption": "(a) Disaggregated data (c) Subgroup trends",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig2.jpg",
      "image_filename": "1805.03094_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_5",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig3.jpg",
      "image_filename": "1805.03094_page0_fig3.jpg",
      "caption": "(b) Number of samples (d) Aggregate trend Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.",
      "context_before": "",
      "context_after": "scores. For example, when examining how performance— probability to solve a problem correctly—changes over the course of a day $X _ { j }$ is hour24), the relevant disaggregation conditions the data on all first attempts, i.e., the number of all problems the user solved correctly on their first attempt. On the other hand, several disaggregations can explain the trends in performance as a function of month. Conditioning on first five attempts has the most explanatory power, followed by disaggregations conditioned on session index, the total time it took the user to solve all problems, the timestamp and weekday of the attempt. Many of the conditioning variables used in the disaggregations represent different aspects of user experience on the site: the number of problems they tried to solve or correctly solved, their tenure on the site, and how much time they spent solving problems.\n\nFigure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users solved correctly on their first try, captures both user’s motivation to use KA (the more motivated, the more problems they attempt), and skill (the more skilled, the more problems they will solve on their first attempt). The high-achieving users actually perform better in the morning, in contrast to aggregate trends.\n\nFigure 4 shows the disaggregation corresponding to the",
      "referring_paragraphs": [
        "Of the 110 potential disaggregations of SE data arising from all possible pairs of covariates, our method identified 8 as significant. Table 1 ranks these disaggregations along with their pseudo- $R ^ { 2 }$ scores. Note that user experience, either in terms of the reputation or the number of answers written by the user over his or her tenure, comes up as an important conditioning variable in several disaggregations. Features related to user activity, such as answer position within a session, se",
        "Figure 1 visualizes the data, disaggregated on the number of answers. Each horizontal band in the heatmap in Fig. 1(a) is a different bin of the conditioning variable number of answers, and it corresponds to a distinct subgroup within the data. The first bin ranges in value from one to eleven answers, the second bin from 12 to over 50 answers, etc. Within each bin, the color shows the relationship between the outcome—the probability the answer is accepted—and answer’s position within a session. ",
        "The trends corresponding to these empirical observations are captured in Fig. 1(c). Note that the decreasing trends are in contrast to the trend in aggregate data (Fig. 1(d)), which shows performance increasing with answer position within the session. This suggests that user experience, as captured by the number of answers, is an important factor differentiating the behavior of users.",
        "Figure 1 visualizes the data, disaggregated on the number of answers.",
        "Figure 1: Disaggregation of Stack Exchange data."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig4.jpg",
      "image_filename": "1805.03094_page0_fig4.jpg",
      "caption": "Figure 4 shows the disaggregation corresponding to the",
      "context_before": "scores. For example, when examining how performance— probability to solve a problem correctly—changes over the course of a day $X _ { j }$ is hour24), the relevant disaggregation conditions the data on all first attempts, i.e., the number of all problems the user solved correctly on their first attempt. On the other hand, several disaggregations can explain the trends in performance as a function of month. Conditioning on first five attempts has the most explanatory power, followed by disaggregations conditioned on session index, the total time it took the user to solve all problems, the timestamp and weekday of the attempt. Many of the conditioning variables used in the disaggregations represent different aspects of user experience on the site: the number of problems they tried to solve or correctly solved, their tenure on the site, and how much time they spent solving problems.\n\nFigure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users solved correctly on their first try, captures both user’s motivation to use KA (the more motivated, the more problems they attempt), and skill (the more skilled, the more problems they will solve on their first attempt). The high-achieving users actually perform better in the morning, in contrast to aggregate trends.\n\nFigure 4 shows the disaggregation corresponding to the",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg",
      "image_filename": "1805.03094_page0_fig5.jpg",
      "caption": "(a) Disaggregated data (c) Subgroup trends",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig6.jpg",
      "image_filename": "1805.03094_page0_fig6.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_9",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig7.jpg",
      "image_filename": "1805.03094_page0_fig7.jpg",
      "caption": "(b) Number of samples (d) Aggregate trend Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.",
      "context_before": "",
      "context_after": "covariate month, conditioned on five first attempts. When data is aggregated over the entire population, there appears to be a slight seasonal variation, with performance higher on average during the summer months (Fig. 4(d)). Once data is disaggregated by five first attempts, the seasonal trends are no longer so obvious in several subgroups (Fig. 4(c)). Interestingly, it appears to be the high achieving users (who correctly answer more of the five first problems), who perform better during the summer months. This suggests that population of KA changes over the course of the year, with motivated, high achieving students using the platform during their summer break.\n\nDuolingo (DL) is an online language learning platform, which allows users to learn dozens of different lan-\n\nguages. DL offers a gamified learning environment, where users progress through levels by practicing vocabulary and dictation skills. The DL halflife-regression (Settles and Meeder 2016) dataset (https://github.com/duolingo/halfliferegression) follows a subset of learners over a period of two weeks. Users are shown vocabulary words and asked to recall them correctly. Users may be shown between 7 and 20 words per lesson, and may have multiple lessons in a session. Sessions are defined in a similar way as before—a period of activity without a break longer than one hour.",
      "referring_paragraphs": [
        "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation. This disaggregation is slightly worse, resulting in a somewhat lower value pseudo- $R ^ { \\bar { 2 } }$ . While performance declines in the lower reputation subgroups as a function of answer position, the highest reputation users appear to write better answers in longer sessions. The acceptance probability for high reputation users is more than 0.50, potentially indicatin",
        "Our method identified 32 significant disaggregations of KA data, out of 342 potential disaggregations. Some of these are presented in Table 2. The table lists conditioning variables for selected covariates, sorted by their pseudo- $R ^ { \\breve { 2 } }$",
        "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation.",
        "Some of these are presented in Table 2.",
        "Figure 2: Disaggregation of Stack Exchange data similar to Fig.",
        "Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their pseudo- $\\bar { \\boldsymbol { R } } ^ { 2 }$ scores."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_10",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/8bd05b07d0e6d05f775ed38ac28e5e81c45814624464a4cca206aad87ef1b92e.jpg",
      "image_filename": "8bd05b07d0e6d05f775ed38ac28e5e81c45814624464a4cca206aad87ef1b92e.jpg",
      "caption": "Table 2: Variables defining important disaggregations of the Khan Academy data, along with their pseudo- $\\bar { \\boldsymbol { R } } ^ { 2 }$ scores.",
      "context_before": "Duolingo (DL) is an online language learning platform, which allows users to learn dozens of different lan-\n\nguages. DL offers a gamified learning environment, where users progress through levels by practicing vocabulary and dictation skills. The DL halflife-regression (Settles and Meeder 2016) dataset (https://github.com/duolingo/halfliferegression) follows a subset of learners over a period of two weeks. Users are shown vocabulary words and asked to recall them correctly. Users may be shown between 7 and 20 words per lesson, and may have multiple lessons in a session. Sessions are defined in a similar way as before—a period of activity without a break longer than one hour.\n\nUsers in general perform quite well, correctly recalling a large number of words in a lesson. This makes it difficult to discern changes in performance. Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation. This disaggregation is slightly worse, resulting in a somewhat lower value pseudo- $R ^ { \\bar { 2 } }$ . While performance declines in the lower reputation subgroups as a function of answer position, the highest reputation users appear to write better answers in longer sessions. The acceptance probability for high reputation users is more than 0.50, potentially indicatin",
        "Our method identified 32 significant disaggregations of KA data, out of 342 potential disaggregations. Some of these are presented in Table 2. The table lists conditioning variables for selected covariates, sorted by their pseudo- $R ^ { \\breve { 2 } }$",
        "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation.",
        "Some of these are presented in Table 2.",
        "Figure 2: Disaggregation of Stack Exchange data similar to Fig.",
        "Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their pseudo- $\\bar { \\boldsymbol { R } } ^ { 2 }$ scores."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig8.jpg",
      "image_filename": "1805.03094_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig9.jpg",
      "image_filename": "1805.03094_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig10.jpg",
      "image_filename": "1805.03094_page0_fig10.jpg",
      "caption": "(a) Disaggregated data",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig11.jpg",
      "image_filename": "1805.03094_page0_fig11.jpg",
      "caption": "(b) Number of samples (c) Subgroup trends (d) Aggregate trend",
      "context_before": "",
      "context_after": "erwise. We used more than two dozen features to describe performance. These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration, etc. User-related features include the number of five first lessons correctly answers, number of all perfect lessons with perfect performance, total number of lessons,",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_15",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig12.jpg",
      "image_filename": "1805.03094_page0_fig12.jpg",
      "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
      "context_before": "erwise. We used more than two dozen features to describe performance. These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration, etc. User-related features include the number of five first lessons correctly answers, number of all perfect lessons with perfect performance, total number of lessons,",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users so",
        "Of the 462 potential disaggregations of DL data, 51 were found to be significant using the $\\chi ^ { 2 }$ test. Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc. The trends with respect to some of the covariates could be explained by several different disaggregations, with some of them having relatively high values of pseudo- $R ^ { 2 }$ . Again, user expe",
        "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24.",
        "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
        "Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg",
      "image_filename": "1805.03094_page0_fig13.jpg",
      "caption": "(a) Disaggregated data (c) Subgroup trends",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig14.jpg",
      "image_filename": "1805.03094_page0_fig14.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_18",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig15.jpg",
      "image_filename": "1805.03094_page0_fig15.jpg",
      "caption": "(b) Number of samples (d) Aggregate trend Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
      "context_before": "",
      "context_after": "the total number of words seen and the correctly answered, and the time the user was active.\n\nOf the 462 potential disaggregations of DL data, 51 were found to be significant using the $\\chi ^ { 2 }$ test. Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc. The trends with respect to some of the covariates could be explained by several different disaggregations, with some of them having relatively high values of pseudo- $R ^ { 2 }$ . Again, user experience (all perfect lessons) and initial skill (five first lessons) appear as significant conditioning variables.\n\nFigure 5 examines the impact of experience on performance. In the aggregate data Fig. 5(d), performance appears to increase as function of experience (lesson index): users who have more practice perform better. However, once the data is disaggregated by initial performance (five first lessons), or skill, in Fig. 5(c), a subtler picture emerges. Users who initially performed the worst (bottom bins in Fig. 5(a)) improve their performance as they have more lessons, while the best performers initially (top bins) decline. This may be due to “regression to the mean”, as pure luck could have helped the initially best performers and hurt the initially worst performers.",
      "referring_paragraphs": [
        "Figure 4 shows the disaggregation corresponding to the",
        "covariate month, conditioned on five first attempts. When data is aggregated over the entire population, there appears to be a slight seasonal variation, with performance higher on average during the summer months (Fig. 4(d)). Once data is disaggregated by five first attempts, the seasonal trends are no longer so obvious in several subgroups (Fig. 4(c)). Interestingly, it appears to be the high achieving users (who correctly answer more of the five first problems), who perform better during the ",
        "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_19",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/92ee48bdf21a8197424a5607949a67cea5f830f08da8366a8957711fda894d90.jpg",
      "image_filename": "92ee48bdf21a8197424a5607949a67cea5f830f08da8366a8957711fda894d90.jpg",
      "caption": "Table 3: Variables defining important disaggregations of Duolingo data, along with their pseudo- $R ^ { 2 }$ scores.",
      "context_before": "Of the 462 potential disaggregations of DL data, 51 were found to be significant using the $\\chi ^ { 2 }$ test. Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc. The trends with respect to some of the covariates could be explained by several different disaggregations, with some of them having relatively high values of pseudo- $R ^ { 2 }$ . Again, user experience (all perfect lessons) and initial skill (five first lessons) appear as significant conditioning variables.\n\nFigure 5 examines the impact of experience on performance. In the aggregate data Fig. 5(d), performance appears to increase as function of experience (lesson index): users who have more practice perform better. However, once the data is disaggregated by initial performance (five first lessons), or skill, in Fig. 5(c), a subtler picture emerges. Users who initially performed the worst (bottom bins in Fig. 5(a)) improve their performance as they have more lessons, while the best performers initially (top bins) decline. This may be due to “regression to the mean”, as pure luck could have helped the initially best performers and hurt the initially worst performers.\n\nAnother disaggregation of DL data is shown in Figure 6. The plots show performance as a function of lesson correct, the number of words correctly answered in a lesson. In the aggregate data, performance shows an overall decline; however, conditioned on distinct words (the total number of unique words shown in a lesson), performance shows more complex trends. The red values appearing initially",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24. In the aggregate data (Fig. 3(d)), there is a small but significant upward trend in performance over the course of a day. It looks like performance is higher at night than during the day. However, when data is disaggregated by all first attempts, only a couple of subgroups have the up-trend: the rest stay flat or even decline in performance. All first attempts, which represents how many of all problems users so",
        "Of the 462 potential disaggregations of DL data, 51 were found to be significant using the $\\chi ^ { 2 }$ test. Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc. The trends with respect to some of the covariates could be explained by several different disaggregations, with some of them having relatively high values of pseudo- $R ^ { 2 }$ . Again, user expe",
        "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24.",
        "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
        "Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig16.jpg",
      "image_filename": "1805.03094_page0_fig16.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig17.jpg",
      "image_filename": "1805.03094_page0_fig17.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig18.jpg",
      "image_filename": "1805.03094_page0_fig18.jpg",
      "caption": "(a) Disaggregated data",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig19.jpg",
      "image_filename": "1805.03094_page0_fig19.jpg",
      "caption": "(b) Number of samples (c) Subgroup trends (d) Aggregate trend",
      "context_before": "",
      "context_after": "along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance. After 20 new words are shown in a lesson, users can no longer answer all the words correctly. Also in-",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_24",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg",
      "image_filename": "1805.03094_page0_fig20.jpg",
      "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
      "context_before": "along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance. After 20 new words are shown in a lesson, users can no longer answer all the words correctly. Also in-",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 5 examines the impact of experience on performance. In the aggregate data Fig. 5(d), performance appears to increase as function of experience (lesson index): users who have more practice perform better. However, once the data is disaggregated by initial performance (five first lessons), or skill, in Fig. 5(c), a subtler picture emerges. Users who initially performed the worst (bottom bins in Fig. 5(a)) improve their performance as they have more lessons, while the best performers initial",
        "Figure 5 examines the impact of experience on performance.",
        "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
      "image_filename": "1805.03094_page0_fig21.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig22.jpg",
      "image_filename": "1805.03094_page0_fig22.jpg",
      "caption": "(a) Disaggregated data",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_27",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig23.jpg",
      "image_filename": "1805.03094_page0_fig23.jpg",
      "caption": "(b) Number of samples (c) Subgroup trends (d) Aggregate trend Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
      "context_before": "",
      "context_after": "teresting is a region of lower performance starting around values of lesson correct near 20 and distinct words between 3 and 10, and continues upwards and to the right. For some reason user performance drops in this regime.\n\nThere are several commonalities emerging from the three data sets we studied. Across platforms, initial performance, captured by first five attempts in the KA data or first five lessons in the DL data, appeared as an important conditioning variable differentiating the subgroups. Those users who were initially high performers appear to be different from the low performers, especially when looking at how their performance changes over time. While initial performance could capture skill or background knowledge, further analysis is needed to link it to this characteristic.\n\nExperience also appeared as an important feature differentiating users. As a proxy of experience we used such features as the number of lessons in DL data, user tenure in KA data, and number of answers and reputation in SE data. However, whether this variable reflects the benefits of practice, or simply captures user motivation, is not clear.",
      "referring_paragraphs": [
        "Another disaggregation of DL data is shown in Figure 6. The plots show performance as a function of lesson correct, the number of words correctly answered in a lesson. In the aggregate data, performance shows an overall decline; however, conditioned on distinct words (the total number of unique words shown in a lesson), performance shows more complex trends. The red values appearing initially",
        "Another disaggregation of DL data is shown in Figure 6.",
        "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03094_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1805.03677": [
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg",
      "image_filename": "1805.03677_page0_fig0.jpg",
      "caption": "Figure 1.​ Model Development Pipeline",
      "context_before": "2 http://datanutrition.media.mit.edu/demo.html\n\nData driven decision making systems play an increasingly important and impactful role in our lives. These frameworks are built on increasingly sophisticated artificial intelligence (AI) systems and are tuned by a growing population of data specialists to infer a vast diversity of outcomes: the song that plays next3 on your playlist, the type of advertisement you are most likely to see, or whether you qualify for a mortgage and at what rate [1]. These systems deliver untold societal and economic benefits, but they can also pose harm. Researchers continue to uncover troubling consequences of these systems [2,3].\n\nData is a fundamental ingredient in AI, and the quality of a dataset used to build a model will directly influence the outcomes it produces. Like the fruit of a poisoned tree, an AI model trained on problematic or missing data will likely produce problematic outcomes [4, 5]. Examples of these problems include gender bias in language translations surfaced through natural language processing [4], and skin shade bias in facial recognition systems due to non-representative data [5]. Typically the model development pipeline (Figure 1​) begins with a question or goal. Within the realm of supervised learning, for example, a data specialist will curate a labeled dataset of previous answers in response to the guiding question. Such data is then used to train a model to respond in a way that accurately correlates with past occurrences. In this way, past answers are used to forecast the future. This is particularly problematic when outcomes of past events are contaminated with (often unintentional) bias.",
      "context_after": "Models often come under scrutiny only after they are built, trained, and deployed. If a model is found to perpetuate a bias - for example, over-indexing for a particular race or gender - the data specialist returns to the development stage in order to identify and address the issue. This feedback loop is inefficient, costly, and does not always mitigate harm; the time and energy of the data specialist is a sunk cost, and if in use, the model may have already caused harm. Some of this harm could be avoided by undertaking thorough interrogation of data at the outset of model development. However, this is still not a widespread or standardized practice.\n\nWe conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem. Although many $( 4 7 \\% )$ respondents report conducting some form of data analysis prior to model development, most $( 7 4 \\% )$ indicate that their organizations do not have explicit best practices for\n\n[Section: INTRODUCTION]",
      "referring_paragraphs": [
        "Data is a fundamental ingredient in AI, and the quality of a dataset used to build a model will directly influence the outcomes it produces. Like the fruit of a poisoned tree, an AI model trained on problematic or missing data will likely produce problematic outcomes [4, 5]. Examples of these problems include gender bias in language translations surfaced through natural language processing [4], and skin shade bias in facial recognition systems due to non-representative data [5]. Typically the mo",
        "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets. This format also caters to a wide range of requirements and information available for a specific dataset. During label generation and subsequent updates, it also accommodates data specialists of different backgrounds and technical sk",
        "Modules (Table 1 & 2) range from the purely non-technical, such as the Metadata module, to the highly technical, such as the Probabilistic Computing module. Some modules require manual effort to generate, such as those that provide qualitative descriptions of the data (Metadata, Provenance, Variables), while others can ideally be the result of an automated process (Statistics, Pair Plots). Modules also vary in their subjectivity, especially where there exists a reliance on the Label author to id",
        "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "Supplement figure 1.​ Prototype Label demonstrating the metadata, provenance, and variables modules.",
        "Typically the model development pipeline (Figure 1​) begins with a question or goal.",
        "Figure 1.​ Model Development Pipeline\n\nModels often come under scrutiny only after they are built, trained, and deployed.",
        "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​).",
        "Figure 1​) provide as-is dataset information."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg",
      "image_filename": "1805.03677_page0_fig1.jpg",
      "caption": "such analysis. Fifty-nine percent of respondents reported relying primarily on experience and self-directed learning (through online tutorials, blogs, academic papers, stack overflow, and online data competitions) to inf",
      "context_before": "[Section: INTRODUCTION]\n\n3 The term “data specialist” is used instead of “data scientist” in the interest of using a term that is broadly scoped to include all professionals utilizing data in automated decision making systems: data scientists, analysts, machine learning engineers, model developers, artificial intelligence researchers, and a variety of others in this space.\n\nsuch analysis. Fifty-nine percent of respondents reported relying primarily on experience and self-directed learning (through online tutorials, blogs, academic papers, stack overflow, and online data competitions) to inform their data analysis methods and practices. This survey indicates that despite limited current standards, there is widespread interest to improve data analysis practices and make them accessible and standardized.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig2.jpg",
      "image_filename": "1805.03677_page0_fig2.jpg",
      "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
      "context_before": "",
      "context_after": "To improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability and fitness of datasets, and more easily find and use better quality data to train their models. As a proposed solution, we introduce the Dataset Nutrition Label, a diagnostic framework to address and mitigate some of these challenges by providing critical information to data specialists at the point of data analysis.\n\nThis study begins with a review of related work, drawing from the fields of nutrition and privacy, where labels are a useful mechanism to distill essential information and enable better decision-making and influence best practices. We then discuss the Dataset Nutrition Label prototype and our methodology, demonstration dataset, and key results. This is followed by an overview of the benefits of the tool, its potential limitations, and ways to mitigate those limitations. We briefly summarize some future directions, including research and public policy agendas that would further advance the goals of the Label. Lastly, we discuss implementation of the prototype and key takeaways.\n\n1.1 LABELS IN CONTEXT",
      "referring_paragraphs": [
        "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem. Although many $( 4 7 \\% )$ respondents report conducting some form of data analysis prior to model development, most $( 7 4 \\% )$ indicate that their organizations do not have explicit best practices for",
        "The label is envisioned as a digital object that can be both generated and viewed by web-based applications. The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​). Given a specific dataset, the label maker application allows users to select the desired modules and generate them. While the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the Pair Plo",
        "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "Supplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables into 4 groups: ordinal, nominal, continuous, and discrete.",
        "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.",
        "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data\n\nTo improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability and fitness of datasets, and more easily find and use better quality data to train their models.",
        "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   \n\n<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>",
        "While the generation of some modules is fully automated, some require human input (Table 2​).",
        "Figure 2​) starts to offer a glimpse into the dataset distributions.",
        "Ordinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td>number</td><td>500</td><td>4</td><td>100000000232 (...)</td><td>multiple detected</td><td>0%</td></tr><tr><td>date_of-payment</td><td>date</td><td>500</td><td>213 including mi...</td><td>missing value (27)</td><td>multiple detected</td><td>5.40%</td></tr><tr><td>general(transac...)</td><td>number</td><td>500</td><td>467 including mi...</td><td>missing value (34)</td><td>multiple detected</td><td>6.80%</td></tr><tr><td>program_year</td><td>number</td><td>500</td><td>2 including missi...</td><td>2014 (495)</td><td>missing value (5)</td><td>1.00%</td></tr></table>\n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>product_name</td><td>string</td><td>500</td><td>16 including mis...</td><td>Xarelto (200)</td><td>Aciphex (1)</td><td>3.20%</td></tr><tr><td>original_product...</td><td>string</td><td>500</td><td>15</td><td>Xarelto (212)</td><td>Aciphex (1)</td><td>0%</td></tr><tr><td>product_ndc</td><td>number</td><td>500</td><td>21 including mis...</td><td>5045857810 (201)</td><td>multiple detected</td><td>5.00%</td></tr><tr><td>product_is_drug</td><td>boolean</td><td>500</td><td>2 including miss...</td><td>t (492)</td><td>missing value (8)</td><td>1.60%</td></tr><tr><td>payment_has_m...</td><td>boolean</td><td>500</td><td>3 including miss...</td><td>f (267)</td><td>missing value (29)</td><td>5.80%</td></tr><tr><td>teaching_hospit...</td><td>number</td><td>500</td><td>2 including miss...</td><td>0 (464)</td><td>missing value (36)</td><td>7.20%</td></tr><tr><td>physician_profile...</td><td>number</td><td>500</td><td>230 including mi...</td><td>missing value (32)</td><td>multiple detected</td><td>6.40%</td></tr><tr><td>recipient_state</td><td>string</td><td>500</td><td>40</td><td>CA (56)</td><td>multiple detected</td><td>0%</td></tr><tr><td>applicable_man...</td><td>string</td><td>500</td><td>5 including miss...</td><td>Janssen Pharm...</td><td>multiple detected</td><td>7.00%</td></tr><tr><td>teaching_hospit...</td><td>number</td><td>500</td><td>2 including miss...</td><td>0 (481)</td><td>missing value (19)</td><td>3.80%</td></tr><tr><td>product_slug</td><td>string</td><td>500</td><td>15 including mis...</td><td>drug-xarelto (196)</td><td>drug-aciphex (1)</td><td>8.20%</td></tr></table>\n\nContinuous   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>min</td><td>median</td><td>max</td><td>mean</td><td>standardD...</td><td>missing</td><td>zeros</td></tr><tr><td>total_amo...</td><td>number</td><td>500</td><td>0.14</td><td>14.00</td><td>5000</td><td>134.21</td><td>501.99</td><td>9.40%</td><td>0%</td></tr></table>\n\nDiscrete   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>min</td><td>median</td><td>max</td><td>mean</td><td>standardD...</td><td>missing</td><td>zeros</td></tr><tr><td>number_o...</td><td>number</td><td>500</td><td>1</td><td>1.00</td><td>1</td><td>1.00</td><td>0.00</td><td>4.80%</td><td>0%</td></tr></table>\n\nSupplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables into 4 groups: ordinal, nominal, continuous, and discrete."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/19c066fc428ea874c5a58b80ccef1c6ff4f5434de74568c7e9175b48f508971d.jpg",
      "image_filename": "19c066fc428ea874c5a58b80ccef1c6ff4f5434de74568c7e9175b48f508971d.jpg",
      "caption": "Table 1.​ Table illustrating 7 modules of the Dataset Nutrition Label, together with their description, role, and contents.",
      "context_before": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets. This format also caters to a wide range of requirements and information available for a specific dataset. During label generation and subsequent updates, it also accommodates data specialists of different backgrounds and technical skill levels.\n\nModules (Table 1 & 2) range from the purely non-technical, such as the Metadata module, to the highly technical, such as the Probabilistic Computing module. Some modules require manual effort to generate, such as those that provide qualitative descriptions of the data (Metadata, Provenance, Variables), while others can ideally be the result of an automated process (Statistics, Pair Plots). Modules also vary in their subjectivity, especially where there exists a reliance on the Label author to identify which questions should be asked of the data and in what way (e.g. Probabilistic Computing). Many of the example modules are also interactive, highlighting a crucial benefit of a label living on a platform (such as a web page) that supports user interaction. This allows Label users to interrogate various dataset aspects with great flexibility and free of preconceived notions developed during Label generation. Lastly, some modules could be designed to act as proxies for their corresponding dataset as they do not expose the underlying data. This could be key when dealing with proprietary datasets, as much of this data will not or cannot be released to the public based on intellectual property or other constraints. Other modules expose information such as distribution metrics which, in theory, would allow adversaries to approximate the dataset contents. The choice of module(s) is thus based on the availability of information, level of willingness and effort volunteered to document the dataset, and privacy concerns.\n\n[Section: METHODS]",
      "context_after": "The list of modules currently examined in this study, while not exhaustive, provides a solid representation of the kinds of flexibility supported by the Label framework. Other modules considered for future iterations or additional datasets include but are not limited to: a comments section for users to interact with authors of the Label for feedback or other purposes; an extension of the Provenance section that includes the versioning history and change logs of the dataset and associated Labels over time, similar to Git; a privacy-focused module that indicates any sensitive information and whether the data was collected with consent; and finally, a usage tracking module that documents data utilization and references using some form of identifier, similar to the Digital Object Identifier [30] and associated citation systems in scientific publishing.",
      "referring_paragraphs": [
        "Data is a fundamental ingredient in AI, and the quality of a dataset used to build a model will directly influence the outcomes it produces. Like the fruit of a poisoned tree, an AI model trained on problematic or missing data will likely produce problematic outcomes [4, 5]. Examples of these problems include gender bias in language translations surfaced through natural language processing [4], and skin shade bias in facial recognition systems due to non-representative data [5]. Typically the mo",
        "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets. This format also caters to a wide range of requirements and information available for a specific dataset. During label generation and subsequent updates, it also accommodates data specialists of different backgrounds and technical sk",
        "Modules (Table 1 & 2) range from the purely non-technical, such as the Metadata module, to the highly technical, such as the Probabilistic Computing module. Some modules require manual effort to generate, such as those that provide qualitative descriptions of the data (Metadata, Provenance, Variables), while others can ideally be the result of an automated process (Statistics, Pair Plots). Modules also vary in their subjectivity, especially where there exists a reliance on the Label author to id",
        "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "Supplement figure 1.​ Prototype Label demonstrating the metadata, provenance, and variables modules.",
        "Typically the model development pipeline (Figure 1​) begins with a question or goal.",
        "Figure 1.​ Model Development Pipeline\n\nModels often come under scrutiny only after they are built, trained, and deployed.",
        "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​).",
        "Figure 1​) provide as-is dataset information."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/e41dba2f8756e3e2b2b7a74316f2c7e95adf3efdfa25f932084f96e69829d07c.jpg",
      "image_filename": "e41dba2f8756e3e2b2b7a74316f2c7e95adf3efdfa25f932084f96e69829d07c.jpg",
      "caption": "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label",
      "context_before": "The list of modules currently examined in this study, while not exhaustive, provides a solid representation of the kinds of flexibility supported by the Label framework. Other modules considered for future iterations or additional datasets include but are not limited to: a comments section for users to interact with authors of the Label for feedback or other purposes; an extension of the Provenance section that includes the versioning history and change logs of the dataset and associated Labels over time, similar to Git; a privacy-focused module that indicates any sensitive information and whether the data was collected with consent; and finally, a usage tracking module that documents data utilization and references using some form of identifier, similar to the Digital Object Identifier [30] and associated citation systems in scientific publishing.",
      "context_after": "2.2 WEB-BASED APPLICATION\n\nThe label is envisioned as a digital object that can be both generated and viewed by web-based applications. The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​). Given a specific dataset, the label maker application allows users to select the desired modules and generate them. While the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the Pair Plots module can be generated automatically from the dataset. The Label generator pre-populates as many fields as possible and alerts users to those requiring action. The Label itself lives in a .json format, as one that is human readable and well supported. The Label can then be viewed within the label viewer application where formating is carried out to achieve the desired user interface and user interaction effects. In terms of visual appearance and design, format and typeface requirements of the “Nutrition Facts” label [31] is used. These guidelines, such as the all black font color on white contrasting background, are optimized for clarity and conciseness. Design changes are anticipated in further iterations, and should be informed by user testing.",
      "referring_paragraphs": [
        "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem. Although many $( 4 7 \\% )$ respondents report conducting some form of data analysis prior to model development, most $( 7 4 \\% )$ indicate that their organizations do not have explicit best practices for",
        "The label is envisioned as a digital object that can be both generated and viewed by web-based applications. The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​). Given a specific dataset, the label maker application allows users to select the desired modules and generate them. While the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the Pair Plo",
        "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "Supplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables into 4 groups: ordinal, nominal, continuous, and discrete.",
        "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.",
        "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data\n\nTo improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability and fitness of datasets, and more easily find and use better quality data to train their models.",
        "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   \n\n<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>",
        "While the generation of some modules is fully automated, some require human input (Table 2​).",
        "Figure 2​) starts to offer a glimpse into the dataset distributions.",
        "Ordinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td>number</td><td>500</td><td>4</td><td>100000000232 (...)</td><td>multiple detected</td><td>0%</td></tr><tr><td>date_of-payment</td><td>date</td><td>500</td><td>213 including mi...</td><td>missing value (27)</td><td>multiple detected</td><td>5.40%</td></tr><tr><td>general(transac...)</td><td>number</td><td>500</td><td>467 including mi...</td><td>missing value (34)</td><td>multiple detected</td><td>6.80%</td></tr><tr><td>program_year</td><td>number</td><td>500</td><td>2 including missi...</td><td>2014 (495)</td><td>missing value (5)</td><td>1.00%</td></tr></table>\n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>product_name</td><td>string</td><td>500</td><td>16 including mis...</td><td>Xarelto (200)</td><td>Aciphex (1)</td><td>3.20%</td></tr><tr><td>original_product...</td><td>string</td><td>500</td><td>15</td><td>Xarelto (212)</td><td>Aciphex (1)</td><td>0%</td></tr><tr><td>product_ndc</td><td>number</td><td>500</td><td>21 including mis...</td><td>5045857810 (201)</td><td>multiple detected</td><td>5.00%</td></tr><tr><td>product_is_drug</td><td>boolean</td><td>500</td><td>2 including miss...</td><td>t (492)</td><td>missing value (8)</td><td>1.60%</td></tr><tr><td>payment_has_m...</td><td>boolean</td><td>500</td><td>3 including miss...</td><td>f (267)</td><td>missing value (29)</td><td>5.80%</td></tr><tr><td>teaching_hospit...</td><td>number</td><td>500</td><td>2 including miss...</td><td>0 (464)</td><td>missing value (36)</td><td>7.20%</td></tr><tr><td>physician_profile...</td><td>number</td><td>500</td><td>230 including mi...</td><td>missing value (32)</td><td>multiple detected</td><td>6.40%</td></tr><tr><td>recipient_state</td><td>string</td><td>500</td><td>40</td><td>CA (56)</td><td>multiple detected</td><td>0%</td></tr><tr><td>applicable_man...</td><td>string</td><td>500</td><td>5 including miss...</td><td>Janssen Pharm...</td><td>multiple detected</td><td>7.00%</td></tr><tr><td>teaching_hospit...</td><td>number</td><td>500</td><td>2 including miss...</td><td>0 (481)</td><td>missing value (19)</td><td>3.80%</td></tr><tr><td>product_slug</td><td>string</td><td>500</td><td>15 including mis...</td><td>drug-xarelto (196)</td><td>drug-aciphex (1)</td><td>8.20%</td></tr></table>\n\nContinuous   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>min</td><td>median</td><td>max</td><td>mean</td><td>standardD...</td><td>missing</td><td>zeros</td></tr><tr><td>total_amo...</td><td>number</td><td>500</td><td>0.14</td><td>14.00</td><td>5000</td><td>134.21</td><td>501.99</td><td>9.40%</td><td>0%</td></tr></table>\n\nDiscrete   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>min</td><td>median</td><td>max</td><td>mean</td><td>standardD...</td><td>missing</td><td>zeros</td></tr><tr><td>number_o...</td><td>number</td><td>500</td><td>1</td><td>1.00</td><td>1</td><td>1.00</td><td>0.00</td><td>4.80%</td><td>0%</td></tr></table>\n\nSupplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables into 4 groups: ordinal, nominal, continuous, and discrete."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg",
      "image_filename": "1805.03677_page0_fig3.jpg",
      "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.",
      "context_before": "2.2 WEB-BASED APPLICATION\n\nThe label is envisioned as a digital object that can be both generated and viewed by web-based applications. The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​). Given a specific dataset, the label maker application allows users to select the desired modules and generate them. While the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the Pair Plots module can be generated automatically from the dataset. The Label generator pre-populates as many fields as possible and alerts users to those requiring action. The Label itself lives in a .json format, as one that is human readable and well supported. The Label can then be viewed within the label viewer application where formating is carried out to achieve the desired user interface and user interaction effects. In terms of visual appearance and design, format and typeface requirements of the “Nutrition Facts” label [31] is used. These guidelines, such as the all black font color on white contrasting background, are optimized for clarity and conciseness. Design changes are anticipated in further iterations, and should be informed by user testing.",
      "context_after": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends with the ultimate aim of providing the Label authors with a diverse set of options, fueled by the plethora of tools developed by research groups for automating the generation of summaries, insights, and understandings of datasets. The Label thus becomes a medium for the continuous deployment and testing of these tools. A somewhat recent and particularly powerful example of this is probabilistic computing, and specifically, BayesDB [32], an open source platform developed by researchers at MIT. With minimal modeling and programming effort, BayesDB enables inference of a model that captures the structure underlying the data and generates statistical summaries based on such structure.\n\nTo test the concept generally and the modular framework specifically, we built a prototype with a dataset that included information about people and was maintained by an organization invested in better understanding the data. This combination of factors provides necessary information and access to build a wide variety of modules, including those that require full knowledge of the data and the ability to contact the organization that maintains the dataset. We were granted access to the “Dollars for Docs” database from ProPublica, an independent, nonprofit newsroom that produces investigative journalism in the public interest . The dataset, which contains payments to doctors and teaching hospitals from pharmaceutical and4 medical device companies over a two-year time period (August 2013 - December 2015), was originally released by the U.S. Centers for Medicare and Medicaid Services (CMS) and compiled by ProPublica into a single, comprehensive database.\n\n4 https://projects.propublica.org/docdollars/",
      "referring_paragraphs": [
        "The label is envisioned as a digital object that can be both generated and viewed by web-based applications. The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​). Given a specific dataset, the label maker application allows users to select the desired modules and generate them. While the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the Pair Plo",
        "The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​).",
        "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg",
      "image_filename": "1805.03677_page0_fig4.jpg",
      "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.",
      "context_before": "To test the concept generally and the modular framework specifically, we built a prototype with a dataset that included information about people and was maintained by an organization invested in better understanding the data. This combination of factors provides necessary information and access to build a wide variety of modules, including those that require full knowledge of the data and the ability to contact the organization that maintains the dataset. We were granted access to the “Dollars for Docs” database from ProPublica, an independent, nonprofit newsroom that produces investigative journalism in the public interest . The dataset, which contains payments to doctors and teaching hospitals from pharmaceutical and4 medical device companies over a two-year time period (August 2013 - December 2015), was originally released by the U.S. Centers for Medicare and Medicaid Services (CMS) and compiled by ProPublica into a single, comprehensive database.\n\n4 https://projects.propublica.org/docdollars/\n\nThe resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label authors as well as provide a standard format for both the generation and consumption of such data. The Statistics module (Supp. Figure 2​) starts to offer a glimpse into the dataset distributions. For instance, the skewness of a 500 row dataset subset towards a particular drug \"Xarelto\" can be quickly identified as the most frequent entry under the variable \"product_name\", and “Aciphex” as the least frequent entry. The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another. A specialist building a model predicting marketing spend in each state, for example, may choose to compare “recipient_state” and “total_amount_of_payment_usdollars,” and will observe that some states (CA, NY) are more highly correlated with spend. In this case, the specialist would probably normalize for population as the next step beyond consulting the Label in order to identify anomalous spending trends.",
      "context_after": "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend. Computed from an inferred generative model, this module allows for the full benefits of Bayesian analysis [33], such as interpretability of inferences, coping with missing data, and robustness to outliers and regions of sparse data. In this specific use case, an underrepresented drug is chosen from the dataset and the probability of this drug receiving a payment in different states is inferred. With the inevitable variation in the representation of different groups in datasets, such analyses are of great utility in extracting insights - even from relatively small sample sizes. A quick toggle indicates that the top few states for marketing spend are likely the same few states - with a few exceptions, including that NJ is likely to receive much more money for marketing activities relating to the drug Xarelto. Again, this information only acts as a flag for the “what”; specialists will ideally continue to investigate the data in order to identify the “why”.",
      "referring_paragraphs": [
        "The resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label",
        "The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another.",
        "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg",
      "image_filename": "1805.03677_page0_fig5.jpg",
      "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.",
      "context_before": "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend. Computed from an inferred generative model, this module allows for the full benefits of Bayesian analysis [33], such as interpretability of inferences, coping with missing data, and robustness to outliers and regions of sparse data. In this specific use case, an underrepresented drug is chosen from the dataset and the probability of this drug receiving a payment in different states is inferred. With the inevitable variation in the representation of different groups in datasets, such analyses are of great utility in extracting insights - even from relatively small sample sizes. A quick toggle indicates that the top few states for marketing spend are likely the same few states - with a few exceptions, including that NJ is likely to receive much more money for marketing activities relating to the drug Xarelto. Again, this information only acts as a flag for the “what”; specialists will ideally continue to investigate the data in order to identify the “why”.",
      "context_after": "It is unavoidable that datasets collected from the real-world have relationships to demographics that the data specialist or other entities do not wish to propagate into the learned model and the inferences produced from it. For example, is a variable or an aggregate of a variable strongly correlated with the Hispanic population in a given region? To surface relationships like this, it is often necessary to explicitly compute a comparison between the dataset and demographic “ground truth” data, which is a task that can be both time consuming and challenging. The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis. In order to surface any anomalies in the demographic distribution of these variables, we selected the 2010 US Census data as “ground truth” for zip code and race. The module then correlates zip code Census data with the dataset and calculates the Pearson correlation between demographics and field aggregates. To demonstrate its utility, the Label (Figure 6, top​) highlights the negative correlations between the (sum of the) amount of payment field and demographics. A second example (Figure 6, bottom​), highlights the positive correlation between a “spend_per_person” aggregate and demographics. This module demonstrates, in a straightforward way, specific anomalous relationships in the data that the data specialist should pay attention to during model training. In the prototype, we observe a slight positive\n\ncorrelation between white zip codes and payments, and a slight negative correlation between rural zip codes and payments. Toggling to per person spend underscores similar overall trends.",
      "referring_paragraphs": [
        "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend. Computed from an inferred generative model, this module allows for the full benefits of Bayesian analysis [33], such as interpretability of inferences, coping with missing data, and robustness to outliers and regions of sparse data. In this specific use case, an underrepresented drug is chosen from the dataset a",
        "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend.",
        "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_9",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg",
      "image_filename": "1805.03677_page0_fig6.jpg",
      "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.",
      "context_before": "It is unavoidable that datasets collected from the real-world have relationships to demographics that the data specialist or other entities do not wish to propagate into the learned model and the inferences produced from it. For example, is a variable or an aggregate of a variable strongly correlated with the Hispanic population in a given region? To surface relationships like this, it is often necessary to explicitly compute a comparison between the dataset and demographic “ground truth” data, which is a task that can be both time consuming and challenging. The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis. In order to surface any anomalies in the demographic distribution of these variables, we selected the 2010 US Census data as “ground truth” for zip code and race. The module then correlates zip code Census data with the dataset and calculates the Pearson correlation between demographics and field aggregates. To demonstrate its utility, the Label (Figure 6, top​) highlights the negative correlations between the (sum of the) amount of payment field and demographics. A second example (Figure 6, bottom​), highlights the positive correlation between a “spend_per_person” aggregate and demographics. This module demonstrates, in a straightforward way, specific anomalous relationships in the data that the data specialist should pay attention to during model training. In the prototype, we observe a slight positive\n\ncorrelation between white zip codes and payments, and a slight negative correlation between rural zip codes and payments. Toggling to per person spend underscores similar overall trends.",
      "context_after": "The Label offers many benefits. Overall, it prompts critical questions and interrogation in the preprocessing phase of model development. It also expedites decision making, which saves time in the overall model development phase without sacrificing the quality or thoroughness of the data interrogation itself, perhaps encouraging better practices at scale. These benefits apply across the spectrum of data specialists’ skill and experience, but are particularly useful for those new to the field or less attuned to concerns around bias and algorithmic accountability. First, the Label creates a pre-generated “floor” for basic data interrogation in the data selection phase. It also indicates key dataset attributes in a standardized format. This gives data specialists a distilled yet comprehensive overview of the “ingredients” of the dataset, which allows for a quick and effective comparison of multiple datasets before committing to one for further investigation. It also enables the data specialist to better understand and ascertain the fitness of a dataset by scanning missing values, summary statistics of the data, correlations\n\nor proxies, and other important factors. As a result, the data specialist may discard a problematic dataset or work to improve its viability prior to utilizing it.\n\nImproved dataset selection affords a secondary benefit: higher quality models. The Label provides data specialists improved means by which to interrogate the selected dataset during model development, previously a costly and onerous enterprise. The Ground Truth Correlation module, in particular, provides a helpful point of reference for the data specialist before model completion, and surfaces issues such as surprising variable correlations, missing data, anomalous data distributions, or other factors that could reinforce or perpetuate bias in the dataset. Addressing these factors in the model creation and training phase saves costs, time, and effort, and also could prevent bad outcomes early on, rather than addressing them after the fact.",
      "referring_paragraphs": [
        "It is unavoidable that datasets collected from the real-world have relationships to demographics that the data specialist or other entities do not wish to propagate into the learned model and the inferences produced from it. For example, is a variable or an aggregate of a variable strongly correlated with the Hispanic population in a given region? To surface relationships like this, it is often necessary to explicitly compute a comparison between the dataset and demographic “ground truth” data, ",
        "The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis.",
        "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/9cb872666b61d6cc1a6732b683abb16a97490c505e1ffc3710c0a9d74bc9c076.jpg",
      "image_filename": "9cb872666b61d6cc1a6732b683abb16a97490c505e1ffc3710c0a9d74bc9c076.jpg",
      "caption": "Metadata",
      "context_before": "We are grateful to the ProPublica team, including Celeste LeCompte, Ryann Jones, Scott Klein, and Hannah Fresques, for their generosity in providing the Dollars for Docs dataset and for their assistance throughout prototype development, and to the BayesDB team in the Probabilistic Computing Group at MIT, including Vikash Mansinghka, Sara Rendtorff-Smith, and Ulrich Schaechtle for their valuable work and ongoing advice and assistance. We are also thankful to Patrick Gage Kelley for bringing key work to our attention and for his constructive feedback, and to the 2018 Assembly Cohort and Advisory Board, in particular Matt Taylor, Jack Clark, Rachel Kalmar, Kathy Pham, James Mickens, Andy Ellis, and Nathan Freitas; the City of Boston Office of New Urban Mechanics; and Eric Breck and Mahima Pushkarna of Google Brain for productive and insightful discussions. This work was made possible by the Assembly program led by Jonathan Zittrain of the Berkman Klein Center for Internet & Society and Joi Ito of the MIT Media Lab.\n\n[Section: ACKNOWLEDGMENTS]\n\nProPublica'sDollars forDocsData",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/54bdfb0c1729a8f433c136a2495a542862023b7641aaaba8e71f4fd6d7b06ca6.jpg",
      "image_filename": "54bdfb0c1729a8f433c136a2495a542862023b7641aaaba8e71f4fd6d7b06ca6.jpg",
      "caption": "Provenance",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/de09df3ed3feaf6e88f616dbb9063d518cc175b311d9d06b2e348ee748b37b9c.jpg",
      "image_filename": "de09df3ed3feaf6e88f616dbb9063d518cc175b311d9d06b2e348ee748b37b9c.jpg",
      "caption": "Variables",
      "context_before": "",
      "context_after": "Supplement figure 1.​ Prototype Label demonstrating the metadata, provenance, and variables modules.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/48475df834565d7c1db6cd0bf4162e10c28e88bbbaf99f43f64f5ad71691826a.jpg",
      "image_filename": "48475df834565d7c1db6cd0bf4162e10c28e88bbbaf99f43f64f5ad71691826a.jpg",
      "caption": "Ordinal Nominal",
      "context_before": "Supplement figure 1.​ Prototype Label demonstrating the metadata, provenance, and variables modules.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/869c5629d812928056191b5d5322d8443c72b18f9aa671fb93f3de6a3201928a.jpg",
      "image_filename": "869c5629d812928056191b5d5322d8443c72b18f9aa671fb93f3de6a3201928a.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/916244d114a550bb0a89c068ae4456631e998c48360284e990b56d654a9f717f.jpg",
      "image_filename": "916244d114a550bb0a89c068ae4456631e998c48360284e990b56d654a9f717f.jpg",
      "caption": "Continuous",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/865830d0c3498c6140d5a7a6b40469004e2f5114e1d8e5036b1b8abb311d00fc.jpg",
      "image_filename": "865830d0c3498c6140d5a7a6b40469004e2f5114e1d8e5036b1b8abb311d00fc.jpg",
      "caption": "Discrete",
      "context_before": "",
      "context_after": "Supplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables into 4 groups: ordinal, nominal, continuous, and discrete.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.03677_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1805.05859": [
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg",
      "image_filename": "1805.05859_page0_fig0.jpg",
      "caption": "The corresponding graph is shown in Figure 1(a).",
      "context_before": "$$ A = \\lambda_ {a z} Z + U _ {A}, \\tag {6} $$\n\n$$ Y = \\lambda_ {y a} A + \\lambda_ {y z} Z + U _ {Y}. \\tag {7} $$\n\nThe corresponding graph is shown in Figure 1(a). Assuming that the background variables follow a standard Gaussian with diagonal covariance matrix, standard algebraic manipulations allows us to calculate that $P ( Y = y ~ \\vert ~ A = a )$ has a Gaussian density with a mean that depends on $\\lambda _ { a z } , \\lambda _ { y a }$ and $\\lambda _ { y z }$ . In contrast, $\\mathsf E [ Y \\mid d o ( A = a ) ] = \\lambda _ { y a } a$ , which can be obtained by first erasing (6) and replacing $A$ with $a$ on the right-hand side of (7) followed by marginalizing the",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.05859_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig1.jpg",
      "image_filename": "1805.05859_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.05859_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig2.jpg",
      "image_filename": "1805.05859_page0_fig2.jpg",
      "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .",
      "context_before": "",
      "context_after": "remaining variables. The difference illustrates the dictum “causation is not correlation”: $Z$ acts as a confounder (common cause) of exposure $A$ and outcome $Y$ . In a randomised controlled trial (RCT), $A$ is set by design, which breaks its link with $Z$ . In an observational study, data is generated by the system above, and standard measures of correlation between $A$ and $Y$ will not provide the correct interventional distribution: $P ( Y \\mid d o ( A = a ) )$ . The $d o ( \\cdot )$ operator captures the notion of effect of a cause, typically reported in terms of a contrast such as $\\mathsf { E } [ Y \\mid d o ( A = a ) ] - \\mathsf { E } [ Y \\mid d o ( A = a ^ { \\prime } ) ]$ for two different intervention levels $a , a ^ { \\prime }$ .\n\nAnother causal inference task is the computation of counterfactuals implied from causal assumptions and observations: informally, these are outcomes following from alternative interventions on the same unit. A “unit” is the snapshot of a system at a specific context, such as a person at a particular instant in time. Operationally, a unit can be understood as a particular instantiation of the background variable set $U$ , which determine all variables in $V$ except for those being intervened upon. Lower-case $u$ will be used to represent such realisations, with $U$ interpreted as a random unit. The name “counterfactual” comes from the understanding that, if the corresponding exposure already took place, then any such alternative outcomes would be (in general) contrary to the realised facts. Another commonly used term is potential outcomes [34], a terminology reflecting that strictly speaking such outcomes are not truly counterfactual until an exposure effectively takes place.\n\nFor any possible intervention $V _ { j } = v _ { j }$ for unit $u$ , we denote the counterfactual of $V _ { i }$ under this intervention as $V _ { i } ( v _ { j } , u )$ . This notation also accommodates the simultaneous hypothetical interventions on the corresponding set of background variables $U$ at level $u$ . The factual realisation of a random variable $V _ { i }$ for unit $u$ is still denoted by $V _ { i } ( u )$ . The random counterfactual corresponding to intervention $V _ { j } = v _ { j }$ for an unspecified unit $U$ is denoted as $V _ { i } ( v _ { j } , U )$ or, equivalently, $V _ { i } ( v _ { j } )$ for notational simplicity.",
      "referring_paragraphs": [
        "The corresponding graph is shown in Figure 1(a). Assuming that the background variables follow a standard Gaussian with diagonal covariance matrix, standard algebraic manipulations allows us to calculate that $P ( Y = y ~ \\vert ~ A = a )$ has a Gaussian density with a mean that depends on $\\lambda _ { a z } , \\lambda _ { y a }$ and $\\lambda _ { y z }$ . In contrast, $\\mathsf E [ Y \\mid d o ( A = a ) ] = \\lambda _ { y a } a$ , which can be obtained by first erasing (6) and replacing $A$ with $a$ ",
        "By treating $U$ as a set of random variables, this implies that factuals and counterfactuals have a joint distribution. One way of understanding it is via Figure 1(b), which represents a factual world and two parallel worlds where $A$",
        "is set to intervention levels $a$ and $a ^ { \\prime }$ . A joint distribution for $Y ( a )$ and $Y ( a ^ { \\prime } )$ is implied by the model. Conditional distributions, such as $P ( Y ( a ) = y _ { a } , Y ( a ^ { \\prime } ) =$ $y _ { a ^ { \\prime } } \\mid A = a , Y = y , Z = z )$ are also defined. Figure 1(c) shows the case for interventions on $Y$ . It is not difficult to show, as $Y$ is not an ancestor of $A$ in the graph, that $A ( y , u ) = A ( y ^ { \\prime } , u ) = A ( u )$ for all $u ,",
        "The corresponding graph is shown in Figure 1(a).",
        "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.05859_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg",
      "image_filename": "1805.05859_page0_fig3.jpg",
      "caption": "Our own suggestion for path-specific counterfactual fairness builds directly on the original: just extract latent fair variables from observed variables that are known to be (path-specifically) fair and build a black-box",
      "context_before": "is the previously discussed case study of gender bias in the admissions to the University of California at Berkeley in the 1970s: gender ( $A$ ) and admission ( $Y$ ) were found to be associated in the data, which lead to questions about fairness of the admission process. One explanation found was that this was due to the choice of department each individual was applying to ( $X$ ). By postulating the causal structure $A X Y$ , we could claim that, even though $A$ is a cause of $Y$ , the mechanism by which it changes $Y$ is “fair” in the sense that we assume free-will in the choice of department made by each applicant. This is of course a judgement call that leaves unexplained why there is an interaction between $A$ and other causes of $X$ , but one that many analysts would agree with. The problem gets more complicated if edge $A Y$ is also present.\n\nThe approach by [28] can tap directly from existing methods for deriving path-specific effects as functions of $P ( V )$ (see [36] for a review). The method by [21] and the recent variation of counterfactual fairness by [7] consist of adding “corrections” to a causal model to deactivate particular causal contributions to $Y$ . This is done by defining a particular fairness difference we want to minimise, typically the expected difference of the outcomes $Y$ under different levels of $A$ . [7] suggest doing this without parameterising a family of $\\hat { Y }$ to be optimised. At its most basic formulation, if we define $P S E ( V )$ as the particular path-specific effect from $A$ to $Y$ for a particular set of observed variables $V$ , by taking expectations under $Y$ we have that a fair $\\hat { Y }$ can be simply defined as $\\mathsf { E } [ Y \\mid V \\backslash \\{ Y \\} ] - P S E ( V )$ . Like in [28], it is however not obvious which optimally is being achieved for the prediction of $Y$ as it is unclear how such a transformation would translate in terms of projecting $Y$ into a constrained space when using a particular loss function.\n\nOur own suggestion for path-specific counterfactual fairness builds directly on the original: just extract latent fair variables from observed variables that are known to be (path-specifically) fair and build a black-box predictor around them. For interpretation, it is easier to include $\\hat { Y }$ in the causal graph (removing $Y$ , which plays no role as an input to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ . Assume that, similar to the Berkeley case, we forbid path $A $ $X _ { 1 } { \\hat { Y } }$ as an unfair contribution to $\\hat { Y }$ , while allowing contributions via $X _ { 2 }$ (that is, paths $A X _ { 2 } { \\hat { Y } }$ and $A \\to X _ { 2 } \\to X _ { 1 } \\to { \\hat { Y } }$ . This generalises the Berkeley example, where $X _ { 2 }$ would correspond to department choice and $X _ { 1 }$ to, say, some source of funding that for some reason is also affected by the gender of the applicant). Moreover, we also want to exclude the direct contribution $A { \\hat { Y } }$ . Assuming that a unit would be set to a factual baseline value $a$ for $A$ , the “unfair propagation” of a counterfactual value $a ^ { \\prime }$ of $A$ could be understood as passing it only through $A \\to X _ { 1 } \\to { \\hat { Y } }$ and $A { \\hat { Y } }$ in Figure 2(b), leaving the inputs “through the other edges” at the baseline [30, 36]. The relevant counterfactual for $\\hat { Y }$ is the nested counterfactual $\\hat { Y } ( a ^ { \\prime } , X _ { 1 } ( a ^ { \\prime } , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) )$ . A direct extension of the definition of counterfactual fairness applies to this",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.05859_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig4.jpg",
      "image_filename": "1805.05859_page0_fig4.jpg",
      "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
      "context_before": "",
      "context_after": "path-specific scenario: we require\n\n$$ \\begin{array}{l} P (\\hat {Y} (a ^ {\\prime}, X _ {1} (a ^ {\\prime}, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a) = \\\\ P (\\hat {Y} / \\left. \\left(\\mathbf {X} _ {1} - \\mathbf {X} _ {2} (\\cdot)\\right), \\mathbf {X} _ {2} (\\cdot)\\right) \\mid \\mathbf {X} _ {1} = \\mathbf {X} _ {2} = A, \\end{array} \\tag {9} $$\n\n$$ P (\\tilde {Y} (a, X _ {1} (a, X _ {2} (a)), X _ {2} (a)) \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, A = a). $$",
      "referring_paragraphs": [
        "Our own suggestion for path-specific counterfactual fairness builds directly on the original: just extract latent fair variables from observed variables that are known to be (path-specifically) fair and build a black-box predictor around them. For interpretation, it is easier to include $\\hat { Y }$ in the causal graph (removing $Y$ , which plays no role as an input to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A , ",
        "Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ .",
        "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.\n\npath-specific scenario: we require"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.05859_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1805.09458": [
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/aba42000b58fdd8adc6fce49c8ac8c735643ba43d4f3dab0fce4eaafb7cc2612.jpg",
      "image_filename": "aba42000b58fdd8adc6fce49c8ac8c735643ba43d4f3dab0fce4eaafb7cc2612.jpg",
      "caption": "3In some papers the protected factor for the Adult dataset is reported as Age, but those papers also reference Zemel et al. [23] as the processing and experimental scheme, which specifies Gender.",
      "context_before": "The first dataset is the German dataset, containing 1000 samples of personal financial data. The objective is to predict whether a person has a good credit score, and the protected class is Age (which, as per [23], is binarized). The second dataset is the Adult dataset, containing 45,222 data points of US census data. The objective is to predict whether or not a person has over 50,000 dollars saved in the bank. The protected factor for the Adult dataset is Gender3.\n\nWherever possible we use architectural constraints from previous papers. All encoders and decoders are single layer, as specified by Louizos et al. [15] (including those in the baselines), and for both datasets we use 64 hidden units in our method as in Xie et al., while for VFAE we use their described architecture. We use a latent space of 30 dimensions for each case. We train using Adam using the same hyperparameter settings as in Xie et al., and a batch size of 128. Optimization and parameter tuning is done via a held-out validation set.\n\n3In some papers the protected factor for the Adult dataset is reported as Age, but those papers also reference Zemel et al. [23] as the processing and experimental scheme, which specifies Gender.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/00ee247f91e86e2dbf3b129b788a2a17951ae1fca99a0d37a6f9e5cb5d8df0b0.jpg",
      "image_filename": "00ee247f91e86e2dbf3b129b788a2a17951ae1fca99a0d37a6f9e5cb5d8df0b0.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg",
      "image_filename": "1805.09458_page0_fig0.jpg",
      "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have",
        "For the larger Adult dataset shown on the bottom table of Figure 1, all three methods again have comparable predictive accuracy. However, against stronger adversaries each baseline has very high loss. Our proposed method has comparable accuracy with the VFAE, while providing the best adversarial error across all four adversarial difficulty levels.",
        "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets.",
        "For the larger Adult dataset shown on the bottom table of Figure 1, all three methods again have comparable predictive accuracy."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg",
      "image_filename": "1805.09458_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig2.jpg",
      "image_filename": "1805.09458_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig3.jpg",
      "image_filename": "1805.09458_page0_fig3.jpg",
      "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.",
      "context_before": "",
      "context_after": "For each tested method we train a discriminator to predict $c$ from generated latent codes $z$ . These discriminators are trained independently from the encoder/decoder/within-method adversaries. We use the architecture from Xie et al. [22] for these post-hoc adversaries, which describes a three-layer feed-forward network trained using batch normalization and Adam (using $\\gamma = 1$ and a learning rate of 0.001), with 64 hidden units per layer, using absolute error. We generalize this to four adversaries, increasing in the number of hidden layers. Each discriminator is trained post-hoc for each model, even in cases with a discriminator in the model (e.g. the model proposed by Xie et al. [22]).\n\n3.2 Unsupervised Learning\n\nWe demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written. This allows us to manipulate the decoder at test time to produce different artificial digits based on the style of one digit. We use 2 hidden layers with 512 nodes for both the encoder and the decoder.",
      "referring_paragraphs": [
        "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "image_filename": "1805.09458_page0_fig4.jpg",
      "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "context_before": "3.2 Unsupervised Learning\n\nWe demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written. This allows us to manipulate the decoder at test time to produce different artificial digits based on the style of one digit. We use 2 hidden layers with 512 nodes for both the encoder and the decoder.\n\nFor the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have",
      "context_after": "competitive adversarial loss. In general however, the smaller dataset does not differentiate the methods.\n\nFor the larger Adult dataset shown on the bottom table of Figure 1, all three methods again have comparable predictive accuracy. However, against stronger adversaries each baseline has very high loss. Our proposed method has comparable accuracy with the VFAE, while providing the best adversarial error across all four adversarial difficulty levels.\n\nWe further visualized a projection of the latent codes $z$ using t-SNE [17]; invariant representations should produce inseparable embeddings for each class. All methods have large red-only regions; this is somewhat expected for the majority class. However, both baseline methods have blue-only regions, while the proposed method has only a heterogenous region4.",
      "referring_paragraphs": [
        "Figure 3 demonstrates our ability to manipulate the conditional decoder. The left column contain the actual images (randomly selected from the test set), while the right columns contain images generated using the decoder. Particularly notable are the transfer of azimuth and thickness, and the failure of some styles to transfer to some digits (usually curved to straight digits or vice versa).",
        "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.09458_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1805.11202": [
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg",
      "image_filename": "1805.11202_page0_fig0.jpg",
      "caption": "Figure 1: Illustration of generative adversarial networks",
      "context_before": "disc η P η s P η sThe classification fairness on a dataset is achieved if both the disparate treatment and disparate impact are removed from the data. To remove the disparate treatment, the classifier cannot use the protected attribute to make decisions. As for the disparate impact, research in [9] proposed the concept of $\\epsilon$ -fairness to examine the potential disparate impact.\n\nDefinition 3 $\\epsilon$ -fairness [9]). A labeled dataset $\\mathcal { D } = ( \\boldsymbol { \\chi } , \\boldsymbol { y } , \\boldsymbol { s } )$ is said to be $\\epsilon$ ϵ-fair if for any classification algorithm $f : X \\to S$\n\n$$ B E R (f (\\mathcal {X}), \\mathcal {S}) > \\epsilon $$",
      "context_after": "with empirical probabilities estimated from $\\mathcal { D }$ , where (balanced error rate) is defined as\n\n$$ B E R (f (\\mathcal {X}), \\mathcal {S}) = \\frac {P [ f (\\mathcal {X}) = 0 | \\mathcal {S} = 1 ] + P [ f (\\mathcal {X}) = 1 | \\mathcal {S} = 0 ]}{2}. $$\n\nindicates the average class-conditioned error of $f$ on distribu-BERtion $\\mathcal { D }$ over the pair $( \\chi , s )$ .",
      "referring_paragraphs": [
        "Figure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .",
        "Fairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models on fair data generation. Table 1",
        "Figure 1: Illustration of generative adversarial networks\n\nwith empirical probabilities estimated from $\\mathcal { D }$ , where  (balanced error rate) is defined as",
        "Figure 1 illustrates the structure of GAN.",
        "Table 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>\n\nshows the risk differences in the real and synthetic datasets."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg",
      "image_filename": "1805.11202_page0_fig1.jpg",
      "caption": "Figure 2: The Structure of FairGAN",
      "context_before": "$$ V (G, D) = \\mathbb {E} _ {\\mathbf {x} \\sim P _ {\\mathrm {d a t a}}} [ \\log D (\\mathbf {x}) ] + \\mathbb {E} _ {\\mathbf {z} \\sim P _ {\\mathbf {z}}} [ \\log (1 - D (G (\\mathbf {z}))) ]. \\tag {3} $$\n\nFigure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .\n\nPG PGAN for discrete data generation. The generator of a regular GAN cannot generate discrete samples because $G$ is trained by the loss from $D$ Gvia backpropagation [10]. In order to tackle this Dlimitation, medGAN incorporates an autoencoder model in a regular GAN model to generate high-dimensional discrete variables [5].",
      "context_after": "Autoencoder is a feedforward neural network used for unsupervised learning. A basic autoencoder consists of an encoder and Enca decoder . Both the encoder and decoder are multilayer neural networks. Given an input $\\mathbf { x } \\in \\mathbb { R } ^ { n }$ , the encoder computes the hidden representation of an input $E n c ( \\mathbf { x } ) \\in \\mathbb { R } ^ { h }$ , and the decoder computes the reconstructed input $D e c ( E n c ( \\mathbf { x } ) ) \\in \\mathbb { R } ^ { n }$ based on the hidden rep-Dec Encresentation. To train the autoencoder model, the objective function of the autoencoder is to make the reconstructed input close to the original input:\n\n$$ \\mathcal {L} _ {A E} = \\left\\| \\mathbf {x} ^ {\\prime} - \\mathbf {x} \\right\\| _ {2} ^ {2}, \\tag {4} $$\n\nwhere $\\mathbf { x } ^ { \\prime } = D e c ( E n c ( \\mathbf { x } ) )$ . Because the hidden representation can Dec Encbe used to reconstruct the original input, it captures the salient information of the input.",
      "referring_paragraphs": [
        "FairGAN consists of one generator $G _ { D e c }$ and two discriminators $D _ { 1 }$ and $D _ { 2 }$ GDec. We adopt the revised generator from medGAN [5] to D Dgenerate both discrete and continuous data. Figure 2 shows the structure of FairGAN. In FairGAN, every generated sample has a corresponding value of the protected attribute $s \\sim P _ { \\mathrm { d a t a } } ( s )$ . The generator $G _ { D e c }$ generates a fake pair $( \\hat { \\mathbf { x } } , \\hat { y } )$ s P sfollowing the condi-Dec",
        "In Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s. The Euclidean distance is calculated between the estimated probability vectors (probability mass function) on the sample space from the synthetic dataset and the real dataset. A smaller distance indicates better clo",
        "Figure 2: The Structure of FairGAN\n\nAutoencoder is a feedforward neural network used for unsupervised learning.",
        "Figure 2 shows the structure of FairGAN.",
        "In Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig2.jpg",
      "image_filename": "1805.11202_page0_fig2.jpg",
      "caption": "4.4 NaïveFairGAN models",
      "context_before": "Proof. Given $D _ { 1 } ^ { * }$ and $D _ { 2 } ^ { * }$ , we reformulate Equation 6 as:\n\n$$ \\begin{array}{l} C (G _ {D e c}) \\\\ = \\max _ {D _ {1}, D _ {2}} V (G _ {D e c}, D _ {1}, D _ {2}) \\\\ = \\mathbb {E} _ {(\\mathbf {x}, y, s) \\sim P _ {\\mathrm {d a t a}} (\\mathbf {x}, y, s)} [ \\log \\frac {P _ {\\mathrm {d a t a}} (\\mathbf {x} , y , s)}{P _ {\\mathrm {d a t a}} (\\mathbf {x} , y , s) + P _ {G} (\\mathbf {x} , y , s)} ] \\\\ + \\mathbb {E} _ {(\\mathbf {x}, y, s) \\sim P _ {G} (\\mathbf {x}, y, s)} [ \\log \\frac {P _ {G} (\\mathbf {x} , y , s)}{P _ {\\mathrm {d a t a}} (\\mathbf {x} , y , s) + P _ {G} (\\mathbf {x} , y , s)} ] \\tag {10} \\\\ + \\lambda \\mathbb {E} _ {(\\mathbf {x}, y) \\sim P _ {G} (\\mathbf {x}, y | s = 1)} [ \\log \\frac {P _ {G} (\\mathbf {x} , y | s = 1)}{P _ {G} (\\mathbf {x} , y | s = 1) + P _ {G} (\\mathbf {x} , y | s = 0)} ] \\\\ + \\lambda \\mathbb {E} _ {(\\mathbf {x}, y) \\sim P _ {G} (\\mathbf {x}, y | s = 0)} [ \\log \\frac {P _ {G} (\\mathbf {x} , y | s = 0)}{P _ {G} (\\mathbf {x} , y | s = 1) + P _ {G} (\\mathbf {x} , y | s = 0)} ] \\\\ = - (2 + \\lambda) \\log 4 + 2 \\cdot J S D \\left(P _ {\\text {d a t a}} (\\mathbf {x}, y, s) | | P _ {G} (\\mathbf {x}, y, s)\\right) \\\\ + 2 \\lambda \\cdot J S D (P _ {G} (\\mathbf {x}, y | s = 1) | | P _ {G} (\\mathbf {x}, y | s = 0)), \\\\ \\end{array} $$\n\nλ JSD PG , y s PG ,While the two pairs of distributions $\\bar { P } _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s ) = P _ { G } ( \\hat { \\mathbf { x } } , \\hat { y } , s )$ and $P _ { G } ( \\hat { \\mathbf { x } } , \\hat { y } | s = 1 ) = P _ { G } ( \\hat { \\mathbf { x } } , \\hat { y } | s = 0 )$ P ,y, s PG ,y, s cannot be achieved simultaneously PG ,y s PG ,y sin Equation 10, the two JSD terms can still be converged to a global optimal point due to convexity of JSD. Hence, the optimum value of $V ( G _ { D e c } , D _ { 1 } , D _ { 2 } ) = - ( 2 + \\lambda ) \\log 4 + \\Delta$ , where $\\Delta$ is the minimum V GDec , D , D λ ∆ ∆value when the two JSD terms are converged to a point.",
      "context_after": "4.4 NaïveFairGAN models\n\nIn this subsection, we discuss two naive approaches which can only achieve fair data generation (disparate treatment) but cannot achieve fair classification (disparate impact).\n\nTo mitigate the disparate treatment, a straightforward approach is to remove $s$ from the dataset. Hence, if a GAN model ensures the generated samples have the same distribution as the real data with unprotected attributes and decision, i.e., $P _ { G } ( \\mathbf { x } , y ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ , and Grandomly assigns the values of protected attribute with only preserving the ratio of protected group to unprotected group the same as the real data, the completely generated dataset could achieve",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig3.jpg",
      "image_filename": "1805.11202_page0_fig3.jpg",
      "caption": "(a) Toy Dataset",
      "context_before": "Following the similar theoretical analysis as FairGAN, the global minimum of NaïveFairGAN-II $V ( G _ { D e c } , D _ { 1 } , D _ { 2 } )$ is achieved if and only if $P _ { G } ( \\mathbf { x } , y | s = 1 ) \\ = \\ P _ { G } ( \\mathbf { x } , y | s = 0 ) \\ = \\ ( P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 ) \\ + $ $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s ~ = ~ 0 ) \\big ) / 2$ with $D _ { 1 } ^ { * } ~ = ~ \\frac { 1 } { 2 }$ and $D _ { 2 } ^ { * } ~ = ~ \\frac { 1 } { 2 }$ . At that point, $V ( G _ { D e c } , D _ { 1 } , D _ { 2 } )$ Dachieves the value $- 4 \\log 4$ . The detailed proof V GDec , D , Dis described in the appendix.\n\nAlthough NaïveFairGAN-II considers the protected attribute, it achieves $P _ { G } ( \\mathbf { x } , y | s = 1 ) = P _ { G } ( \\mathbf { x } , y | s = 0 ) = \\left( P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 ) + \\right.$ $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 0 ) ) / 2$ s PG ,y without ensuring $P _ { G } ( \\mathbf { x } , y | s = 1 ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s =$ 1) or $P _ { G } ( \\mathbf { x } , y | s = 0 ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 0 )$ . Hence, NaïveFairGAN-II PG ,y s P ,y sachieves the fair data generation by randomly shuffling the protected attribute $\\hat { s }$ . Similar to the data from NaïveFairGAN-I, the generated data from NaïveFairGAN-II also incurs disparate impact because the correlation between generated unprotected attributes $\\hat { X }$ and the real protected attribute $s$ is not removed.\n\nFairGAN vs. NaïveFairGAN-I vs. NaïveFairGAN-II",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
      "image_filename": "1805.11202_page0_fig4.jpg",
      "caption": "(b) NaïveFairGAN-I",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig5.jpg",
      "image_filename": "1805.11202_page0_fig5.jpg",
      "caption": "(c) NaïveFairGAN-II",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig6.jpg",
      "image_filename": "1805.11202_page0_fig6.jpg",
      "caption": "(d) FairGAN Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
      "context_before": "",
      "context_after": "We compare FairGAN with NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset which consists of one unprotected attribute $x \\in \\mathbb { R }$ and one protected attribute $s \\in \\{ 0 , 1 \\}$ . The toy dataset is drawn from $x \\sim 0 . 5 * N ( 1 , 0 . 5 ) + 0 . 5 * N ( 3 , 0 . 5 )$ , where $P _ { \\mathrm { d a t a } } ( x | s = 1 ) = N ( 1 , 0 . 5 )$ xand $P _ { \\mathrm { d a t a } } ( x | s = 0 ) = N ( 3 , 0 . 5 )$ . P x s , .. Hence, the unprotected attribute $x$ P x s , .is strong correlated with the protected attribute .\n\nsWe train FairGAN and NaïveFairGAN models to approximate the distribution of $P _ { \\mathrm { d a t a } } ( x )$ . Figure 3 shows the data probability $P ( x )$ P xand two conditional probabilities $P ( x | s = 1 )$ and $P ( x | s = 0 )$ P x of the P x s P x stoy dataset (shown in Figure 3a) and synthetic datasets (Figures 3b to 3d) from FairGAN and NaïveFairGAN models.\n\nFor NaïveFairGAN-I, it is a regular GAN model which aims to make $P _ { G } ( x ) = P _ { \\mathrm { d a t a } } ( x )$ while is independently generated. There-PG x P x sfore, in this toy example, as shown in Figure 3b, we can observe that $P _ { G } ( x )$ is similar to $P _ { \\mathrm { d a t a } } ( x )$ . Meanwhile, because $s$ is independently PG x P x sassigned instead of generated from the GAN model, $P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ G are almost identical to each other, which avoids PG x sdisparate treatment. However, due to the high similarity between $P _ { G } ( x )$ and $P _ { \\mathrm { d a t a } } ( x )$ , given the generated $\\hat { x }$ and the real , the poten-Gtial disparate impact isn’t mitigated.",
      "referring_paragraphs": [
        "sWe train FairGAN and NaïveFairGAN models to approximate the distribution of $P _ { \\mathrm { d a t a } } ( x )$ . Figure 3 shows the data probability $P ( x )$ P xand two conditional probabilities $P ( x | s = 1 )$ and $P ( x | s = 0 )$ P x of the P x s P x stoy dataset (shown in Figure 3a) and synthetic datasets (Figures 3b to 3d) from FairGAN and NaïveFairGAN models.",
        "Fairness. We adopt the risk difference in a classifier $( d i s c ( \\eta ) =$ $P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( \\eta ( \\mathbf { x } ) = 1 | s = 0 ) \\}$ disc η) to evaluate the performance P η s P η sof classifier on fair prediction. Table 3 shows the risk differences in classifiers on various training and testing settings. We can observe that when the classifiers are trained and tested on real datasets (i.e., REAL2REAL), the risk differences in classifiers are high. It indicates t",
        "Classification accuracy. Table 3 further shows the classification accuracies of different classifiers on various training and testing settings. We can observe that the accuracies of classifiers on the SYN2REAL setting are close to the results on the REAL2REAL setting. It indicates synthetic datasets generated by different GAN models are similar to the real dataset, showing the good data generation utility of GAN models. Meanwhile, accuracies of classifiers which are trained on SYN4-FairGAN and t",
        "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset.",
        "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called\n\nTable 3: Risk differences in classifiers and classification accuracies on various training and testing settings   \n\n<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Classifier</td><td>REAL2REAL</td><td colspan=\"4\">SYN2SYN</td><td colspan=\"4\">SYN2REAL</td></tr><tr><td></td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td rowspan=\"3\">Risk Difference</td><td>SVM (Linear)</td><td>0.1784</td><td>0.1341±0.0023</td><td>0.0018±0.0021</td><td>0.0073±0.0039</td><td>0.0371±0.0189</td><td>0.1712±0.0062</td><td>0.1580±0.0076</td><td>0.1579±0.0079</td><td>0.0461±0.0424</td></tr><tr><td>SVM (RBF)</td><td>0.1788</td><td>0.1292±0.0049</td><td>0.0018±0.0025</td><td>0.0074±0.0028</td><td>0.0354±0.0206</td><td>0.1623±0.0050</td><td>0.1602±0.0053</td><td>0.1603±0.0087</td><td>0.0526±0.0353</td></tr><tr><td>Decision Tree</td><td>0.1547</td><td>0.1396±0.0089</td><td>0.0015±0.0035</td><td>0.0115±0.0061</td><td>0.0535±0.0209</td><td>0.1640±0.0077</td><td>0.1506±0.0070</td><td>0.1588±0.0264</td><td>0.0754±0.0641</td></tr><tr><td rowspan=\"3\">Accuracy</td><td>SVM (Linear)</td><td>0.8469</td><td>0.8281±0.0103</td><td>0.8162±0.0133</td><td>0.8226±0.0126</td><td>0.8247±0.0115</td><td>0.8363±0.0108</td><td>0.8340±0.0091</td><td>0.8356±0.0018</td><td>0.8217±0.0093</td></tr><tr><td>SVM (RBF)</td><td>0.8433</td><td>0.8278±0.0099</td><td>0.8160±0.0100</td><td>0.8215±0.0130</td><td>0.8233±0.0103</td><td>0.8342±0.0036</td><td>0.8337±0.0060</td><td>0.8349±0.0012</td><td>0.8178±0.0128</td></tr><tr><td>Decision Tree</td><td>0.8240</td><td>0.8091±0.0059</td><td>0.7926±0.0083</td><td>0.8055±0.0102</td><td>0.8077±0.0144</td><td>0.8190±0.0051</td><td>0.8199±0.0041</td><td>0.8158±0.0069</td><td>0.8044±0.0140</td></tr></table>\n\nSYN2SYN; 3) the classifiers are trained on the synthetic datasets and tested on the real dataset, called SYN2REAL."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig7.jpg",
      "image_filename": "1805.11202_page0_fig7.jpg",
      "caption": "(a) Real Dataset",
      "context_before": "We evaluate the performance of FairGAN on fair data generation and fair classification.\n\n5.1 Experimental Setup\n\nBaselines. To evaluate the effectiveness of FairGAN, we compare the performance of FairGAN with the regular GAN model and two NaïveFairGAN models. GAN aims to generate the synthetic samples that have the same distribution as the real data, i.e., $P _ { G } ( \\mathbf { x } , y , s ) ~ = ~ P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s )$ . The regular GAN model cannot Gachieve fair data generation. We adopt GAN as a baseline to evaluate the utility of data generation. NaïveFairGAN models include NaïveFairGAN-I and NaïveFairGAN-II.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig8.jpg",
      "image_filename": "1805.11202_page0_fig8.jpg",
      "caption": "(b) SYN1-GAN",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg",
      "image_filename": "1805.11202_page0_fig9.jpg",
      "caption": "(c) SYN2-NFGANI",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig10.jpg",
      "image_filename": "1805.11202_page0_fig10.jpg",
      "caption": "(d) SYN3-NFGANII",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_12",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig11.jpg",
      "image_filename": "1805.11202_page0_fig11.jpg",
      "caption": "(e) SYN4-FairGAN Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
      "context_before": "",
      "context_after": "In this paper, we don’t compare with the pre-process methods, because the classical methods like Massaging cannot remove disparate treatment and disparate impact [14]. Although the certifying framework proposed algorithms to remove disparate impact, they only work on numerical attributes [9].\n\nDatasets. We evaluate FairGAN and baselines on the UCI Adult income dataset which contains 48,842 instances [6]. The decision indicates whether the income is higher than $\\$ 50 k$ per year, and the protected attribute is gender. Each instance in the dataset consists of 14 attributes. We convert each attribute to a one-hot vector and combine all of them to a feature vector with 57 dimensions.\n\nIn our experiments, besides adopting the original Adult dataset, we also generate four types of synthetic data, SYN1-GAN that is generated by a regular GAN model, SYN2-NFGANI that is generated by NaïveFairGAN-I, SYN3-NFGANII that is generated by NaïveFairGAN-II, and SYN4-FairGAN that is generated by Fair-GAN with $\\lambda = 1$ . For each type of synthetic data, we generate five λdatasets to evaluate the data fairness and classification fairness. We then report the mean and stand deviation of evaluation results. The sizes of the synthetic datasets are same as the real dataset.",
      "referring_paragraphs": [
        "In Figure 4, we compare the dimension-wise conditional probability distributions between $P ( \\mathbf { x } , y | s = 1 )$ and $P ( \\mathbf { x } , y | s = 0 )$ . Each P ,y s P ,y sdot indicates one attribute. The diagonal line indicates the ideal fairness, where the conditional probability distributions of each attribute given $s = 1$ and $s = 0$ are identical. We can observe that the dimension-wise distributions of datasets with lower risk differences are closer to the diagonal line. For examp",
        "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs.",
        "In Figure 4, we compare the dimension-wise conditional probability distributions between $P ( \\mathbf { x } , y | s = 1 )$ and $P ( \\mathbf { x } , y | s = 0 )$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_13",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/e04a8e814aa1d5583860a4f45bddac8907e0f98d4d9d4462ad6f04470699f3b5.jpg",
      "image_filename": "e04a8e814aa1d5583860a4f45bddac8907e0f98d4d9d4462ad6f04470699f3b5.jpg",
      "caption": "Table 1: Risk differences of real and synthetic datasets",
      "context_before": "5.2 Fair Data Generation\n\nWe evaluate FairGAN on data generation from two perspectives, fairness and utility. Fairness is to check whether FairGAN can generate fair data, while the utility is to check whether FairGAN can learn the distribution of real data precisely.\n\nFairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models on fair data generation. Table 1",
      "context_after": "shows the risk differences in the real and synthetic datasets. The risk difference in the Adult dataset is 0.1989, which indicates discrimination against female. The SYN-GAN, which is trained to be close to the real dataset, has the similar risk difference to the real dataset. On the contrary, SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN have lower risk differences than the real dataset. In particular, both SYN2-NFGANI and SYN3-NFGANII have extremely small risk differences. This is because the protected attribute of SYN2-NFGANI and SYN3-NFGANII is independently assigned, i.e., $\\hat { y } \\perp \\perp \\hat { s }$ . Hence, the synthetic datasets from SYN2-NFGANI and SYN3-NFGANII are free from disparate treatment. FairGAN prevents the disparate treatment by generating revised $\\hat { y }$ to make $\\hat { y } \\perp \\perp \\hat { s } .$ . The risk differy y sence of SYN4-FairGAN is 0.0411, which shows the effectiveness of FairGAN on fair data generation.\n\nIn Figure 4, we compare the dimension-wise conditional probability distributions between $P ( \\mathbf { x } , y | s = 1 )$ and $P ( \\mathbf { x } , y | s = 0 )$ . Each P ,y s P ,y sdot indicates one attribute. The diagonal line indicates the ideal fairness, where the conditional probability distributions of each attribute given $s = 1$ and $s = 0$ are identical. We can observe that the dimension-wise distributions of datasets with lower risk differences are closer to the diagonal line. For example, dimension-wise conditional probabilities of real dataset and SYN1-GAN are spread around the diagonal line (shown in Figures 4a and 4b), while conditional probabilities of SYN2-NFGANI and SYN3-NFGANII with the lowest risk difference are just on the diagonal line (shown in Figure 4c). SYN4-FairGAN also achieves reasonable risk difference, so the attribute dots are close to the diagonal line. Overall, the synthetic datasets from SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN can prevent the disparate treatment.\n\nWe further evaluate the $\\epsilon$ -fairness (disparate impact) by calcuϵlating the balanced error rates (BERs) in the real data and SYN4- FairGAN. Because the protected attribute in SYN2-NFGANI and SYN3-NFGANII are randomly assigned, the real given xˆ is unsknown. The BERs in SYN2-NFGANI and SYN3-NFGANII cannot be calculated. The BER in the real dataset is 0.1538, which means a classifier can predict given x with high accuracy. Hence, there is",
      "referring_paragraphs": [
        "Figure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .",
        "Fairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models on fair data generation. Table 1",
        "Figure 1: Illustration of generative adversarial networks\n\nwith empirical probabilities estimated from $\\mathcal { D }$ , where  (balanced error rate) is defined as",
        "Figure 1 illustrates the structure of GAN.",
        "Table 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>\n\nshows the risk differences in the real and synthetic datasets."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig12.jpg",
      "image_filename": "1805.11202_page0_fig12.jpg",
      "caption": "(a) SYN1-GAN: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
      "context_before": "shows the risk differences in the real and synthetic datasets. The risk difference in the Adult dataset is 0.1989, which indicates discrimination against female. The SYN-GAN, which is trained to be close to the real dataset, has the similar risk difference to the real dataset. On the contrary, SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN have lower risk differences than the real dataset. In particular, both SYN2-NFGANI and SYN3-NFGANII have extremely small risk differences. This is because the protected attribute of SYN2-NFGANI and SYN3-NFGANII is independently assigned, i.e., $\\hat { y } \\perp \\perp \\hat { s }$ . Hence, the synthetic datasets from SYN2-NFGANI and SYN3-NFGANII are free from disparate treatment. FairGAN prevents the disparate treatment by generating revised $\\hat { y }$ to make $\\hat { y } \\perp \\perp \\hat { s } .$ . The risk differy y sence of SYN4-FairGAN is 0.0411, which shows the effectiveness of FairGAN on fair data generation.\n\nIn Figure 4, we compare the dimension-wise conditional probability distributions between $P ( \\mathbf { x } , y | s = 1 )$ and $P ( \\mathbf { x } , y | s = 0 )$ . Each P ,y s P ,y sdot indicates one attribute. The diagonal line indicates the ideal fairness, where the conditional probability distributions of each attribute given $s = 1$ and $s = 0$ are identical. We can observe that the dimension-wise distributions of datasets with lower risk differences are closer to the diagonal line. For example, dimension-wise conditional probabilities of real dataset and SYN1-GAN are spread around the diagonal line (shown in Figures 4a and 4b), while conditional probabilities of SYN2-NFGANI and SYN3-NFGANII with the lowest risk difference are just on the diagonal line (shown in Figure 4c). SYN4-FairGAN also achieves reasonable risk difference, so the attribute dots are close to the diagonal line. Overall, the synthetic datasets from SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN can prevent the disparate treatment.\n\nWe further evaluate the $\\epsilon$ -fairness (disparate impact) by calcuϵlating the balanced error rates (BERs) in the real data and SYN4- FairGAN. Because the protected attribute in SYN2-NFGANI and SYN3-NFGANII are randomly assigned, the real given xˆ is unsknown. The BERs in SYN2-NFGANI and SYN3-NFGANII cannot be calculated. The BER in the real dataset is 0.1538, which means a classifier can predict given x with high accuracy. Hence, there is",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig13.jpg",
      "image_filename": "1805.11202_page0_fig13.jpg",
      "caption": "(b) SYN1-GAN:$P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs.$P _ { G } ( \\mathbf { x } , y | s = 1 )$ )",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig14.jpg",
      "image_filename": "1805.11202_page0_fig14.jpg",
      "caption": "(c) SYN1-GAN: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig15.jpg",
      "image_filename": "1805.11202_page0_fig15.jpg",
      "caption": "(d) SYN2-NFGANI: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig16.jpg",
      "image_filename": "1805.11202_page0_fig16.jpg",
      "caption": "(e) SYN2-NFGANI: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 1 )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig17.jpg",
      "image_filename": "1805.11202_page0_fig17.jpg",
      "caption": "(f) SYN2-NFGANI: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig18.jpg",
      "image_filename": "1805.11202_page0_fig18.jpg",
      "caption": "(g) SYN3-NFGANII: $\\bar { P } _ { \\mathbf { d a t a } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig19.jpg",
      "image_filename": "1805.11202_page0_fig19.jpg",
      "caption": "(h) SYN3-NFGANII: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 1 )$ )",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig20.jpg",
      "image_filename": "1805.11202_page0_fig20.jpg",
      "caption": "(i) SYN3-NFGANII: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg",
      "image_filename": "1805.11202_page0_fig21.jpg",
      "caption": "(j) SYN4-FairGAN: $P _ { \\mathrm { { d a t a } } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig22.jpg",
      "image_filename": "1805.11202_page0_fig22.jpg",
      "caption": "(k) SYN4-FairGAN: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 1 )$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_25",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig23.jpg",
      "image_filename": "1805.11202_page0_fig23.jpg",
      "caption": "(l) SYN4-FairGAN: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$ ) Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.",
      "context_before": "",
      "context_after": "disparate impact in the real dataset. On the contrary, the BER in SYN4-FairGAN is $0 . 3 8 6 2 { \\scriptstyle \\pm 0 . 0 0 3 6 }$ , which indicates using the generated xˆ in SYN4-FairGAN to predict the real has much higher error srate. The disparate impact in SYN4-FairGAN is small. It shows the effectiveness of FairGAN on removal of the disparate impact in terms of the real . Note that we adopt a linear SVM as a classifier to predict .\n\nUtility. We then evaluate the data utility of synthetic datasets. We adopt the dimension-wise probability to check whether the generated data have the similar distribution to the real data on each dimension. Figure 5 compares dimension-wise probability distributions of different GAN models in both joint probability $P ( \\mathbf { x } , y )$ and conditional probability $P ( \\mathbf { x } , y | s )$ P ,y. From Figures 5a, 5d, 5g and P ,y s5j, we can observe that the four synthetic datasets generated by different GAN models have similar $P ( \\mathbf { x } , y )$ to the real dataset. Meanwhile, $P _ { G } ( \\mathbf { x } , y | s = 1 )$ and $P _ { G } ( \\mathbf { x } , y | s = 0 )$ ) on SYN1-GAN perfectly PG ,y s PG ,y smatch the real dataset (shown in Figures 5b and 5c), which indicates the effectiveness of the regular GAN model on data generation. We can also observe that SYN4-FariGAN better preserves $P ( \\mathbf { x } , y | s )$ P ,y sthan SYN2-NFGANI and SYN3-NFGANII by comparing the Figures 5k and 5l with Figures 5e, 5f, 5h and 5i. This is because neither NaïveFairGAN-I nor NaïveFairGAN-II ensures the generated samples have the same conditional probability distribution given as the real data.\n\nIn Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s. The Euclidean distance is calculated between the estimated probability vectors (probability mass function) on the sample space from the synthetic dataset and the real dataset. A smaller distance indicates better closeness between the real data and the synthetic data. As expected, SYN1-GAN has the smallest distance to the real dataset for joint and conditional probabilities. For synthetic datasets generated by FairGAN and NaïveFair-GAN models, SYN2-NFGANI has the smallest distance in terms of $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y ) - P _ { G } ( \\mathbf { x } , y ) | | _ { 2 }$ since its objective is $P _ { G } ( \\mathbf { x } , y ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ P ,y PG ,y PG ,y P ,ywhile SYN4-FairGAN has the smallest distance in terms of conditional probability $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s ) - P _ { G } ( \\mathbf { x } , y | s ) | | _ { 2 }$ and joint probability $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s ) - P _ { G } ( \\mathbf { x } , y , s ) | | _ { 2 }$ since only FairGAN aims to ensure $P _ { G } ( \\mathbf { x } , y , s ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s )$ . Overall, without considering the PG , y, s P , y, sprotected attribute, all the synthetic datasets from FairGAN and NaïveFairGAN models are close to the real dataset. When considering the protected attribute, FairGAN has better performance than NaïveFairGAN models. Therefore, after removing disparate impact, FairGAN still achieves good data utility.",
      "referring_paragraphs": [
        "Utility. We then evaluate the data utility of synthetic datasets. We adopt the dimension-wise probability to check whether the generated data have the similar distribution to the real data on each dimension. Figure 5 compares dimension-wise probability distributions of different GAN models in both joint probability $P ( \\mathbf { x } , y )$ and conditional probability $P ( \\mathbf { x } , y | s )$ P ,y. From Figures 5a, 5d, 5g and P ,y s5j, we can observe that the four synthetic datasets generat",
        "Figure 5: Dimension-wise probability distributions."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_26",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/f7292ad4b93a243e252756e4f5181f42f624b01d4e8e8ce40ff1b813d55322ab.jpg",
      "image_filename": "f7292ad4b93a243e252756e4f5181f42f624b01d4e8e8ce40ff1b813d55322ab.jpg",
      "caption": "Table 2: Euclidean distances of joint and conditional probabilities between synthetic datasets and real dataset",
      "context_before": "disparate impact in the real dataset. On the contrary, the BER in SYN4-FairGAN is $0 . 3 8 6 2 { \\scriptstyle \\pm 0 . 0 0 3 6 }$ , which indicates using the generated xˆ in SYN4-FairGAN to predict the real has much higher error srate. The disparate impact in SYN4-FairGAN is small. It shows the effectiveness of FairGAN on removal of the disparate impact in terms of the real . Note that we adopt a linear SVM as a classifier to predict .\n\nUtility. We then evaluate the data utility of synthetic datasets. We adopt the dimension-wise probability to check whether the generated data have the similar distribution to the real data on each dimension. Figure 5 compares dimension-wise probability distributions of different GAN models in both joint probability $P ( \\mathbf { x } , y )$ and conditional probability $P ( \\mathbf { x } , y | s )$ P ,y. From Figures 5a, 5d, 5g and P ,y s5j, we can observe that the four synthetic datasets generated by different GAN models have similar $P ( \\mathbf { x } , y )$ to the real dataset. Meanwhile, $P _ { G } ( \\mathbf { x } , y | s = 1 )$ and $P _ { G } ( \\mathbf { x } , y | s = 0 )$ ) on SYN1-GAN perfectly PG ,y s PG ,y smatch the real dataset (shown in Figures 5b and 5c), which indicates the effectiveness of the regular GAN model on data generation. We can also observe that SYN4-FariGAN better preserves $P ( \\mathbf { x } , y | s )$ P ,y sthan SYN2-NFGANI and SYN3-NFGANII by comparing the Figures 5k and 5l with Figures 5e, 5f, 5h and 5i. This is because neither NaïveFairGAN-I nor NaïveFairGAN-II ensures the generated samples have the same conditional probability distribution given as the real data.\n\nIn Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s. The Euclidean distance is calculated between the estimated probability vectors (probability mass function) on the sample space from the synthetic dataset and the real dataset. A smaller distance indicates better closeness between the real data and the synthetic data. As expected, SYN1-GAN has the smallest distance to the real dataset for joint and conditional probabilities. For synthetic datasets generated by FairGAN and NaïveFair-GAN models, SYN2-NFGANI has the smallest distance in terms of $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y ) - P _ { G } ( \\mathbf { x } , y ) | | _ { 2 }$ since its objective is $P _ { G } ( \\mathbf { x } , y ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ P ,y PG ,y PG ,y P ,ywhile SYN4-FairGAN has the smallest distance in terms of conditional probability $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s ) - P _ { G } ( \\mathbf { x } , y | s ) | | _ { 2 }$ and joint probability $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s ) - P _ { G } ( \\mathbf { x } , y , s ) | | _ { 2 }$ since only FairGAN aims to ensure $P _ { G } ( \\mathbf { x } , y , s ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s )$ . Overall, without considering the PG , y, s P , y, sprotected attribute, all the synthetic datasets from FairGAN and NaïveFairGAN models are close to the real dataset. When considering the protected attribute, FairGAN has better performance than NaïveFairGAN models. Therefore, after removing disparate impact, FairGAN still achieves good data utility.",
      "context_after": "5.3 Fair Classification\n\nIn this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness. We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called",
      "referring_paragraphs": [
        "FairGAN consists of one generator $G _ { D e c }$ and two discriminators $D _ { 1 }$ and $D _ { 2 }$ GDec. We adopt the revised generator from medGAN [5] to D Dgenerate both discrete and continuous data. Figure 2 shows the structure of FairGAN. In FairGAN, every generated sample has a corresponding value of the protected attribute $s \\sim P _ { \\mathrm { d a t a } } ( s )$ . The generator $G _ { D e c }$ generates a fake pair $( \\hat { \\mathbf { x } } , \\hat { y } )$ s P sfollowing the condi-Dec",
        "In Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s. The Euclidean distance is calculated between the estimated probability vectors (probability mass function) on the sample space from the synthetic dataset and the real dataset. A smaller distance indicates better clo",
        "Figure 2: The Structure of FairGAN\n\nAutoencoder is a feedforward neural network used for unsupervised learning.",
        "Figure 2 shows the structure of FairGAN.",
        "In Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_27",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/746b172e73430c8eadbfe872c52f7680ce8d6455337c2aba97dab3b9d6d2a893.jpg",
      "image_filename": "746b172e73430c8eadbfe872c52f7680ce8d6455337c2aba97dab3b9d6d2a893.jpg",
      "caption": "Table 3: Risk differences in classifiers and classification accuracies on various training and testing settings",
      "context_before": "5.3 Fair Classification\n\nIn this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness. We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called",
      "context_after": "SYN2SYN; 3) the classifiers are trained on the synthetic datasets and tested on the real dataset, called SYN2REAL. The ratio of the training set to testing set in these three settings is 1:1. We emphasize that only SYN2REAL is meaningful in practice as the classifiers are trained from the generated data and are adopted for decision making on the real data.\n\nWe adopt the following classifiers to evaluate the fair classification: 1) SVM (linear) which is a linear support vector machine with $C = 1$ ; 2) SVM (RBF) which is a support vector machine with Cthe radial basis kernel function; 3) Decision Tree with maximum tree depth as 5; Note that we do not adopt the protected attribute and only use the unprotected attributes to train classifiers, which ensures there is no disparate treatment in classifiers.\n\nFairness. We adopt the risk difference in a classifier $( d i s c ( \\eta ) =$ $P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( \\eta ( \\mathbf { x } ) = 1 | s = 0 ) \\}$ disc η) to evaluate the performance P η s P η sof classifier on fair prediction. Table 3 shows the risk differences in classifiers on various training and testing settings. We can observe that when the classifiers are trained and tested on real datasets (i.e., REAL2REAL), the risk differences in classifiers are high. It indicates that if there is disparate impact in the training dataset, the classifiers also incur discrimination for prediction. Since SYN1- GAN is close to the real dataset, classifiers trained on SYN1-GAN also have discrimination in both SYN2SYN and SYN2REAL settings.",
      "referring_paragraphs": [
        "sWe train FairGAN and NaïveFairGAN models to approximate the distribution of $P _ { \\mathrm { d a t a } } ( x )$ . Figure 3 shows the data probability $P ( x )$ P xand two conditional probabilities $P ( x | s = 1 )$ and $P ( x | s = 0 )$ P x of the P x s P x stoy dataset (shown in Figure 3a) and synthetic datasets (Figures 3b to 3d) from FairGAN and NaïveFairGAN models.",
        "Fairness. We adopt the risk difference in a classifier $( d i s c ( \\eta ) =$ $P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( \\eta ( \\mathbf { x } ) = 1 | s = 0 ) \\}$ disc η) to evaluate the performance P η s P η sof classifier on fair prediction. Table 3 shows the risk differences in classifiers on various training and testing settings. We can observe that when the classifiers are trained and tested on real datasets (i.e., REAL2REAL), the risk differences in classifiers are high. It indicates t",
        "Classification accuracy. Table 3 further shows the classification accuracies of different classifiers on various training and testing settings. We can observe that the accuracies of classifiers on the SYN2REAL setting are close to the results on the REAL2REAL setting. It indicates synthetic datasets generated by different GAN models are similar to the real dataset, showing the good data generation utility of GAN models. Meanwhile, accuracies of classifiers which are trained on SYN4-FairGAN and t",
        "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset.",
        "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called\n\nTable 3: Risk differences in classifiers and classification accuracies on various training and testing settings   \n\n<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Classifier</td><td>REAL2REAL</td><td colspan=\"4\">SYN2SYN</td><td colspan=\"4\">SYN2REAL</td></tr><tr><td></td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td rowspan=\"3\">Risk Difference</td><td>SVM (Linear)</td><td>0.1784</td><td>0.1341±0.0023</td><td>0.0018±0.0021</td><td>0.0073±0.0039</td><td>0.0371±0.0189</td><td>0.1712±0.0062</td><td>0.1580±0.0076</td><td>0.1579±0.0079</td><td>0.0461±0.0424</td></tr><tr><td>SVM (RBF)</td><td>0.1788</td><td>0.1292±0.0049</td><td>0.0018±0.0025</td><td>0.0074±0.0028</td><td>0.0354±0.0206</td><td>0.1623±0.0050</td><td>0.1602±0.0053</td><td>0.1603±0.0087</td><td>0.0526±0.0353</td></tr><tr><td>Decision Tree</td><td>0.1547</td><td>0.1396±0.0089</td><td>0.0015±0.0035</td><td>0.0115±0.0061</td><td>0.0535±0.0209</td><td>0.1640±0.0077</td><td>0.1506±0.0070</td><td>0.1588±0.0264</td><td>0.0754±0.0641</td></tr><tr><td rowspan=\"3\">Accuracy</td><td>SVM (Linear)</td><td>0.8469</td><td>0.8281±0.0103</td><td>0.8162±0.0133</td><td>0.8226±0.0126</td><td>0.8247±0.0115</td><td>0.8363±0.0108</td><td>0.8340±0.0091</td><td>0.8356±0.0018</td><td>0.8217±0.0093</td></tr><tr><td>SVM (RBF)</td><td>0.8433</td><td>0.8278±0.0099</td><td>0.8160±0.0100</td><td>0.8215±0.0130</td><td>0.8233±0.0103</td><td>0.8342±0.0036</td><td>0.8337±0.0060</td><td>0.8349±0.0012</td><td>0.8178±0.0128</td></tr><tr><td>Decision Tree</td><td>0.8240</td><td>0.8091±0.0059</td><td>0.7926±0.0083</td><td>0.8055±0.0102</td><td>0.8077±0.0144</td><td>0.8190±0.0051</td><td>0.8199±0.0041</td><td>0.8158±0.0069</td><td>0.8044±0.0140</td></tr></table>\n\nSYN2SYN; 3) the classifiers are trained on the synthetic datasets and tested on the real dataset, called SYN2REAL."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_28",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg",
      "image_filename": "1805.11202_page0_fig24.jpg",
      "caption": "5.4 Parameter Sensitivity",
      "context_before": "xˆ that don’t have correlations with the real , i.e. free from the dissparate impact, the classifier trained on SYN4-FairGAN can achieve fair classification on the real dataset. It demonstrates the advantage of FairGAN over the NaïveFairGAN models on fair classification.\n\nClassification accuracy. Table 3 further shows the classification accuracies of different classifiers on various training and testing settings. We can observe that the accuracies of classifiers on the SYN2REAL setting are close to the results on the REAL2REAL setting. It indicates synthetic datasets generated by different GAN models are similar to the real dataset, showing the good data generation utility of GAN models. Meanwhile, accuracies of classifiers which are trained on SYN4-FairGAN and tested on real dataset are only slightly lower than those trained on SYN1-GAN, which means the FairGAN model can achieve a good balance between utility and fairness. The small utility loss is caused by modifying unprotected attributes to remove disparate impact in terms of the real .\n\n5.4 Parameter Sensitivity",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_29",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig25.jpg",
      "image_filename": "1805.11202_page0_fig25.jpg",
      "caption": "(a) Utility and fairness in synthetic datasets from FairGAN with various . (b) Accuracy and fairness in a linear SVM which is trained on synthetic datasets from FairGAN with various and tested on real dataset. Figure 6: The sensitivity analysis of FairGAN with various",
      "context_before": "",
      "context_after": "We evaluate how the in FairGAN affects the synthetic datasets λfor fair data generation and fair classification. For fair data generation, we evaluate risk differences of the generated datasets and the Euclidean distances of joint probabilities $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s ) -$ $P _ { G } ( \\mathbf { x } , y , s ) | | _ { 2 }$ P ,y, sbetween real and synthetic datasets. From Figure 6a, PG ,y, swe can observe that the risk differences of the generated datasets decrease significantly when increases. Meanwhile, the Euclidean λdistances of joint probabilities $| | P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y , s ) - P _ { G } ( \\mathbf { x } , y , s ) | | _ { 2 }$ keep Psteady with slightly increases while $\\lambda$ ,y, s PG ,y, schanges from 0 to 2. Meanλwhile, the standard deviations of Euclidean distances with various are smaller than $1 0 ^ { - 3 }$ . Overall, with the increase of from 0 to 2, λ λthe discrimination in the synthetic datasets becomes smaller while data generation utility keeps steady.\n\nFor fair classification, we train a linear SVM on different synthetic datasets generated by FairGAN with various and evaluate on the λreal dataset. Figure 6b shows how the accuracies and risk differences vary with different values. We can observe that the risk difference λin SVM when predicting on the real dataset decreases as increases. λMeanwhile, the prediction accuracy keeps relatively steady with a slightly decrease. The standard deviations of accuracies with various are smaller than $1 0 ^ { - 2 }$ . Overall, it indicates that increasing $\\lambda$ can prevent the classification discrimination on the real dataset λwhile achieving good classification utility.\n\n6 CONCLUSIONS AND FUTURE WORK",
      "referring_paragraphs": [
        "Figure 6: The sensitivity analysis of FairGAN with various\n\nWe evaluate how the  in FairGAN affects the synthetic datasets λfor fair data generation and fair classification."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1805.11202_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1808.08166": [
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg",
      "image_filename": "1808.08166_page0_fig0.jpg",
      "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
      "context_before": "The most common definitions of fairness in machine learning are statistical in nature. They proceed by fixing a small number of “protected subgroups” (such as racial or gender groups), and then ask that some statistic of interest be approximately equalized across groups. Standard choices for these statistics include positive classification rates [Calders and Verwer, 2010], false positive or false negative rates [Hardt et al., 2016, Kleinberg et al., 2017, Chouldechova, 2017] and positive predictive value [Chouldechova, 2017, Kleinberg et al., 2017] — see Berk et al. [2018] for more examples. These definitions are pervasive in large part because they are easy to check, although there are interesting computational challenges in learning subject to these constraints in the worst case — see e.g. Woodworth et al. [2017].\n\nUnfortunately, these statistical definitions are not very meaningful to individuals: because they are constraints only over averages taken over large populations, they promise essentially nothing about how an individual person will be treated. Dwork et al. [2012] enumerate a “catalogue of evils” which show how definitions of this sort can fail to provide meaningful guarantees. Kearns et al. [2018] identify a particularly troubling failure of standard statistical definitions of fairness, which can arise naturally without malicious intent, called “fairness gerrymandering”. They illustrate the idea with the following toy example shown in Figure 1, described as follows.\n\narXiv:1808.08166v1 [cs.LG] 24 Aug 2018",
      "context_after": "Suppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female). Suppose that these two attributes are distributed independently and uniformly at random, and are uncorrelated with a binary label that is also distributed uniformly at random. If we view gender and race as defining classes of people that we wish to protect, we could take a standard statistical fairness definition from the literature — say the equal odds condition of Hardt et al. [2016], which asks to equalize false positive rates across protected groups, and instantiate it with the four protected groups: “Men”, “Women”, “blue people”, and “green people”. The following classifier satisfies this condition, although only by “cheating” and packing its unfairness into structured subgroups of the protected populations: it labels a person as positive only if they are a blue man or a green woman. This equalizes false positive rates across the four specified groups, but of course not over the finer-grained subgroups defined by the intersections of the two protected attributes.\n\nKearns et al. [2018] also proposed an approach to the problem of fairness gerrymandering: rather than asking for statistical definitions of fairness that hold over a small number of coarsely defined groups, ask for them to hold over a combinatorially or infinitely large collection of subgroups defined by a set of functions $\\vec { \\mathcal { G } }$ of the protected attributes (H´ebert-Johnson et al. [2018] independently made a similar proposal). For example, we could ask to equalize false positive rates across every subgroup that can be defined as the intersection or conjunction of $d$ protected attributes, for which there are $2 ^ { d }$ such groups. Kearns et al. [2018] showed that as long as the class of functions defining these subgroups has bounded VC dimension, the statistical learning problem of finding the best (distribution over) classifiers in $\\mathcal { H }$ subject to the constraint of equalizing the positive classification rate, the false positive rate, or the false negative rate over every subgroup defined over $\\vec { \\mathcal { G } }$ is solvable whenever the dataset size is sufficiently large relative to the VC dimension of $\\mathcal { G }$ and $\\mathcal { H }$ . Taking inspiration from the technique of Agarwal et al. [2018], they were able to show that even with combinatorially many subgroup fairness constraints, the computational problem of learning the optimal fair classifier is once again solvable efficiently whenever the learner has access to a black-box classifier (oracle) which can solve the unconstrained learning problems over $\\vec { \\mathcal { G } }$ and $\\mathcal { H }$ respectively. Similarly, given access to an oracle for $\\mathcal { G }$ , they were able to efficiently solve the problem of auditing for rich subgroup fairness: finding the $g \\in { \\mathcal { G } }$ that corresponds to the subgroup for whom the statistical fairness constraint was most violated.\n\nWhile the work of Kearns et al. [2018] is satisfying from a theocratical point of view, it leaves open a number of pressing empirical questions. For example, their theory is built for an idealized setting with perfect learning oracles — in practice heuristic oracles may fail. Moreover, perhaps rich subgroup fairness is asking for too much in practice — maybe enforcing combinatorially many constraints leads to an untenable tradeoff with error. Finally, perhaps enforcing combinatorially many constraints is not necessary — perhaps on real data, it is enough to call upon the algorithm of Agarwal et al. [2018] for enforcing statistical fairness constraints on the small number of groups defined by the marginal protected attributes, and rich subgroup fairness will follow incidentally. Put another way: Is the so-called fairness gerrymandering problem only a theoretical curiosity, or does it arise organically when standard classifiers are optimized subject to marginal",
      "referring_paragraphs": [
        "Unfortunately, these statistical definitions are not very meaningful to individuals: because they are constraints only over averages taken over large populations, they promise essentially nothing about how an individual person will be treated. Dwork et al. [2012] enumerate a “catalogue of evils” which show how definitions of this sort can fail to provide meaningful guarantees. Kearns et al. [2018] identify a particularly troubling failure of standard statistical definitions of fairness, which ca",
        "The properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall number of features (which varies from 10 to 128), the number of protected features in the subgroup class (which varies from 3 to 18), the nature of the protected features, and the baseline (majority class) error rate.",
        "They illustrate the idea with the following toy example shown in Figure 1, described as follows.",
        "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]\n\nSuppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).",
        "The heuristic finds a linear threshold function as follows:\n\nTable 1: Description of Data Sets."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/0316712cab10fb743166e741482c718fc19c00e3e8610fd406a4a41dd4162e0a.jpg",
      "image_filename": "0316712cab10fb743166e741482c718fc19c00e3e8610fd406a4a41dd4162e0a.jpg",
      "caption": "Table 1: Description of Data Sets.",
      "context_before": "$$ \\hat {h} \\in \\underset {h \\in \\mathcal {H}} {\\operatorname {a r g m i n}} \\sum_ {i = 1} ^ {n} \\left[ h \\left(X _ {i}\\right) c _ {i} ^ {1} + \\left(1 - h \\left(X _ {i}\\right)\\right) c _ {i} ^ {0} \\right] \\tag {1} $$\n\nFollowing both Agarwal et al. [2018] and Kearns et al. [2018], in all of the experiments in this paper we take the classes $\\mathcal { H }$ and $\\mathcal { G }$ to be linear threshold functions, and we use a linear regression heuristic for both auditing and learning. The heuristic finds a linear threshold function as follows:\n\n1or more generally to any fairness constraint that can be expressed as a linear equality on the conditional moments $\\mathbb { E } \\left[ t ( X , y , D ( X ) | \\boldsymbol { \\varepsilon } ( X , y ) ) \\right]$ , where $\\varepsilon ( X , y )$ is an event defined with respect to $( X , y )$ , and $t : X \\times \\{ 0 , 1 \\} \\times \\{ 0 , 1 \\} [ 0 , 1 ]$ Agarwal et al. [2018]. Equality of false positive rate is a particular instantiation of this kind of constraint where $\\varepsilon$ is the event $y = 0$ and $t = { \\bf 1 } \\{ D ( X ) = 1 \\}$ .",
      "context_after": "We leave the precise descriptions of the algorithm from Kearns et al. [2018] — which we will refer to as the SUBGROUP algorithm — to the appendix. We refer the reader to Kearns et al. [2018] for details about its derivation and guarantees.2 At this point we remark only that the algorithm operates by expressing the optimization problem to be solved (minimize error, subject to subgroup fairness constraints) as solving for the equilibrium in a two player zero-sum game, between a Learner and an Auditor. The Learner has the set of hypothesis $\\mathcal { H }$ as its action (pure strategy) space, and the Auditor has the set of subgroups $\\mathcal { G }$ as its action space. The best response problem for the Auditor corresponds to the auditing problem: finding the subgroup $g \\in { \\mathcal { G } }$ for which the strategy of the learner violates the fairness constraints the most. The best response problem for the Learner corresponds to solving a weighted (but unconstrained) empirical risk minimization problem. The best response problem for both players can be expressed as solving a cost sensitive classification problem. The algorithm SUBGROUP essentially simulates the fictitious play of this game, which proceeds over rounds, and in each round $t$ both players best respond to their opponent’s empirical history of play:\n\nThis can be done efficiently assuming access to oracles which solve the cost sensitive classification problem over $\\mathcal { G }$ and $\\mathcal { H }$ respectively.\n\n3 Empirical Evaluation",
      "referring_paragraphs": [
        "Unfortunately, these statistical definitions are not very meaningful to individuals: because they are constraints only over averages taken over large populations, they promise essentially nothing about how an individual person will be treated. Dwork et al. [2012] enumerate a “catalogue of evils” which show how definitions of this sort can fail to provide meaningful guarantees. Kearns et al. [2018] identify a particularly troubling failure of standard statistical definitions of fairness, which ca",
        "The properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall number of features (which varies from 10 to 128), the number of protected features in the subgroup class (which varies from 3 to 18), the nature of the protected features, and the baseline (majority class) error rate.",
        "They illustrate the idea with the following toy example shown in Figure 1, described as follows.",
        "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]\n\nSuppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).",
        "The heuristic finds a linear threshold function as follows:\n\nTable 1: Description of Data Sets."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig1.jpg",
      "image_filename": "1808.08166_page0_fig1.jpg",
      "caption": "(a)",
      "context_before": "We ran experiments on 3 datasets from the UCI Machine Learning Repository Dheeru and Karra Taniskidou [2017]: Communities and Crime [Redmond and Baveja, 2002], Adult, and Student [Cortez and Silva, 2008], and the Law School dataset from the Law School Admission Council’s National Longitudinal Bar Passage Study [Wightman, 1998]. These datasets were selected due to their potential fairness concerns, including:\n\nThe properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall number of features (which varies from 10 to 128), the number of protected features in the subgroup class (which varies from 3 to 18), the nature of the protected features, and the baseline (majority class) error rate.\n\nSome methodological notes:",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg",
      "image_filename": "1808.08166_page0_fig2.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig3.jpg",
      "image_filename": "1808.08166_page0_fig3.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig4.jpg",
      "image_filename": "1808.08166_page0_fig4.jpg",
      "caption": "(d) Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .",
      "context_before": "",
      "context_after": "3.2 Empirical Convergence of SUBGROUP\n\nWe begin with an examination of the convergence properties of the SUBGROUP algorithm on the four datasets. Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\gamma$ provides an appealing trade-off between error and fairness. In addition to replicating those findings for Communities and Crime, we also find that they are not an optimistic anomaly. For example, for the Law School dataset, in Figure 2 we plot both the error $\\varepsilon _ { t }$ (panel (a)) and the fairness violation $\\gamma _ { t }$ (panel (b)) as a function of the iteration $t$ , for values of the input $\\gamma$ ranging from 0 to 0.03. We see that the algorithm converges relatively quickly (on the order of thousands of iterations), and that increasing the input $\\gamma$ generally yields decreasing error and increasing fairness violation (typically saturating the input $\\gamma$ ), as suggested by the idealized theory.\n\nBut on other datasets the empirical convergence does not match the idealized theory as cleanly, presumably due to the use of imperfect Learner and Auditor heuristics. In panels (c) and (d) of Figure 2 we again",
      "referring_paragraphs": [
        "We begin with an examination of the convergence properties of the SUBGROUP algorithm on the four datasets. Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\gamma$ provides an appealing trade-off between error and fairness. In addition to replicating those findings for Communities and Crime, we also find that they are not an optimistic anomaly. For example, ",
        "But on other datasets the empirical convergence does not match the idealized theory as cleanly, presumably due to the use of imperfect Learner and Auditor heuristics. In panels (c) and (d) of Figure 2 we again",
        "Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all $t$ and all input $\\gamma$ , and compute the undominated or Pareto frontier of these pairs. This frontier represents the accuracy-fairness tradeoff achieved by the SUBGROUP algorithm on a given data set, which is arguably its most important output. The choice of where one wants to be on the frontier is a policy question that should be m",
        "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .",
        "For example, for the Law School dataset, in Figure 2 we plot both the error $\\varepsilon _ { t }$ (panel (a)) and the fairness violation $\\gamma _ { t }$ (panel (b)) as a function of the iteration $t$ , for values of the input $\\gamma$ ranging from 0 to 0.03.",
        "Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all $t$ and all input $\\gamma$ , and compute the undominated or Pareto frontier of these pairs."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig5.jpg",
      "image_filename": "1808.08166_page0_fig5.jpg",
      "caption": "(a)",
      "context_before": "It is also of interest to compare the subgroup fairness achieved by the SUBGROUP algorithm (which is explicitly optimizing under a subgroup fairness constraint) with an algorithm only optimizing under weaker and more traditional marginal fairness constraints. To this end, we also implemented a version of the algorithm from Agarwal et al. [2018] — which we will refer to as the MARGINAL algorithm — for marginal fairness.3 From a theoretical perspective, a priori we would expect models trained for marginal fairness to fare poorly on subgroup fairness. But it is an empirical question — perhaps on some datasets, demanding marginal fairness already suffices to enforce subgroup fairness as well. Thus the high-level question is whether the SUBGROUP framework and algorithm are worth the added analytical and computational overhead.\n\nIn the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm. In the right column, we also separately show the marginal fairness frontier achieved by the MARGINAL algorithm. Before discussing the particulars of each dataset, we first make the following general observations:\n\n3Since some of the protected attributes are continuous rather than discrete, and the MARGINAL algorithm only handles discrete attributes, in order to run the marginal fairness algorithm we create sensitive groups by thresholding on the mean of each sensitive attribute.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig6.jpg",
      "image_filename": "1808.08166_page0_fig6.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig7.jpg",
      "image_filename": "1808.08166_page0_fig7.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig8.jpg",
      "image_filename": "1808.08166_page0_fig8.jpg",
      "caption": "(d)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig9.jpg",
      "image_filename": "1808.08166_page0_fig9.jpg",
      "caption": "(e)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "image_filename": "1808.08166_page0_fig10.jpg",
      "caption": "(f)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig11.jpg",
      "image_filename": "1808.08166_page0_fig11.jpg",
      "caption": "(g)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_14",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig12.jpg",
      "image_filename": "1808.08166_page0_fig12.jpg",
      "caption": "(h) Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "context_before": "",
      "context_after": "• By matching points between the MARGINAL marginal and subgroup fairness plots, we find that with the exception of the Student data set, there is a systematic relationship between marginal and subgroup unfairness: asking the MARGINAL algorithm to reduce marginal unfairness also causes it to reduce subgroup unfairness — but not by as much as the SUBGROUP algorithm achieves.\n\nTogether these observations let us conclude that subgroup fairness is a strong but achievable notion in practice (at least on these datasets), and that the SUBGROUP algorithm appears to be an effective tool for its investigation.\n\nIt is also worth commenting on the differences across datasets, and focusing not just on the qualitative shapes of the Pareto curves but their actual numerical specifics — especially since in real applications, these will matter to stakeholders. For instance, the actual range of error values spanned by the SUBGROUP Pareto curves ranges from nearly 10% (Communities and Crime) to less than 2% (Student). So perhaps for Communities and Crime, the tradeoff is starker from an accuracy perspective. We now provide some brief commentary on each dataset.",
      "referring_paragraphs": [
        "In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm. In the right column, we also separately show the marginal fairness frontier achieved by the MARGINAL algorithm. Before discussing the particulars of each dataset, we first make the following general observations:",
        "In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm.",
        "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig13.jpg",
      "image_filename": "1808.08166_page0_fig13.jpg",
      "caption": "(a)",
      "context_before": "As the algorithm proceeds, we see this discrimination flip by $t = 7$ (panel (b)), into a regime with a higher false positive rate for predominantly black communities, and then revert again by $t = 1 3$ . Over the early iterations these oscillations continue, growing less drastic as the $\\gamma$ -unfairness surface starts to flatten out noticeably by $t = 3 7$ (panel (g)). In panel (h) we plot $t = 1 3 0 1$ and see that the surface has almost completely flattened, with maximum $\\gamma$ -unfairness below .0028. So over the course of the first 1300 iterations of SUBGROUP we’ve reduced the $\\gamma$ -unfairness from over 0.02 in most of the subgroups, to less than 0.0028 in every subgroup. Recall again that this corresponds to false positive rate disparities of at most 2.8% in subgroups that represent 10% of the population — a reduction from false positive rate disparities of 20% many similarly sized subgroups. This represents an order of magnitude improvement that results from using the classifier learned by SUBGROUP.\n\n3.5 Understanding the Dynamics\n\nWe conclude by examining the dynamics of the SUBGROUP algorithm on the Communities and Crime dataset in greater detail. More specifically, since the algorithm is formulated as a game between a Learner who at each iteration $t$ is trying to minimize the error $\\varepsilon _ { t }$ , and an Auditor who is trying to minimize subgroup unfairness $\\gamma _ { t }$ , we visualize the trajectories traced in $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ space as $t$ increases.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig14.jpg",
      "image_filename": "1808.08166_page0_fig14.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig15.jpg",
      "image_filename": "1808.08166_page0_fig15.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig16.jpg",
      "image_filename": "1808.08166_page0_fig16.jpg",
      "caption": "(d)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig17.jpg",
      "image_filename": "1808.08166_page0_fig17.jpg",
      "caption": "(e)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg",
      "image_filename": "1808.08166_page0_fig18.jpg",
      "caption": "(f)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig19.jpg",
      "image_filename": "1808.08166_page0_fig19.jpg",
      "caption": "(g)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_22",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig20.jpg",
      "image_filename": "1808.08166_page0_fig20.jpg",
      "caption": "(h) Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Note that in addition to making brute force auditing tractable, restricting to two dimensions permits direct visualization of discrimination. In Figure 4, we show a sequence of “discrimination surfaces” for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$ . The $x \\mathrm { ~ - ~ } y$ axes are the coefficients of $\\theta$ corresponding to whitepct and blackpct respectively, and the $z$ -axis is the $\\gamma$ -unfairness of the corresponding subgroup. This is our first",
        "In Figure 4, we show a sequence of “discrimination surfaces” for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$ .",
        "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12"
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig21.jpg",
      "image_filename": "1808.08166_page0_fig21.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg",
      "image_filename": "1808.08166_page0_fig22.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig23.jpg",
      "image_filename": "1808.08166_page0_fig23.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_26",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig24.jpg",
      "image_filename": "1808.08166_page0_fig24.jpg",
      "caption": "(d) Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .",
      "context_before": "",
      "context_after": "The plots in Figure 5 correspond to such trajectories for input $\\gamma$ values of $0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9$ , and 0.022 (panels (a), (b), (c) and (d) respectively), which are denoted by the dashed lines on the $\\gamma _ { t }$ axis of each figure. The 0.001 and $0 . 0 0 5 , 0 . 0 0 9$ values correspond to small and intermediate $\\gamma$ regimes, whereas 0.022 is close to (but slightly below) the subgroup unfairness of the unconstrained classifier. The trajectories are color coded from colder to warmer colors according to their iteration number to give a sense of speed of convergence.\n\nThe first plot in all four trajectories corresponds to the $\\left. \\varepsilon _ { 0 } , \\gamma _ { 0 } \\right.$ of the unconstrained classifier. Furthermore, as long as the current $\\gamma _ { t }$ values remain above the horizontal dashed line representing the input $\\gamma$ , the trajectories remain identical, as the same subgroups are being presented to the learner in each trajectory. But when $\\gamma _ { t }$ falls below a given input $\\gamma$ , that trajectory will follow its own path going forward.\n\nWe first observe that the dynamics exhibit a fair amount of complexity and subtlety. They all begin with low error and large unfairness, and quickly follow a brief but large increase in $\\varepsilon _ { t }$ as fairness starts to be enforced. There are steps in which both $\\varepsilon _ { t }$ and $\\gamma _ { t }$ increase, and a large early loop in trajectory space is observed. But the first three trajectories (panels (a), (b) and (c), corresponding to the three smaller values of $\\gamma$ ) quickly settle near the input $\\gamma$ line, at which point begins a long, oscillatory “border war” around this line, as the Learner tries to minimize error, but is pushed back below the line by the Auditor anytime $\\gamma$ -fairness is violated. The idealized theory predicts that each trajectory should end at the input $\\gamma$ line (subgroup fairness constraint saturated), and with larger input $\\gamma$ (weaker fairness constraint) resulting in lower error. The empirical trajectories indeed conform nicely to the theory, with the final (red) points near the dashed lines, and further left for larger $\\gamma$ .",
      "referring_paragraphs": [
        "The plots in Figure 5 correspond to such trajectories for input $\\gamma$ values of $0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9$ , and 0.022 (panels (a), (b), (c) and (d) respectively), which are denoted by the dashed lines on the $\\gamma _ { t }$ axis of each figure. The 0.001 and $0 . 0 0 5 , 0 . 0 0 9$ values correspond to small and intermediate $\\gamma$ regimes, whereas 0.022 is close to (but slightly below) the subgroup unfairness of the unconstrained classifier. The trajectories are color coded from",
        "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1808.08166_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1809.01496": [
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg",
      "image_filename": "1809.01496_page0_fig0.jpg",
      "caption": "(a) $w ^ { ( g ) }$ dimension for all the professions",
      "context_before": "Separate protected attribute First, we demonstrate that GN-GloVe preserves the gender association (either definitional or stereotypical associations) in $w ^ { ( g ) 4 }$ . To illustrate the distribution of gender information of different words, we plot Fig. 1a using $w ^ { ( g ) }$ for the $\\mathbf { X }$ -axis and a random value for the y-axis to spread out words in the plot. As shown in the figure, the gender-definition words, e.g. “waiter” and “waitress”, fall far away from each other in $w ^ { ( g ) }$ . In addition, words such as “housekeeper” and “doctor” are inclined to different genders and their $w ^ { ( g ) }$ preserves such information.\n\nNext, we demonstrate that GN-GloVe reduces gender stereotype using a list of profession titles from (Bolukbasi et al., 2016). All these profession titles are neutral to gender by definition. In Fig. 1b and Fig. 1c, we plot the cosine similarity between each word vector $w ^ { ( a ) }$ and the gender direction $v _ { g }$ (i.e., kwkkvg $\\frac { w ^ { T } v _ { g } } { \\| w \\| \\| v _ { g } \\| } )$ wT vg . Result shows that words, such as “doctor” and “nurse”, possess no gender association by definition, but their GloVe word vectors exhibit strong gender stereotype. In contrast, the gender projects of GN-GloVe word vectors $w ^ { ( a ) }$ are closer to zero. This demonstrates\n\n4We follow the original GloVe implementation using the summation of word vector and context vector to represent a word. Therefore, the elements of the word vectors are constrained in [-2, 2]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.01496_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig1.jpg",
      "image_filename": "1809.01496_page0_fig1.jpg",
      "caption": "(b) Gender-neutral profession words projected to gender direction in GloVe",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.01496_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig2.jpg",
      "image_filename": "1809.01496_page0_fig2.jpg",
      "caption": "(c) Gender-neutral profession words projected to gender direction in GN-GloVe Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.",
      "context_before": "",
      "context_after": "the gender information has been substantially diminished from $w ^ { ( a ) }$ in the GN-GloVe embedding.\n\nWe further quantify the gender information exhibited in the embedding models. For each model, we project the word vectors of occupational words into the gender sub-space defined by “he-she” and compute their average size. A larger projection indicates an embedding model is more biased. Results show that the average projection of GloVe is 0.080, the projection of Hard-GloVe is 0.019, and the projection of Gn-Glove is 0.052. Comparing with GloVe, GN-GloVe reduces the bias by $3 5 \\%$ . Although Hard-GloVe contains less gender information, we will show later GN-GloVe can tell difference between gender-stereotype and genderdefinition words better.\n\nGender Relational Analogy To study the quality of the gender information present in each model, we follow SemEval 2012 Task2 (Jurgens et al., 2012) to create an analogy dataset, SemBias, with the goal to identify the correct analogy of “he - she” from four pairs of words. Each instance in the dataset consists of four word pairs: a genderdefinition word pair (Definition; e.g., “waiter - waitress”), a gender-stereotype word pair (Stereotyp; e.g., “doctor - nurse”) and two other pairs of words that have similar meanings (None; e.g., “dog - cat”, “cup - lid”)5. We consider 20 genderstereotype word pairs and 22 gender-definition word pairs and use their Cartesian product to generate 440 instances. Among the 22 genderdefinition word pairs, there are 2 word pairs that",
      "referring_paragraphs": [
        "Table 1 lists the percentage of times that each class of pair is on the top based on a word embedding model (Mikolov et al., 2013c). GN-GloVe achieves $9 7 . 7 \\%$ accuracy in identifying genderdefinition word pairs as an analogy to “he - she”. In contrast, GloVe and Hard-GloVe makes significantly more mistakes. On the subset, GN-GloVe also achieves significantly better performance than Hard-Glove and GloVe, indicating that it can generalize the gender pairs on the training set to identify other",
        "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.01496_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/cce2b0bde6ccb8fc8cf3d0e7374aaaa19f4605fa8353f73b5dbc00931c5b44ac.jpg",
      "image_filename": "cce2b0bde6ccb8fc8cf3d0e7374aaaa19f4605fa8353f73b5dbc00931c5b44ac.jpg",
      "caption": "Table 1: Percentage of predictions for each category on gender relational analogy task.",
      "context_before": "the gender information has been substantially diminished from $w ^ { ( a ) }$ in the GN-GloVe embedding.\n\nWe further quantify the gender information exhibited in the embedding models. For each model, we project the word vectors of occupational words into the gender sub-space defined by “he-she” and compute their average size. A larger projection indicates an embedding model is more biased. Results show that the average projection of GloVe is 0.080, the projection of Hard-GloVe is 0.019, and the projection of Gn-Glove is 0.052. Comparing with GloVe, GN-GloVe reduces the bias by $3 5 \\%$ . Although Hard-GloVe contains less gender information, we will show later GN-GloVe can tell difference between gender-stereotype and genderdefinition words better.\n\nGender Relational Analogy To study the quality of the gender information present in each model, we follow SemEval 2012 Task2 (Jurgens et al., 2012) to create an analogy dataset, SemBias, with the goal to identify the correct analogy of “he - she” from four pairs of words. Each instance in the dataset consists of four word pairs: a genderdefinition word pair (Definition; e.g., “waiter - waitress”), a gender-stereotype word pair (Stereotyp; e.g., “doctor - nurse”) and two other pairs of words that have similar meanings (None; e.g., “dog - cat”, “cup - lid”)5. We consider 20 genderstereotype word pairs and 22 gender-definition word pairs and use their Cartesian product to generate 440 instances. Among the 22 genderdefinition word pairs, there are 2 word pairs that",
      "context_after": "are not used as a seed word during the training. To test the generalization ability of the model, we generate a subset of data (SemBias (subset)) of 40 instances associated with these 2 pairs.\n\nTable 1 lists the percentage of times that each class of pair is on the top based on a word embedding model (Mikolov et al., 2013c). GN-GloVe achieves $9 7 . 7 \\%$ accuracy in identifying genderdefinition word pairs as an analogy to “he - she”. In contrast, GloVe and Hard-GloVe makes significantly more mistakes. On the subset, GN-GloVe also achieves significantly better performance than Hard-Glove and GloVe, indicating that it can generalize the gender pairs on the training set to identify other gender-definition word pairs.\n\nWord Similarity and Analogy In addition, we evaluate the word embeddings on the benchmark tasks to ensure their quality. The word similarity tasks measure how well a word embedding model captures the similarity between words comparing to human annotated rating scores. Embeddings are tested on multiple datasets: WS353-ALL (Finkelstein et al., 2001), RG-65 (Rubenstein and Goodenough, 1965), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), RW (Luong et al., 2013), and MEN-TR-3k (Bruni et al., 2012)",
      "referring_paragraphs": [
        "Table 1 lists the percentage of times that each class of pair is on the top based on a word embedding model (Mikolov et al., 2013c). GN-GloVe achieves $9 7 . 7 \\%$ accuracy in identifying genderdefinition word pairs as an analogy to “he - she”. In contrast, GloVe and Hard-GloVe makes significantly more mistakes. On the subset, GN-GloVe also achieves significantly better performance than Hard-Glove and GloVe, indicating that it can generalize the gender pairs on the training set to identify other",
        "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.01496_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/f7672efa57ff2c2f889183e03ec2ef64fc718048c3de5b09678ab2d9c137f548.jpg",
      "image_filename": "f7672efa57ff2c2f889183e03ec2ef64fc718048c3de5b09678ab2d9c137f548.jpg",
      "caption": "Table 2: Results on the benchmark datasets. Performance is measured in accuracy and in Spearman rank correlation for word analogy and word similarity tasks, respectively.",
      "context_before": "Table 1 lists the percentage of times that each class of pair is on the top based on a word embedding model (Mikolov et al., 2013c). GN-GloVe achieves $9 7 . 7 \\%$ accuracy in identifying genderdefinition word pairs as an analogy to “he - she”. In contrast, GloVe and Hard-GloVe makes significantly more mistakes. On the subset, GN-GloVe also achieves significantly better performance than Hard-Glove and GloVe, indicating that it can generalize the gender pairs on the training set to identify other gender-definition word pairs.\n\nWord Similarity and Analogy In addition, we evaluate the word embeddings on the benchmark tasks to ensure their quality. The word similarity tasks measure how well a word embedding model captures the similarity between words comparing to human annotated rating scores. Embeddings are tested on multiple datasets: WS353-ALL (Finkelstein et al., 2001), RG-65 (Rubenstein and Goodenough, 1965), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), RW (Luong et al., 2013), and MEN-TR-3k (Bruni et al., 2012)\n\n5The pair is sampled from the list of word pairs with “SIMILAR: Coordinates” relation annotated in (Jurgens et al., 2012). The original list has 38 pairs. After removing gender-definition word pairs, 29 are left.",
      "context_after": "datasets. The analogy tasks are to answer the question $^ { 6 6 } A$ is to $B$ as $C$ is to ?” by finding a word vector $w$ that is closest to $w _ { A } - w _ { B } + w _ { C }$ in the embedding space. Google (Mikolov et al., 2013a) and MSR (Mikolov et al., 2013c) datasets are utilized for this evaluation. The results are shown in Table 2, where the suffix “-L1” and “-L2” of GN-GloVe stand for the GN-GloVe using $J _ { D } ^ { L 1 }$ and $J _ { D } ^ { L 2 }$ , respectively. Compared with others, GN-GloVe achieves a higher accuracy in the similarity tasks and its analogy score slightly drops indicating that GN-GloVe is capable of preserving proximity among words.\n\nCoreference Resolution Finally, we investigate how the gender bias in word embeddings affects a downstream application, such as coreference resolution. Coreference resolution aims at clustering the denotative noun phrases referring to the same entity in the given text. We evaluate our models on the Ontonotes 5.0 (Weischedel et al., 2012) benchmark dataset and the WinoBias dataset (Zhao et al., 2018).6 In particular, the WinoBias dataset is composed of pro-stereotype (PRO) and antistereotype (ANTI) subsets. The PRO subset consists of sentences where a gender pronoun refers to a profession, which is dominated by the same gender. Example sentences include “The CEO raised the salary of the receptionist because he is generous.” In this sentence, the pronoun “he” refers to “CEO” and this reference is consistent with societal stereotype. The ANTI subset contains the same set of sentences, but the gender pronoun in each sentence is replaced by the opposite gender. For instance, the gender pronoun “he” is replaced by “she” in the aforementioned example. Despite the sentence is almost identical, the gender pronoun now refers to a profession that is less represented by the gender. Details about the dataset are in (Zhao et al., 2018).",
      "referring_paragraphs": [
        "datasets. The analogy tasks are to answer the question $^ { 6 6 } A$ is to $B$ as $C$ is to ?” by finding a word vector $w$ that is closest to $w _ { A } - w _ { B } + w _ { C }$ in the embedding space. Google (Mikolov et al., 2013a) and MSR (Mikolov et al., 2013c) datasets are utilized for this evaluation. The results are shown in Table 2, where the suffix “-L1” and “-L2” of GN-GloVe stand for the GN-GloVe using $J _ { D } ^ { L 1 }$ and $J _ { D } ^ { L 2 }$ , respectively. Compared with other",
        "Embeddings are tested on multiple datasets: WS353-ALL (Finkelstein et al., 2001), RG-65 (Rubenstein and Goodenough, 1965), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), RW (Luong et al., 2013), and MEN-TR-3k (Bruni et al., 2012)\n\nTable 2: Results on the benchmark datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.01496_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/90d8200d4681163228a6c63fb384f0c97291707e8acd4f8973cfdc731bd33025.jpg",
      "image_filename": "90d8200d4681163228a6c63fb384f0c97291707e8acd4f8973cfdc731bd33025.jpg",
      "caption": "Table 3: F1 score $( \\% )$ on the coreference system.",
      "context_before": "datasets. The analogy tasks are to answer the question $^ { 6 6 } A$ is to $B$ as $C$ is to ?” by finding a word vector $w$ that is closest to $w _ { A } - w _ { B } + w _ { C }$ in the embedding space. Google (Mikolov et al., 2013a) and MSR (Mikolov et al., 2013c) datasets are utilized for this evaluation. The results are shown in Table 2, where the suffix “-L1” and “-L2” of GN-GloVe stand for the GN-GloVe using $J _ { D } ^ { L 1 }$ and $J _ { D } ^ { L 2 }$ , respectively. Compared with others, GN-GloVe achieves a higher accuracy in the similarity tasks and its analogy score slightly drops indicating that GN-GloVe is capable of preserving proximity among words.\n\nCoreference Resolution Finally, we investigate how the gender bias in word embeddings affects a downstream application, such as coreference resolution. Coreference resolution aims at clustering the denotative noun phrases referring to the same entity in the given text. We evaluate our models on the Ontonotes 5.0 (Weischedel et al., 2012) benchmark dataset and the WinoBias dataset (Zhao et al., 2018).6 In particular, the WinoBias dataset is composed of pro-stereotype (PRO) and antistereotype (ANTI) subsets. The PRO subset consists of sentences where a gender pronoun refers to a profession, which is dominated by the same gender. Example sentences include “The CEO raised the salary of the receptionist because he is generous.” In this sentence, the pronoun “he” refers to “CEO” and this reference is consistent with societal stereotype. The ANTI subset contains the same set of sentences, but the gender pronoun in each sentence is replaced by the opposite gender. For instance, the gender pronoun “he” is replaced by “she” in the aforementioned example. Despite the sentence is almost identical, the gender pronoun now refers to a profession that is less represented by the gender. Details about the dataset are in (Zhao et al., 2018).",
      "context_after": "We train the end-to-end coreference resolution model (Lee et al., 2017) with different word embeddings on OntoNote and report their performance in Table 3. For the WinoBias dataset, we also report the average (Avg) and absolute difference (Diff) of F1 scores on two subsets. A smaller Diff value indicates less bias in a system. Results show that GN-GloVe achieves comparable performance as Glove and Hard-GloVe on the OntoNotes dataset while distinctly reducing the bias on the WinoBias dataset. When only the $\\mathbf { \\chi } _ { w } ( a )$ potion of the embedding is used in representing words, ${ \\mathrm { G N - G l o V e } } ( w ^ { ( a ) } )$ further reduces the bias in coreference resolution.\n\n5 Conclusion and Discussion\n\nIn this paper, we introduced an algorithm for training gender-neutral word embedding. Our method is general and can be applied in any language as long as a list of gender definitional words is provided as seed words (e.g., gender pronouns). Future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender.",
      "referring_paragraphs": [
        "We train the end-to-end coreference resolution model (Lee et al., 2017) with different word embeddings on OntoNote and report their performance in Table 3. For the WinoBias dataset, we also report the average (Avg) and absolute difference (Diff) of F1 scores on two subsets. A smaller Diff value indicates less bias in a system. Results show that GN-GloVe achieves comparable performance as Glove and Hard-GloVe on the OntoNotes dataset while distinctly reducing the bias on the WinoBias dataset. Whe",
        "Table 3: F1 score $( \\% )$ on the coreference system."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.01496_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1809.02208": [
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg",
      "image_filename": "1809.02208_page0_fig0.jpg",
      "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
      "context_before": "On a 2014 article, Londa Schiebinger suggested that scientific research fails to take gender issues into account, arguing that the phenomenon of male defaults on new technologies such as Google Translate provides a window into this asymmetry [35]. Since then, recent worrisome results in machine learning have somewhat supported Schiebinger’s view. Not only Google photos’ statistical image labeling algorithm has been found to classify darkskinned people as gorillas [15] and purportedly intelligent programs have been suggested to be negatively biased against black prisoners when predicting criminal behavior [1] but the machine learning revolution has also indirectly revived heated debates about the controversial field of physiognomy, with proposals of AI systems capable of identifying the sexual orientation of an individual through its facial characteristics [38]. Similar concerns are growing at an unprecedented rate in the media, with reports of Apple’s Iphone X face unlock feature failing to differentiate between two different Asian people [32] and automatic soap dispensers which reportedly do not recognize black hands [28]. Machine bias, the phenomenon by which trained statistical models unbeknownst to their creators grow to reflect controversial societal asymmetries, is growing into a pressing concern for the modern times, invites us to ask ourselves whether there are limits to our dependence on these techniques – and more importantly, whether some of these limits have already been traversed. In the wave of algorithmic bias, some have argued for the creation of some kind of agency in the likes of the Food and Drug Administration, with the sole purpose of regulating algorithmic discrimination [23].\n\nWith this in mind, we propose a quantitative analysis of the phenomenon of gender bias in machine translation. We illustrate how this can be done by simply exploiting Google Translate to map sentences from a gender neutral language into English. As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and $\\it C E O$ are translated with male ones.\n\nAs of 2018, Google Translate is one of the largest publicly available machine translation tools in existence, amounting 200 million users daily[36]. Initially relying on United Nations and European Parliament transcripts to gather data, since 2014 Google Translate has inputed content from its users through the Translate Community initiative[22]. Recently however there has been a growing concern about gender asymmetries in the translation mechanism, with some heralding it as “sexist” [31]. This concern has to at least some extent a scientific backup: A recent study has shown that word embeddings are particularly prone to yielding gender stereotypes[5]. Fortunately, the researchers propose a relatively simple debiasing",
      "context_after": "algorithm with promising results: they were able to cut the proportion of stereotypical analogies from 19% to 6% without any significant compromise in the performance of the word embedding technique. They are not alone: there is a growing effort to systematically discover and resolve issues of algorithmic bias in black-box algorithms[18]. The success of these results suggest that a similar technique could be used to remove gender bias from Google Translate outputs, should it exist. This paper intends to investigate whether it does. We are optimistic that our research endeavors can be used to argue that there is a positive payoff in redesigning modern statistical translation tools.\n\n3. Assumptions and Preliminaries\n\nIn this paper we assume that a statistical translation tool should reflect at most the inequality existent in society – it is only logical that a translation tool will poll from examples that society produced and, as such, will inevitably retain some of that bias. It has been argued that one’s language affects one’s knowledge and cognition about the world [21], and this leads to the discussion that languages that distinguish between female and male genders grammatically may enforce a bias in the person’s perception of the world, with some studies",
      "referring_paragraphs": [
        "With this in mind, we propose a quantitative analysis of the phenomenon of gender bias in machine translation. We illustrate how this can be done by simply exploiting Google Translate to map sentences from a gender neutral language into English. As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer an",
        "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate. ",
        "As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and $\\it C E O$ are translated with male ones.",
        "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation.",
        "As Figure 1 clearly shows, the same template yields a male pronoun when “nurse” is replaced by “engineer”."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/e4fcd5d62aeba2df429ee96eb9f61b0581e6af843d097ef524695c63a5150ecc.jpg",
      "image_filename": "e4fcd5d62aeba2df429ee96eb9f61b0581e6af843d097ef524695c63a5150ecc.jpg",
      "caption": "4.1 for further explanation.",
      "context_before": "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate. As Figure 1 clearly shows, the same template yields a male pronoun when “nurse” is replaced by “engineer”. The same basic template can be ported to all other gender neutral languages, as depicted in Table 3. Given the success of Google Translate, which amounts to 200 million users daily, we have chosen to exploit its API to obtain the desired thermometer of gender bias. Also, in order to solidify our results, we have decided to work with a fair amount of gender neutral languages, forming a list of these with help from the World Atlas of Language Structures (WALS) [13] and other sources. Table 1 compiles all languages we chose to use, with additional columns informing whether they (1) exhibit a gender markers in the sentence and (2) are supported by Google Translate. However, we stumbled on some difficulties which led to some of those langauges being removed, which will be explained in .\n\nThere is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be expanded since each line contained multiple occupations and sometimes very specific ones. Fortunately this table also provided a percentage of women participation in the jobs shown, for those that had more than 50 thousand workers. We filtered some of these because they were too generic ( “Computer occupations, all other”, and others) or because they had gender specific words for the profession (“host/hostess”, “waiter/waitress”). We then separated the curated jobs into broader categories (Artistic, Corporate, Theatre, etc.) as shown in Table 2. Finally, Table 4 shows thirty examples of randomly selected occupations from our dataset. For\n\nthe occupations that had less than 50 thousand workers, and thus no data about the participation of women, we assumed that its women participation was that of its upper category. Finally, as complementary evidence we have decided to include a small subset of 21 adjectives in our study. All adjectives were obtained from the top one thousand most frequent words in this category as featured in the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, but it was necessary to manually curate them because a substantial fraction of these adjectives cannot be applied to human subjects. Also because the sentiment associated with each adjective is not as easily accessible as for example the occupation category of each job position, we performed a manual selection of a subset of such words which we believe to be meaningful to this study. These words are presented in Table 5. We made all code and data used to generate and compile the results presented in the following sections publicly available in the following Github repository: https://github.com/marceloprates/Gender-Bias. Note however that because the Google Translate algorithm can change, unfortunately we cannot guarantee full reproducibility of our results. All experiments reported here were conducted on April 2018.",
      "context_after": "4.1 for further explanation.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/b8adfa959ece866790a3a3c85261d4c5c6f5ddc8f326457c69012c0dead6e94c.jpg",
      "image_filename": "b8adfa959ece866790a3a3c85261d4c5c6f5ddc8f326457c69012c0dead6e94c.jpg",
      "caption": "4.1 for further explanation.",
      "context_before": "4.1 for further explanation.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/623aace85d297f2ef89881cd6eed80405fa061ebb9c33ec0f7fb12967d5607bf.jpg",
      "image_filename": "623aace85d297f2ef89881cd6eed80405fa061ebb9c33ec0f7fb12967d5607bf.jpg",
      "caption": "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate. ",
        "Plotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fiel",
        "adjectives are not applicable to human subjects, we manually curated a reasonable subset of such words. The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3.",
        "The same basic template can be ported to all other gender neutral languages, as depicted in Table 3.",
        "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/e5972c74aaa26dae21310e3352657e78fe977238057864a2475ce03e85151257.jpg",
      "image_filename": "e5972c74aaa26dae21310e3352657e78fe977238057864a2475ce03e85151257.jpg",
      "caption": "Table 4: A randomly selected example subset of thirty occupations obtained from our dataset with a total of 1019 different occupations.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "There is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be",
        "Plotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fiel",
        "Finally, Table 4 shows thirty examples of randomly selected occupations from our dataset.",
        "<table><tr><td>Language</td><td>Occupation sentence template</td><td>Adjective sentence template</td></tr><tr><td>Malay</td><td>dia adalah &lt;occupation&gt;</td><td>dia &lt;adjective&gt;</td></tr><tr><td>Estonian</td><td>ta on &lt;occupation&gt;</td><td>ta on &lt;adjective&gt;</td></tr><tr><td>Finnish</td><td>hãn on &lt;occupation&gt;</td><td>hãn on &lt;adjective&gt;</td></tr><tr><td>Hungarian</td><td>ő egy &lt;occupation&gt;</td><td>ő &lt;adjective&gt;</td></tr><tr><td>Armenian</td><td>na &lt;occupation&gt; e</td><td>na &lt;adjective&gt; e</td></tr><tr><td>Bengali</td><td>E ekajana &lt;occupation&gt;\nYini ekajana &lt;occupation&gt;\nÖ ekajana &lt;occupation&gt;\nUni ekajana &lt;occupation&gt;\nSe ekajana &lt;occupation&gt;\nTini ekajana &lt;occupation&gt;</td><td>E &lt;adjective&gt;\nYini &lt;adjective&gt;\nÖ &lt;adjective&gt;\nUni &lt;adjective&gt;\nSe &lt;adjective&gt;\nTini &lt;adjective&gt;</td></tr><tr><td>Japanese</td><td>あの人は &lt;occupation&gt;aidu</td><td>あの人は &lt;adjective&gt;aidu</td></tr><tr><td>Turkish</td><td>o bir &lt;occupation&gt;</td><td>o &lt;adjective&gt;</td></tr><tr><td>Yoruba</td><td>o je &lt;occupation&gt;</td><td>o je &lt;adjective&gt;</td></tr><tr><td>Basque</td><td>&lt;occupation&gt;bat da</td><td>&lt;adjective&gt;da</td></tr><tr><td>Swahili</td><td>yeye ni &lt;occupation&gt;</td><td>yeye ni &lt;adjective&gt;</td></tr><tr><td>Chinese</td><td>ta shi &lt;occupation&gt;</td><td>ta hen &lt;adjective&gt;</td></tr></table>\n\nTable 4: A randomly selected example subset of thirty occupations obtained from our dataset with a total of 1019 different occupations.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_6",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/81319fe4336973328dea866c2fe5e72a8491ad87fa34b0dc5ad7b79dbf180ddc.jpg",
      "image_filename": "81319fe4336973328dea866c2fe5e72a8491ad87fa34b0dc5ad7b79dbf180ddc.jpg",
      "caption": "Table 5: Curated list of 21 adjectives obtained from the top one thousand most frequent words in this category in the Corpus of Contemporary American English (COCA)",
      "context_before": "",
      "context_after": "https://corpus.byu.edu/coca/.\n\n4.1 Rationale for language exceptions\n\nWhile it is possible to construct gender neutral sentences in two of the languages omitted in our experiments (namely Korean and Nepali), we have chosen to omit them for the following reasons:",
      "referring_paragraphs": [
        "the occupations that had less than 50 thousand workers, and thus no data about the participation of women, we assumed that its women participation was that of its upper category. Finally, as complementary evidence we have decided to include a small subset of 21 adjectives in our study. All adjectives were obtained from the top one thousand most frequent words in this category as featured in the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, but it was necessary to m",
        "These words are presented in Table 5.",
        "<table><tr><td>Insurance sales agent</td><td>Editor</td><td>Rancher</td></tr><tr><td>Ticket taker</td><td>Pile-driver operator</td><td>Tool maker</td></tr><tr><td>Jeweler</td><td>Judicial law clerk</td><td>Auditing clerk</td></tr><tr><td>Physician</td><td>Embalmer</td><td>Door-to-door salesperson</td></tr><tr><td>Packer</td><td>Bookkeeping clerk</td><td>Community health worker</td></tr><tr><td>Sales worker</td><td>Floor finisher</td><td>Social science technician</td></tr><tr><td>Probation officer</td><td>Paper goods machine setter</td><td>Heating installer</td></tr><tr><td>Animal breeder</td><td>Instructor</td><td>Teacher assistant</td></tr><tr><td>Statistical assistant</td><td>Shipping clerk</td><td>Trapper</td></tr><tr><td>Pharmacy aide</td><td>Sewing machine operator</td><td>Service unit operator</td></tr></table>\n\nTable 5: Curated list of 21 adjectives obtained from the top one thousand most frequent words in this category in the Corpus of Contemporary American English (COCA)   \n\n<table><tr><td>Happy</td><td>Sad</td><td>Right</td></tr><tr><td>Wrong</td><td>Afraid</td><td>Brave</td></tr><tr><td>Smart</td><td>Dumb</td><td>Proud</td></tr><tr><td>Strong</td><td>Polite</td><td>Cruel</td></tr><tr><td>Desirable</td><td>Loving</td><td>Sympathetic</td></tr><tr><td>Modest</td><td>Successful</td><td>Guilty</td></tr><tr><td>Innocent</td><td>Mature</td><td>Shy</td></tr></table>\n\nhttps://corpus.byu.edu/coca/.",
        "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).",
        "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_7",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/04fbac2c00caa23f23399a378c2dedb61b0c183bf0d8231e222b7e5a7f916ac2.jpg",
      "image_filename": "04fbac2c00caa23f23399a378c2dedb61b0c183bf0d8231e222b7e5a7f916ac2.jpg",
      "caption": "Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
      "context_before": "5. Distribution of translated gender pronouns per occupation category\n\nA sensible way to group translation data is to coalesce occupations in the same category and collect statistics among languages about how prominent male defaults are in each field. What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics [29]. Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation. For instance, STEM (Science, Technology, Engineering and Mathematics) fields are grouped into a single category, which helps us compare the large asymmetry between gender pro-\n\nnouns in these fields (72% of male defaults) to that of more evenly distributed fields such as healthcare (50%).",
      "context_after": "1. Note that rows do not in general add up to 100%, as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.",
      "referring_paragraphs": [
        "A sensible way to group translation data is to coalesce occupations in the same category and collect statistics among languages about how prominent male defaults are in each field. What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physic",
        "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_8",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/4826cc8b1e129e96519312a6c4e2b74e8e9b19146e4acf4238f078f5a3b860bf.jpg",
      "image_filename": "4826cc8b1e129e96519312a6c4e2b74e8e9b19146e4acf4238f078f5a3b860bf.jpg",
      "caption": "Table 7: Percentage of female, male and neutral gender pronouns obtained for each of the merged occupation category, averaged over all occupations in said category and tested languages detailed in Table",
      "context_before": "1. Note that rows do not in general add up to 100%, as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.",
      "context_after": "1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.\n\nPlotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fields (labeled in beige exhibit predominantly male defaults – amounting predominantly near $X = 0$ in the female histogram although much to the right in the male histogram.\n\nThese values contrast with BLS’ report of gender participation, which will be discussed in more detail in Section 8.",
      "referring_paragraphs": [
        "A sensible way to group translation data is to coalesce occupations in the same category and collect statistics among languages about how prominent male defaults are in each field. What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physic",
        "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns. In this context, STEM fields, which show a predominance of male defaults, are contrasted with Healthcare and educations, which show a larger proportion of female pronouns.",
        "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.",
        "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_9",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg",
      "image_filename": "1809.02208_page0_fig1.jpg",
      "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
      "context_before": "1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.\n\nPlotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fields (labeled in beige exhibit predominantly male defaults – amounting predominantly near $X = 0$ in the female histogram although much to the right in the male histogram.\n\nThese values contrast with BLS’ report of gender participation, which will be discussed in more detail in Section 8.",
      "context_after": "",
      "referring_paragraphs": [
        "There is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be",
        "Plotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fiel",
        "We can also visualize male, female, and gender neutral histograms side by side, in which context is useful to compare the dissimilar distributions of translated STEM and Healthcare occupations (Figures 5 and 6 respectively). The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to m",
        "The U.S. Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13. The data shows us that Google Translat",
        "We then separated the curated jobs into broader categories (Artistic, Corporate, Theatre, etc.) as shown in Table 2.",
        "<table><tr><td>Category</td><td>Group</td><td># Occupations</td><td>Female Partici-pation</td></tr><tr><td>Education, training, and library</td><td>Education</td><td>22</td><td>73.0%</td></tr><tr><td>Business and financial operations</td><td>Corporate</td><td>46</td><td>54.0%</td></tr><tr><td>Office and administra-tive support</td><td>Service</td><td>87</td><td>72.2%</td></tr><tr><td>Healthcare support</td><td>Healthcare</td><td>16</td><td>87.1%</td></tr><tr><td>Management</td><td>Corporate</td><td>46</td><td>39.8%</td></tr><tr><td>Installation, maintenance, and repair</td><td>Service</td><td>91</td><td>4.0%</td></tr><tr><td>Healthcare practitioners and technical</td><td>Healthcare</td><td>43</td><td>75.0%</td></tr><tr><td>Community and social service</td><td>Service</td><td>14</td><td>66.1%</td></tr><tr><td>Sales and related</td><td>Corporate</td><td>28</td><td>49.1%</td></tr><tr><td>Production</td><td>Production</td><td>264</td><td>28.9%</td></tr><tr><td>Architecture and engineering</td><td>STEM</td><td>29</td><td>16.2%</td></tr><tr><td>Life, physical, and so-cial science</td><td>STEM</td><td>34</td><td>47.4%</td></tr><tr><td>Transportation and material moving</td><td>Service</td><td>70</td><td>17.3%</td></tr><tr><td>Arts, design, entertainment, sports, and media</td><td>Arts / Entertainment</td><td>37</td><td>46.9%</td></tr><tr><td>Legal</td><td>Legal</td><td>7</td><td>52.8%</td></tr><tr><td>Protective Service</td><td>Service</td><td>28</td><td>22.3%</td></tr><tr><td>Food preparation and serving related</td><td>Service</td><td>17</td><td>53.8%</td></tr><tr><td>Farming, fishing, and forestry</td><td>Farming / Fishing / Forestry</td><td>13</td><td>23.4%</td></tr><tr><td>Computer and mathematical</td><td>STEM</td><td>16</td><td>25.5%</td></tr><tr><td>Personal care and service</td><td>Service</td><td>33</td><td>76.1%</td></tr><tr><td>Construction and extraction</td><td>Construction / Extraction</td><td>68</td><td>3.0%</td></tr><tr><td>Building and grounds cleaning and maintenance</td><td>Service</td><td>10</td><td>40.7%</td></tr><tr><td>Total</td><td>-</td><td>1019</td><td>41.3%</td></tr></table>\n\nTable 2: Selected occupations obtained from the U.S.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
        "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to most prominent.",
        "Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_10",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg",
      "image_filename": "1809.02208_page0_fig2.jpg",
      "caption": "7) extends to higher values. Figure 3: In contrast to Figure 2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian “˝o egy ´apol´on˝o”, where “´apol´on˝o” translates to “nurse” and “˝o” is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result “she’s a nurse” on Google Translate. ",
        "Plotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fiel",
        "adjectives are not applicable to human subjects, we manually curated a reasonable subset of such words. The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3.",
        "The same basic template can be ported to all other gender neutral languages, as depicted in Table 3.",
        "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_11",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg",
      "image_filename": "1809.02208_page0_fig3.jpg",
      "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .",
      "context_before": "",
      "context_after": "We can also visualize male, female, and gender neutral histograms side by side, in which context is useful to compare the dissimilar distributions of translated STEM and Healthcare occupations (Figures 5 and 6 respectively). The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to most prominent.",
      "referring_paragraphs": [
        "There is a prohibitively large class of nouns and adjectives that could in principle be substituted into our templates. To simplify our dataset, we have decided to focus our work on job positions – which, we believe, are an interesting window into the nature of gender bias –, and were able to obtain a comprehensive list of professional occupations from the Bureau of Labor Statistics’ detailed occupations table [7], from the United States Department of Labor. The values inside, however, had to be",
        "Plotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fiel",
        "Finally, Table 4 shows thirty examples of randomly selected occupations from our dataset.",
        "<table><tr><td>Language</td><td>Occupation sentence template</td><td>Adjective sentence template</td></tr><tr><td>Malay</td><td>dia adalah &lt;occupation&gt;</td><td>dia &lt;adjective&gt;</td></tr><tr><td>Estonian</td><td>ta on &lt;occupation&gt;</td><td>ta on &lt;adjective&gt;</td></tr><tr><td>Finnish</td><td>hãn on &lt;occupation&gt;</td><td>hãn on &lt;adjective&gt;</td></tr><tr><td>Hungarian</td><td>ő egy &lt;occupation&gt;</td><td>ő &lt;adjective&gt;</td></tr><tr><td>Armenian</td><td>na &lt;occupation&gt; e</td><td>na &lt;adjective&gt; e</td></tr><tr><td>Bengali</td><td>E ekajana &lt;occupation&gt;\nYini ekajana &lt;occupation&gt;\nÖ ekajana &lt;occupation&gt;\nUni ekajana &lt;occupation&gt;\nSe ekajana &lt;occupation&gt;\nTini ekajana &lt;occupation&gt;</td><td>E &lt;adjective&gt;\nYini &lt;adjective&gt;\nÖ &lt;adjective&gt;\nUni &lt;adjective&gt;\nSe &lt;adjective&gt;\nTini &lt;adjective&gt;</td></tr><tr><td>Japanese</td><td>あの人は &lt;occupation&gt;aidu</td><td>あの人は &lt;adjective&gt;aidu</td></tr><tr><td>Turkish</td><td>o bir &lt;occupation&gt;</td><td>o &lt;adjective&gt;</td></tr><tr><td>Yoruba</td><td>o je &lt;occupation&gt;</td><td>o je &lt;adjective&gt;</td></tr><tr><td>Basque</td><td>&lt;occupation&gt;bat da</td><td>&lt;adjective&gt;da</td></tr><tr><td>Swahili</td><td>yeye ni &lt;occupation&gt;</td><td>yeye ni &lt;adjective&gt;</td></tr><tr><td>Chinese</td><td>ta shi &lt;occupation&gt;</td><td>ta hen &lt;adjective&gt;</td></tr></table>\n\nTable 4: A randomly selected example subset of thirty occupations obtained from our dataset with a total of 1019 different occupations.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_12",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg",
      "image_filename": "1809.02208_page0_fig4.jpg",
      "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).",
      "context_before": "We can also visualize male, female, and gender neutral histograms side by side, in which context is useful to compare the dissimilar distributions of translated STEM and Healthcare occupations (Figures 5 and 6 respectively). The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to most prominent.",
      "context_after": "",
      "referring_paragraphs": [
        "the occupations that had less than 50 thousand workers, and thus no data about the participation of women, we assumed that its women participation was that of its upper category. Finally, as complementary evidence we have decided to include a small subset of 21 adjectives in our study. All adjectives were obtained from the top one thousand most frequent words in this category as featured in the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, but it was necessary to m",
        "These words are presented in Table 5.",
        "<table><tr><td>Insurance sales agent</td><td>Editor</td><td>Rancher</td></tr><tr><td>Ticket taker</td><td>Pile-driver operator</td><td>Tool maker</td></tr><tr><td>Jeweler</td><td>Judicial law clerk</td><td>Auditing clerk</td></tr><tr><td>Physician</td><td>Embalmer</td><td>Door-to-door salesperson</td></tr><tr><td>Packer</td><td>Bookkeeping clerk</td><td>Community health worker</td></tr><tr><td>Sales worker</td><td>Floor finisher</td><td>Social science technician</td></tr><tr><td>Probation officer</td><td>Paper goods machine setter</td><td>Heating installer</td></tr><tr><td>Animal breeder</td><td>Instructor</td><td>Teacher assistant</td></tr><tr><td>Statistical assistant</td><td>Shipping clerk</td><td>Trapper</td></tr><tr><td>Pharmacy aide</td><td>Sewing machine operator</td><td>Service unit operator</td></tr></table>\n\nTable 5: Curated list of 21 adjectives obtained from the top one thousand most frequent words in this category in the Corpus of Contemporary American English (COCA)   \n\n<table><tr><td>Happy</td><td>Sad</td><td>Right</td></tr><tr><td>Wrong</td><td>Afraid</td><td>Brave</td></tr><tr><td>Smart</td><td>Dumb</td><td>Proud</td></tr><tr><td>Strong</td><td>Polite</td><td>Cruel</td></tr><tr><td>Desirable</td><td>Loving</td><td>Sympathetic</td></tr><tr><td>Modest</td><td>Successful</td><td>Guilty</td></tr><tr><td>Innocent</td><td>Mature</td><td>Shy</td></tr></table>\n\nhttps://corpus.byu.edu/coca/.",
        "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).",
        "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_13",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg",
      "image_filename": "1809.02208_page0_fig5.jpg",
      "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.",
      "context_before": "",
      "context_after": "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns. In this context, STEM fields, which show a predominance of male defaults, are contrasted with Healthcare and educations, which show a larger proportion of female pronouns.",
      "referring_paragraphs": [
        "A sensible way to group translation data is to coalesce occupations in the same category and collect statistics among languages about how prominent male defaults are in each field. What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physic",
        "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_14",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg",
      "image_filename": "1809.02208_page0_fig6.jpg",
      "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively",
      "context_before": "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns. In this context, STEM fields, which show a predominance of male defaults, are contrasted with Healthcare and educations, which show a larger proportion of female pronouns.",
      "context_after": "Although computing our statistics over the set of all languages has practical value, this may erase subtleties characteristic to each individual idiom. In this context, it is also important to visualize how each language translates job occupations in each category. The heatmaps in Figures 8, 9 and 10 show the translation probabilities into female, male and neutral pronouns, respectively, for each pair of language and category (blue is 0% and red is 100%). Both axes are sorted in these Figures, which helps us visualize both languages and categories in an spectrum of increasing male/female/neutral translation tendencies. In\n\nagreement with suggested stereotypes, [29] STEM fields are second only to Legal ones in the prominence of male defaults. These two are followed by Arts & Entertainment and Corporate, in this order, while Healthcare, Production and Education lie on the opposite end of the spectrum.",
      "referring_paragraphs": [
        "A sensible way to group translation data is to coalesce occupations in the same category and collect statistics among languages about how prominent male defaults are in each field. What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physic",
        "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns. In this context, STEM fields, which show a predominance of male defaults, are contrasted with Healthcare and educations, which show a larger proportion of female pronouns.",
        "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.",
        "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_15",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg",
      "image_filename": "1809.02208_page0_fig7.jpg",
      "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
      "context_before": "Although computing our statistics over the set of all languages has practical value, this may erase subtleties characteristic to each individual idiom. In this context, it is also important to visualize how each language translates job occupations in each category. The heatmaps in Figures 8, 9 and 10 show the translation probabilities into female, male and neutral pronouns, respectively, for each pair of language and category (blue is 0% and red is 100%). Both axes are sorted in these Figures, which helps us visualize both languages and categories in an spectrum of increasing male/female/neutral translation tendencies. In\n\nagreement with suggested stereotypes, [29] STEM fields are second only to Legal ones in the prominence of male defaults. These two are followed by Arts & Entertainment and Corporate, in this order, while Healthcare, Production and Education lie on the opposite end of the spectrum.",
      "context_after": "",
      "referring_paragraphs": [
        "Our analysis is not truly complete without tests for statistical significant differences in the translation tendencies among female, male and gender neutral pronouns. We want to know for which languages and categories does Google Translate translate sentences with significantly more male than female, or male than neutral, or neutral than female, pronouns. We ran one-sided t-tests to assess this question for each pair of language and category and also totaled among either languages or categories.",
        "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
        "Because of this, Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue (see Table 8 for the three examples cited above."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_16",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig8.jpg",
      "image_filename": "1809.02208_page0_fig8.jpg",
      "caption": "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
        "<table><tr><td></td><td>Mal.</td><td>Est.</td><td>Fin.</td><td>Hun.</td><td>Arm.</td><td>Ben.</td><td>Jap.</td><td>Tur.</td><td>Yor.</td><td>Bas.</td><td>Swa.</td><td>Chi.</td><td>Total</td></tr><tr><td>Service</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>STEM</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.984</td><td>&lt;α</td><td>.07</td><td>&lt;α</td></tr><tr><td>Farming</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">.135</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">.068</td><td rowspan=\"3\">1.0</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td></tr><tr><td>Fishing</td></tr><tr><td>Forestry</td></tr><tr><td>Corporate</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Healthcare</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.39</td><td>1.0</td><td>&lt;α</td><td>.088</td><td>&lt;α</td></tr><tr><td>Legal</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.145</td><td>&lt;α</td><td>&lt;α</td><td>.771</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Arts Entertainment</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.07</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Education</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.093</td><td>&lt;α</td><td>&lt;α</td><td>.5</td><td>&lt;α</td><td>.068</td><td>&lt;α</td></tr><tr><td>Production</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.412</td><td>1.0</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Construction</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">.92</td><td rowspan=\"2\">1.0</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td></tr><tr><td>Extraction</td></tr><tr><td>Total</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr></table>\n\nTable 9: Computed p-values relative to the null hypothesis that the number of translated male pronouns is not significantly greater than that of gender neutral pronouns, organized for each language and each occupation category."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_17",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg",
      "image_filename": "1809.02208_page0_fig9.jpg",
      "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
      "context_before": "",
      "context_after": "Our analysis is not truly complete without tests for statistical significant differences in the translation tendencies among female, male and gender neutral pronouns. We want to know for which languages and categories does Google Translate translate sentences with significantly more male than female, or male than neutral, or neutral than female, pronouns. We ran one-sided t-tests to assess this question for each pair of language and category and also totaled among either languages or categories. The corresponding p-values are presented in Tables 8, 9, 10 respectively. Language-Category pairs for which the null hypothesis was not rejected for a confidence level of $\\alpha = . 0 0 5$ are highlighted in blue. It is important to note that when the null hypothesis is accepted, we cannot discard the possibility of the complementary null hypothesis being rejected. For example, neither male nor female pronouns are significantly more common for Healthcare positions in the Estonian language, but female pronouns are significantly more common for the same category in Finnish and Hungarian. Because of this, Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue (see Table 8 for the three examples cited above.\n\nAlthough there is a noticeable level of variation among languages and categories, the null hypothesis that male pronouns are not significantly more frequent than female ones was\n\nconsistently rejected for all languages and all categories examined. The same is true for the null hypothesis that male pronouns are not significantly more frequent than gender neutral pronouns, with the one exception of the Basque language (which exhibits a rather strong tendency towards neutral pronouns). The null hypothesis that neutral pronouns are not significantly more frequent than female ones is accepted with much more frequency, namely for the languages Malay, Estonian, Finnish, Hungarian, Armenian and for the categories Farming & Fishing & Forestry, Healthcare, Legal, Arts & Entertainment, Education. In all three cases, the null hypothesis corresponding to the aggregate for all languages and categories is rejected. We can learn from this, in summary, that Google Translate translates male pronouns more frequently than both female and gender neutral ones, either in general for Language-Category pairs or consistently among languages and among categories (with the notable exception of the Basque idiom).",
      "referring_paragraphs": [
        "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/d32da5160d0e9b3737e7352fb7a55ebd2b9e655c087317954ecf91763cd72d53.jpg",
      "image_filename": "d32da5160d0e9b3737e7352fb7a55ebd2b9e655c087317954ecf91763cd72d53.jpg",
      "caption": "consistently rejected for all languages and all categories examined.",
      "context_before": "Our analysis is not truly complete without tests for statistical significant differences in the translation tendencies among female, male and gender neutral pronouns. We want to know for which languages and categories does Google Translate translate sentences with significantly more male than female, or male than neutral, or neutral than female, pronouns. We ran one-sided t-tests to assess this question for each pair of language and category and also totaled among either languages or categories. The corresponding p-values are presented in Tables 8, 9, 10 respectively. Language-Category pairs for which the null hypothesis was not rejected for a confidence level of $\\alpha = . 0 0 5$ are highlighted in blue. It is important to note that when the null hypothesis is accepted, we cannot discard the possibility of the complementary null hypothesis being rejected. For example, neither male nor female pronouns are significantly more common for Healthcare positions in the Estonian language, but female pronouns are significantly more common for the same category in Finnish and Hungarian. Because of this, Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue (see Table 8 for the three examples cited above.\n\nAlthough there is a noticeable level of variation among languages and categories, the null hypothesis that male pronouns are not significantly more frequent than female ones was\n\nconsistently rejected for all languages and all categories examined. The same is true for the null hypothesis that male pronouns are not significantly more frequent than gender neutral pronouns, with the one exception of the Basque language (which exhibits a rather strong tendency towards neutral pronouns). The null hypothesis that neutral pronouns are not significantly more frequent than female ones is accepted with much more frequency, namely for the languages Malay, Estonian, Finnish, Hungarian, Armenian and for the categories Farming & Fishing & Forestry, Healthcare, Legal, Arts & Entertainment, Education. In all three cases, the null hypothesis corresponding to the aggregate for all languages and categories is rejected. We can learn from this, in summary, that Google Translate translates male pronouns more frequently than both female and gender neutral ones, either in general for Language-Category pairs or consistently among languages and among categories (with the notable exception of the Basque idiom).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/3840f36b5aa7a8b0b871c08872b8a33ea48cd48a3fb936cd6c499a0c40d35192.jpg",
      "image_filename": "3840f36b5aa7a8b0b871c08872b8a33ea48cd48a3fb936cd6c499a0c40d35192.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/53110b64a93ccee18cf9412e9a2065316cc38f1aec228578f305b9f9b21fa715.jpg",
      "image_filename": "53110b64a93ccee18cf9412e9a2065316cc38f1aec228578f305b9f9b21fa715.jpg",
      "caption": "6. Distribution of translated gender pronouns per language",
      "context_before": "",
      "context_after": "6. Distribution of translated gender pronouns per language\n\nWe have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias. Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good example of this difficulty, although the quality of Bengali, Yoruba, Chinese and Turkish translations are also compromised.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_21",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/7e3bb1e6eb1a50335176c50ab9ce8de2e9a8e05c472cc36ae6876a0435e4b9d1.jpg",
      "image_filename": "7e3bb1e6eb1a50335176c50ab9ce8de2e9a8e05c472cc36ae6876a0435e4b9d1.jpg",
      "caption": "Table 11: Percentage of female, male and neutral gender pronouns obtained for each language, averaged over all occupations detailed in Table",
      "context_before": "6. Distribution of translated gender pronouns per language\n\nWe have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias. Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good example of this difficulty, although the quality of Bengali, Yoruba, Chinese and Turkish translations are also compromised.",
      "context_after": "1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.",
      "referring_paragraphs": [
        "We have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias. Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good ex",
        "Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages.",
        "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.",
        "61–79 (2003)   \n[7] Bureau of Labor Statistics: ”Table 11: Employed persons by detailed occupation, sex, race, and Hispanic or Latino ethnicity, 2017”."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_22",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg",
      "image_filename": "1809.02208_page0_fig10.jpg",
      "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.",
      "context_before": "1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.",
      "context_after": "7. Distribution of translated gender pronouns for varied adjectives\n\nWe queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus [https://corpus.byu.edu/coca/], but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being. The list of adjectives extracted from the corpus is available on the Github repository: https://github.com/marceloprates/Gender-Bias.\n\nApart from occupations, which we have exhaustively examined by collecting labor data from the U.S. Bureau of Labor Statistics, we have also selected a small subset of adjectives from the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, in an attempt to provide preliminary evidence that the phenomenon of gender bias may extend beyond the professional context examined in this paper. Because a large number of",
      "referring_paragraphs": [
        "We have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias. Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good ex",
        "Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages.",
        "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.",
        "61–79 (2003)   \n[7] Bureau of Labor Statistics: ”Table 11: Employed persons by detailed occupation, sex, race, and Hispanic or Latino ethnicity, 2017”."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_23",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1a2bbcd64ab14b2bc747061b7da51a480e65b9a85b12d9dc6eb00c5c425b5896.jpg",
      "image_filename": "1a2bbcd64ab14b2bc747061b7da51a480e65b9a85b12d9dc6eb00c5c425b5896.jpg",
      "caption": "Table 12: Number of female, male and neutral pronominal genders in the translated sentences for each selected adjective.",
      "context_before": "Apart from occupations, which we have exhaustively examined by collecting labor data from the U.S. Bureau of Labor Statistics, we have also selected a small subset of adjectives from the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, in an attempt to provide preliminary evidence that the phenomenon of gender bias may extend beyond the professional context examined in this paper. Because a large number of\n\nadjectives are not applicable to human subjects, we manually curated a reasonable subset of such words. The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3.\n\nOnce again the data points towards male defaults, but some variation can be observed throughout different adjectives. Sentences containing the words Shy, Attractive, Happy, Kind and Ashamed are predominantly female translated (Attractive is translated as female and gender-neutral in equal parts), while Arrogant, Cruel and Guilty are disproportionately translated with male pronouns (Guilty is in fact never translated with female or neutral pronouns).",
      "context_after": "",
      "referring_paragraphs": [
        "Table 12: Number of female, male and neutral pronominal genders in the translated sentences for each selected adjective.",
        "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_24",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg",
      "image_filename": "1809.02208_page0_fig11.jpg",
      "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.",
      "context_before": "",
      "context_after": "8. Comparison with women participation data across job positions\n\nA sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those that have less than 25% of women participation[40], according to the U.S. Department of Labor Women’s Bureau). In this context, the argument in favor of a critical revision of statistic translation algorithms weakens considerably, and possibly shifts the blame away from these tools.\n\nThe U.S. Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13. The data shows us that Google Translate outputs fail to follow the real-world distribution of female workers across a comprehensive set of job positions. The",
      "referring_paragraphs": [
        "Table 12: Number of female, male and neutral pronominal genders in the translated sentences for each selected adjective.",
        "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_25",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg",
      "image_filename": "1809.02208_page0_fig12.jpg",
      "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.",
      "context_before": "A sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those that have less than 25% of women participation[40], according to the U.S. Department of Labor Women’s Bureau). In this context, the argument in favor of a critical revision of statistic translation algorithms weakens considerably, and possibly shifts the blame away from these tools.\n\nThe U.S. Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13. The data shows us that Google Translate outputs fail to follow the real-world distribution of female workers across a comprehensive set of job positions. The\n\ndistribution of translated female pronouns is consistently inversely distributed, with female pronouns accumulating in the first 12-quantile. By contrast, BLS data shows that female participation peaks in the fourth 12-quantile and remains significant throughout the next ones.",
      "context_after": "Averaged over occupations and languages, sentences are translated with female pronouns $1 1 . 7 6 \\%$ of the time. In contrast, the gender participation frequency for female workers averaged over all occupations in the BLS report yields a consistently larger figure of $3 5 . 9 4 \\%$ . The variance reported for the translation results is also lower, at $\\approx 0 . 0 2 8$ in contrast with the report’s $\\approx 0 . 0 6 7$ . We ran an one-sided t-test to evaluate the null hypothesis that the female participation frequency is not significantly greater then the GT female pronoun frequency for the same job positions, obtaining a p-value $p \\approx 6 . 2 1 0 ^ { - 9 4 }$ vastly inferior to our confidence level of $\\alpha = 0 . 0 0 5$ and thus rejecting H0 and concluding that Google Translate’s female translation frequencies sub-estimates female participation frequencies in US job positions.\n\nAs a result, it is not possible to understand this asymmetry as a reflection of workplace demographics, and the prominence of male defaults in Google Translate is, we believe, yet lacking a clear justification.\n\nAt the time of the writing up this paper, Google Translate offered only one official translation for each input word, along with a list of synonyms. In this context, all experiments reported here offer an analysis of a “screenshot” of that tool as of August 2018, the moment they were carried out. A preprint version of this paper was posted the in well-known Cornell University-based arXiv.org open repository on September 6, 2018. The manuscript soon enjoyed a significant amount of media coverage, featuring on The Register [10], Datanews [3], t3n [33], among others, and more recently on Slator [12] and Jornal do Comercio [24]. On December 6, 2018 the company’s policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation (Figure 14). According to the company, this decision is part of a broader goal of promoting fairness and reducing biases in machine learning. They also acknowledged the technical reasons behind gender bias in their model, stating that:",
      "referring_paragraphs": [
        "The U.S. Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13. The data shows us that Google Translat",
        "We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13.",
        "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg",
      "image_filename": "1809.02208_page0_fig13.jpg",
      "caption": "In addition, one should note that further evidence is mounting about the kind of bias examined in this paper: it is becoming clear that this is a statistical phenomenon independent from any proprietary tool.",
      "context_before": "Their statement is very similar to the conclusions drawn on this paper, as is their motivation for redesigning the tool. As authors, we are incredibly happy to see our vision and beliefs align with those of Google in such a short timespan from the initial publishing of our work, although the company’s statement does not cite any study or report in particular and thus we cannot know for sure whether this paper had an effect on their decision or not. Regardless of whether their decision was monocratic, guided by public opinion or based on published research, we understand it as an important first step on an ongoing fight against algorithmic bias, and we praise the Google Translate team for their efforts.\n\nGoogle Translate’s new feminine and masculine forms for translated sentences exemplifies how, as this paper also suggests, machine learning translation tools can be debiased, dropping the need for resorting to a balanced training set. However, it should be noted that important as it is, GT’s new feature is still a first step. It does not address all of the shortcomings described in this paper, and the limited language coverage means that many users will still experience gender biased translation results. Furthermore, the system does not yet have support for non-binary results, which may exclude part of their user base.\n\nIn addition, one should note that further evidence is mounting about the kind of bias examined in this paper: it is becoming clear that this is a statistical phenomenon independent from any proprietary tool. In this context, the research carried out in [5] presents a very convincing argument for the sensitivity of word embeddings to gender bias in the training",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_27",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig14.jpg",
      "image_filename": "1809.02208_page0_fig14.jpg",
      "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.",
      "context_before": "",
      "context_after": "dataset. This suggests that machine translation engineers should be especially aware of their training data when designing a system. It is not feasible to train these models on unbiased texts, as they are probably scarce. What must be done instead is to engineer solutions to remove bias from the system after an initial training, which seems to be the goal of Google Translate’s recent efforts. Fortunately, as [5] also show, debiasing can be implemented with relatively low effort and modest resources. The technology to promote social justice on machine translation in particular and machine learning in general is often already available. The most significant effort which must be taken in this context is to promote social awareness on these issues so that society can be invited into the conversation.\n\nIn this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults. Although implicit, these biases possibly stem from the real world data which is used to train them, and in this context possibly provide a window into the way our society talks (and writes) about women in the workplace. In this paper, we suggest that and test the hypothesis that statistical translation tools can be probed to yield insights about stereotypical gender roles in our society – or at least in their training data. By translating professional-related sentences such as “He/She is an engineer” from gender neutral languages such as Hungarian\n\nand Chinese into English, we were able to collect statistics about the asymmetry between female and male pronominal genders in the translation outputs. Our results show that male defaults are not only prominent, but exaggerated in fields suggested to be troubled with gender stereotypes, such as STEM (Science, Technology, Engineering and Mathematics) occupations. And because Google Translate typically uses English as a lingua franca to translate between other languages (e.g. Chinese English Portuguese) [16, 4], our findings possibly extend to translations between gender neutral languages and non-gender neutral languages (apart from English) in general, although we have not tested this hypothesis.",
      "referring_paragraphs": [
        "At the time of the writing up this paper, Google Translate offered only one official translation for each input word, along with a list of synonyms. In this context, all experiments reported here offer an analysis of a “screenshot” of that tool as of August 2018, the moment they were carried out. A preprint version of this paper was posted the in well-known Cornell University-based arXiv.org open repository on September 6, 2018. The manuscript soon enjoyed a significant amount of media coverage,",
        "On December 6, 2018 the company’s policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation (Figure 14).",
        "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02208_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1809.02244": [
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg",
      "image_filename": "1809.02244_page0_fig0.jpg",
      "caption": "(a)",
      "context_before": "Nabi & Shpitser (2018) argue that fair inference for prediction requires imposing hard constraints on the prediction problem, in the form of restricting certain path-specific effects. We adapt this approach to optimal sequential decision-making. A feature of this approach is that the relevant restrictions are user-specified and context-specific; thus we will generally require input from policymakers, legal experts, bioethicists, or the general public in applied settings. Which pathways may be considered impermissible depends on the domain and the semantics of the variables involved. We do not defend this perspective on fairness here for lack of space; please see Nabi & Shpitser (2018) for more details.\n\nWe summarize the proposal from Nabi & Shpitser (2018) with a brief example, inspired by the aforementioned child welfare case. Consider a simple causal model for this scenario, shown in Fig. 1(b). Hotline operators receive thousands of calls per year, and must decide on an action $A$ for each call, e.g., whether or not to send a caseworker. These decisions are made on the basis of a (high-dimensional) vectors of covariates $X$ and $M$ , as well as possibly sensitive features $S$ , such as race. $M$ consists of mediators of the effect of $S$ on $A$ . $Y_{1}$ corresponds to an indicator for whether the child is separated from their family by child protective services, and $Y_{2}$ corresponds to child hospitalization (presumably attributed to domestic abuse or neglect). The observed joint distribution generated by this causal model\n\n[Section: Learning Optimal Fair Policies]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig1.jpg",
      "image_filename": "1809.02244_page0_fig1.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig2.jpg",
      "image_filename": "1809.02244_page0_fig2.jpg",
      "caption": "(c) Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .",
      "context_before": "",
      "context_after": "would be $p(Y_1, Y_2, A, M, S, X)$ . The proposal from Nabi & Shpitser (2018) is that fairness corresponds to the impermissibility of certain path-specific effects, and so fair inference requires decisions to be made from a counterfactual distribution $p^*(Y_1, Y_2, A, M, S, X)$ which is \"nearby\" to $p$ (in the sense of minimal Kullback-Leibler divergence) but where these PSEs are constrained to be zero. They call $p^*$ the distribution generated by a \"fair world.\"\n\nMultiple fairness concerns have been raised by experts and advocates in discussions of the child protection decision-making process (Chouldchova et al., 2018; Hurley, 2018). For example, it is clearly impermissible that race has any direct effect on the decision made by the hotline screener, i.e., that all else being held fixed, members from one group have a higher probability of being surveilled by the agency. However, it is perhaps permissible that race has an indirect effect via some mediated pathway, e.g., if race is associated with some behaviors or features which themselves ought to be taken into consideration by hotline staffers, because they are predictive of abuse. If that's true, then $S \\rightarrow A$ would be labeled an impermissible pathway whereas $S \\rightarrow M \\rightarrow A$ (for some $M$ ) would be permissible. Similarly, it would be unacceptable if race had an effect on whether children are separated from their families; arguably both the direct pathway $S \\rightarrow Y_{1}$ and indirect pathway though hotline decisions $S \\rightarrow A \\rightarrow Y_{1}$ should be considered impermissible. Rather than defend any particular choice of path-specific constraints, we note that the framework outlined in Nabi & Shpitser (2018) can flexibly accommodate any set of given constraints, as long as the PSEs are identifiable from the observed distribution.\n\n3.1. Inference in a nearby \"fair world\"",
      "referring_paragraphs": [
        "The goal of policy learning is to find policies that map vectors in $\\mathcal{H}_k$ to values in $\\mathcal{A}_k$ (for all $k$ ) that maximize the expected value of outcome $Y$ . In offline settings, where exploration by direct experimentation is impossible, finding such policies requires reasoning counterfactually, as is common in causal inference. The value of $Y$ under an assignment of value $a$ to variable $A$ is called a potential outcome variable, denoted $Y(a)$ . In causal inference, quant",
        "A causal parameter is said to be identified in a causal model if it is a function of the observed data distribution $p(Z)$ . In causal DAGs, distributions of potential outcomes are identified by the $g$ -formula. For background on general identification theory, see Shpitser (2018). As an example, the distribution of $Y(a)$ in the DAG in Fig. 1(a) is identified by $\\sum_{X,M} p(Y|a, M, X)p(M|a, X)p(X)$ . Note that some causal parameters may be identified even in causal models with hidden (\"latent",
        "As an example, $Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$ , and its distribution is identified as $\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$ .",
        "We summarize the proposal from Nabi & Shpitser (2018) with a brief example, inspired by the aforementioned child welfare case. Consider a simple causal model for this scenario, shown in Fig. 1(b). Hotline operators receive thousands of calls per year, and must decide on an action $A$ for each call, e.g., whether or not to send a caseworker. These decisions are made on the basis of a (high-dimensional) vectors of covariates $X$ and $M$ , as well as possibly sensitive features $S$ , such as race. ",
        "Consider a $K$ -stage decision problem given by a DAG where every vertex pair is connected, and with vertices in a topological order $X, S, M, A_1, Y_1, \\ldots, A_K, Y_K$ . See Fig. 1(c). Note that the setting where $S$ can be assumed exogenous is a special case of this model with missing edge between $X$ and $S$ . Though we only assume a single set of permissible mediators $\\bar{M}$ here, at the expense of some added cumbersome notation all of the following can be extended to the case where the",
        "Theorem 2 Consider the $K$ -stage decision problem described by the DAG in Fig. 1(c). Let $p^*(M|S, X; \\alpha_m)$ and $p^*(S|X; \\alpha_s)$ be the constrained models chosen to satisfy $PSE^{sy} = 0$ and $PSE^{sa_k} = 0$ . Let $\\tilde{p}(Z)$ be the joint distribution induced by $p^*(M|S, X; \\alpha_m)$ and $p^*(S|X; \\alpha_s)$ , and where all other distributions in the factorization are unrestricted. That is,",
        "Note that at each stage $k$ , the identity $Q_{k}(H_{k},A_{k}) = \\mathbb{E}[V_{k + 1}(H_{k + 1},A_{k})\\mid H_{k}] = \\mathbb{E}[V_{k + 1}(H_{k + 1})\\mid A_{k},H_{k}]$ only holds under our causal model if the entire past $H_{k}$ is conditioned on. In particular, $\\mathbb{E}[V_{k + 1}(H_{k + 1},A_k)\\mid$ $H_{k}\\setminus \\{M,S\\} ]\\neq \\mathbb{E}[V_{k + 1}(H_{k + 1})\\mid A_k,H_k\\setminus \\{M,S\\} ]$ . To see a simple example of this, note that $Y_{K}(a_{1})$ is not independent of $A_{1}$ conditional o",
        "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$ , where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$ . Details on the specific models used are reported in the Supplement. We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$ .",
        "A third method for estimating policies is to directly model the counterfactual contrasts known as optimal blip-to-zero functions and then learn these functions by g-estimation (Robins, 2004); see Appendix A. We implemented our modified fair g-estimation for a single-stage decision problem and compared the results with Q-learning and value search. The results are provided in Table 1. The data generating process for the single-stage decision problem matches the causal model shown in Fig. ??(a) whe",
        "Figure 1.",
        "Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg",
      "image_filename": "1809.02244_page0_fig3.jpg",
      "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "context_before": "COMPAS is a criminal justice risk assessment tool created by the company Northpointe that has been used across the US to determine whether to release or detain a defendant before their trial. Each pretrial defendant receives several COMPAS scores based on factors including but not limited to demographics, criminal history, family history, and social status. Among these scores, we are primarily interested in the \"risk of recidivism.\" We use the data made available by Propublica and described in Angwin et al. (2016). The COMPAS risk score for each defendant ranges from 1 to 10, with 10 being the highest risk. In addition to this score $(A)$ , the data also includes records on defendants age $(X_{1} \\in X)$ , gender $(X_{2} \\in X)$ , race $(S)$ , prior convictions $(M)$ , and whether or not recidivism occurred in a span of two years $(R)$ . We limited our attention to the cohort consisting of African-Americans and Caucasians, and to individuals who either had not been arrested for a new offense or who had recidivated within two years. Our sample size is 5278. All variables were binarized including the COMPAS score, which we treat as an indicator of a binary decision to incarcerate versus release (pretrial) \"high risk\" individuals, i.e., we assume those with score $\\geq 7$ were incarcerated. In this data, $28.9\\%$ of individuals had scores $\\geq 7$ .\n\nSince the data does not include any variable that corresponds to utility, and there is no uncontroversial definition of what function one should optimize, we define a heuristic utility function from the data as follows. We assume there is some (social, economic, and human) cost, i.e., negative utility, associated with incarceration (deciding $A = 1$ ), and that there is some cost to releasing individuals who go on to reoffend (i.e., for whom $A = 0$ and $R = 1$ ). Also, there is positive utility associated with releasing individuals who do not go on to recidivate (i.e., for whom $A = 0$ and $R = 0$ ). A crucial feature of any realistic utility function is how to balance these relative costs, e.g., how much (if any) \"worse\" it is to release an individual who goes on to reoffend than to incarcerate them. To model these considerations we define utility $Y \\equiv (1 - A) \\times \\{-\\theta R + (1 - R)\\} - A$ . The utility function is thus parameterized by $\\theta$ , which quantifies how much \"worse\" is the case where individuals are released and reoffend as compared with the other two possibilities which are treated symmetrically. We emphasize that this utility function is a heuristic we use to illustrate our optimal policy learning method, and that a realistic utility function would be much more complicated (possibly depending also on factors not recorded in the available data).\n\nWe apply our proposed Q-learning procedure to optimize $\\mathbb{E}[Y]$ , assuming $K = 1$ and exogenous $S$ . The fair policy constrains the $S \\to A$ and $S \\to Y$ pathways. We describe details of our implementation as well as additional results in the Supplement. The proportion of individuals incarcerated",
      "context_after": "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarceration rates, which also vary among the policies. The region of particular interest is between $\\theta = 2$ and 3, where fair and unrestricted optimal policies differ and both recommend lower-than-observed overall incarceration rates (see Supplement). For most $\\theta$ values, the fair policy recommends a decision rule which narrows the racial gap in incarceration rates as compared with the unrestricted policy, though does not eliminate this gap entirely. (Constraining the causal effects of race through mediator $M$ would go further in eliminating this gap.) In regions where $\\theta > 3$ , both optimal policies in fact recommend higher-than-observed overall incarceration rates but a narrower racial gap, particularly for the fair policy. Comparing fair and unconstrained policy learning on this data serves to simultaneously illustrate how the proposed methods can be applied to real problems and how the choice of utility function is not innocuous.\n\nWe have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints in the induced joint distribution. In this sense, our approach can be said to \"break the cycle of injustice\" in decision-making. We investigated the performance of our proposals on synthetic and real data, where in the latter case we have supplemented the data with a heuristic utility function. In future work, we hope to develop and implement more sophisticated constrained optimization methods, to use information as efficiently as possible while satisfying the desired theoretical guarantee, and to explore nonparametric techniques for complex settings where the likelihood is not known.\n\n[Section: Learning Optimal Fair Policies]",
      "referring_paragraphs": [
        "$(A = 1)$ is a function of $\\theta$ , which we plot in Fig. 2 stratified by racial group. See the Supplement for results on overall incarceration rates, which also vary among the policies. The region of particular interest is between $\\theta = 2$ and 3, where fair and unrestricted optimal policies differ and both recommend lower-than-observed overall incarceration rates (see Supplement). For most $\\theta$ values, the fair policy recommends a decision rule which narrows the racial gap in incarcer",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_5",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/fb2916a64f5c6a8bc2f95d4d77e04208ef3c6c2c8c431253f728775492340976.jpg",
      "image_filename": "fb2916a64f5c6a8bc2f95d4d77e04208ef3c6c2c8c431253f728775492340976.jpg",
      "caption": "Table 1. Comparison of population outcomes $\\mathbb{E}[Y]$ under policies learned by different methods. The value under the observed policy was $0.24 \\pm 0.006$ .",
      "context_before": "$^{1}$ Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA. Correspondence to: Razieh Nabi <rnabi@jhu.edu>.\n\nProceedings of the $36^{th}$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\n\narXiv:1809.02244v3 [cs.LG] 27 May 2019",
      "context_after": "For this two-stage setting we estimated the optimal policies using Q-learning and value search. In value search, we considered restricted class of polices of the form $p(A_1 = 1|X,S,M) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX$ , and $p(A_2 = 1|X,S,M,A_1,Y_1) = -1 + \\alpha_xX + \\alpha_sS + \\alpha_mM + \\alpha_aA + \\alpha_{y_1}Y_1 + \\alpha_{sx}SX + \\alpha_{sm}SM + \\alpha_{mx}MX + \\alpha_{as}AS + \\alpha_{ax}AX$ where all $\\alpha$ s range from -3 to 3 by 0.5 increments and estimated the value of policies for each combination of $\\alpha$ s using equation (??).\n\nA third method for estimating policies is to directly model the counterfactual contrasts known as optimal blip-to-zero functions and then learn these functions by g-estimation (Robins, 2004); see Appendix A. We implemented our modified fair g-estimation for a single-stage decision problem and compared the results with Q-learning and value search. The results are provided in Table 1. The data generating process for the single-stage decision problem matches the causal model shown in Fig. ??(a) where $X, S, M$ , and $A$ were generated the same way as described above. The outcome $Y$ was generated from a standard normal distribution with mean $-2 + X + S + M + A - 3SX_2 + MS + AS + AM + AX_2 + AX_3$ . We used estimators in Theorem ?? to compute $\\mathrm{PSE}^{sy}$ and $\\mathrm{PSE}^{sa}$ which require using $M$ and $S$ models. In this synthetic data, the $\\mathrm{PSE}^{sy}$ was 1.618 (on the mean scale) and was restricted to lie between $-0.1$ and 0.1. The $\\mathrm{PSE}^{sa}$ was 0.685 (on the odds ratio scale) and was restricted to lie between 0.95 and 1.05.\n\nAppendix C: Details and additional results on the COMPAS data experiment",
      "referring_paragraphs": [
        "The goal of policy learning is to find policies that map vectors in $\\mathcal{H}_k$ to values in $\\mathcal{A}_k$ (for all $k$ ) that maximize the expected value of outcome $Y$ . In offline settings, where exploration by direct experimentation is impossible, finding such policies requires reasoning counterfactually, as is common in causal inference. The value of $Y$ under an assignment of value $a$ to variable $A$ is called a potential outcome variable, denoted $Y(a)$ . In causal inference, quant",
        "A causal parameter is said to be identified in a causal model if it is a function of the observed data distribution $p(Z)$ . In causal DAGs, distributions of potential outcomes are identified by the $g$ -formula. For background on general identification theory, see Shpitser (2018). As an example, the distribution of $Y(a)$ in the DAG in Fig. 1(a) is identified by $\\sum_{X,M} p(Y|a, M, X)p(M|a, X)p(X)$ . Note that some causal parameters may be identified even in causal models with hidden (\"latent",
        "As an example, $Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$ , and its distribution is identified as $\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$ .",
        "We summarize the proposal from Nabi & Shpitser (2018) with a brief example, inspired by the aforementioned child welfare case. Consider a simple causal model for this scenario, shown in Fig. 1(b). Hotline operators receive thousands of calls per year, and must decide on an action $A$ for each call, e.g., whether or not to send a caseworker. These decisions are made on the basis of a (high-dimensional) vectors of covariates $X$ and $M$ , as well as possibly sensitive features $S$ , such as race. ",
        "Consider a $K$ -stage decision problem given by a DAG where every vertex pair is connected, and with vertices in a topological order $X, S, M, A_1, Y_1, \\ldots, A_K, Y_K$ . See Fig. 1(c). Note that the setting where $S$ can be assumed exogenous is a special case of this model with missing edge between $X$ and $S$ . Though we only assume a single set of permissible mediators $\\bar{M}$ here, at the expense of some added cumbersome notation all of the following can be extended to the case where the",
        "Theorem 2 Consider the $K$ -stage decision problem described by the DAG in Fig. 1(c). Let $p^*(M|S, X; \\alpha_m)$ and $p^*(S|X; \\alpha_s)$ be the constrained models chosen to satisfy $PSE^{sy} = 0$ and $PSE^{sa_k} = 0$ . Let $\\tilde{p}(Z)$ be the joint distribution induced by $p^*(M|S, X; \\alpha_m)$ and $p^*(S|X; \\alpha_s)$ , and where all other distributions in the factorization are unrestricted. That is,",
        "Note that at each stage $k$ , the identity $Q_{k}(H_{k},A_{k}) = \\mathbb{E}[V_{k + 1}(H_{k + 1},A_{k})\\mid H_{k}] = \\mathbb{E}[V_{k + 1}(H_{k + 1})\\mid A_{k},H_{k}]$ only holds under our causal model if the entire past $H_{k}$ is conditioned on. In particular, $\\mathbb{E}[V_{k + 1}(H_{k + 1},A_k)\\mid$ $H_{k}\\setminus \\{M,S\\} ]\\neq \\mathbb{E}[V_{k + 1}(H_{k + 1})\\mid A_k,H_k\\setminus \\{M,S\\} ]$ . To see a simple example of this, note that $Y_{K}(a_{1})$ is not independent of $A_{1}$ conditional o",
        "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$ , where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$ . Details on the specific models used are reported in the Supplement. We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$ .",
        "A third method for estimating policies is to directly model the counterfactual contrasts known as optimal blip-to-zero functions and then learn these functions by g-estimation (Robins, 2004); see Appendix A. We implemented our modified fair g-estimation for a single-stage decision problem and compared the results with Q-learning and value search. The results are provided in Table 1. The data generating process for the single-stage decision problem matches the causal model shown in Fig. ??(a) whe",
        "Figure 1.",
        "Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg",
      "image_filename": "1809.02244_page0_fig4.jpg",
      "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "context_before": "The regression models we used in the COMPAS data analysis were specified as follows:\n\n$$ \\begin{array}{l} \\operatorname {l o g i t} (p (M = 1)) \\sim X _ {1} + X _ {2} + S + S X _ {1} + S X _ {2} \\\\ \\operatorname {l o g i t} (p (A = 1)) \\sim X _ {1} + X _ {2} + S + M + S X _ {1} \\\\ + S X _ {2} + M S + M X _ {1} + M X _ {2} \\\\ Y \\sim X _ {1} + X _ {2} + S + M + A + S X _ {1} + S X _ {2} \\\\ + A S + A M + M S + M X _ {1} + M X _ {2} \\\\ + A X _ {1} + A X _ {2} \\\\ \\end{array} $$\n\nFor estimating the PSEs which we constrain, we used the same IPW estimators described in the main paper and reproduced in the theorem below. We constrained the PSEs to lie between $-0.05$ and 0.05 and 0.95 and 1.05, respectively.",
      "context_after": "In Fig. 3, we compare the overall incarceration rates recommended by the optimal fair and unconstrained policies on the COMPAS data, as a function of the utility parameter $\\theta$ . For low values of $\\theta$ the incarceration rate is zero, and becomes higher as $\\theta$ increases, but differentially for the fair and unconstrained optimal policies. The difference between the policies depends crucially on the utility function. For some values of the utility parameter, the unfair and fair policies coincide, but for other values we would expect significantly different overall incarceration rates as well as different disparities between racial groups (see result in the main paper).\n\nIn Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained policies, as well as the utility of the observed decision pattern, as a function of $\\theta$ . As expected, choosing an optimal policy improves on the observed policy, with the unfair (unconstrained) choice being higher utility than the fair (constrained) choice; we sacrifice some optimality to satisfy the fairness constraints. However, the difference depends on the utility parameter and for a range of parameter values the fair and unfair policies are nearly the same in terms of optimality (even when they may disagree on the resulting incarceration rate, around $\\theta = 2.6$ ). The fair and unfair policies drift far apart in terms of utility around $\\theta = 3$ , when the policies recommend an incarceration rate comparable to or higher than the observed rate.\n\n[Section: Supplemental Appendix for \"Learning Optimal Fair Policies\"]",
      "referring_paragraphs": [
        "In Fig. 3, we compare the overall incarceration rates recommended by the optimal fair and unconstrained policies on the COMPAS data, as a function of the utility parameter $\\theta$ . For low values of $\\theta$ the incarceration rate is zero, and becomes higher as $\\theta$ increases, but differentially for the fair and unconstrained optimal policies. The difference between the policies depends crucially on the utility function. For some values of the utility parameter, the unfair and fair policie",
        "Figure 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg",
      "image_filename": "1809.02244_page0_fig5.jpg",
      "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "context_before": "In Fig. 3, we compare the overall incarceration rates recommended by the optimal fair and unconstrained policies on the COMPAS data, as a function of the utility parameter $\\theta$ . For low values of $\\theta$ the incarceration rate is zero, and becomes higher as $\\theta$ increases, but differentially for the fair and unconstrained optimal policies. The difference between the policies depends crucially on the utility function. For some values of the utility parameter, the unfair and fair policies coincide, but for other values we would expect significantly different overall incarceration rates as well as different disparities between racial groups (see result in the main paper).\n\nIn Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained policies, as well as the utility of the observed decision pattern, as a function of $\\theta$ . As expected, choosing an optimal policy improves on the observed policy, with the unfair (unconstrained) choice being higher utility than the fair (constrained) choice; we sacrifice some optimality to satisfy the fairness constraints. However, the difference depends on the utility parameter and for a range of parameter values the fair and unfair policies are nearly the same in terms of optimality (even when they may disagree on the resulting incarceration rate, around $\\theta = 2.6$ ). The fair and unfair policies drift far apart in terms of utility around $\\theta = 3$ , when the policies recommend an incarceration rate comparable to or higher than the observed rate.\n\n[Section: Supplemental Appendix for \"Learning Optimal Fair Policies\"]",
      "context_after": "Theorem 1 Assume $S$ is binary. Under the causal model above, the following are consistent estimators of $PSE^{sy}$ and $PSE^{sak_k}$ , assuming all models are correctly specified:\n\n$$ \\begin{array}{l} \\widehat {g} ^ {s y} (Z) = \\tag {2} \\\\ \\frac {1}{N} \\sum_ {n = 1} ^ {N} \\left\\{\\frac {\\mathbb {I} (S _ {n} = s)}{p (S _ {n} | X _ {n})} \\frac {p (M _ {n} | s ^ {\\prime} , X _ {n})}{p (M _ {n} | s , X _ {n})} - \\frac {\\mathbb {I} (S _ {n} = s ^ {\\prime})}{p (S _ {n} | X _ {n})} \\right\\} Y _ {n} \\\\ \\end{array} $$\n\n$$ \\widehat {g} ^ {s a _ {k}} (Z) = \\tag {3} $$",
      "referring_paragraphs": [
        "In Fig. 4, we show the relative utility achieved by the optimal fair and unconstrained policies, as well as the utility of the observed decision pattern, as a function of $\\theta$ . As expected, choosing an optimal policy improves on the observed policy, with the unfair (unconstrained) choice being higher utility than the fair (constrained) choice; we sacrifice some optimality to satisfy the fairness constraints. However, the difference depends on the utility parameter and for a range of paramet",
        "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.02244_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1809.04737": [
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
      "image_filename": "1809.04737_page0_fig0.jpg",
      "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
      "context_before": "subject to $\\mathbb { R D } ( f ) \\leq \\tau , \\quad - \\mathbb { R D } ( f ) \\leq \\tau .$\n\nObviously, the above optimization problem is non-convex. Similar to the loss function, we adopt surrogate functions to convert the risk difference to convex constraints. By using predictive function $h$ and the indicator function, we can rewrite the risk difference as\n\n$$ \\begin{array}{l} \\mathbb {R} \\mathbb {D} (f) = \\mathbb {E} _ {\\mathbf {X} | S = s ^ {+}} \\left[ \\mathbb {1} \\left[ \\operatorname {s i g n} (h (\\mathbf {x})) = 1 \\right] \\right] \\\\ - \\mathbb {E} _ {\\mathbf {X} | S = s ^ {-}} \\left[ \\mathbb {1} \\left[ \\operatorname {s i g n} (h (\\mathbf {x})) = 1 \\right] \\right] \\\\ = \\mathbb {E} _ {\\mathbf {X} | S = s ^ {+}} [ \\mathbb {1} _ {h (\\mathbf {x}) > 0} ] - \\mathbb {E} _ {\\mathbf {X} | S = s ^ {-}} [ \\mathbb {1} _ {h (\\mathbf {x}) > 0} ] \\\\ = \\mathbb {E} _ {\\mathbf {X} | S = s +} [ \\mathbb {1} _ {h (\\mathbf {x}) > 0} ] + \\mathbb {E} _ {\\mathbf {X} | S = s -} [ \\mathbb {1} _ {h (\\mathbf {x}) < 0} ] - 1. \\\\ \\end{array} $$",
      "context_after": "$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave problem, where only heuristic solutions for finding local optima are known to exist. Therefore, we adopt two surrogate functions, a convex one $\\kappa ( \\cdot )$ and a concave one $\\delta ( \\cdot )$ , each of which replaces the indicator function for one constraint. As a result, the formulated constrained optimization problem is convex and can be efficiently solved. We call the risk difference represented by $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ as the $\\kappa , \\delta$ -risk difference, denoted by $\\mathbb { R D } _ { \\kappa } ( h )$ and $\\mathbb { R D } _ { \\delta } ( h )$ . Almost all commonly-used surrogate functions can be adopted for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ , by performing some shift or flip. Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.",
      "referring_paragraphs": [
        "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave p",
        "Adding constraints into the classification models increases the computational complexity and also decreases the predictive accuracy. It is desired not to incorporate any fairness constraint if it is guaranteed that the classifier learned will be fair. This situation is possible. Consider an example of admitting students. The application profile contains two attributes, a sensitive attribute $\\operatorname { S e x }$ and a non-sensitive attribute GPA. The statistics of the dataset is shown in Tab",
        "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .\n\nIt follows that",
        "Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.",
        "The statistics of the dataset is shown in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/495d8c4fb2630144c0e955d73ff0f6d19dbd2a0330ae67b2fb3e834cf0b8f831.jpg",
      "image_filename": "495d8c4fb2630144c0e955d73ff0f6d19dbd2a0330ae67b2fb3e834cf0b8f831.jpg",
      "caption": "Table 1: An example of admitting students.",
      "context_before": "$$ \\begin{array}{l} \\mathbb {R D} (f) = \\mathbb {E} _ {\\mathbf {X}} \\left[ \\frac {P (S = s ^ {+} | \\mathbf {x})}{P (S = s ^ {+})} \\mathbb {1} _ {h (\\mathbf {x}) > 0} \\right. \\tag {3} \\\\ + \\frac {P (S = s ^ {-} | \\mathbf {x})}{P (S = s ^ {-})} \\mathbb {1} _ {h (\\mathbf {x}) < 0} - 1 ]. \\\\ \\end{array} $$\n\nFor simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave problem, where only heuristic solutions for finding local optima are known to exist. Therefore, we adopt two surrogate functions, a convex one $\\kappa ( \\cdot )$ and a concave one $\\delta ( \\cdot )$ , each of which replaces the indicator function for one constraint. As a result, the formulated constrained optimization problem is convex and can be efficiently solved. We call the risk difference represented by $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ as the $\\kappa , \\delta$ -risk difference, denoted by $\\mathbb { R D } _ { \\kappa } ( h )$ and $\\mathbb { R D } _ { \\delta } ( h )$ . Almost all commonly-used surrogate functions can be adopted for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ , by performing some shift or flip. Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.",
      "context_after": "To sum up, we obtain the following convex optimization formulation for learning fair classifiers.\n\nProblem Formulation 1. The goal of the fairness-aware classification is to find a classifier $f$ which minimizes the empirical loss $\\mathbb { L } ( f )$ while satisfying fairness constraint $| \\mathbb { R } \\mathbb { D } ( f ) | \\leq \\tau$ . It can be approached by solving the following constrained optimization problem\n\n$$ \\min _ {h \\in \\mathcal {H}} \\quad \\mathbb {L} _ {\\phi} (h) $$",
      "referring_paragraphs": [
        "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave p",
        "Adding constraints into the classification models increases the computational complexity and also decreases the predictive accuracy. It is desired not to incorporate any fairness constraint if it is guaranteed that the classifier learned will be fair. This situation is possible. Consider an example of admitting students. The application profile contains two attributes, a sensitive attribute $\\operatorname { S e x }$ and a non-sensitive attribute GPA. The statistics of the dataset is shown in Tab",
        "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .\n\nIt follows that",
        "Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.",
        "The statistics of the dataset is shown in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg",
      "image_filename": "1809.04737_page0_fig1.jpg",
      "caption": "(a) A classifier that meets the κ, δ-risk difference constraint makes unfair predictions.",
      "context_before": "$$ \\mathbb {R D} (f) \\geq \\mathbb {R D} ^ {+} - \\psi_ {\\delta} ^ {- 1} \\big (\\mathbb {R D} _ {\\delta} ^ {+} - \\mathbb {R D} _ {\\delta} (h) \\big). $$\n\nBased on the upper and lower bounds of $\\mathbb { R D } ( f )$ , we modify Problem Formulation 1 to obtain Problem Formulation 2 with refined fairness constraints which guarantee the real fairness requirement.\n\n1Based on Scott et al. (Scott 2012), $\\psi _ { \\kappa } ( \\cdot )$ and $\\psi _ { \\delta } ( \\cdot )$ are invertible if $H _ { \\kappa } ^ { \\circ } ( \\eta ) \\ > \\ H _ { \\kappa } ^ { - } ( \\eta )$ and $H _ { \\delta } ^ { \\circ } ( \\eta ) \\ < \\ H _ { \\delta } ^ { + } ( \\eta )$ for all $\\eta \\in$ $[ 0 , 1 ] , \\eta \\neq p$ .",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig2.jpg",
      "image_filename": "1809.04737_page0_fig2.jpg",
      "caption": "(b) A classifier that doesn’t meet the $\\kappa , \\delta$ -risk difference constraint makes fair predictions. Figure 2: Two classifiers and their predictions.",
      "context_before": "",
      "context_after": "Problem Formulation 2. A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization\n\n$$ \\min _ {h \\in \\mathcal {H}} \\quad \\mathbb {L} _ {\\phi} (h) \\tag {6} $$\n\nsubject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq \\psi _ { \\kappa } \\big ( c _ { 1 } - \\mathbb { R } \\mathbb { D } ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$",
      "referring_paragraphs": [
        "Figure 2: Two classifiers and their predictions.\n\nProblem Formulation 2. A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization",
        "For our\n\nTable 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa } ( \\mu )$ and $\\psi _ { \\delta } ( \\mu )$   \n\n<table><tr><td>Name of κ-δ</td><td>κ(α) for α ∈ R</td><td>δ(α) for α ∈ R</td><td>ψκ(μ) or ψδ(μ) for μ ∈ (0,1/p]</td></tr><tr><td>Hinge</td><td>max{α+1,0}</td><td>min{α,1}</td><td>μ</td></tr><tr><td>Square</td><td>(α+1)2</td><td>1-(1-α)2</td><td>μ2</td></tr><tr><td>Exponential</td><td>exp(α)</td><td>1-exp(-α)</td><td>(√(1-p)μ+1-√1-pμ)2</td></tr></table>"
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/ce1a8e6f3a4c75d9a39bdd187ecc517a09c48930ce7027fb7374c13cfb7dc2c7.jpg",
      "image_filename": "ce1a8e6f3a4c75d9a39bdd187ecc517a09c48930ce7027fb7374c13cfb7dc2c7.jpg",
      "caption": "Table 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa } ( \\mu )$ and $\\psi _ { \\delta } ( \\mu )$",
      "context_before": "To demonstrate the sufficiency criterion of learning fair classifiers, we build the maximal/minimal risk difference classifiers $f _ { \\mathrm { m i n } }$ , $f _ { \\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\mathbb { R D } ^ { - } , \\mathbb { R D } ^ { + }$ . The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences. In order to evaluate a situation with small a risk difference, we also create a variant of Adult, referred to as Adult*, where all attributes are binarized and the sensitive attribute sex is shuffled to incur a small risk difference. Then, we build a number of classifiers including Linear Regression (LR), Support Vector Machine (SVM) with linear kernel, Decision Tree (DT), and Naive Bayes (NB), using the three datasets as the training data with with 5-fold crossvalidation. After that, their risk differences are quantified on the testing data, as shown in the last four rows in Table 3. We can see that all values are within $\\mathbb { R D } ^ { - }$ , $\\mathbb { R } \\mathbb { D } ^ { + }$ which are consistent with our criterion.\n\nLearning Fair Classifiers\n\nWe build our fair classifiers on both Adult and Dutch datasets by solving the optimization problem defined in Problem Formulation 2. For surrogate functions, we use the logistic function for $\\phi ( \\cdot )$ , and the hinge function for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ . We also compare our methods with Zafar-1 and Zafar-2. The results are shown in Figure 3, which depict the relationship between the obtained risk difference and empirical loss. For our",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2: Two classifiers and their predictions.\n\nProblem Formulation 2. A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization",
        "For our\n\nTable 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa } ( \\mu )$ and $\\psi _ { \\delta } ( \\mu )$   \n\n<table><tr><td>Name of κ-δ</td><td>κ(α) for α ∈ R</td><td>δ(α) for α ∈ R</td><td>ψκ(μ) or ψδ(μ) for μ ∈ (0,1/p]</td></tr><tr><td>Hinge</td><td>max{α+1,0}</td><td>min{α,1}</td><td>μ</td></tr><tr><td>Square</td><td>(α+1)2</td><td>1-(1-α)2</td><td>μ2</td></tr><tr><td>Exponential</td><td>exp(α)</td><td>1-exp(-α)</td><td>(√(1-p)μ+1-√1-pμ)2</td></tr></table>"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg",
      "image_filename": "1809.04737_page0_fig3.jpg",
      "caption": "(a) Adult",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig4.jpg",
      "image_filename": "1809.04737_page0_fig4.jpg",
      "caption": "(b) Dutch Figure 3: Comparison of fair classifiers.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "To demonstrate the sufficiency criterion of learning fair classifiers, we build the maximal/minimal risk difference classifiers $f _ { \\mathrm { m i n } }$ , $f _ { \\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\mathbb { R D } ^ { - } , \\mathbb { R D } ^ { + }$ . The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences. In order to evaluate a situatio",
        "We build our fair classifiers on both Adult and Dutch datasets by solving the optimization problem defined in Problem Formulation 2. For surrogate functions, we use the logistic function for $\\phi ( \\cdot )$ , and the hinge function for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ . We also compare our methods with Zafar-1 and Zafar-2. The results are shown in Figure 3, which depict the relationship between the obtained risk difference and empirical loss. For our",
        "The results are shown in the first two rows in Table 3.",
        "The results are shown in Figure 3, which depict the relationship between the obtained risk difference and empirical loss.",
        "Figure 3: Comparison of fair classifiers."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/e08f9fa96b5b3a9318ae8ec916d1bf7bd9f9ba769b1936b0ffe54b0e9f630944.jpg",
      "image_filename": "e08f9fa96b5b3a9318ae8ec916d1bf7bd9f9ba769b1936b0ffe54b0e9f630944.jpg",
      "caption": "Table $3 \\colon \\mathbb { R D } ^ { + } , \\mathbb { R D } ^ { - }$ $\\mathbb { R } \\mathbb { D } ^ { - }$ and risk differences of Linear Regression (LR), Support Vector Machine (SVM), Decision Tree (DT), and Naive Bayes (NB).",
      "context_before": "",
      "context_after": "method, different risk differences are obtained by adjusting relax terms $c _ { 1 }$ and $c _ { 2 }$ , while for Zafar-1 and Zafar-2 different risk differences are obtained by adjusting the multiplication factor $m$ . As can be seen, our method can achieve much smaller risk difference than Zafar-1 and Zafar-2. This may be because Zafar-1 linear functions to formulate the fairness constraints, which may incur large estimation errors; while Zafar-2 formulates a convex-concave optimization problem, where only local optima can be reached. For the same reason, we can observe that our method produces better empirical loss than Zafar-2 given any same risk difference.\n\nMany methods have been proposed for constructing fairnessaware classifiers, which can be broadly classified into\n\npre/post-processing and in-processing methods. The pre/postprocessing methods propose to modify the training data and/or tweak the predictions to obtain fair predictions. Data mining techniques have been proposed to remove bias from a dataset since 2008 (Pedreshi, Ruggieri, and Turini 2008). After that, a number of techniques have been proposed either based on correlations between the sensitive attribute and the decision (Dwork et al. 2012; Feldman et al. 2015; Wu and Wu 2016; Zliobaite, Kamiran, and Calders 2011) or the causal relationship among all attributes (Kilbertus et al. 2017; Zhang and Bareinboim 2018; Zhang and Wu 2017; Zhang, Wu, and Wu 2017b). In (Hardt et al. 2016), the authors proposed to tweak the output of the classifier after the classifier makes predictions. As suggested by a recent work (Zhang, Wu, and Wu 2018), both the pre-processing and post-processing phases are necessary in achieving fair predictions. Another category of methods are in-processing methods which adjust the learning process of the classifier (Kamishima, Akaho, and Sakuma 2011; Agarwal et al. 2017; Menon and Williamson 2018). In recent years, a number of methods are proposed to incorporate fairness as constraints in the optimization, e.g., (Kamishima, Akaho, and Sakuma 2011; Goh et al. 2016; Krishna and Williamson 2018; Zafar et al. 2017a; Zafar et al. 2017b; Woodworth et al. 2017; Olfat and Aswani 2018). As discussed in the paper, there lacks a theoretical framework for guiding the formulation of the constrained optimization problem. This paper proposes a general framework for fairness-aware classification.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.04737_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1809.10083": [
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg",
      "image_filename": "1809.10083_page0_fig0.jpg",
      "caption": "(a)",
      "context_before": "3.1 Unsupervised Invariance Induction\n\nData samples $( x )$ can be abstractly represented as a set of underlying factors of variation $F = \\{ f _ { i } \\}$ . This can be as simple as a collection of numbers denoting the position of a point in space or as complicated as information pertaining to various facial attributes that combine non-trivially to form the image of someone’s face. Understanding and modeling the interactions between factors of variation of data is an open problem. However, supervised learning of the mapping of $x$ to target $( y )$ involves a relatively simpler (yet challenging) problem of finding those factors of variation $( F _ { y } )$ that contain all the information required for predicting $y$ and discarding all the others $( \\overline { { F } } _ { y } )$ . Thus, $F _ { y }$ and $\\overline { { F } } _ { y }$ form a partition of $F$ , where we are more interested in the former than the latter. Since $y$ is independent of $\\overline { { F } } _ { y }$ , i.e., $\\boldsymbol { y } \\perp \\overline { { \\boldsymbol { F } } } _ { y }$ , we get $p ( y | x ) = p ( y | F _ { y } )$ . Estimating $p ( y | x )$ as $q ( y | F _ { y } )$ from data is beneficial because the nuisance factors, which comprise $\\overline { { F } } _ { y }$ , are never presented to the estimator, thus avoiding inaccurate learning of associations between nuisance factors and $y$ . This forms the basis for “feature selection”, a research area that has been well-studied.\n\nWe incorporate the idea of splitting $F$ into $F _ { y }$ and $\\overline { { F } } _ { y }$ in our framework in a more relaxed sense as learning a disentangled latent representation of $x$ in the form of $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , where $e _ { 1 }$ aims to capture all the information in $F _ { y }$ and $e _ { 2 }$ that in $\\overline { { F } } _ { y }$ . Once trained, the model can be used to infer $e _ { 1 }$ from $x$ followed by $y$ from $e _ { 1 }$ . More formally, our general framework for unsupervised invariance induction comprises four core modules: (1) an encoder $E n c$ that embeds $x$ into $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , (2) a predictor P red that infers $y$ from $e _ { 1 }$ , (3) a noisy-transformer $\\psi$ that converts $e _ { 1 }$ into its noisy version $\\tilde { e } _ { 1 }$ , and",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig1.jpg",
      "image_filename": "1809.10083_page0_fig1.jpg",
      "caption": "(b) Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design",
      "context_before": "",
      "context_after": "(4) a decoder Dec that reconstructs $x$ from $\\tilde { e } _ { 1 }$ and $e _ { 2 }$ . Additionally, the training objective contains a loss-term that enforces disentanglement between $E n c ( x ) _ { 1 } = e _ { 1 }$ and $E n c ( x ) _ { 2 } \\stackrel { - } { = } e _ { 2 }$ . Figure 1a shows our generalized framework. The training objective for this system can be written as Equation 1.\n\n$$ \\begin{array}{l} L = \\alpha L _ {p r e d} (y, P r e d (e _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (e _ {1}), e _ {2})) + \\gamma L _ {d i s} ((e _ {1}, e _ {2})) \\\\ = \\alpha L _ {p r e d} (y, P r e d (E n c (x) _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (E n c (x) _ {1}), E n c (x) _ {2})) + \\gamma L _ {d i s} (E n c (x)) \\tag {1} \\\\ \\end{array} $$\n\nThe predictor and the decoder are designed to enter into a competition, where P red tries to pull information relevant to $y$ into $e _ { 1 }$ while $D e c$ tries to extract all the information about $x$ into $e _ { 2 }$ . This is made possible by $\\psi$ , which makes $\\tilde { e } _ { 1 }$ an unreliable source of information for reconstructing $x$ . Moreover, a version of this framework without $\\psi$ can converge to a degenerate solution where $e _ { 1 }$ contains all the information about $x$ and $e _ { 2 }$ contains nothing (noise), because absence of $\\psi$ allows $e _ { 1 }$ to be readily available to Dec. The competitive pulling of information into $e _ { 1 }$ and $e _ { 2 }$ induces information separation such that $e _ { 1 }$ tends to contain more information relevant for predicting $y$ and $e _ { 2 }$ more information irrelevant to the prediction task. However, this competition is not sufficient to completely partition information of $x$ into $e _ { 1 }$ and $e _ { 2 }$ . Without the disentanglement term $( L _ { d i s } )$ in the objective, $e _ { 1 }$ and $e _ { 2 }$ can contain redundant information such that $e _ { 2 }$ has information relevant to $y$ and, more importantly, $e _ { 1 }$ contains nuisance factors. The disentanglement term in the training objective encourages the desired clean partition. Thus, essential factors required for predicting $y$ concentrate into $e _ { 1 }$ and all other factors migrate to $e _ { 2 }$ .",
      "referring_paragraphs": [
        "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat",
        "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design\n\n(4) a decoder Dec that reconstructs $x$ from $\\tilde { e } _ { 1 }$ and $e _ { 2 }$ . Additionally, the training objective contains a loss-term that enforces disentanglement between $E n c ( x ) _ { 1 } = e _ { 1 }$ and $E n c ( x ) _ { 2 } \\stackrel { - } { = } e _ { 2 }$ . Figure 1a shows our generalized framework. The training objective for this system can be written as Equation 1.",
        "We optimize the proposed adversarial model using a scheduled update scheme where we freeze the weights of a composite\n\nTable 1: Results on Extended Yale-B dataset   \n\n<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>\n\nplayer model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other.",
        "Table 1 summarizes the results."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/376892aa650edc34a908909a1d1fb986876238d52212645f26ceec328acb4fcd.jpg",
      "image_filename": "376892aa650edc34a908909a1d1fb986876238d52212645f26ceec328acb4fcd.jpg",
      "caption": "Table 1: Results on Extended Yale-B dataset",
      "context_before": "We augment these core modules with two adversarial disentanglers $D i s _ { 1 }$ and $D i s _ { 2 }$ . While $D i s _ { 1 }$ aims to predict $e _ { 2 }$ from $e _ { 1 }$ , $D i s _ { 2 }$ aims to do the inverse. Hence, their objectives are in direct opposition to the desired disentanglement, forming the basis for adversarial minimax optimization. Thus, Enc, P red and Dec can be thought of as a composite model $( M _ { 1 } )$ that is pit against another composite model $( M _ { 2 } )$ containing $D i s _ { 1 }$ and $D i s _ { 2 }$ . Figure 1b shows our complete model design with $M _ { 1 }$ represented by the color blue and $M _ { 2 }$ with orange. The model is trained end-to-end through backpropagation by playing the minimax game described in Equation 2.\n\n$$ \\begin{array}{l} \\min _ {E n c, P r e d, D e c} \\max _ {D i s _ {1}, D i s _ {2}} J (E n c, P r e d, D e c, D i s _ {1}, D i s _ {2}); \\text {w h e r e :} \\\\ J (E n c, P r e d, D e c, D i s _ {1}, D i s _ {2}) \\\\ = \\alpha L _ {p r e d} (y, P r e d (e _ {1})) + \\beta L _ {d e c} (x, D e c (\\psi (e _ {1}), e _ {2})) + \\gamma \\tilde {L} _ {d i s} ((e _ {1}, e _ {2})) \\\\ = \\alpha L _ {p r e d} \\left(y, P r e d \\left(E n c (x) _ {1}\\right)\\right) + \\beta L _ {d e c} \\left(x, D e c \\left(\\psi \\left(E n c (x) _ {1}\\right), E n c (x) _ {2}\\right)\\right) \\\\ + \\gamma \\left\\{\\tilde {L} _ {d i s _ {1}} \\left(E n c (x) _ {2}, D i s _ {1} \\left(E n c (x) _ {1}\\right)\\right) + \\tilde {L} _ {d i s _ {2}} \\left(E n c (x) _ {1}, D i s _ {2} \\left(E n c (x) _ {2}\\right)\\right) \\right\\} \\tag {2} \\\\ \\end{array} $$\n\nWe use mean squared error for the disentanglement losses $\\tilde { L } _ { d i s _ { 1 } }$ and $\\tilde { L } _ { d i s _ { 2 } }$ . We optimize the proposed adversarial model using a scheduled update scheme where we freeze the weights of a composite",
      "context_after": "player model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other. $M _ { 2 }$ should ideally be trained to convergence before updating $M _ { 1 }$ in each training epoch to backpropagate accurate and stable disentanglement-inducing gradients to Enc. However, this is not scalable in practice. We update $M _ { 1 }$ and $M _ { 2 }$ in the frequency ratio of $1 : k$ . We found $k = 5$ to perform well in our experiments.\n\nCompetition between prediction and reconstruction. The prediction and reconstruction tasks in our framework are designed to compete with each other. Thus, $\\begin{array} { r } { \\eta = \\frac { \\alpha } { \\beta } } \\end{array}$ influences which task has higher priority in the overall objective. We analyze the affect of $\\eta$ on the behavior of our framework at optimality, considering perfect disentanglement of $e _ { 1 }$ and $e _ { 2 }$ . There are two asymptotic scenarios with respect to $\\eta$ : (1) $\\eta \\infty$ and (2) $\\eta 0$ . In case (1), our framework reduces to a predictor model, where the reconstruction task is completely disregarded. Only the branch $x e _ { 1 } y$ remains functional. Consequently, $e _ { 1 }$ contains all $f \\in F ^ { \\prime }$ at optimality, where $F _ { y } \\subseteq F ^ { \\prime } \\subseteq F$ . In contrast, case (2) reduces the framework to an autoencoder, where the prediction task is completely disregarded, and only the branch $x e _ { 2 } x ^ { \\prime }$ remains functional because the other input to Dec, $\\dot { \\psi } ( e _ { 1 } )$ , is noisy. Thus, $e _ { 2 }$ contains all $f \\in F$ and $e _ { 1 }$ contains nothing at optimality, under perfect disentanglement. In transition from case (1) to case (2), by keeping $\\alpha$ fixed and increasing $\\beta$ , the reconstruction loss starts contributing more to the overall objective, thus inducing more competition between the two tasks. As $\\eta$ is gradually decreased, $f \\in ( F ^ { \\prime } \\setminus F _ { y } ) \\subseteq \\overline { { F } } _ { y }$ migrate from $e _ { 1 }$ to $e _ { 2 }$ because $f \\in \\overline { { F } } _ { y }$ are irrelevant to the prediction task but can improve reconstruction by being more readily available to $D e c$ through $e _ { 2 }$ instead of $\\psi ( e _ { 1 } )$ . After a point, further decreasing $\\eta$ is, however, detrimental to the prediction task as the reconstruction task starts dominating the overall objective and pulling $f \\in F _ { y }$ from $e _ { 1 }$ to $e _ { 2 }$ .\n\nEquilibrium analysis of adversarial instantiation. The disentanglement and prediction objectives in our adversarial model design can simultaneously reach an optimum where $e _ { 1 }$ contains $F _ { y }$ and $e _ { 2 }$ contains $\\overline { { F } } _ { y }$ . Hence, the minimax objective in our method has a win-win equilibrium.",
      "referring_paragraphs": [
        "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat",
        "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design\n\n(4) a decoder Dec that reconstructs $x$ from $\\tilde { e } _ { 1 }$ and $e _ { 2 }$ . Additionally, the training objective contains a loss-term that enforces disentanglement between $E n c ( x ) _ { 1 } = e _ { 1 }$ and $E n c ( x ) _ { 2 } \\stackrel { - } { = } e _ { 2 }$ . Figure 1a shows our generalized framework. The training objective for this system can be written as Equation 1.",
        "We optimize the proposed adversarial model using a scheduled update scheme where we freeze the weights of a composite\n\nTable 1: Results on Extended Yale-B dataset   \n\n<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>\n\nplayer model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other.",
        "Table 1 summarizes the results."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig2.jpg",
      "image_filename": "1809.10083_page0_fig2.jpg",
      "caption": "(a)",
      "context_before": "5.1 Invariance to inherent nuisance factors\n\nWe provide results of our framework at the task of learning invariance to inherent nuisance factors on two datasets – Extended Yale-B [7] and Chairs [2].\n\nExtended Yale-B. This dataset contains face-images of 38 subjects under various lighting conditions. The target $y$ is the subject identity whereas the inherent nuisance factor $z$ is the lighting condition. We compare our framework to existing state-of-the-art supervised invariance induction methods, CAI [19], VFAE [14], and NN+MMD [13]. We use the prior works’ version of the dataset, which has lighting conditions classified into five groups – front, upper-left, upper-right, lower-left",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig3.jpg",
      "image_filename": "1809.10083_page0_fig3.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg",
      "image_filename": "1809.10083_page0_fig4.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig5.jpg",
      "image_filename": "1809.10083_page0_fig5.jpg",
      "caption": "(d)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_8",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig6.jpg",
      "image_filename": "1809.10083_page0_fig6.jpg",
      "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat",
        "Chairs. This dataset consists of 1393 different chair types rendered at 31 yaw angles and two pitch angles using a computer aided design model. We treat the chair identity as the target $y$ and the yaw angle $\\theta$ as $z$ . We split the data into training and testing sets by picking alternate yaw angles. Therefore, there is no overlap of θ between the two sets. We compare the performance of our model to CAI. In order to train the CAI model, we group $\\theta$ into four categories – front, left,",
        "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
        "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   \n\n<table><tr><td>Metric</td><td>CAI</td><td>Ours</td></tr><tr><td>Ay</td><td>0.68</td><td>0.74</td></tr><tr><td>Az</td><td>0.69</td><td>0.34</td></tr></table>",
        "Table 2 summarizes the results, showing that our model outperforms CAI on both $A _ { y }$ and $A _ { z }$ ."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig7.jpg",
      "image_filename": "1809.10083_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig8.jpg",
      "image_filename": "1809.10083_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig9.jpg",
      "image_filename": "1809.10083_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig10.jpg",
      "image_filename": "1809.10083_page0_fig10.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig11.jpg",
      "image_filename": "1809.10083_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg",
      "image_filename": "1809.10083_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig13.jpg",
      "image_filename": "1809.10083_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_16",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig14.jpg",
      "image_filename": "1809.10083_page0_fig14.jpg",
      "caption": "(b) Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .",
      "context_before": "",
      "context_after": "and lower-right, with the same split as $3 8 \\times 5 = 1 9 0$ samples used for training and the rest used for testing [13, 14, 19]. We use the same architecture for the predictor and the encoder as CAI (as presented in [19]), i.e., single-layer neural networks, except that our encoder produces two encodings instead of one. We also model the decoder and the disentanglers as single-layer neural networks.\n\nTable 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model. While raw data is clustered by lighting conditions $z$ , $e _ { 1 }$ exhibits clustering by $y$ with no grouping based on $z$ , and $e _ { 2 }$ exhibits near-perfect clustering by $z$ . Figure 3a shows reconstructions from $e _ { 1 }$ and $e _ { 2 }$ . Dedicated decoder networks were trained (with weights of Enc frozen) to generate these visualizations. As evident, $e _ { 1 }$ captures identity-related information but not lighting while $e _ { 2 }$ captures the inverse.",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs.",
        "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic\n\nTable 3: Results on MNIST-ROT.",
        "Table 3 summarizes the results, showing that our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg",
      "image_filename": "1809.10083_page0_fig15.jpg",
      "caption": "(a)",
      "context_before": "and lower-right, with the same split as $3 8 \\times 5 = 1 9 0$ samples used for training and the rest used for testing [13, 14, 19]. We use the same architecture for the predictor and the encoder as CAI (as presented in [19]), i.e., single-layer neural networks, except that our encoder produces two encodings instead of one. We also model the decoder and the disentanglers as single-layer neural networks.\n\nTable 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model. While raw data is clustered by lighting conditions $z$ , $e _ { 1 }$ exhibits clustering by $y$ with no grouping based on $z$ , and $e _ { 2 }$ exhibits near-perfect clustering by $z$ . Figure 3a shows reconstructions from $e _ { 1 }$ and $e _ { 2 }$ . Dedicated decoder networks were trained (with weights of Enc frozen) to generate these visualizations. As evident, $e _ { 1 }$ captures identity-related information but not lighting while $e _ { 2 }$ captures the inverse.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig16.jpg",
      "image_filename": "1809.10083_page0_fig16.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_19",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/be3e8354bab5d176a172943ce17c9b4687da273e098cf39992d9da03e1695c2b.jpg",
      "image_filename": "be3e8354bab5d176a172943ce17c9b4687da273e098cf39992d9da03e1695c2b.jpg",
      "caption": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 1 summarizes the results. The proposed unsupervised method outperforms existing state-of-theart (supervised) invariance induction methods on both $A _ { y }$ and $A _ { z }$ metrics, providing a significant boost on $A _ { y }$ and complete removal of lighting information from $e _ { 1 }$ reflected by $A _ { z }$ . Furthermore, the accuracy of predicting $z$ from $e _ { 2 }$ is 0.89, which validates its automatic migration to $e _ { 2 }$ . Figure 2 shows t-SNE [15] visualization of raw dat",
        "Chairs. This dataset consists of 1393 different chair types rendered at 31 yaw angles and two pitch angles using a computer aided design model. We treat the chair identity as the target $y$ and the yaw angle $\\theta$ as $z$ . We split the data into training and testing sets by picking alternate yaw angles. Therefore, there is no overlap of θ between the two sets. We compare the performance of our model to CAI. In order to train the CAI model, we group $\\theta$ into four categories – front, left,",
        "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
        "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   \n\n<table><tr><td>Metric</td><td>CAI</td><td>Ours</td></tr><tr><td>Ay</td><td>0.68</td><td>0.74</td></tr><tr><td>Az</td><td>0.69</td><td>0.34</td></tr></table>",
        "Table 2 summarizes the results, showing that our model outperforms CAI on both $A _ { y }$ and $A _ { z }$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_20",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg",
      "image_filename": "1809.10083_page0_fig17.jpg",
      "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$ (a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "in the training data. Thus, information about these $z$ cannot be used to train supervised invariance induction models. We also provide ablation results on this dataset using the same baselines $B _ { 0 }$ and $B _ { 1 }$ . Table 4 summarizes the results of this experiment. The results show significantly better performance of our model compared to CAI and the baselines. More notably, CAI performs significantly worse than our baseline models, indicating that the supervised approach of invariance ",
        "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   \n(a)",
        "<table><tr><td>Metric</td><td>Angle</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td rowspan=\"3\">Ay</td><td>Θ</td><td>0.958</td><td>0.977</td><td>0.974</td><td>0.972</td></tr><tr><td>±55°</td><td>0.826</td><td>0.856</td><td>0.826</td><td>0.829</td></tr><tr><td>±65°</td><td>0.662</td><td>0.696</td><td>0.674</td><td>0.682</td></tr><tr><td>Az</td><td>-</td><td>0.384</td><td>0.338</td><td>0.586</td><td>0.409</td></tr></table>\n\nTable 4: MNIST-DIL – Accuracy of predicting y $( A _ { y } )$ .",
        "Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg",
      "image_filename": "1809.10083_page0_fig18.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig19.jpg",
      "image_filename": "1809.10083_page0_fig19.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_23",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig20.jpg",
      "image_filename": "1809.10083_page0_fig20.jpg",
      "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .",
      "context_before": "",
      "context_after": "Chairs. This dataset consists of 1393 different chair types rendered at 31 yaw angles and two pitch angles using a computer aided design model. We treat the chair identity as the target $y$ and the yaw angle $\\theta$ as $z$ . We split the data into training and testing sets by picking alternate yaw angles. Therefore, there is no overlap of θ between the two sets. We compare the performance of our model to CAI. In order to train the CAI model, we group $\\theta$ into four categories – front, left, right and back, and provide it this information as a one-hot encoded vector. We model the encoder and the predictor as two-layer neural networks for both CAI and our model. We also model the decoder as a two-layer network and the disentanglers as single-layer networks. Table 2 summarizes the results, showing that our model outperforms CAI on both $A _ { y }$ and $A _ { z }$ . Moreover, the accuracy of predicting $\\theta$ from $e _ { 2 }$ is 0.73, which shows that this information migrates to $e _ { 2 }$ . Figure 3b shows results of reconstructing $x$ from $e _ { 1 }$ and $e _ { 2 }$ generated in the same way as for Extended Yale-B above. The figure shows that $e _ { 1 }$ contains identity information but nothing about $\\theta$ while $e _ { 2 }$ contains $\\theta$ with limited identity information.\n\n5.2 Effective use of synthetic data augmentation for learning invariance\n\nData is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "Domain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the “domain” information. We evaluate the performance of our model at domain adaptation on the Amazon Reviews dataset [4] using the same preprocessing as [14]. The dataset contains text reviews on products in four domains – “books”, “dvd”, “electronics”, and “kitchen”. Each review is represented as a feature vector of unigram and bigram counts",
        "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d).",
        "Figure 5 shows the results.",
        "Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods VFAE and Domain Adversarial Neural Network (DANN) [6]."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_24",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/9cb5fff679498486028f84d122596e45b1de605343dfa64031021f243018915e.jpg",
      "image_filename": "9cb5fff679498486028f84d122596e45b1de605343dfa64031021f243018915e.jpg",
      "caption": "Table 3: Results on MNIST-ROT. $\\Theta \\ : = \\ :$ $\\{ 0 , \\pm 2 2 . 5 ^ { \\circ } , \\pm 4 5 ^ { \\circ } \\}$ was used for training. High $A _ { y }$ and low $A _ { z }$ are desired.",
      "context_before": "Chairs. This dataset consists of 1393 different chair types rendered at 31 yaw angles and two pitch angles using a computer aided design model. We treat the chair identity as the target $y$ and the yaw angle $\\theta$ as $z$ . We split the data into training and testing sets by picking alternate yaw angles. Therefore, there is no overlap of θ between the two sets. We compare the performance of our model to CAI. In order to train the CAI model, we group $\\theta$ into four categories – front, left, right and back, and provide it this information as a one-hot encoded vector. We model the encoder and the predictor as two-layer neural networks for both CAI and our model. We also model the decoder as a two-layer network and the disentanglers as single-layer networks. Table 2 summarizes the results, showing that our model outperforms CAI on both $A _ { y }$ and $A _ { z }$ . Moreover, the accuracy of predicting $\\theta$ from $e _ { 2 }$ is 0.73, which shows that this information migrates to $e _ { 2 }$ . Figure 3b shows results of reconstructing $x$ from $e _ { 1 }$ and $e _ { 2 }$ generated in the same way as for Extended Yale-B above. The figure shows that $e _ { 1 }$ contains identity information but nothing about $\\theta$ while $e _ { 2 }$ contains $\\theta$ with limited identity information.\n\n5.2 Effective use of synthetic data augmentation for learning invariance\n\nData is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic",
      "context_after": "",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs.",
        "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic\n\nTable 3: Results on MNIST-ROT.",
        "Table 3 summarizes the results, showing that our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_25",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/2b4755848ea72b2ca99f765cf8df8f273712577fce37300b75b247392d1eaf08.jpg",
      "image_filename": "2b4755848ea72b2ca99f765cf8df8f273712577fce37300b75b247392d1eaf08.jpg",
      "caption": "Table 4: MNIST-DIL – Accuracy of predicting y $( A _ { y } )$ . $k = - 2$ represents erosion with kernel-size of 2.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "in the training data. Thus, information about these $z$ cannot be used to train supervised invariance induction models. We also provide ablation results on this dataset using the same baselines $B _ { 0 }$ and $B _ { 1 }$ . Table 4 summarizes the results of this experiment. The results show significantly better performance of our model compared to CAI and the baselines. More notably, CAI performs significantly worse than our baseline models, indicating that the supervised approach of invariance ",
        "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   \n(a)",
        "<table><tr><td>Metric</td><td>Angle</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td rowspan=\"3\">Ay</td><td>Θ</td><td>0.958</td><td>0.977</td><td>0.974</td><td>0.972</td></tr><tr><td>±55°</td><td>0.826</td><td>0.856</td><td>0.826</td><td>0.829</td></tr><tr><td>±65°</td><td>0.662</td><td>0.696</td><td>0.674</td><td>0.682</td></tr><tr><td>Az</td><td>-</td><td>0.384</td><td>0.338</td><td>0.586</td><td>0.409</td></tr></table>\n\nTable 4: MNIST-DIL – Accuracy of predicting y $( A _ { y } )$ .",
        "Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_26",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg",
      "image_filename": "1809.10083_page0_fig21.jpg",
      "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .",
      "context_before": "",
      "context_after": "variations. The prediction model is then trained on the expanded dataset. The resulting model, thus, becomes robust to specific forms of variations of certain nuisance factors that it has seen during training. Invariance induction, on the other hand, aims to completely prevent prediction models from using information about nuisance factors. Data augmentation methods can be more effectively used for improving the prediction of $y$ by using the expanded dataset for inducing invariance by exclusion rather than inclusion. We use two variants of the MNIST [12] dataset of handwritten digits to (1) show the advantage of unsupervised invariance induction at this task over its supervised variant through comparison with CAI, and (2) perform ablation experiments for our model to justify our framework design. We use the same two-layer architectures for the encoder and the predictor in both our model and CAI, except that our encoder generates two encodings instead of one. We model the decoder as a three-layer neural network and the disentanglers as single-layer neural networks. We train two baseline versions of our model for our ablation experiments – $B _ { 0 }$ composed of Enc and P red, i.e., a single feed-forward network $x h y$ and $B _ { 1 }$ , which is the same as the composite model $M _ { 1 }$ , i.e., the proposed model trained non-adversarially without the disentanglers. $B _ { 0 }$ is used to validate the phenomenon that invariance by exclusion is a better approach than robustness through inclusion whereas $B _ { 1 }$ helps evaluate the importance of disentanglement in our framework.\n\nMNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta \\not \\in \\Theta$ to gauge the performance of these models on unseen variations of the rotation nuisance factor. Table 3 summarizes the results, showing that our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle. The difference in $A _ { y }$ is especially notable for the cases where $\\theta \\not \\in \\Theta$ . Results on $A _ { z }$ show that our model discards more information about $\\theta$ than CAI even though CAI uses $\\theta$ information during training. The information about $\\theta$ migrates to $e _ { 2 }$ , indicated by the accuracy of predicting it from $e _ { 2 }$ being 0.77. Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model. While raw data tends to cluster by the rotation angle, $e _ { 1 }$ shows near-perfect grouping based on the digit-class. We further visualize the $e _ { 1 }$ embedding learned by the proposed model and the baseline $B _ { 0 }$ , which models the classifier $x h y$ , to investigate the effectiveness of invariance induction by exclusion versus inclusion, respectively. Both the models were trained on digits rotated by $\\theta \\in \\Theta$ and t-SNE visualizations were generated for $\\theta \\in \\{ \\pm 5 5 \\}$ . Figure 5 shows the results. As evident, $e _ { 1 }$ learned by the proposed model shows no clustering by the rotation angle, while that learned by $B _ { 0 }$ does, with encodings of some digit classes forming multiple clusters corresponding to rotation angles. Figure 6 shows results of reconstructing $x$ from $e _ { 1 }$ and $e _ { 2 }$ generated in the same way as Extended Yale-B above. The figures show that reconstructions from $e _ { 1 }$ reflect the digit class but contain no information about $\\theta$ , while those from $e _ { 2 }$ exhibit the inverse.\n\nMNIST-DIL. We create this variant of MNIST by eroding or dilating MNIST digits using various kernel-sizes $( k )$ . We use models trained on MNIST-ROT to report evaluation results on this dataset, to show the advantage of unsupervised invariance induction in cases where certain $z$ are not annotated",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_27",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/e5ff4b31639748861b3e76c258fe19abd26f73e6a328e70e08e82e3524f6c1e6.jpg",
      "image_filename": "e5ff4b31639748861b3e76c258fe19abd26f73e6a328e70e08e82e3524f6c1e6.jpg",
      "caption": "Table 5: Results on Amazon Reviews dataset – Accuracy of predicting $y$ from $e _ { 1 }$ (Ay)",
      "context_before": "variations. The prediction model is then trained on the expanded dataset. The resulting model, thus, becomes robust to specific forms of variations of certain nuisance factors that it has seen during training. Invariance induction, on the other hand, aims to completely prevent prediction models from using information about nuisance factors. Data augmentation methods can be more effectively used for improving the prediction of $y$ by using the expanded dataset for inducing invariance by exclusion rather than inclusion. We use two variants of the MNIST [12] dataset of handwritten digits to (1) show the advantage of unsupervised invariance induction at this task over its supervised variant through comparison with CAI, and (2) perform ablation experiments for our model to justify our framework design. We use the same two-layer architectures for the encoder and the predictor in both our model and CAI, except that our encoder generates two encodings instead of one. We model the decoder as a three-layer neural network and the disentanglers as single-layer neural networks. We train two baseline versions of our model for our ablation experiments – $B _ { 0 }$ composed of Enc and P red, i.e., a single feed-forward network $x h y$ and $B _ { 1 }$ , which is the same as the composite model $M _ { 1 }$ , i.e., the proposed model trained non-adversarially without the disentanglers. $B _ { 0 }$ is used to validate the phenomenon that invariance by exclusion is a better approach than robustness through inclusion whereas $B _ { 1 }$ helps evaluate the importance of disentanglement in our framework.\n\nMNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta \\not \\in \\Theta$ to gauge the performance of these models on unseen variations of the rotation nuisance factor. Table 3 summarizes the results, showing that our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle. The difference in $A _ { y }$ is especially notable for the cases where $\\theta \\not \\in \\Theta$ . Results on $A _ { z }$ show that our model discards more information about $\\theta$ than CAI even though CAI uses $\\theta$ information during training. The information about $\\theta$ migrates to $e _ { 2 }$ , indicated by the accuracy of predicting it from $e _ { 2 }$ being 0.77. Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model. While raw data tends to cluster by the rotation angle, $e _ { 1 }$ shows near-perfect grouping based on the digit-class. We further visualize the $e _ { 1 }$ embedding learned by the proposed model and the baseline $B _ { 0 }$ , which models the classifier $x h y$ , to investigate the effectiveness of invariance induction by exclusion versus inclusion, respectively. Both the models were trained on digits rotated by $\\theta \\in \\Theta$ and t-SNE visualizations were generated for $\\theta \\in \\{ \\pm 5 5 \\}$ . Figure 5 shows the results. As evident, $e _ { 1 }$ learned by the proposed model shows no clustering by the rotation angle, while that learned by $B _ { 0 }$ does, with encodings of some digit classes forming multiple clusters corresponding to rotation angles. Figure 6 shows results of reconstructing $x$ from $e _ { 1 }$ and $e _ { 2 }$ generated in the same way as Extended Yale-B above. The figures show that reconstructions from $e _ { 1 }$ reflect the digit class but contain no information about $\\theta$ , while those from $e _ { 2 }$ exhibit the inverse.\n\nMNIST-DIL. We create this variant of MNIST by eroding or dilating MNIST digits using various kernel-sizes $( k )$ . We use models trained on MNIST-ROT to report evaluation results on this dataset, to show the advantage of unsupervised invariance induction in cases where certain $z$ are not annotated",
      "context_after": "in the training data. Thus, information about these $z$ cannot be used to train supervised invariance induction models. We also provide ablation results on this dataset using the same baselines $B _ { 0 }$ and $B _ { 1 }$ . Table 4 summarizes the results of this experiment. The results show significantly better performance of our model compared to CAI and the baselines. More notably, CAI performs significantly worse than our baseline models, indicating that the supervised approach of invariance induction can worsen performance with respect to nuisance factors not accounted for during training.\n\n5.3 Domain Adaptation\n\nDomain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the “domain” information. We evaluate the performance of our model at domain adaptation on the Amazon Reviews dataset [4] using the same preprocessing as [14]. The dataset contains text reviews on products in four domains – “books”, “dvd”, “electronics”, and “kitchen”. Each review is represented as a feature vector of unigram and bigram counts. The target $y$ is the sentiment of the review – either positive or negative. We use the same experimental setup as [6, 14] where the model is trained on one domain and tested on another, thus creating 12 source-target combinations. We design the architectures of the encoder and the decoder in our model to be similar to those of VFAE, as presented in [14]. Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods VFAE and Domain Adversarial Neural Network (DANN) [6]. The results of the prior works are quoted directly from [14]. The results show that our model outperforms both VFAE and DANN at nine out of the twelve tasks. Thus, our model can also be used effectively for domain adaptation.",
      "referring_paragraphs": [
        "MNIST-ROT. We create this variant of the MNIST dataset by randomly rotating each image by an angle $\\theta \\in \\{ - 4 5 ^ { \\circ } , - 2 2 . 5 ^ { \\circ } , 0 ^ { \\circ } , 2 2 . 5 ^ { \\circ } , 4 5 ^ { \\circ } \\}$ about the Y-axis. We denote this set of angles as $\\Theta$ . The angle information is used as a one-hot encoding while training the CAI model. We evaluate all the models on the same metrics $A _ { y }$ and $A _ { z }$ we previously used. We additionally test all the models on $\\theta",
        "Domain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the “domain” information. We evaluate the performance of our model at domain adaptation on the Amazon Reviews dataset [4] using the same preprocessing as [14]. The dataset contains text reviews on products in four domains – “books”, “dvd”, “electronics”, and “kitchen”. Each review is represented as a feature vector of unigram and bigram counts",
        "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d).",
        "Figure 5 shows the results.",
        "Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods VFAE and Domain Adversarial Neural Network (DANN) [6]."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1809.10083_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1810.01943": [
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg",
      "image_filename": "1810.01943_page0_fig0.jpg",
      "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.",
      "context_before": "[Section: AI Fairness 360]\n\n1http://www.sphinx-doc.org/en/master/\n\n2http://scikit-learn.org",
      "context_after": "ted a pull request asking to add his group’s bias mitigation algorithm. In a subsequent interview, this contributor informed us that contributing to the toolkit did not take much time as: “...it was very well structured and very easy to follow”.\n\nA simplified UML class diagram of the code is provided in Appendix A for reference. Code snippets for an instantiation of the pipeline based on our AIF360 implementation is provided in Appendix B.\n\nThe Dataset class and its subclasses are a key abstraction that handle all forms of data. Training data is used to learn classifiers. Testing data is used to make predictions and compare metrics. Besides these standard aspects of a machine learning pipeline, fairness applications also require associating protected attributes with each instance or record in the data. To maintain a common format, independent of what algorithm or metric is being applied, we chose to structure the Dataset class so that all of these relevant attributes — features, labels, protected attributes, and their respective identifiers (names describing each) — are present and accessible from each instance of the class. Subclasses add further attributes that differentiate the dataset and dictate with which algorithms and metrics it is able to",
      "referring_paragraphs": [
        "Figure 1 shows our generic pipeline for bias mitigation. Every output in this process (rectangles in the figure) is a new dataset that shares, at least, the same protected attributes as other datasets in the pipeline. Every transition is a transformation that may modify the features or labels or both between its input and output. Trapezoids represent learned models that can be used to make predictions on test data. There are also various stages in the pipeline where we can assess if bias is pres",
        "There are three main paths to the goal of making fair predictions (bottom right) — these are labelled in bold: fair pre-processing, fair in-processing, and fair post-processing. Each corresponds to a category of bias mitigation algorithms we have implemented in AIF360. Functionally, however, all three classes of algorithms act on an input dataset and produce an output dataset. This paradigm and the terminology we use for method names are familiar to the machine learning/data science community an",
        "and its subclass BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset. The metrics therein are the group fairness measures of disparate (DI) and statistical parity difference (SPD) — the ratio and difference, respectively, of the base rate conditioned on the protected attribute — and the individual fairness measure consist",
        "The bias mitigation algorithm categories are based on the location where these algorithms can intervene in a complete machine learning pipeline. If the algorithm is allowed to modify the training data, then pre-processing can be used. If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-pr",
        "Our repository has two types of tests: (1) unit tests that test individual helper functions, and (2) integration tests that test a complete flowof bias mitigation algorithms in Jupyter notebooks. Table 1 provides the statistics and code coverage information as reported by the tool py.test --cov and Jupyter notebook coverage using py.test --nbval .",
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4. For all datasets, the Reweighing and Optimized pre-processing algorithms improve fairness in both metrics presented. However, the least improvement is with German Credit dataset, possibly because it is the smallest in size. Results for disparate impact remover and learning fair representations algo",
        "Figure 1 shows our generic pipeline for bias mitigation.",
        "Figure 1.",
        "and its subclass BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.",
        "This is illustrated in Figure 1.",
        "Table 1 provides the statistics and code coverage information as reported by the tool py.test --cov and Jupyter notebook coverage using py.test --nbval .",
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg",
      "image_filename": "1810.01943_page0_fig1.jpg",
      "caption": "(a)",
      "context_before": "There is a need for a large number and variety of fairness metrics in the toolkit because there is no one best metric relevant for all contexts. It must be chosen carefully, based on subject matter expertise and worldview (Friedler et al., 2016). The comprehensiveness of the toolkit allows a user to not be hamstrung in making the most appropriate choice.\n\nThe Explainer class is intended to be associated with the Metric class and provide further insights about computed fairness metrics. Different subclasses of varying\n\n[Section: AI Fairness 360]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig2.jpg",
      "image_filename": "1810.01943_page0_fig2.jpg",
      "caption": "(b) Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.",
      "context_before": "",
      "context_after": "complexity that extend the Explainer class can be created to output explanations that are meaningful to different user personas. To the best of our knowledge, this is the first fairness toolkit that stresses the need for explanations. The explainer capability implemented in the first release of AIF360 is basic reporting through ”pretty print” and JSON outputs. Future releases may include methodologies such as fine-grained localization of bias (we describe the approach herein), actionable recourse analysis (Ustun et al., 2018), and counterfactual fairness (Wachter et al., 2018).\n\nTextExplainer, a subclass of Explainer, returns a plain text string with a metric value. For example, the explanation for the accuracy metric is simply the text string “Classification accuracy on hcounti instances: haccuracyi”, where hcounti represents the number of records, and haccuracyi the accuracy. This can be invoked for both the privileged and unprivileged instances by passing arguments.\n\nJSONExplainer extends TextExplainer and produces three output attributes in JSON format: (a) metaattributes about the metric such as its name, a natural language description of its definition and its ideal value in a bias-free world, (b) statistical attributes that include the raw and derived numbers, and (c) the plain text explanation passed unchanged from the superclass TextExplainer. Outputs from this class are consumed by the Web application described in Section 11.",
      "referring_paragraphs": [
        "enhanced (privileged group) compared to the entire group. In the feature space, the approach computes the given fairness metric across all feature values and localizes on ones that are most objectionable. Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute. Figure 3 illustrates feature bias localization on the Stanford Open Policing dataset (Pierson et al., 2017) for Connecticut, with county name as the feature and race as ",
        "Table 2 provides the datasets, metrics, classifiers, and bias mitigation algorithms used in our experiments. Additional details on the datasets and metrics are available in Appendix C. The processed Adult Census Income, German Credit, and COMPAS datasets contain 45,222, 1,000 and 6,167 records respectively. Except Adversarial debiasing and Disparate impact remover, all other bias mitigation algorithms use datasets that are cleaned and pre-processed in a similar way. Each dataset is randomly divi",
        "Figure 2.",
        "Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute.",
        "Table 2."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg",
      "image_filename": "1810.01943_page0_fig3.jpg",
      "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.",
      "context_before": "JSONExplainer extends TextExplainer and produces three output attributes in JSON format: (a) metaattributes about the metric such as its name, a natural language description of its definition and its ideal value in a bias-free world, (b) statistical attributes that include the raw and derived numbers, and (c) the plain text explanation passed unchanged from the superclass TextExplainer. Outputs from this class are consumed by the Web application described in Section 11.\n\n7.2 Fine-grained localization\n\nA more insightful explanation for fairness metrics is the localization of the source of bias at a fine granularity in the protected attribute and feature spaces. In the protected attribute space, the approach finds the values in which the given fairness metric is diminished (unprivileged group) or",
      "context_after": "enhanced (privileged group) compared to the entire group. In the feature space, the approach computes the given fairness metric across all feature values and localizes on ones that are most objectionable. Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute. Figure 3 illustrates feature bias localization on the Stanford Open Policing dataset (Pierson et al., 2017) for Connecticut, with county name as the feature and race as the protected attribute.\n\nBias mitigation algorithms attempt to improve the fairness metrics by modifying the training data, the learning algorithm, or the predictions. These algorithm categories are known as pre-processing, in-processing, and post-processing, respectively (d’Alessandro et al., 2017).\n\n8.1 Bias mitigation approaches",
      "referring_paragraphs": [
        "enhanced (privileged group) compared to the entire group. In the feature space, the approach computes the given fairness metric across all feature values and localizes on ones that are most objectionable. Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute. Figure 3 illustrates feature bias localization on the Stanford Open Policing dataset (Pierson et al., 2017) for Connecticut, with county name as the feature and race as ",
        "Figure 3."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_5",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/e3089ab9f4e3c89f8158cd685804910ce84c46411a7b542e69c0279c5c0ffc4d.jpg",
      "image_filename": "e3089ab9f4e3c89f8158cd685804910ce84c46411a7b542e69c0279c5c0ffc4d.jpg",
      "caption": "Table 1. Statistics on the Test Suite for AIF360",
      "context_before": "Unit test cases ensure that classes and functions defined in the different libraries are functionally correct and do not break the flow of the fairness detection and mitigation pipeline. Each of our classes is equipped with unit tests that attempt to cover every aspect of the class/module/functions.\n\nWe have also developed a test suite to compute the metrics reported in Section 6. Our measurements include aspects of the fairness metrics, classification metrics, dataset metrics, and distortion metrics, covering a total of 71 metrics at the time of this writing. These metrics tests can be invoked directly with any fairness algorithm. The test suite also provides unit tests for all bias mitigation algorithms and basic validation of the datasets.\n\nOur repository has two types of tests: (1) unit tests that test individual helper functions, and (2) integration tests that test a complete flowof bias mitigation algorithms in Jupyter notebooks. Table 1 provides the statistics and code coverage information as reported by the tool py.test --cov and Jupyter notebook coverage using py.test --nbval .",
      "context_after": "10 EVALUATION OF THE ALGORITHMS\n\nFairness is a complex construct that cannot be captured with a one-size-fits-all solution. Hence, our goal in this evaluation is two-fold: (a) demonstrating the capabilities of our toolkit in terms of the various fairness metrics and bias mitigation algorithms, (b) showing how a user can understand the behavior of various metrics and bias mitigation algorithms on her dataset, and make an appropriate choice according to her needs.\n\n[Section: AI Fairness 360]",
      "referring_paragraphs": [
        "Figure 1 shows our generic pipeline for bias mitigation. Every output in this process (rectangles in the figure) is a new dataset that shares, at least, the same protected attributes as other datasets in the pipeline. Every transition is a transformation that may modify the features or labels or both between its input and output. Trapezoids represent learned models that can be used to make predictions on test data. There are also various stages in the pipeline where we can assess if bias is pres",
        "There are three main paths to the goal of making fair predictions (bottom right) — these are labelled in bold: fair pre-processing, fair in-processing, and fair post-processing. Each corresponds to a category of bias mitigation algorithms we have implemented in AIF360. Functionally, however, all three classes of algorithms act on an input dataset and produce an output dataset. This paradigm and the terminology we use for method names are familiar to the machine learning/data science community an",
        "and its subclass BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset. The metrics therein are the group fairness measures of disparate (DI) and statistical parity difference (SPD) — the ratio and difference, respectively, of the base rate conditioned on the protected attribute — and the individual fairness measure consist",
        "The bias mitigation algorithm categories are based on the location where these algorithms can intervene in a complete machine learning pipeline. If the algorithm is allowed to modify the training data, then pre-processing can be used. If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-pr",
        "Our repository has two types of tests: (1) unit tests that test individual helper functions, and (2) integration tests that test a complete flowof bias mitigation algorithms in Jupyter notebooks. Table 1 provides the statistics and code coverage information as reported by the tool py.test --cov and Jupyter notebook coverage using py.test --nbval .",
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4. For all datasets, the Reweighing and Optimized pre-processing algorithms improve fairness in both metrics presented. However, the least improvement is with German Credit dataset, possibly because it is the smallest in size. Results for disparate impact remover and learning fair representations algo",
        "Figure 1 shows our generic pipeline for bias mitigation.",
        "Figure 1.",
        "and its subclass BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.",
        "This is illustrated in Figure 1.",
        "Table 1 provides the statistics and code coverage information as reported by the tool py.test --cov and Jupyter notebook coverage using py.test --nbval .",
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/d4f38b4876b06c9ce98ba5ebfc95824d49af6bd369cd4c16f0a2789ecb795f11.jpg",
      "image_filename": "d4f38b4876b06c9ce98ba5ebfc95824d49af6bd369cd4c16f0a2789ecb795f11.jpg",
      "caption": "Table 2. Overview of the experimental setup",
      "context_before": "Fairness is a complex construct that cannot be captured with a one-size-fits-all solution. Hence, our goal in this evaluation is two-fold: (a) demonstrating the capabilities of our toolkit in terms of the various fairness metrics and bias mitigation algorithms, (b) showing how a user can understand the behavior of various metrics and bias mitigation algorithms on her dataset, and make an appropriate choice according to her needs.\n\n[Section: AI Fairness 360]\n\n5https://travis-ci.org/",
      "context_after": "Table 2 provides the datasets, metrics, classifiers, and bias mitigation algorithms used in our experiments. Additional details on the datasets and metrics are available in Appendix C. The processed Adult Census Income, German Credit, and COMPAS datasets contain 45,222, 1,000 and 6,167 records respectively. Except Adversarial debiasing and Disparate impact remover, all other bias mitigation algorithms use datasets that are cleaned and pre-processed in a similar way. Each dataset is randomly divided into $5 0 \\%$ training, $2 0 \\%$ validation, and $3 0 \\%$ test partitions. Each point in the figures of results consists of a mean and a spread $\\pm 1$ standard deviation) computed using 25 such random splits. For the random forest classifier, we set the number of trees to be 100, and the minimum samples at a leaf node to be 20.\n\nFor fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4. For all datasets, the Reweighing and Optimized pre-processing algorithms improve fairness in both metrics presented. However, the least improvement is with German Credit dataset, possibly because it is the smallest in size. Results for disparate impact remover and learning fair representations algorithms are not shown since they do not modify the labels or protected attributes directly when transforming the dataset. Hence the SPD and DI values do not change during transformation.\n\nWe also train classifiers with and without bias mitigation for all the dataset and algorithm combinations. We then measure statistical parity difference and disparate impact using the predicted dataset, and average odds and equal opportunity difference using the input and predicted datasets. In all possible cases, the validation datasets were used to obtain the score threshold for the classifiers to maximize balanced accuracy. The results are all obtained on test partitions.",
      "referring_paragraphs": [
        "enhanced (privileged group) compared to the entire group. In the feature space, the approach computes the given fairness metric across all feature values and localizes on ones that are most objectionable. Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute. Figure 3 illustrates feature bias localization on the Stanford Open Policing dataset (Pierson et al., 2017) for Connecticut, with county name as the feature and race as ",
        "Table 2 provides the datasets, metrics, classifiers, and bias mitigation algorithms used in our experiments. Additional details on the datasets and metrics are available in Appendix C. The processed Adult Census Income, German Credit, and COMPAS datasets contain 45,222, 1,000 and 6,167 records respectively. Except Adversarial debiasing and Disparate impact remover, all other bias mitigation algorithms use datasets that are cleaned and pre-processed in a similar way. Each dataset is randomly divi",
        "Figure 2.",
        "Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute.",
        "Table 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig4.jpg",
      "image_filename": "1810.01943_page0_fig4.jpg",
      "caption": "(a) SPD - Re-weighing",
      "context_before": "The Web experience was designed to provide useful information for a diverse consumers. For business users, the interactive demonstration offers a sample of the toolkit’s fairness checking and mitigation capabilities without requiring any programming knowledge. For new developers, the demonstration, notebook-based tutorials, guidance on algorithm selection, and access to a user community provide multiple ways to progress from their current level of understanding into the details of the code. For more advanced developers, the detailed documentation and code are directly accessible.\n\nThe design of the Web experience proceeded through several iterations. Early clickable mock-ups of the interactive demonstration had users first select a dataset, one or two\n\n[Section: AI Fairness 360]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig5.jpg",
      "image_filename": "1810.01943_page0_fig5.jpg",
      "caption": "(b) SPD - Optimized pre-proc.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg",
      "image_filename": "1810.01943_page0_fig6.jpg",
      "caption": "(c) DI - Re-weighing",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig7.jpg",
      "image_filename": "1810.01943_page0_fig7.jpg",
      "caption": "(d) DI - Optimized pre-proc.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_11",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig8.jpg",
      "image_filename": "1810.01943_page0_fig8.jpg",
      "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1. (a) Statistical parity difference",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4. For all datasets, the Reweighing and Optimized pre-processing algorithms improve fairness in both metrics presented. However, the least improvement is with German Credit dataset, possibly because it is the smallest in size. Results for disparate impact remover and learning fair representations algo",
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4.",
        "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   \n(a) Statistical parity difference"
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "image_filename": "1810.01943_page0_fig9.jpg",
      "caption": "(b) Disparate impact",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig10.jpg",
      "image_filename": "1810.01943_page0_fig10.jpg",
      "caption": "(c) Average odds difference",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_14",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig11.jpg",
      "image_filename": "1810.01943_page0_fig11.jpg",
      "caption": "(d) Equal opportunity difference Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "context_before": "",
      "context_after": "protected attributes to check for bias, and one of up to five metrics to use for checking. We learned, however, that this was overwhelming, even for those familiar with AI, since it required choices they were not yet equipped to make. As a result, we simplified the experience by asking users to first select only one of three datasets to explore. Bias checking results were then graphically presented for two protected attributes across five different metrics. Users could then select a mitigation algorithm leading to a report comparing bias before and after mitigation. The design of the charts for each bias metric also evolved in response to user feedback as we learned the importance of depicting a colorcoded range of values considered fair or biased with more detailed information being available in an overlay. Figure 6 shows the before and after mitigation graphs from the interactive Web experience.\n\nThe design of the rest of the site also went through several iterations. Of particular concern, the front page sought to",
      "referring_paragraphs": [
        "race as protected attribute is shown in Figure 5. The rest of the results referenced here are available in Appendix D. Disparate impact remover and adversarial debiasing use differently processed datasets and hence their metrics in the top panel are different from others. The first thing that strikes when glancing at the figure is that the four different metrics seem to be correlated. Also the uncertainty in classification accuracy is much smaller compared to the uncertainty in the fairness metr",
        "An example result for Adult Census Income dataset with\n\nrace as protected attribute is shown in Figure 5.",
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig12.jpg",
      "image_filename": "1810.01943_page0_fig12.jpg",
      "caption": "Before mitigation",
      "context_before": "protected attributes to check for bias, and one of up to five metrics to use for checking. We learned, however, that this was overwhelming, even for those familiar with AI, since it required choices they were not yet equipped to make. As a result, we simplified the experience by asking users to first select only one of three datasets to explore. Bias checking results were then graphically presented for two protected attributes across five different metrics. Users could then select a mitigation algorithm leading to a report comparing bias before and after mitigation. The design of the charts for each bias metric also evolved in response to user feedback as we learned the importance of depicting a colorcoded range of values considered fair or biased with more detailed information being available in an overlay. Figure 6 shows the before and after mitigation graphs from the interactive Web experience.\n\nThe design of the rest of the site also went through several iterations. Of particular concern, the front page sought to",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_16",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg",
      "image_filename": "1810.01943_page0_fig13.jpg",
      "caption": "After adversarial debiasing mitigation Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.",
      "context_before": "",
      "context_after": "[Section: AI Fairness 360]\n\nconvey toolkit richness while still being approachable. In the final design, a short textual introduction to the content of the site, along with direct links to the API documentation and code repository, is followed by a number of direct links to various levels of advice and examples. Further links to the individual datasets, the bias checkers, and the mitigation algorithms are also provided. In all this, we ensured the site was suitably responsive across all major desktop and mobile platforms.\n\n11.2 Design of the back-end service",
      "referring_paragraphs": [
        "protected attributes to check for bias, and one of up to five metrics to use for checking. We learned, however, that this was overwhelming, even for those familiar with AI, since it required choices they were not yet equipped to make. As a result, we simplified the experience by asking users to first select only one of three datasets to explore. Bias checking results were then graphically presented for two protected attributes across five different metrics. Users could then select a mitigation a",
        "Figure 6 shows the before and after mitigation graphs from the interactive Web experience.",
        "After adversarial debiasing mitigation   \nFigure 6."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_17",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg",
      "image_filename": "1810.01943_page0_fig14.jpg",
      "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.",
      "context_before": "[Section: AI Fairness 360]\n\n[Section: AI Fairness 360]\n\n[Section: AI Fairness 360]",
      "context_after": "This example provides Python code snippets for some common tasks that the user might perform using our toolbox. The example involves the user loading a dataset, splitting it into training and testing partitions, understanding the outcome disparity between two demographic groups, and transforming the dataset to mitigate this disparity. A more detailed version of this example is available in url.redacted.\n\nB.1 Dataset operations\n\n[Section: AI Fairness 360]",
      "referring_paragraphs": [
        "Figure 7."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg",
      "image_filename": "1810.01943_page0_fig15.jpg",
      "caption": "(a) Statistical parity difference (b) Disparate impact (c) Average odds difference",
      "context_before": "D EVALUATION ON DIFFERENT DATA SETS\n\nWe present additional results with bias mitigation obtained for various datasets and protected attributes. These correspond to the setting described in Section 10.\n\n[Section: AI Fairness 360]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_19",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig16.jpg",
      "image_filename": "1810.01943_page0_fig16.jpg",
      "caption": "(d) Equal opportunity difference Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex. (a) Statistical parity difference (b) Disparate impact (c) Average odds difference (d) Equal opportunity difference Figure 9. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: sex.",
      "context_before": "",
      "context_after": "[Section: AI Fairness 360]",
      "referring_paragraphs": [
        "Figure 8."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg",
      "image_filename": "1810.01943_page0_fig17.jpg",
      "caption": "(a) Statistical parity difference (b) Disparate impact (c) Average odds difference",
      "context_before": "[Section: AI Fairness 360]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_21",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig18.jpg",
      "image_filename": "1810.01943_page0_fig18.jpg",
      "caption": "(d) Equal opportunity difference Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age. (a) Statistical parity difference (b) Disparate impact (c) Average odds difference (d) Equal opportunity difference Figure 11. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: sex.",
      "context_before": "",
      "context_after": "[Section: AI Fairness 360]",
      "referring_paragraphs": [
        "Figure 10."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_22",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg",
      "image_filename": "1810.01943_page0_fig19.jpg",
      "caption": "(a) Statistical parity difference (b) Disparate impact (c) Average odds difference (d) Equal opportunity difference Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.",
      "context_before": "[Section: AI Fairness 360]",
      "context_after": "[Section: AI Fairness 360]",
      "referring_paragraphs": [
        "Figure 12."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig20.jpg",
      "image_filename": "1810.01943_page0_fig20.jpg",
      "caption": "4.Compare original vs. mitigated results",
      "context_before": "[Section: AI Fairness 360]",
      "context_after": "4.Compare original vs. mitigated results\n\nDataset: Adult census income\n\nMitigation: Optimized Pre-processing algorithm applied",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig21.jpg",
      "image_filename": "1810.01943_page0_fig21.jpg",
      "caption": "(1 of 5 metrics stillindicate bias for unprivileged group)",
      "context_before": "Accuracy after mitigation changed from $8 2 \\%$ to $7 4 \\%$\n\nBias against unprivileged group was reduced to acceptable levels*for1 of 2 previously biased metrics\n\n(1 of 5 metrics stillindicate bias for unprivileged group)",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig22.jpg",
      "image_filename": "1810.01943_page0_fig22.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig23.jpg",
      "image_filename": "1810.01943_page0_fig23.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_27",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig24.jpg",
      "image_filename": "1810.01943_page0_fig24.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_28",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig25.jpg",
      "image_filename": "1810.01943_page0_fig25.jpg",
      "caption": "Figure 13. A screen shot from the web interactive experience, showing the results of mitigation applied to one of the available datasets.",
      "context_before": "",
      "context_after": "[Section: AI Fairness 360]",
      "referring_paragraphs": [
        "Figure 13. A screen shot from the web interactive experience, showing the results of mitigation applied to one of the available datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.01943_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1810.03611": [
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1e353753481ec4eb115083998af059774f8e9434add5bb5a581494f3fa6ac451.jpg",
      "image_filename": "1e353753481ec4eb115083998af059774f8e9434add5bb5a581494f3fa6ac451.jpg",
      "caption": "Table 1. WEAT Target and Attribute Sets",
      "context_before": "Experimental Methodology. To test the accuracy of our methodology, ideally we would simply remove a single document from a word embedding’s corpus, train a new embedding, and compare the change in bias with our differential bias approximation. However, the cosine similarities between small sets of word vectors in two word embeddings trained on the same corpus can differ considerably simply because of the stochastic nature of the optimization (Antoniak & Mimno, 2018). As a result, the WEAT biases vary between training runs. The effect of removing a single document, which is near zero for a typical document, is hidden in this variation. Fixing the random seed is not a practical approach. Many popular word embedding implementations also require limiting training to a single thread to fully eliminate randomness. This would make experimentation prohibitively slow.\n\nIn order to obtain measurable changes, we instead remove sets of documents, resulting in larger corpus perturbations. Accuracy is assessed by comparing our method’s predictions to the actual change in bias measured when each document set is removed from the corpus and a new embedding is trained on this perturbed corpus. Furthermore, we make all predictions and assessments using several embeddings, each\n\n[Section: Understanding the Origins of Bias in Word Embeddings]",
      "context_after": "",
      "referring_paragraphs": [
        "Choice of experimental bias metric. Throughout our experiments, we consider the effect size of two different WEAT biases as presented by Caliskan et al. (2017). Recall that these metrics have been shown to correlate with known human biases as measured by the Implicit Association Test. In WEAT1, the target word sets are science and arts terms, while the attribute word sets are male and female terms. In WEAT2, the target word sets are musical instruments and weapons, while the attribute word sets ",
        "We construct three types of perturbation sets: increase, random, and decrease. The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1. The random perturbation sets are simply documents chosen from the corpus uniformly at random. For a more detailed description, please refer to the supplemental material. Most ",
        "A histogram of the differential bias of removal for each document in our NYT setup (WEAT1) can be seen in Figure 1. Notice the log scale on the vertical axis, and how the vast majority of documents are predicted to have a very small impact on the differential bias.",
        "They are summarized in Table 1.",
        "The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.",
        "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/9a4f8c055cbaeb974d353607b6c58e301306654378d4b17e4eda856296cd5d42.jpg",
      "image_filename": "9a4f8c055cbaeb974d353607b6c58e301306654378d4b17e4eda856296cd5d42.jpg",
      "caption": "Table 2. Baseline WEAT Effect Sizes",
      "context_before": "",
      "context_after": "trained with the same hyperparameters, but differing in their random seeds.\n\nWe construct three types of perturbation sets: increase, random, and decrease. The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1. The random perturbation sets are simply documents chosen from the corpus uniformly at random. For a more detailed description, please refer to the supplemental material. Most of the code used in the experimentation has been made available online2.\n\nExperimental Results. Here we present a subset of our experimental results, principally from NYT WEAT1 (science vs. arts). Complete sets of results from the four configurations $( \\{ \\mathrm { N Y T } , \\mathrm { W i k i } \\} \\times \\{ \\mathrm { W E A T 1 } , \\mathrm { W E A T 2 } \\} )$ ) can be found in the supplemental materials.",
      "referring_paragraphs": [
        "The baseline WEAT effect sizes ( $\\pm 1$ std. dev.) are shown in Table 2. It is worth noting that the WEAT2 (weapons vs. instruments) bias was not significant in our Wiki setup. However, our analysis does not require that the bias under consideration fall within any particular range of values.",
        "We assess the accuracy of our approximations by measuring how they correlate with the ground truth change in bias (as measured by retraining the embedding after removing a subset of the training corpus). Recall these ground truth changes are obtained using several retraining runs with different random seeds. We find extremely strong correlations $( r ^ { 2 } \\geq 0 . 9 8 5 )$ in every configuration, for example Figure 2.",
        "9 8 5 )$ in every configuration, for example Figure 2.",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg",
      "image_filename": "1810.03611_page0_fig0.jpg",
      "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.",
      "context_before": "A histogram of the differential bias of removal for each document in our NYT setup (WEAT1) can be seen in Figure 1. Notice the log scale on the vertical axis, and how the vast majority of documents are predicted to have a very small impact on the differential bias.\n\nWe assess the accuracy of our approximations by measuring how they correlate with the ground truth change in bias (as measured by retraining the embedding after removing a subset of the training corpus). Recall these ground truth changes are obtained using several retraining runs with different random seeds. We find extremely strong correlations $( r ^ { 2 } \\geq 0 . 9 8 5 )$ in every configuration, for example Figure 2.\n\nWe further compare our approximations to the ground truth",
      "context_after": "",
      "referring_paragraphs": [
        "Choice of experimental bias metric. Throughout our experiments, we consider the effect size of two different WEAT biases as presented by Caliskan et al. (2017). Recall that these metrics have been shown to correlate with known human biases as measured by the Implicit Association Test. In WEAT1, the target word sets are science and arts terms, while the attribute word sets are male and female terms. In WEAT2, the target word sets are musical instruments and weapons, while the attribute word sets ",
        "We construct three types of perturbation sets: increase, random, and decrease. The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1. The random perturbation sets are simply documents chosen from the corpus uniformly at random. For a more detailed description, please refer to the supplemental material. Most ",
        "A histogram of the differential bias of removal for each document in our NYT setup (WEAT1) can be seen in Figure 1. Notice the log scale on the vertical axis, and how the vast majority of documents are predicted to have a very small impact on the differential bias.",
        "They are summarized in Table 1.",
        "The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.",
        "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg",
      "image_filename": "1810.03611_page0_fig1.jpg",
      "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.",
      "context_before": "",
      "context_after": "in Figure 3. We see that while our approximations underestimate the magnitude of the change in effect size when the perturbation causes the bias to invert, relative ranking is nonetheless preserved. There was no apparent change in the TOP-1 analogy performance of the perturbed embeddings.\n\nWe ran a Welch’s t-test comparing the perturbed embeddings’ biases with the baseline biases measured in the original (unperturbed) embeddings. For 36 random perturbation sets, only 2 differed significantly $( p < 0 . 0 5 )$ from the baseline. Both of these sets were perturbations of the smaller Wiki corpus and they only caused a significant difference for WEAT2. This is in strong contrast to the 40 targeted perturbation sets, where only 2 did not significantly differ from their respective baselines. In this case, both were from the smallest (10 document) perturbation sets.\n\n5.3. Comparison to a PPMI Baseline",
      "referring_paragraphs": [
        "The baseline WEAT effect sizes ( $\\pm 1$ std. dev.) are shown in Table 2. It is worth noting that the WEAT2 (weapons vs. instruments) bias was not significant in our Wiki setup. However, our analysis does not require that the bias under consideration fall within any particular range of values.",
        "We assess the accuracy of our approximations by measuring how they correlate with the ground truth change in bias (as measured by retraining the embedding after removing a subset of the training corpus). Recall these ground truth changes are obtained using several retraining runs with different random seeds. We find extremely strong correlations $( r ^ { 2 } \\geq 0 . 9 8 5 )$ in every configuration, for example Figure 2.",
        "9 8 5 )$ in every configuration, for example Figure 2.",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg",
      "image_filename": "1810.03611_page0_fig2.jpg",
      "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)",
      "context_before": "We have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more\n\n[Section: Understanding the Origins of Bias in Word Embeddings]\n\n2Code at https://github.com/mebrunet/understanding-bias",
      "context_after": "naive, straightforward approach? The positive point-wise mutual information (PPMI) matrix is a count-based distributed representation commonly used in natural language processing (Levy et al., 2015). We compare the WEAT effect size in our NYT GloVe embeddings versus when measured in the corpus’ PPMI representation (on 2000 randomly generated word sets). As expected, there is a clear correlation $( r ^ { 2 } = 0 . 7 2 5 )$ . It is therefore sensible to use the change in PPMI WEAT effect size to predict how the GloVe WEAT effect size will change.\n\nA change in the PPMI representation due to a co-occurrence perturbation (e.g. document removal) can be computed rapidly. This allows us to scan the whole corpus for the most bias influencing documents. However, we find that the documents identified in this way have a much smaller impact on the bias than those identified by our method. For example in our Wiki setup (WEAT1) removing the 10 documents identified as most bias increasing by the PPMI method reduced the WEAT effect size by $4 \\%$ . In contrast, the 10 identified by our method reduced it by $40 \\%$ . Further comparisons are tabulated in the supplemental material.\n\n5.4. Impact on Word2Vec and Other Bias Metrics",
      "referring_paragraphs": [
        "in Figure 3. We see that while our approximations underestimate the magnitude of the change in effect size when the perturbation causes the bias to invert, relative ranking is nonetheless preserved. There was no apparent change in the TOP-1 analogy performance of the perturbed embeddings.",
        "Importantly, we also noticed a large portion of the most bias influencing documents dealt with astronomy or contained hers, the rarest words their respective WEAT subsets. Upon further investigation, we found that the log of a word’s frequency is correlated with the extent to which its relative position (among WEAT words) is affected by the perturbation sets $' r ^ { 2 } = 0 . 8 2 8 )$ ). This can be seen in Figure 5. Not surprisingly, our results indicate that the embedded representations of ra",
        "Table 3 presents a summary of the corpora and embedding hyperparameters used throughout our experimentation. We list the complete set of words used in each of the two WEATs below.",
        "I - Train a baseline. We start by training 10 word embeddings using the parameters in Table 3 above, but using different random seeds. These embeddings create a baseline for the unperturbed bias $B ( w ^ { * } )$ .",
        "in Figure 3.",
        "Figure 3.",
        "0 7 \\%$ of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000.",
        "Table 3 presents a summary of the corpora and embedding hyperparameters used throughout our experimentation.",
        "We start by training 10 word embeddings using the parameters in Table 3 above, but using different random seeds."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg",
      "image_filename": "1810.03611_page0_fig3.jpg",
      "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)",
      "context_before": "5.4. Impact on Word2Vec and Other Bias Metrics\n\nThe documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings. Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\n\nWe start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
      "context_after": "how the WEAT effect size changes in GloVe, the PPMI, and word2vec for each set (NYT-WEAT1). We see that while the response is weaker, both the PPMI representation and the word2vec embeddings show a clear change in effect size due to the perturbations. For example, the baseline WEAT effect size in word2vec is 1.35 in the unperturbed corpus, but after removing decrease-10000 (the 10k most bias contributing documents for GloVe), the effect size drops to 0.11. This means we have nearly neutralized the bias in word2vec through the removal of less than $1 \\%$ of the corpus (and there is no significant change in TOP-1 analogy performance).\n\nWe also see a change as measured by other bias metrics in our perturbed GloVe embeddings. The metric proposed by Bolukbasi et al. (2016) involves computing a single dimensional gender subspace using a definitional sets of words. One can then project test words onto this axis and measure how the embedding implicitly genders them. We explore this in our NYT setup by using the WEAT 1 attribute word sets (male, female) to construct a gender axis, then projecting the target words (science, arts) onto it. In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents. We see a strong response to the perturbations in the expected directions.\n\n5.5. Qualitative Analysis",
      "referring_paragraphs": [
        "We start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
        "The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings. Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\n\nWe start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
        "Figure 4.",
        "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.\n\nTable 4. Correlation of Approximated and Validated Mean Biases   \n\n<table><tr><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td>Wiki</td><td>r2: 0.986</td><td>r2: 0.993</td></tr><tr><td>NYT</td><td>r2: 0.995</td><td>r2: 0.997</td></tr></table>",
        "ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_7",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg",
      "image_filename": "1810.03611_page0_fig4.jpg",
      "caption": "Figure 5. The effect of removing the 10k most bias increasing and bias decreasing documents as identified by our method on the projection of the target words onto the gender axis vs. unperturbed corpus (base); error bars show one std dev; corpus word frequency noted in parentheses. (NYT - WEAT1)",
      "context_before": "We comment here on the 50 most bias influencing doc-\n\n[Section: Understanding the Origins of Bias in Word Embeddings]\n\n3We use a CBOW architecture with the same vocabulary, vector dimensions, and window size as our GloVe embeddings.",
      "context_after": "uments in the New York Times corpus, considering the WEAT 1 bias metric ({male, female}, {science, arts}). This list is included in the supplemental materials. We indeed found that most of these documents could be readily understood to affect the bias in the expected semantic sense. For example, the second most bias decreasing document is entitled “For Women in Astronomy, a Glass Ceiling in the Sky”, which investigates the pay and recognition gap in astronomy. Many of the other bias decreasing documents included interviews with female doctors or scientists.\n\nCorrespondingly, the most bias increasing documents consisted mainly of articles describing the work of male engineers and scientists. There were several obituary entries detailing the scientific accomplishments of men, e.g., “Kaj Aage Strand, 93, Astronomer At the U.S. Naval Observatory”. Perhaps the most self-evident example was an article entitled “60 New Members Elected to Academy of Sciences”, a list of almost exclusively male scientists receiving awards.\n\nThere were, however, a few examples of articles that seemed like their semantic content should affect the bias inversely to how they were categorized. For example, an article entitled “The Guide”, a guide to events in Long Island, mentions that the group Woman in Science would be hosting an astronomy event, but nonetheless increases the bias. Only 2 or 3 documents seemed altogether unrelated to the bias’ theme.",
      "referring_paragraphs": [
        "We also see a change as measured by other bias metrics in our perturbed GloVe embeddings. The metric proposed by Bolukbasi et al. (2016) involves computing a single dimensional gender subspace using a definitional sets of words. One can then project test words onto this axis and measure how the embedding implicitly genders them. We explore this in our NYT setup by using the WEAT 1 attribute word sets (male, female) to construct a gender axis, then projecting the target words (science, arts) onto",
        "Importantly, we also noticed a large portion of the most bias influencing documents dealt with astronomy or contained hers, the rarest words their respective WEAT subsets. Upon further investigation, we found that the log of a word’s frequency is correlated with the extent to which its relative position (among WEAT words) is affected by the perturbation sets $' r ^ { 2 } = 0 . 8 2 8 )$ ). This can be seen in Figure 5. Not surprisingly, our results indicate that the embedded representations of ra",
        "In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents.",
        "Figure 5.",
        "Table 5."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/f5d68b0539e03f9b53b9cb62ec6c8dfca6a6a396b8b337ee9f3b683ff24d41d6.jpg",
      "image_filename": "f5d68b0539e03f9b53b9cb62ec6c8dfca6a6a396b8b337ee9f3b683ff24d41d6.jpg",
      "caption": "Table 3. Experimental Setups",
      "context_before": "Note that since $\\nabla _ { w _ { i } } L ( \\tilde { X } _ { i } ( Y ) , w ^ { * } )$ is not differentiable in $Y$ at $Y = 0$ where $X _ { i j } = 0$ , the bias gradient is only defined at non-zero co-occurrences. This prevents us from using the bias gradient to study corpus additions which create previously unseen word co-occurrences. However, this does not affect our ability to study arbitrary removals from the corpus, since removals cannot affect a zero-valued co-occurrence. Of course, nothing limits us from using the bias gradient to also consider additions to the corpus that not change the set of zero co-occurrences.\n\nB. Experimental Setup\n\nTable 3 presents a summary of the corpora and embedding hyperparameters used throughout our experimentation. We list the complete set of words used in each of the two WEATs below.",
      "context_after": "",
      "referring_paragraphs": [
        "in Figure 3. We see that while our approximations underestimate the magnitude of the change in effect size when the perturbation causes the bias to invert, relative ranking is nonetheless preserved. There was no apparent change in the TOP-1 analogy performance of the perturbed embeddings.",
        "Importantly, we also noticed a large portion of the most bias influencing documents dealt with astronomy or contained hers, the rarest words their respective WEAT subsets. Upon further investigation, we found that the log of a word’s frequency is correlated with the extent to which its relative position (among WEAT words) is affected by the perturbation sets $' r ^ { 2 } = 0 . 8 2 8 )$ ). This can be seen in Figure 5. Not surprisingly, our results indicate that the embedded representations of ra",
        "Table 3 presents a summary of the corpora and embedding hyperparameters used throughout our experimentation. We list the complete set of words used in each of the two WEATs below.",
        "I - Train a baseline. We start by training 10 word embeddings using the parameters in Table 3 above, but using different random seeds. These embeddings create a baseline for the unperturbed bias $B ( w ^ { * } )$ .",
        "in Figure 3.",
        "Figure 3.",
        "0 7 \\%$ of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000.",
        "Table 3 presents a summary of the corpora and embedding hyperparameters used throughout our experimentation.",
        "We start by training 10 word embeddings using the parameters in Table 3 above, but using different random seeds."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/28a64e64e216b283fd407ce4b66c706c6b5cfe566ef3368dbda7e030b12f7236.jpg",
      "image_filename": "28a64e64e216b283fd407ce4b66c706c6b5cfe566ef3368dbda7e030b12f7236.jpg",
      "caption": "WEAT 1",
      "context_before": "",
      "context_after": "[Section: Understanding the Origins of Bias in Word Embeddings]",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig5.jpg",
      "image_filename": "1810.03611_page0_fig5.jpg",
      "caption": "C. Detailed Experimental Methodology",
      "context_before": "[Section: Understanding the Origins of Bias in Word Embeddings]",
      "context_after": "C. Detailed Experimental Methodology\n\nHere we detail the experimental methodology used to test our method’s accuracy.\n\nI - Train a baseline. We start by training 10 word embeddings using the parameters in Table 3 above, but using different random seeds. These embeddings create a baseline for the unperturbed bias $B ( w ^ { * } )$ .",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg",
      "image_filename": "1810.03611_page0_fig6.jpg",
      "caption": "Here we include additional experimental results.",
      "context_before": "[Section: Understanding the Origins of Bias in Word Embeddings]\n\nD. Additional experimental results\n\nHere we include additional experimental results.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig7.jpg",
      "image_filename": "1810.03611_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig8.jpg",
      "image_filename": "1810.03611_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_14",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig9.jpg",
      "image_filename": "1810.03611_page0_fig9.jpg",
      "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.\n\nTable 4. Correlation of Approximated and Validated Mean Biases   \n\n<table><tr><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td>Wiki</td><td>r2: 0.986</td><td>r2: 0.993</td></tr><tr><td>NYT</td><td>r2: 0.995</td><td>r2: 0.997</td></tr></table>"
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_15",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/f67744498774048a822faebebb8b66cdf66845809b95d0fec99be0377f9bcaa1.jpg",
      "image_filename": "f67744498774048a822faebebb8b66cdf66845809b95d0fec99be0377f9bcaa1.jpg",
      "caption": "Table 4. Correlation of Approximated and Validated Mean Biases",
      "context_before": "",
      "context_after": "[Section: Understanding the Origins of Bias in Word Embeddings]",
      "referring_paragraphs": [
        "We start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
        "The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings. Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\n\nWe start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
        "Figure 4.",
        "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.\n\nTable 4. Correlation of Approximated and Validated Mean Biases   \n\n<table><tr><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td>Wiki</td><td>r2: 0.986</td><td>r2: 0.993</td></tr><tr><td>NYT</td><td>r2: 0.995</td><td>r2: 0.997</td></tr></table>",
        "ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig10.jpg",
      "image_filename": "1810.03611_page0_fig10.jpg",
      "caption": "Validated Effect Size",
      "context_before": "[Section: Understanding the Origins of Bias in Word Embeddings]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig11.jpg",
      "image_filename": "1810.03611_page0_fig11.jpg",
      "caption": "Validated Effect Size",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg",
      "image_filename": "1810.03611_page0_fig12.jpg",
      "caption": "Validated Effect Size",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_19",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig13.jpg",
      "image_filename": "1810.03611_page0_fig13.jpg",
      "caption": "Validated Effect Size Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Validated Effect Size   \nFigure 7."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_20",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/c36c3e32b74e5e45128cea39a1877830404fdf253dea6a041a5cdb932f5625e3.jpg",
      "image_filename": "c36c3e32b74e5e45128cea39a1877830404fdf253dea6a041a5cdb932f5625e3.jpg",
      "caption": "Table 5. A comparison of the effect of removing the most impactful documents as identified by a PPMI baseline technique versus when identified by our method (Wiki setup, mean of WEAT1 in 10 retrained GloVe embeddings).",
      "context_before": "",
      "context_after": "[Section: Understanding the Origins of Bias in Word Embeddings]",
      "referring_paragraphs": [
        "We also see a change as measured by other bias metrics in our perturbed GloVe embeddings. The metric proposed by Bolukbasi et al. (2016) involves computing a single dimensional gender subspace using a definitional sets of words. One can then project test words onto this axis and measure how the embedding implicitly genders them. We explore this in our NYT setup by using the WEAT 1 attribute word sets (male, female) to construct a gender axis, then projecting the target words (science, arts) onto",
        "Importantly, we also noticed a large portion of the most bias influencing documents dealt with astronomy or contained hers, the rarest words their respective WEAT subsets. Upon further investigation, we found that the log of a word’s frequency is correlated with the extent to which its relative position (among WEAT words) is affected by the perturbation sets $' r ^ { 2 } = 0 . 8 2 8 )$ ). This can be seen in Figure 5. Not surprisingly, our results indicate that the embedded representations of ra",
        "In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents.",
        "Figure 5.",
        "Table 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg",
      "image_filename": "1810.03611_page0_fig14.jpg",
      "caption": "Understanding the Origins of Bias in Word Embeddings",
      "context_before": "[Section: Understanding the Origins of Bias in Word Embeddings]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig15.jpg",
      "image_filename": "1810.03611_page0_fig15.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig16.jpg",
      "image_filename": "1810.03611_page0_fig16.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_24",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg",
      "image_filename": "1810.03611_page0_fig17.jpg",
      "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines"
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig18.jpg",
      "image_filename": "1810.03611_page0_fig18.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/f9eedf8c4064d831a6777d5bf1d8df73e2716a1d5b8ae9e82c057700e431567b.jpg",
      "image_filename": "f9eedf8c4064d831a6777d5bf1d8df73e2716a1d5b8ae9e82c057700e431567b.jpg",
      "caption": "Understanding the Origins of Bias in Word Embeddings",
      "context_before": "The below documents were identified to be the 50 most WEAT1 bias influencing documents in our NYT setup. We list the article titles. Publication dates range from January 1, 1987 to June 19, 2007. Most can be found through https://www.nytimes.com/search. A subscription may be required for access.\n\n∆docB Bias Decreasing\n\n[Section: Understanding the Origins of Bias in Word Embeddings]",
      "context_after": "[Section: Understanding the Origins of Bias in Word Embeddings]\n\nF. Influence of Mulitple Perturbations\n\nHere we show how we can extend the influence function equations presented by Koh & Liang (2017) to address the case of multiple training point perturbations. We do not intend this to be a rigorous mathematical proof, but rather to provide insight into the logical steps we followed.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03611_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    }
  ],
  "1810.03993": [
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg",
      "image_filename": "1810.03993_page0_fig0.jpg",
      "caption": "Quantitative Analyses",
      "context_before": "Ethical Considerations\n\n• Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.\n\nCaveats and Recommendations Caveats and Recommendations",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg",
      "image_filename": "1810.03993_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig2.jpg",
      "image_filename": "1810.03993_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig3.jpg",
      "image_filename": "1810.03993_page0_fig3.jpg",
      "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.",
      "context_before": "",
      "context_after": "[Section: Model Cards for Model Reporting]\n\n[Section: FAT* ’19, January 29–31, 2019, Atlanta, GA, USA]\n\nModel Card - Toxicity in Text",
      "referring_paragraphs": [
        "As a step towards this goal, we propose that released machine learning models be accompanied by short (one to two page) records we call model cards. Model cards (for model reporting) are complements to “Datasheets for Datasets” [21] and similar recently proposed documentation paradigms [3, 28] that report details of the datasets used to train and test machine learning models. Model cards are also similar to the tripod statement proposal in medicine [25]. We provide two example model cards in Sec",
        "Not only does this practice improve model understanding and help to standardize decision making processes for invested stakeholders, but it also encourages forward-looking model analysis techniques. For example, slicing the evaluation across groups functions to highlight errors that may fall disproportionately on some groups of people, and accords with many recent notions of mathematical fairness (discussed further in the example model card in Figure 2). Including group analysis as part of the r",
        "Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the model according to the chosen metrics, providing confidence interval values when possible. Parity on the different metrics across disaggregated population subgroups corresponds to how fairness is often defined [37, 48]. Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and vis",
        "To show an example of a model card for an image classification problem, we use the public CelebA dataset [36] to examine the performance of a trained “smiling” classifier across both age and gender categories. Figure 2 shows our prototype.",
        "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).",
        "For example, slicing the evaluation across groups functions to highlight errors that may fall disproportionately on some groups of people, and accords with many recent notions of mathematical fairness (discussed further in the example model card in Figure 2).",
        "Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2.",
        "Figure 2 shows our prototype.",
        "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig4.jpg",
      "image_filename": "1810.03993_page0_fig4.jpg",
      "caption": "Quantitative Analyses",
      "context_before": "• Following [31], the Perspective API uses a set of values to guide their work. These values are Community, Transparency, Inclusivity, Privacy, and Topic-neutrality. Because of privacy considerations, the model does not take into account user history when making judgments about toxicity.\n\nCaveats and Recommendations\n\n• Synthetic test data covers only a small set of very specific comments. While these are designed to be representative of common use cases and concerns, it is not comprehensive.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg",
      "image_filename": "1810.03993_page0_fig5.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig6.jpg",
      "image_filename": "1810.03993_page0_fig6.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig7.jpg",
      "image_filename": "1810.03993_page0_fig7.jpg",
      "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.",
      "context_before": "",
      "context_after": "[Section: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Timnit Gebru]\n\ncards are intended to accompany a model after careful review has determined that the foreseeable benefits outweigh the foreseeable risks in the model’s use or release.\n\nTo demonstrate the use of model cards in practice, we have provided two examples: A model card for a smiling classifier tested on the CelebA dataset, and a model card for a public toxicity detector tested on the Identity Phrase Templates dataset. We report confusion matrix metrics for the smile classifier and Pinned AUC for the toxicity detector, along with model details, intended use, pointers to information about training and evaluation data, ethical considerations, and further caveats and recommendations.",
      "referring_paragraphs": [
        "As a step towards this goal, we propose that released machine learning models be accompanied by short (one to two page) records we call model cards. Model cards (for model reporting) are complements to “Datasheets for Datasets” [21] and similar recently proposed documentation paradigms [3, 28] that report details of the datasets used to train and test machine learning models. Model cards are also similar to the tripod statement proposal in medicine [25]. We provide two example model cards in Sec",
        "Our second example provides a model card for Perspective API’s TOXICITY classifier built to detect ‘toxicity’ in text [32], and is presented in Figure 3. To evaluate the model, we use an intersectional version of the open source, synthetically created Identity Phrase Templates test set published in [11]. We show two versions of the quantitative analysis: one for TOXICITY v. 1, the initial version of the this model, and one for TOXICITY v. 5, the latest version.",
        "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).",
        "Our second example provides a model card for Perspective API’s TOXICITY classifier built to detect ‘toxicity’ in text [32], and is presented in Figure 3.",
        "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.03993_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1810.08683": [
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1dabac05aef99790f7a4466ca775295b179421fae3be7f224ecb6c0ae06fc2e5.jpg",
      "image_filename": "1dabac05aef99790f7a4466ca775295b179421fae3be7f224ecb6c0ae06fc2e5.jpg",
      "caption": "Table 1: Adult dataset: statistics with reference to the sensitive features.",
      "context_before": "[Section: L. Oneto et al.]\n\n7https://archive.ics.uci.edu/ml/datasets/adult\n\n8www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm",
      "context_after": "criminal defendants likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants based on a 2-years follow up study. This dataset contains variables used by the COMPAS algorithm in scoring defendants, along with their outcomes within two years of the decision, for over 10000 criminal defendants in Broward County, Florida. In the original data, 3 subsets are provided. We concentrate on the one that includes only violent recividism. Table 2, analogously to Table 1, reports the statistics with reference to the sensitive features.\n\nIn all the experiments, we compare STL, ITL, and MTL in different settings. Specifically we test each method in the following cases: when the models use the sensitive feature $\\scriptstyle ( S = 1 )$ ) or not $( \\mathsf { S } \\mathrm { = } 0 )$ , when the fairness constraint is active $\\left( \\mathrm { F } { = } 1 \\right)$ ) or not $( \\mathrm { F } { = } 0 )$ , when we consider the group specific models $\\mathrm { ( D } { = } 1 \\mathrm { ) }$ ) or the shared model between groups $\\mathrm { ( D = } 0 )$ ), and when we use the true sensitive feature $( \\mathrm { P } { = } 1 )$ or the predicted one $( { \\mathrm { P } } { = } 0 )$ . Note that when $\\mathrm { D } { = } 0$ we can only compare STL with MTL, since only these two models produce a shared model between the groups, and furthermore, when $\\mathrm { D } { = } 1$ we can only compare ITL with MTL, since these produce group specific models.\n\nWe collect statistics concerning the classification average accuracy per group in percentage (ACC) on the test set, difference of equal opportunities on both the positive and negative class (denoted as $\\mathrm { D E O ^ { + } }$ and $\\mathrm { D E O ^ { - } }$ , respectively), and the difference of equalized odds (DEOd) of the selected model - see Section 2 for a definition of these quantities.",
      "referring_paragraphs": [
        "Figure 1: Our proposal in a graphical abstract: rather than using the sensitive feature as a predictor we propose to learn, with any learning algorithm, a function $g _ { \\mathrm { . } }$ , which captures the relationship between $\\pmb { x }$ and , and then use $g ( { \\pmb x } ) _ { \\pmb { \\bot \\chi } }$ , instead of , to learn group specific models via MTL.",
        "come such limitations, we propose to first use the non-sensitive features to predict the value of the sensitive one and then use the predicted sensitive feature to learn group specific models via MTL. The proposal is depicted in the graphical abstract of Figure 1. We experimentally demonstrate that the proposed approach matches the classification accuracy of the best performing model which uses the sensitive information during testing, in addition to further improving upon measures of fairness.",
        "then exploiting MTL with fairness constraints in order to increase both accuracy and fairness measures (see Figure 1). In Section 5 we test the proposal on two well known fairness related datasets (Adult and COMPAS) demonstrating the potentiality of it. We conclude the paper with a brief discussion in Section 6.",
        "Again, note that the group specific models trained by MTL may not be permitted. Likewise the shared model trained by MTL may not be permitted if we include the sensitive variable to the input. However if the sensitive variable is predicted from an external classifier and then MTL retrained with the predicted values, then this model treats different groups equally (see Figure 1).",
        "The Adult dataset contains 14 features concerning demographic characteristics of 45222 instances (32561 for training and 12661 for testing), 2 features, Gender (G) and Race (R), can be considered sensitive. The task is to predict if a person has an income per year that is more (or less) than 50 000$. Some statistics of the adult ,dataset with reference to the sensitive features are reported in Table 1.",
        "criminal defendants likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants based on a 2-years follow up study. This dataset contains variables used by the COMPAS algorithm in scoring defendants, along with their outcomes within two years of the decision, for over 10000 criminal defendants in Broward County, Florida. In the original data, 3 subsets are provided. We concentrate on the one that includes only violent recividism. Table 2, a",
        "Figure 1: Our proposal in a graphical abstract: rather than using the sensitive feature  as a predictor we propose to learn, with any learning algorithm, a function $g _ { \\mathrm { .",
        "However if the sensitive variable is predicted from an external classifier and then MTL retrained with the predicted values, then this model treats different groups equally (see Figure 1).",
        "Some statistics of the adult ,dataset with reference to the sensitive features are reported in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/4797ac40c3ffa829bf6f1dc186770c247141eb84e8546ec633374403e80fe3bb.jpg",
      "image_filename": "4797ac40c3ffa829bf6f1dc186770c247141eb84e8546ec633374403e80fe3bb.jpg",
      "caption": "We selected the best hyperparameters9 by the two steps 10-fold cross validation (CV) procedure described in [16].",
      "context_before": "In all the experiments, we compare STL, ITL, and MTL in different settings. Specifically we test each method in the following cases: when the models use the sensitive feature $\\scriptstyle ( S = 1 )$ ) or not $( \\mathsf { S } \\mathrm { = } 0 )$ , when the fairness constraint is active $\\left( \\mathrm { F } { = } 1 \\right)$ ) or not $( \\mathrm { F } { = } 0 )$ , when we consider the group specific models $\\mathrm { ( D } { = } 1 \\mathrm { ) }$ ) or the shared model between groups $\\mathrm { ( D = } 0 )$ ), and when we use the true sensitive feature $( \\mathrm { P } { = } 1 )$ or the predicted one $( { \\mathrm { P } } { = } 0 )$ . Note that when $\\mathrm { D } { = } 0$ we can only compare STL with MTL, since only these two models produce a shared model between the groups, and furthermore, when $\\mathrm { D } { = } 1$ we can only compare ITL with MTL, since these produce group specific models.\n\nWe collect statistics concerning the classification average accuracy per group in percentage (ACC) on the test set, difference of equal opportunities on both the positive and negative class (denoted as $\\mathrm { D E O ^ { + } }$ and $\\mathrm { D E O ^ { - } }$ , respectively), and the difference of equalized odds (DEOd) of the selected model - see Section 2 for a definition of these quantities.\n\nWe selected the best hyperparameters9 by the two steps 10-fold cross validation (CV) procedure described in [16]. In the first step, the value of the hyperparameters with highest accuracy is identified. In the second step, we shortlist all the hyperparameters with",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/3a63818253b54564812856c87e7102c8e06b0675cee3650b6028508a746141f1.jpg",
      "image_filename": "3a63818253b54564812856c87e7102c8e06b0675cee3650b6028508a746141f1.jpg",
      "caption": "Table 2: COMPAS dataset: statistics with reference to the sensitive features. Table 3: Adult Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
      "context_before": "",
      "context_after": "accuracy close to the best one (in our case, above $9 7 \\%$ of the best accuracy). Finally, from this list, we select the hyperparameters with the lowest fairness measure. This validation procedure, ensures that fairness cannot be achieved by a mere modification of hyperparameter selection procedure.\n\nThe results for all possible combinations described above, are reported in Table 5. In Figures 2, 3, and 4, we present a visualization of Table 5 for the Adult dataset (results are analogous for the COMPAS one). Where both the error (i.e., 1-ACC), and the EOd are normalized to be between 0 and 1, column-wise. The closer a point is to the origin, the better the result.\n\n9The ranges of hyperparameters used in the validation procedure of STL, MTL, and ITL are $\\rho { \\in } \\{ 1 0 ^ { - 6 . 0 } , 1 0 ^ { - 5 . 5 } , \\ . \\ . \\ . , 1 0 ^ { + 6 . 0 } \\} ^ { \\ }$ $\\rho \\in \\{ 1 0 ^ { - 6 . 0 }$ and ∈ { 0 2 − 15 2 − 14 2 − 1 $1 - 2 ^ { - 2 }$ ρ , 1−2−15 1 }.",
      "referring_paragraphs": [
        "criminal defendants likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants based on a 2-years follow up study. This dataset contains variables used by the COMPAS algorithm in scoring defendants, along with their outcomes within two years of the decision, for over 10000 criminal defendants in Broward County, Florida. In the original data, 3 subsets are provided. We concentrate on the one that includes only violent recividism. Table 2, a",
        "Table 2, analogously to Table 1, reports the statistics with reference to the sensitive features.",
        "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/8e673b6294e3d4a04ff2a42e66dc24cd4b3957c5de058afc83ee29902b94bef7.jpg",
      "image_filename": "8e673b6294e3d4a04ff2a42e66dc24cd4b3957c5de058afc83ee29902b94bef7.jpg",
      "caption": "Table 4: COMPAS Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
      "context_before": "accuracy close to the best one (in our case, above $9 7 \\%$ of the best accuracy). Finally, from this list, we select the hyperparameters with the lowest fairness measure. This validation procedure, ensures that fairness cannot be achieved by a mere modification of hyperparameter selection procedure.\n\nThe results for all possible combinations described above, are reported in Table 5. In Figures 2, 3, and 4, we present a visualization of Table 5 for the Adult dataset (results are analogous for the COMPAS one). Where both the error (i.e., 1-ACC), and the EOd are normalized to be between 0 and 1, column-wise. The closer a point is to the origin, the better the result.\n\n9The ranges of hyperparameters used in the validation procedure of STL, MTL, and ITL are $\\rho { \\in } \\{ 1 0 ^ { - 6 . 0 } , 1 0 ^ { - 5 . 5 } , \\ . \\ . \\ . , 1 0 ^ { + 6 . 0 } \\} ^ { \\ }$ $\\rho \\in \\{ 1 0 ^ { - 6 . 0 }$ and ∈ { 0 2 − 15 2 − 14 2 − 1 $1 - 2 ^ { - 2 }$ ρ , 1−2−15 1 }.",
      "context_after": "",
      "referring_paragraphs": [
        "Table 9 reports the comparison between the most accurate, fair and legal10 model (the shared model trained with MTL, with fairness constraint, and no sensitive feature in the predictors) and the most accurate, fair and illegal model (the group specific models trained with MTL, with fairness constraint, the sensitive feature used as predictor). From the table one can note that the illegal model remarkably improves over the legal one in terms of accuracy and in some cases it is even better than th",
        "Table 4: COMPAS Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
        "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).",
        "Table 3 and Table 4 report the confusion matrices computed on the test set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg",
      "image_filename": "1810.08683_page0_fig0.jpg",
      "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).",
      "context_before": "",
      "context_after": "Table 6 presents the performance of the shared model trained with STL or MTL, with or without the sensitive feature as a predictor, and with or without the fairness constraint. From Table 6 it is possible to see that MTL reaches higher accuracies compared to STL while the fairness measure is mostly comparable, this means that there is a relation between the tasks which can be captured with MTL. This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying . Figure 5 shows that there are λcommonalities between the groups which increase by increasing the number of groups: the optimal parameter $\\lambda$ it is smaller than one when we consider the shared model $( D { = } 0 )$ λ) and it is larger than Dzeros when we consider group specific models $\\left( D { = } 1 \\right)$ ). Moreover, as",
      "referring_paragraphs": [
        "criminal defendants likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants based on a 2-years follow up study. This dataset contains variables used by the COMPAS algorithm in scoring defendants, along with their outcomes within two years of the decision, for over 10000 criminal defendants in Broward County, Florida. In the original data, 3 subsets are provided. We concentrate on the one that includes only violent recividism. Table 2, a",
        "Table 2, analogously to Table 1, reports the statistics with reference to the sensitive features.",
        "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig1.jpg",
      "image_filename": "1810.08683_page0_fig1.jpg",
      "caption": "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
      "context_before": "Table 6 presents the performance of the shared model trained with STL or MTL, with or without the sensitive feature as a predictor, and with or without the fairness constraint. From Table 6 it is possible to see that MTL reaches higher accuracies compared to STL while the fairness measure is mostly comparable, this means that there is a relation between the tasks which can be captured with MTL. This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying . Figure 5 shows that there are λcommonalities between the groups which increase by increasing the number of groups: the optimal parameter $\\lambda$ it is smaller than one when we consider the shared model $( D { = } 0 )$ λ) and it is larger than Dzeros when we consider group specific models $\\left( D { = } 1 \\right)$ ). Moreover, as",
      "context_after": "",
      "referring_paragraphs": [
        "Table 9 reports the comparison between the most accurate, fair and legal10 model (the shared model trained with MTL, with fairness constraint, and no sensitive feature in the predictors) and the most accurate, fair and illegal model (the group specific models trained with MTL, with fairness constraint, the sensitive feature used as predictor). From the table one can note that the illegal model remarkably improves over the legal one in terms of accuracy and in some cases it is even better than th",
        "Table 3: Adult Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
        "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
        "Table 3 and Table 4 report the confusion matrices computed on the test set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig2.jpg",
      "image_filename": "1810.08683_page0_fig2.jpg",
      "caption": "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).",
      "context_before": "",
      "context_after": "expected the fairness constraint has a negative impact on the accuracy (less strong for MTL) whilst having a highly positive impact on fairness. Having the sensitive feature as a predictor increases the accuracy, but decreases the fairness measure, as expected.\n\nTable 7 reports the case when the group specific models are trained with ITL or MTL, the same setting as Table 6. MTL notably improves both accuracy and fairness. The fairness constraints do\n\n[Section: L. Oneto et al.]",
      "referring_paragraphs": [
        "Table 9 reports the comparison between the most accurate, fair and legal10 model (the shared model trained with MTL, with fairness constraint, and no sensitive feature in the predictors) and the most accurate, fair and illegal model (the group specific models trained with MTL, with fairness constraint, the sensitive feature used as predictor). From the table one can note that the illegal model remarkably improves over the legal one in terms of accuracy and in some cases it is even better than th",
        "Table 4: COMPAS Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
        "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).",
        "Table 3 and Table 4 report the confusion matrices computed on the test set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/a98c7db49e35b628e81380937aca4d37600cbed460b8a796b17e0ad3e3bed954.jpg",
      "image_filename": "a98c7db49e35b628e81380937aca4d37600cbed460b8a796b17e0ad3e3bed954.jpg",
      "caption": "L. Oneto et al.",
      "context_before": "expected the fairness constraint has a negative impact on the accuracy (less strong for MTL) whilst having a highly positive impact on fairness. Having the sensitive feature as a predictor increases the accuracy, but decreases the fairness measure, as expected.\n\nTable 7 reports the case when the group specific models are trained with ITL or MTL, the same setting as Table 6. MTL notably improves both accuracy and fairness. The fairness constraints do\n\n[Section: L. Oneto et al.]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_9",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/3a7a662f65684e31614e35670137da6dd8d9f6c7505f62286592291f13688d29.jpg",
      "image_filename": "3a7a662f65684e31614e35670137da6dd8d9f6c7505f62286592291f13688d29.jpg",
      "caption": "Table 5: Complete results set. Table 6: Results for a shared model trained with STL and MTL, with or without the sensitive feature as predictor, and with or without the fairness constraint.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "The results for all possible combinations described above, are reported in Table 5. In Figures 2, 3, and 4, we present a visualization of Table 5 for the Adult dataset (results are analogous for the COMPAS one). Where both the error (i.e., 1-ACC), and the EOd are normalized to be between 0 and 1, column-wise. The closer a point is to the origin, the better the result.",
        "Table 6 presents the performance of the shared model trained with STL or MTL, with or without the sensitive feature as a predictor, and with or without the fairness constraint. From Table 6 it is possible to see that MTL reaches higher accuracies compared to STL while the fairness measure is mostly comparable, this means that there is a relation between the tasks which can be captured with MTL. This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and f",
        "The results for all possible combinations described above, are reported in Table 5.",
        "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying .",
        "The fairness constraints do\n\n<table><tr><td rowspan=\"3\" colspan=\"2\"></td><td colspan=\"101\">Adult Dataset</td></tr><tr><td>-1</td><td>01</td><td>-1-</td><td>-1</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>P</td><td>D</td><td>F</td><td>S</td><td colspan=\"3\">ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"16\">G</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.2</td><td colspan=\"3\">0.11</td><td>83.4</td><td>0.13</td><td>80.4</td><td colspan=\"3\">0.09</td><td>84.3</td><td colspan=\"3\">0.12</td><td>80.3</td><td colspan=\"2\">0.10</td><td>83.6</td><td>0.13</td><td>76.1</td><td colspan=\"3\">0.15</td><td>78.1</td><td colspan=\"3\">0.12</td><td>76.3</td><td colspan=\"3\">0.14</td><td>78.0</td><td colspan=\"3\">0.11</td><td>76.2</td><td colspan=\"3\">0.13</td><td>77.3</td><td colspan=\"2\">0.10</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.3</td><td colspan=\"3\">0.14</td><td>83.9</td><td>0.13</td><td>83.5</td><td colspan=\"3\">0.12</td><td>84.8</td><td colspan=\"3\">0.12</td><td>83.4</td><td colspan=\"2\">0.13</td><td>84.1</td><td>0.13</td><td>79.3</td><td colspan=\"3\">0.15</td><td>79.2</td><td colspan=\"3\">0.13</td><td>79.5</td><td colspan=\"3\">0.14</td><td>79.1</td><td colspan=\"3\">0.12</td><td>79.4</td><td colspan=\"3\">0.13</td><td>78.4</td><td colspan=\"2\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.7</td><td colspan=\"3\">0.03</td><td>81.8</td><td>0.06</td><td>75.8</td><td colspan=\"3\">0.02</td><td>82.7</td><td colspan=\"3\">0.05</td><td>75.7</td><td colspan=\"2\">0.03</td><td>82.0</td><td>0.06</td><td>71.5</td><td colspan=\"3\">0.03</td><td>76.5</td><td colspan=\"3\">0.03</td><td>71.7</td><td colspan=\"3\">0.03</td><td>76.4</td><td colspan=\"3\">0.03</td><td>71.6</td><td colspan=\"3\">0.03</td><td>75.7</td><td colspan=\"2\">0.03</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>78.6</td><td colspan=\"3\">0.06</td><td>82.4</td><td>0.04</td><td>78.8</td><td colspan=\"3\">0.05</td><td>83.3</td><td colspan=\"3\">0.04</td><td>78.7</td><td colspan=\"2\">0.05</td><td>82.6</td><td>0.04</td><td>74.4</td><td colspan=\"3\">0.05</td><td>77.4</td><td colspan=\"3\">0.05</td><td>74.6</td><td colspan=\"3\">0.05</td><td>77.3</td><td colspan=\"3\">0.04</td><td>74.5</td><td colspan=\"3\">0.05</td><td>76.6</td><td colspan=\"2\">0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>0</td><td>74.5</td><td colspan=\"3\">0.18</td><td>90.0</td><td>0.14</td><td>74.7</td><td colspan=\"3\">0.15</td><td>91.0</td><td colspan=\"3\">0.13</td><td>74.6</td><td colspan=\"2\">0.17</td><td>90.2</td><td>0.14</td><td>70.7</td><td colspan=\"3\">0.19</td><td>84.5</td><td colspan=\"3\">0.15</td><td>70.9</td><td colspan=\"3\">0.17</td><td>84.4</td><td colspan=\"3\">0.14</td><td>70.8</td><td colspan=\"3\">0.16</td><td>83.6</td><td colspan=\"2\">0.13</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>74.6</td><td colspan=\"3\">0.17</td><td>89.7</td><td>0.14</td><td>74.7</td><td colspan=\"3\">0.15</td><td>90.7</td><td colspan=\"3\">0.13</td><td>74.7</td><td colspan=\"2\">0.16</td><td>90.0</td><td>0.14</td><td>70.9</td><td colspan=\"3\">0.19</td><td>84.5</td><td colspan=\"3\">0.14</td><td>71.1</td><td colspan=\"3\">0.18</td><td>84.4</td><td colspan=\"3\">0.13</td><td>71.0</td><td colspan=\"3\">0.16</td><td>83.6</td><td colspan=\"2\">0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>69.7</td><td colspan=\"3\">0.08</td><td>88.3</td><td>0.04</td><td>69.9</td><td colspan=\"3\">0.07</td><td>89.2</td><td colspan=\"3\">0.04</td><td>69.8</td><td colspan=\"2\">0.08</td><td>88.5</td><td>0.04</td><td>66.1</td><td colspan=\"3\">0.08</td><td>83.0</td><td colspan=\"3\">0.04</td><td>66.3</td><td colspan=\"3\">0.08</td><td>82.8</td><td colspan=\"3\">0.04</td><td>66.2</td><td colspan=\"3\">0.07</td><td>82.1</td><td colspan=\"2\">0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>69.7</td><td colspan=\"3\">0.08</td><td>88.1</td><td>0.03</td><td>69.9</td><td colspan=\"3\">0.07</td><td>89.1</td><td colspan=\"3\">0.03</td><td>69.8</td><td colspan=\"2\">0.08</td><td>88.3</td><td>0.03</td><td>66.1</td><td colspan=\"3\">0.09</td><td>82.9</td><td colspan=\"3\">0.07</td><td>66.3</td><td colspan=\"3\">0.08</td><td>82.8</td><td colspan=\"3\">0.06</td><td>66.2</td><td colspan=\"3\">0.08</td><td>82.1</td><td colspan=\"2\">0.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>78.4</td><td colspan=\"3\">0.09</td><td>82.3</td><td>0.09</td><td>78.6</td><td colspan=\"3\">0.07</td><td>83.2</td><td colspan=\"3\">0.09</td><td>78.5</td><td colspan=\"2\">0.08</td><td>82.5</td><td>0.09</td><td>74.6</td><td colspan=\"3\">0.12</td><td>77.3</td><td colspan=\"3\">0.10</td><td>74.8</td><td colspan=\"3\">0.11</td><td>77.2</td><td colspan=\"3\">0.09</td><td>74.7</td><td colspan=\"3\">0.10</td><td>76.5</td><td colspan=\"2\">0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>81.7</td><td colspan=\"3\">0.13</td><td>83.1</td><td>0.08</td><td>81.9</td><td colspan=\"3\">0.11</td><td>84.0</td><td colspan=\"3\">0.07</td><td>81.8</td><td colspan=\"2\">0.12</td><td>83.3</td><td>0.08</td><td>77.6</td><td colspan=\"3\">0.13</td><td>78.1</td><td colspan=\"3\">0.09</td><td>77.8</td><td colspan=\"3\">0.12</td><td>78.0</td><td colspan=\"3\">0.09</td><td>77.7</td><td colspan=\"3\">0.11</td><td>77.3</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>73.7</td><td colspan=\"3\">0.02</td><td>80.7</td><td>0.01</td><td>73.9</td><td colspan=\"3\">0.02</td><td>81.6</td><td colspan=\"3\">0.01</td><td>73.8</td><td colspan=\"2\">0.02</td><td>80.9</td><td>0.01</td><td>70.1</td><td colspan=\"3\">0.03</td><td>75.9</td><td colspan=\"3\">0.01</td><td>70.3</td><td colspan=\"3\">0.03</td><td>75.8</td><td colspan=\"3\">0.01</td><td>70.2</td><td colspan=\"3\">0.03</td><td>75.1</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>76.8</td><td colspan=\"3\">0.03</td><td>81.5</td><td>0.01</td><td>77.0</td><td colspan=\"3\">0.03</td><td>82.4</td><td colspan=\"3\">0.01</td><td>76.9</td><td colspan=\"2\">0.03</td><td>81.7</td><td>0.01</td><td>73.1</td><td colspan=\"3\">0.05</td><td>76.7</td><td colspan=\"3\">0.01</td><td>73.3</td><td colspan=\"3\">0.05</td><td>76.6</td><td colspan=\"3\">0.01</td><td>73.2</td><td colspan=\"3\">0.04</td><td>75.9</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>73.0</td><td colspan=\"3\">0.14</td><td>89.1</td><td>0.09</td><td>73.2</td><td colspan=\"3\">0.12</td><td>90.1</td><td colspan=\"3\">0.08</td><td>73.1</td><td colspan=\"2\">0.13</td><td>89.3</td><td>0.09</td><td>69.3</td><td colspan=\"3\">0.17</td><td>83.7</td><td colspan=\"3\">0.09</td><td>69.5</td><td colspan=\"3\">0.15</td><td>83.6</td><td colspan=\"3\">0.08</td><td>69.4</td><td colspan=\"3\">0.14</td><td>82.8</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>0</td><td>72.8</td><td colspan=\"3\">0.15</td><td>88.9</td><td>0.10</td><td>73.0</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.09</td><td>72.9</td><td colspan=\"2\">0.14</td><td>89.1</td><td>0.10</td><td>69.3</td><td colspan=\"3\">0.15</td><td>83.7</td><td colspan=\"3\">0.10</td><td>69.5</td><td colspan=\"3\">0.14</td><td>83.6</td><td colspan=\"3\">0.09</td><td>69.4</td><td colspan=\"3\">0.13</td><td>82.9</td><td colspan=\"2\">0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>68.0</td><td colspan=\"3\">0.06</td><td>87.4</td><td>0.01</td><td>68.2</td><td colspan=\"3\">0.05</td><td>88.3</td><td colspan=\"3\">0.01</td><td>68.1</td><td colspan=\"2\">0.05</td><td>87.6</td><td>0.01</td><td>64.7</td><td colspan=\"3\">0.06</td><td>82.3</td><td colspan=\"3\">0.01</td><td>64.9</td><td colspan=\"3\">0.05</td><td>82.1</td><td colspan=\"3\">0.01</td><td>64.8</td><td colspan=\"3\">0.05</td><td>81.4</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>68.0</td><td colspan=\"3\">0.06</td><td>87.4</td><td>0.01</td><td>68.1</td><td colspan=\"3\">0.05</td><td>88.3</td><td colspan=\"3\">0.01</td><td>68.1</td><td colspan=\"2\">0.06</td><td>87.6</td><td>0.01</td><td>64.6</td><td colspan=\"3\">0.06</td><td>82.1</td><td colspan=\"3\">0.01</td><td>64.8</td><td colspan=\"3\">0.06</td><td>82.0</td><td colspan=\"3\">0.01</td><td>64.7</td><td colspan=\"3\">0.05</td><td>81.3</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"14\">R</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.3</td><td colspan=\"3\">0.08</td><td>84.2</td><td>0.07</td><td>80.5</td><td colspan=\"3\">0.07</td><td>85.1</td><td colspan=\"3\">0.06</td><td>80.4</td><td colspan=\"2\">0.08</td><td>84.4</td><td>0.07</td><td>80.2</td><td colspan=\"3\">0.09</td><td>84.2</td><td colspan=\"3\">0.08</td><td>80.4</td><td colspan=\"3\">0.08</td><td>85.1</td><td colspan=\"3\">0.07</td><td>80.3</td><td colspan=\"3\">0.09</td><td>84.4</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.2</td><td colspan=\"3\">0.09</td><td>85.3</td><td>0.09</td><td>83.4</td><td colspan=\"3\">0.08</td><td>86.2</td><td colspan=\"3\">0.08</td><td>83.3</td><td colspan=\"2\">0.09</td><td>85.5</td><td>0.09</td><td>83.2</td><td colspan=\"3\">0.10</td><td>84.9</td><td colspan=\"3\">0.08</td><td>83.4</td><td colspan=\"3\">0.09</td><td>85.8</td><td colspan=\"3\">0.07</td><td>83.3</td><td colspan=\"3\">0.10</td><td>85.1</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.3</td><td colspan=\"3\">0.02</td><td>82.6</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.02</td><td>83.5</td><td colspan=\"3\">0.01</td><td>75.4</td><td colspan=\"2\">0.02</td><td>82.8</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.04</td><td>82.4</td><td colspan=\"3\">0.03</td><td>75.7</td><td colspan=\"3\">0.04</td><td>83.3</td><td colspan=\"3\">0.03</td><td>75.6</td><td colspan=\"3\">0.04</td><td>82.6</td><td>0.03</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>78.4</td><td colspan=\"3\">0.03</td><td>83.4</td><td>0.03</td><td>78.6</td><td colspan=\"3\">0.03</td><td>84.3</td><td colspan=\"3\">0.02</td><td>78.5</td><td colspan=\"2\">0.03</td><td>83.6</td><td>0.03</td><td>78.5</td><td colspan=\"3\">0.05</td><td>83.5</td><td colspan=\"3\">0.02</td><td>78.7</td><td colspan=\"3\">0.04</td><td>84.4</td><td colspan=\"3\">0.02</td><td>78.6</td><td colspan=\"3\">0.05</td><td>83.7</td><td>0.02</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>0</td><td>67.4</td><td colspan=\"3\">0.13</td><td>91.8</td><td>0.10</td><td>67.6</td><td colspan=\"3\">0.11</td><td>92.8</td><td colspan=\"3\">0.08</td><td>67.5</td><td colspan=\"2\">0.13</td><td>92.0</td><td>0.10</td><td>67.3</td><td colspan=\"3\">0.12</td><td>91.7</td><td colspan=\"3\">0.08</td><td>67.5</td><td colspan=\"3\">0.11</td><td>92.7</td><td colspan=\"3\">0.07</td><td>67.4</td><td colspan=\"3\">0.12</td><td>92.0</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>67.2</td><td colspan=\"3\">0.13</td><td>91.8</td><td>0.08</td><td>67.4</td><td colspan=\"3\">0.12</td><td>92.8</td><td colspan=\"3\">0.07</td><td>67.3</td><td colspan=\"2\">0.13</td><td>92.1</td><td>0.08</td><td>67.4</td><td colspan=\"3\">0.13</td><td>91.8</td><td colspan=\"3\">0.09</td><td>67.5</td><td colspan=\"3\">0.11</td><td>92.8</td><td colspan=\"3\">0.08</td><td>67.4</td><td colspan=\"3\">0.13</td><td>92.0</td><td>0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>62.5</td><td colspan=\"3\">0.05</td><td>90.0</td><td>0.03</td><td>62.7</td><td colspan=\"3\">0.05</td><td>90.9</td><td colspan=\"3\">0.03</td><td>62.6</td><td colspan=\"2\">0.05</td><td>90.2</td><td>0.03</td><td>62.4</td><td colspan=\"3\">0.07</td><td>90.1</td><td colspan=\"3\">0.02</td><td>62.6</td><td colspan=\"3\">0.06</td><td>91.0</td><td colspan=\"3\">0.02</td><td>62.5</td><td>0.07</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>62.6</td><td colspan=\"3\">0.06</td><td>90.4</td><td>0.03</td><td>62.7</td><td colspan=\"3\">0.05</td><td>91.3</td><td colspan=\"3\">0.03</td><td>62.6</td><td colspan=\"2\">0.06</td><td>90.6</td><td>0.03</td><td>62.4</td><td colspan=\"3\">0.07</td><td>90.0</td><td colspan=\"3\">0.03</td><td>62.5</td><td colspan=\"3\">0.07</td><td>91.0</td><td colspan=\"3\">0.03</td><td>62.4</td><td>0.07</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>78.5</td><td colspan=\"3\">0.07</td><td>83.2</td><td>0.04</td><td>78.7</td><td colspan=\"3\">0.06</td><td>84.1</td><td colspan=\"3\">0.04</td><td>78.6</td><td colspan=\"2\">0.07</td><td>83.4</td><td>0.04</td><td>78.4</td><td colspan=\"3\">0.08</td><td>83.3</td><td colspan=\"3\">0.06</td><td>78.6</td><td colspan=\"3\">0.07</td><td>84.2</td><td colspan=\"3\">0.05</td><td>78.5</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>67.1</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td>77.3</td><td colspan=\"3\">0.01</td><td>83.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"2\">0.01</td><td>82.7</td><td>0.01</td><td>77.0</td><td colspan=\"3\">0.02</td><td>82.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"3\">0.01</td><td>83.2</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>77.1</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td>77.3</td><td colspan=\"3\">0.01</td><td>83.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"2\">0.01</td><td>82.7</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.12</td><td>90.8</td><td colspan=\"3\">0.05</td><td>65.7</td><td colspan=\"3\">0.11</td><td>91.8</td><td colspan=\"3\">0.05</td><td>65.6</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>65.8</td><td colspan=\"3\">0.12</td><td>90.8</td><td>0.06</td><td>66.0</td><td colspan=\"3\">0.11</td><td>91.8</td><td colspan=\"3\">0.05</td><td>65.9</td><td colspan=\"2\">0.12</td><td>91.0</td><td>0.06</td><td>65.7</td><td colspan=\"3\">0.12</td><td>90.8</td><td colspan=\"3\">0.07</td><td>65.8</td><td colspan=\"3\">0.11</td><td>91.7</td><td colspan=\"3\">0.07</td><td>65.7</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>61.2</td><td colspan=\"3\">0.06</td><td>89.3</td><td>0.01</td><td>61.3</td><td colspan=\"3\">0.05</td><td>90.3</td><td colspan=\"3\">0.01</td><td>61.2</td><td colspan=\"2\">0.06</td><td>89.5</td><td>0.01</td><td>60.8</td><td colspan=\"3\">0.05</td><td>89.2</td><td colspan=\"3\">0.01</td><td>61.0</td><td colspan=\"3\">0.05</td><td>60.9</td><td colspan=\"3\">0.01</td><td>60.9</td><td>0.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>60.8</td><td colspan=\"3\">0.06</td><td>89.2</td><td>0.01</td><td>61.0</td><td colspan=\"3\">0.05</td><td>90.2</td><td colspan=\"3\">0.01</td><td>60.9</td><td colspan=\"2\">0.06</td><td>89.4</td><td>0.01</td><td>60.9</td><td colspan=\"3\">0.04</td><td>89.0</td><td colspan=\"3\">0.01</td><td>61.1</td><td colspan=\"3\">0.04</td><td>89.9</td><td colspan=\"3\">0.01</td><td>61.0</td><td>0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"14\">G-R</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.2</td><td colspan=\"3\">0.16</td><td>84.6</td><td>0.14</td><td>80.4</td><td colspan=\"3\">0.14</td><td>85.3</td><td colspan=\"3\">0.14</td><td>80.3</td><td colspan=\"2\">0.15</td><td>84.9</td><td>0.14</td><td>80.2</td><td colspan=\"3\">0.16</td><td>84.8</td><td colspan=\"3\">0.14</td><td>80.4</td><td colspan=\"3\">0.14</td><td>85.5</td><td colspan=\"3\">0.14</td><td>80.3</td><td>0.15</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.1</td><td colspan=\"3\">0.18</td><td>85.7</td><td>0.16</td><td>83.4</td><td colspan=\"3\">0.16</td><td>86.4</td><td colspan=\"3\">0.16</td><td>83.3</td><td colspan=\"2\">0.17</td><td>86.0</td><td>0.16</td><td>83.3</td><td colspan=\"3\">0.18</td><td>85.5</td><td colspan=\"3\">0.16</td><td>83.5</td><td colspan=\"3\">0.15</td><td>86.2</td><td colspan=\"3\">0.16</td><td>83.4</td><td>0.16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.2</td><td colspan=\"3\">0.05</td><td>83.2</td><td>0.04</td><td>75.3</td><td colspan=\"3\">0.04</td><td>83.9</td><td colspan=\"3\">0.04</td><td>75.3</td><td colspan=\"2\">0.05</td><td>83.5</td><td>0.04</td><td>75.3</td><td colspan=\"3\">0.05</td><td>83.1</td><td colspan=\"3\">0.05</td><td>75.5</td><td colspan=\"3\">0.04</td><td>83.8</td><td colspan=\"3\">0.05</td><td>75.4</td><td>0.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>78.5</td><td colspan=\"3\">0.05</td><td>83.9</td><td>0.05</td><td>78.7</td><td colspan=\"3\">0.04</td><td>84.6</td><td colspan=\"3\">0.05</td><td>78.6</td><td colspan=\"2\">0.05</td><td>84.2</td><td>0.05</td><td>78.6</td><td colspan=\"3\">0.06</td><td>84.1</td><td colspan=\"3\">0.04</td><td>78.8</td><td colspan=\"3\">0.05</td><td>84.7</td><td colspan=\"3\">0.04</td><td>78.7</td><td>0.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>64.0</td><td colspan=\"3\">0.23</td><td>91.5</td><td>0.15</td><td>64.2</td><td colspan=\"3\">0.20</td><td>92.2</td><td colspan=\"3\">0.15</td><td>64.1</td><td colspan=\"2\">0.22</td><td>91.8</td><td>0.15</td><td>64.2</td><td colspan=\"3\">0.24</td><td>91.4</td><td colspan=\"3\">0.16</td><td>64.3</td><td colspan=\"3\">0.21</td><td>92.2</td><td colspan=\"3\">0.16</td><td>64.3</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>63.9</td><td colspan=\"3\">0.24</td><td>91.7</td><td>0.16</td><td>64.0</td><td colspan=\"3\">0.21</td><td>92.4</td><td colspan=\"3\">0.16</td><td>64.0</td><td colspan=\"2\">0.23</td><td>92.0</td><td>0.16</td><td>64.1</td><td colspan=\"3\">0.23</td><td>91.5</td><td colspan=\"3\">0.15</td><td>64.3</td><td colspan=\"3\">0.20</td><td>92.2</td><td colspan=\"3\">0.15</td><td>64.2</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>59.3</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.05</td><td>59.4</td><td colspan=\"3\">0.12</td><td>90.6</td><td colspan=\"3\">0.12</td><td>59.3</td><td colspan=\"2\">0.13</td><td>90.1</td><td>0.05</td><td>59.2</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.05</td><td>59.5</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.05</td><td>59.5</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>59.2</td><td colspan=\"3\">0.13</td><td>90.0</td><td>0.15</td><td>59.4</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.15</td><td>59.3</td><td colspan=\"2\">0.12</td><td>90.3</td><td>0.05</td><td>59.2</td><td colspan=\"3\">0.13</td><td>83.8</td><td colspan=\"3\">0.09</td><td>78.6</td><td colspan=\"3\">0.12</td><td>84.5</td><td>0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>58.5</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.11</td><td>58.7</td><td colspan=\"3\">0.11</td><td>84.6</td><td colspan=\"3\">0.12</td><td>78.6</td><td colspan=\"2\">0.12</td><td>84.2</td><td>0.12</td><td>78.4</td><td colspan=\"3\">0.13</td><td>82.5</td><td colspan=\"3\">0.11</td><td>78.9</td><td colspan=\"3\">0.12</td><td>83.4</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.9</td><td colspan=\"3\">0.14</td><td>82.3</td><td>0.01</td><td>73.9</td><td colspan=\"3\">0.01</td><td>83.0</td><td colspan=\"3\">0.11</td><td>73.8</td><td colspan=\"2\">0.11</td><td>82.6</td><td>0.01</td><td>73.6</td><td colspan=\"3\">0.02</td><td>82.5</td><td colspan=\"3\">0.01</td><td>73.8</td><td colspan=\"3\">0.02</td><td>83.1</td><td>0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>56.8</td><td colspan=\"3\">0.04</td><td>83.1</td><td>0.01</td><td>76.9</td><td colspan=\"3\">0.04</td><td>83.8</td><td colspan=\"3\">0.01</td><td>76.8</td><td colspan=\"2\">0.04</td><td>83.4</td><td>0.01</td><td>76.8</td><td colspan=\"3\">0.04</td><td>83.2</td><td colspan=\"3\">0.01</td><td>77.0</td><td colspan=\"3\">0.04</td><td>76.8</td><td>0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>62.5</td><td colspan=\"3\">0.13</td><td>89.8</td><td>0.11</td><td>59.4</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.12</td><td>59.3</td><td colspan=\"2\">0.12</td><td>90.3</td><td>0.12</td><td>59.2</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.11</td><td>59.5</td><td colspan=\"3\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.9</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.11</td><td>57.9</td><td colspan=\"3\">0.18</td><td>89.9</td><td colspan=\"3\">0.12</td><td>57.8</td><td colspan=\"2\">0.19</td><td>89.9</td><td>0.12</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td colspan=\"3\">0.11</td><td>57.8</td><td colspan=\"3\">0.19</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td>0.12</td><td>57.9</td><td colspan=\"3\">0.19</td><td>89.8</td><td colspan=\"3\">0.11</td><td>57.8</td><td colspan=\"2\">0.19</td><td>89.9</td><td>0.12</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td colspan=\"3\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 5: Complete results set.",
        "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "image_filename": "1810.08683_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig4.jpg",
      "image_filename": "1810.08683_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_12",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig5.jpg",
      "image_filename": "1810.08683_page0_fig5.jpg",
      "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "context_before": "",
      "context_after": "not affect the accuracy too much, while giving remarkable improvements in fairness. ITL and MTL are not affected by not including or including the sensitive feature predictor, as expected from the theory given that the models already have already different biases. Table 8 reports a comparison between STL, ITL, and MTL on the Adult dataset, showing the accuracy on each group for the different models for the case that $P { = } 0$ , $F { = } 0$ , and $S { = } 0$ . These results P F Sclearly demonstrate that STL and ITL tend to generalize poorly on smaller groups, whereas MTL generalizes better. Results on COMPAS datasets are analogous.\n\nTable 9 reports the comparison between the most accurate, fair and legal10 model (the shared model trained with MTL, with fairness constraint, and no sensitive feature in the predictors) and the most accurate, fair and illegal model (the group specific models trained with MTL, with fairness constraint, the sensitive feature used as predictor). From the table one can note that the illegal model remarkably improves over the legal one in terms of accuracy and in some cases it is even better than the legal one in terms of fairness. Based on the result of Table 9 we would like to be able to use the ’illegal’ model’. In order to do so make use of the trick described in the previous sections, namely we use the predicted sensitive feature based on the non-sensitive features, instead of the true one. For this purpose we used a Random Forests model [11] where we weighted the errors differently based on the group membership. Table 3 and Table 4 report the confusion matrices computed on the test set.\n\nFinally, in Table 10 we report a comparison between the best illegal model and the same model, but for which uses we used the predicted sensitive feature, instead of the true one, both in training and in testing. Notably, Table 10 shows that using the predicted",
      "referring_paragraphs": [
        "The results for all possible combinations described above, are reported in Table 5. In Figures 2, 3, and 4, we present a visualization of Table 5 for the Adult dataset (results are analogous for the COMPAS one). Where both the error (i.e., 1-ACC), and the EOd are normalized to be between 0 and 1, column-wise. The closer a point is to the origin, the better the result.",
        "Table 6 presents the performance of the shared model trained with STL or MTL, with or without the sensitive feature as a predictor, and with or without the fairness constraint. From Table 6 it is possible to see that MTL reaches higher accuracies compared to STL while the fairness measure is mostly comparable, this means that there is a relation between the tasks which can be captured with MTL. This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and f",
        "The results for all possible combinations described above, are reported in Table 5.",
        "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying .",
        "The fairness constraints do\n\n<table><tr><td rowspan=\"3\" colspan=\"2\"></td><td colspan=\"101\">Adult Dataset</td></tr><tr><td>-1</td><td>01</td><td>-1-</td><td>-1</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>P</td><td>D</td><td>F</td><td>S</td><td colspan=\"3\">ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"16\">G</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.2</td><td colspan=\"3\">0.11</td><td>83.4</td><td>0.13</td><td>80.4</td><td colspan=\"3\">0.09</td><td>84.3</td><td colspan=\"3\">0.12</td><td>80.3</td><td colspan=\"2\">0.10</td><td>83.6</td><td>0.13</td><td>76.1</td><td colspan=\"3\">0.15</td><td>78.1</td><td colspan=\"3\">0.12</td><td>76.3</td><td colspan=\"3\">0.14</td><td>78.0</td><td colspan=\"3\">0.11</td><td>76.2</td><td colspan=\"3\">0.13</td><td>77.3</td><td colspan=\"2\">0.10</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.3</td><td colspan=\"3\">0.14</td><td>83.9</td><td>0.13</td><td>83.5</td><td colspan=\"3\">0.12</td><td>84.8</td><td colspan=\"3\">0.12</td><td>83.4</td><td colspan=\"2\">0.13</td><td>84.1</td><td>0.13</td><td>79.3</td><td colspan=\"3\">0.15</td><td>79.2</td><td colspan=\"3\">0.13</td><td>79.5</td><td colspan=\"3\">0.14</td><td>79.1</td><td colspan=\"3\">0.12</td><td>79.4</td><td colspan=\"3\">0.13</td><td>78.4</td><td colspan=\"2\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.7</td><td colspan=\"3\">0.03</td><td>81.8</td><td>0.06</td><td>75.8</td><td colspan=\"3\">0.02</td><td>82.7</td><td colspan=\"3\">0.05</td><td>75.7</td><td colspan=\"2\">0.03</td><td>82.0</td><td>0.06</td><td>71.5</td><td colspan=\"3\">0.03</td><td>76.5</td><td colspan=\"3\">0.03</td><td>71.7</td><td colspan=\"3\">0.03</td><td>76.4</td><td colspan=\"3\">0.03</td><td>71.6</td><td colspan=\"3\">0.03</td><td>75.7</td><td colspan=\"2\">0.03</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>78.6</td><td colspan=\"3\">0.06</td><td>82.4</td><td>0.04</td><td>78.8</td><td colspan=\"3\">0.05</td><td>83.3</td><td colspan=\"3\">0.04</td><td>78.7</td><td colspan=\"2\">0.05</td><td>82.6</td><td>0.04</td><td>74.4</td><td colspan=\"3\">0.05</td><td>77.4</td><td colspan=\"3\">0.05</td><td>74.6</td><td colspan=\"3\">0.05</td><td>77.3</td><td colspan=\"3\">0.04</td><td>74.5</td><td colspan=\"3\">0.05</td><td>76.6</td><td colspan=\"2\">0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>0</td><td>74.5</td><td colspan=\"3\">0.18</td><td>90.0</td><td>0.14</td><td>74.7</td><td colspan=\"3\">0.15</td><td>91.0</td><td colspan=\"3\">0.13</td><td>74.6</td><td colspan=\"2\">0.17</td><td>90.2</td><td>0.14</td><td>70.7</td><td colspan=\"3\">0.19</td><td>84.5</td><td colspan=\"3\">0.15</td><td>70.9</td><td colspan=\"3\">0.17</td><td>84.4</td><td colspan=\"3\">0.14</td><td>70.8</td><td colspan=\"3\">0.16</td><td>83.6</td><td colspan=\"2\">0.13</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>74.6</td><td colspan=\"3\">0.17</td><td>89.7</td><td>0.14</td><td>74.7</td><td colspan=\"3\">0.15</td><td>90.7</td><td colspan=\"3\">0.13</td><td>74.7</td><td colspan=\"2\">0.16</td><td>90.0</td><td>0.14</td><td>70.9</td><td colspan=\"3\">0.19</td><td>84.5</td><td colspan=\"3\">0.14</td><td>71.1</td><td colspan=\"3\">0.18</td><td>84.4</td><td colspan=\"3\">0.13</td><td>71.0</td><td colspan=\"3\">0.16</td><td>83.6</td><td colspan=\"2\">0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>69.7</td><td colspan=\"3\">0.08</td><td>88.3</td><td>0.04</td><td>69.9</td><td colspan=\"3\">0.07</td><td>89.2</td><td colspan=\"3\">0.04</td><td>69.8</td><td colspan=\"2\">0.08</td><td>88.5</td><td>0.04</td><td>66.1</td><td colspan=\"3\">0.08</td><td>83.0</td><td colspan=\"3\">0.04</td><td>66.3</td><td colspan=\"3\">0.08</td><td>82.8</td><td colspan=\"3\">0.04</td><td>66.2</td><td colspan=\"3\">0.07</td><td>82.1</td><td colspan=\"2\">0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>69.7</td><td colspan=\"3\">0.08</td><td>88.1</td><td>0.03</td><td>69.9</td><td colspan=\"3\">0.07</td><td>89.1</td><td colspan=\"3\">0.03</td><td>69.8</td><td colspan=\"2\">0.08</td><td>88.3</td><td>0.03</td><td>66.1</td><td colspan=\"3\">0.09</td><td>82.9</td><td colspan=\"3\">0.07</td><td>66.3</td><td colspan=\"3\">0.08</td><td>82.8</td><td colspan=\"3\">0.06</td><td>66.2</td><td colspan=\"3\">0.08</td><td>82.1</td><td colspan=\"2\">0.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>78.4</td><td colspan=\"3\">0.09</td><td>82.3</td><td>0.09</td><td>78.6</td><td colspan=\"3\">0.07</td><td>83.2</td><td colspan=\"3\">0.09</td><td>78.5</td><td colspan=\"2\">0.08</td><td>82.5</td><td>0.09</td><td>74.6</td><td colspan=\"3\">0.12</td><td>77.3</td><td colspan=\"3\">0.10</td><td>74.8</td><td colspan=\"3\">0.11</td><td>77.2</td><td colspan=\"3\">0.09</td><td>74.7</td><td colspan=\"3\">0.10</td><td>76.5</td><td colspan=\"2\">0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>81.7</td><td colspan=\"3\">0.13</td><td>83.1</td><td>0.08</td><td>81.9</td><td colspan=\"3\">0.11</td><td>84.0</td><td colspan=\"3\">0.07</td><td>81.8</td><td colspan=\"2\">0.12</td><td>83.3</td><td>0.08</td><td>77.6</td><td colspan=\"3\">0.13</td><td>78.1</td><td colspan=\"3\">0.09</td><td>77.8</td><td colspan=\"3\">0.12</td><td>78.0</td><td colspan=\"3\">0.09</td><td>77.7</td><td colspan=\"3\">0.11</td><td>77.3</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>73.7</td><td colspan=\"3\">0.02</td><td>80.7</td><td>0.01</td><td>73.9</td><td colspan=\"3\">0.02</td><td>81.6</td><td colspan=\"3\">0.01</td><td>73.8</td><td colspan=\"2\">0.02</td><td>80.9</td><td>0.01</td><td>70.1</td><td colspan=\"3\">0.03</td><td>75.9</td><td colspan=\"3\">0.01</td><td>70.3</td><td colspan=\"3\">0.03</td><td>75.8</td><td colspan=\"3\">0.01</td><td>70.2</td><td colspan=\"3\">0.03</td><td>75.1</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>76.8</td><td colspan=\"3\">0.03</td><td>81.5</td><td>0.01</td><td>77.0</td><td colspan=\"3\">0.03</td><td>82.4</td><td colspan=\"3\">0.01</td><td>76.9</td><td colspan=\"2\">0.03</td><td>81.7</td><td>0.01</td><td>73.1</td><td colspan=\"3\">0.05</td><td>76.7</td><td colspan=\"3\">0.01</td><td>73.3</td><td colspan=\"3\">0.05</td><td>76.6</td><td colspan=\"3\">0.01</td><td>73.2</td><td colspan=\"3\">0.04</td><td>75.9</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>73.0</td><td colspan=\"3\">0.14</td><td>89.1</td><td>0.09</td><td>73.2</td><td colspan=\"3\">0.12</td><td>90.1</td><td colspan=\"3\">0.08</td><td>73.1</td><td colspan=\"2\">0.13</td><td>89.3</td><td>0.09</td><td>69.3</td><td colspan=\"3\">0.17</td><td>83.7</td><td colspan=\"3\">0.09</td><td>69.5</td><td colspan=\"3\">0.15</td><td>83.6</td><td colspan=\"3\">0.08</td><td>69.4</td><td colspan=\"3\">0.14</td><td>82.8</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>0</td><td>72.8</td><td colspan=\"3\">0.15</td><td>88.9</td><td>0.10</td><td>73.0</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.09</td><td>72.9</td><td colspan=\"2\">0.14</td><td>89.1</td><td>0.10</td><td>69.3</td><td colspan=\"3\">0.15</td><td>83.7</td><td colspan=\"3\">0.10</td><td>69.5</td><td colspan=\"3\">0.14</td><td>83.6</td><td colspan=\"3\">0.09</td><td>69.4</td><td colspan=\"3\">0.13</td><td>82.9</td><td colspan=\"2\">0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>68.0</td><td colspan=\"3\">0.06</td><td>87.4</td><td>0.01</td><td>68.2</td><td colspan=\"3\">0.05</td><td>88.3</td><td colspan=\"3\">0.01</td><td>68.1</td><td colspan=\"2\">0.05</td><td>87.6</td><td>0.01</td><td>64.7</td><td colspan=\"3\">0.06</td><td>82.3</td><td colspan=\"3\">0.01</td><td>64.9</td><td colspan=\"3\">0.05</td><td>82.1</td><td colspan=\"3\">0.01</td><td>64.8</td><td colspan=\"3\">0.05</td><td>81.4</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>68.0</td><td colspan=\"3\">0.06</td><td>87.4</td><td>0.01</td><td>68.1</td><td colspan=\"3\">0.05</td><td>88.3</td><td colspan=\"3\">0.01</td><td>68.1</td><td colspan=\"2\">0.06</td><td>87.6</td><td>0.01</td><td>64.6</td><td colspan=\"3\">0.06</td><td>82.1</td><td colspan=\"3\">0.01</td><td>64.8</td><td colspan=\"3\">0.06</td><td>82.0</td><td colspan=\"3\">0.01</td><td>64.7</td><td colspan=\"3\">0.05</td><td>81.3</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"14\">R</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.3</td><td colspan=\"3\">0.08</td><td>84.2</td><td>0.07</td><td>80.5</td><td colspan=\"3\">0.07</td><td>85.1</td><td colspan=\"3\">0.06</td><td>80.4</td><td colspan=\"2\">0.08</td><td>84.4</td><td>0.07</td><td>80.2</td><td colspan=\"3\">0.09</td><td>84.2</td><td colspan=\"3\">0.08</td><td>80.4</td><td colspan=\"3\">0.08</td><td>85.1</td><td colspan=\"3\">0.07</td><td>80.3</td><td colspan=\"3\">0.09</td><td>84.4</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.2</td><td colspan=\"3\">0.09</td><td>85.3</td><td>0.09</td><td>83.4</td><td colspan=\"3\">0.08</td><td>86.2</td><td colspan=\"3\">0.08</td><td>83.3</td><td colspan=\"2\">0.09</td><td>85.5</td><td>0.09</td><td>83.2</td><td colspan=\"3\">0.10</td><td>84.9</td><td colspan=\"3\">0.08</td><td>83.4</td><td colspan=\"3\">0.09</td><td>85.8</td><td colspan=\"3\">0.07</td><td>83.3</td><td colspan=\"3\">0.10</td><td>85.1</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.3</td><td colspan=\"3\">0.02</td><td>82.6</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.02</td><td>83.5</td><td colspan=\"3\">0.01</td><td>75.4</td><td colspan=\"2\">0.02</td><td>82.8</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.04</td><td>82.4</td><td colspan=\"3\">0.03</td><td>75.7</td><td colspan=\"3\">0.04</td><td>83.3</td><td colspan=\"3\">0.03</td><td>75.6</td><td colspan=\"3\">0.04</td><td>82.6</td><td>0.03</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>78.4</td><td colspan=\"3\">0.03</td><td>83.4</td><td>0.03</td><td>78.6</td><td colspan=\"3\">0.03</td><td>84.3</td><td colspan=\"3\">0.02</td><td>78.5</td><td colspan=\"2\">0.03</td><td>83.6</td><td>0.03</td><td>78.5</td><td colspan=\"3\">0.05</td><td>83.5</td><td colspan=\"3\">0.02</td><td>78.7</td><td colspan=\"3\">0.04</td><td>84.4</td><td colspan=\"3\">0.02</td><td>78.6</td><td colspan=\"3\">0.05</td><td>83.7</td><td>0.02</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>0</td><td>67.4</td><td colspan=\"3\">0.13</td><td>91.8</td><td>0.10</td><td>67.6</td><td colspan=\"3\">0.11</td><td>92.8</td><td colspan=\"3\">0.08</td><td>67.5</td><td colspan=\"2\">0.13</td><td>92.0</td><td>0.10</td><td>67.3</td><td colspan=\"3\">0.12</td><td>91.7</td><td colspan=\"3\">0.08</td><td>67.5</td><td colspan=\"3\">0.11</td><td>92.7</td><td colspan=\"3\">0.07</td><td>67.4</td><td colspan=\"3\">0.12</td><td>92.0</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>67.2</td><td colspan=\"3\">0.13</td><td>91.8</td><td>0.08</td><td>67.4</td><td colspan=\"3\">0.12</td><td>92.8</td><td colspan=\"3\">0.07</td><td>67.3</td><td colspan=\"2\">0.13</td><td>92.1</td><td>0.08</td><td>67.4</td><td colspan=\"3\">0.13</td><td>91.8</td><td colspan=\"3\">0.09</td><td>67.5</td><td colspan=\"3\">0.11</td><td>92.8</td><td colspan=\"3\">0.08</td><td>67.4</td><td colspan=\"3\">0.13</td><td>92.0</td><td>0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>62.5</td><td colspan=\"3\">0.05</td><td>90.0</td><td>0.03</td><td>62.7</td><td colspan=\"3\">0.05</td><td>90.9</td><td colspan=\"3\">0.03</td><td>62.6</td><td colspan=\"2\">0.05</td><td>90.2</td><td>0.03</td><td>62.4</td><td colspan=\"3\">0.07</td><td>90.1</td><td colspan=\"3\">0.02</td><td>62.6</td><td colspan=\"3\">0.06</td><td>91.0</td><td colspan=\"3\">0.02</td><td>62.5</td><td>0.07</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>62.6</td><td colspan=\"3\">0.06</td><td>90.4</td><td>0.03</td><td>62.7</td><td colspan=\"3\">0.05</td><td>91.3</td><td colspan=\"3\">0.03</td><td>62.6</td><td colspan=\"2\">0.06</td><td>90.6</td><td>0.03</td><td>62.4</td><td colspan=\"3\">0.07</td><td>90.0</td><td colspan=\"3\">0.03</td><td>62.5</td><td colspan=\"3\">0.07</td><td>91.0</td><td colspan=\"3\">0.03</td><td>62.4</td><td>0.07</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>78.5</td><td colspan=\"3\">0.07</td><td>83.2</td><td>0.04</td><td>78.7</td><td colspan=\"3\">0.06</td><td>84.1</td><td colspan=\"3\">0.04</td><td>78.6</td><td colspan=\"2\">0.07</td><td>83.4</td><td>0.04</td><td>78.4</td><td colspan=\"3\">0.08</td><td>83.3</td><td colspan=\"3\">0.06</td><td>78.6</td><td colspan=\"3\">0.07</td><td>84.2</td><td colspan=\"3\">0.05</td><td>78.5</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>67.1</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td>77.3</td><td colspan=\"3\">0.01</td><td>83.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"2\">0.01</td><td>82.7</td><td>0.01</td><td>77.0</td><td colspan=\"3\">0.02</td><td>82.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"3\">0.01</td><td>83.2</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>77.1</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td>77.3</td><td colspan=\"3\">0.01</td><td>83.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"2\">0.01</td><td>82.7</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.12</td><td>90.8</td><td colspan=\"3\">0.05</td><td>65.7</td><td colspan=\"3\">0.11</td><td>91.8</td><td colspan=\"3\">0.05</td><td>65.6</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>65.8</td><td colspan=\"3\">0.12</td><td>90.8</td><td>0.06</td><td>66.0</td><td colspan=\"3\">0.11</td><td>91.8</td><td colspan=\"3\">0.05</td><td>65.9</td><td colspan=\"2\">0.12</td><td>91.0</td><td>0.06</td><td>65.7</td><td colspan=\"3\">0.12</td><td>90.8</td><td colspan=\"3\">0.07</td><td>65.8</td><td colspan=\"3\">0.11</td><td>91.7</td><td colspan=\"3\">0.07</td><td>65.7</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>61.2</td><td colspan=\"3\">0.06</td><td>89.3</td><td>0.01</td><td>61.3</td><td colspan=\"3\">0.05</td><td>90.3</td><td colspan=\"3\">0.01</td><td>61.2</td><td colspan=\"2\">0.06</td><td>89.5</td><td>0.01</td><td>60.8</td><td colspan=\"3\">0.05</td><td>89.2</td><td colspan=\"3\">0.01</td><td>61.0</td><td colspan=\"3\">0.05</td><td>60.9</td><td colspan=\"3\">0.01</td><td>60.9</td><td>0.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>60.8</td><td colspan=\"3\">0.06</td><td>89.2</td><td>0.01</td><td>61.0</td><td colspan=\"3\">0.05</td><td>90.2</td><td colspan=\"3\">0.01</td><td>60.9</td><td colspan=\"2\">0.06</td><td>89.4</td><td>0.01</td><td>60.9</td><td colspan=\"3\">0.04</td><td>89.0</td><td colspan=\"3\">0.01</td><td>61.1</td><td colspan=\"3\">0.04</td><td>89.9</td><td colspan=\"3\">0.01</td><td>61.0</td><td>0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"14\">G-R</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.2</td><td colspan=\"3\">0.16</td><td>84.6</td><td>0.14</td><td>80.4</td><td colspan=\"3\">0.14</td><td>85.3</td><td colspan=\"3\">0.14</td><td>80.3</td><td colspan=\"2\">0.15</td><td>84.9</td><td>0.14</td><td>80.2</td><td colspan=\"3\">0.16</td><td>84.8</td><td colspan=\"3\">0.14</td><td>80.4</td><td colspan=\"3\">0.14</td><td>85.5</td><td colspan=\"3\">0.14</td><td>80.3</td><td>0.15</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.1</td><td colspan=\"3\">0.18</td><td>85.7</td><td>0.16</td><td>83.4</td><td colspan=\"3\">0.16</td><td>86.4</td><td colspan=\"3\">0.16</td><td>83.3</td><td colspan=\"2\">0.17</td><td>86.0</td><td>0.16</td><td>83.3</td><td colspan=\"3\">0.18</td><td>85.5</td><td colspan=\"3\">0.16</td><td>83.5</td><td colspan=\"3\">0.15</td><td>86.2</td><td colspan=\"3\">0.16</td><td>83.4</td><td>0.16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.2</td><td colspan=\"3\">0.05</td><td>83.2</td><td>0.04</td><td>75.3</td><td colspan=\"3\">0.04</td><td>83.9</td><td colspan=\"3\">0.04</td><td>75.3</td><td colspan=\"2\">0.05</td><td>83.5</td><td>0.04</td><td>75.3</td><td colspan=\"3\">0.05</td><td>83.1</td><td colspan=\"3\">0.05</td><td>75.5</td><td colspan=\"3\">0.04</td><td>83.8</td><td colspan=\"3\">0.05</td><td>75.4</td><td>0.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>78.5</td><td colspan=\"3\">0.05</td><td>83.9</td><td>0.05</td><td>78.7</td><td colspan=\"3\">0.04</td><td>84.6</td><td colspan=\"3\">0.05</td><td>78.6</td><td colspan=\"2\">0.05</td><td>84.2</td><td>0.05</td><td>78.6</td><td colspan=\"3\">0.06</td><td>84.1</td><td colspan=\"3\">0.04</td><td>78.8</td><td colspan=\"3\">0.05</td><td>84.7</td><td colspan=\"3\">0.04</td><td>78.7</td><td>0.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>64.0</td><td colspan=\"3\">0.23</td><td>91.5</td><td>0.15</td><td>64.2</td><td colspan=\"3\">0.20</td><td>92.2</td><td colspan=\"3\">0.15</td><td>64.1</td><td colspan=\"2\">0.22</td><td>91.8</td><td>0.15</td><td>64.2</td><td colspan=\"3\">0.24</td><td>91.4</td><td colspan=\"3\">0.16</td><td>64.3</td><td colspan=\"3\">0.21</td><td>92.2</td><td colspan=\"3\">0.16</td><td>64.3</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>63.9</td><td colspan=\"3\">0.24</td><td>91.7</td><td>0.16</td><td>64.0</td><td colspan=\"3\">0.21</td><td>92.4</td><td colspan=\"3\">0.16</td><td>64.0</td><td colspan=\"2\">0.23</td><td>92.0</td><td>0.16</td><td>64.1</td><td colspan=\"3\">0.23</td><td>91.5</td><td colspan=\"3\">0.15</td><td>64.3</td><td colspan=\"3\">0.20</td><td>92.2</td><td colspan=\"3\">0.15</td><td>64.2</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>59.3</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.05</td><td>59.4</td><td colspan=\"3\">0.12</td><td>90.6</td><td colspan=\"3\">0.12</td><td>59.3</td><td colspan=\"2\">0.13</td><td>90.1</td><td>0.05</td><td>59.2</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.05</td><td>59.5</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.05</td><td>59.5</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>59.2</td><td colspan=\"3\">0.13</td><td>90.0</td><td>0.15</td><td>59.4</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.15</td><td>59.3</td><td colspan=\"2\">0.12</td><td>90.3</td><td>0.05</td><td>59.2</td><td colspan=\"3\">0.13</td><td>83.8</td><td colspan=\"3\">0.09</td><td>78.6</td><td colspan=\"3\">0.12</td><td>84.5</td><td>0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>58.5</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.11</td><td>58.7</td><td colspan=\"3\">0.11</td><td>84.6</td><td colspan=\"3\">0.12</td><td>78.6</td><td colspan=\"2\">0.12</td><td>84.2</td><td>0.12</td><td>78.4</td><td colspan=\"3\">0.13</td><td>82.5</td><td colspan=\"3\">0.11</td><td>78.9</td><td colspan=\"3\">0.12</td><td>83.4</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.9</td><td colspan=\"3\">0.14</td><td>82.3</td><td>0.01</td><td>73.9</td><td colspan=\"3\">0.01</td><td>83.0</td><td colspan=\"3\">0.11</td><td>73.8</td><td colspan=\"2\">0.11</td><td>82.6</td><td>0.01</td><td>73.6</td><td colspan=\"3\">0.02</td><td>82.5</td><td colspan=\"3\">0.01</td><td>73.8</td><td colspan=\"3\">0.02</td><td>83.1</td><td>0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>56.8</td><td colspan=\"3\">0.04</td><td>83.1</td><td>0.01</td><td>76.9</td><td colspan=\"3\">0.04</td><td>83.8</td><td colspan=\"3\">0.01</td><td>76.8</td><td colspan=\"2\">0.04</td><td>83.4</td><td>0.01</td><td>76.8</td><td colspan=\"3\">0.04</td><td>83.2</td><td colspan=\"3\">0.01</td><td>77.0</td><td colspan=\"3\">0.04</td><td>76.8</td><td>0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>62.5</td><td colspan=\"3\">0.13</td><td>89.8</td><td>0.11</td><td>59.4</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.12</td><td>59.3</td><td colspan=\"2\">0.12</td><td>90.3</td><td>0.12</td><td>59.2</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.11</td><td>59.5</td><td colspan=\"3\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.9</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.11</td><td>57.9</td><td colspan=\"3\">0.18</td><td>89.9</td><td colspan=\"3\">0.12</td><td>57.8</td><td colspan=\"2\">0.19</td><td>89.9</td><td>0.12</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td colspan=\"3\">0.11</td><td>57.8</td><td colspan=\"3\">0.19</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td>0.12</td><td>57.9</td><td colspan=\"3\">0.19</td><td>89.8</td><td colspan=\"3\">0.11</td><td>57.8</td><td colspan=\"2\">0.19</td><td>89.9</td><td>0.12</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td colspan=\"3\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 5: Complete results set.",
        "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_13",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/363f69272a58605d352a8b5bbe8391cf7733eeb7355f019bbf8844961af58a12.jpg",
      "image_filename": "363f69272a58605d352a8b5bbe8391cf7733eeb7355f019bbf8844961af58a12.jpg",
      "caption": "Table 7: Results when group specific models are trained with ITL and MTL with or without the sensitive feature as predictor and with or without the fairness constraint.",
      "context_before": "We have presented two novel, but related, ideas in this work. Firstly, to resolve the tension between accuracy gains obtained by using a sensitive feature as part of the model, and the potential inapplicability of such an approach, we have suggested to first predict the sensitive feature based on the non-sensitive features, and then use the predicted value in the functional form of a model, allowing to treat people belonging to different groups, but having similar non-sensitive features, equally. Furthermore, we have demonstrated how the predicted sensitive feature can then used in a fairness constrained MTL framework. We confirmed the validity of the above approach empirically, giving us substantial improvements in both accuracy and fairness, compared to STL and ITL. We believe this to be a fruitful area of possible future research. Of course, a non-linear extension of the above framework would be interesting to study, although we did not notice any substantial improvements on the Adult and COMPAS datasets considered in this work. Moreover, it would be interesting to see if the above framework can be extended to include other fairness definitions, apart from the EOp and EOd\n\n[Section: L. Oneto et al.]\n\n10From now, for sake of simplicity, we use the word illegal (legal) in order to define a model which uses (not-uses), either implicitly or explicitly, the sensitive feature as part of its functional form.",
      "context_after": "",
      "referring_paragraphs": [
        "Table 7 reports the case when the group specific models are trained with ITL or MTL, the same setting as Table 6. MTL notably improves both accuracy and fairness. The fairness constraints do",
        "Table 7 reports the case when the group specific models are trained with ITL or MTL, the same setting as Table 6.",
        "Moreover, it would be interesting to see if the above framework can be extended to include other fairness definitions, apart from the EOp and EOd\n\nTable 7: Results when group specific models are trained with ITL and MTL with or without the sensitive feature as predictor and with or without the fairness constraint."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_14",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/46a83481010f9c99558adeb62ed47336f56ddfa618bcba37d0d34379cd856540.jpg",
      "image_filename": "46a83481010f9c99558adeb62ed47336f56ddfa618bcba37d0d34379cd856540.jpg",
      "caption": "Table 8: Adult dataset: ACC of STL, ITL, and MTL when $P { = } 0$ , $F { = } 0$ , and $S { = } 0$ .",
      "context_before": "",
      "context_after": "that we have tested. Finally, it would be valuable to provide theoretical conditions on the data distribution for which our approach provably works.\n\nThis work was supported by the Amazon AWS Machine Learning Research Award.",
      "referring_paragraphs": [
        "not affect the accuracy too much, while giving remarkable improvements in fairness. ITL and MTL are not affected by not including or including the sensitive feature predictor, as expected from the theory given that the models already have already different biases. Table 8 reports a comparison between STL, ITL, and MTL on the Adult dataset, showing the accuracy on each group for the different models for the case that $P { = } 0$ , $F { = } 0$ , and $S { = } 0$ . These results P F Sclearly demonst",
        "Table 8 reports a comparison between STL, ITL, and MTL on the Adult dataset, showing the accuracy on each group for the different models for the case that $P { = } 0$ , $F { = } 0$ , and $S { = } 0$ .",
        "<table><tr><td rowspan=\"3\" colspan=\"2\"></td><td colspan=\"31\">Adult Dataset</td><td></td></tr><tr><td>-01</td><td>-1</td><td>-1</td><td>-1</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"4\">MTL</td><td></td><td></td></tr><tr><td>P</td><td>D</td><td>F</td><td>S</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOd</td><td>ACC</td><td></td></tr><tr><td rowspan=\"3\">G</td><td>0</td><td>1</td><td>0</td><td>0</td><td></td><td>74.5</td><td>0.18</td><td>90.0</td><td>0.14</td><td></td><td>74.7</td><td>0.15</td><td>91.0</td><td>0.13</td><td></td><td>74.6</td><td>0.17</td><td>90.2</td><td>0.14</td><td></td><td>70.7</td><td>0.19</td><td>84.5</td><td>0.15</td><td>70.9</td><td>0.17</td><td>84.4</td><td>0.14</td><td>70.8</td><td>0.16</td><td>83.6</td><td>0.13</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td></td><td>69.7</td><td>0.08</td><td>88.3</td><td>0.04</td><td></td><td>69.9</td><td>0.07</td><td>89.2</td><td>0.04</td><td></td><td>69.8</td><td>0.08</td><td>88.5</td><td>0.04</td><td></td><td>66.1</td><td>0.08</td><td>83.0</td><td>0.04</td><td>66.3</td><td>0.08</td><td>82.8</td><td>0.04</td><td>66.2</td><td>0.07</td><td>82.1</td><td>0.04</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td></td><td>69.7</td><td>0.08</td><td>88.1</td><td>0.03</td><td></td><td>69.9</td><td>0.07</td><td>89.1</td><td>0.03</td><td></td><td>69.8</td><td>0.08</td><td>88.3</td><td>0.03</td><td></td><td>66.1</td><td>0.09</td><td>82.9</td><td>0.07</td><td>66.3</td><td>0.08</td><td>82.8</td><td>0.06</td><td>66.2</td><td>0.08</td><td>82.1</td><td>0.06</td><td></td></tr><tr><td rowspan=\"3\">R</td><td>0</td><td>1</td><td>0</td><td>0</td><td></td><td>67.4</td><td>0.13</td><td>91.8</td><td>0.10</td><td></td><td>67.6</td><td>0.11</td><td>92.8</td><td>0.08</td><td></td><td>67.5</td><td>0.13</td><td>92.0</td><td>0.10</td><td></td><td>67.3</td><td>0.12</td><td>91.7</td><td>0.08</td><td>67.5</td><td>0.11</td><td>92.7</td><td>0.07</td><td>67.4</td><td>0.12</td><td>92.0</td><td>0.08</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td></td><td>62.5</td><td>0.05</td><td>90.0</td><td>0.03</td><td></td><td>62.7</td><td>0.05</td><td>90.9</td><td>0.03</td><td></td><td>62.6</td><td>0.05</td><td>90.2</td><td>0.03</td><td></td><td>62.4</td><td>0.07</td><td>90.1</td><td>0.02</td><td>62.6</td><td>0.06</td><td>91.0</td><td>0.02</td><td>62.5</td><td>0.07</td><td>90.3</td><td>0.02</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td></td><td>62.6</td><td>0.06</td><td>90.4</td><td>0.03</td><td></td><td>62.7</td><td>0.05</td><td>91.3</td><td>0.03</td><td></td><td>62.6</td><td>0.06</td><td>90.6</td><td>0.03</td><td></td><td>62.4</td><td>0.07</td><td>90.0</td><td>0.03</td><td>62.5</td><td>0.07</td><td>91.0</td><td>0.03</td><td>62.4</td><td>0.07</td><td>90.2</td><td>0.03</td><td></td></tr><tr><td rowspan=\"3\">G+r</td><td>0</td><td>1</td><td>0</td><td>0</td><td></td><td>64.0</td><td>0.23</td><td>91.5</td><td>0.15</td><td></td><td>64.2</td><td>0.20</td><td>92.2</td><td>0.15</td><td></td><td>64.1</td><td>0.22</td><td>91.8</td><td>0.15</td><td></td><td>64.2</td><td>0.24</td><td>91.4</td><td>0.16</td><td>64.3</td><td>0.21</td><td>92.2</td><td>0.16</td><td>64.3</td><td>0.22</td><td>91.7</td><td>0.16</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td></td><td>59.3</td><td>0.14</td><td>89.8</td><td>0.05</td><td></td><td>59.4</td><td>0.12</td><td>90.6</td><td>0.05</td><td></td><td>59.3</td><td>0.13</td><td>90.1</td><td>0.05</td><td></td><td>59.2</td><td>0.13</td><td>90.1</td><td>0.05</td><td>59.4</td><td>0.11</td><td>90.8</td><td>0.05</td><td>59.3</td><td>0.12</td><td>90.4</td><td>0.05</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td></td><td>59.2</td><td>0.13</td><td>90.0</td><td>0.05</td><td></td><td>59.4</td><td>0.11</td><td>90.8</td><td>0.05</td><td></td><td>59.3</td><td>0.12</td><td>90.3</td><td>0.05</td><td></td><td>59.4</td><td>0.13</td><td>89.9</td><td>0.05</td><td>59.5</td><td>0.11</td><td>90.7</td><td>0.05</td><td>59.5</td><td>0.12</td><td>90.3</td><td>0.05</td><td></td></tr></table>\n\nTable 8: Adult dataset: ACC of STL, ITL, and MTL when $P { = } 0$ , $F { = } 0$ , and $S { = } 0$ ."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1cdbdd23114702af7f3973ce4167d6a94ec929c8ecca6ca3f78421a185aab752.jpg",
      "image_filename": "1cdbdd23114702af7f3973ce4167d6a94ec929c8ecca6ca3f78421a185aab752.jpg",
      "caption": "Adult Dataset",
      "context_before": "that we have tested. Finally, it would be valuable to provide theoretical conditions on the data distribution for which our approach provably works.\n\nThis work was supported by the Amazon AWS Machine Learning Research Award.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/a4b704c4254d06cea611e4164d3b66917711432a7b4a0fbe28ff164f99ec98f0.jpg",
      "image_filename": "a4b704c4254d06cea611e4164d3b66917711432a7b4a0fbe28ff164f99ec98f0.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1fbe9bb8c301233edb68fb6d0d5b3ee8eb97d0afc51c8e97b2c91074c25e7b36.jpg",
      "image_filename": "1fbe9bb8c301233edb68fb6d0d5b3ee8eb97d0afc51c8e97b2c91074c25e7b36.jpg",
      "caption": "COMPAS Dataset",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/759b082d24fba15fa0fade076f9f4e6fc7ce08807a60302ce4d33fafddad99d5.jpg",
      "image_filename": "759b082d24fba15fa0fade076f9f4e6fc7ce08807a60302ce4d33fafddad99d5.jpg",
      "caption": "Adult Dataset",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/8704adf1b77fe4a5254e61de371bff669fe29dd157f9f17d6164c14040eadf74.jpg",
      "image_filename": "8704adf1b77fe4a5254e61de371bff669fe29dd157f9f17d6164c14040eadf74.jpg",
      "caption": "COMPAS Dataset",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_20",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/c7d5343d5e895a57de5f524ebb703d783b9c3996a07f1096b698c6847f302630.jpg",
      "image_filename": "c7d5343d5e895a57de5f524ebb703d783b9c3996a07f1096b698c6847f302630.jpg",
      "caption": "Table 10: Comparison between the group specific models trained with MTL, with fairness constraint, and the true sensitive feature exploited as a predictor, against the same model when the predicted sensitive feature exploited as predictor.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Finally, in Table 10 we report a comparison between the best illegal model and the same model, but for which uses we used the predicted sensitive feature, instead of the true one, both in training and in testing. Notably, Table 10 shows that using the predicted",
        "Finally, in Table 10 we report a comparison between the best illegal model and the same model, but for which uses we used the predicted sensitive feature, instead of the true one, both in training and in testing.",
        "Adult Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">P</td><td rowspan=\"2\">D</td><td rowspan=\"2\">F</td><td rowspan=\"2\">S</td><td colspan=\"2\">MTL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">MTL</td></tr><tr><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOd</td></tr></table>\n\nCOMPAS Dataset   \n\n<table><tr><td>G</td><td>0 1 1 1 1</td><td>1 1 1 1</td><td>1 1 1 1</td><td>88.1 87.4</td><td>0.03 0.01</td><td>89.1 88.3</td><td>0.03 0.01</td><td>88.3 87.6</td><td>0.03 0.01</td></tr><tr><td>R</td><td>0 1 1 1 1</td><td>1 1 1 1</td><td>1 1 1 1</td><td>90.4 89.2</td><td>0.03 0.01</td><td>91.3 90.2</td><td>0.03 0.01</td><td>90.6 89.4</td><td>0.03 0.01</td></tr><tr><td>G+R</td><td>0 1 1 1 1</td><td>1 1 1 1</td><td>1 1 1 1</td><td>90.0 89.0</td><td>0.05 0.01</td><td>90.8 89.8</td><td>0.05 0.01</td><td>90.3 89.3</td><td>0.05 0.01</td></tr></table>\n\nTable 10: Comparison between the group specific models trained with MTL, with fairness constraint, and the true sensitive feature exploited as a predictor, against the same model when the predicted sensitive feature exploited as predictor."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_21",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/027c27c3d05e75931f5f5c733bfd3dff42bab8b5d68cf0560e70cd788d0f8bdb.jpg",
      "image_filename": "027c27c3d05e75931f5f5c733bfd3dff42bab8b5d68cf0560e70cd788d0f8bdb.jpg",
      "caption": "Table 11: Adult dataset: accuracy in percentage of prediction based on the distance from the MTL separator which uses the predicted sensitive feature (see Table 10).",
      "context_before": "",
      "context_after": "[Section: L. Oneto et al.]",
      "referring_paragraphs": [
        "sensitive feature in place of the true one preserves the accuracy of the learned model, but with a notable improvement in fairness. In attempt to explain this phenomena, in Table 11 we report the average group accuracy for predicting the sensitive features gender and race, as a function of the distance from the group specific models separators trained with MTL on the Adult dataset. Table 11 shows that the accuracy in predicting the sensitive feature decreases as we get closer to the separator. T",
        "In attempt to explain this phenomena, in Table 11 we report the average group accuracy for predicting the sensitive features gender and race, as a function of the distance from the group specific models separators trained with MTL on the Adult dataset.",
        "<table><tr><td>G</td><td>0 1</td><td>1 1</td><td>1 1</td><td>82.9 82.1</td><td>0.07 0.01</td><td>82.8 82.0</td><td>0.06 0.01</td><td>82.1 81.3</td><td>0.06 0.01</td></tr><tr><td>R</td><td>0 1</td><td>1 1</td><td>1 1</td><td>90.0 89.0</td><td>0.03 0.01</td><td>91.0 89.9</td><td>0.03 0.01</td><td>90.2 89.2</td><td>0.03 0.01</td></tr><tr><td>G+R</td><td>0 1</td><td>1 1</td><td>1 1</td><td>89.9 89.0</td><td>0.05 0.01</td><td>90.7 89.8</td><td>0.05 0.01</td><td>90.3 89.3</td><td>0.05 0.01</td></tr></table>\n\nTable 11: Adult dataset: accuracy in percentage of prediction based on the distance from the MTL separator which uses the predicted sensitive feature (see Table 10)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1810.08683_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1811.00103": [
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg",
      "image_filename": "1811.00103_page0_fig0.jpg",
      "caption": "Average reconstruction error (RE) of PCA on LFW",
      "context_before": "Many of the proposed solutions to “biased data” problems amount to re-weighting the training set or adding noise to some of the labels; for “biased algorithms”, most work has focused on maximizing accuracy subject to a constraint forbidding (or penalizing) an unfair model. Both of these concerns\n\narXiv:1811.00103v1 [cs.LG] 31 Oct 2018\n\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg",
      "image_filename": "1811.00103_page0_fig1.jpg",
      "caption": "Average reconstruction error (RE) of PCA on LFW (resampled) Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
      "context_before": "",
      "context_after": "and approaches have significant merit, but form an incomplete picture of the ML pipeline and where unfairness might be introduced therein. Our work takes another step in fleshing out this picture by analyzing when dimensionality reduction might inadvertently introduce bias. We focus on principal component analysis (henceforth PCA), perhaps the most fundamental dimensionality reduction technique in the sciences [Pearson, 1901; Hotelling, 1933; Jolliffe, 1986]. We show several real-world data sets for which PCA incurs much higher average reconstruction error for one population than another, even when the populations are of similar sizes. Figure 1 shows that PCA on labeled faces in the wild data set (LFW) has higher reconstruction error for women than men even if male and female faces are sampled with equal weight.\n\nThis work underlines the importance of considering fairness and bias at every stage of data science, not only in gathering and documenting a data set [Gebru et al., 2018] and in training a model, but also in any interim data processing steps. Many scientific disciplines have adopted PCA as a default preprocessing step, both to avoid the curse of dimensionality and also to do exploratory/explanatory data analysis (projecting the data into a number of dimensions that humans can more easily visualize). The study of human biology, disease, and the development of health interventions all face both aforementioned difficulties, as do numerous economic and financial analysis. In such high-stakes settings, where statistical tools will help in making decisions that affect a diverse set of people, we must take particular care to ensure that we share the benefits of data science with a diverse community.\n\nWe also emphasize this work has implications for representational rather than just allocative harms, a distinction drawn by Crawford [2017] between how people are represented and what goods or opportunities they receive. Showing primates in search results for African Americans is repugnant primarily due to its representing and reaffirming a racist painting of African Americans, not because it directly reduces any one person’s access to a resource. If the default template for a data set begins with running PCA, and PCA does a better job representing men than women, or white people over minorities, the new representation of the data set itself may rightly be considered an unacceptable sketch of the world it aims to describe.",
      "referring_paragraphs": [
        "and approaches have significant merit, but form an incomplete picture of the ML pipeline and where unfairness might be introduced therein. Our work takes another step in fleshing out this picture by analyzing when dimensionality reduction might inadvertently introduce bias. We focus on principal component analysis (henceforth PCA), perhaps the most fundamental dimensionality reduction technique in the sciences [Pearson, 1901; Hotelling, 1933; Jolliffe, 1986]. We show several real-world data sets",
        "Results We focus on projections into relatively few dimensions, as those are used ubiquitously in early phases of data exploration. As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women on the LFW data set. This gap is at the scale of up to $10 \\%$ of the total reconstruction error when we project to 20 dimensions. This still holds when we subsample male and female faces with equal probability from the data",
        "Average reconstruction error (RE) of PCA on LFW (resampled)   \nFigure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender.",
        "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women on the LFW data set."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig2.jpg",
      "image_filename": "1811.00103_page0_fig2.jpg",
      "caption": "1Lipton et al. [2017] has asked whether equal treatment requires different models for two groups.",
      "context_before": "Definition 4.2 (Reconstruction error). Given two matrices $Y$ and $Z$ of the same size, the reconstruction error of $Y$ with respect to $Z$ is defined as\n\n$$ e r r o r (Y, Z) = \\| Y - Z \\| _ {F} ^ {2}. $$\n\n1Lipton et al. [2017] has asked whether equal treatment requires different models for two groups.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig3.jpg",
      "image_filename": "1811.00103_page0_fig3.jpg",
      "caption": "Figure 2",
      "context_before": "",
      "context_after": "Definition 4.3 (Reconstruction loss). Given a matrix $Y \\in \\mathbb { R } ^ { a \\times n }$ , let $\\widehat { Y } \\in \\mathbb { R } ^ { a \\times n }$ be the optimal rank-d approximation of Y . For a matrix $Z \\in \\mathbb { R } ^ { a \\times n }$ with rank at most d we define\n\n$$ l o s s (Y, Z) := \\| Y - Z \\| _ {F} ^ {2} - \\| Y - \\widehat {Y} \\| _ {F} ^ {2}. $$\n\nThen, the optimization that we study asks to minimize the maximum loss suffered by any group. This captures the idea that, fixing a feasible solution, the objective will only improve if it improves the loss for the group whose current representation is worse. Furthermore, considering the reconstruction loss and not the reconstruction error prevents the optimization from incurring error for one subpopulation without improving the error for the other one as described in Figure 2b.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig4.jpg",
      "image_filename": "1811.00103_page0_fig4.jpg",
      "caption": "The result of Theorem 5.1 in two groups generalizes to more than two groups as follows.",
      "context_before": "$$ \\begin{array}{l} l o s s (A, A P ^ {*}) = \\| A - A P ^ {*} \\| _ {F} ^ {2} - \\| A - \\widehat {A} \\| _ {F} ^ {2} = \\operatorname {T r} \\left((A - A P ^ {*}) (A - A P ^ {*}) ^ {\\top}\\right) - \\| A \\| _ {F} ^ {2} + \\| \\widehat {A} \\| _ {F} ^ {2} \\\\ = \\operatorname {T r} \\left((A - A P ^ {*}) (A - A P ^ {*}) ^ {\\top}\\right) - \\| A \\| _ {F} ^ {2} + \\| \\widehat {A} \\| _ {F} ^ {2} = \\| \\widehat {A} \\| _ {F} ^ {2} - 2 \\operatorname {T r} (A P ^ {*} A ^ {\\top}) + \\operatorname {T r} (A P ^ {* 2} A ^ {\\top}) \\\\ = \\| \\widehat {A} \\| _ {F} ^ {2} - \\sum_ {i = 1} ^ {n} \\left(2 \\lambda_ {i} ^ {*} - \\lambda_ {i} ^ {* 2}\\right) \\langle A ^ {T} A, u _ {i} u _ {i} ^ {T} \\rangle = \\| \\widehat {A} \\| _ {F} ^ {2} - \\sum_ {i = 1} ^ {n} \\bar {\\lambda} \\langle A ^ {T} A, u _ {i} u _ {i} ^ {T} \\rangle , \\\\ \\end{array} $$\n\nwhere the last inequality is by the choice of $\\lambda _ { j } ^ { * } = 1 - \\sqrt { 1 - \\bar { \\lambda _ { j } } }$ . The same equality holds true for group $B$ . Therefore, $P ^ { * }$ gives the equal loss of $z ^ { * } \\leq \\hat { z }$ for two groups. The embedding x → (x · u1, . . . , x · ud−1, pλ∗d x · ud, pλ∗d+1 x · ud+1) corresponds to the affine projection of any point (row) of $A , B$ defined by the solution $P ^ { * }$ .\n\nIn both cases, the objective value is at most that of the original fairness objective.",
      "context_after": "The result of Theorem 5.1 in two groups generalizes to more than two groups as follows. Given $m$ data points in $\\mathbb { R } ^ { n }$ with $k$ subgroups $A _ { 1 } , A _ { 2 } , \\ldots , A _ { k }$ , and $d \\leq n$ the desired number of dimensions of projected space, we generalize Definition 4.4 of fair PCA problem as optimizing\n\n$$ \\min _ {U \\in \\mathbb {R} ^ {m \\times n}, \\operatorname {r a n k} (U) \\leq d} \\max _ {i \\in \\{1, \\dots , k \\}} \\left\\{\\frac {1}{| A _ {i} |} \\operatorname {l o s s} \\left(A _ {i}, U _ {A _ {i}}\\right)\\right) \\Bigg \\}, \\tag {10} $$\n\nwhere $U _ { A _ { i } }$ are matrices with rows corresponding to rows of $U$ for groups $A _ { i }$ .",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg",
      "image_filename": "1811.00103_page0_fig5.jpg",
      "caption": "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data. As we expect,",
      "context_before": "We use two common human-centric data sets for our experiments. The first one is labeled faces in the wild (LFW) [Huang et al., 2007], the second is the Default Credit data set [Yeh and Lien, 2009]. We preprocess all data to have its mean at the origin. For the LFW data, we normalized each pixel value by $\\scriptstyle { \\frac { 1 } { 2 5 5 } }$ . The gender information for LFW was taken from Afifi and Abdelhamed [2017], who manually verified the correctness of these labels. For the credit data, since different attributes are measurements of incomparable units, we normalized the variance of each attribute to be equal to 1. The code of all experiments is publicly available at https://github.com/samirasamadi/Fair-PCA.\n\nResults We focus on projections into relatively few dimensions, as those are used ubiquitously in early phases of data exploration. As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women on the LFW data set. This gap is at the scale of up to $10 \\%$ of the total reconstruction error when we project to 20 dimensions. This still holds when we subsample male and female faces with equal probability from the data set, and so men and women have equal magnitude in the objective function of PCA (Figure 1 right).\n\nFigure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data. As we expect,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig6.jpg",
      "image_filename": "1811.00103_page0_fig6.jpg",
      "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data. As we expect,",
        "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data.",
        "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg",
      "image_filename": "1811.00103_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig8.jpg",
      "image_filename": "1811.00103_page0_fig8.jpg",
      "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.",
      "context_before": "",
      "context_after": "as the number of dimensions increase, the average reconstruction error of every population decreases. For LFW, the original data is in 1764 dimensions $4 2 \\times 4 2$ images), therefore, at 20 dimensions we still see a considerable reconstruction error. For the Credit data, we see that at 21 dimensions, the average reconstruction error of both populations reach 0, as this data originally lies in 21 dimensions. In order to see how fair are each of these methods, we need to zoom in further and look at the average loss of populations.\n\nFigure 4 shows the average loss of each population as the result of applying vanilla PCA and Fair PCA on both data sets. Note that at the optimal solution of Fair PCA, the average loss of two populations are the same, therefore we have one line for “Fair loss”. We observe that PCA suffers much higher average loss for female faces than male faces. After running fair PCA, we observe that the average loss for fair PCA is relatively in the middle of the average loss for male and female. So, there is improvement in terms of the female average loss which comes with a cost in terms of male average loss. Similar observation holds for the Credit data set. In this context, it appears there is some cost to optimizing for the less well represented population in terms of the better-represented population.\n\nThis work is far from a complete study of when and how dimensionality reduction might help or hurt the fair treatment of different populations. Several concrete theoretical questions remain using our framework. What is the complexity of optimizing the fairness objective? Is it NP-hard, even for $d = 1 2$ Our work naturally extends to $k$ predefined subgroups rather than just 2, where the number of additional dimensions our algorithm uses is $k - 1$ . Are these additional dimensions necessary for computational efficiency?",
      "referring_paragraphs": [
        "Figure 4 shows the average loss of each population as the result of applying vanilla PCA and Fair PCA on both data sets. Note that at the optimal solution of Fair PCA, the average loss of two populations are the same, therefore we have one line for “Fair loss”. We observe that PCA suffers much higher average loss for female faces than male faces. After running fair PCA, we observe that the average loss for fair PCA is relatively in the middle of the average loss for male and female. So, there is",
        "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig9.jpg",
      "image_filename": "1811.00103_page0_fig9.jpg",
      "caption": "A.3 Application of multiplicative update method to the fair PCA problem",
      "context_before": "$$ 0 \\leq (1 + \\eta) \\left(A _ {i} \\bar {x} - b _ {i} + z ^ {*}\\right) + \\frac {\\epsilon}{2} \\Rightarrow A _ {i} \\bar {x} - b _ {i} + z ^ {*} \\geq - \\frac {\\epsilon}{2} \\tag {24} $$\n\n$$ 0 \\leq (1 - \\eta) \\left(A _ {i} \\bar {x} - b _ {i} + z ^ {*}\\right) + \\frac {\\epsilon}{2} \\Rightarrow A _ {i} \\bar {x} - b _ {i} + z ^ {*} \\geq - \\epsilon \\tag {25} $$\n\nusing the fact that $\\eta \\leq 1 / 2$ .",
      "context_after": "A.3 Application of multiplicative update method to the fair PCA problem\n\nIn this section, we apply MW results for solving LP to solve the SDP relaxation (4) of fair PCA.\n\nLP formulation of fair PCA relaxation The SDP relaxation (4) of fair PCA can be written in the form (13) as an LP with two constraints",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig10.jpg",
      "image_filename": "1811.00103_page0_fig10.jpg",
      "caption": "Proof of Lemma 4.7: From Lemma 3.1, we know that there exist a matrix $W _ { A } \\in \\mathbb { R } ^ { n \\times d }$ such that $W _ { A } ^ { T } W _ { A } = I$ and $\\widehat { A } = A W _ { A } W _ { A } ^ { T }$ .",
      "context_before": "$$ l o s s (A, A V V ^ {T}) = \\| A - A V V ^ {T} \\| _ {F} ^ {2} - \\| A - \\widehat {A} \\| _ {F} ^ {2} = \\sum \\| A _ {i} - A _ {i} V V ^ {T} \\| ^ {2} - \\| A - \\widehat {A} \\| _ {F} ^ {2} $$\n\nThis finished the proof that $\\mathit { l o s s } ( A , A V V ^ { T } ) ~ \\leq ~ \\mathit { l o s s } ( A , U _ { A } )$ . Similarly, we can see that $l o s s ( B , B V V ^ { T } ) \\leq \\bar { l } o s s ( B , U _ { B } )$ . Therefore\n\n$$ \\begin{array}{l} f (\\left[ \\begin{array}{c} A \\\\ B \\end{array} \\right] V V ^ {T}) = \\max \\big (\\frac {1}{| A |} l o s s (A, A V V ^ {T}), \\frac {1}{| B |} l o s s (B, B V V ^ {T}) \\big) \\\\ \\leq \\max \\left(\\frac {1}{| A |} \\operatorname {l o s s} (A, U _ {A}), \\frac {1}{| B |} \\operatorname {l o s s} (B, U _ {B})\\right) \\\\ = f (U) \\\\ \\end{array} $$",
      "context_after": "Proof of Lemma 4.7: From Lemma 3.1, we know that there exist a matrix $W _ { A } \\in \\mathbb { R } ^ { n \\times d }$ such that $W _ { A } ^ { T } W _ { A } = I$ and $\\widehat { A } = A W _ { A } W _ { A } ^ { T }$ . Considering this and the fact that $V ^ { T } V = I$\n\n$$ \\begin{array}{l} \\operatorname {l o s s} (A, A V V ^ {T}) = \\| A - A V V ^ {T} \\| _ {F} ^ {2} - \\| A - A W _ {A} W _ {A} ^ {T} \\| _ {F} ^ {2} \\\\ = \\sum_ {i} \\| A _ {i} - A _ {i} V V ^ {T} \\| ^ {2} - \\| A _ {i} - A _ {i} W _ {A} W _ {A} ^ {T} \\| ^ {2} \\\\ = \\sum_ {i} A _ {i} A _ {i} ^ {T} - A _ {i} V V ^ {T} A _ {i} ^ {T} - \\left(\\sum_ {i} A _ {i} A _ {i} ^ {T} - \\sum_ {i} A _ {i} W _ {A} W _ {A} ^ {T}\\right) \\\\ = \\sum_ {i} A _ {i} W _ {A} W _ {A} ^ {T} A _ {i} ^ {T} - \\sum_ {i} A _ {i} V V ^ {T} A _ {i} ^ {T} \\\\ \\end{array} $$\n\n$$ \\begin{array}{l} \\sum_ {i} A _ {i} W _ {A} W _ {A} ^ {T} A _ {i} ^ {T} = \\sum_ {i} \\| A _ {i} W _ {A} \\| ^ {2} = \\| A W _ {A} \\| _ {F} ^ {2} = \\| A W _ {A} W _ {A} ^ {T} \\| _ {F} ^ {2} = \\| \\widehat {A} \\| _ {F} ^ {2} \\\\ \\sum_ {i} A _ {i} V V ^ {T} A _ {i} ^ {T} = \\sum_ {i} \\| A _ {i} V \\| ^ {2} = \\| A V \\| _ {F} ^ {2} = \\sum_ {i} \\| A v _ {i} \\| ^ {2} \\\\ \\sum_ {i} A _ {i} V V ^ {T} A _ {i} ^ {T} = \\sum_ {i} \\| A _ {i} V \\| ^ {2} = \\| A V \\| _ {F} ^ {2} = \\operatorname {T r} (V ^ {T} A ^ {T} A V) = \\operatorname {T r} (V V ^ {T} A ^ {T} A) = \\langle A ^ {T} A, V V ^ {T} \\rangle \\\\ \\end{array} $$",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig11.jpg",
      "image_filename": "1811.00103_page0_fig11.jpg",
      "caption": "Proof of Lemma 4.8:",
      "context_before": "$$ \\begin{array}{l} \\sum_ {i} A _ {i} W _ {A} W _ {A} ^ {T} A _ {i} ^ {T} = \\sum_ {i} \\| A _ {i} W _ {A} \\| ^ {2} = \\| A W _ {A} \\| _ {F} ^ {2} = \\| A W _ {A} W _ {A} ^ {T} \\| _ {F} ^ {2} = \\| \\widehat {A} \\| _ {F} ^ {2} \\\\ \\sum_ {i} A _ {i} V V ^ {T} A _ {i} ^ {T} = \\sum_ {i} \\| A _ {i} V \\| ^ {2} = \\| A V \\| _ {F} ^ {2} = \\sum_ {i} \\| A v _ {i} \\| ^ {2} \\\\ \\sum_ {i} A _ {i} V V ^ {T} A _ {i} ^ {T} = \\sum_ {i} \\| A _ {i} V \\| ^ {2} = \\| A V \\| _ {F} ^ {2} = \\operatorname {T r} (V ^ {T} A ^ {T} A V) = \\operatorname {T r} (V V ^ {T} A ^ {T} A) = \\langle A ^ {T} A, V V ^ {T} \\rangle \\\\ \\end{array} $$\n\nTherefore $\\begin{array} { r } { l o s s ( \\boldsymbol { A } , \\boldsymbol { A } \\boldsymbol { V } \\boldsymbol { V } ^ { T } ) = \\| \\widehat { \\boldsymbol { A } } \\| _ { F } ^ { 2 } - \\sum _ { i = 1 } ^ { d } \\| \\boldsymbol { A } \\boldsymbol { v } _ { i } \\| ^ { 2 } = \\| \\widehat { \\boldsymbol { A } } \\| _ { F } ^ { 2 } - \\langle \\boldsymbol { A } ^ { T } \\boldsymbol { A } , \\boldsymbol { V } \\boldsymbol { V } ^ { T } \\rangle . } \\end{array}$\n\n$$ \\begin{array}{l} \\| A - A V V ^ {T} \\| _ {F} ^ {2} = \\sum_ {i} \\| A _ {i} - A _ {i} V V ^ {T} \\| ^ {2} = \\sum_ {i} A _ {i} A _ {i} ^ {T} - \\sum_ {i} A _ {i} V V ^ {T} A _ {i} ^ {T} \\\\ = \\| A \\| _ {F} ^ {2} - \\sum_ {i} \\| A v _ {i} \\| ^ {2} = \\| A \\| _ {F} ^ {2} - \\| A V \\| _ {F} ^ {2} \\\\ \\end{array} $$",
      "context_after": "We prove that the value of function $g _ { A }$ at its local minima is equal to its value at its global minimum, which we know is the subspace spanned by a top $d$ eigenvectors of $A ^ { T } A$ . More precisely, we prove the following: Let $\\{ v _ { 1 } , \\ldots , v _ { n } \\}$ be an orthonormal basis of eigenvectors of $A ^ { T } A$ with corresponding eigenvalues $\\lambda _ { 1 } \\geq \\lambda _ { 2 } \\geq . . . \\geq \\lambda _ { n }$ where ties are broken arbitrarily. Let $V ^ { * }$ be the subspace spanned by $\\{ v _ { 1 } , \\ldots , v _ { d } \\}$ and let $U$ be some $d$ -dimensional subspace s.t. $g _ { A } ( U ) > g _ { A } ( V ^ { * } )$ . There is a continuous path from $U$ to $V ^ { * }$ s.t. the value of $g _ { A }$ is monotonically decreasing for every $d$ -dimensional subspace on the path.\n\nBefore starting the proof, we will make a couple of notes which would be used throughout the proof. First note that $g _ { A } ( V )$ is well-defined i.e., the value of $g _ { A } ( V )$ is only a function of the subspace $V$ . More precisely, ${ \\dot { g _ { A } } } ( V )$ is invariant with respect to different choices of orthonormal basis of the subspace $V$ . Second, given Lemma 4.7, $g _ { A } ( \\dot { V } ) = \\| A \\| _ { F } ^ { 2 } - \\textstyle \\sum _ { i } \\| A v _ { i } \\| ^ { 2 }$ . Therefore, proving that $g _ { A } ( V )$ is decreasing is equivalent to proving that $\\textstyle \\sum _ { i } \\| A v _ { i } \\| ^ { 2 }$ is increasing as a function of any choice of orthonormal basis of the subspaces on the path.\n\n$g _ { A } ( U ) > g _ { A } ( V ^ { * } )$ therefore $U \\neq V ^ { * }$ . Let $k$ be the smallest index such that $v _ { k } \\notin U$ . Extend $\\{ v _ { 1 } , \\dotsc , v _ { k - 1 } \\}$ to an orthonormal basis of $U$ : $\\{ v _ { 1 } , \\ldots , v _ { k - 1 } , v _ { k } ^ { \\prime } , \\ldots , v _ { d } ^ { \\prime } \\}$ . Let $q \\geq k$ be the smallest index such that $\\| A v _ { q } \\| ^ { 2 } > \\| A v _ { q } ^ { \\prime } \\| ^ { 2 }$ . Such an index $q$ must exist given that $g _ { A } ( U ) > g _ { A } ( V ^ { * } )$ . Without loss of generality we can assume that $q = 1$ (this will be clear throughout the proof). Therefore, we assume that $v _ { 1 }$ , the top eigenvector of $A ^ { T } A$ , is not in $U$ and that it strictly maximizes the function $\\| A u \\| ^ { 2 }$ over the space of unit vectors $u$ . Specifically, for any unit vector $u \\in U$ , $\\| A u \\| ^ { 2 } < \\| A \\ddot { v } _ { 1 } \\| ^ { 2 } = \\lambda _ { 1 }$ .",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.00103_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1811.03654": [
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg",
      "image_filename": "1811.03654_page0_fig0.jpg",
      "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .",
      "context_before": "Hypothesis 2. Participants perceive the “Equal” decision as more fair than the “All A” decision in Treatment 1. That\n\nis, participants may view the candidates in Treatment 1 as “similar enough” to be treated similarly.\n\nHypothesis 3. Participants perceive the “All A” decision as more fair than the “Equal” decision in Treatments 3 and 4.",
      "context_after": "First, we tested hypotheses H1A and H1B, which conjecture that participants will consider the “Ratio” decision as the most fair. We found evidence in support of H1A in all treatments: participants consistently rated dividing the $\\$ 50,000$ between the two individuals in proportion of their loan repayment rates (the “Ratio” decision) as more fair than splitting the $\\$ 50,000$ equally (the “Equal” decision) (see Figure 1). We found partial support for H1B: participants rated the “Ratio” decision as more fair than the “All A” decision in Treatments 1 and 2 (see Figure 1).\n\nSecond, we found that participants in Treatment 1 rated the “Equal” decision as more fair than the “All A” definition (see Figure 1), supporting H2. We see that when the difference in the loan repayment rates of the individuals was small $( 5 \\% )$ , participants perceived the decision to divide the money equally between the individuals as more fair than giving all the money to the individual with the higher loan repayment rate.\n\nThird, we found that participants rated the “All A” decision as more fair than the “Equal” decision in Treatment 3, but not in Treatment 4 (see Figure 1).",
      "referring_paragraphs": [
        "First, we tested hypotheses H1A and H1B, which conjecture that participants will consider the “Ratio” decision as the most fair. We found evidence in support of H1A in all treatments: participants consistently rated dividing the $\\$ 50,000$ between the two individuals in proportion of their loan repayment rates (the “Ratio” decision) as more fair than splitting the $\\$ 50,000$ equally (the “Equal” decision) (see Figure 1). We found partial support for H1B: participants rated the “Ratio” decision",
        "Second, we found that participants in Treatment 1 rated the “Equal” decision as more fair than the “All A” definition (see Figure 1), supporting H2. We see that when the difference in the loan repayment rates of the individuals was small $( 5 \\% )$ , participants perceived the decision to divide the money equally between the individuals as more fair than giving all the money to the individual with the higher loan repayment rate.",
        "Third, we found that participants rated the “All A” decision as more fair than the “Equal” decision in Treatment 3, but not in Treatment 4 (see Figure 1).",
        "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .",
        "We found evidence in support of H1A in all treatments: participants consistently rated dividing the $\\$ 50,000$ between the two individuals in proportion of their loan repayment rates (the “Ratio” decision) as more fair than splitting the $\\$ 50,000$ equally (the “Equal” decision) (see Figure 1)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg",
      "image_filename": "1811.03654_page0_fig1.jpg",
      "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
      "context_before": "We presented participants with the same scenario as in Study 1, but this time also providing the candidates’ race and gender. We held the gender of the candidates constant (both were male), and randomized race (black or white). Thus, either the white candidate had the higher loan repayment rate, or the black candidate had the higher loan repayment rate.\n\nThe question presented to the participants in Study 2 can be found in Figure Figure 5 in the appendix.\n\nWe presented the same question and choices for loan allocations, and tested the same hypotheses, as in Study 1.",
      "context_after": "We found that participants viewed the “Ratio” decision as more fair than the “Equal” decision in Treatments 2, 3, and 4, regardless of race, in support of H1A. We also found that participants viewed the “Ratio” decision as more fair than the “All A” decision in all treatments, regardless of race, thus supporting H1B. (See Figures 2 and 3.) Thus, participants in Study 2 consistently gave most support to the decision to divide the $\\$ 50,000$ between the two individuals in proportion to their loan repayment rates.\n\nFurthermore, we found that participants viewed the “Equal” decision as more fair than the “All A” decision in Treatment 1, regardless of race, in support of H2 (see Figures 2 and 3). Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2).\n\nWhen the difference between the two candidates’ repay-",
      "referring_paragraphs": [
        "Furthermore, we found that participants viewed the “Equal” decision as more fair than the “All A” decision in Treatment 1, regardless of race, in support of H2 (see Figures 2 and 3). Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2).",
        "ment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3). By contrast, when the candidate with the higher loan repayment rate was white, participants did not rate the two decisions differently (see Figure 2).",
        "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
        "Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg",
      "image_filename": "1811.03654_page0_fig2.jpg",
      "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm { ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
      "context_before": "Furthermore, we found that participants viewed the “Equal” decision as more fair than the “All A” decision in Treatment 1, regardless of race, in support of H2 (see Figures 2 and 3). Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2).\n\nWhen the difference between the two candidates’ repay-\n\nment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3). By contrast, when the candidate with the higher loan repayment rate was white, participants did not rate the two decisions differently (see Figure 2).",
      "context_after": "In Study 2, we tested whether participants’ perceptions of these three fairness definitions could be influenced by additional information regarding the candidates’ race.\n\nOur results show that participants perceived the “Ratio” decision to be more fair than the other two, hence supporting the results from Study 1 and the related discussion. These results are not dependent on the race attribute. Furthermore, regardless of race, when the difference between the loan repayment rates was small (Treatment 1), participants preferred the “Equal” decision to the “All A” decision. This supports the corresponding results from Study 1, Treatment 1, which indicate that one should account for similarity of individuals when designing fair rules.\n\nHowever, we also found evidence that race does affect participants’ perception of fairness. When the difference in",
      "referring_paragraphs": [
        "ment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3). By contrast, when the candidate with the higher loan repayment rate was white, participants did not rate the two decisions differently (see Figure 2).",
        "When the difference between the two candidates’ repay-\n\nment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3).",
        "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/969ca0f0cbe16d7e6822943a0797b2fcc1f56cb5772f9fe0c90bd2181e508976.jpg",
      "image_filename": "969ca0f0cbe16d7e6822943a0797b2fcc1f56cb5772f9fe0c90bd2181e508976.jpg",
      "caption": "To what extent do you think the following decisions are fair?",
      "context_before": "The loan offcer has decided to split the money 50/50 between the two candidates,giving $25,000 to PersonAand$25,000 toPersonB.\n\nThe loan oficer has decided to give Person A $41,666, which is proportional to that person's payback rateof10o%,and give PersonBS,33,which isproportionaltothat person'spayback rateof 20%.\n\nThere are two candidates - Person Aand Person B,they are identical in every way,except their race and loan repayment rates .Both of them have applied fora $5o,oo loan to start a business,and the loan offcer only has $50,000.",
      "context_after": "To what extent do you think the following decisions are fair? For each decision, please indicate how fair you think the decision is by dragging the slider bar to a point on the line,where 1 means \"not fair at all\",and 9 means \"completely fair\".\n\nNot fair at all Completely fair 1 2 3 4 5 6 7 8 9 The loan officer has decided to split the money 50/50 between the two candidates,giving $25,000 toPerson Aand$25,000toPersonB.\n\nThe loan officer has decided to give Person A $31,818,which is proportional to that person's payback rate of 70%,and give Person B $18,181, which is proportional to that person's payback rateof $4 0 \\%$",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_5",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg",
      "image_filename": "1811.03654_page0_fig3.jpg",
      "caption": "Study 1: Demographic information of the participants Figure 6: Age distribution of the participants in Study 1.",
      "context_before": "5. In what type of community do you live:\n\n6. What is your age?\n\n7. Which political party do you identify with?",
      "context_after": "",
      "referring_paragraphs": [
        "Study 1: Demographic information of the participants   \nFigure 6: Age distribution of the participants in Study 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_6",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig4.jpg",
      "image_filename": "1811.03654_page0_fig4.jpg",
      "caption": "Figure 7: Education of the participants in Study 1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 7: Education of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_7",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg",
      "image_filename": "1811.03654_page0_fig5.jpg",
      "caption": "Gender Figure 8: Gender breakdown of the participants in Study 1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Gender   \nFigure 8: Gender breakdown of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_8",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig6.jpg",
      "image_filename": "1811.03654_page0_fig6.jpg",
      "caption": "Figure 9: Political Affiliation of the participants in Study 1.",
      "context_before": "",
      "context_after": "Political Affiliation",
      "referring_paragraphs": [
        "Figure 9: Political Affiliation of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_9",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg",
      "image_filename": "1811.03654_page0_fig7.jpg",
      "caption": "State breakup Figure 11: Breakup by state of the participants in Study 1.",
      "context_before": "Political Affiliation",
      "context_after": "",
      "referring_paragraphs": [
        "State breakup   \nFigure 11: Breakup by state of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_10",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig8.jpg",
      "image_filename": "1811.03654_page0_fig8.jpg",
      "caption": "Figure 10: Race of the participants in Study 1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 10: Race of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_11",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg",
      "image_filename": "1811.03654_page0_fig9.jpg",
      "caption": "Residential Community Figure 12: Residential breakdown of the participants in Study 1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Residential Community   \nFigure 12: Residential breakdown of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig10.jpg",
      "image_filename": "1811.03654_page0_fig10.jpg",
      "caption": "Study 2: Demographic information of the participants Age",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_13",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg",
      "image_filename": "1811.03654_page0_fig11.jpg",
      "caption": "Figure 13: Age distribution of the participants in Study 2. Education Figure 14: Education of the participants in Study 2.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 13: Age distribution of the participants in Study 2.   \nEducation   \nFigure 14: Education of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_14",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig12.jpg",
      "image_filename": "1811.03654_page0_fig12.jpg",
      "caption": "Gender Figure 15: Gender breakdown of the participants in Study 2.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Gender   \nFigure 15: Gender breakdown of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_15",
      "figure_number": 16,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg",
      "image_filename": "1811.03654_page0_fig13.jpg",
      "caption": "Political Affiliation Figure 16: Political Affiliation of the participants in Study 2.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg",
      "image_filename": "1811.03654_page0_fig14.jpg",
      "caption": "Race",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_17",
      "figure_number": 17,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig15.jpg",
      "image_filename": "1811.03654_page0_fig15.jpg",
      "caption": "Figure 17: Race of the participants in Study 2. Residential Community Figure 18: Residential breakdown of the participants in Study 2.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 17: Race of the participants in Study 2.   \nResidential Community   \nFigure 18: Residential breakdown of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.03654_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1811.10104": [
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg",
      "image_filename": "1811.10104_page0_fig0.jpg",
      "caption": "(a) Labels on regression lines indicate which subgroup they fit.",
      "context_before": "2.1 1960s: Bias and Unfair Discrimination\n\nConcerned with the fairness of tests for black and white students, T. Anne Cleary defined a quantitative measure of test bias for the\n\narXiv:1811.10104v2 [cs.AI] 3 Dec 2018",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig1.jpg",
      "image_filename": "1811.10104_page0_fig1.jpg",
      "caption": "(b) The regression line labeled $\\pi _ { c }$ fits both subgroups separately (and hence also their union). Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.",
      "context_before": "",
      "context_after": "first time, cast in terms of a formal model for predicting educational outcomes from test scores [10, 11]:\n\nA test is biased for members of a subgroup of the population if, in the prediction of a criterion for which the test was designed, consistent nonzero errors of prediction are made for members of the subgroup. In other words, the test is biased if the criterion score predicted from the common regression line is consistently too high or too low for members of the subgroup. With this definition of bias, there may be a connotation of “unfair,\" particularly if the use of the test produces a prediction that is too low. (Emphasis added.)\n\nAccording to Cleary’s criterion, the situation depicted in Figure 1a is biased for members of subgroup $\\pi _ { 2 }$ if the regression line $\\pi _ { 1 }$ is used to predict their ability, since it underpredicts their true ability. For Cleary, the situation depicted in Figure 1b is not biased: since data from each of the subgroups produce the same regression line, that line can be used to make predictions for either group.",
      "referring_paragraphs": [
        "Work from the mid-1960s to mid-1970s can be summarized along four distinct categories: individual, non-comparative, subgroup parity, and correlation, defined in Table 1. It should be emphasized that in not all cases where a researcher defined a criterion did they also advocate for it. In particular, Darlington, Linn, Jones, and Peterson and Novick all define criteria purely for the purposes of exploring the space of concepts related to fairness. A summary of fairness technical definitions during",
        "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria.",
        "Cleary worked for Educational\n\nTable 1: Categories of Fairness Criteria   \n\n<table><tr><td>Category</td><td>Description</td></tr><tr><td>INDIVIDUAL</td><td>Fairness criterion defined purely in terms of individuals</td></tr><tr><td>NON-COMPARATIVE</td><td>Fairness criterion for each subgroup does not reference other subgroups</td></tr><tr><td>SUBGROUP PARITY</td><td>Fairness criterion defined in terms of parity of some value across subgroups</td></tr><tr><td>CORRELATION</td><td>Fairness criterion defined in terms of the correlation of the demographic variable with the model output</td></tr></table>\n\nTesting Services, and one can imagine a test being designed allowing for a range of use cases, since it may not be knowable in advance either i) the precise populations on which it will be deployed, nor ii) the number of students which an institution deploying the test is able to offer places to."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg",
      "image_filename": "1811.10104_page0_fig2.jpg",
      "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)",
      "context_before": "Almost as an aside, Thorndike mentions the existence of another regression line ignored by Cleary: the line that estimates the value of the test score $R$ given the target variable . This idea hints at the notion of equal opportunity for those with a given value of , an idea which soon was picked up by Darlington [19] and Cole [14].\n\nAt a glance, Cleary’s and Thorndike’s definitions are difficult to compare directly because of the different ways in which they’re defined. Darlington [19] helped to shed light on the relationship between Cleary and Thorndike’s conceptions of fairness by expressing them in a common formalism. He defines four fairness criteria in terms of the correlation $\\rho _ { A R }$ between the demographic variable ρand the test score. Following Darlington,\n\nDarlington’s mapping of Cleary’s and Thorndike’s criteria lets him prove that they’re incompatible except in the special cases",
      "context_after": "where the test perfectly predicts the target variable $\\begin{array} { r } { ( \\rho _ { R Y } = 1 ) } \\end{array}$ ), or ρwhere the target variable is uncorrelated with the demographic variable $\\left( \\rho _ { A Y } = 0 \\right.$ ). Figure 2, reproduced from Darlington’s 1971 work, shows that, for any given non-zero correlation between the demographic and target variables, definitions (1), (2), and (3) converge as the correlation between the test score and the target variable approach 1. When the test has only a poor correlation with the target variable, there may be no fair solution using definition (1).\n\nFigure 2 enables a range of further observations. According to definition (1), for a given correlation between demographic and target variables, the lower the correlation of the test with the target variable, the higher it is allowed to correlate with the demographic variable and still be considered fair. Definition (3), on the other hand, is the opposite, in that the lower the correlation of the test with the target variable, the lower too must be the the test’s correlation with the demographic variable. Darlington’s criterion (2) is the geometric mean of criteria (1) and (3): “a compromise position midway between [the] two... however, a compromise may end up satisfying nobody; psychometricians are not in the habit of agreeing on important definitions or theorems by compromise.” Darlington shows that definition (3) is the only one of the four whose errors are uncorrelated with the demographic variable, where by “errors”, he means errors in the regression task of estimating from .\n\nY RIn 1973, Cole [14] continued exploring ideas of equal outcomes across subgroups, defining fairness as all subgroups having the same True Positive Rate (TPR), recognizable as modern day equality of opportunity [32]. That same year, Linn [43] introduced (but did not advocate for) equal Positive Predictive Value (PPV) as a fairness criterion, recognizable as modern day predictive parity [9].4",
      "referring_paragraphs": [
        "where the test perfectly predicts the target variable $\\begin{array} { r } { ( \\rho _ { R Y } = 1 ) } \\end{array}$ ), or ρwhere the target variable is uncorrelated with the demographic variable $\\left( \\rho _ { A Y } = 0 \\right.$ ). Figure 2, reproduced from Darlington’s 1971 work, shows that, for any given non-zero correlation between the demographic and target variables, definitions (1), (2), and (3) converge as the correlation between the test score and the target variable approach 1. When th",
        "Figure 2 enables a range of further observations. According to definition (1), for a given correlation between demographic and target variables, the lower the correlation of the test with the target variable, the higher it is allowed to correlate with the demographic variable and still be considered fair. Definition (3), on the other hand, is the opposite, in that the lower the correlation of the test with the target variable, the lower too must be the the test’s correlation with the demographic",
        "Work from the mid-1960s to mid-1970s can be summarized along four distinct categories: individual, non-comparative, subgroup parity, and correlation, defined in Table 1. It should be emphasized that in not all cases where a researcher defined a criterion did they also advocate for it. In particular, Darlington, Linn, Jones, and Peterson and Novick all define criteria purely for the purposes of exploring the space of concepts related to fairness. A summary of fairness technical definitions during",
        "This appendix provides some details of fairness definitions included in Table 2 that were not introduced in the text of Section 2.",
        "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4).",
        "With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial\n\nTable 2: Early technical definitions of fairness in educational and employment testing."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/0698761ea2977c71bea3305d5cf1cc2e612919a6d2d4c21609f219c267daa172.jpg",
      "image_filename": "0698761ea2977c71bea3305d5cf1cc2e612919a6d2d4c21609f219c267daa172.jpg",
      "caption": "Table 1: Categories of Fairness Criteria",
      "context_before": "[Section: FAT* ’19, January 29–31, 2019, Atlanta, GA, USA]\n\n2Although Darlington does not mention this additional constraint, we believe the criterion only holds if $A$ , $R$ and $Y$ have a multivariate normal distribution. 3See footnote 2\n\n4Although he cites [30] and [24], a seeming misattribution, as pointed out by [52].",
      "context_after": "Testing Services, and one can imagine a test being designed allowing for a range of use cases, since it may not be knowable in advance either i) the precise populations on which it will be deployed, nor ii) the number of students which an institution deploying the test is able to offer places to.\n\nBy March 1976, the interest in fairness in the educational testing community was so strong that an entire issue of the Journal of Education Measurement was devoted to the topic [47], including a lengthy lead article by Peterson and Novick [52], in which they consider for the first time the equality of True Negative Rates (TNR) across subgroups, and equal TPR / equal TNR across subgroups (modern day equalized odds [32]). Similarly, they consider the case of equal PPV and equal NPV across subgroups.\n\nWork from the mid-1960s to mid-1970s can be summarized along four distinct categories: individual, non-comparative, subgroup parity, and correlation, defined in Table 1. It should be emphasized that in not all cases where a researcher defined a criterion did they also advocate for it. In particular, Darlington, Linn, Jones, and Peterson and Novick all define criteria purely for the purposes of exploring the space of concepts related to fairness. A summary of fairness technical definitions during this time is listed in Table 2.",
      "referring_paragraphs": [
        "Work from the mid-1960s to mid-1970s can be summarized along four distinct categories: individual, non-comparative, subgroup parity, and correlation, defined in Table 1. It should be emphasized that in not all cases where a researcher defined a criterion did they also advocate for it. In particular, Darlington, Linn, Jones, and Peterson and Novick all define criteria purely for the purposes of exploring the space of concepts related to fairness. A summary of fairness technical definitions during",
        "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria.",
        "Cleary worked for Educational\n\nTable 1: Categories of Fairness Criteria   \n\n<table><tr><td>Category</td><td>Description</td></tr><tr><td>INDIVIDUAL</td><td>Fairness criterion defined purely in terms of individuals</td></tr><tr><td>NON-COMPARATIVE</td><td>Fairness criterion for each subgroup does not reference other subgroups</td></tr><tr><td>SUBGROUP PARITY</td><td>Fairness criterion defined in terms of parity of some value across subgroups</td></tr><tr><td>CORRELATION</td><td>Fairness criterion defined in terms of the correlation of the demographic variable with the model output</td></tr></table>\n\nTesting Services, and one can imagine a test being designed allowing for a range of use cases, since it may not be knowable in advance either i) the precise populations on which it will be deployed, nor ii) the number of students which an institution deploying the test is able to offer places to."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/bd2432f7a6d167e71a5922ee25d4264afae2a130f6caf41927bc9754322e7607.jpg",
      "image_filename": "bd2432f7a6d167e71a5922ee25d4264afae2a130f6caf41927bc9754322e7607.jpg",
      "caption": "Table 2: Early technical definitions of fairness in educational and employment testing. Variables: is the test score; is the R Ytarget variable; is the demographic variable. The Proposition column indicates whether fairness is considered a property of Athe way in which a test is used, or of the test itself. † indicates that the criterion is discussed in the appendix.",
      "context_before": "[Section: FAT* ’19, January 29–31, 2019, Atlanta, GA, USA]\n\n[Section: Ben Hutchinson and Margaret Mitchell]\n\n5 They do not advocate for either combination (neither equal TPR and TNR, nor equal PPV and NPV) on the grounds that either combination requires unusual circumstances. However there is a flaw in their reasoning. For example, arguing against equal TPR and equal TNR, they claim that this requires equal base rates in the ground truth in addition to equal TPR.",
      "context_after": "Bias in Mental Testing [36]. Political opponents of group-based considerations in educational and employment practices framed them in terms of “preferential treatment” for minorities and “reverse discrimination” against whites. Despite, or perhaps because of, much public debate, neither Congress nor the courts gave unambiguous answers to the question of how to balance social justice considerations with the historical and legal importance placed on the individual in the United States [18].\n\nInto the 1980s, courts were asked to rule on many cases involving (un)fairness in educational testing. To give just one example, Zwick and Dorans [71] described the case of Debra P. v. Turlington 1984, in which a lawsuit was filed on behalf of “present and future twelfth grade students who had failed or would fail” a high school\n\ngraduation test. The initial ruling found that the test perpetuated past discrimination and was in violation of the Civil Rights Act. More examples of court rulings on fairness are given by [53, 71].",
      "referring_paragraphs": [
        "where the test perfectly predicts the target variable $\\begin{array} { r } { ( \\rho _ { R Y } = 1 ) } \\end{array}$ ), or ρwhere the target variable is uncorrelated with the demographic variable $\\left( \\rho _ { A Y } = 0 \\right.$ ). Figure 2, reproduced from Darlington’s 1971 work, shows that, for any given non-zero correlation between the demographic and target variables, definitions (1), (2), and (3) converge as the correlation between the test score and the target variable approach 1. When th",
        "Figure 2 enables a range of further observations. According to definition (1), for a given correlation between demographic and target variables, the lower the correlation of the test with the target variable, the higher it is allowed to correlate with the demographic variable and still be considered fair. Definition (3), on the other hand, is the opposite, in that the lower the correlation of the test with the target variable, the lower too must be the the test’s correlation with the demographic",
        "Work from the mid-1960s to mid-1970s can be summarized along four distinct categories: individual, non-comparative, subgroup parity, and correlation, defined in Table 1. It should be emphasized that in not all cases where a researcher defined a criterion did they also advocate for it. In particular, Darlington, Linn, Jones, and Peterson and Novick all define criteria purely for the purposes of exploring the space of concepts related to fairness. A summary of fairness technical definitions during",
        "This appendix provides some details of fairness definitions included in Table 2 that were not introduced in the text of Section 2.",
        "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4).",
        "With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial\n\nTable 2: Early technical definitions of fairness in educational and employment testing."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg",
      "image_filename": "1811.10104_page0_fig3.jpg",
      "caption": "Figure 3: Original graph from [22] illustrating DIF.",
      "context_before": "By the early 1980s, ideas about fairness were having a widespread influence on U.S. employment practices. In 1981, with no public debate, the United States Employment Services implemented scoreadjustment strategy that was sometimes called “race-norming” [54]. Each individual is assigned a percentile ranking within their own ethnic group, rather than to the test-taking population. By the mid-1980s, race-norming was “a highly controversial issue sparking heated debate.” The debate was settled through legislation, with the 1991 Civil Rights Act banning the practice of race-norming [65].\n\n[Section: 50 Years of Test (Un)fairness: Lessons for Machine Learning]\n\n[Section: FAT* ’19, January 29–31, 2019, Atlanta, GA, USA]",
      "context_after": "3 CONNECTIONS TO ML FAIRNESS\n\n3.1 Equivalent Notions\n\nMany of the fairness criteria we have overviewed are identical to modern-day fairness definitions. Here is a brief summary of these connections:",
      "referring_paragraphs": [
        "Figure 3 illustrates DIF for a test item.",
        "As briefly mentioned above, modern day ML fairness has categorized fairness definitions in terms of independence of variables, which includes sufficiency and separation [4]. Some historical notions of fairness neatly fit into this categorization, but others shed light on further dimensions of fairness criteria. Table 3 summarizes these connections, linking the historical criteria introduced in Section 2 to modern day categories. (Utility-based criteria are omitted, but will be discussed below.)",
        "Figure 3: Original graph from [22] illustrating DIF.",
        "Table 3 summarizes these connections, linking the historical criteria introduced in Section 2 to modern day categories."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/372adf707d4590541182fdf2471dc2b00bcff21d90d3f23809c6c0ca7d50e5b6.jpg",
      "image_filename": "372adf707d4590541182fdf2471dc2b00bcff21d90d3f23809c6c0ca7d50e5b6.jpg",
      "caption": "Table 3: Relationships between testing criteria and ML’s independence criteria",
      "context_before": "For a binary classifier, Thorndike’s 1971 group parity criterion is equivalent to requiring that the ratio of positive predictions to ground truth positives be equal for all subgroups. This ratio has no common name that we could find (unlike e.g., precision, recall, etc.), although [52] refer to this as the “Constant Ratio Model”. It is closely related to coverage constraints [29], class mass normalization [70] and expectation regularization [45]. Similar arguments can be made for Darlington’s criterion (2) and Jones’ criteria “at position $n ^ { \\dag }$ and\n\n[Section: FAT* ’19, January 29–31, 2019, Atlanta, GA, USA]\n\n[Section: Ben Hutchinson and Margaret Mitchell]",
      "context_after": "“general criterion”. When viewed as a model of subgroup quotas [34], Thorndike’s criterion is reminiscent of fair division in economics.\n\n3.3 Regression and Correlation\n\nIn reviewing the history of fairness in testing, it becomes clear that regression models have played a much larger role than in the ML community. Similarly, the use of correlation as a fairness criterion is all but absent in modern ML Fairness literature.",
      "referring_paragraphs": [
        "Figure 3 illustrates DIF for a test item.",
        "As briefly mentioned above, modern day ML fairness has categorized fairness definitions in terms of independence of variables, which includes sufficiency and separation [4]. Some historical notions of fairness neatly fit into this categorization, but others shed light on further dimensions of fairness criteria. Table 3 summarizes these connections, linking the historical criteria introduced in Section 2 to modern day categories. (Utility-based criteria are omitted, but will be discussed below.)",
        "Figure 3: Original graph from [22] illustrating DIF.",
        "Table 3 summarizes these connections, linking the historical criteria introduced in Section 2 to modern day categories."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1811.10104_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1901.10436": [
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/24131125b9e54268b58ec223258e75225a06003daa5109a98537d788449e35b8.jpg",
      "image_filename": "24131125b9e54268b58ec223258e75225a06003daa5109a98537d788449e35b8.jpg",
      "caption": "Table 1: Summary of the ten facial coding schemes used in the $D i F$ data set and their references.",
      "context_before": "In this paper, we describe the development and analysis of the $D i F$ data set. The paper is organized as follows: in Section 2, we review the state of face recognition technology and examine how different face image data sets are used today. We discuss some of the shortcomings from over-reliance on narrow data sets. In Section 3, we describe the process for creating the $D i F$ data set. In Section 4, we describe the implementation of the ten facial coding schemes. In Section 5, we provide a statistical analysis of the coding schemes extracted for the face images. In Section 6, we summarize and discuss future directions.\n\nFace recognition is a long-standing topic in computer vision, and AI broadly. Computer-based face recognition was addressed as far back as the 1970s with Takeo Kanade‘s seminal thesis on recognizing faces using a set of manually defined points corresponding to nose, mouth, eyes and other features. Modest by today’s standards, his work processed 800 photographs and conducted experiments involving identification of 20 people [11]. Two decades later, a significant development came from Matthew Turk and Alex Pentland, who developed an appearance-based technique called eigenfaces that models faces holistically from image data [12]. This kind of data-driven methodology was subsequently helped by numerous efforts of curating large and growing face image data sets. The community has built open evaluations around these data sets, such as MegaFace [13], MS-Celeb [14] and the NIST Face Recognition Vendor Test (FRVT)1.\n\n1https://www.nist.gov/programs-projects/face-recognition-vendor-test-frvt",
      "context_after": "One prominent example of an early face data set and open evaluation is Labeled Faces in the Wild (LFW), which is comprised of 13, 233 face photos from the Web of 5, 749 individuals, mostly celebrities and public figures, captured in unconstrained conditions of lighting, pose and expression [15]. LFW gained significant focus from the research community upon its release. Eventually, with the advent of deep learning techniques [16, 17], face recognition performance on LFW reached near-perfect results with 99.8% accuracy [18,19]. Megaface defined a follow-on larger data set comprised of faces from 690, 572 unique individuals which was made more difficult with the addition of 1 million face image distractors [13]. Although early results produced low accuracy in the range of $5 0 \\% - 6 0 \\%$ , ultimately, performance reached near-perfect levels of 99.9% [20, 21]. Other data sets and evaluations such as CelebA have brought focus to a wider set of problems in face recognition such as face attribute recognition. CelebA provides a data set of 202, 599 face images with annotations of 40 attributes such as ‘eyeglasses,’ ‘smiling,’ and ‘mustache’ [9]. State-of-art systems have achieved greater than 90% mean accuracy across the CelebA attribute set and as high as 99% for some attributes. Many other face data sets and evaluations have produced similar improvements using deep learning methods [8,9,14,22–34]. The healthy progress on face recognition, as measured on these data sets and evaluations, has raised expectations in the technology.",
      "referring_paragraphs": [
        "Diversity in Faces $( D i F )$ is a new large and diverse data set designed to advance the study of fairness and accuracy in face recognition technology. $D i F$ provides a data set of annotations of one million face images. The $D i F$ annotations are made on faces sampled from the publicly available YFCC-100M data set of 100 million images [1]. The $D i F$ data set provides a comprehensive set of annotations of intrinsic facial features that includes craniofacial distances, areas and ratios, f",
        "The YFCC-100M data set gives a set of URLs that point to the Flickr web page for each of the photos. The first step we took was to check whether the URL was still active. If so, we then checked the license. We proceeded with the download only if the license type was Creative Commons. Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object detector trained for faces based on Fast",
        "The facial coding schemes, summarized in Table 1, are among the strongest identified in the scientific literature and build a solid foundation to our collective knowledge.",
        "Table 1: Summary of the ten facial coding schemes used in the $D i F$ data set and their references.",
        "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61].",
        "The overall process is shown in Figure 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/2d373bb050fcf942e384b7500c4e8d75245e6ded364777938ae761aaa4fd5c3f.jpg",
      "image_filename": "2d373bb050fcf942e384b7500c4e8d75245e6ded364777938ae761aaa4fd5c3f.jpg",
      "caption": "Table 2: Distribution of age groups for seven prominent face image data sets.",
      "context_before": "One prominent example of an early face data set and open evaluation is Labeled Faces in the Wild (LFW), which is comprised of 13, 233 face photos from the Web of 5, 749 individuals, mostly celebrities and public figures, captured in unconstrained conditions of lighting, pose and expression [15]. LFW gained significant focus from the research community upon its release. Eventually, with the advent of deep learning techniques [16, 17], face recognition performance on LFW reached near-perfect results with 99.8% accuracy [18,19]. Megaface defined a follow-on larger data set comprised of faces from 690, 572 unique individuals which was made more difficult with the addition of 1 million face image distractors [13]. Although early results produced low accuracy in the range of $5 0 \\% - 6 0 \\%$ , ultimately, performance reached near-perfect levels of 99.9% [20, 21]. Other data sets and evaluations such as CelebA have brought focus to a wider set of problems in face recognition such as face attribute recognition. CelebA provides a data set of 202, 599 face images with annotations of 40 attributes such as ‘eyeglasses,’ ‘smiling,’ and ‘mustache’ [9]. State-of-art systems have achieved greater than 90% mean accuracy across the CelebA attribute set and as high as 99% for some attributes. Many other face data sets and evaluations have produced similar improvements using deep learning methods [8,9,14,22–34]. The healthy progress on face recognition, as measured on these data sets and evaluations, has raised expectations in the technology.",
      "context_after": "However, high accuracy on these data sets does not readily translate into equivalent accuracy in deployments [36, 37]. The reason is that different or broader distributions of faces, as well as varied environmental conditions, are found in real applications. Face recognition systems that are trained within only a narrow context of a specific data set will inevitably acquire bias that skews learning towards the specific characteristics of the data set. This narrow context appears as underrepresentation or over-representation of certain types of faces in many of the publicly available data sets. Table 2 shows some of the big differences in distribution of age groups for seven prominent face image data sets. Generally, there is a skew away from younger and older ages. Some of the differences are quite dramatic. For example, 36.5% of faces in IMDB-Face are for individuals 20-30 years of age, whereas IJB-C has 16.2% of faces in this age group.",
      "referring_paragraphs": [
        "However, high accuracy on these data sets does not readily translate into equivalent accuracy in deployments [36, 37]. The reason is that different or broader distributions of faces, as well as varied environmental conditions, are found in real applications. Face recognition systems that are trained within only a narrow context of a specific data set will inevitably acquire bias that skews learning towards the specific characteristics of the data set. This narrow context appears as underrepresen",
        "In order to extract the 19 facial landmark points, we leveraged standard DLIB facial key-point extraction tools that provide a set 68 key-points for each face. As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2]. These 19 landmarks were used for extracting the craniofacial features. Note that for illustrative purposes, the example face used in Figure 2 was adopted from [66] and was generated synthetically using a progressive Generative Adversarial Network (GAN) ",
        "Table 2: Distribution of age groups for seven prominent face image data sets.",
        "As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2].",
        "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/325cd745dbcf6c9a5703f83fb80fcb3d4bb323f231d01aa3cde28e882f53c343.jpg",
      "image_filename": "325cd745dbcf6c9a5703f83fb80fcb3d4bb323f231d01aa3cde28e882f53c343.jpg",
      "caption": "Table 3: Distribution of gender and skin color/type for seven prominent face image data sets.",
      "context_before": "However, high accuracy on these data sets does not readily translate into equivalent accuracy in deployments [36, 37]. The reason is that different or broader distributions of faces, as well as varied environmental conditions, are found in real applications. Face recognition systems that are trained within only a narrow context of a specific data set will inevitably acquire bias that skews learning towards the specific characteristics of the data set. This narrow context appears as underrepresentation or over-representation of certain types of faces in many of the publicly available data sets. Table 2 shows some of the big differences in distribution of age groups for seven prominent face image data sets. Generally, there is a skew away from younger and older ages. Some of the differences are quite dramatic. For example, 36.5% of faces in IMDB-Face are for individuals 20-30 years of age, whereas IJB-C has 16.2% of faces in this age group.",
      "context_after": "Similarly, Table 3 shows the distribution of gender and skin color/type for eight face image data sets. LFW is highly skewed towards male faces with $7 7 . 4 \\%$ corresponding to male. Six of the eight data sets have more male faces. A similar skew is seen with skin color/type when grouped coarsely into darker and lighter groups. Note that different methods were used for characterizing skin color/type in Table 3, and the meaning of darker and lighter is not the same across these eight data sets. For all but two data sets the distribution shows $> 8 0 \\%$ faces that are lighter. AgeDb is the most heavily skewed, with 94.6% faces that are lighter. The Pilot Parliaments Benchmark (PPB) data set was designed to be balanced for gender and skin type, where a board certified dermatologist provided the ground-truth labels using the Fitzpatrick six-point system [36, 38]. However, the age distribution in PPB is skewed, having been built from official photos of members of parliaments, all adults, from six countries. Face recognition systems developed from skewed training data are bound\n\nto produce biased models. This mismatch has been evidenced in the significant drop in performance for different groupings of faces [39–41]. A published study showed that gender estimation from face images is biased against dark-skinned females over white-skinned males [36, 37]. Such biases may have serious impacts in practice. Yet much of the prior research on face recognition does not take these issues under consideration, having focused strongly on driving up accuracy on narrow data sets. Note also that the gender categorization in Table 3, as in much of the prior work, uses a binary system for gender classification that corresponds to biological sex – male and female. However, different interpretations of gender in practice can include biological gender, psychological gender and social gender roles. As with race and ethnicity, over-simplification of gender by imposing an incomplete system of categorization can result in face recognition technologies that do not work fairly for all of us. Some recent efforts, such as InclusiveFaceNet [42], show that imperfect categorization of race and gender can help with face attribute recognition. However, we expect that more nuanced treatment of race, ethnicity and gender is important towards improving diversity in face data sets.\n\n2.1 Bias and Fairness",
      "referring_paragraphs": [
        "Similarly, Table 3 shows the distribution of gender and skin color/type for eight face image data sets. LFW is highly skewed towards male faces with $7 7 . 4 \\%$ corresponding to male. Six of the eight data sets have more male faces. A similar skew is seen with skin color/type when grouped coarsely into darker and lighter groups. Note that different methods were used for characterizing skin color/type in Table 3, and the meaning of darker and lighter is not the same across these eight data sets.",
        "to produce biased models. This mismatch has been evidenced in the significant drop in performance for different groupings of faces [39–41]. A published study showed that gender estimation from face images is biased against dark-skinned females over white-skinned males [36, 37]. Such biases may have serious impacts in practice. Yet much of the prior research on face recognition does not take these issues under consideration, having focused strongly on driving up accuracy on narrow data sets. Note",
        "Facial symmetry has been found in psychology and anthropology studies to be correlated with subjective and objective traits including expression variation [67] and attractiveness [5]. We adopted facial symmetry for coding scheme 4, given its intrinsic nature. To represent the symmetry of each face we computed two measures, following the work of Liu et al. [67]. We processed each face as shown in Figure 3. We used three of the DLIB key-points detected in the face image to spatially normalize and ",
        "Table 3: Distribution of gender and skin color/type for seven prominent face image data sets.",
        "We processed each face as shown in Figure 3.",
        "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ )."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/7633cd52a33708a661c8b2d7491c9a25d2b81e95ba8d1d702f3c3d28cd3fe591.jpg",
      "image_filename": "7633cd52a33708a661c8b2d7491c9a25d2b81e95ba8d1d702f3c3d28cd3fe591.jpg",
      "caption": "3 $D i F$ Data Set Construction",
      "context_before": "2.1 Bias and Fairness\n\nThe study of bias and fairness has recently gained broad interest in computer vision and machine learning [43–45]. Torralba and Efros [46] presented an evaluation of metrics related to bias and framed bias in visual classification as a domain transfer problem. Tommasi et al. [47] and Hoffman et al. [48] conducted a similar evaluation with deep features, showing that data set bias can be reduced but not eliminated. Khosla et al. [49] proposed a method that learns bias vectors associated with individual data sets, as well as weights common across data sets, which are learned by undoing unwanted bias from each data set. Hardt et al. [50] proposed a framework for fairness called equalized odds, also referred to as disparate mistreatment [51], where the goal is to predict a true outcome based on labeled training data, while ensuring it is ‘non-discriminatory’ with respect to a chosen protected attribute. More recently, Burns et al. [45] addressed bias in image captioning, proposing a model that ensures equal gender probability when gender evidence is occluded in a scene, and otherwise predicts gender from relevant information when present. The problem of gender-neutral smile classification was addressed in [42]. Bias in face detection for skin tone, pose and expression was studied in [52]. Buolamwini et al. [36] proposed an approach to evaluate unwanted bias in face recognition and data sets with respect to phenotypic subgroups and introduced the PPB data set balanced by gender and skin type.\n\nAs described above, the last decade has seen an ever-growing collection of face recognition data sets. Table 4 summarizes many of the prominent face image data sets used for evaluating face recognition technology. Returning to Labeled Faces in the Wild (LFW) [15], it presented considerable technical challenges upon its release in 2007, whereas nearly perfect results are being attained today. Several data sets such as IJBC [53], UMD [26] and VGGFace [22, 23] provide a larger set of face images with a wider range of pose and lighting variations. Other large-scale face recognition data sets include MegaFace [13], MS-Celeb [14] and CASIA [25]. Many other data sets have been proposed for different facial analysis tasks, such as age and gender classification [24, 27, 31, 54–56], facial expression analysis [55], memorability [57], attributes [35] and aging [58]. Unlike the prior data sets, which focus on robustness under variations of pose, lighting, occlusion, and scale, the $D i F$ data set is aimed understanding diversity with respect to intrinsic facial features.",
      "context_after": "3 $D i F$ Data Set Construction\n\nGiven the above issues, we were motivated to create the $D i F$ data set to obtain a scientific and computationally practical basis for ensuring fairness and accuracy in face recognition. At one extreme the challenge of diversity could be solved by building a data set comprised from the face of every person in the world. However, this would not be practical or even possible, let alone the significant privacy concerns. For one, our facial appearances are constantly changing due to ageing, among other factors. At best this would give a solution for a point in time. Rather, a solution needs to come from obtaining or generating a representative sample of faces with sufficient coverage and balance. That, however, is also not a simple task. There are many challenging questions: what does coverage mean computationally? How should balance be measured? Are age, gender and skin color sufficient? What about other highly personal attributes that are part of our identity, such as race, ethnicity, culture, geography, or visible forms of self-expression that are reflected in our faces in a myriad of ways? We realized very quickly that until these questions were answered we could not construct a complete and balanced data set of face images.\n\nWe formulated a new approach that would help answer these questions. We designed the $D i F$ data set to provide a scientific foundation for research into facial diversity. We reviewed the scientific literature on face analysis and studied prior work in fields as diverse as psychology, sociology, dermatology, cosmetology and facial surgery. We concluded that no single facial feature or combination of commonly used classifications – such as age, gender and skin color – would suffice. Therefore, we formulated a novel multi-modal approach that incorporates a diversity of face analysis methods. Based on study of the large body of prior work, we chose to implement a solid starter-set of ten facial coding schemes. The criteria for selecting these coding schemes included several important considerations: (1) strong scientific basis as evidenced by highly cited prior work, (2) extracting the coding scheme was computationally feasible, (3) the coding scheme produced continuous valued dimensions that could feed subsequent analysis, as opposed to generating only categorical values or labels, and (4) the coding scheme would allow for human interpretation to help with our understanding.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg",
      "image_filename": "1901.10436_page0_fig0.jpg",
      "caption": "(a) face detection (size)",
      "context_before": "We formulated a new approach that would help answer these questions. We designed the $D i F$ data set to provide a scientific foundation for research into facial diversity. We reviewed the scientific literature on face analysis and studied prior work in fields as diverse as psychology, sociology, dermatology, cosmetology and facial surgery. We concluded that no single facial feature or combination of commonly used classifications – such as age, gender and skin color – would suffice. Therefore, we formulated a novel multi-modal approach that incorporates a diversity of face analysis methods. Based on study of the large body of prior work, we chose to implement a solid starter-set of ten facial coding schemes. The criteria for selecting these coding schemes included several important considerations: (1) strong scientific basis as evidenced by highly cited prior work, (2) extracting the coding scheme was computationally feasible, (3) the coding scheme produced continuous valued dimensions that could feed subsequent analysis, as opposed to generating only categorical values or labels, and (4) the coding scheme would allow for human interpretation to help with our understanding.\n\nWe chose YFCC-100M [1] to be the source for the sample of images. There were a number of important reasons for this. Ideally, we would be able to automatically obtain any large sample of images from any source meeting any characteristics of diversity we desire. However, practical considerations prevent this, including the fact that various copyright laws and privacy regulations must be respected. YFCC-100M is one of the largest image collections, consisting of more than 100 millions photos. It was populated by users of the Flickr photo service. There is a large diversity in these photos overall, where people and faces appear in an enormous number of ways. Also, importantly, a large portion of the photos have Creative Commons license. The downside of using YFCC-100M is that there is skew in the Flickr user community that contributed the photos. We cannot rely on the set of users or their photos to be inherently diverse. A consequence of this is that the set of images used in the $D i F$ is not completely balanced on its own. However, it still provides the desired basis for studying methods for characterizing facial diversity.\n\nWhile the YFCC-100M photo data set is large, not all images could be considered. Naturally, we excluded photos that did not contain a face. We also excluded black and white and grayscale photos and those with significant blur. Although face recognition needs to be robust for non-color photos, we deferred incorporating these images in the initial $D i F$ data set in order to focus on intrinsic facial variation rather than image variation due to color processing.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig1.jpg",
      "image_filename": "1901.10436_page0_fig1.jpg",
      "caption": "(b) face keypoints (pose and iod)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_7",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig2.jpg",
      "image_filename": "1901.10436_page0_fig2.jpg",
      "caption": "(c) face rectification Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).",
      "context_before": "",
      "context_after": "3.2 Pre-processing Pipeline\n\nThe YFCC-100M data set gives a set of URLs that point to the Flickr web page for each of the photos. The first step we took was to check whether the URL was still active. If so, we then checked the license. We proceeded with the download only if the license type was Creative Commons. Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object detector trained for faces based on Faster-RCNN [61]. For each detected face, we then extracted both pose and 68 face key-points using the open source DLIB toolkit [62]. If there was any failure in the image processing steps, we excluded the face from further consideration. We also removed faces of size less than $5 0 \\times 5 0$ pixels or with inter-ocular distance of less than 30 pixels. We removed faces with substantial non-frontal pose. The overall process is shown in Figure 1.\n\nFinally, we generated two instances of each face. One is a rectified instance whereby the center points of each eye are fixed to a specific location in the overall image. The second crops an expanded region surrounding each face to give 50% additional spatial context. This overall process filtered the 100 million YFCC-100M photos down to approximately one million mostly frontal faces with adequate size. The surviving face images were the ones used for the $D i F$ data set. Note that the overall process of sampling YFCC-100M used only factors described above including color, size, quality and pose. We did not bias the sampling towards intrinsic facial characteristics or by using metadata associated with each photo, such as a geo-tag, date, labels or Flickr user name. In this sense, the $D i F$ data distribution is expected to closely follow the overall distribution of the YFCC-100M photos. In future efforts to grow the $D i F$ data set, we may relax some of the constraints based on size, pose and quality, or we may bias the sampling based on other properties. However, one million publicly available face images provides a good start. Given this compiled set of faces, we next process each one by extracting the ten facial coding schemes.",
      "referring_paragraphs": [
        "Diversity in Faces $( D i F )$ is a new large and diverse data set designed to advance the study of fairness and accuracy in face recognition technology. $D i F$ provides a data set of annotations of one million face images. The $D i F$ annotations are made on faces sampled from the publicly available YFCC-100M data set of 100 million images [1]. The $D i F$ data set provides a comprehensive set of annotations of intrinsic facial features that includes craniofacial distances, areas and ratios, f",
        "The YFCC-100M data set gives a set of URLs that point to the Flickr web page for each of the photos. The first step we took was to check whether the URL was still active. If so, we then checked the license. We proceeded with the download only if the license type was Creative Commons. Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object detector trained for faces based on Fast",
        "The facial coding schemes, summarized in Table 1, are among the strongest identified in the scientific literature and build a solid foundation to our collective knowledge.",
        "Table 1: Summary of the ten facial coding schemes used in the $D i F$ data set and their references.",
        "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61].",
        "The overall process is shown in Figure 1."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/2304d2c24fec3a76315a97056fbb1e85e5a6b36c3b1bde2515452334dc555d57.jpg",
      "image_filename": "2304d2c24fec3a76315a97056fbb1e85e5a6b36c3b1bde2515452334dc555d57.jpg",
      "caption": "Table 5: Anatomical terms and corresponding abbreviations (as in [2]) for the set of facial landmarks employed to compute the craniofacial measurements for facial coding schemes 1–3.",
      "context_before": "Finally, we generated two instances of each face. One is a rectified instance whereby the center points of each eye are fixed to a specific location in the overall image. The second crops an expanded region surrounding each face to give 50% additional spatial context. This overall process filtered the 100 million YFCC-100M photos down to approximately one million mostly frontal faces with adequate size. The surviving face images were the ones used for the $D i F$ data set. Note that the overall process of sampling YFCC-100M used only factors described above including color, size, quality and pose. We did not bias the sampling towards intrinsic facial characteristics or by using metadata associated with each photo, such as a geo-tag, date, labels or Flickr user name. In this sense, the $D i F$ data distribution is expected to closely follow the overall distribution of the YFCC-100M photos. In future efforts to grow the $D i F$ data set, we may relax some of the constraints based on size, pose and quality, or we may bias the sampling based on other properties. However, one million publicly available face images provides a good start. Given this compiled set of faces, we next process each one by extracting the ten facial coding schemes.\n\n4 Facial Coding Scheme Implementation\n\nIn this Section, we describe the implementation of the ten facial coding schemes and the process of extracting them from the $D i F$ face images. The advantage of using ten coding schemes is that it gives a diversity of methods and allows us to compare statistical measures for facial diversity. As described above, the ten schemes have been selected based on their strong scientific basis, computational feasibility, numerical representation and interpretability. Overall the chosen ten coding schemes capture multiple modalities of facial features, which includes craniofacial distances, areas and ratios, facial symmetry and contrast, skin color, age and gender predictions, subjective annotations, and pose and resolution. Three of the $D i F$ facial coding schemes are based on craniofacial features. As prior work has pointed out, skin color alone is not a strong predictor of race, and other features such as facial proportions are important [6, 63–65]. Face morphology is also relevant for attributes such as age and gender [4]. We incorporated multiple facial coding schemes aimed at capturing facial morphology using craniofacial features [2–4]. The basis of craniofacial science is the measurement of the face in terms of distances, sizes and ratios between specific points such as the tip of the nose, corner of the eyes, lips, chin, and so on. Many of these measures can be reliably estimated from photos of frontal faces using 47 landmark points of the head and face [2]. To provide the basis for the three craniofacial feature coding schemes used in $D i F$ , we built on the subset of 19 facial landmarks listed in Table 5. For brevity we adopt the abbreviations from [2] when referring to these facial landmark points instead of using the full anatomical terms.",
      "context_after": "In order to extract the 19 facial landmark points, we leveraged standard DLIB facial key-point extraction tools that provide a set 68 key-points for each face. As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2]. These 19 landmarks were used for extracting the craniofacial features. Note that for illustrative purposes, the example face used in Figure 2 was adopted from [66] and was generated synthetically using a progressive Generative Adversarial Network (GAN) model. The face does not correspond to a known individual person. However, the image is subject to license terms as per [66]. In order to incorporate a diversity of approaches, we implemented three facial coding schemes for craniofacial features. The first, coding scheme 1, provides a set of craniofacial distance measures from [2]. The second, coding scheme 2, provides an expanded set of craniofacial areas from [3]. The third, coding scheme 3, provides a set of craniofacial ratios from [4].",
      "referring_paragraphs": [
        "In this Section, we describe the implementation of the ten facial coding schemes and the process of extracting them from the $D i F$ face images. The advantage of using ten coding schemes is that it gives a diversity of methods and allows us to compare statistical measures for facial diversity. As described above, the ten schemes have been selected based on their strong scientific basis, computational feasibility, numerical representation and interpretability. Overall the chosen ten coding schem",
        "The first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: ",
        "Skin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for skin color, even within the dermatology field",
        "It is important to note that that ITA is a point measurement. Hence, every pixel corresponding to skin can have an ITA measurement. In order to generate a feature measure for the whole face, we extract ITA for pixels within a masked face region as shown in Figure 5(g). This masked region is determined in the following steps:",
        "To provide the basis for the three craniofacial feature coding schemes used in $D i F$ , we built on the subset of 19 facial landmarks listed in Table 5.",
        "In referring to the implementation of the coding scheme, we use the abbreviations from Table 5.",
        "Figure 5 depicts the image processing steps for extracting the coding scheme 6 for skin color.",
        "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_9",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig3.jpg",
      "image_filename": "1901.10436_page0_fig3.jpg",
      "caption": "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.",
      "context_before": "In order to extract the 19 facial landmark points, we leveraged standard DLIB facial key-point extraction tools that provide a set 68 key-points for each face. As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2]. These 19 landmarks were used for extracting the craniofacial features. Note that for illustrative purposes, the example face used in Figure 2 was adopted from [66] and was generated synthetically using a progressive Generative Adversarial Network (GAN) model. The face does not correspond to a known individual person. However, the image is subject to license terms as per [66]. In order to incorporate a diversity of approaches, we implemented three facial coding schemes for craniofacial features. The first, coding scheme 1, provides a set of craniofacial distance measures from [2]. The second, coding scheme 2, provides an expanded set of craniofacial areas from [3]. The third, coding scheme 3, provides a set of craniofacial ratios from [4].",
      "context_after": "4.1 Coding Scheme 1: Craniofacial Distances\n\nThe first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: tn was computed as the topmost point vertically above $n$ in the rectified facial image, and sto was computed from the vertical average of $\\mathit { l s }$ and $l i$ . The eight dimensions of craniofacial distances are summarized in Table 6.\n\n4.2 Coding Scheme 2: Craniofacial Areas",
      "referring_paragraphs": [
        "However, high accuracy on these data sets does not readily translate into equivalent accuracy in deployments [36, 37]. The reason is that different or broader distributions of faces, as well as varied environmental conditions, are found in real applications. Face recognition systems that are trained within only a narrow context of a specific data set will inevitably acquire bias that skews learning towards the specific characteristics of the data set. This narrow context appears as underrepresen",
        "In order to extract the 19 facial landmark points, we leveraged standard DLIB facial key-point extraction tools that provide a set 68 key-points for each face. As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2]. These 19 landmarks were used for extracting the craniofacial features. Note that for illustrative purposes, the example face used in Figure 2 was adopted from [66] and was generated synthetically using a progressive Generative Adversarial Network (GAN) ",
        "Table 2: Distribution of age groups for seven prominent face image data sets.",
        "As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2].",
        "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_10",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/e8f943608ca263af6b73342f7e5c7e0c1d985844db5ec5722fc0f6354054c775.jpg",
      "image_filename": "e8f943608ca263af6b73342f7e5c7e0c1d985844db5ec5722fc0f6354054c775.jpg",
      "caption": "Table 6: Coding scheme 1 is made up eight craniofacial measures corresponding to different vertical distances in the face [2].",
      "context_before": "The first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: tn was computed as the topmost point vertically above $n$ in the rectified facial image, and sto was computed from the vertical average of $\\mathit { l s }$ and $l i$ . The eight dimensions of craniofacial distances are summarized in Table 6.\n\n4.2 Coding Scheme 2: Craniofacial Areas\n\nThe second coding scheme is adopted from a later development from Farkas et al. [3]. It comprises measures corresponding to different areas of the cranium. Similar to the craniofacial distances, the extraction of craniofacial areas relied on the mapped DLIB key-points to the corresponding facial landmarks. Table 7 summarizes the twelve dimensions of the craniofacial area features.",
      "context_after": "",
      "referring_paragraphs": [
        "The first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: ",
        "Shannon $H$ and Simpson $D$ are diversity measures and Shannon $E$ and Simpson $E$ are evenness measures. To see how they work, consider a 20 class problem ( $S = 2 0$ ) with uniform distribution ( $p _ { i } = 0 . 0 5 ,$ ). These measures take the following values: Shannon $H = 2 . 9 9 9$ , Shannon $E = 1 . 0$ , Simpson $D = 2 . 5 6 3$ , and Simpson $E = 1 . 0$ . Evenness is constant at 1.0 as expected. Shannon $\\boldsymbol { D }$ represents the diversity of 20 classes ( $e ^ { 2 . 9 9 9 } \\app",
        "The eight dimensions of craniofacial distances are summarized in Table 6.",
        "Table 6: Coding scheme 1 is made up eight craniofacial measures corresponding to different vertical distances in the face [2].",
        "Figure 6 illustrates these measures on two example distributions.",
        "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_11",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/53430c5d8ef60984f6124042932fa98f409f6d4e8a4a4646690456e21b792917.jpg",
      "image_filename": "53430c5d8ef60984f6124042932fa98f409f6d4e8a4a4646690456e21b792917.jpg",
      "caption": "Table 7: Coding scheme 2 is made up of twelve craniofacial measures that correspond to different areas of the face [3].",
      "context_before": "",
      "context_after": "4.3 Coding Scheme 3: Craniofacial Ratios\n\nThe third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.\n\n4.4 Coding Scheme 4: Facial Symmetry",
      "referring_paragraphs": [
        "The second coding scheme is adopted from a later development from Farkas et al. [3]. It comprises measures corresponding to different areas of the cranium. Similar to the craniofacial distances, the extraction of craniofacial areas relied on the mapped DLIB key-points to the corresponding facial landmarks. Table 7 summarizes the twelve dimensions of the craniofacial area features.",
        "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1. The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777. Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes. Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.",
        "Table 7 summarizes the twelve dimensions of the craniofacial area features.",
        "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.",
        "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_12",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/8b204503166b8698fd42b62c3bb881128a086c142049d271ade717f2d5d66970.jpg",
      "image_filename": "8b204503166b8698fd42b62c3bb881128a086c142049d271ade717f2d5d66970.jpg",
      "caption": "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].",
      "context_before": "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.\n\n4.4 Coding Scheme 4: Facial Symmetry\n\nFacial symmetry has been found in psychology and anthropology studies to be correlated with subjective and objective traits including expression variation [67] and attractiveness [5]. We adopted facial symmetry for coding scheme 4, given its intrinsic nature. To represent the symmetry of each face we computed two measures, following the work of Liu et al. [67]. We processed each face as shown in Figure 3. We used three of the DLIB key-points detected in the face image to spatially normalize and rectify it to the following locations: the inner canthus of each eye (C1 and C2) to reference locations $C 1 = ( 4 0 , 4 8 )$ , $C 2 = ( 8 8 , 4 8 )$ and the philtrum C3 was mapped to $C 3 = ( 6 4 , 8 4 )$ . Next, the face mid-line (point $b$ in Figure 3(a)) was computed as the line passing through the mid-point of the line segment connecting $C 1 - C 2$ (point $a$ in Figure 3(a)) and the philtrum C3.",
      "context_after": "",
      "referring_paragraphs": [
        "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.",
        "Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2. The highest Simpson $D$ value is 5.888 and the smallest is 5.858. The highest Shannon $H$ value is 1.782 and the lowest is 1.780. Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes. Evenness ranges between 0.981 and 0.976.",
        "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].",
        "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg",
      "image_filename": "1901.10436_page0_fig4.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig5.jpg",
      "image_filename": "1901.10436_page0_fig5.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_15",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig6.jpg",
      "image_filename": "1901.10436_page0_fig6.jpg",
      "caption": "(c) Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.",
      "context_before": "",
      "context_after": "We point out that although a face image is spatially transformed during rectification, facial symmetry with respect to the face mid-line is preserved according to the topological properties of the affine transformation [68]. Each image is then cropped to 128x128 pixels to create a squared image with the face mid-line centered vertically. Next we convert the spatially transformed image to grayscale to measure intensity. Each point $( x , y )$ on this normalized face intensity image $I$ on the left of the face mid-line has a unique corresponding horizontally mirrored point on the other side of the face image $I ^ { \\prime } ( x , y )$ (right of the mid-line). We also extract edges in this image $I$ to produce $I _ { e }$ using a Sobel filter. Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part ( $I ^ { \\prime }$ and $I _ { e } ^ { \\prime }$ ) are computed as summarized in Table 9, where $\\phi ( I _ { e } ( x , y ) , I _ { e } ^ { \\prime } ( x , y ) )$ is the angle between the two edge orientations of images $I _ { e }$ and $I _ { e } ^ { \\prime }$ at pixel $( x , y )$ . We compute the average value of $D D ( x , y )$ and $E O S ( x , y )$ to be the two measures for facial symmetry.\n\nIt is interesting to notice that the two symmetry measurements capture facial symmetry from different perspectives: density difference is affected by the left-right relative intensity variations of a face, while edge orientation similarity is affected by the zero-crossing of the intensity field. Higher values of density difference correspond to more asymmetrical faces, while the higher the values of edge orientation similarity refer to more symmetrical faces.",
      "referring_paragraphs": [
        "Similarly, Table 3 shows the distribution of gender and skin color/type for eight face image data sets. LFW is highly skewed towards male faces with $7 7 . 4 \\%$ corresponding to male. Six of the eight data sets have more male faces. A similar skew is seen with skin color/type when grouped coarsely into darker and lighter groups. Note that different methods were used for characterizing skin color/type in Table 3, and the meaning of darker and lighter is not the same across these eight data sets.",
        "to produce biased models. This mismatch has been evidenced in the significant drop in performance for different groupings of faces [39–41]. A published study showed that gender estimation from face images is biased against dark-skinned females over white-skinned males [36, 37]. Such biases may have serious impacts in practice. Yet much of the prior research on face recognition does not take these issues under consideration, having focused strongly on driving up accuracy on narrow data sets. Note",
        "Facial symmetry has been found in psychology and anthropology studies to be correlated with subjective and objective traits including expression variation [67] and attractiveness [5]. We adopted facial symmetry for coding scheme 4, given its intrinsic nature. To represent the symmetry of each face we computed two measures, following the work of Liu et al. [67]. We processed each face as shown in Figure 3. We used three of the DLIB key-points detected in the face image to spatially normalize and ",
        "Table 3: Distribution of gender and skin color/type for seven prominent face image data sets.",
        "We processed each face as shown in Figure 3.",
        "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ )."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_16",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/7ea83c5c3700a1a18a9dd4356c7adde0124baf25d9689d7577d382616d95223b.jpg",
      "image_filename": "7ea83c5c3700a1a18a9dd4356c7adde0124baf25d9689d7577d382616d95223b.jpg",
      "caption": "Table 9: Coding scheme 4 is made up of two measures of facial symmetry [3].",
      "context_before": "We point out that although a face image is spatially transformed during rectification, facial symmetry with respect to the face mid-line is preserved according to the topological properties of the affine transformation [68]. Each image is then cropped to 128x128 pixels to create a squared image with the face mid-line centered vertically. Next we convert the spatially transformed image to grayscale to measure intensity. Each point $( x , y )$ on this normalized face intensity image $I$ on the left of the face mid-line has a unique corresponding horizontally mirrored point on the other side of the face image $I ^ { \\prime } ( x , y )$ (right of the mid-line). We also extract edges in this image $I$ to produce $I _ { e }$ using a Sobel filter. Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part ( $I ^ { \\prime }$ and $I _ { e } ^ { \\prime }$ ) are computed as summarized in Table 9, where $\\phi ( I _ { e } ( x , y ) , I _ { e } ^ { \\prime } ( x , y ) )$ is the angle between the two edge orientations of images $I _ { e }$ and $I _ { e } ^ { \\prime }$ at pixel $( x , y )$ . We compute the average value of $D D ( x , y )$ and $E O S ( x , y )$ to be the two measures for facial symmetry.\n\nIt is interesting to notice that the two symmetry measurements capture facial symmetry from different perspectives: density difference is affected by the left-right relative intensity variations of a face, while edge orientation similarity is affected by the zero-crossing of the intensity field. Higher values of density difference correspond to more asymmetrical faces, while the higher the values of edge orientation similarity refer to more symmetrical faces.",
      "context_after": "4.5 Coding Scheme 5: Facial Regions Contrast\n\nPrior studies have shown that facial contrast is a cross-cultural cue for perceiving facial attributes such as age. An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast faces. The study also found that artificially increasing the aspects of facial contrast that decrease with age across diverse races makes faces look younger, independent of the ethnic origin of the face or cultural origin of the observers [6]. On one hand, the age that you are is one dimension that needs to be addressed in terms of fairness and accuracy of face recognition. However, the age that you look, considering possible artificial changes, should not change requirements for fairness and accuracy.",
      "referring_paragraphs": [
        "We point out that although a face image is spatially transformed during rectification, facial symmetry with respect to the face mid-line is preserved according to the topological properties of the affine transformation [68]. Each image is then cropped to 128x128 pixels to create a squared image with the face mid-line centered vertically. Next we convert the spatially transformed image to grayscale to measure intensity. Each point $( x , y )$ on this normalized face intensity image $I$ on the lef",
        "Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870. Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes. While Simpson $E$ has a range between 0.978 to 0.984, Shannon $E$ ranges between 0.994 to 0.995. The evenness of coding scheme 3 is similar to coding scheme 2.",
        "Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part ( $I ^ { \\prime }$ and $I _ { e } ^ { \\prime }$ ) are computed as summarized in Table 9, where $\\phi ( I _ { e } ( x , y ) , I _ { e } ^ { \\prime } ( x , y ) )$ is the angle between the two edge orientations of images $I _ { e }$ and $I _ { e } ^ { \\prime }$ at pixel $( x , y )$ .",
        "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   \n(a)"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_17",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg",
      "image_filename": "1901.10436_page0_fig7.jpg",
      "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.",
      "context_before": "4.5 Coding Scheme 5: Facial Regions Contrast\n\nPrior studies have shown that facial contrast is a cross-cultural cue for perceiving facial attributes such as age. An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast faces. The study also found that artificially increasing the aspects of facial contrast that decrease with age across diverse races makes faces look younger, independent of the ethnic origin of the face or cultural origin of the observers [6]. On one hand, the age that you are is one dimension that needs to be addressed in terms of fairness and accuracy of face recognition. However, the age that you look, considering possible artificial changes, should not change requirements for fairness and accuracy.",
      "context_after": "We adopted facial regions contrast as the basis for coding scheme 5. To compute facial contrast, we measured contrast individually for each image color channel $I _ { L }$ , $I _ { a }$ , $\\mathit { 1 } _ { b }$ , corresponding to the CIE-Lab color space, for three facial regions: lips, eyes, and eyebrows, as shown in Figure 4. First, we defined the internal regions ringed by facial key points computed from DLIB for each of these facial parts (shown as the inner rings around lips, eyes, and eyebrows in Figure 4). Then, we expanded this region by 50% to define an outer region around each of these facial parts (shown as the outer rings in Figure 4). The contrast is then measured as the difference between the average pixel intensities in the outer and inner regions. This is repeated for each of the three CIE-Lab color channels. Given the three facial regions, this gives a total of nine measures, where the contrast values for the eyes and eyebrows are based on the average of the left and right regions. The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
      "referring_paragraphs": [
        "As described above, the last decade has seen an ever-growing collection of face recognition data sets. Table 4 summarizes many of the prominent face image data sets used for evaluating face recognition technology. Returning to Labeled Faces in the Wild (LFW) [15], it presented considerable technical challenges upon its release in 2007, whereas nearly perfect results are being attained today. Several data sets such as IJBC [53], UMD [26] and VGGFace [22, 23] provide a larger set of face images wi",
        "We adopted facial regions contrast as the basis for coding scheme 5. To compute facial contrast, we measured contrast individually for each image color channel $I _ { L }$ , $I _ { a }$ , $\\mathit { 1 } _ { b }$ , corresponding to the CIE-Lab color space, for three facial regions: lips, eyes, and eyebrows, as shown in Figure 4. First, we defined the internal regions ringed by facial key points computed from DLIB for each of these facial parts (shown as the inner rings around lips, eyes, and eyeb",
        "There are multiple next directions for this work. Table 4 outlined many of the currently used face data sets. We plan to perform the equivalent statistical analysis on some of these other data sets using the ten coding schemes. This will provide an important basis for comparing data sets in terms of diversity. Using the statistical measures outlined in this paper, including diversity, evenness and variance, we will begin to answer questions of whether one data set is better than another, or wher",
        "Table 4 summarizes many of the prominent face image data sets used for evaluating face recognition technology.",
        "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5.",
        "Table 4 outlined many of the currently used face data sets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_18",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/46c910b802ad3710630e11b6c13af37ad49098790ff333427f62a57b6e5bd901.jpg",
      "image_filename": "46c910b802ad3710630e11b6c13af37ad49098790ff333427f62a57b6e5bd901.jpg",
      "caption": "Table 10: Coding scheme 5 is made up of three measures of facial region contrast [6].",
      "context_before": "We adopted facial regions contrast as the basis for coding scheme 5. To compute facial contrast, we measured contrast individually for each image color channel $I _ { L }$ , $I _ { a }$ , $\\mathit { 1 } _ { b }$ , corresponding to the CIE-Lab color space, for three facial regions: lips, eyes, and eyebrows, as shown in Figure 4. First, we defined the internal regions ringed by facial key points computed from DLIB for each of these facial parts (shown as the inner rings around lips, eyes, and eyebrows in Figure 4). Then, we expanded this region by 50% to define an outer region around each of these facial parts (shown as the outer rings in Figure 4). The contrast is then measured as the difference between the average pixel intensities in the outer and inner regions. This is repeated for each of the three CIE-Lab color channels. Given the three facial regions, this gives a total of nine measures, where the contrast values for the eyes and eyebrows are based on the average of the left and right regions. The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
      "context_after": "4.6 Coding Scheme 6: Skin Color\n\nSkin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for skin color, even within the dermatology field. In a study of 556 participants in South Africa, self-identified as either black, Indian/Asian, white, or mixed, Wilkes et al. found a high correlation between the Melanin Index (MI), which is frequently used to assign FST, with Individual Typology Angle (ITA) [72]. Since a dermatology expert is typically needed to assign the FST, the high correlation of MI and ITA indicates that ITA may be a practical method for measuring skin color given the simplicity of computing ITA. In order to explore this further, we designed coding scheme 6 to use ITA for representing skin color [7]. ITA has a strong advantage over Fitzpatrick in that it can be computed directly from an image. As in [7], we implemented ITA in the CIE-Lab space. For obvious practical reasons, we could not obtain measurements through a device directly applied on the skin of each individual, but instead converted the $R G B$ image to CIE-Lab space using standard image processing. The $L$ axis quantifies luminance, whereas $a$ quantifies absence or presence of redness, and $b$ quantifies yellowness. Figure 5 depicts the image processing steps for extracting the coding scheme 6 for skin color.\n\nIt is important to note that that ITA is a point measurement. Hence, every pixel corresponding to skin can have an ITA measurement. In order to generate a feature measure for the whole face, we extract ITA for pixels within a masked face region as shown in Figure 5(g). This masked region is determined in the following steps:",
      "referring_paragraphs": [
        "We adopted facial regions contrast as the basis for coding scheme 5. To compute facial contrast, we measured contrast individually for each image color channel $I _ { L }$ , $I _ { a }$ , $\\mathit { 1 } _ { b }$ , corresponding to the CIE-Lab color space, for three facial regions: lips, eyes, and eyebrows, as shown in Figure 4. First, we defined the internal regions ringed by facial key points computed from DLIB for each of these facial parts (shown as the inner rings around lips, eyes, and eyeb",
        "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highest Simpson $E$ value being 0.918 and highest Shannon $E$ value being 0.975. The Shannon $H$ value of 1.692 translates to about 5.4 classes.",
        "The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
        "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig8.jpg",
      "image_filename": "1901.10436_page0_fig8.jpg",
      "caption": "(a)",
      "context_before": "4.6 Coding Scheme 6: Skin Color\n\nSkin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for skin color, even within the dermatology field. In a study of 556 participants in South Africa, self-identified as either black, Indian/Asian, white, or mixed, Wilkes et al. found a high correlation between the Melanin Index (MI), which is frequently used to assign FST, with Individual Typology Angle (ITA) [72]. Since a dermatology expert is typically needed to assign the FST, the high correlation of MI and ITA indicates that ITA may be a practical method for measuring skin color given the simplicity of computing ITA. In order to explore this further, we designed coding scheme 6 to use ITA for representing skin color [7]. ITA has a strong advantage over Fitzpatrick in that it can be computed directly from an image. As in [7], we implemented ITA in the CIE-Lab space. For obvious practical reasons, we could not obtain measurements through a device directly applied on the skin of each individual, but instead converted the $R G B$ image to CIE-Lab space using standard image processing. The $L$ axis quantifies luminance, whereas $a$ quantifies absence or presence of redness, and $b$ quantifies yellowness. Figure 5 depicts the image processing steps for extracting the coding scheme 6 for skin color.\n\nIt is important to note that that ITA is a point measurement. Hence, every pixel corresponding to skin can have an ITA measurement. In order to generate a feature measure for the whole face, we extract ITA for pixels within a masked face region as shown in Figure 5(g). This masked region is determined in the following steps:",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig9.jpg",
      "image_filename": "1901.10436_page0_fig9.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig10.jpg",
      "image_filename": "1901.10436_page0_fig10.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig11.jpg",
      "image_filename": "1901.10436_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig12.jpg",
      "image_filename": "1901.10436_page0_fig12.jpg",
      "caption": "(e)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg",
      "image_filename": "1901.10436_page0_fig13.jpg",
      "caption": "(f)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig14.jpg",
      "image_filename": "1901.10436_page0_fig14.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_26",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig15.jpg",
      "image_filename": "1901.10436_page0_fig15.jpg",
      "caption": "(h) Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In this Section, we describe the implementation of the ten facial coding schemes and the process of extracting them from the $D i F$ face images. The advantage of using ten coding schemes is that it gives a diversity of methods and allows us to compare statistical measures for facial diversity. As described above, the ten schemes have been selected based on their strong scientific basis, computational feasibility, numerical representation and interpretability. Overall the chosen ten coding schem",
        "The first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: ",
        "Skin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for skin color, even within the dermatology field",
        "It is important to note that that ITA is a point measurement. Hence, every pixel corresponding to skin can have an ITA measurement. In order to generate a feature measure for the whole face, we extract ITA for pixels within a masked face region as shown in Figure 5(g). This masked region is determined in the following steps:",
        "To provide the basis for the three craniofacial feature coding schemes used in $D i F$ , we built on the subset of 19 facial landmarks listed in Table 5.",
        "In referring to the implementation of the coding scheme, we use the abbreviations from Table 5.",
        "Figure 5 depicts the image processing steps for extracting the coding scheme 6 for skin color.",
        "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA)."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_27",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/88b2065bfcbf451e41f2b998f0cfbaa7b03c5176eb58cebd78c39dadae99a545.jpg",
      "image_filename": "88b2065bfcbf451e41f2b998f0cfbaa7b03c5176eb58cebd78c39dadae99a545.jpg",
      "caption": "Table 11 gives the formula for computing the ITA values for each pixel in the masked face region. Table 11: Coding scheme 6 measures skin color using Individual Typology Angle (ITA) [3].",
      "context_before": "",
      "context_after": "4.7 Coding Scheme 7: Age Prediction\n\nAge is an attribute we all possess and our faces are predictors of our age, whether it is our actual age or manipulated age appearance [6]. As discussed in Section 4.5, particular facial features such as facial contrast are correlated with age. As an alternative to designing specific feature representations for predicting age, for coding scheme 7, we adopt a Convolutional Neural Network (CNN) that is trained from face images to predict age. We adopt the DEX model [8, 74] that is among the highest performing on some of the known face image data sets. The model is based on a pre-trained VGG16-face neural network for face identity that was subsequently fine-tuned on the IMDB-wiki data set [8] to predict age (years in the range 0-100). Since the DEX model was trained within a narrow context, it is not likely to be fair. However, our initial use here is to get some continuous measure of age in order to study diversity. Ultimately, it will require an iterative process of understanding diversity to make more balanced data sets and create more fair models. In order to predict age using DEX, each face was pre-processed as in [74]. First, the bounding box\n\nwas expanded by 40% both horizontally and vertically, then resized to 256x256 pixels. Inferencing was then performed on the 224x224 square cropped at the center of the image. Since softmax loss was used during the fine-tuning process, age prediction is output from the softmax layer, which is computed from $\\begin{array} { r } { E ( P ) = \\sum _ { i = 0 } ^ { 1 0 0 } p _ { i } y _ { i } } \\end{array}$ , where $p _ { i } \\in P$ are the softmax output probabilities for each of the 101 discrete years $y _ { i } \\in Y$ corresponding to each class $i$ , with $Y = \\{ 0 , . . . , 1 0 0 \\}$ .",
      "referring_paragraphs": [
        "Figure 11 summarizes the feature distribution for facial contrast in coding scheme 5. The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes. The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to even.",
        "Average the values to give a single ITA score for each face\n\nTable 11 gives the formula for computing the ITA values for each pixel in the masked face region.",
        "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   \nFigure 12: Feature distribution of skin color using Individual Typology Angle (ITA) (coding scheme 6) for the $D i F$ data set."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_28",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/4b4dd293e676133b2f43121ecfed1dce9223dcd3184c67d76b27966b4b4c1d98.jpg",
      "image_filename": "4b4dd293e676133b2f43121ecfed1dce9223dcd3184c67d76b27966b4b4c1d98.jpg",
      "caption": "Shannon $H$ and Simpson $D$ are diversity measures and Shannon $E$ and Simpson $E$ are evenness measures.",
      "context_before": "5 Statistical Analysis\n\nIn this Section, we report on the statistical analysis of the ten facial coding schemes in the $D i F$ data set. Intuitively, in order to provide sufficient coverage and balance, a data set needs to include data with diverse population characteristics. This type of analysis comes up in multiple disciplines, including bio-diversity [76, 77], where an important objective is to quantify species diversity of ecological communities. It has been reported that species diversity has two separate components: (1) species richness, or the number of species present, and (2) their relative abundances, called evenness. We use these same measures to quantify the diversity of face images using the ten facial coding schemes. We compute diversity using Shannon $H$ and $E$ scores and Simpson $D$ and $E$ scores [76]. Additionally, we measure mean and variance for each of the feature dimensions of the ten facial coding schemes The computation of diversity is as follows: given individual $p _ { i }$ in a\n\nprobability distribution for each feature measure, and the $S$ being the number of classes for the attribute, we compute:",
      "context_after": "Shannon $H$ and Simpson $D$ are diversity measures and Shannon $E$ and Simpson $E$ are evenness measures. To see how they work, consider a 20 class problem ( $S = 2 0$ ) with uniform distribution ( $p _ { i } = 0 . 0 5 ,$ ). These measures take the following values: Shannon $H = 2 . 9 9 9$ , Shannon $E = 1 . 0$ , Simpson $D = 2 . 5 6 3$ , and Simpson $E = 1 . 0$ . Evenness is constant at 1.0 as expected. Shannon $\\boldsymbol { D }$ represents the diversity of 20 classes ( $e ^ { 2 . 9 9 9 } \\approx 2 0 $ ). For complex distributions, it may not be easy to understand the meaning of specific values of these scores. Generally, a higher diversity value is better than a lower value, whereas an evenness value closer to 1.0 is better. Figure 6 illustrates these measures on two example distributions. Figure 6 (a) and (b) show how diversity and evenness values vary for a uniform distribution, respectively, as the number of classes increase from 2 to 20. Figure 6 (c) and (d) show the same information for a random distribution.",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_29",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig16.jpg",
      "image_filename": "1901.10436_page0_fig16.jpg",
      "caption": "(a)",
      "context_before": "Shannon $H$ and Simpson $D$ are diversity measures and Shannon $E$ and Simpson $E$ are evenness measures. To see how they work, consider a 20 class problem ( $S = 2 0$ ) with uniform distribution ( $p _ { i } = 0 . 0 5 ,$ ). These measures take the following values: Shannon $H = 2 . 9 9 9$ , Shannon $E = 1 . 0$ , Simpson $D = 2 . 5 6 3$ , and Simpson $E = 1 . 0$ . Evenness is constant at 1.0 as expected. Shannon $\\boldsymbol { D }$ represents the diversity of 20 classes ( $e ^ { 2 . 9 9 9 } \\approx 2 0 $ ). For complex distributions, it may not be easy to understand the meaning of specific values of these scores. Generally, a higher diversity value is better than a lower value, whereas an evenness value closer to 1.0 is better. Figure 6 illustrates these measures on two example distributions. Figure 6 (a) and (b) show how diversity and evenness values vary for a uniform distribution, respectively, as the number of classes increase from 2 to 20. Figure 6 (c) and (d) show the same information for a random distribution.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_30",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg",
      "image_filename": "1901.10436_page0_fig17.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_31",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig18.jpg",
      "image_filename": "1901.10436_page0_fig18.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_32",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig19.jpg",
      "image_filename": "1901.10436_page0_fig19.jpg",
      "caption": "(d) Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.",
      "context_before": "",
      "context_after": "Table 12 summarizes the diversity scores computed for the ten facial coding schemes in the $D i F$ data set. As described in Section 4, many of the coding schemes have multiple dimensions. Hence the table has more than ten rows. The craniofacial measurements across the three coding scheme types total 28 features corresponding to craniofacial distances, craniofacial areas and craniofacial ratios. The diversity scores of the different dimensions of the remaining seven coding schemes can similarly be seen in Table 12.\n\n5.1 Coding Scheme 1: Craniofacial Distances\n\nFigure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1. The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777. Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes. Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.",
      "referring_paragraphs": [
        "The first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: ",
        "Shannon $H$ and Simpson $D$ are diversity measures and Shannon $E$ and Simpson $E$ are evenness measures. To see how they work, consider a 20 class problem ( $S = 2 0$ ) with uniform distribution ( $p _ { i } = 0 . 0 5 ,$ ). These measures take the following values: Shannon $H = 2 . 9 9 9$ , Shannon $E = 1 . 0$ , Simpson $D = 2 . 5 6 3$ , and Simpson $E = 1 . 0$ . Evenness is constant at 1.0 as expected. Shannon $\\boldsymbol { D }$ represents the diversity of 20 classes ( $e ^ { 2 . 9 9 9 } \\app",
        "The eight dimensions of craniofacial distances are summarized in Table 6.",
        "Table 6: Coding scheme 1 is made up eight craniofacial measures corresponding to different vertical distances in the face [2].",
        "Figure 6 illustrates these measures on two example distributions.",
        "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_33",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/f0ffa3a4ce9901b6ab3c3a8e23ad15d66f4c8b364345980258380429c974db74.jpg",
      "image_filename": "f0ffa3a4ce9901b6ab3c3a8e23ad15d66f4c8b364345980258380429c974db74.jpg",
      "caption": "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.",
      "context_before": "Table 12 summarizes the diversity scores computed for the ten facial coding schemes in the $D i F$ data set. As described in Section 4, many of the coding schemes have multiple dimensions. Hence the table has more than ten rows. The craniofacial measurements across the three coding scheme types total 28 features corresponding to craniofacial distances, craniofacial areas and craniofacial ratios. The diversity scores of the different dimensions of the remaining seven coding schemes can similarly be seen in Table 12.\n\n5.1 Coding Scheme 1: Craniofacial Distances\n\nFigure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1. The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777. Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes. Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_34",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig20.jpg",
      "image_filename": "1901.10436_page0_fig20.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_35",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig21.jpg",
      "image_filename": "1901.10436_page0_fig21.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_36",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig22.jpg",
      "image_filename": "1901.10436_page0_fig22.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_37",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig23.jpg",
      "image_filename": "1901.10436_page0_fig23.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_38",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig24.jpg",
      "image_filename": "1901.10436_page0_fig24.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_39",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg",
      "image_filename": "1901.10436_page0_fig25.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_40",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig26.jpg",
      "image_filename": "1901.10436_page0_fig26.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_41",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig27.jpg",
      "image_filename": "1901.10436_page0_fig27.jpg",
      "caption": "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set.",
      "context_before": "",
      "context_after": "5.2 Coding Scheme 2: Craniofacial Areas\n\nFigure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2. The highest Simpson $D$ value is 5.888 and the smallest is 5.858. The highest Shannon $H$ value is 1.782 and the lowest is 1.780. Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes. Evenness ranges between 0.981 and 0.976.\n\n5.3 Coding Scheme 3: Craniofacial Ratios",
      "referring_paragraphs": [
        "The second coding scheme is adopted from a later development from Farkas et al. [3]. It comprises measures corresponding to different areas of the cranium. Similar to the craniofacial distances, the extraction of craniofacial areas relied on the mapped DLIB key-points to the corresponding facial landmarks. Table 7 summarizes the twelve dimensions of the craniofacial area features.",
        "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1. The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777. Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes. Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.",
        "Table 7 summarizes the twelve dimensions of the craniofacial area features.",
        "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.",
        "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_42",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig28.jpg",
      "image_filename": "1901.10436_page0_fig28.jpg",
      "caption": "Figure 11 summarizes the feature distribution for facial contrast in coding scheme 5.",
      "context_before": "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highest Simpson $E$ value being 0.918 and highest Shannon $E$ value being 0.975. The Shannon $H$ value of 1.692 translates to about 5.4 classes.\n\n5.5 Coding Scheme 5: Facial Regions Contrast\n\nFigure 11 summarizes the feature distribution for facial contrast in coding scheme 5. The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes. The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to even.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_43",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig29.jpg",
      "image_filename": "1901.10436_page0_fig29.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_44",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig30.jpg",
      "image_filename": "1901.10436_page0_fig30.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_45",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig31.jpg",
      "image_filename": "1901.10436_page0_fig31.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_46",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig32.jpg",
      "image_filename": "1901.10436_page0_fig32.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_47",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig33.jpg",
      "image_filename": "1901.10436_page0_fig33.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_48",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig34.jpg",
      "image_filename": "1901.10436_page0_fig34.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_49",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig35.jpg",
      "image_filename": "1901.10436_page0_fig35.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_50",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig36.jpg",
      "image_filename": "1901.10436_page0_fig36.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_51",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg",
      "image_filename": "1901.10436_page0_fig37.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_52",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig38.jpg",
      "image_filename": "1901.10436_page0_fig38.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_53",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig39.jpg",
      "image_filename": "1901.10436_page0_fig39.jpg",
      "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.",
      "context_before": "",
      "context_after": "5.6 Coding Scheme 6: Skin Color\n\nFigure 12 summarizes the feature distribution for skin color in coding scheme 6. The Simpson $D$ value is 5.283 and Shannon $H$ value is 1.773 which translates to about 5.88 classes, which shows a good match with the number of bins we used. The evenness is weaker than a uniform distribution.\n\n5.7 Coding Scheme 7: Age Prediction",
      "referring_paragraphs": [
        "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.",
        "Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2. The highest Simpson $D$ value is 5.888 and the smallest is 5.858. The highest Shannon $H$ value is 1.782 and the lowest is 1.780. Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes. Evenness ranges between 0.981 and 0.976.",
        "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].",
        "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_54",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig40.jpg",
      "image_filename": "1901.10436_page0_fig40.jpg",
      "caption": "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8.",
      "context_before": "Figure 13(a) summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]. The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even, we can see a lower $E$ value around 0.624. The Shannon $H$ value of 1.601 maps to 5 classes.\n\n5.8 Coding Scheme 8: Gender Prediction\n\nFigure 13 also summarizes the feature distribution for gender prediction in coding scheme 8. Even though this has two classes, male and female, the confidence score ranges between $0 - 1$ . The gender score distribution is shown in Figure 13 (b). The Simpson $D$ is 3.441 and Shannon $H$ is 1.488. The Shannon $H$ value translates to 4.4 classes, which is beyond the typical two classes used for gender, possibly reflecting the presence of sub-classes. The Simpson evenness score of 0.573 reflect some unevenness as well.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_55",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig41.jpg",
      "image_filename": "1901.10436_page0_fig41.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_56",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig42.jpg",
      "image_filename": "1901.10436_page0_fig42.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_57",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig43.jpg",
      "image_filename": "1901.10436_page0_fig43.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_58",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig44.jpg",
      "image_filename": "1901.10436_page0_fig44.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_59",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig45.jpg",
      "image_filename": "1901.10436_page0_fig45.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_60",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg",
      "image_filename": "1901.10436_page0_fig46.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_61",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig47.jpg",
      "image_filename": "1901.10436_page0_fig47.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_62",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig48.jpg",
      "image_filename": "1901.10436_page0_fig48.jpg",
      "caption": "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set. (a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We point out that although a face image is spatially transformed during rectification, facial symmetry with respect to the face mid-line is preserved according to the topological properties of the affine transformation [68]. Each image is then cropped to 128x128 pixels to create a squared image with the face mid-line centered vertically. Next we convert the spatially transformed image to grayscale to measure intensity. Each point $( x , y )$ on this normalized face intensity image $I$ on the lef",
        "Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870. Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes. While Simpson $E$ has a range between 0.978 to 0.984, Shannon $E$ ranges between 0.994 to 0.995. The evenness of coding scheme 3 is similar to coding scheme 2.",
        "Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part ( $I ^ { \\prime }$ and $I _ { e } ^ { \\prime }$ ) are computed as summarized in Table 9, where $\\phi ( I _ { e } ( x , y ) , I _ { e } ^ { \\prime } ( x , y ) )$ is the angle between the two edge orientations of images $I _ { e }$ and $I _ { e } ^ { \\prime }$ at pixel $( x , y )$ .",
        "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   \n(a)"
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_63",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg",
      "image_filename": "1901.10436_page0_fig49.jpg",
      "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.",
      "context_before": "",
      "context_after": "5.9 Coding Scheme 9: Subjective Annotation\n\nFigure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9. The Simpson $D$ for gender distribution is 2.0 and Shannon $H$ is 0.693, indicating the equivalent classes to be near 2, which is understandable. The evenness is very high, indicating a nearly flat distribution. The Simpson $D$ is 4.368 and Shannon $H$ is 1.675, resulting in a equivalent class index of approximately 5.3. However, the evenness scores are low at 0.629, indicating unevenness, as is visible in the distribution of the annotated age scores.\n\n5.10 Coding Scheme 10: Pose and Resolution",
      "referring_paragraphs": [
        "We adopted facial regions contrast as the basis for coding scheme 5. To compute facial contrast, we measured contrast individually for each image color channel $I _ { L }$ , $I _ { a }$ , $\\mathit { 1 } _ { b }$ , corresponding to the CIE-Lab color space, for three facial regions: lips, eyes, and eyebrows, as shown in Figure 4. First, we defined the internal regions ringed by facial key points computed from DLIB for each of these facial parts (shown as the inner rings around lips, eyes, and eyeb",
        "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highest Simpson $E$ value being 0.918 and highest Shannon $E$ value being 0.975. The Shannon $H$ value of 1.692 translates to about 5.4 classes.",
        "The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
        "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_64",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig50.jpg",
      "image_filename": "1901.10436_page0_fig50.jpg",
      "caption": "Figure 15 summarizes the feature distribution for pose and resolution for coding scheme 10.",
      "context_before": "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9. The Simpson $D$ for gender distribution is 2.0 and Shannon $H$ is 0.693, indicating the equivalent classes to be near 2, which is understandable. The evenness is very high, indicating a nearly flat distribution. The Simpson $D$ is 4.368 and Shannon $H$ is 1.675, resulting in a equivalent class index of approximately 5.3. However, the evenness scores are low at 0.629, indicating unevenness, as is visible in the distribution of the annotated age scores.\n\n5.10 Coding Scheme 10: Pose and Resolution\n\nFigure 15 summarizes the feature distribution for pose and resolution for coding scheme 10. Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted Left -1, Frontal 0, and Frontal Tilted Right 1. The IOD and box size distribution",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_65",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig51.jpg",
      "image_filename": "1901.10436_page0_fig51.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_66",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig52.jpg",
      "image_filename": "1901.10436_page0_fig52.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_67",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig53.jpg",
      "image_filename": "1901.10436_page0_fig53.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_68",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig54.jpg",
      "image_filename": "1901.10436_page0_fig54.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_69",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig55.jpg",
      "image_filename": "1901.10436_page0_fig55.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_70",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig56.jpg",
      "image_filename": "1901.10436_page0_fig56.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_71",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg",
      "image_filename": "1901.10436_page0_fig57.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_72",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig58.jpg",
      "image_filename": "1901.10436_page0_fig58.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_73",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig59.jpg",
      "image_filename": "1901.10436_page0_fig59.jpg",
      "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set. Figure 12: Feature distribution of skin color using Individual Typology Angle (ITA) (coding scheme 6) for the $D i F$ data set.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 11 summarizes the feature distribution for facial contrast in coding scheme 5. The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes. The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to even.",
        "Average the values to give a single ITA score for each face\n\nTable 11 gives the formula for computing the ITA values for each pixel in the masked face region.",
        "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   \nFigure 12: Feature distribution of skin color using Individual Typology Angle (ITA) (coding scheme 6) for the $D i F$ data set."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_74",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg",
      "image_filename": "1901.10436_page0_fig60.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_75",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig61.jpg",
      "image_filename": "1901.10436_page0_fig61.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_76",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig62.jpg",
      "image_filename": "1901.10436_page0_fig62.jpg",
      "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set. (a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 13(a) summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]. The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even, we can see a lower $E$ value around 0.624. The Shannon $H$ value of 1.601 maps to 5 classes.",
        "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8. Even though this has two classes, male and female, the confidence score ranges between $0 - 1$ . The gender score distribution is shown in Figure 13 (b). The Simpson $D$ is 3.441 and Shannon $H$ is 1.488. The Shannon $H$ value translates to 4.4 classes, which is beyond the typical two classes used for gender, possibly reflecting the presence of sub-classes. The Simpson evenness score of 0.573 reflect som",
        "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8.",
        "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   \n(a)"
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_77",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg",
      "image_filename": "1901.10436_page0_fig63.jpg",
      "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.",
      "context_before": "",
      "context_after": "are shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes.\n\nSome observations come from this statistical analysis of the ten coding schemes on the $D i F$ face image data. One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one. What this shows is that there is higher variability in these measures, and they are capturing information that age and gender alone do not. Interestingly, facial regions contrast, which was designed to capture information about age, has a higher diversity score and better evenness than that for either neural network prediction of age or subjective human annotation of age. Again, it implies that this continuous valued feature of facial contrast is capturing information that goes beyond simple age prediction or labeling. The only feature dimension with lower diversity is understandably pose, which was a controlled variable in selecting images for the $D i F$ data set, since only mostly frontal faces were incorporated. In future work, we will use these methods to assess diversity of other face data sets, which will provide a basis for comparison and further insight.",
      "referring_paragraphs": [
        "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9. The Simpson $D$ for gender distribution is 2.0 and Shannon $H$ is 0.693, indicating the equivalent classes to be near 2, which is understandable. The evenness is very high, indicating a nearly flat distribution. The Simpson $D$ is 4.368 and Shannon $H$ is 1.675, resulting in a equivalent class index of approximately 5.3. However, the evenness scores are low at 0.629, indicating une",
        "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9.",
        "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.\n\nare shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_78",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg",
      "image_filename": "1901.10436_page0_fig64.jpg",
      "caption": "(a)",
      "context_before": "are shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes.\n\nSome observations come from this statistical analysis of the ten coding schemes on the $D i F$ face image data. One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one. What this shows is that there is higher variability in these measures, and they are capturing information that age and gender alone do not. Interestingly, facial regions contrast, which was designed to capture information about age, has a higher diversity score and better evenness than that for either neural network prediction of age or subjective human annotation of age. Again, it implies that this continuous valued feature of facial contrast is capturing information that goes beyond simple age prediction or labeling. The only feature dimension with lower diversity is understandably pose, which was a controlled variable in selecting images for the $D i F$ data set, since only mostly frontal faces were incorporated. In future work, we will use these methods to assess diversity of other face data sets, which will provide a basis for comparison and further insight.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_79",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig65.jpg",
      "image_filename": "1901.10436_page0_fig65.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_80",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig66.jpg",
      "image_filename": "1901.10436_page0_fig66.jpg",
      "caption": "(c) Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).",
      "context_before": "",
      "context_after": "6 Summary and Future Work\n\nWe described the new Diversity in Faces $( D i F )$ data set, which has been developed to help advance the study of fairness and accuracy in face recognition technology. $D i F$ provides a data set of annotations of publicly available face images sampled from the YFCC-100M data set of 100 million images. The annotations are defined from facial coding schemes that provide quantitative measures related to intrinsic characteristics of faces including craniofacial features, facial symmetry, facial contrast, skin color, age, gender, subjective annotations and pose and resolution. We described the process for generating the $D i F$ data set as well as the implementation and extraction of the ten facial coding schemes. We also provided a statistical analysis of the facial coding scheme measures on the one million $D i F$ images using measures of diversity, evenness and variance. For one, this kind of analysis has provided insight into how the 47 total feature dimensions within the ten facial coding schemes provide measures of data set diversity for the one million images. While it may not yet be possible to conclude that the goal is to drive all of these feature dimensions to be maximally diverse and even, we believe the approach outlined in this work provides a needed methodology for advancing the study of diversity for face recognition.\n\nThere are multiple next directions for this work. Table 4 outlined many of the currently used face data sets. We plan to perform the equivalent statistical analysis on some of these other data sets using the ten coding schemes. This will provide an important basis for comparing data sets in terms of diversity. Using the statistical measures outlined in this paper, including diversity, evenness and variance, we will begin to answer questions of whether one data set is better than another, or where a data set falls short in terms of coverage and balance. We also strongly encourage others to build on this work. We selected a solid starting point by using over one million publicly available face images and by implementing ten facial coding schemes. We hope that others will find ways to grow the data set to include more faces. As more insight comes from the type of analysis outlined in this paper, we see that an iterative process can more proactive sampling to fill in gaps. For example, as technologies like Generative Adversarial Networks (GANs) continue to improve [66, 78], it may be possible to generate faces of any variety to synthesize training data as needed. We expect that $D i F$ will be valuable to steer these generative methods towards gaps and blind spots in the training data, for example, to assist methods that automatically generate faces corresponding to a specific geographical area [79]. We also hope that others will see ways to improve on the initial ten coding schemes and add new ones. Pulling together our collective efforts is the best way to make progress on this important topic. We hope that the $D i F$ data set provides a useful foundation for creating more fair and accurate face recognition systems in practice.",
      "referring_paragraphs": [
        "Figure 15 summarizes the feature distribution for pose and resolution for coding scheme 10. Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted Left -1, Frontal 0, and Frontal Tilted Right 1. The IOD and box size distribution",
        "are shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes.",
        "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.\n\nare shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes.",
        "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1901.10436_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1902.03519": [
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig0.jpg",
      "image_filename": "1902.03519_page0_fig0.jpg",
      "caption": "(a) The original space partitioned into hypercubes.",
      "context_before": "Preprocessing phase: embedding to $\\gamma$ -HST. An important step in our algorithm is to embed the input pointset $P$ into a $\\gamma$ -HST (see Section 2 for more details on HST metrics). To this end, we exploit the following standard construction of $\\gamma$ -HST using randomly shifted grids.\n\nSuppose that all points in $P$ lie in $\\{ - \\Delta , \\cdots , \\Delta \\} ^ { d }$ . We generate a random tree $T$ (which is a -HST embedding of $P$ ) recursively. We translate the $d$ -dimensional hypercube $H = [ - 2 \\Delta , 2 \\Delta ] ^ { d }$ $\\gamma$ via a uniformly random shift vector $\\sigma \\in \\{ - \\Delta , \\cdot \\cdot \\cdot , \\Delta \\} ^ { d }$ . It is straightforward to verify that all points in $P$ are enclosed in $H + \\sigma$ . We then split each dimension of $H$ into equal pieces to create $\\gamma$ a grid with $\\gamma ^ { d }$ cells. Then we proceed recursively with each non-empty cell to create a hierarchy of nested $d$ -dimensional grids with $O ( \\log _ { \\gamma } { \\frac { \\Delta } { \\varepsilon } } )$ levels (each cell in the final level of the recursion either contains exactly one point of $P$ or has side length $\\varepsilon$ ). Next, we construct a tree $T$ corresponding to the described hierarchy nested $d$ -dimensional grids as follows. Consider a cell $C$ in the $i$ -th level (level 0 denote the initial hypercube $H$ ) of the hierarchy. Let $T _ { C } ^ { 1 } , \\cdots , T _ { C } ^ { \\ell }$ denote the trees constructed recursively for each non-empty cells of $C$ . Denote the root of each tree $T _ { C } ^ { j }$ by $u _ { C } ^ { j }$ . Then we connect $u _ { C }$ (corresponding to cell $C$ ) to each of $u _ { j } ^ { C }$ with an edge of length proportional to the diameter of $C$ (i.e., $( \\sqrt { d } \\cdot \\Delta ) / \\gamma ^ { i } \\rangle$ ).\n\nNote that the final tree generated by the above construction is a $\\gamma$ -HST: on each path from the root to a leaf, the length of consecutive edges decrease exponentially (by a factor of $\\gamma$ ) and the distance from any node to all of its children are the same. Moreover, we assume that $\\Delta / \\varepsilon = n ^ { O ( 1 ) }$ .",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig1.jpg",
      "image_filename": "1902.03519_page0_fig1.jpg",
      "caption": "(b) A 2-HST embedding of the input points.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig2.jpg",
      "image_filename": "1902.03519_page0_fig2.jpg",
      "caption": "(c) Stage 1: we must connect 3 blue points from the left node through the root.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg",
      "image_filename": "1902.03519_page0_fig3.jpg",
      "caption": "(d) Stage 2: we can connect 1 red point from the middle node through the root.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig4.jpg",
      "image_filename": "1902.03519_page0_fig4.jpg",
      "caption": "(e) Stage 3: we add the unsaturated fairlet in the right node to the root and make it balanced.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_6",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig5.jpg",
      "image_filename": "1902.03519_page0_fig5.jpg",
      "caption": "(f) The final fairlet clustering. Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
      "context_before": "",
      "context_after": "Phase 1: computing $( r , b )$ -fairlet decomposition. This phase operates on the probabilistic embedding of the input into a -HST $T$ from the preprocessing phase, where $\\gamma = \\mathrm { p o l y } ( r , b )$ . The $\\gamma$ distortion of the embedding is $O ( d \\cdot \\gamma \\cdot \\log _ { \\gamma } n )$ . Additionally, we augment each node $v \\in T$ with integers $N _ { r }$ and $N _ { b }$ denoting the number of red and blue points, respectively, in the subtree $T ( v )$ rooted at $\\boldsymbol { v }$ .\n\nStep 1. Compute an approximately minimum number of points that are required to be removed from the children of $\\boldsymbol { v }$ so that (1) the set of points contained by each child becomes $( r , b )$ -balanced, and (2) the union of the set of removed points is also $( r , b )$ -balanced. More formally, we solve Question 3.2 approximately (recall that for each child $v _ { i }$ , $N _ { r } ^ { \\imath }$ and $N _ { b } ^ { i }$ respectively denotes the number of red and blue points in $T ( v _ { i } )$ ).\n\nDefinition 3.1 (Heavy Point). A point $p \\in T ( v )$ is heavy with respect to $v$ if it belongs to a fairlet $D$ such that $I c a ( D ) = v$ . For each fairlet $D \\in { \\mathcal { X } }$ , $\\mathinner { I c a \\mathopen { \\left( D \\right) } }$ denotes the least common ancestor (lca) of the points contained in $D$ in $T$ .",
      "referring_paragraphs": [
        "Results. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our",
        "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .",
        "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ .",
        "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs\n\nTable 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017).",
        "Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_7",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/102e9d8de3b4a4062b1e1a379b8059afbe981b23412bb83be4a6b52fcd960bcb.jpg",
      "image_filename": "102e9d8de3b4a4062b1e1a379b8059afbe981b23412bb83be4a6b52fcd960bcb.jpg",
      "caption": "Table 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017). We remark that the number for (Chierichetti et al., 2017) mentioned in this table are not explicitly stated in their paper and we have extracted them from Figure 3 in their paper. Note that the cost denotes the total distances of the points to their fairlet/cluster centroids.",
      "context_before": "$$ \\# \\text {p o i n t s} \\bar {c} \\geq \\frac {r}{b} \\sum_ {j \\in Q} b _ {j}. \\tag {3} $$\n\nMoreover, since in the beginning of the process the number of points of color $c$ is more than the number of points of color $c$ and also in each non-saturated fairlest the number of points of color $c$ is more than the number of points of color $c$ , at the end of the process, in heavy points, the size of color $c$ is larger than the size of color c. Thus, by (2) and (3), at the end of stage 3, the extended heavy points has size $O ( r b \\cdot N _ { \\mathrm { H } } )$ and is $( r , b )$ -balanced as promised in Lemma 4.4.\n\nRuntime analysis of MinHeavyPoints. Here we analyze the runtime of MinHeavyPoints which corresponds to step 1 in FairletDecomposition. Note that stage 1 only requires $O ( 1 )$ operations on the number of red and blue points in $T ( v )$ . Each of stage 2 and stage 3 requires $O ( 1 )$ operations on the number of red and blue points in all non-empty children of $T ( v )$ . Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs",
      "context_after": "$O ( 1 )$ operations on the number of red and blue points in $T ( v )$ exactly twice: when it is called on $v$ and the parent of $v$ . Hence, in total MinHeavyPoints performs $O ( 1 )$ time on each node in $T$ which in total is $O ( n )$ .\n\n$\\mathbf { A l g o r i t h m 6 } \\ \\mathrm { N O N S A T U R F A I R L E T } ( N _ { r } , N _ { b } , r , b )$ : returns the non-saturated fairlet in a set with $( N _ { r } , N _ { b } )$ points.\n\nIn this section we show the performance of our proposed algorithm for $( r , b )$ -fair $k$ -median problem on three different standard data sets considered in (Chierichetti et al., 2017) which are from UCI Machine Learning Repository (Dheeru and Karra Taniskidou, 2017)3. Furthermore, to exhibit the performance of our algorithms on large and high-dimensional scale datasets, we consider an additional data set.",
      "referring_paragraphs": [
        "Results. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our",
        "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .",
        "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ .",
        "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs\n\nTable 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017).",
        "Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig6.jpg",
      "image_filename": "1902.03519_page0_fig6.jpg",
      "caption": "5https://archive.ics.uci.edu/ml/datasets/Bank+Marketing",
      "context_before": "3https://archive.ics.uci.edu/ml/datasets/diabetes\n\n4https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008\n\n5https://archive.ics.uci.edu/ml/datasets/Bank+Marketing",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg",
      "image_filename": "1902.03519_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig8.jpg",
      "image_filename": "1902.03519_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_11",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig9.jpg",
      "image_filename": "1902.03519_page0_fig9.jpg",
      "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
      "context_before": "",
      "context_after": "“balance”, “duration-of-account”) as attributes to represent the dimensions of the points in the space. Moreover, we consider “marital-status” as the sensitive information.\n\n6https://archive.ics.uci.edu/ml/datasets/adult\n\n7https://archive.ics.uci.edu/ml/datasets/US+Census+Data+(1990)",
      "referring_paragraphs": [
        "We complement our theoretical analysis with empirical evaluation. Our experiments show that the quality of the clustering obtained by our algorithm is comparable to that of Chierichetti et al. (2017). At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets (see Figure 2).",
        "Results. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our",
        "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .",
        "At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets (see Figure 2).",
        "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_12",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/d747062c086b9540755e835ab6177a7a5a6263a2d9d13f4ad01503309b9cc7ee.jpg",
      "image_filename": "d747062c086b9540755e835ab6177a7a5a6263a2d9d13f4ad01503309b9cc7ee.jpg",
      "caption": "Table 2: The description of the three datasets used in our empirical evaluation. In each dataset, the goal is find a fair $k$ -median with respect to the sensitive attribute.",
      "context_before": "“balance”, “duration-of-account”) as attributes to represent the dimensions of the points in the space. Moreover, we consider “marital-status” as the sensitive information.\n\n6https://archive.ics.uci.edu/ml/datasets/adult\n\n7https://archive.ics.uci.edu/ml/datasets/US+Census+Data+(1990)",
      "context_after": "Algorithm. We essentially implement the algorithm described in Section 4.8 However, instead of building poly $( r , b )$ -HST, in our implementation, we embed the points into a 2-HST. After computing a fairlet-decomposition of the points with given balance parameters, we run an existing $K$ -medoids clustering subroutine9.\n\nResults. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our algorithm on the whole dataset (see Table 3). Empirically, the running time of our algorithm scales almost linearly in the number points in the input pointset (see Figure 2).\n\nIn Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .",
      "referring_paragraphs": [
        "We complement our theoretical analysis with empirical evaluation. Our experiments show that the quality of the clustering obtained by our algorithm is comparable to that of Chierichetti et al. (2017). At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets (see Figure 2).",
        "Results. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our",
        "In Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .",
        "At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets (see Figure 2).",
        "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_13",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1d5d90654dbfd15d0ecf75d7a7e2a1c0b060c1bd05c049a01dcdfcdd29b316e4.jpg",
      "image_filename": "1d5d90654dbfd15d0ecf75d7a7e2a1c0b060c1bd05c049a01dcdfcdd29b316e4.jpg",
      "caption": "Table 3: The performance of our algorithm on all points in each dataset. We provide the runtime of both fairlet decomposition and the whole clustering process. Since Census dataset is not $( 1 , 2 )$ - balanced, we picked a lower balance-threshold for this dataset.",
      "context_before": "Algorithm. We essentially implement the algorithm described in Section 4.8 However, instead of building poly $( r , b )$ -HST, in our implementation, we embed the points into a 2-HST. After computing a fairlet-decomposition of the points with given balance parameters, we run an existing $K$ -medoids clustering subroutine9.\n\nResults. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our algorithm on the whole dataset (see Table 3). Empirically, the running time of our algorithm scales almost linearly in the number points in the input pointset (see Figure 2).\n\nIn Figure 2 and both Table 1 and 3, the reported runtime for each sample size $S$ is the median runtime of our algorithm on 10 different sample sets from the given pointset each of size $S$ .",
      "context_after": "The authors would like to thank Ravi Kumar for many helpful discussions. This project was supported by funds from the MIT-IBM Watson AI Lab, NSF, and Simons Foundation.\n\n8Our code is publicly available at https://github.com/talwagner/fair_clustering.\n\n9https://www.mathworks.com/help/stats/kmedoids.html",
      "referring_paragraphs": [
        "Results. Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances. The main reason is that our algorithm is particularly efficient when the input pointset lies in a low dimensional space which is the case in all three datasets “Diabetes”, “Bank” and “Census”. Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our",
        "We remark that the number for (Chierichetti et al., 2017) mentioned in this table are not explicitly stated in their paper and we have extracted them from Figure 3 in their paper.",
        "Moreover, unlike (Chierichetti et al., 2017), for each dataset, we can afford running our algorithm on the whole dataset (see Table 3)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.03519_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1902.07823": [
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig0.jpg",
      "image_filename": "1902.07823_page0_fig0.jpg",
      "caption": "2.3 The stable and fair optimization problem",
      "context_before": "Hence, if $| \\mathcal { A } _ { S } ( \\cdot ) | > \\tau \\beta _ { N }$ , then we have\n\n$$ \\operatorname {I} \\left[ \\mathcal {A} _ {S} (x) \\geq 0 \\right] = \\operatorname {I} \\left[ \\mathcal {A} _ {S ^ {i}} (x) \\geq 0 \\right]. $$\n\nBy Definition 2.2, this implies the lemma.",
      "context_after": "2.3 The stable and fair optimization problem\n\nOur goal is to design fair classification algorithms that have a uniform stability guarantee. We focus on extending fair classification algorithms that are formulated as constrained empirical risk minimization problem over the collection $\\mathcal { F }$ of classifiers that is a reproducing kernel Hilbert space (RKHS), e.g., [77, 78, 34]; see the following program.\n\n$$ \\min _ {f \\in \\mathcal {F}} \\frac {1}{N} \\sum_ {i \\in [ N ]} L (f, s _ {i}) \\quad s. t. $$",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.07823_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/5fbafdf67f8b6822ae2ce61ab4c05294f70538e986454198fd30e0620f0cad5c.jpg",
      "image_filename": "5fbafdf67f8b6822ae2ce61ab4c05294f70538e986454198fd30e0620f0cad5c.jpg",
      "caption": "Table 1: The performance (mean and standard deviation in parenthesis), of KAAS-St and ZVRG-St with respect to accuracy and the fairness metrics $\\gamma$ on the Adult dataset with race/sex attribute.",
      "context_before": "Let $h = f ^ { \\star }$ , we have\n\n$$ \\begin{array}{l} \\mathbb {E} _ {S \\sim \\Im^ {N}} \\left[ R \\left(\\mathcal {A} _ {S}\\right) \\right] - \\mathbb {E} _ {s \\sim \\Im} \\left[ L \\left(f ^ {\\star}, s\\right) \\right] \\\\ = \\mathbb {E} _ {S \\sim \\Im^ {N}} \\left[ R (\\mathcal {A} _ {S}) - \\frac {1}{N} \\sum_ {i \\in [ N ]} L (f ^ {\\star}, s _ {i}) \\right] \\\\ = \\mathbb {E} _ {S \\sim \\mathfrak {I} ^ {N}} \\left[ F (g) - \\lambda \\| g \\| _ {k} ^ {2} - F \\left(f ^ {\\star}\\right) + \\lambda \\| f ^ {\\star} \\| _ {k} ^ {2} \\right] \\quad (\\text {D e f n s . o f} g \\text {a n d} F (\\cdot)) \\\\ \\leq \\mathbb {E} _ {S \\sim \\mathfrak {I} ^ {N}} [ F (g) - F (f ^ {\\star}) ] + \\lambda \\| f ^ {\\star} \\| _ {k} ^ {2} \\quad (\\| g \\| _ {k} ^ {2} \\geq 0) \\\\ \\leq \\frac {\\sigma^ {2} \\kappa^ {2}}{\\lambda N} + \\lambda B ^ {2} \\quad \\text {(I n e q . (6) a n d \\| f ^ {\\star} \\| _ {k} \\leq B)}. \\\\ \\end{array} $$\n\nThis completes the proof.",
      "context_after": "5.1 Empirical setting\n\nAlgorithms and baselines. We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair). We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32]. We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair). Specifically, [34] showed that the performance of GYF is comparable to ZVRG over the Adult dataset. We extend them by introducing a stability-focused regularization term.4\n\nDataset. Our simulations are over an income dataset Adult [23], that records the demographics of 45222 individuals, along with a binary label indicating whether the income of an individual is greater than 50k USD or not. We use the pre-processed dataset as in [32]. We take race and sex to be the sensitive attributes, that are binary in the dataset.",
      "referring_paragraphs": [
        "Our framework can be easily extended to other fairness metrics; see a summary in Table 1 of [14].",
        "Our simulations indicate that introducing a stability-focused regularization term can make the algorithm more stable by slightly sacrificing accuracy. Table 1 summarizes the accuracy",
        "Table 1: The performance (mean and standard deviation in parenthesis), of KAAS-St and ZVRG-St with respect to accuracy and the fairness metrics $\\gamma$ on the Adult dataset with race/sex attribute.",
        "Adult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.",
        "Table 1 summarizes the accuracy\n\nand fairness metric under different regularization parameters $\\lambda$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.07823_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg",
      "image_filename": "1902.07823_page0_fig1.jpg",
      "caption": "Adult dataset,race attribute Figure 1: stab vs. $\\lambda$ for race attribute.",
      "context_before": "$$ | T | \\cdot \\operatorname * {P r} _ {S, S ^ {\\prime} \\sim \\mathfrak {S} ^ {N}, X \\sim T, \\mathcal {A}} \\left[ \\mathrm {I} \\left[ \\mathcal {A} _ {S} (X) \\geq 0 \\right] \\neq \\mathrm {I} \\left[ \\mathcal {A} _ {S ^ {\\prime}} (X) \\geq 0 \\right] \\right]. $$\n\n4The codes are available on https://github.com/huanglx12/Stable-Fair-Classification.\n\n5There exists a threshold parameter in the constraints. In this paper, we set the parameter to be default 0.1.",
      "context_after": "",
      "referring_paragraphs": [
        "Our framework can be easily extended to other fairness metrics; see a summary in Table 1 of [14].",
        "Our simulations indicate that introducing a stability-focused regularization term can make the algorithm more stable by slightly sacrificing accuracy. Table 1 summarizes the accuracy",
        "Table 1: The performance (mean and standard deviation in parenthesis), of KAAS-St and ZVRG-St with respect to accuracy and the fairness metrics $\\gamma$ on the Adult dataset with race/sex attribute.",
        "Adult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.",
        "Table 1 summarizes the accuracy\n\nand fairness metric under different regularization parameters $\\lambda$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.07823_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg",
      "image_filename": "1902.07823_page0_fig2.jpg",
      "caption": "Adult dataset, sex attribute Figure 2: stab vs. $\\lambda$ for sex attribute.",
      "context_before": "",
      "context_after": "$\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ indicates the expected number of different predictions of $A _ { S }$ and $\\mathbf { \\boldsymbol { A } } _ { S ^ { \\prime } }$ over the testing set $T$ . Note that this metric is considered in [32], but is slightly different from prediction stability since $S$ and $S ^ { \\prime }$ may differ by more than one training sample. We investigate $\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ instead of prediction stability so that we can distinguish the performances of prediction difference under different regularization parameters. Since $\\Im$ is unknown, we generate $n$ training sets $S _ { 1 } , \\ldots , S _ { n }$ and use the following metric to estimate $\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ :\n\n$$ \\operatorname {s t a b} _ {T, n} (\\mathcal {A}) := \\frac {1}{n (n - 1)} \\sum_ {i, j \\in [ n ]: i \\neq j} \\sum_ {s = (x, z, y) \\in T} \\tag {7} $$\n\n$$ \\left| \\operatorname {I} \\left[ \\mathcal {A} _ {S _ {i}} (x) \\geq 0 \\right] - \\operatorname {I} \\left[ \\mathcal {A} _ {S _ {j}} (x) \\geq 0 \\right] \\right|. $$",
      "referring_paragraphs": [
        "Adult dataset, sex attribute   \nFigure 2: stab vs."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1902.07823_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1903.08136": [
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "image_filename": "1903.08136_page0_fig0.jpg",
      "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "context_before": "USC Information Sciences Institute\n\nUSC Information Sciences Institute\n\nThe goal of this paper is twofold. First, we demonstrate empirically the existence of biases in existing community detection approaches using real-world datasets. Through the analysis, we show that current state-of-the-art community detection methods suffer from ignoring low-degree users that have few links either by failing to incorporate them, or by putting them into small groups which are then ignored in the study. These low-degree users have value to be included in the communities, as they offer a more diverse, nuanced representation of the communities. Second, to overcome this issue, we introduce a new community detection method, Communities with Lowly-connected Attributed Nodes (CLAN), that would mitigate the existence of this bias towards low-degree nodes.",
      "context_after": "Our contributions are as follows:\n\narXiv:1903.08136v1 [cs.SI] 19 Mar 2019",
      "referring_paragraphs": [
        "Community detection is a fundamental task in social network analysis [19], which identifies sub-groups within social networks. These groups can represent a variety of things including karate club membership, political leanings, and deeply-held beliefs. Traditionally, these groups are identified by searching for densely-connected groups of nodes in the graph [14]. More recently, attributed approaches go beyond merely the links to cluster nodes based upon their attributes and their network connect",
        "Analysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches. We use two state-of-the-art approaches, CESNA [21], and the Louvain method [3]. Louvain uses only the network while assigning communities, while CESNA uses both the network and user",
        "Consider the toy example shown in Figure 1.",
        "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods.",
        "Table 1 shows the information that is omitted by excluding the lowly-connected users."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/f2a6fd9909f962d48f501bec1abc12f02918e8efb6c84cdd5a3af228f36d6e82.jpg",
      "image_filename": "f2a6fd9909f962d48f501bec1abc12f02918e8efb6c84cdd5a3af228f36d6e82.jpg",
      "caption": "Table 1: Number of missed hashtags which can be representative of missing information along with some examples in biased methods.",
      "context_before": "Our contributions are as follows:\n\narXiv:1903.08136v1 [cs.SI] 19 Mar 2019",
      "context_after": "to classify more users. We conduct further experiments to test the resiliency of CLAN to different data regimes.\n\n(3) We demonstrate the existence of bias in community detection approaches that is introduced from ignoring low-degree users. We show that CLAN is able to overcome this challenge by classifying low-degree users.\n\n2 BIASES IN COMMUNITY DETECTION METHODS",
      "referring_paragraphs": [
        "Community detection is a fundamental task in social network analysis [19], which identifies sub-groups within social networks. These groups can represent a variety of things including karate club membership, political leanings, and deeply-held beliefs. Traditionally, these groups are identified by searching for densely-connected groups of nodes in the graph [14]. More recently, attributed approaches go beyond merely the links to cluster nodes based upon their attributes and their network connect",
        "Analysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches. We use two state-of-the-art approaches, CESNA [21], and the Louvain method [3]. Louvain uses only the network while assigning communities, while CESNA uses both the network and user",
        "Consider the toy example shown in Figure 1.",
        "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods.",
        "Table 1 shows the information that is omitted by excluding the lowly-connected users."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg",
      "image_filename": "1903.08136_page0_fig1.jpg",
      "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.",
      "context_before": "The populations used in this study are drawn from social media with a particular focus on datasets with node attributes, such as text, and a social network structure. Moreover, we want datasets where the underlying communities come from different backgrounds. To satisfy this, we utilized two different datasets: Gamergate and U.S. Presidential Election. Both datasets have ground truth community labels. Our process for obtaining these labels will be discussed in detail.\n\nThe Gamergate dataset consists of tweets posted in 2014 between months of August through October. The tweets surround the Gamergate controversy [16]. It contains 21,441 users who collectively produced 104,914 tweets. These users fall into one of the two groups surrounding the controversy. One group consists of Gamergate supporters who are tweeting about ethics in journalism and believe that regardless of the relationship between journalists and game developers, journalists should give honest reviews to game developers. The other group, Gamergate opposers, argues that Gamergate supporters attack female game developers and also feminist critics, and that they are not concerned with ethics in journalism, but are using the opportunity to attack women in the gaming industry.\n\nIn this study, we conducted an Amazon Mechanical Turk experiment discussed later in the paper to obtain ground truth labels for each of the users in this controversy. The retweet network of this dataset is shown in Figure 3.",
      "context_after": "[Section: Debiasing Community Detection: The Importance of Lowly-Connected Nodes]\n\n4.2 U.S. Presidential Election\n\nThis dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user. This dataset comes from [1] in which we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label propagation. The network structure of this dataset is shown in Figure 2.",
      "referring_paragraphs": [
        "This dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user. This dataset comes from [1] in which we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label propagation. The network structure of this da",
        "After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement which will be discussed in detail in \"Community Detection Results\" section. Figure 2 confirms this fact by showing the disagreement between the left hand si",
        "7.1.1 Quantitative Results. In this section, we would report quantitative and numerical results from our experiments. We will first report the scores for the F1 and Jaccard similarity scores between the ground truth labels and three different methods. We will show that CLAN outperforms other methods in terms of F1 and Jaccard similarity scores for both of the datasets. Table 2 contains results for the F1 and Jaccard similarity scores obtained from comparisons done between the ground truth labels",
        "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.",
        "The network structure of this dataset is shown in Figure 2.",
        "Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.",
        "Table 2: The quantitative results obtained from calculating the F1 and Jaccard similarity scores with regards to the ground truth labels for each of the methods.",
        "Table 2 contains results for the F1 and Jaccard similarity scores obtained from comparisons done between the ground truth labels and labels obtained by applying each of the methods on the two datasets on hand."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg",
      "image_filename": "1903.08136_page0_fig2.jpg",
      "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.",
      "context_before": "In order to collect labels for each user in the Gamergate dataset, all the tweets associated to a user were mapped to the particular user so that the dataset was on a user level. Out of 21,441 total users, we excluded users who had only single tweets and duplicated users with same tweets. We then asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left based on their tweets into one of the following groups:\n\nTurkers were given complete description of the controversy and a detailed explanation of the labeling procedure. In order to make sure that the turkers were following the standards, some sanity check questions were put under each page for us to be able to identify bot turkers. These sanity check questions were trivial, made-up users with tweets that were easy to be categorized into one of the three groups, Gamergate Opposer, Gamergate Supporter, and Unaffiliated. After identifying bot turkers and excluding their labels from the total labels, we took the maximum agreement between 8,128 users that were labeled by at least three turkers. This resulted in a new dataset discussed in the next section which served as our ground truth.\n\nThe Gamergate dataset had 21,441 users initially. In this study, we consider only users who posted at least two original tweets. The resulting dataset contains 8,128 users which were then labeled by the turkers. After analyzing the Turker agreement between 8,128 labeled users, we considered only users where turkers agreed two out of three times on a single label. After this filtering, we have 7,320 labeled users in total. These 7,320 users served as our ground truth labels.",
      "context_after": "After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement which will be discussed in detail in \"Community Detection Results\" section. Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels. There is a significant amount of disagreement between these two results. The purple nodes represent Gamergate opposers and the green nodes represent Gamergate supporters. Using network structure and attributes would put almost all of Gamergate supporters in the green portion of the network and Gamergate opposers in the purple section of the network completely separated; however, the ground truth labels tend to have mixed users into each of the sections. The ground truth results are expected as many opposers may retweet Gamergate supporters; therefore, using network attributes merely on the retweet network might not be a good idea for separating these users, and other attributes and characteristics of users can be used for a more accurate community detection task. These results illustrate the fact that network does not explain everything and additional information is required in order to obtain accurate communities of users. This motivated us to come up with a new method that would not only address the bias from creating small communities and exclusion of low-degree nodes but would also have higher agreement with the ground truth communities by using other node attributes in addition to the network structure which will be discussed in the next section.\n\nWe have confirmed that ignoring low-degree users introduces bias into the resulting analysis of the data. In light of this, we propose a community detection approach that addresses the following issues:\n\n[Section: Mehrabi, et al.]",
      "referring_paragraphs": [
        "In this study, we conducted an Amazon Mechanical Turk experiment discussed later in the paper to obtain ground truth labels for each of the users in this controversy. The retweet network of this dataset is shown in Figure 3.",
        "that were labeled by each of the methods. These numbers show the number of users that the method has excluded by not labeling them. This exclusion shows the bias of the method towards those users. Therefore, the more unlabeled users a method has the more susceptible to bias it is. In Table 3, we reported the percentage of the users who were left unlabeled or put in insignificant communities in each of the methods from the two datasets. These results confirm the fact that our method, CLAN, has mi",
        "showing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network in the Gamergate dataset in the three methods discussed in this paper. Each node in these graphs represent a user and the nodes are color coded based on their agreement with the ground truth labels. The green nodes represent agreement between the label that was assigned to that ",
        "The retweet network of this dataset is shown in Figure 3.",
        "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.",
        "Presidential Election Dataset</td></tr><tr><td>Method</td><td>F1 Score</td><td>Jaccard</td><td>F1 Score</td><td>Jaccard</td></tr><tr><td>CESNA</td><td>0.343</td><td>0.211</td><td>0.253</td><td>0.149</td></tr><tr><td>Modularity</td><td>0.434</td><td>0.282</td><td>0.753</td><td>0.604</td></tr><tr><td>CLAN</td><td>0.478</td><td>0.318</td><td>0.787</td><td>0.649</td></tr></table>\n\nTable 3: Percentage of unlabeled users in each of the methods.",
        "In Table 3, we reported the percentage of the users who were left unlabeled or put in insignificant communities in each of the methods from the two datasets.",
        "As expected, CESNA would have many red nodes as this method tends to have a very low recall value and as shown in Table 3, this method has a high tendency to exclude many users by not assigning them a label."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/b7c795d98f2468f702c8259bd31f6a8466abe659bfd4112d57d6d2e75ed7ea0f.jpg",
      "image_filename": "b7c795d98f2468f702c8259bd31f6a8466abe659bfd4112d57d6d2e75ed7ea0f.jpg",
      "caption": "Table 2: The quantitative results obtained from calculating the F1 and Jaccard similarity scores with regards to the ground truth labels for each of the methods.",
      "context_before": "7 COMMUNITY DETECTION RESULTS\n\nOur goal is to report quantitative and qualitative results obtained through different experimentation in this paper. Hence, we use different visualizations and examples from datasets in hand, in addition to our numerical results to give the reader a better intuition of how our method, CLAN, performs compared to the existing state of the art.\n\n[Section: Debiasing Community Detection: The Importance of Lowly-Connected Nodes]",
      "context_after": "",
      "referring_paragraphs": [
        "This dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user. This dataset comes from [1] in which we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label propagation. The network structure of this da",
        "After obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement which will be discussed in detail in \"Community Detection Results\" section. Figure 2 confirms this fact by showing the disagreement between the left hand si",
        "7.1.1 Quantitative Results. In this section, we would report quantitative and numerical results from our experiments. We will first report the scores for the F1 and Jaccard similarity scores between the ground truth labels and three different methods. We will show that CLAN outperforms other methods in terms of F1 and Jaccard similarity scores for both of the datasets. Table 2 contains results for the F1 and Jaccard similarity scores obtained from comparisons done between the ground truth labels",
        "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.",
        "The network structure of this dataset is shown in Figure 2.",
        "Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.",
        "Table 2: The quantitative results obtained from calculating the F1 and Jaccard similarity scores with regards to the ground truth labels for each of the methods.",
        "Table 2 contains results for the F1 and Jaccard similarity scores obtained from comparisons done between the ground truth labels and labels obtained by applying each of the methods on the two datasets on hand."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/22a7d876fc6951488d2dea3b400601ebb0f92352878f4226647f3417a823771d.jpg",
      "image_filename": "22a7d876fc6951488d2dea3b400601ebb0f92352878f4226647f3417a823771d.jpg",
      "caption": "Table 3: Percentage of unlabeled users in each of the methods.",
      "context_before": "",
      "context_after": "7.1 Evaluation Metrics\n\nFor evaluation purposes, the F1 and Jaccard similarity scores are calculated for each of the datasets with respect to the ground truth labels. These scores are the average values of the total communities found in each of the datasets.\n\n7.1.1 Quantitative Results. In this section, we would report quantitative and numerical results from our experiments. We will first report the scores for the F1 and Jaccard similarity scores between the ground truth labels and three different methods. We will show that CLAN outperforms other methods in terms of F1 and Jaccard similarity scores for both of the datasets. Table 2 contains results for the F1 and Jaccard similarity scores obtained from comparisons done between the ground truth labels and labels obtained by applying each of the methods on the two datasets on hand.",
      "referring_paragraphs": [
        "In this study, we conducted an Amazon Mechanical Turk experiment discussed later in the paper to obtain ground truth labels for each of the users in this controversy. The retweet network of this dataset is shown in Figure 3.",
        "that were labeled by each of the methods. These numbers show the number of users that the method has excluded by not labeling them. This exclusion shows the bias of the method towards those users. Therefore, the more unlabeled users a method has the more susceptible to bias it is. In Table 3, we reported the percentage of the users who were left unlabeled or put in insignificant communities in each of the methods from the two datasets. These results confirm the fact that our method, CLAN, has mi",
        "showing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network in the Gamergate dataset in the three methods discussed in this paper. Each node in these graphs represent a user and the nodes are color coded based on their agreement with the ground truth labels. The green nodes represent agreement between the label that was assigned to that ",
        "The retweet network of this dataset is shown in Figure 3.",
        "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.",
        "Presidential Election Dataset</td></tr><tr><td>Method</td><td>F1 Score</td><td>Jaccard</td><td>F1 Score</td><td>Jaccard</td></tr><tr><td>CESNA</td><td>0.343</td><td>0.211</td><td>0.253</td><td>0.149</td></tr><tr><td>Modularity</td><td>0.434</td><td>0.282</td><td>0.753</td><td>0.604</td></tr><tr><td>CLAN</td><td>0.478</td><td>0.318</td><td>0.787</td><td>0.649</td></tr></table>\n\nTable 3: Percentage of unlabeled users in each of the methods.",
        "In Table 3, we reported the percentage of the users who were left unlabeled or put in insignificant communities in each of the methods from the two datasets.",
        "As expected, CESNA would have many red nodes as this method tends to have a very low recall value and as shown in Table 3, this method has a high tendency to exclude many users by not assigning them a label."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig3.jpg",
      "image_filename": "1903.08136_page0_fig3.jpg",
      "caption": "CLAN",
      "context_before": "that were labeled by each of the methods. These numbers show the number of users that the method has excluded by not labeling them. This exclusion shows the bias of the method towards those users. Therefore, the more unlabeled users a method has the more susceptible to bias it is. In Table 3, we reported the percentage of the users who were left unlabeled or put in insignificant communities in each of the methods from the two datasets. These results confirm the fact that our method, CLAN, has mitigated the bias towards the introverted users by incorporating them into the significant communities which would not be excluded from various down stream data analysis tasks.\n\nThe two sets of results reported in Tables 2 and 3 confirm that not only is our method able to achieve superior predictive accuracy but also mitigate bias against introverted users by assigning them labels preventing them from exclusion.\n\n7.1.2 Qualitative Results. Besides the numerical results reported in the previous subsection, we want to further our analysis by",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig4.jpg",
      "image_filename": "1903.08136_page0_fig4.jpg",
      "caption": "Modularity",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg",
      "image_filename": "1903.08136_page0_fig5.jpg",
      "caption": "CESNA Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.",
      "context_before": "",
      "context_after": "[Section: Mehrabi, et al.]",
      "referring_paragraphs": [
        "showing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network in the Gamergate dataset in the three methods discussed in this paper. Each node in these graphs represent a user and the nodes are color coded based on their agreement with the ground truth labels. The green nodes represent agreement between the label that was assigned to that ",
        "This also confirms the existence of bias towards these red users who suffered from CESNA’s low recall issue. The result associated to this method is shown on the far right side of Figure 4. As we move to the next method in the middle of Figure 4, we see less red nodes. This is because using modularity value has a higher recall and generally more agreement with the ground truth, but one can still spot many red users in this method. Moving on to the last graph on the far left side of Figure 4, we ",
        "CESNA   \nFigure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/67abe1c8046489f75d519117897619ab15a0c42ff9e34e97869975f15871042e.jpg",
      "image_filename": "67abe1c8046489f75d519117897619ab15a0c42ff9e34e97869975f15871042e.jpg",
      "caption": "showing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network",
      "context_before": "[Section: Mehrabi, et al.]",
      "context_after": "showing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network in the Gamergate dataset in the three methods discussed in this paper. Each node in these graphs represent a user and the nodes are color coded based on their agreement with the ground truth labels. The green nodes represent agreement between the label that was assigned to that particular user obtained from the method used and the ground truth label, and the red node represent disagreement between the two labels associated to that node. Therefore, more green nodes in a graph represent the degree of agreement of that method with the ground truth label and generally its superiority in terms of agreement with the ground truth compared to the other methods. The results of these visualizations are shown in Figure 4. In addition to disagreement, the red nodes may also represent the fact that a method has low recall value and that many users were assigned no labels while the ground truth has assigned it a label. This of course is a sort of disagreement between the labels, so the nodes are colored as red. As expected, CESNA would have many red nodes as this method tends to have a very low recall value and as shown in Table 3, this method has a high tendency to exclude many users by not assigning them a label. Therefore, it suffers from low agreement with the ground truth and as expected highly covered by red nodes.\n\nThis also confirms the existence of bias towards these red users who suffered from CESNA’s low recall issue. The result associated to this method is shown on the far right side of Figure 4. As we move to the next method in the middle of Figure 4, we see less red nodes. This is because using modularity value has a higher recall and generally more agreement with the ground truth, but one can still spot many red users in this method. Moving on to the last graph on the far left side of Figure 4, we can see the graph associated to CLAN. Due to CLAN’s use of node attributes in addition to the network attributes, it is able to obtain higher agreement with the ground truth and therefore less red nodes compared to the previous method. CLAN was able to address many red nodes in the top portion as well as the bottom portion of the graph compared to the modularity method. In addition to the agreement graphs shown in Figure 4, we provided two sets of tables, Table 4 and 5, in which some examples from each of the datasets are provided where it shows how each of the methods labeled a particular user with sets of tweets they tweeted. The ground truth labels are also listed for comparison purposes. Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset. These examples also highlight the fact that the baseline approaches are suffering from the type of bias that roots from exclusion of users by not assigning",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/7be6a544da51a0c31481f3ce3d211ade48aea78a6aba7457991861050f123558.jpg",
      "image_filename": "7be6a544da51a0c31481f3ce3d211ade48aea78a6aba7457991861050f123558.jpg",
      "caption": "Debiasing Community Detection: The Importance of Lowly-Connected Nodes",
      "context_before": "showing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network in the Gamergate dataset in the three methods discussed in this paper. Each node in these graphs represent a user and the nodes are color coded based on their agreement with the ground truth labels. The green nodes represent agreement between the label that was assigned to that particular user obtained from the method used and the ground truth label, and the red node represent disagreement between the two labels associated to that node. Therefore, more green nodes in a graph represent the degree of agreement of that method with the ground truth label and generally its superiority in terms of agreement with the ground truth compared to the other methods. The results of these visualizations are shown in Figure 4. In addition to disagreement, the red nodes may also represent the fact that a method has low recall value and that many users were assigned no labels while the ground truth has assigned it a label. This of course is a sort of disagreement between the labels, so the nodes are colored as red. As expected, CESNA would have many red nodes as this method tends to have a very low recall value and as shown in Table 3, this method has a high tendency to exclude many users by not assigning them a label. Therefore, it suffers from low agreement with the ground truth and as expected highly covered by red nodes.\n\nThis also confirms the existence of bias towards these red users who suffered from CESNA’s low recall issue. The result associated to this method is shown on the far right side of Figure 4. As we move to the next method in the middle of Figure 4, we see less red nodes. This is because using modularity value has a higher recall and generally more agreement with the ground truth, but one can still spot many red users in this method. Moving on to the last graph on the far left side of Figure 4, we can see the graph associated to CLAN. Due to CLAN’s use of node attributes in addition to the network attributes, it is able to obtain higher agreement with the ground truth and therefore less red nodes compared to the previous method. CLAN was able to address many red nodes in the top portion as well as the bottom portion of the graph compared to the modularity method. In addition to the agreement graphs shown in Figure 4, we provided two sets of tables, Table 4 and 5, in which some examples from each of the datasets are provided where it shows how each of the methods labeled a particular user with sets of tweets they tweeted. The ground truth labels are also listed for comparison purposes. Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset. These examples also highlight the fact that the baseline approaches are suffering from the type of bias that roots from exclusion of users by not assigning",
      "context_after": "[Section: Debiasing Community Detection: The Importance of Lowly-Connected Nodes]",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig6.jpg",
      "image_filename": "1903.08136_page0_fig6.jpg",
      "caption": "Debiasing Community Detection: The Importance of Lowly-Connected Nodes",
      "context_before": "[Section: Debiasing Community Detection: The Importance of Lowly-Connected Nodes]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig7.jpg",
      "image_filename": "1903.08136_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg",
      "image_filename": "1903.08136_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig9.jpg",
      "image_filename": "1903.08136_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_16",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig10.jpg",
      "image_filename": "1903.08136_page0_fig10.jpg",
      "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.",
      "context_before": "",
      "context_after": "them any labels or putting them into insignificant communities that would be excluded.\n\nThe qualitative results reported in this subsection also confirm the fact that the baseline methods have low agreement with the ground truth labels and suffer from bias towards low-degree and some users who are excluded from being labeled. The results from this and previous subsection also show the superiority of our method in terms of addressing these issues through various examples provided.\n\n8 CLAN’S RESILIENCE TO SKEWED DATA",
      "referring_paragraphs": [
        "This also confirms the existence of bias towards these red users who suffered from CESNA’s low recall issue. The result associated to this method is shown on the far right side of Figure 4. As we move to the next method in the middle of Figure 4, we see less red nodes. This is because using modularity value has a higher recall and generally more agreement with the ground truth, but one can still spot many red users in this method. Moving on to the last graph on the far left side of Figure 4, we ",
        "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network representation. Figure 6 contains the same graphs and networks for the U.S. Presidential Election dataset.",
        "Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset.",
        "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig11.jpg",
      "image_filename": "1903.08136_page0_fig11.jpg",
      "caption": "Mehrabi, et al.",
      "context_before": "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network representation. Figure 6 contains the same graphs and networks for the U.S. Presidential Election dataset.\n\nThe results for the Gamergate and U.S. Presidential Election datasets are shown in Figures 5 and 6 respectively. The graphs located in the top left corner of the figures show the Jaccard similarity scores for each of the distributional settings, and the graph on the bottom left corner contains the results for the F1 scores. The networks shown under each of the slope values in Figures 5 and 6 are the network of the users in the new distributional environments that have that particular slope range values. This confirms the fact that under different degree distributional settings CLAN can have reasonable and superior performance over the state of the art methods.\n\n[Section: Mehrabi, et al.]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig12.jpg",
      "image_filename": "1903.08136_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg",
      "image_filename": "1903.08136_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig14.jpg",
      "image_filename": "1903.08136_page0_fig14.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_21",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig15.jpg",
      "image_filename": "1903.08136_page0_fig15.jpg",
      "caption": "Degree Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.",
      "context_before": "",
      "context_after": "9 CONCLUSIONS AND FUTURE WORK\n\nIn this paper, we introduce a new community detection method that mitigates the bias in existing detection methods that fail to properly account for sparsely-connected nodes in social networks. CLAN minimizes such biases by including the lowly-connected nodes into their true communities. Our empirical results demonstrate that inclusion of those users enables CLAN to achieve overall superior performance in terms of F1-score and Jaccard similarity. We reported these results by providing evidence through our qualitative and quantitative experiments. Through qualitative analysis, we are able to show that these lowly-connected users, in aggregate, offer information that can be of use for analysis of social network data.\n\nFinally, we show that our method is capable of outperforming other methods not only in real datasets but also in different types of synthetic environments with different population distributions, namely distributions where the users community is correlated with their connectivity. The results reported the performance of methods with regards to F1 and Jaccard similarity scores.",
      "referring_paragraphs": [
        "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network representation. Figure 6 contains the same graphs and networks for the U.S. Presidential Election dataset.",
        "Degree   \nFigure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.08136_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1903.10561": [
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/5dea010797d851115cf930abfa604dd292de67088759d862a96f682b16295ba2.jpg",
      "image_filename": "5dea010797d851115cf930abfa604dd292de67088759d862a96f682b16295ba2.jpg",
      "caption": "Table 1: Subsets of target concepts and attributes from Caliskan Test 3. Concept and attribute names are in italics. The test compares the strength of association between the two target concepts and two attributes, where all four are represented as sets of words.",
      "context_before": "We find varying evidence of human-like bias in sentence encoders using SEAT. Sentence-tovector encoders largely exhibit the angry black woman stereotype and Caliskan biases, and to a lesser degree the double bind biases. Recent sentence encoders such as BERT (Devlin et al., 2018) display limited evidence of the tested biases. However, while SEAT can confirm the existence of bias, negative results do not indicate the model is bias-free. Furthermore, discrepancies in the results suggest that the confirmed biases may not generalize beyond the specific words and sentences in our test data, and in particular that cosine similarity may not be a suitable measure of representational similarity in recent models, indicating a need for alternate bias detection techniques.\n\narXiv:1903.10561v1 [cs.CL] 25 Mar 2019\n\n1 While encoder training data may contain perspectives from outside the U.S., we focus on biases in U.S. contexts.",
      "context_after": "",
      "referring_paragraphs": [
        "To extend a word-level test to sentence contexts, we slot each word into each of several semantically bleached sentence templates such as “This is <word>.”, “<word> is here.”, “This will <word>.”, and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th",
        "To measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype in Collins (2004, pp. 87-90) and their antonyms. We also produce a version of the test with attributes consisting of terms describing black women and white women as groups, as well as sentence versions in which a",
        "We also provide a visualization of our results: Figure 1 depicts the significant results in our matrix of models and bias tests.",
        "Table 1: Subsets of target concepts and attributes from Caliskan Test 3.",
        "These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2.",
        "To measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype in Collins (2004, pp.",
        "(2017, Table 1) row $N$ ; *: significant at 0.01, **: significant at 0.01 after multiple testing correction.",
        "Figure 1: Significance of results for all models and tests."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10561_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/c4cbf72f8ad41e4073c0e73a086159d8ca9a9bb59b5ca0d357e71ad268de1ec1.jpg",
      "image_filename": "c4cbf72f8ad41e4073c0e73a086159d8ca9a9bb59b5ca0d357e71ad268de1ec1.jpg",
      "caption": "Table 2: Subsets of target concepts and attributes from the bleached sentence version of Caliskan Test 3.",
      "context_before": "",
      "context_after": "The Word Embedding Association Test WEAT imitates the human implicit association test (Greenwald et al., 1998) for word embeddings, measuring the association between two sets of target concepts and two sets of attributes. Let $X$ and $Y$ be equal-size sets of target concept embeddings and let $A$ and $B$ be sets of attribute embeddings. The test statistic is a difference between sums over the respective target concepts,\n\n$$ \\begin{array}{l} s (X, Y, A, B) = \\left[ \\sum_ {x \\in X} s (x, A, B) - \\right. \\\\ \\left. \\sum_ {y \\in Y} s (y, A, B) \\right], \\\\ \\end{array} $$\n\nwhere each addend is the difference between mean cosine similarities of the respective attributes,",
      "referring_paragraphs": [
        "To extend a word-level test to sentence contexts, we slot each word into each of several semantically bleached sentence templates such as “This is <word>.”, “<word> is here.”, “This will <word>.”, and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th",
        "<table><tr><td>Target Concepts</td><td>Attributes</td></tr><tr><td>European American names: \nAdam, Harry, Nancy, Ellen, \nAlan, Paul, Katie, ...</td><td>Pleasant: love, cheer, \nmiracle, peace, friend, \nhappy, ...</td></tr><tr><td>African American names: \nJamel, Lavar, Lavon, Tia, \nLatisha, Malika, ...</td><td>Unpleasant: ugly, evil, \nabuse, murder, assault, \nrotten, ...</td></tr></table>\n\nTable 2: Subsets of target concepts and attributes from the bleached sentence version of Caliskan Test 3.",
        "These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10561_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/37249aa2222e5ddf3f8713f9ccf4c139757b5b0e8bf15bb4440dca6dbd583c0b.jpg",
      "image_filename": "37249aa2222e5ddf3f8713f9ccf4c139757b5b0e8bf15bb4440dca6dbd583c0b.jpg",
      "caption": "Table 3: Models tested (disambiguated with notation from cited paper), aggregation functions applied across token representations, and representation dimensions.",
      "context_before": "To measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype in Collins (2004, pp. 87-90) and their antonyms. We also produce a version of the test with attributes consisting of terms describing black women and white women as groups, as well as sentence versions in which attribute and target concept terms are inserted in sentence templates.\n\nDouble Binds Women face many double binds, contradictory or unsatisfiable expectations of femininity and masculinity (Stone and Lovejoy, 2004; Harris-Perry, 2011; Mitchell, 2012). If women clearly succeed in a male gender-typed job, they are perceived less likable and more hostile than men in similar positions; if success is ambiguous, they are perceived less competent and achievement-oriented than men. Both outcomes can interfere in performance evaluations (Heilman et al., 2004), contributing to the glass ceiling impeding women’s career advancement.5\n\nWe test this double bind in sentence encoders by translating Heilman et al.’s experiment to two SEAT tests. In the first, we represent the two tar-",
      "context_after": "get concepts by names of women and men, respectively, in the single sentence template “<word> is an engineer with superior technical skills.”; the attributes are likable and non-hostile terms, based on Heilman et al.’s design, in the sentence template “The engineer is <word>.” In the second, we use the shortened target concept sentence template “<word> is an engineer” and fill the attribute templates from before with competent and achievement-oriented terms based on Heilman et al.’s design.6 We refer to these tests as semantically unbleached because the context contains important information about the bias. We produce two variations of these tests: word-level tests in which target concepts are names in isolation and attributes are adjectives in isolation, as well as corresponding semantically bleached sentence-level tests. These control conditions allow us to probe the extent to which observed associations are attributable to gender independent of context.\n\n4 Experiments and Results\n\nWe apply SEAT to seven sentence encoders (listed in Table 3) including simple bag-of-words encoders, sentence-to-vector models, and state-ofthe-art sequence models.7 For all models, we use publicly available pretrained parameters.",
      "referring_paragraphs": [
        "We apply SEAT to seven sentence encoders (listed in Table 3) including simple bag-of-words encoders, sentence-to-vector models, and state-ofthe-art sequence models.7 For all models, we use publicly available pretrained parameters.",
        "In the first, we represent the two tar-\n\nTable 3: Models tested (disambiguated with notation from cited paper), aggregation functions applied across token representations, and representation dimensions."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10561_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/7b5162eba12253d38dfc2abea3431d050d19014fcf512240b573e0eb1c8fdfd7.jpg",
      "image_filename": "7b5162eba12253d38dfc2abea3431d050d19014fcf512240b573e0eb1c8fdfd7.jpg",
      "caption": "Table 4: SEAT effect sizes for select tests, including word-level (word), bleached sentence-level (sent), and unbleached sentence-level (sent (u)) versions. $\\mathrm { C } N$ : test from Caliskan et al. (2017, Table 1) row $N$ ; *: significant at 0.01, **: significant at 0.01 after multiple testing correction.",
      "context_before": "7 We provide further details and explore variations on these model configurations in the supplement.\n\n8 We use the full set of tests and models when comput-\n\n5 See Schluter (2018) for a recent exposition of the glass ceiling in the NLP research community.",
      "context_after": "Specifically, we select Caliskan Test 1 associating flowers/insects with pleasant/unpleasant, Test 3 associating European/African American names with pleasant/unpleasant, and Test 6 associating male/female names with career/family, as well as the angry black woman stereotype and the competent and likable double bind tests. We observe that tests based on given names more often find a significant association than those based on group terms; we only show the given-name results here.\n\nWe find varying evidence of bias in sentence encoders according to these tests. Bleached sentence-level tests tend to elicit more significant associations than word-level tests, while the latter tend to have larger effect sizes. We find stronger evidence for the Caliskan and ABW stereotype tests than for the double bind. After the multiple testing correction, we only find evidence of the double bind in bleached, sentence-level competent control tests; that is, we find women are associated with incompetence independent of context.9\n\nSome patterns in the results cast doubt on the reasonableness of SEAT as an evaluation. For instance, Caliskan Test 7 (association between math/art and male/female) and Test 8 (science/art and male/female) elicit counterintuitive results from several models. These tests have the same sizes of target concept and attribute sets. For CBoW on the word versions of those tests, we see $p$ -values of 0.016 and $1 0 ^ { - 2 }$ , respectively.",
      "referring_paragraphs": [
        "Table 4 shows effect size and significance at 0.01 before and after applying the Holm-Bonferroni multiple testing correction (Holm, 1979) for a subset of tests and models; complete results are provided in the supplement.8",
        "Table 4 shows effect size and significance at 0.01 before and after applying the Holm-Bonferroni multiple testing correction (Holm, 1979) for a subset of tests and models; complete results are provided in the supplement.8\n\nTable 4: SEAT effect sizes for select tests, including word-level (word), bleached sentence-level (sent), and unbleached sentence-level (sent (u)) versions."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10561_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/206a5b16daa64b37180e45b22f09f7ce211b9a3eeb520dadd55591a126ab5deb.jpg",
      "image_filename": "206a5b16daa64b37180e45b22f09f7ce211b9a3eeb520dadd55591a126ab5deb.jpg",
      "caption": "Table 5: Names and descriptions of columns in results.tsv.",
      "context_before": "A full set of results is provided in the included tab-separated value (TSV) file, results.tsv, of the supplementary data. This file has nine columns; the first row is a header containing the names of the columns, as described in Table 5.\n\nThe Holm-Bonferroni multiple testing correction applied in the paper is computed over all rows in this file (except the header), as follows. Let $n$ be the number of rows. Sort the rows by $p { \\cdot }$ - value in increasing order. Let $P _ { ( r ) }$ be the $p$ -value at rank $r$ in the sorted list and let $H _ { ( r ) }$ be the corresponding (null) hypothesis, such that $r = 1$ for the first (smallest) $p$ -value and $ { \\boldsymbol { r } } \\equiv n$ for the last (largest) $p$ -value. Given a significance level $\\alpha$ (in our case $\\alpha ~ = ~ 0 . 0 1$ ), find the smallest rank $k$ such that $P _ { ( k ) } > \\alpha / ( 1 + n - k )$ , reject $H _ { ( 1 ) } , \\ldots , H _ { ( k - 1 ) }$ at significance level $\\alpha$ and do not reject $H _ { ( k ) } , \\ldots , H _ { ( n ) }$ (Holm, 1979).\n\nWe also provide a visualization of our results: Figure 1 depicts the significant results in our matrix of models and bias tests.",
      "context_after": "",
      "referring_paragraphs": [
        "A full set of results is provided in the included tab-separated value (TSV) file, results.tsv, of the supplementary data. This file has nine columns; the first row is a header containing the names of the columns, as described in Table 5.",
        "This file has nine columns; the first row is a header containing the names of the columns, as described in Table 5."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10561_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_6",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg",
      "image_filename": "1903.10561_page0_fig0.jpg",
      "caption": "Figure 1: Significance of results for all models and tests.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "To extend a word-level test to sentence contexts, we slot each word into each of several semantically bleached sentence templates such as “This is <word>.”, “<word> is here.”, “This will <word>.”, and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose th",
        "To measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype in Collins (2004, pp. 87-90) and their antonyms. We also produce a version of the test with attributes consisting of terms describing black women and white women as groups, as well as sentence versions in which a",
        "We also provide a visualization of our results: Figure 1 depicts the significant results in our matrix of models and bias tests.",
        "Table 1: Subsets of target concepts and attributes from Caliskan Test 3.",
        "These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2.",
        "To measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype in Collins (2004, pp.",
        "(2017, Table 1) row $N$ ; *: significant at 0.01, **: significant at 0.01 after multiple testing correction.",
        "Figure 1: Significance of results for all models and tests."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10561_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1903.10598": [
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg",
      "image_filename": "1903.10598_page0_fig0.jpg",
      "caption": "Family·MIP △DADT■CART•logistic",
      "context_before": "Fairness and Interpretability. Figures 2(a)-(b) show how the MIP objective and the accuracy and fairness values change in dependence of tree depth (a proxy for interpretability) on a fold from the Adult dataset. Such graphs can help non-technical decision-makers understand the trade-offs between fairness, accuracy, and interpretability. Figure 2(d) shows that the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART on the same dataset: this is in line with our metric – in this experiment, DTDI was $0 . 3 2 \\%$ $( 0 . 7 \\% )$ for MIP (CART).\n\nSolution Times Discussion. As seen, our approaches exhibit better performance but higher training computational cost. We emphasize that training decision-support systems for socially sensitive tasks is usually not time sensitive. At the same time, predicting the outcome of a new (unseen) sample with our approach, which is time-sensitive, is extremely fast (in the order of milliseconds). In addition, as seen in Figure 2(c), a near optimal solution is typically found very rapidly (these are results from a fold from the Adult dataset).\n\n1We modeled the MIP using JuMP in Julia (Dunning, Huchette, and Lubin 2017) and solved it using Gurobi 7.5.2 on a computer node with 20 CPUs and 64 GB of RAM. We imposed a 5 (10) hour solve time limit for classification (regression).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig1.jpg",
      "image_filename": "1903.10598_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig2.jpg",
      "image_filename": "1903.10598_page0_fig2.jpg",
      "caption": "IGCS CARTlog-ind",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg",
      "image_filename": "1903.10598_page0_fig3.jpg",
      "caption": "-AIGCS-RLblog log-grp ·MIP △CART ■Regression MIP-DTCARTregLR-indLR-grp Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Fairness and Accuracy. In all our experiments, we use $\\mathsf { D T D l } _ { \\mathrm { c / r } }$ as the discrimination index. First, we investigate the fairness/accuracy trade-off of all methods by evaluating the performance of the most accurate models with low discrimination. We do $k$ -fold cross validation where for classification (regression) $k$ is 5(4). For each (fold, approach) pair, we select the optimal $\\lambda$ (call it $\\lambda ^ { \\star }$ ) in the objective (6) as follows: for e",
        "0 1 \\%$ and return $\\lambda$ as $\\lambda ^ { \\star }$ ; we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with $\\lambda ^ { \\star }$ on the test set and add this as a point in the corresponding graph in Figure 1.",
        "-AIGCS-RLblog log-grp   \n·MIP △CART ■Regression MIP-DTCARTregLR-indLR-grp   \nFigure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig4.jpg",
      "image_filename": "1903.10598_page0_fig4.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg",
      "image_filename": "1903.10598_page0_fig5.jpg",
      "caption": "(b)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig6.jpg",
      "image_filename": "1903.10598_page0_fig6.jpg",
      "caption": "(c)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_8",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig7.jpg",
      "image_filename": "1903.10598_page0_fig7.jpg",
      "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Fairness and Interpretability. Figures 2(a)-(b) show how the MIP objective and the accuracy and fairness values change in dependence of tree depth (a proxy for interpretability) on a fold from the Adult dataset. Such graphs can help non-technical decision-makers understand the trade-offs between fairness, accuracy, and interpretability. Figure 2(d) shows that the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same wa",
        "Solution Times Discussion. As seen, our approaches exhibit better performance but higher training computational cost. We emphasize that training decision-support systems for socially sensitive tasks is usually not time sensitive. At the same time, predicting the outcome of a new (unseen) sample with our approach, which is time-sensitive, is extremely fast (in the order of milliseconds). In addition, as seen in Figure 2(c), a near optimal solution is typically found very rapidly (these are result",
        "Figure 2(d) shows that the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART on the same dataset: this is in line with our metric – in this experiment, DTDI was $0 .",
        "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg",
      "image_filename": "1903.10598_page0_fig8.jpg",
      "caption": "(a)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_10",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig9.jpg",
      "image_filename": "1903.10598_page0_fig9.jpg",
      "caption": "(b) Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.",
      "context_before": "",
      "context_after": "The authors gratefully acknowledge support from Schmidt Futures and from the James H. Zumberge Faculty Research and Innovation Fund at the University of Southern California. They thank the 6 anonymous referees whose reviews helped substantially improve the quality of the paper.\n\nAdler, P.; Falk, C.; Friedler, S. A.; Nix, T.; Rybeck, G.; Scheidegger, C.; Smith, B.; and Venkatasubramanian, S. 2018. Auditing black-box models for indirect influence. Knowl. Inf. Syst. 54(1):95–122.\n\nAltman, A. 2016. Discrimination. In Zalta, E. N., ed., The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, winter 2016 edition.",
      "referring_paragraphs": [
        "Fairness and Accuracy. In all our experiments, we use $\\mathsf { D T D l } _ { \\mathrm { c / r } }$ as the discrimination index. First, we investigate the fairness/accuracy trade-off of all methods by evaluating the performance of the most accurate models with low discrimination. We do $k$ -fold cross validation where for classification (regression) $k$ is 5(4). For each (fold, approach) pair, we select the optimal $\\lambda$ (call it $\\lambda ^ { \\star }$ ) in the objective (6) as follows: for e",
        "Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.",
        "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.10598_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1903.12262": [
    {
      "doc_id": "1903.12262",
      "figure_id": "1903.12262_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.12262/1903.12262/hybrid_auto/images/d501bdf57dc2496608a0e743129d2b25097c43497bedebc98f917848c705beb4.jpg",
      "image_filename": "d501bdf57dc2496608a0e743129d2b25097c43497bedebc98f917848c705beb4.jpg",
      "caption": "The Data itself Use of the Data in Conjunction with Models",
      "context_before": "collected and harnessed from different sources, or made available from a single-source. Data can be basic collated information (e.g. a range of measurements such as temperature, location) or be formed of more complex information (e.g. pictures, maps).\n\n3.2 NEW TAXONOMY DESCRIBING USE OF DATA IN AI AND ML\n\nAs described above, it is clear that use of data in ML and AI is an incremental process that reaches from extraction and refinement to actionable output. Along such value chain, different actions may be taken with data: it is used in different ways, for different purposes. The below cases illustrate how data is used within ML and AI. These use cases are the foundation of the Montreal Data License, the proposed text of which is found at Appendix 4. Where the definitions appear prescriptive, or be deliberately mutually exclusive, the reader should understand that this was done to clearly delineate use cases.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.12262_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.12262",
      "figure_id": "1903.12262_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.12262/1903.12262/hybrid_auto/images/8839c948dba1d59235513a63a2d43d0576296f389942588658a54ec862b8d08a.jpg",
      "image_filename": "8839c948dba1d59235513a63a2d43d0576296f389942588658a54ec862b8d08a.jpg",
      "caption": "3.3 ILLUSTRATION OF HOW THE TAXONOMY IS APPLIED",
      "context_before": "",
      "context_after": "3.3 ILLUSTRATION OF HOW THE TAXONOMY IS APPLIED\n\nIn order to illustrate the above uses, we can take the example of a database of historical equities trades (prices, volumes etc.).\n\nWith the right to Evaluate models, a licensee working on a trading models (the Model) would be able to take any existing (untrained/pretrained) versions of the Model it has and train them on all or part of that data in order to test its performance (i.e. the quality of the Output) on historical data. They could also test a variety of different Models in this way in order to choose which is the best for given circumstances. They could also iteratively make changes to the architecture/code of the Model, and test how those different iterations perform, and would not have any restrictions on using those versions of the of the Model. However, they could not use any of the Output to inform any stock trades, nor reuse the Model as modified by access to the Data (ex. the weights of the Trained Model would have to be deleted).",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.12262_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.12262",
      "figure_id": "1903.12262_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.12262/1903.12262/hybrid_auto/images/a065d46c5636b84cb5310416902edc75efa2e981fe6fd98fdf183eeb5088a72f.jpg",
      "image_filename": "a065d46c5636b84cb5310416902edc75efa2e981fe6fd98fdf183eeb5088a72f.jpg",
      "caption": "Overview of commonly used datasets",
      "context_before": "This paper aimed to bring a step forward to bring about this clarity from a legal standpoint by providing a licensing framework anchored in practical realities of ML and AI. The goal is ambitious: providing a common frame of reference to create standards for data licensing to compare with those found in open source software. When combined with recent works such as the two-sided markets conceptualized by Agarwal, et al.(Agarwal et al., 2018), one can construct the foundation of markets for data that can help foster transparency, algorithmic fairness and fair market dynamics.\n\nA small step towards this is the creation of the Montreal Data License (MDL) a modular approach to data licensing in AI and ML that the authors hope will break ground and be adopted by the AI and ML communities. Interested readers may wish to send feedback on this paper, the license language and the License Generator tool by writing via email to info@montrealdatalicense.com.\n\nAnish Agarwal, Munther Dahleh, and Tuhin Sarkar. A marketplace for data: An algorithmic solution. arXiv preprint arXiv:1805.08125, 2018.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.12262_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.12262",
      "figure_id": "1903.12262_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.12262/1903.12262/hybrid_auto/images/6f05a41aa939bb03eea37a7f716728f24e71c356e904d7c9f8f846a3ecd7c21a.jpg",
      "image_filename": "6f05a41aa939bb03eea37a7f716728f24e71c356e904d7c9f8f846a3ecd7c21a.jpg",
      "caption": "Summary of rights granted in conjunction with Models",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.12262_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1903.12262",
      "figure_id": "1903.12262_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.12262/1903.12262/hybrid_auto/images/4a57305616adfee2ab4d301631f60c7870e39813909d0c044ae49cbb3504d826.jpg",
      "image_filename": "4a57305616adfee2ab4d301631f60c7870e39813909d0c044ae49cbb3504d826.jpg",
      "caption": "Top Sheet for Licensed Rights",
      "context_before": "",
      "context_after": "Appendix 4: Licence Language\n\nThe following licensing language is made available under CC-BY4. Attribution should be made to Montreal Data License (MDL), or License language based on Montreal Data License.\n\nThe authors are not legal advisors to the individuals and entities making use of these licensing terms. The licensing terms can be combined as needed to match the rights conferred by the licensor.",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1903.12262_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1904.03035": [
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg",
      "image_filename": "1904.03035_page0_fig0.jpg",
      "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.",
      "context_before": "Towards this pursuit, we aim to evaluate the effect of gender bias on word-level language models that are trained on a text corpus. Our contributions in this work include: (i) an analysis of the gender bias exhibited by publicly available datasets used in building state-of-the-art language models; (ii) an analysis of the effect of this bias on recurrent neural networks (RNNs) based word-level language models; (iii) a method for reducing bias learned in these models; and (iv) an analysis of the results of our method.\n\nA number of methods have been proposed for evaluating and addressing biases that exist in datasets and the models that use them. Recasens et al. (2013) studies the neutral point of view (NPOV) edit tags in the Wikipedia edit histories to understand linguistic realization of bias. According to their study, bias can be broadly categorized into two classes: framing and epistemological. While the framing bias is more explicit, the epistemological bias is implicit and subtle. Framing bias occurs when subjective or one-sided words are used. For example, in the\n\narXiv:1904.03035v1 [cs.CL] 5 Apr 2019",
      "context_after": "sentence—“Usually, smaller cottage-style houses have been demolished to make way for these Mc-Mansions.”, the word McMansions has a negative connotation towards large and pretentious houses. Epistemological biases are entailed, asserted or hedged in the text. For example, in the sentence—“Kuypers claimed that the mainstream press in America tends to favor liberal viewpoints,” the word claimed has a doubtful effect on Kuypers statement as opposed to stated in the sentence—“Kuypers stated that the mainstream press in America tends to favor liberal viewpoints.” It may be possible to capture both of these kinds of biases through the distributions of co-occurrences. In this paper, we deal with identifying and reducing gender bias based on words co-occurring in a context window.\n\nBolukbasi et al. (2016) propose an approach to investigate gender bias present in popular word embeddings, such as word2vec (Mikolov et al., 2013). They construct a gender subspace using a set of binary gender pairs. For words that are not explicitly gendered, the component of the word embeddings that project onto this subspace can be removed to debias the embeddings in the gender direction. They also propose a softer variation that balances reconstruction of the original embeddings while minimizing the part of the embeddings that project onto the gender subspace. We use the softer variation to debias the embeddings while training our language model.\n\nZhao et al. (2017) look at gender bias in the context of using structured prediction for visual object classification and semantic role labeling. They ob-",
      "referring_paragraphs": [
        "We first examine the bias existing in the datasets through qualitative and quantitative analysis of trained embeddings and cooccurrence patterns. We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs. As shown in Figure 1, we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender. We debias the input and the output embeddings individually as well as simultaneously. Finally, we as",
        "Figure 1: Word level language model is a three layer LSTM model.",
        "As shown in Figure 1, we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender.",
        "Table 1: Experimental results for Penn Treebank and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.83</td><td>1.00</td><td></td><td>3.81</td><td>4.65</td><td></td><td></td></tr><tr><td>0.0</td><td>0.74</td><td>0.91</td><td>0.40</td><td>2.23</td><td>2.90</td><td>0.38</td><td>62.56</td></tr><tr><td>0.001</td><td>0.69</td><td>0.88</td><td>0.34</td><td>2.43</td><td>2.98</td><td>0.35</td><td>62.69</td></tr><tr><td>0.01</td><td>0.63</td><td>0.81</td><td>0.31</td><td>2.56</td><td>3.40</td><td>0.36</td><td>62.83</td></tr><tr><td>0.1</td><td>0.64</td><td>0.82</td><td>0.33</td><td>2.30</td><td>3.09</td><td>0.24</td><td>62.48</td></tr><tr><td>0.5</td><td>0.70</td><td>0.91</td><td>0.39</td><td>2.91</td><td>3.76</td><td>0.38</td><td>62.5</td></tr><tr><td>0.8</td><td>0.76</td><td>0.96</td><td>0.45</td><td>3.43</td><td>4.06</td><td>0.26</td><td>63.36</td></tr><tr><td>1.0</td><td>0.84</td><td>0.94</td><td>0.38</td><td>2.42</td><td>3.02</td><td>-0.30</td><td>62.63</td></tr></table>\n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.80</td><td>1.00</td><td></td><td>3.70</td><td>4.60</td><td></td><td></td></tr><tr><td>0.0</td><td>0.70</td><td>0.84</td><td>0.29</td><td>3.48</td><td>4.29</td><td>0.15</td><td>67.67</td></tr><tr><td>0.001</td><td>0.69</td><td>0.84</td><td>0.27</td><td>2.32</td><td>3.12</td><td>0.16</td><td>67.84</td></tr><tr><td>0.01</td><td>0.61</td><td>0.79</td><td>0.20</td><td>1.88</td><td>2.69</td><td>0.14</td><td>67.78</td></tr><tr><td>0.1</td><td>0.65</td><td>0.82</td><td>0.24</td><td>2.26</td><td>3.11</td><td>0.06</td><td>67.89</td></tr><tr><td>0.5</td><td>0.70</td><td>0.88</td><td>0.31</td><td>2.25</td><td>3.17</td><td>0.20</td><td>69.07</td></tr><tr><td>0.8</td><td>0.65</td><td>0.84</td><td>0.28</td><td>2.07</td><td>2.98</td><td>0.18</td><td>69.36</td></tr><tr><td>1.0</td><td>0.74</td><td>0.92</td><td>0.27</td><td>2.32</td><td>3.21</td><td>-0.08</td><td>69.56</td></tr></table>\n\nTable 2: Experimental results for WikiText-2 and generated text for different $\\lambda$ values   \nTable 3: Experimental results for CNN/Daily Mail and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.72</td><td>0.94</td><td></td><td>0.77</td><td>1.05</td><td></td><td></td></tr><tr><td>0.0</td><td>0.51</td><td>0.68</td><td>0.22</td><td>0.43</td><td>0.59</td><td>0.29</td><td>118.01</td></tr><tr><td>0.1</td><td>0.38</td><td>0.52</td><td>0.19</td><td>0.85</td><td>1.38</td><td>0.22</td><td>116.49</td></tr><tr><td>0.5</td><td>0.34</td><td>0.48</td><td>0.14</td><td>0.79</td><td>1.31</td><td>0.20</td><td>116.19</td></tr><tr><td>0.8</td><td>0.40</td><td>0.56</td><td>0.19</td><td>0.96</td><td>1.57</td><td>0.23</td><td>121.00</td></tr><tr><td>1.0</td><td>0.62</td><td>0.83</td><td>0.21</td><td>1.71</td><td>2.65</td><td>0.31</td><td>120.55</td></tr></table>"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/94d9c131d5ddb637c4b80466a8814f442ba956f2af874db00857db3c78f7e724.jpg",
      "image_filename": "94d9c131d5ddb637c4b80466a8814f442ba956f2af874db00857db3c78f7e724.jpg",
      "caption": "Table 1: Experimental results for Penn Treebank and generated text for different $\\lambda$ values",
      "context_before": "All three baseline models achieve reasonable perplexities indicating them to be good proxies for standard language models.\n\n2https://github.com/BordiaS/language-model-bias\n\n3https://github.com/salesforce/awd-lstm-lm",
      "context_after": "",
      "referring_paragraphs": [
        "We first examine the bias existing in the datasets through qualitative and quantitative analysis of trained embeddings and cooccurrence patterns. We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs. As shown in Figure 1, we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender. We debias the input and the output embeddings individually as well as simultaneously. Finally, we as",
        "Figure 1: Word level language model is a three layer LSTM model.",
        "As shown in Figure 1, we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender.",
        "Table 1: Experimental results for Penn Treebank and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.83</td><td>1.00</td><td></td><td>3.81</td><td>4.65</td><td></td><td></td></tr><tr><td>0.0</td><td>0.74</td><td>0.91</td><td>0.40</td><td>2.23</td><td>2.90</td><td>0.38</td><td>62.56</td></tr><tr><td>0.001</td><td>0.69</td><td>0.88</td><td>0.34</td><td>2.43</td><td>2.98</td><td>0.35</td><td>62.69</td></tr><tr><td>0.01</td><td>0.63</td><td>0.81</td><td>0.31</td><td>2.56</td><td>3.40</td><td>0.36</td><td>62.83</td></tr><tr><td>0.1</td><td>0.64</td><td>0.82</td><td>0.33</td><td>2.30</td><td>3.09</td><td>0.24</td><td>62.48</td></tr><tr><td>0.5</td><td>0.70</td><td>0.91</td><td>0.39</td><td>2.91</td><td>3.76</td><td>0.38</td><td>62.5</td></tr><tr><td>0.8</td><td>0.76</td><td>0.96</td><td>0.45</td><td>3.43</td><td>4.06</td><td>0.26</td><td>63.36</td></tr><tr><td>1.0</td><td>0.84</td><td>0.94</td><td>0.38</td><td>2.42</td><td>3.02</td><td>-0.30</td><td>62.63</td></tr></table>\n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.80</td><td>1.00</td><td></td><td>3.70</td><td>4.60</td><td></td><td></td></tr><tr><td>0.0</td><td>0.70</td><td>0.84</td><td>0.29</td><td>3.48</td><td>4.29</td><td>0.15</td><td>67.67</td></tr><tr><td>0.001</td><td>0.69</td><td>0.84</td><td>0.27</td><td>2.32</td><td>3.12</td><td>0.16</td><td>67.84</td></tr><tr><td>0.01</td><td>0.61</td><td>0.79</td><td>0.20</td><td>1.88</td><td>2.69</td><td>0.14</td><td>67.78</td></tr><tr><td>0.1</td><td>0.65</td><td>0.82</td><td>0.24</td><td>2.26</td><td>3.11</td><td>0.06</td><td>67.89</td></tr><tr><td>0.5</td><td>0.70</td><td>0.88</td><td>0.31</td><td>2.25</td><td>3.17</td><td>0.20</td><td>69.07</td></tr><tr><td>0.8</td><td>0.65</td><td>0.84</td><td>0.28</td><td>2.07</td><td>2.98</td><td>0.18</td><td>69.36</td></tr><tr><td>1.0</td><td>0.74</td><td>0.92</td><td>0.27</td><td>2.32</td><td>3.21</td><td>-0.08</td><td>69.56</td></tr></table>\n\nTable 2: Experimental results for WikiText-2 and generated text for different $\\lambda$ values   \nTable 3: Experimental results for CNN/Daily Mail and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.72</td><td>0.94</td><td></td><td>0.77</td><td>1.05</td><td></td><td></td></tr><tr><td>0.0</td><td>0.51</td><td>0.68</td><td>0.22</td><td>0.43</td><td>0.59</td><td>0.29</td><td>118.01</td></tr><tr><td>0.1</td><td>0.38</td><td>0.52</td><td>0.19</td><td>0.85</td><td>1.38</td><td>0.22</td><td>116.49</td></tr><tr><td>0.5</td><td>0.34</td><td>0.48</td><td>0.14</td><td>0.79</td><td>1.31</td><td>0.20</td><td>116.19</td></tr><tr><td>0.8</td><td>0.40</td><td>0.56</td><td>0.19</td><td>0.96</td><td>1.57</td><td>0.23</td><td>121.00</td></tr><tr><td>1.0</td><td>0.62</td><td>0.83</td><td>0.21</td><td>1.71</td><td>2.65</td><td>0.31</td><td>120.55</td></tr></table>"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1e85a7f306b041de6cf3f591af1e0c15cecaf42b5fc25d46b9d39adf8afb79e7.jpg",
      "image_filename": "1e85a7f306b041de6cf3f591af1e0c15cecaf42b5fc25d46b9d39adf8afb79e7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/fc6125724a4e4f35ea5cc251298cd5979f751f0449552063dc1d24140f355f11.jpg",
      "image_filename": "fc6125724a4e4f35ea5cc251298cd5979f751f0449552063dc1d24140f355f11.jpg",
      "caption": "Table 2: Experimental results for WikiText-2 and generated text for different $\\lambda$ values Table 3: Experimental results for CNN/Daily Mail and generated text for different $\\lambda$ values",
      "context_before": "",
      "context_after": "3.3 Quantifying Biases\n\nFor numeric data, bias can be caused simply by class imbalance, which is relatively easy to quantify and fix. For text and image data, the complexity in the nature of the data increases and it becomes difficult to quantify. Nonetheless, defining relevant metrics is crucial in assessing the bias exhibited in a dataset or in a model’s behavior.\n\n3.3.1 Bias Score Definition",
      "referring_paragraphs": [
        "Table 1: Experimental results for Penn Treebank and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.83</td><td>1.00</td><td></td><td>3.81</td><td>4.65</td><td></td><td></td></tr><tr><td>0.0</td><td>0.74</td><td>0.91</td><td>0.40</td><td>2.23</td><td>2.90</td><td>0.38</td><td>62.56</td></tr><tr><td>0.001</td><td>0.69</td><td>0.88</td><td>0.34</td><td>2.43</td><td>2.98</td><td>0.35</td><td>62.69</td></tr><tr><td>0.01</td><td>0.63</td><td>0.81</td><td>0.31</td><td>2.56</td><td>3.40</td><td>0.36</td><td>62.83</td></tr><tr><td>0.1</td><td>0.64</td><td>0.82</td><td>0.33</td><td>2.30</td><td>3.09</td><td>0.24</td><td>62.48</td></tr><tr><td>0.5</td><td>0.70</td><td>0.91</td><td>0.39</td><td>2.91</td><td>3.76</td><td>0.38</td><td>62.5</td></tr><tr><td>0.8</td><td>0.76</td><td>0.96</td><td>0.45</td><td>3.43</td><td>4.06</td><td>0.26</td><td>63.36</td></tr><tr><td>1.0</td><td>0.84</td><td>0.94</td><td>0.38</td><td>2.42</td><td>3.02</td><td>-0.30</td><td>62.63</td></tr></table>\n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.80</td><td>1.00</td><td></td><td>3.70</td><td>4.60</td><td></td><td></td></tr><tr><td>0.0</td><td>0.70</td><td>0.84</td><td>0.29</td><td>3.48</td><td>4.29</td><td>0.15</td><td>67.67</td></tr><tr><td>0.001</td><td>0.69</td><td>0.84</td><td>0.27</td><td>2.32</td><td>3.12</td><td>0.16</td><td>67.84</td></tr><tr><td>0.01</td><td>0.61</td><td>0.79</td><td>0.20</td><td>1.88</td><td>2.69</td><td>0.14</td><td>67.78</td></tr><tr><td>0.1</td><td>0.65</td><td>0.82</td><td>0.24</td><td>2.26</td><td>3.11</td><td>0.06</td><td>67.89</td></tr><tr><td>0.5</td><td>0.70</td><td>0.88</td><td>0.31</td><td>2.25</td><td>3.17</td><td>0.20</td><td>69.07</td></tr><tr><td>0.8</td><td>0.65</td><td>0.84</td><td>0.28</td><td>2.07</td><td>2.98</td><td>0.18</td><td>69.36</td></tr><tr><td>1.0</td><td>0.74</td><td>0.92</td><td>0.27</td><td>2.32</td><td>3.21</td><td>-0.08</td><td>69.56</td></tr></table>\n\nTable 2: Experimental results for WikiText-2 and generated text for different $\\lambda$ values   \nTable 3: Experimental results for CNN/Daily Mail and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.72</td><td>0.94</td><td></td><td>0.77</td><td>1.05</td><td></td><td></td></tr><tr><td>0.0</td><td>0.51</td><td>0.68</td><td>0.22</td><td>0.43</td><td>0.59</td><td>0.29</td><td>118.01</td></tr><tr><td>0.1</td><td>0.38</td><td>0.52</td><td>0.19</td><td>0.85</td><td>1.38</td><td>0.22</td><td>116.49</td></tr><tr><td>0.5</td><td>0.34</td><td>0.48</td><td>0.14</td><td>0.79</td><td>1.31</td><td>0.20</td><td>116.19</td></tr><tr><td>0.8</td><td>0.40</td><td>0.56</td><td>0.19</td><td>0.96</td><td>1.57</td><td>0.23</td><td>121.00</td></tr><tr><td>1.0</td><td>0.62</td><td>0.83</td><td>0.21</td><td>1.71</td><td>2.65</td><td>0.31</td><td>120.55</td></tr></table>"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/9ddbc5e5d05a8d78eb2c0c9243d4c466f17287c54da57e7fe000dbf3f6919d15.jpg",
      "image_filename": "9ddbc5e5d05a8d78eb2c0c9243d4c466f17287c54da57e7fe000dbf3f6919d15.jpg",
      "caption": "Table 4: Generated text comparison for CNN/Daily Mail for different $\\lambda$ values",
      "context_before": "The difference between the pairs encodes the gender information corresponding to the gender pair. We then perform singular value decomposition on $C$ , obtaining UΣV . The gender subspace $B$ is then defined as the first $k$ columns (where $k$ is chosen to capture $5 0 \\%$ of the variation) of the right singular matrix $V$ :\n\nLet $N$ be the matrix consisting of the embeddings for which we would like the corresponding words to exhibit unbiased behavior. If we want the embeddings in $N$ to have minimal bias, then its projection onto the gender subspace $B$ should be small in terms its the squared Frobenius norm.\n\n4See the supplement for corpus-wise defining sets",
      "context_after": "Therefore, to reduce the bias learned by the embedding layer in the model, we can add the following bias regularization term to the training loss:\n\n$$ \\mathcal {L} _ {B} = \\lambda \\| N B \\| _ {F} ^ {2} $$\n\nwhere $\\lambda$ controls the importance of minimizing bias in the embedding matrix W (from which $N$ and $B$ are derived) relative to the other components of the model loss. The matrices $N$ and $C$ are updated each iteration during the model training.",
      "referring_paragraphs": [
        "Table 4 shows excerpts around selected target words from the generated corpora to demonstrate the effect of debiasing for different values of $\\lambda$ . We highlight the words crying and fragile that are typically associated with feminine qualities, along",
        "Table 4: Generated text comparison for CNN/Daily Mail for different $\\lambda$ values   \n\n<table><tr><td>Target Word</td><td>λ</td><td>Sample From Generated Text</td></tr><tr><td rowspan=\"3\">crying</td><td>0.0</td><td>“she was put on her own machine to raise money for her own wedding &lt;unk&gt; route which saw her crying and &lt;unk&gt; down a programme today .",
        "The detailed analysis is presented in Section 4.3\n\nTable 4 shows excerpts around selected target words from the generated corpora to demonstrate the effect of debiasing for different values of $\\lambda$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_6",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/cd54944eb4907d632cdaf2c53efd2a567328ff6ad2768c701dadb666cd1ae269.jpg",
      "image_filename": "cd54944eb4907d632cdaf2c53efd2a567328ff6ad2768c701dadb666cd1ae269.jpg",
      "caption": "Table 5: WikiText-2 bias scores for the words biased towards male gender for different $\\lambda$ values",
      "context_before": "B Word Level Bias Examples\n\nTables 5 and 6 show the bias scores at individual word level for selected words for Wikitext-2. The tables show how the scores vary for the training text and the generated text for different values of $\\lambda$\n\nTables 7 and 8 show the bias scores at individual word level for selected words for CNN/Daily Mail. The tables show how the scores vary for the training text and the generated text for different values of $\\lambda$",
      "context_after": "",
      "referring_paragraphs": [
        "The tables show how the scores vary for the training text and the generated text for different values of $\\lambda$\n\nTable 5: WikiText-2 bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Arts</td><td>-0.76</td><td>-1.20</td><td>-0.87</td><td>-0.32</td><td>-0.17</td><td>0.13</td><td>-1.48</td></tr><tr><td>Boston</td><td>-0.95</td><td>-1.06</td><td>-0.23</td><td>-1.06</td><td>-0.13</td><td>-0.37</td><td>-0.94</td></tr><tr><td>Edward</td><td>-0.68</td><td>-1.06</td><td>0.09</td><td>-0.56</td><td>-0.14</td><td>-0.44</td><td>-0.23</td></tr><tr><td>George</td><td>-0.52</td><td>-0.91</td><td>-0.26</td><td>-0.22</td><td>-0.48</td><td>-0.26</td><td>0.01</td></tr><tr><td>Henry</td><td>-0.59</td><td>-1.06</td><td>0.11</td><td>-0.34</td><td>-0.84</td><td>-0.92</td><td>-0.61</td></tr><tr><td>Peter</td><td>-0.69</td><td>-2.06</td><td>-0.09</td><td>-0.14</td><td>-0.32</td><td>0.08</td><td>0.53</td></tr><tr><td>Royal</td><td>-0.01</td><td>-1.89</td><td>-0.39</td><td>-0.61</td><td>-0.64</td><td>-1.14</td><td>-0.56</td></tr><tr><td>Sir</td><td>-0.01</td><td>-1.76</td><td>-0.99</td><td>-0.86</td><td>-0.64</td><td>-0.16</td><td>0.07</td></tr><tr><td>Stephen</td><td>-0.35</td><td>-1.20</td><td>-0.18</td><td>-1.01</td><td>-0.84</td><td>-0.11</td><td>0.36</td></tr><tr><td>Taylor</td><td>-0.84</td><td>-0.91</td><td>0.57</td><td>0.00</td><td>-0.01</td><td>-0.39</td><td>-0.83</td></tr><tr><td>ambassador</td><td>-0.76</td><td>-1.20</td><td>-0.23</td><td>-0.63</td><td>-0.74</td><td>0.43</td><td>-0.81</td></tr><tr><td>failed</td><td>-0.46</td><td>-2.06</td><td>0.03</td><td>-0.36</td><td>-1.00</td><td>0.17</td><td></td></tr><tr><td>focused</td><td>-0.22</td><td>-0.91</td><td>-0.12</td><td>-0.41</td><td>-0.40</td><td>-0.57</td><td>-0.53</td></tr><tr><td>idea</td><td>-0.20</td><td>-1.06</td><td>-0.36</td><td>-0.16</td><td>-0.27</td><td>-0.06</td><td>-0.42</td></tr><tr><td>manager</td><td>-1.58</td><td>-1.60</td><td>-0.04</td><td>-0.30</td><td>-1.08</td><td>-0.30</td><td>-1.06</td></tr><tr><td>students</td><td>-0.60</td><td>-0.79</td><td>-0.31</td><td>-0.29</td><td>-0.32</td><td>-0.51</td><td>-0.50</td></tr><tr><td>university</td><td>-0.12</td><td>-1.06</td><td>0.17</td><td>-1.01</td><td>-0.79</td><td>-0.95</td><td>-0.70</td></tr><tr><td>wife</td><td>-0.92</td><td>-1.29</td><td>-0.81</td><td>-1.02</td><td>-0.57</td><td>-0.67</td><td>-1.03</td></tr><tr><td>work</td><td>-0.24</td><td>-0.88</td><td>-0.48</td><td>-0.23</td><td>-0.49</td><td>-0.52</td><td>-0.13</td></tr><tr><td>youth</td><td>-0.39</td><td>-1.20</td><td>0.54</td><td>-0.16</td><td>-0.68</td><td>0.58</td><td></td></tr></table>\n\nTable 6: WikiText-2 bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Katherine</td><td>1.78</td><td>2.27</td><td>1.38</td><td>0.69</td><td>0.95</td><td>0.75</td><td>0.70</td></tr><tr><td>Zenobia</td><td>0.05</td><td>0.88</td><td>1.84</td><td>0.47</td><td>0.65</td><td>1.24</td><td></td></tr><tr><td>childhood</td><td>0.48</td><td>1.80</td><td>0.12</td><td>1.10</td><td>0.37</td><td>0.38</td><td>0.34</td></tr><tr><td>cousin</td><td>0.13</td><td>0.88</td><td>0.67</td><td>0.13</td><td>0.09</td><td>0.67</td><td>0.71</td></tr><tr><td>humor</td><td>0.34</td><td>1.29</td><td></td><td>0.69</td><td>0.61</td><td>0.34</td><td></td></tr><tr><td>invitation</td><td>0.19</td><td>1.80</td><td>-0.87</td><td>0.69</td><td>0.57</td><td>-0.44</td><td>-0.25</td></tr><tr><td>parents</td><td>0.51</td><td>0.76</td><td>0.77</td><td>0.08</td><td>0.45</td><td>0.57</td><td>1.11</td></tr><tr><td>partners</td><td>0.85</td><td>2.27</td><td>-0.28</td><td>0.98</td><td>0.87</td><td>-0.17</td><td>3.22</td></tr><tr><td>performances</td><td>0.79</td><td>1.02</td><td>-0.20</td><td>0.16</td><td>0.03</td><td>0.10</td><td>-1.80</td></tr><tr><td>producers</td><td>1.04</td><td>1.58</td><td>0.33</td><td>0.78</td><td>1.35</td><td>-1.45</td><td>0.18</td></tr><tr><td>readers</td><td>0.22</td><td>0.88</td><td>0.28</td><td>0.29</td><td>0.36</td><td>-0.32</td><td>-1.29</td></tr><tr><td>stars</td><td>0.85</td><td>1.58</td><td>0.16</td><td>0.90</td><td>0.46</td><td>-0.28</td><td>-0.08</td></tr><tr><td>talent</td><td>0.02</td><td>0.88</td><td>-0.75</td><td>0.10</td><td>0.31</td><td>-0.86</td><td></td></tr><tr><td>wore</td><td>0.09</td><td>0.88</td><td>0.48</td><td>0.29</td><td>0.65</td><td>0.16</td><td>-0.69</td></tr></table>\n\nTable 7: CNN/Daily Mail bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusers</td><td>-0.66</td><td>-1.17</td><td>-0.56</td><td>-0.77</td><td>-0.16</td><td>-1.93</td></tr><tr><td>acting</td><td>-0.23</td><td>-0.81</td><td>-0.59</td><td>-0.35</td><td>-0.54</td><td>0.60</td></tr><tr><td>actions</td><td>-0.27</td><td>-0.51</td><td>-0.06</td><td>-0.07</td><td>-0.53</td><td>-0.45</td></tr><tr><td>barrister</td><td>-1.35</td><td>-2.00</td><td>-0.64</td><td>-0.76</td><td>-0.08</td><td>-0.69</td></tr><tr><td>battle</td><td>-0.27</td><td>-0.53</td><td>-0.10</td><td>-0.32</td><td>-0.16</td><td>0.21</td></tr><tr><td>beneficiary</td><td>-1.64</td><td>-1.87</td><td>-1.06</td><td>-0.22</td><td>0.63</td><td></td></tr><tr><td>bills</td><td>-0.32</td><td>-0.53</td><td>-0.18</td><td>-0.50</td><td>0.23</td><td>0.69</td></tr><tr><td>businessman</td><td>-0.19</td><td>-1.81</td><td>-0.71</td><td>-0.45</td><td>-0.53</td><td>-1.93</td></tr><tr><td>cars</td><td>-0.43</td><td>-0.55</td><td>-0.32</td><td>-0.11</td><td>-0.24</td><td>-0.27</td></tr><tr><td>citizen</td><td>-0.03</td><td>-0.30</td><td>-0.03</td><td>-0.22</td><td>-0.01</td><td>0.04</td></tr><tr><td>cocaine</td><td>-0.59</td><td>-1.00</td><td>-0.84</td><td>-0.44</td><td>-0.42</td><td>-0.32</td></tr><tr><td>conspiracy</td><td>-0.57</td><td>-0.73</td><td>-0.66</td><td>-0.39</td><td>-0.83</td><td>-0.43</td></tr><tr><td>controversial</td><td>-0.21</td><td>-0.39</td><td>-0.39</td><td>-0.02</td><td>-0.17</td><td>-0.43</td></tr><tr><td>cooking</td><td>-0.48</td><td>-0.53</td><td>-0.24</td><td>-0.22</td><td>0.07</td><td>-0.52</td></tr><tr><td>cop</td><td>-1.30</td><td>-1.42</td><td>-0.77</td><td>-0.72</td><td>0.00</td><td>0.26</td></tr><tr><td>drug</td><td>-0.76</td><td>-0.82</td><td>-0.53</td><td>-0.42</td><td>-0.54</td><td>-0.63</td></tr><tr><td>executive</td><td>-0.04</td><td>-0.34</td><td>-0.22</td><td>-0.04</td><td>-0.48</td><td>-0.36</td></tr><tr><td>fighter</td><td>-0.59</td><td>-0.90</td><td>-0.48</td><td>-0.36</td><td>-0.89</td><td>-0.11</td></tr><tr><td>fraud</td><td>-0.17</td><td>-0.30</td><td>-0.16</td><td>-0.19</td><td>0.10</td><td>-0.63</td></tr><tr><td>friendly</td><td>-0.48</td><td>-0.53</td><td>-0.30</td><td>-0.23</td><td>0.36</td><td>-0.21</td></tr><tr><td>heroin</td><td>-0.57</td><td>-0.67</td><td>-0.28</td><td>-0.26</td><td>-0.66</td><td>0.49</td></tr><tr><td>journalists</td><td>-0.25</td><td>-1.08</td><td>-0.55</td><td>-0.76</td><td>-0.44</td><td></td></tr><tr><td>lawyer</td><td>-0.39</td><td>-0.47</td><td>-0.14</td><td>-0.10</td><td>-0.20</td><td>-0.50</td></tr><tr><td>lead</td><td>-0.47</td><td>-0.50</td><td>-0.40</td><td>-0.09</td><td>-0.07</td><td>-0.32</td></tr><tr><td>leadership</td><td>-0.25</td><td>-0.74</td><td>-0.28</td><td>-0.68</td><td>-0.57</td><td>-0.99</td></tr><tr><td>notorious</td><td>-0.18</td><td>-0.64</td><td>-0.36</td><td>-0.22</td><td>-0.12</td><td>-1.49</td></tr><tr><td>offensive</td><td>-0.17</td><td>-0.39</td><td>-0.28</td><td>-0.17</td><td>-0.52</td><td>-0.11</td></tr><tr><td>officer</td><td>-0.25</td><td>-0.29</td><td>-0.21</td><td>-0.13</td><td>-0.17</td><td>-0.19</td></tr><tr><td>outstanding</td><td>-0.25</td><td>-1.55</td><td>-0.98</td><td>-0.50</td><td>0.03</td><td>0.04</td></tr><tr><td>parole</td><td>-0.54</td><td>-0.86</td><td>0.00</td><td>-0.08</td><td>0.07</td><td>-1.30</td></tr><tr><td>pensioners</td><td>-0.48</td><td>-0.86</td><td>-0.77</td><td>-0.07</td><td>0.64</td><td>0.40</td></tr><tr><td>prisoners</td><td>-0.52</td><td>-0.99</td><td>-0.18</td><td>-0.29</td><td>-0.17</td><td>-1.59</td></tr><tr><td>religion</td><td>-0.41</td><td>-0.97</td><td>-0.15</td><td>-0.48</td><td>0.18</td><td>-1.68</td></tr><tr><td>reporters</td><td>-0.60</td><td>-0.93</td><td>-0.26</td><td>-0.05</td><td>-0.52</td><td>-0.97</td></tr><tr><td>representatives</td><td>-0.07</td><td>-0.48</td><td>-0.40</td><td>-0.18</td><td>-0.46</td><td>-0.83</td></tr><tr><td>research</td><td>-0.34</td><td>-0.46</td><td>-0.05</td><td>-0.33</td><td>0.03</td><td>-0.58</td></tr><tr><td>resignation</td><td>-0.95</td><td>-1.67</td><td>-0.61</td><td>-0.58</td><td>-0.40</td><td>-1.12</td></tr><tr><td>sacrifice</td><td>-0.03</td><td>-1.08</td><td>-0.38</td><td>-0.17</td><td>-1.29</td><td></td></tr><tr><td>supervisor</td><td>-0.66</td><td>-0.92</td><td>-0.44</td><td>-0.25</td><td>-0.17</td><td>0.48</td></tr><tr><td>violent</td><td>-0.17</td><td>-0.54</td><td>-0.07</td><td>-0.22</td><td>-0.19</td><td>-0.19</td></tr></table>\n\nTable 8: CNN/Daily Mail bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusive</td><td>0.00</td><td>0.40</td><td>0.06</td><td>0.39</td><td>0.48</td><td>-0.65</td></tr><tr><td>appealing</td><td>0.44</td><td>1.22</td><td>0.23</td><td>0.30</td><td>-0.68</td><td>1.16</td></tr><tr><td>bags</td><td>0.34</td><td>1.42</td><td>0.48</td><td>0.05</td><td>0.16</td><td>0.64</td></tr><tr><td>beloved</td><td>0.17</td><td>0.35</td><td>0.27</td><td>0.15</td><td>0.52</td><td>0.36</td></tr><tr><td>carol</td><td>0.76</td><td>1.41</td><td>0.20</td><td>0.39</td><td>0.27</td><td>0.48</td></tr><tr><td>chatted</td><td>0.03</td><td>1.83</td><td>0.20</td><td>0.19</td><td>-0.14</td><td>-0.25</td></tr><tr><td>children</td><td>0.29</td><td>0.46</td><td>0.36</td><td>0.26</td><td>0.41</td><td>0.27</td></tr><tr><td>comments</td><td>0.17</td><td>0.46</td><td>0.04</td><td>0.02</td><td>-0.35</td><td>-0.14</td></tr><tr><td>crying</td><td>0.28</td><td>0.70</td><td>0.19</td><td>0.57</td><td>0.17</td><td>0.87</td></tr><tr><td>designer</td><td>0.73</td><td>0.80</td><td>0.57</td><td>0.69</td><td>0.53</td><td>-1.53</td></tr><tr><td>designers</td><td>0.44</td><td>2.14</td><td>1.29</td><td>0.76</td><td>-0.11</td><td>1.11</td></tr><tr><td>distressed</td><td>0.15</td><td>0.53</td><td>0.23</td><td>0.26</td><td>-0.56</td><td>1.36</td></tr><tr><td>divorced</td><td>0.68</td><td>0.70</td><td>0.18</td><td>0.10</td><td>0.31</td><td>0.88</td></tr><tr><td>dollar</td><td>0.44</td><td>1.63</td><td>0.65</td><td>0.59</td><td>-0.24</td><td></td></tr><tr><td>donated</td><td>0.52</td><td>0.57</td><td>0.06</td><td>0.15</td><td>0.68</td><td>0.26</td></tr><tr><td>donating</td><td>1.29</td><td>1.38</td><td>0.27</td><td>0.80</td><td>-0.03</td><td>-0.21</td></tr><tr><td>embracing</td><td>1.13</td><td>1.78</td><td>0.74</td><td>0.55</td><td>1.48</td><td>-0.94</td></tr><tr><td>encouragement</td><td>0.85</td><td>0.94</td><td>0.22</td><td>0.50</td><td>0.37</td><td>0.55</td></tr><tr><td>endure</td><td>0.85</td><td>0.94</td><td>0.26</td><td>0.29</td><td>1.02</td><td></td></tr><tr><td>expecting</td><td>1.01</td><td>1.07</td><td>0.26</td><td>0.12</td><td>0.53</td><td>0.06</td></tr><tr><td>feeling</td><td>0.21</td><td>0.84</td><td>0.16</td><td>0.25</td><td>0.16</td><td>0.29</td></tr><tr><td>festive</td><td>0.15</td><td>0.53</td><td>0.52</td><td>0.14</td><td>0.21</td><td>0.26</td></tr><tr><td>fragile</td><td>0.44</td><td>0.94</td><td>0.20</td><td>0.45</td><td>-0.20</td><td></td></tr><tr><td>happy</td><td>0.32</td><td>0.66</td><td>0.10</td><td>0.11</td><td>0.11</td><td>0.25</td></tr><tr><td>healthy</td><td>0.52</td><td>0.64</td><td>0.26</td><td>0.45</td><td>0.24</td><td>0.25</td></tr><tr><td>hooked</td><td>0.78</td><td>1.38</td><td>0.12</td><td>0.12</td><td>-0.11</td><td>-0.09</td></tr><tr><td>hurting</td><td>0.75</td><td>1.13</td><td>0.33</td><td>0.34</td><td>0.44</td><td>0.26</td></tr><tr><td>indian</td><td>0.18</td><td>0.28</td><td>0.15</td><td>0.02</td><td>-0.02</td><td>-0.26</td></tr><tr><td>kissed</td><td>0.31</td><td>1.03</td><td>0.17</td><td>0.19</td><td>0.28</td><td>-0.22</td></tr><tr><td>kissing</td><td>0.26</td><td>1.14</td><td>0.54</td><td>0.61</td><td>0.44</td><td>-0.14</td></tr><tr><td>loving</td><td>0.41</td><td>0.73</td><td>0.43</td><td>0.18</td><td>0.15</td><td>-0.34</td></tr><tr><td>luxurious</td><td>0.59</td><td>0.82</td><td>0.17</td><td>0.44</td><td>-0.03</td><td>-0.83</td></tr><tr><td>makeup</td><td>1.60</td><td>1.63</td><td>0.07</td><td>0.22</td><td>1.09</td><td></td></tr><tr><td>mannequin</td><td>0.95</td><td>1.92</td><td>0.70</td><td>0.04</td><td>1.42</td><td></td></tr><tr><td>married</td><td>0.29</td><td>0.37</td><td>0.34</td><td>0.09</td><td>0.30</td><td>0.42</td></tr><tr><td>models</td><td>0.35</td><td>1.22</td><td>0.28</td><td>0.38</td><td>0.90</td><td>0.08</td></tr><tr><td>pictures</td><td>0.08</td><td>0.50</td><td>0.10</td><td>0.04</td><td>-0.06</td><td>0.59</td></tr><tr><td>pray</td><td>0.62</td><td>1.58</td><td>0.25</td><td>0.35</td><td>-0.25</td><td>0.96</td></tr><tr><td>relationship</td><td>0.53</td><td>0.62</td><td>0.39</td><td>0.32</td><td>0.58</td><td>0.43</td></tr><tr><td>scholarship</td><td>0.80</td><td>1.16</td><td>0.80</td><td>0.70</td><td>0.53</td><td>0.45</td></tr><tr><td>sharing</td><td>0.58</td><td>0.73</td><td>0.33</td><td>0.67</td><td>0.42</td><td>0.17</td></tr><tr><td>sleeping</td><td>0.18</td><td>0.71</td><td>0.27</td><td>0.35</td><td>0.56</td><td>0.58</td></tr><tr><td>stealing</td><td>0.10</td><td>0.48</td><td>0.32</td><td>0.18</td><td>0.06</td><td>-0.53</td></tr><tr><td>tears</td><td>0.50</td><td>0.58</td><td>0.44</td><td>0.12</td><td>0.45</td><td>0.35</td></tr><tr><td>thanksgiving</td><td>0.85</td><td>2.14</td><td>1.14</td><td>1.08</td><td>0.90</td><td></td></tr><tr><td>waist</td><td>1.33</td><td>1.45</td><td>0.68</td><td>0.02</td><td>0.31</td><td>0.96</td></tr></table>"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_7",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/221993764724de64126b560603c3e6180ba783cee145d384d14d15611ebb66f9.jpg",
      "image_filename": "221993764724de64126b560603c3e6180ba783cee145d384d14d15611ebb66f9.jpg",
      "caption": "Table 6: WikiText-2 bias scores for the words biased towards female gender for different $\\lambda$ values",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "The tables show how the scores vary for the training text and the generated text for different values of $\\lambda$\n\nTable 5: WikiText-2 bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Arts</td><td>-0.76</td><td>-1.20</td><td>-0.87</td><td>-0.32</td><td>-0.17</td><td>0.13</td><td>-1.48</td></tr><tr><td>Boston</td><td>-0.95</td><td>-1.06</td><td>-0.23</td><td>-1.06</td><td>-0.13</td><td>-0.37</td><td>-0.94</td></tr><tr><td>Edward</td><td>-0.68</td><td>-1.06</td><td>0.09</td><td>-0.56</td><td>-0.14</td><td>-0.44</td><td>-0.23</td></tr><tr><td>George</td><td>-0.52</td><td>-0.91</td><td>-0.26</td><td>-0.22</td><td>-0.48</td><td>-0.26</td><td>0.01</td></tr><tr><td>Henry</td><td>-0.59</td><td>-1.06</td><td>0.11</td><td>-0.34</td><td>-0.84</td><td>-0.92</td><td>-0.61</td></tr><tr><td>Peter</td><td>-0.69</td><td>-2.06</td><td>-0.09</td><td>-0.14</td><td>-0.32</td><td>0.08</td><td>0.53</td></tr><tr><td>Royal</td><td>-0.01</td><td>-1.89</td><td>-0.39</td><td>-0.61</td><td>-0.64</td><td>-1.14</td><td>-0.56</td></tr><tr><td>Sir</td><td>-0.01</td><td>-1.76</td><td>-0.99</td><td>-0.86</td><td>-0.64</td><td>-0.16</td><td>0.07</td></tr><tr><td>Stephen</td><td>-0.35</td><td>-1.20</td><td>-0.18</td><td>-1.01</td><td>-0.84</td><td>-0.11</td><td>0.36</td></tr><tr><td>Taylor</td><td>-0.84</td><td>-0.91</td><td>0.57</td><td>0.00</td><td>-0.01</td><td>-0.39</td><td>-0.83</td></tr><tr><td>ambassador</td><td>-0.76</td><td>-1.20</td><td>-0.23</td><td>-0.63</td><td>-0.74</td><td>0.43</td><td>-0.81</td></tr><tr><td>failed</td><td>-0.46</td><td>-2.06</td><td>0.03</td><td>-0.36</td><td>-1.00</td><td>0.17</td><td></td></tr><tr><td>focused</td><td>-0.22</td><td>-0.91</td><td>-0.12</td><td>-0.41</td><td>-0.40</td><td>-0.57</td><td>-0.53</td></tr><tr><td>idea</td><td>-0.20</td><td>-1.06</td><td>-0.36</td><td>-0.16</td><td>-0.27</td><td>-0.06</td><td>-0.42</td></tr><tr><td>manager</td><td>-1.58</td><td>-1.60</td><td>-0.04</td><td>-0.30</td><td>-1.08</td><td>-0.30</td><td>-1.06</td></tr><tr><td>students</td><td>-0.60</td><td>-0.79</td><td>-0.31</td><td>-0.29</td><td>-0.32</td><td>-0.51</td><td>-0.50</td></tr><tr><td>university</td><td>-0.12</td><td>-1.06</td><td>0.17</td><td>-1.01</td><td>-0.79</td><td>-0.95</td><td>-0.70</td></tr><tr><td>wife</td><td>-0.92</td><td>-1.29</td><td>-0.81</td><td>-1.02</td><td>-0.57</td><td>-0.67</td><td>-1.03</td></tr><tr><td>work</td><td>-0.24</td><td>-0.88</td><td>-0.48</td><td>-0.23</td><td>-0.49</td><td>-0.52</td><td>-0.13</td></tr><tr><td>youth</td><td>-0.39</td><td>-1.20</td><td>0.54</td><td>-0.16</td><td>-0.68</td><td>0.58</td><td></td></tr></table>\n\nTable 6: WikiText-2 bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Katherine</td><td>1.78</td><td>2.27</td><td>1.38</td><td>0.69</td><td>0.95</td><td>0.75</td><td>0.70</td></tr><tr><td>Zenobia</td><td>0.05</td><td>0.88</td><td>1.84</td><td>0.47</td><td>0.65</td><td>1.24</td><td></td></tr><tr><td>childhood</td><td>0.48</td><td>1.80</td><td>0.12</td><td>1.10</td><td>0.37</td><td>0.38</td><td>0.34</td></tr><tr><td>cousin</td><td>0.13</td><td>0.88</td><td>0.67</td><td>0.13</td><td>0.09</td><td>0.67</td><td>0.71</td></tr><tr><td>humor</td><td>0.34</td><td>1.29</td><td></td><td>0.69</td><td>0.61</td><td>0.34</td><td></td></tr><tr><td>invitation</td><td>0.19</td><td>1.80</td><td>-0.87</td><td>0.69</td><td>0.57</td><td>-0.44</td><td>-0.25</td></tr><tr><td>parents</td><td>0.51</td><td>0.76</td><td>0.77</td><td>0.08</td><td>0.45</td><td>0.57</td><td>1.11</td></tr><tr><td>partners</td><td>0.85</td><td>2.27</td><td>-0.28</td><td>0.98</td><td>0.87</td><td>-0.17</td><td>3.22</td></tr><tr><td>performances</td><td>0.79</td><td>1.02</td><td>-0.20</td><td>0.16</td><td>0.03</td><td>0.10</td><td>-1.80</td></tr><tr><td>producers</td><td>1.04</td><td>1.58</td><td>0.33</td><td>0.78</td><td>1.35</td><td>-1.45</td><td>0.18</td></tr><tr><td>readers</td><td>0.22</td><td>0.88</td><td>0.28</td><td>0.29</td><td>0.36</td><td>-0.32</td><td>-1.29</td></tr><tr><td>stars</td><td>0.85</td><td>1.58</td><td>0.16</td><td>0.90</td><td>0.46</td><td>-0.28</td><td>-0.08</td></tr><tr><td>talent</td><td>0.02</td><td>0.88</td><td>-0.75</td><td>0.10</td><td>0.31</td><td>-0.86</td><td></td></tr><tr><td>wore</td><td>0.09</td><td>0.88</td><td>0.48</td><td>0.29</td><td>0.65</td><td>0.16</td><td>-0.69</td></tr></table>\n\nTable 7: CNN/Daily Mail bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusers</td><td>-0.66</td><td>-1.17</td><td>-0.56</td><td>-0.77</td><td>-0.16</td><td>-1.93</td></tr><tr><td>acting</td><td>-0.23</td><td>-0.81</td><td>-0.59</td><td>-0.35</td><td>-0.54</td><td>0.60</td></tr><tr><td>actions</td><td>-0.27</td><td>-0.51</td><td>-0.06</td><td>-0.07</td><td>-0.53</td><td>-0.45</td></tr><tr><td>barrister</td><td>-1.35</td><td>-2.00</td><td>-0.64</td><td>-0.76</td><td>-0.08</td><td>-0.69</td></tr><tr><td>battle</td><td>-0.27</td><td>-0.53</td><td>-0.10</td><td>-0.32</td><td>-0.16</td><td>0.21</td></tr><tr><td>beneficiary</td><td>-1.64</td><td>-1.87</td><td>-1.06</td><td>-0.22</td><td>0.63</td><td></td></tr><tr><td>bills</td><td>-0.32</td><td>-0.53</td><td>-0.18</td><td>-0.50</td><td>0.23</td><td>0.69</td></tr><tr><td>businessman</td><td>-0.19</td><td>-1.81</td><td>-0.71</td><td>-0.45</td><td>-0.53</td><td>-1.93</td></tr><tr><td>cars</td><td>-0.43</td><td>-0.55</td><td>-0.32</td><td>-0.11</td><td>-0.24</td><td>-0.27</td></tr><tr><td>citizen</td><td>-0.03</td><td>-0.30</td><td>-0.03</td><td>-0.22</td><td>-0.01</td><td>0.04</td></tr><tr><td>cocaine</td><td>-0.59</td><td>-1.00</td><td>-0.84</td><td>-0.44</td><td>-0.42</td><td>-0.32</td></tr><tr><td>conspiracy</td><td>-0.57</td><td>-0.73</td><td>-0.66</td><td>-0.39</td><td>-0.83</td><td>-0.43</td></tr><tr><td>controversial</td><td>-0.21</td><td>-0.39</td><td>-0.39</td><td>-0.02</td><td>-0.17</td><td>-0.43</td></tr><tr><td>cooking</td><td>-0.48</td><td>-0.53</td><td>-0.24</td><td>-0.22</td><td>0.07</td><td>-0.52</td></tr><tr><td>cop</td><td>-1.30</td><td>-1.42</td><td>-0.77</td><td>-0.72</td><td>0.00</td><td>0.26</td></tr><tr><td>drug</td><td>-0.76</td><td>-0.82</td><td>-0.53</td><td>-0.42</td><td>-0.54</td><td>-0.63</td></tr><tr><td>executive</td><td>-0.04</td><td>-0.34</td><td>-0.22</td><td>-0.04</td><td>-0.48</td><td>-0.36</td></tr><tr><td>fighter</td><td>-0.59</td><td>-0.90</td><td>-0.48</td><td>-0.36</td><td>-0.89</td><td>-0.11</td></tr><tr><td>fraud</td><td>-0.17</td><td>-0.30</td><td>-0.16</td><td>-0.19</td><td>0.10</td><td>-0.63</td></tr><tr><td>friendly</td><td>-0.48</td><td>-0.53</td><td>-0.30</td><td>-0.23</td><td>0.36</td><td>-0.21</td></tr><tr><td>heroin</td><td>-0.57</td><td>-0.67</td><td>-0.28</td><td>-0.26</td><td>-0.66</td><td>0.49</td></tr><tr><td>journalists</td><td>-0.25</td><td>-1.08</td><td>-0.55</td><td>-0.76</td><td>-0.44</td><td></td></tr><tr><td>lawyer</td><td>-0.39</td><td>-0.47</td><td>-0.14</td><td>-0.10</td><td>-0.20</td><td>-0.50</td></tr><tr><td>lead</td><td>-0.47</td><td>-0.50</td><td>-0.40</td><td>-0.09</td><td>-0.07</td><td>-0.32</td></tr><tr><td>leadership</td><td>-0.25</td><td>-0.74</td><td>-0.28</td><td>-0.68</td><td>-0.57</td><td>-0.99</td></tr><tr><td>notorious</td><td>-0.18</td><td>-0.64</td><td>-0.36</td><td>-0.22</td><td>-0.12</td><td>-1.49</td></tr><tr><td>offensive</td><td>-0.17</td><td>-0.39</td><td>-0.28</td><td>-0.17</td><td>-0.52</td><td>-0.11</td></tr><tr><td>officer</td><td>-0.25</td><td>-0.29</td><td>-0.21</td><td>-0.13</td><td>-0.17</td><td>-0.19</td></tr><tr><td>outstanding</td><td>-0.25</td><td>-1.55</td><td>-0.98</td><td>-0.50</td><td>0.03</td><td>0.04</td></tr><tr><td>parole</td><td>-0.54</td><td>-0.86</td><td>0.00</td><td>-0.08</td><td>0.07</td><td>-1.30</td></tr><tr><td>pensioners</td><td>-0.48</td><td>-0.86</td><td>-0.77</td><td>-0.07</td><td>0.64</td><td>0.40</td></tr><tr><td>prisoners</td><td>-0.52</td><td>-0.99</td><td>-0.18</td><td>-0.29</td><td>-0.17</td><td>-1.59</td></tr><tr><td>religion</td><td>-0.41</td><td>-0.97</td><td>-0.15</td><td>-0.48</td><td>0.18</td><td>-1.68</td></tr><tr><td>reporters</td><td>-0.60</td><td>-0.93</td><td>-0.26</td><td>-0.05</td><td>-0.52</td><td>-0.97</td></tr><tr><td>representatives</td><td>-0.07</td><td>-0.48</td><td>-0.40</td><td>-0.18</td><td>-0.46</td><td>-0.83</td></tr><tr><td>research</td><td>-0.34</td><td>-0.46</td><td>-0.05</td><td>-0.33</td><td>0.03</td><td>-0.58</td></tr><tr><td>resignation</td><td>-0.95</td><td>-1.67</td><td>-0.61</td><td>-0.58</td><td>-0.40</td><td>-1.12</td></tr><tr><td>sacrifice</td><td>-0.03</td><td>-1.08</td><td>-0.38</td><td>-0.17</td><td>-1.29</td><td></td></tr><tr><td>supervisor</td><td>-0.66</td><td>-0.92</td><td>-0.44</td><td>-0.25</td><td>-0.17</td><td>0.48</td></tr><tr><td>violent</td><td>-0.17</td><td>-0.54</td><td>-0.07</td><td>-0.22</td><td>-0.19</td><td>-0.19</td></tr></table>\n\nTable 8: CNN/Daily Mail bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusive</td><td>0.00</td><td>0.40</td><td>0.06</td><td>0.39</td><td>0.48</td><td>-0.65</td></tr><tr><td>appealing</td><td>0.44</td><td>1.22</td><td>0.23</td><td>0.30</td><td>-0.68</td><td>1.16</td></tr><tr><td>bags</td><td>0.34</td><td>1.42</td><td>0.48</td><td>0.05</td><td>0.16</td><td>0.64</td></tr><tr><td>beloved</td><td>0.17</td><td>0.35</td><td>0.27</td><td>0.15</td><td>0.52</td><td>0.36</td></tr><tr><td>carol</td><td>0.76</td><td>1.41</td><td>0.20</td><td>0.39</td><td>0.27</td><td>0.48</td></tr><tr><td>chatted</td><td>0.03</td><td>1.83</td><td>0.20</td><td>0.19</td><td>-0.14</td><td>-0.25</td></tr><tr><td>children</td><td>0.29</td><td>0.46</td><td>0.36</td><td>0.26</td><td>0.41</td><td>0.27</td></tr><tr><td>comments</td><td>0.17</td><td>0.46</td><td>0.04</td><td>0.02</td><td>-0.35</td><td>-0.14</td></tr><tr><td>crying</td><td>0.28</td><td>0.70</td><td>0.19</td><td>0.57</td><td>0.17</td><td>0.87</td></tr><tr><td>designer</td><td>0.73</td><td>0.80</td><td>0.57</td><td>0.69</td><td>0.53</td><td>-1.53</td></tr><tr><td>designers</td><td>0.44</td><td>2.14</td><td>1.29</td><td>0.76</td><td>-0.11</td><td>1.11</td></tr><tr><td>distressed</td><td>0.15</td><td>0.53</td><td>0.23</td><td>0.26</td><td>-0.56</td><td>1.36</td></tr><tr><td>divorced</td><td>0.68</td><td>0.70</td><td>0.18</td><td>0.10</td><td>0.31</td><td>0.88</td></tr><tr><td>dollar</td><td>0.44</td><td>1.63</td><td>0.65</td><td>0.59</td><td>-0.24</td><td></td></tr><tr><td>donated</td><td>0.52</td><td>0.57</td><td>0.06</td><td>0.15</td><td>0.68</td><td>0.26</td></tr><tr><td>donating</td><td>1.29</td><td>1.38</td><td>0.27</td><td>0.80</td><td>-0.03</td><td>-0.21</td></tr><tr><td>embracing</td><td>1.13</td><td>1.78</td><td>0.74</td><td>0.55</td><td>1.48</td><td>-0.94</td></tr><tr><td>encouragement</td><td>0.85</td><td>0.94</td><td>0.22</td><td>0.50</td><td>0.37</td><td>0.55</td></tr><tr><td>endure</td><td>0.85</td><td>0.94</td><td>0.26</td><td>0.29</td><td>1.02</td><td></td></tr><tr><td>expecting</td><td>1.01</td><td>1.07</td><td>0.26</td><td>0.12</td><td>0.53</td><td>0.06</td></tr><tr><td>feeling</td><td>0.21</td><td>0.84</td><td>0.16</td><td>0.25</td><td>0.16</td><td>0.29</td></tr><tr><td>festive</td><td>0.15</td><td>0.53</td><td>0.52</td><td>0.14</td><td>0.21</td><td>0.26</td></tr><tr><td>fragile</td><td>0.44</td><td>0.94</td><td>0.20</td><td>0.45</td><td>-0.20</td><td></td></tr><tr><td>happy</td><td>0.32</td><td>0.66</td><td>0.10</td><td>0.11</td><td>0.11</td><td>0.25</td></tr><tr><td>healthy</td><td>0.52</td><td>0.64</td><td>0.26</td><td>0.45</td><td>0.24</td><td>0.25</td></tr><tr><td>hooked</td><td>0.78</td><td>1.38</td><td>0.12</td><td>0.12</td><td>-0.11</td><td>-0.09</td></tr><tr><td>hurting</td><td>0.75</td><td>1.13</td><td>0.33</td><td>0.34</td><td>0.44</td><td>0.26</td></tr><tr><td>indian</td><td>0.18</td><td>0.28</td><td>0.15</td><td>0.02</td><td>-0.02</td><td>-0.26</td></tr><tr><td>kissed</td><td>0.31</td><td>1.03</td><td>0.17</td><td>0.19</td><td>0.28</td><td>-0.22</td></tr><tr><td>kissing</td><td>0.26</td><td>1.14</td><td>0.54</td><td>0.61</td><td>0.44</td><td>-0.14</td></tr><tr><td>loving</td><td>0.41</td><td>0.73</td><td>0.43</td><td>0.18</td><td>0.15</td><td>-0.34</td></tr><tr><td>luxurious</td><td>0.59</td><td>0.82</td><td>0.17</td><td>0.44</td><td>-0.03</td><td>-0.83</td></tr><tr><td>makeup</td><td>1.60</td><td>1.63</td><td>0.07</td><td>0.22</td><td>1.09</td><td></td></tr><tr><td>mannequin</td><td>0.95</td><td>1.92</td><td>0.70</td><td>0.04</td><td>1.42</td><td></td></tr><tr><td>married</td><td>0.29</td><td>0.37</td><td>0.34</td><td>0.09</td><td>0.30</td><td>0.42</td></tr><tr><td>models</td><td>0.35</td><td>1.22</td><td>0.28</td><td>0.38</td><td>0.90</td><td>0.08</td></tr><tr><td>pictures</td><td>0.08</td><td>0.50</td><td>0.10</td><td>0.04</td><td>-0.06</td><td>0.59</td></tr><tr><td>pray</td><td>0.62</td><td>1.58</td><td>0.25</td><td>0.35</td><td>-0.25</td><td>0.96</td></tr><tr><td>relationship</td><td>0.53</td><td>0.62</td><td>0.39</td><td>0.32</td><td>0.58</td><td>0.43</td></tr><tr><td>scholarship</td><td>0.80</td><td>1.16</td><td>0.80</td><td>0.70</td><td>0.53</td><td>0.45</td></tr><tr><td>sharing</td><td>0.58</td><td>0.73</td><td>0.33</td><td>0.67</td><td>0.42</td><td>0.17</td></tr><tr><td>sleeping</td><td>0.18</td><td>0.71</td><td>0.27</td><td>0.35</td><td>0.56</td><td>0.58</td></tr><tr><td>stealing</td><td>0.10</td><td>0.48</td><td>0.32</td><td>0.18</td><td>0.06</td><td>-0.53</td></tr><tr><td>tears</td><td>0.50</td><td>0.58</td><td>0.44</td><td>0.12</td><td>0.45</td><td>0.35</td></tr><tr><td>thanksgiving</td><td>0.85</td><td>2.14</td><td>1.14</td><td>1.08</td><td>0.90</td><td></td></tr><tr><td>waist</td><td>1.33</td><td>1.45</td><td>0.68</td><td>0.02</td><td>0.31</td><td>0.96</td></tr></table>"
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_8",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/424deba7a5a93a9bd7aa907a704ee474220088de3c0f05834fde71a644113fa5.jpg",
      "image_filename": "424deba7a5a93a9bd7aa907a704ee474220088de3c0f05834fde71a644113fa5.jpg",
      "caption": "Table 7: CNN/Daily Mail bias scores for the words biased towards male gender for different $\\lambda$ values",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "The tables show how the scores vary for the training text and the generated text for different values of $\\lambda$\n\nTable 5: WikiText-2 bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Arts</td><td>-0.76</td><td>-1.20</td><td>-0.87</td><td>-0.32</td><td>-0.17</td><td>0.13</td><td>-1.48</td></tr><tr><td>Boston</td><td>-0.95</td><td>-1.06</td><td>-0.23</td><td>-1.06</td><td>-0.13</td><td>-0.37</td><td>-0.94</td></tr><tr><td>Edward</td><td>-0.68</td><td>-1.06</td><td>0.09</td><td>-0.56</td><td>-0.14</td><td>-0.44</td><td>-0.23</td></tr><tr><td>George</td><td>-0.52</td><td>-0.91</td><td>-0.26</td><td>-0.22</td><td>-0.48</td><td>-0.26</td><td>0.01</td></tr><tr><td>Henry</td><td>-0.59</td><td>-1.06</td><td>0.11</td><td>-0.34</td><td>-0.84</td><td>-0.92</td><td>-0.61</td></tr><tr><td>Peter</td><td>-0.69</td><td>-2.06</td><td>-0.09</td><td>-0.14</td><td>-0.32</td><td>0.08</td><td>0.53</td></tr><tr><td>Royal</td><td>-0.01</td><td>-1.89</td><td>-0.39</td><td>-0.61</td><td>-0.64</td><td>-1.14</td><td>-0.56</td></tr><tr><td>Sir</td><td>-0.01</td><td>-1.76</td><td>-0.99</td><td>-0.86</td><td>-0.64</td><td>-0.16</td><td>0.07</td></tr><tr><td>Stephen</td><td>-0.35</td><td>-1.20</td><td>-0.18</td><td>-1.01</td><td>-0.84</td><td>-0.11</td><td>0.36</td></tr><tr><td>Taylor</td><td>-0.84</td><td>-0.91</td><td>0.57</td><td>0.00</td><td>-0.01</td><td>-0.39</td><td>-0.83</td></tr><tr><td>ambassador</td><td>-0.76</td><td>-1.20</td><td>-0.23</td><td>-0.63</td><td>-0.74</td><td>0.43</td><td>-0.81</td></tr><tr><td>failed</td><td>-0.46</td><td>-2.06</td><td>0.03</td><td>-0.36</td><td>-1.00</td><td>0.17</td><td></td></tr><tr><td>focused</td><td>-0.22</td><td>-0.91</td><td>-0.12</td><td>-0.41</td><td>-0.40</td><td>-0.57</td><td>-0.53</td></tr><tr><td>idea</td><td>-0.20</td><td>-1.06</td><td>-0.36</td><td>-0.16</td><td>-0.27</td><td>-0.06</td><td>-0.42</td></tr><tr><td>manager</td><td>-1.58</td><td>-1.60</td><td>-0.04</td><td>-0.30</td><td>-1.08</td><td>-0.30</td><td>-1.06</td></tr><tr><td>students</td><td>-0.60</td><td>-0.79</td><td>-0.31</td><td>-0.29</td><td>-0.32</td><td>-0.51</td><td>-0.50</td></tr><tr><td>university</td><td>-0.12</td><td>-1.06</td><td>0.17</td><td>-1.01</td><td>-0.79</td><td>-0.95</td><td>-0.70</td></tr><tr><td>wife</td><td>-0.92</td><td>-1.29</td><td>-0.81</td><td>-1.02</td><td>-0.57</td><td>-0.67</td><td>-1.03</td></tr><tr><td>work</td><td>-0.24</td><td>-0.88</td><td>-0.48</td><td>-0.23</td><td>-0.49</td><td>-0.52</td><td>-0.13</td></tr><tr><td>youth</td><td>-0.39</td><td>-1.20</td><td>0.54</td><td>-0.16</td><td>-0.68</td><td>0.58</td><td></td></tr></table>\n\nTable 6: WikiText-2 bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Katherine</td><td>1.78</td><td>2.27</td><td>1.38</td><td>0.69</td><td>0.95</td><td>0.75</td><td>0.70</td></tr><tr><td>Zenobia</td><td>0.05</td><td>0.88</td><td>1.84</td><td>0.47</td><td>0.65</td><td>1.24</td><td></td></tr><tr><td>childhood</td><td>0.48</td><td>1.80</td><td>0.12</td><td>1.10</td><td>0.37</td><td>0.38</td><td>0.34</td></tr><tr><td>cousin</td><td>0.13</td><td>0.88</td><td>0.67</td><td>0.13</td><td>0.09</td><td>0.67</td><td>0.71</td></tr><tr><td>humor</td><td>0.34</td><td>1.29</td><td></td><td>0.69</td><td>0.61</td><td>0.34</td><td></td></tr><tr><td>invitation</td><td>0.19</td><td>1.80</td><td>-0.87</td><td>0.69</td><td>0.57</td><td>-0.44</td><td>-0.25</td></tr><tr><td>parents</td><td>0.51</td><td>0.76</td><td>0.77</td><td>0.08</td><td>0.45</td><td>0.57</td><td>1.11</td></tr><tr><td>partners</td><td>0.85</td><td>2.27</td><td>-0.28</td><td>0.98</td><td>0.87</td><td>-0.17</td><td>3.22</td></tr><tr><td>performances</td><td>0.79</td><td>1.02</td><td>-0.20</td><td>0.16</td><td>0.03</td><td>0.10</td><td>-1.80</td></tr><tr><td>producers</td><td>1.04</td><td>1.58</td><td>0.33</td><td>0.78</td><td>1.35</td><td>-1.45</td><td>0.18</td></tr><tr><td>readers</td><td>0.22</td><td>0.88</td><td>0.28</td><td>0.29</td><td>0.36</td><td>-0.32</td><td>-1.29</td></tr><tr><td>stars</td><td>0.85</td><td>1.58</td><td>0.16</td><td>0.90</td><td>0.46</td><td>-0.28</td><td>-0.08</td></tr><tr><td>talent</td><td>0.02</td><td>0.88</td><td>-0.75</td><td>0.10</td><td>0.31</td><td>-0.86</td><td></td></tr><tr><td>wore</td><td>0.09</td><td>0.88</td><td>0.48</td><td>0.29</td><td>0.65</td><td>0.16</td><td>-0.69</td></tr></table>\n\nTable 7: CNN/Daily Mail bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusers</td><td>-0.66</td><td>-1.17</td><td>-0.56</td><td>-0.77</td><td>-0.16</td><td>-1.93</td></tr><tr><td>acting</td><td>-0.23</td><td>-0.81</td><td>-0.59</td><td>-0.35</td><td>-0.54</td><td>0.60</td></tr><tr><td>actions</td><td>-0.27</td><td>-0.51</td><td>-0.06</td><td>-0.07</td><td>-0.53</td><td>-0.45</td></tr><tr><td>barrister</td><td>-1.35</td><td>-2.00</td><td>-0.64</td><td>-0.76</td><td>-0.08</td><td>-0.69</td></tr><tr><td>battle</td><td>-0.27</td><td>-0.53</td><td>-0.10</td><td>-0.32</td><td>-0.16</td><td>0.21</td></tr><tr><td>beneficiary</td><td>-1.64</td><td>-1.87</td><td>-1.06</td><td>-0.22</td><td>0.63</td><td></td></tr><tr><td>bills</td><td>-0.32</td><td>-0.53</td><td>-0.18</td><td>-0.50</td><td>0.23</td><td>0.69</td></tr><tr><td>businessman</td><td>-0.19</td><td>-1.81</td><td>-0.71</td><td>-0.45</td><td>-0.53</td><td>-1.93</td></tr><tr><td>cars</td><td>-0.43</td><td>-0.55</td><td>-0.32</td><td>-0.11</td><td>-0.24</td><td>-0.27</td></tr><tr><td>citizen</td><td>-0.03</td><td>-0.30</td><td>-0.03</td><td>-0.22</td><td>-0.01</td><td>0.04</td></tr><tr><td>cocaine</td><td>-0.59</td><td>-1.00</td><td>-0.84</td><td>-0.44</td><td>-0.42</td><td>-0.32</td></tr><tr><td>conspiracy</td><td>-0.57</td><td>-0.73</td><td>-0.66</td><td>-0.39</td><td>-0.83</td><td>-0.43</td></tr><tr><td>controversial</td><td>-0.21</td><td>-0.39</td><td>-0.39</td><td>-0.02</td><td>-0.17</td><td>-0.43</td></tr><tr><td>cooking</td><td>-0.48</td><td>-0.53</td><td>-0.24</td><td>-0.22</td><td>0.07</td><td>-0.52</td></tr><tr><td>cop</td><td>-1.30</td><td>-1.42</td><td>-0.77</td><td>-0.72</td><td>0.00</td><td>0.26</td></tr><tr><td>drug</td><td>-0.76</td><td>-0.82</td><td>-0.53</td><td>-0.42</td><td>-0.54</td><td>-0.63</td></tr><tr><td>executive</td><td>-0.04</td><td>-0.34</td><td>-0.22</td><td>-0.04</td><td>-0.48</td><td>-0.36</td></tr><tr><td>fighter</td><td>-0.59</td><td>-0.90</td><td>-0.48</td><td>-0.36</td><td>-0.89</td><td>-0.11</td></tr><tr><td>fraud</td><td>-0.17</td><td>-0.30</td><td>-0.16</td><td>-0.19</td><td>0.10</td><td>-0.63</td></tr><tr><td>friendly</td><td>-0.48</td><td>-0.53</td><td>-0.30</td><td>-0.23</td><td>0.36</td><td>-0.21</td></tr><tr><td>heroin</td><td>-0.57</td><td>-0.67</td><td>-0.28</td><td>-0.26</td><td>-0.66</td><td>0.49</td></tr><tr><td>journalists</td><td>-0.25</td><td>-1.08</td><td>-0.55</td><td>-0.76</td><td>-0.44</td><td></td></tr><tr><td>lawyer</td><td>-0.39</td><td>-0.47</td><td>-0.14</td><td>-0.10</td><td>-0.20</td><td>-0.50</td></tr><tr><td>lead</td><td>-0.47</td><td>-0.50</td><td>-0.40</td><td>-0.09</td><td>-0.07</td><td>-0.32</td></tr><tr><td>leadership</td><td>-0.25</td><td>-0.74</td><td>-0.28</td><td>-0.68</td><td>-0.57</td><td>-0.99</td></tr><tr><td>notorious</td><td>-0.18</td><td>-0.64</td><td>-0.36</td><td>-0.22</td><td>-0.12</td><td>-1.49</td></tr><tr><td>offensive</td><td>-0.17</td><td>-0.39</td><td>-0.28</td><td>-0.17</td><td>-0.52</td><td>-0.11</td></tr><tr><td>officer</td><td>-0.25</td><td>-0.29</td><td>-0.21</td><td>-0.13</td><td>-0.17</td><td>-0.19</td></tr><tr><td>outstanding</td><td>-0.25</td><td>-1.55</td><td>-0.98</td><td>-0.50</td><td>0.03</td><td>0.04</td></tr><tr><td>parole</td><td>-0.54</td><td>-0.86</td><td>0.00</td><td>-0.08</td><td>0.07</td><td>-1.30</td></tr><tr><td>pensioners</td><td>-0.48</td><td>-0.86</td><td>-0.77</td><td>-0.07</td><td>0.64</td><td>0.40</td></tr><tr><td>prisoners</td><td>-0.52</td><td>-0.99</td><td>-0.18</td><td>-0.29</td><td>-0.17</td><td>-1.59</td></tr><tr><td>religion</td><td>-0.41</td><td>-0.97</td><td>-0.15</td><td>-0.48</td><td>0.18</td><td>-1.68</td></tr><tr><td>reporters</td><td>-0.60</td><td>-0.93</td><td>-0.26</td><td>-0.05</td><td>-0.52</td><td>-0.97</td></tr><tr><td>representatives</td><td>-0.07</td><td>-0.48</td><td>-0.40</td><td>-0.18</td><td>-0.46</td><td>-0.83</td></tr><tr><td>research</td><td>-0.34</td><td>-0.46</td><td>-0.05</td><td>-0.33</td><td>0.03</td><td>-0.58</td></tr><tr><td>resignation</td><td>-0.95</td><td>-1.67</td><td>-0.61</td><td>-0.58</td><td>-0.40</td><td>-1.12</td></tr><tr><td>sacrifice</td><td>-0.03</td><td>-1.08</td><td>-0.38</td><td>-0.17</td><td>-1.29</td><td></td></tr><tr><td>supervisor</td><td>-0.66</td><td>-0.92</td><td>-0.44</td><td>-0.25</td><td>-0.17</td><td>0.48</td></tr><tr><td>violent</td><td>-0.17</td><td>-0.54</td><td>-0.07</td><td>-0.22</td><td>-0.19</td><td>-0.19</td></tr></table>\n\nTable 8: CNN/Daily Mail bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusive</td><td>0.00</td><td>0.40</td><td>0.06</td><td>0.39</td><td>0.48</td><td>-0.65</td></tr><tr><td>appealing</td><td>0.44</td><td>1.22</td><td>0.23</td><td>0.30</td><td>-0.68</td><td>1.16</td></tr><tr><td>bags</td><td>0.34</td><td>1.42</td><td>0.48</td><td>0.05</td><td>0.16</td><td>0.64</td></tr><tr><td>beloved</td><td>0.17</td><td>0.35</td><td>0.27</td><td>0.15</td><td>0.52</td><td>0.36</td></tr><tr><td>carol</td><td>0.76</td><td>1.41</td><td>0.20</td><td>0.39</td><td>0.27</td><td>0.48</td></tr><tr><td>chatted</td><td>0.03</td><td>1.83</td><td>0.20</td><td>0.19</td><td>-0.14</td><td>-0.25</td></tr><tr><td>children</td><td>0.29</td><td>0.46</td><td>0.36</td><td>0.26</td><td>0.41</td><td>0.27</td></tr><tr><td>comments</td><td>0.17</td><td>0.46</td><td>0.04</td><td>0.02</td><td>-0.35</td><td>-0.14</td></tr><tr><td>crying</td><td>0.28</td><td>0.70</td><td>0.19</td><td>0.57</td><td>0.17</td><td>0.87</td></tr><tr><td>designer</td><td>0.73</td><td>0.80</td><td>0.57</td><td>0.69</td><td>0.53</td><td>-1.53</td></tr><tr><td>designers</td><td>0.44</td><td>2.14</td><td>1.29</td><td>0.76</td><td>-0.11</td><td>1.11</td></tr><tr><td>distressed</td><td>0.15</td><td>0.53</td><td>0.23</td><td>0.26</td><td>-0.56</td><td>1.36</td></tr><tr><td>divorced</td><td>0.68</td><td>0.70</td><td>0.18</td><td>0.10</td><td>0.31</td><td>0.88</td></tr><tr><td>dollar</td><td>0.44</td><td>1.63</td><td>0.65</td><td>0.59</td><td>-0.24</td><td></td></tr><tr><td>donated</td><td>0.52</td><td>0.57</td><td>0.06</td><td>0.15</td><td>0.68</td><td>0.26</td></tr><tr><td>donating</td><td>1.29</td><td>1.38</td><td>0.27</td><td>0.80</td><td>-0.03</td><td>-0.21</td></tr><tr><td>embracing</td><td>1.13</td><td>1.78</td><td>0.74</td><td>0.55</td><td>1.48</td><td>-0.94</td></tr><tr><td>encouragement</td><td>0.85</td><td>0.94</td><td>0.22</td><td>0.50</td><td>0.37</td><td>0.55</td></tr><tr><td>endure</td><td>0.85</td><td>0.94</td><td>0.26</td><td>0.29</td><td>1.02</td><td></td></tr><tr><td>expecting</td><td>1.01</td><td>1.07</td><td>0.26</td><td>0.12</td><td>0.53</td><td>0.06</td></tr><tr><td>feeling</td><td>0.21</td><td>0.84</td><td>0.16</td><td>0.25</td><td>0.16</td><td>0.29</td></tr><tr><td>festive</td><td>0.15</td><td>0.53</td><td>0.52</td><td>0.14</td><td>0.21</td><td>0.26</td></tr><tr><td>fragile</td><td>0.44</td><td>0.94</td><td>0.20</td><td>0.45</td><td>-0.20</td><td></td></tr><tr><td>happy</td><td>0.32</td><td>0.66</td><td>0.10</td><td>0.11</td><td>0.11</td><td>0.25</td></tr><tr><td>healthy</td><td>0.52</td><td>0.64</td><td>0.26</td><td>0.45</td><td>0.24</td><td>0.25</td></tr><tr><td>hooked</td><td>0.78</td><td>1.38</td><td>0.12</td><td>0.12</td><td>-0.11</td><td>-0.09</td></tr><tr><td>hurting</td><td>0.75</td><td>1.13</td><td>0.33</td><td>0.34</td><td>0.44</td><td>0.26</td></tr><tr><td>indian</td><td>0.18</td><td>0.28</td><td>0.15</td><td>0.02</td><td>-0.02</td><td>-0.26</td></tr><tr><td>kissed</td><td>0.31</td><td>1.03</td><td>0.17</td><td>0.19</td><td>0.28</td><td>-0.22</td></tr><tr><td>kissing</td><td>0.26</td><td>1.14</td><td>0.54</td><td>0.61</td><td>0.44</td><td>-0.14</td></tr><tr><td>loving</td><td>0.41</td><td>0.73</td><td>0.43</td><td>0.18</td><td>0.15</td><td>-0.34</td></tr><tr><td>luxurious</td><td>0.59</td><td>0.82</td><td>0.17</td><td>0.44</td><td>-0.03</td><td>-0.83</td></tr><tr><td>makeup</td><td>1.60</td><td>1.63</td><td>0.07</td><td>0.22</td><td>1.09</td><td></td></tr><tr><td>mannequin</td><td>0.95</td><td>1.92</td><td>0.70</td><td>0.04</td><td>1.42</td><td></td></tr><tr><td>married</td><td>0.29</td><td>0.37</td><td>0.34</td><td>0.09</td><td>0.30</td><td>0.42</td></tr><tr><td>models</td><td>0.35</td><td>1.22</td><td>0.28</td><td>0.38</td><td>0.90</td><td>0.08</td></tr><tr><td>pictures</td><td>0.08</td><td>0.50</td><td>0.10</td><td>0.04</td><td>-0.06</td><td>0.59</td></tr><tr><td>pray</td><td>0.62</td><td>1.58</td><td>0.25</td><td>0.35</td><td>-0.25</td><td>0.96</td></tr><tr><td>relationship</td><td>0.53</td><td>0.62</td><td>0.39</td><td>0.32</td><td>0.58</td><td>0.43</td></tr><tr><td>scholarship</td><td>0.80</td><td>1.16</td><td>0.80</td><td>0.70</td><td>0.53</td><td>0.45</td></tr><tr><td>sharing</td><td>0.58</td><td>0.73</td><td>0.33</td><td>0.67</td><td>0.42</td><td>0.17</td></tr><tr><td>sleeping</td><td>0.18</td><td>0.71</td><td>0.27</td><td>0.35</td><td>0.56</td><td>0.58</td></tr><tr><td>stealing</td><td>0.10</td><td>0.48</td><td>0.32</td><td>0.18</td><td>0.06</td><td>-0.53</td></tr><tr><td>tears</td><td>0.50</td><td>0.58</td><td>0.44</td><td>0.12</td><td>0.45</td><td>0.35</td></tr><tr><td>thanksgiving</td><td>0.85</td><td>2.14</td><td>1.14</td><td>1.08</td><td>0.90</td><td></td></tr><tr><td>waist</td><td>1.33</td><td>1.45</td><td>0.68</td><td>0.02</td><td>0.31</td><td>0.96</td></tr></table>"
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_9",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/6890645510136e15bdce7f1363d57a96f033fa9ce124e9bce441e0b29ab8a710.jpg",
      "image_filename": "6890645510136e15bdce7f1363d57a96f033fa9ce124e9bce441e0b29ab8a710.jpg",
      "caption": "Table 8: CNN/Daily Mail bias scores for the words biased towards female gender for different $\\lambda$ values",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "The tables show how the scores vary for the training text and the generated text for different values of $\\lambda$\n\nTable 5: WikiText-2 bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Arts</td><td>-0.76</td><td>-1.20</td><td>-0.87</td><td>-0.32</td><td>-0.17</td><td>0.13</td><td>-1.48</td></tr><tr><td>Boston</td><td>-0.95</td><td>-1.06</td><td>-0.23</td><td>-1.06</td><td>-0.13</td><td>-0.37</td><td>-0.94</td></tr><tr><td>Edward</td><td>-0.68</td><td>-1.06</td><td>0.09</td><td>-0.56</td><td>-0.14</td><td>-0.44</td><td>-0.23</td></tr><tr><td>George</td><td>-0.52</td><td>-0.91</td><td>-0.26</td><td>-0.22</td><td>-0.48</td><td>-0.26</td><td>0.01</td></tr><tr><td>Henry</td><td>-0.59</td><td>-1.06</td><td>0.11</td><td>-0.34</td><td>-0.84</td><td>-0.92</td><td>-0.61</td></tr><tr><td>Peter</td><td>-0.69</td><td>-2.06</td><td>-0.09</td><td>-0.14</td><td>-0.32</td><td>0.08</td><td>0.53</td></tr><tr><td>Royal</td><td>-0.01</td><td>-1.89</td><td>-0.39</td><td>-0.61</td><td>-0.64</td><td>-1.14</td><td>-0.56</td></tr><tr><td>Sir</td><td>-0.01</td><td>-1.76</td><td>-0.99</td><td>-0.86</td><td>-0.64</td><td>-0.16</td><td>0.07</td></tr><tr><td>Stephen</td><td>-0.35</td><td>-1.20</td><td>-0.18</td><td>-1.01</td><td>-0.84</td><td>-0.11</td><td>0.36</td></tr><tr><td>Taylor</td><td>-0.84</td><td>-0.91</td><td>0.57</td><td>0.00</td><td>-0.01</td><td>-0.39</td><td>-0.83</td></tr><tr><td>ambassador</td><td>-0.76</td><td>-1.20</td><td>-0.23</td><td>-0.63</td><td>-0.74</td><td>0.43</td><td>-0.81</td></tr><tr><td>failed</td><td>-0.46</td><td>-2.06</td><td>0.03</td><td>-0.36</td><td>-1.00</td><td>0.17</td><td></td></tr><tr><td>focused</td><td>-0.22</td><td>-0.91</td><td>-0.12</td><td>-0.41</td><td>-0.40</td><td>-0.57</td><td>-0.53</td></tr><tr><td>idea</td><td>-0.20</td><td>-1.06</td><td>-0.36</td><td>-0.16</td><td>-0.27</td><td>-0.06</td><td>-0.42</td></tr><tr><td>manager</td><td>-1.58</td><td>-1.60</td><td>-0.04</td><td>-0.30</td><td>-1.08</td><td>-0.30</td><td>-1.06</td></tr><tr><td>students</td><td>-0.60</td><td>-0.79</td><td>-0.31</td><td>-0.29</td><td>-0.32</td><td>-0.51</td><td>-0.50</td></tr><tr><td>university</td><td>-0.12</td><td>-1.06</td><td>0.17</td><td>-1.01</td><td>-0.79</td><td>-0.95</td><td>-0.70</td></tr><tr><td>wife</td><td>-0.92</td><td>-1.29</td><td>-0.81</td><td>-1.02</td><td>-0.57</td><td>-0.67</td><td>-1.03</td></tr><tr><td>work</td><td>-0.24</td><td>-0.88</td><td>-0.48</td><td>-0.23</td><td>-0.49</td><td>-0.52</td><td>-0.13</td></tr><tr><td>youth</td><td>-0.39</td><td>-1.20</td><td>0.54</td><td>-0.16</td><td>-0.68</td><td>0.58</td><td></td></tr></table>\n\nTable 6: WikiText-2 bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.01</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>Katherine</td><td>1.78</td><td>2.27</td><td>1.38</td><td>0.69</td><td>0.95</td><td>0.75</td><td>0.70</td></tr><tr><td>Zenobia</td><td>0.05</td><td>0.88</td><td>1.84</td><td>0.47</td><td>0.65</td><td>1.24</td><td></td></tr><tr><td>childhood</td><td>0.48</td><td>1.80</td><td>0.12</td><td>1.10</td><td>0.37</td><td>0.38</td><td>0.34</td></tr><tr><td>cousin</td><td>0.13</td><td>0.88</td><td>0.67</td><td>0.13</td><td>0.09</td><td>0.67</td><td>0.71</td></tr><tr><td>humor</td><td>0.34</td><td>1.29</td><td></td><td>0.69</td><td>0.61</td><td>0.34</td><td></td></tr><tr><td>invitation</td><td>0.19</td><td>1.80</td><td>-0.87</td><td>0.69</td><td>0.57</td><td>-0.44</td><td>-0.25</td></tr><tr><td>parents</td><td>0.51</td><td>0.76</td><td>0.77</td><td>0.08</td><td>0.45</td><td>0.57</td><td>1.11</td></tr><tr><td>partners</td><td>0.85</td><td>2.27</td><td>-0.28</td><td>0.98</td><td>0.87</td><td>-0.17</td><td>3.22</td></tr><tr><td>performances</td><td>0.79</td><td>1.02</td><td>-0.20</td><td>0.16</td><td>0.03</td><td>0.10</td><td>-1.80</td></tr><tr><td>producers</td><td>1.04</td><td>1.58</td><td>0.33</td><td>0.78</td><td>1.35</td><td>-1.45</td><td>0.18</td></tr><tr><td>readers</td><td>0.22</td><td>0.88</td><td>0.28</td><td>0.29</td><td>0.36</td><td>-0.32</td><td>-1.29</td></tr><tr><td>stars</td><td>0.85</td><td>1.58</td><td>0.16</td><td>0.90</td><td>0.46</td><td>-0.28</td><td>-0.08</td></tr><tr><td>talent</td><td>0.02</td><td>0.88</td><td>-0.75</td><td>0.10</td><td>0.31</td><td>-0.86</td><td></td></tr><tr><td>wore</td><td>0.09</td><td>0.88</td><td>0.48</td><td>0.29</td><td>0.65</td><td>0.16</td><td>-0.69</td></tr></table>\n\nTable 7: CNN/Daily Mail bias scores for the words biased towards male gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusers</td><td>-0.66</td><td>-1.17</td><td>-0.56</td><td>-0.77</td><td>-0.16</td><td>-1.93</td></tr><tr><td>acting</td><td>-0.23</td><td>-0.81</td><td>-0.59</td><td>-0.35</td><td>-0.54</td><td>0.60</td></tr><tr><td>actions</td><td>-0.27</td><td>-0.51</td><td>-0.06</td><td>-0.07</td><td>-0.53</td><td>-0.45</td></tr><tr><td>barrister</td><td>-1.35</td><td>-2.00</td><td>-0.64</td><td>-0.76</td><td>-0.08</td><td>-0.69</td></tr><tr><td>battle</td><td>-0.27</td><td>-0.53</td><td>-0.10</td><td>-0.32</td><td>-0.16</td><td>0.21</td></tr><tr><td>beneficiary</td><td>-1.64</td><td>-1.87</td><td>-1.06</td><td>-0.22</td><td>0.63</td><td></td></tr><tr><td>bills</td><td>-0.32</td><td>-0.53</td><td>-0.18</td><td>-0.50</td><td>0.23</td><td>0.69</td></tr><tr><td>businessman</td><td>-0.19</td><td>-1.81</td><td>-0.71</td><td>-0.45</td><td>-0.53</td><td>-1.93</td></tr><tr><td>cars</td><td>-0.43</td><td>-0.55</td><td>-0.32</td><td>-0.11</td><td>-0.24</td><td>-0.27</td></tr><tr><td>citizen</td><td>-0.03</td><td>-0.30</td><td>-0.03</td><td>-0.22</td><td>-0.01</td><td>0.04</td></tr><tr><td>cocaine</td><td>-0.59</td><td>-1.00</td><td>-0.84</td><td>-0.44</td><td>-0.42</td><td>-0.32</td></tr><tr><td>conspiracy</td><td>-0.57</td><td>-0.73</td><td>-0.66</td><td>-0.39</td><td>-0.83</td><td>-0.43</td></tr><tr><td>controversial</td><td>-0.21</td><td>-0.39</td><td>-0.39</td><td>-0.02</td><td>-0.17</td><td>-0.43</td></tr><tr><td>cooking</td><td>-0.48</td><td>-0.53</td><td>-0.24</td><td>-0.22</td><td>0.07</td><td>-0.52</td></tr><tr><td>cop</td><td>-1.30</td><td>-1.42</td><td>-0.77</td><td>-0.72</td><td>0.00</td><td>0.26</td></tr><tr><td>drug</td><td>-0.76</td><td>-0.82</td><td>-0.53</td><td>-0.42</td><td>-0.54</td><td>-0.63</td></tr><tr><td>executive</td><td>-0.04</td><td>-0.34</td><td>-0.22</td><td>-0.04</td><td>-0.48</td><td>-0.36</td></tr><tr><td>fighter</td><td>-0.59</td><td>-0.90</td><td>-0.48</td><td>-0.36</td><td>-0.89</td><td>-0.11</td></tr><tr><td>fraud</td><td>-0.17</td><td>-0.30</td><td>-0.16</td><td>-0.19</td><td>0.10</td><td>-0.63</td></tr><tr><td>friendly</td><td>-0.48</td><td>-0.53</td><td>-0.30</td><td>-0.23</td><td>0.36</td><td>-0.21</td></tr><tr><td>heroin</td><td>-0.57</td><td>-0.67</td><td>-0.28</td><td>-0.26</td><td>-0.66</td><td>0.49</td></tr><tr><td>journalists</td><td>-0.25</td><td>-1.08</td><td>-0.55</td><td>-0.76</td><td>-0.44</td><td></td></tr><tr><td>lawyer</td><td>-0.39</td><td>-0.47</td><td>-0.14</td><td>-0.10</td><td>-0.20</td><td>-0.50</td></tr><tr><td>lead</td><td>-0.47</td><td>-0.50</td><td>-0.40</td><td>-0.09</td><td>-0.07</td><td>-0.32</td></tr><tr><td>leadership</td><td>-0.25</td><td>-0.74</td><td>-0.28</td><td>-0.68</td><td>-0.57</td><td>-0.99</td></tr><tr><td>notorious</td><td>-0.18</td><td>-0.64</td><td>-0.36</td><td>-0.22</td><td>-0.12</td><td>-1.49</td></tr><tr><td>offensive</td><td>-0.17</td><td>-0.39</td><td>-0.28</td><td>-0.17</td><td>-0.52</td><td>-0.11</td></tr><tr><td>officer</td><td>-0.25</td><td>-0.29</td><td>-0.21</td><td>-0.13</td><td>-0.17</td><td>-0.19</td></tr><tr><td>outstanding</td><td>-0.25</td><td>-1.55</td><td>-0.98</td><td>-0.50</td><td>0.03</td><td>0.04</td></tr><tr><td>parole</td><td>-0.54</td><td>-0.86</td><td>0.00</td><td>-0.08</td><td>0.07</td><td>-1.30</td></tr><tr><td>pensioners</td><td>-0.48</td><td>-0.86</td><td>-0.77</td><td>-0.07</td><td>0.64</td><td>0.40</td></tr><tr><td>prisoners</td><td>-0.52</td><td>-0.99</td><td>-0.18</td><td>-0.29</td><td>-0.17</td><td>-1.59</td></tr><tr><td>religion</td><td>-0.41</td><td>-0.97</td><td>-0.15</td><td>-0.48</td><td>0.18</td><td>-1.68</td></tr><tr><td>reporters</td><td>-0.60</td><td>-0.93</td><td>-0.26</td><td>-0.05</td><td>-0.52</td><td>-0.97</td></tr><tr><td>representatives</td><td>-0.07</td><td>-0.48</td><td>-0.40</td><td>-0.18</td><td>-0.46</td><td>-0.83</td></tr><tr><td>research</td><td>-0.34</td><td>-0.46</td><td>-0.05</td><td>-0.33</td><td>0.03</td><td>-0.58</td></tr><tr><td>resignation</td><td>-0.95</td><td>-1.67</td><td>-0.61</td><td>-0.58</td><td>-0.40</td><td>-1.12</td></tr><tr><td>sacrifice</td><td>-0.03</td><td>-1.08</td><td>-0.38</td><td>-0.17</td><td>-1.29</td><td></td></tr><tr><td>supervisor</td><td>-0.66</td><td>-0.92</td><td>-0.44</td><td>-0.25</td><td>-0.17</td><td>0.48</td></tr><tr><td>violent</td><td>-0.17</td><td>-0.54</td><td>-0.07</td><td>-0.22</td><td>-0.19</td><td>-0.19</td></tr></table>\n\nTable 8: CNN/Daily Mail bias scores for the words biased towards female gender for different $\\lambda$ values   \n\n<table><tr><td>Target Words</td><td>training</td><td>λ=0.0</td><td>λ=0.1</td><td>λ=0.5</td><td>λ=0.8</td><td>λ=1.0</td></tr><tr><td>abusive</td><td>0.00</td><td>0.40</td><td>0.06</td><td>0.39</td><td>0.48</td><td>-0.65</td></tr><tr><td>appealing</td><td>0.44</td><td>1.22</td><td>0.23</td><td>0.30</td><td>-0.68</td><td>1.16</td></tr><tr><td>bags</td><td>0.34</td><td>1.42</td><td>0.48</td><td>0.05</td><td>0.16</td><td>0.64</td></tr><tr><td>beloved</td><td>0.17</td><td>0.35</td><td>0.27</td><td>0.15</td><td>0.52</td><td>0.36</td></tr><tr><td>carol</td><td>0.76</td><td>1.41</td><td>0.20</td><td>0.39</td><td>0.27</td><td>0.48</td></tr><tr><td>chatted</td><td>0.03</td><td>1.83</td><td>0.20</td><td>0.19</td><td>-0.14</td><td>-0.25</td></tr><tr><td>children</td><td>0.29</td><td>0.46</td><td>0.36</td><td>0.26</td><td>0.41</td><td>0.27</td></tr><tr><td>comments</td><td>0.17</td><td>0.46</td><td>0.04</td><td>0.02</td><td>-0.35</td><td>-0.14</td></tr><tr><td>crying</td><td>0.28</td><td>0.70</td><td>0.19</td><td>0.57</td><td>0.17</td><td>0.87</td></tr><tr><td>designer</td><td>0.73</td><td>0.80</td><td>0.57</td><td>0.69</td><td>0.53</td><td>-1.53</td></tr><tr><td>designers</td><td>0.44</td><td>2.14</td><td>1.29</td><td>0.76</td><td>-0.11</td><td>1.11</td></tr><tr><td>distressed</td><td>0.15</td><td>0.53</td><td>0.23</td><td>0.26</td><td>-0.56</td><td>1.36</td></tr><tr><td>divorced</td><td>0.68</td><td>0.70</td><td>0.18</td><td>0.10</td><td>0.31</td><td>0.88</td></tr><tr><td>dollar</td><td>0.44</td><td>1.63</td><td>0.65</td><td>0.59</td><td>-0.24</td><td></td></tr><tr><td>donated</td><td>0.52</td><td>0.57</td><td>0.06</td><td>0.15</td><td>0.68</td><td>0.26</td></tr><tr><td>donating</td><td>1.29</td><td>1.38</td><td>0.27</td><td>0.80</td><td>-0.03</td><td>-0.21</td></tr><tr><td>embracing</td><td>1.13</td><td>1.78</td><td>0.74</td><td>0.55</td><td>1.48</td><td>-0.94</td></tr><tr><td>encouragement</td><td>0.85</td><td>0.94</td><td>0.22</td><td>0.50</td><td>0.37</td><td>0.55</td></tr><tr><td>endure</td><td>0.85</td><td>0.94</td><td>0.26</td><td>0.29</td><td>1.02</td><td></td></tr><tr><td>expecting</td><td>1.01</td><td>1.07</td><td>0.26</td><td>0.12</td><td>0.53</td><td>0.06</td></tr><tr><td>feeling</td><td>0.21</td><td>0.84</td><td>0.16</td><td>0.25</td><td>0.16</td><td>0.29</td></tr><tr><td>festive</td><td>0.15</td><td>0.53</td><td>0.52</td><td>0.14</td><td>0.21</td><td>0.26</td></tr><tr><td>fragile</td><td>0.44</td><td>0.94</td><td>0.20</td><td>0.45</td><td>-0.20</td><td></td></tr><tr><td>happy</td><td>0.32</td><td>0.66</td><td>0.10</td><td>0.11</td><td>0.11</td><td>0.25</td></tr><tr><td>healthy</td><td>0.52</td><td>0.64</td><td>0.26</td><td>0.45</td><td>0.24</td><td>0.25</td></tr><tr><td>hooked</td><td>0.78</td><td>1.38</td><td>0.12</td><td>0.12</td><td>-0.11</td><td>-0.09</td></tr><tr><td>hurting</td><td>0.75</td><td>1.13</td><td>0.33</td><td>0.34</td><td>0.44</td><td>0.26</td></tr><tr><td>indian</td><td>0.18</td><td>0.28</td><td>0.15</td><td>0.02</td><td>-0.02</td><td>-0.26</td></tr><tr><td>kissed</td><td>0.31</td><td>1.03</td><td>0.17</td><td>0.19</td><td>0.28</td><td>-0.22</td></tr><tr><td>kissing</td><td>0.26</td><td>1.14</td><td>0.54</td><td>0.61</td><td>0.44</td><td>-0.14</td></tr><tr><td>loving</td><td>0.41</td><td>0.73</td><td>0.43</td><td>0.18</td><td>0.15</td><td>-0.34</td></tr><tr><td>luxurious</td><td>0.59</td><td>0.82</td><td>0.17</td><td>0.44</td><td>-0.03</td><td>-0.83</td></tr><tr><td>makeup</td><td>1.60</td><td>1.63</td><td>0.07</td><td>0.22</td><td>1.09</td><td></td></tr><tr><td>mannequin</td><td>0.95</td><td>1.92</td><td>0.70</td><td>0.04</td><td>1.42</td><td></td></tr><tr><td>married</td><td>0.29</td><td>0.37</td><td>0.34</td><td>0.09</td><td>0.30</td><td>0.42</td></tr><tr><td>models</td><td>0.35</td><td>1.22</td><td>0.28</td><td>0.38</td><td>0.90</td><td>0.08</td></tr><tr><td>pictures</td><td>0.08</td><td>0.50</td><td>0.10</td><td>0.04</td><td>-0.06</td><td>0.59</td></tr><tr><td>pray</td><td>0.62</td><td>1.58</td><td>0.25</td><td>0.35</td><td>-0.25</td><td>0.96</td></tr><tr><td>relationship</td><td>0.53</td><td>0.62</td><td>0.39</td><td>0.32</td><td>0.58</td><td>0.43</td></tr><tr><td>scholarship</td><td>0.80</td><td>1.16</td><td>0.80</td><td>0.70</td><td>0.53</td><td>0.45</td></tr><tr><td>sharing</td><td>0.58</td><td>0.73</td><td>0.33</td><td>0.67</td><td>0.42</td><td>0.17</td></tr><tr><td>sleeping</td><td>0.18</td><td>0.71</td><td>0.27</td><td>0.35</td><td>0.56</td><td>0.58</td></tr><tr><td>stealing</td><td>0.10</td><td>0.48</td><td>0.32</td><td>0.18</td><td>0.06</td><td>-0.53</td></tr><tr><td>tears</td><td>0.50</td><td>0.58</td><td>0.44</td><td>0.12</td><td>0.45</td><td>0.35</td></tr><tr><td>thanksgiving</td><td>0.85</td><td>2.14</td><td>1.14</td><td>1.08</td><td>0.90</td><td></td></tr><tr><td>waist</td><td>1.33</td><td>1.45</td><td>0.68</td><td>0.02</td><td>0.31</td><td>0.96</td></tr></table>"
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03035_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1904.03310": [
    {
      "doc_id": "1904.03310",
      "figure_id": "1904.03310_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/d9e634804c3cea7f267cb762cf258ea413a97ebb5fbbb53afbe575ac7a066ae6.jpg",
      "image_filename": "d9e634804c3cea7f267cb762cf258ea413a97ebb5fbbb53afbe575ac7a066ae6.jpg",
      "caption": "Table 1: Training corpus for ELMo. We show total counts for male (M) and female (F) pronouns in the corpus, and counts corresponding to their cooccurrence with occupation words where the occupations are stereotypically male (M-biased) or female (Fbiased).",
      "context_before": "To mitigate bias from word embeddings, Bolukbasi et al. (2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered. This paper investigates a natural follow-up question: What are effective bias mitigation techniques for contextualized embeddings?\n\n3 Gender Bias in ELMo\n\nIn this section we describe three intrinsic analyses highlighting gender bias in trained ELMo contextual word embeddings (Peters et al., 2018). We show that (1) training data for ELMo contains sig-",
      "context_after": "nificantly more male entities compared to female entities leading to gender bias in the pre-trained contextual word embeddings (2) the geometry of trained ELMo embeddings systematically encodes gender information and (3) ELMo propagates gender information about male and female entities unequally.\n\n3.1 Training Data Bias\n\nTable 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo. We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a). The analysis shows that the Billion Word corpus contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female.",
      "referring_paragraphs": [
        "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo. We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a). The analysis shows that the Billi",
        "Next, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for",
        "To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a). Each sentence in the corpus contains one gendered pronoun and two occupation words, such as “The developer corrected the secretary because she made a mistake” and also the same sentence with the opposite pronoun (he). In Figure 1 on the right, we project the ELMo embeddings of occupation words that are co-referent with the pronoun (e.g. secretary in the above example) for when the pronoun is male (",
        "We show that (1) training data for ELMo contains sig-\n\nTable 1: Training corpus for ELMo.",
        "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo.",
        "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).",
        "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03310_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1904.03310",
      "figure_id": "1904.03310_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg",
      "image_filename": "1904.03310_page0_fig0.jpg",
      "caption": "Next, we analyze the gender subspace in ELMo.",
      "context_before": "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo. We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a). The analysis shows that the Billion Word corpus contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female.\n\n3.2 Geometry of Gender\n\nNext, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016). The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03310_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1904.03310",
      "figure_id": "1904.03310_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig1.jpg",
      "image_filename": "1904.03310_page0_fig1.jpg",
      "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.",
      "context_before": "",
      "context_after": "To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a). Each sentence in the corpus contains one gendered pronoun and two occupation words, such as “The developer corrected the secretary because she made a mistake” and also the same sentence with the opposite pronoun (he). In Figure 1 on the right, we project the ELMo embeddings of occupation words that are co-referent with the pronoun (e.g. secretary in the above example) for when the pronoun is male (blue dots) and female (orange dots) on the two principal components from the PCA analysis. Qualitatively, we can see the first component separates male and female contexts while the second component groups male related words such as lawyer and developer and female related words such as cashier and nurse.\n\n3.3 Unequal Treatment of Gender\n\nTo test how ELMo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence. We collect sentences containing gendered words (e.g., he-she, father-mother) and occupation words (e.g., doctor)1 from the OntoNotes 5.0 corpus (Weischedel et al., 2012), where we treat occupation words as a mention to an entity, and the gender of that entity is taken to the gender of a co-referring gendered word, if one exists. For example, in the sentence “the engineer went back to her home,” we take engineer to be a female mention. Then we split all such instances into training and test, with 539 and 62 instances, respectively and augment these sentences by swapping all the gendered words with words of the opposite gender such that the numbers of male",
      "referring_paragraphs": [
        "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo. We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a). The analysis shows that the Billi",
        "Next, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for",
        "To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a). Each sentence in the corpus contains one gendered pronoun and two occupation words, such as “The developer corrected the secretary because she made a mistake” and also the same sentence with the opposite pronoun (he). In Figure 1 on the right, we project the ELMo embeddings of occupation words that are co-referent with the pronoun (e.g. secretary in the above example) for when the pronoun is male (",
        "We show that (1) training data for ELMo contains sig-\n\nTable 1: Training corpus for ELMo.",
        "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo.",
        "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).",
        "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03310_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1904.03310",
      "figure_id": "1904.03310_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/b3633ea263a13f4d75425ba567cdd360fc825bd621488b6f085121953d4ae5a3.jpg",
      "image_filename": "b3633ea263a13f4d75425ba567cdd360fc825bd621488b6f085121953d4ae5a3.jpg",
      "caption": "Table 2: F1 on OntoNotes and WinoBias development sets. WinoBias dataset is split Semantics Only and w/ Syntactic Cues subsets. ELMo improves the performance on the OntoNotes dataset by $5 \\%$ but shows stronger bias on the WinoBias dataset. Avg. stands for averaged F1 score on the pro- and anti-stereotype subsets while “Diff.” is the absolute difference between these two subsets. * indicates the difference between pro/anti stereotypical conditions is significant $( p < . 0 5 )$ under an approximate randomized test (Graham et al., 2014). Mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level. However, the neutralizing ELMo approach only mitigates bias when there are other strong learning signals for the task.",
      "context_before": "We evaluate bias with respect to the WinoBias dataset (Zhao et al., 2018a), a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015). It contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true.\n\n2We use the $\\nu$ -SVC formulation and tune the hyperparameter $\\nu$ (Chang and Lin, 2011) in the range of [0.1, 1] with a step 0.1.\n\n1We use the list collected in (Zhao et al., 2018a)",
      "context_after": "Each subset consists of two types of sentences: one that requires semantic understanding of the sentence to make coreference resolution (Semantics Only) and another that relies on syntactic cues (w/ Syntactic Cues). Gender bias is measured by taking the difference of the performance in pro- and antistereotypical subsets. Previous work (Zhao et al., 2018a) evaluated the systems based on GloVe embeddings but here we evaluate a state-of-the-art system that trained on the OntoNotes corpus with ELMo embeddings (Lee et al., 2018).\n\n4.2 Bias Mitigation Methods\n\nNext, we describe two methods for mitigating bias in ELMo for the purpose of coreference resolution: (1) a train-time data augmentation approach and (2) a test-time neutralization approach.",
      "referring_paragraphs": [
        "Table 2 summarizes our results on WinoBias.",
        "ELMo Bias Transfers to Coreference Row 3 in Table 2 summarizes performance of the ELMo based coreference system on WinoBias. While ELMo helps to boost the coreference resolution F1 score (OntoNotes) it also propagates bias to the task. It exhibits large differences between pro- and anti-stereotyped sets (|Diff|) on both semantic and syntactic examples in WinoBias.",
        "Bias Mitigation Rows 4-6 in Table 2 summarize the effectiveness of the two bias mitigation approaches we consider. Data augmentation is largely effective at mitigating bias in the coreference resolution system with ELMo (reducing |Diff | to insignificant levels) but requires retraining the system. Neutralization is less effective than augmentation and cannot fully remove gender bias on the Semantics Only portion of WinoBias, indicating it is effective only for simpler cases. This observation is ",
        "Table 2: F1 on OntoNotes and WinoBias development sets."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1904.03310_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1905.03674": [
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "image_filename": "1905.03674_page0_fig0.jpg",
      "caption": "Figure 1: Proportionality Example",
      "context_before": "Definition 1. Let $X \\subseteq { \\mathcal { M } }$ with $| X | = k$ . $S \\subseteq N$ is a blocking coalition against $X$ if $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and $\\exists y \\in { \\mathcal { M } }$ such that $\\forall i \\in S$ , $d ( i , y ) < D _ { i } ( X )$ . $X \\subseteq { \\mathcal { N } }$ is proportional if there is no blocking coalition against $X$ .\n\nEquivalently, $X$ is proportional if $\\forall S \\subseteq { \\mathcal { N } }$ with $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and for all $y \\in \\mathcal M$ , there exists $i \\in S$ with $d ( i , y ) \\geq D _ { i } ( X )$ . It is important to note that this quantification is over all subsets of sufficient size. Hence, in attempting to satisfy the guarantee for a particular subset $S$ , one cannot simply consider a single $i \\in S$ and ignore all of the other points, as $S \\backslash \\{ i \\}$ may itself be a subset to which the guarantee applies.\n\nIt is instructive to briefly consider an example. In Figure 1, $\\mathcal N = \\mathcal M$ , $k = 2$ , and there are 12 individuals, represented by the embedded points. Suppose we want to minimize the $k$ -center objective in the pursuit of fairness: we would then choose the red points. However, this is not a proportional solution because the middle six points constitute half of the points, and would all prefer to be matched to the central blue point. Furthermore, choosing the blue point (and any other center) is a proportional solution, because for any arbitrary group of six points and new proposed center, at least one of the six points will be closer to the blue point than the proposed center.",
      "context_after": "Proportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers. We name a few of these advantages explicitly.\n\nIn the worst case, proportionality is incompatible with all three of the classic $k$ -center, $k$ -means, and $k$ -median objectives; i.e., there exist instances for which any proportional solution has an arbitrarily bad approximation to all objectives. We present such an instance in Example 1, and show in Section 5 that this behavior also arises in real-world datasets.\n\nExample 1. There exist problems for which any proportional clustering has an unbounded approximation to the optimal $k$ -center, $k$ -means, and $k$ -median objectives, and conversely, any clustering with bounded approximation to the optimum on these objectives is not proportional.",
      "referring_paragraphs": [
        "It is instructive to briefly consider an example. In Figure 1, $\\mathcal N = \\mathcal M$ , $k = 2$ , and there are 12 individuals, represented by the embedded points. Suppose we want to minimize the $k$ -center objective in the pursuit of fairness: we would then choose the red points. However, this is not a proportional solution because the middle six points constitute half of the points, and would all prefer to be matched to the central blue point. Furthermore, choosing the blue point (and any ",
        "To parse the definition, again consider Figure 1. Although choosing the red points is not a proportional solution, it is an approximate proportional solution. To see this, suppose the middle six agents wish to deviate to the blue point as before. The green agent would decrease it’s distance to a center by deviating, but not by more than a constant factor, say 3, so the red points would constitute a 3-proportional solution. Note that even with this notion, it remains true that any approximately p",
        "In Figure 1, $\\mathcal N = \\mathcal M$ , $k = 2$ , and there are 12 individuals, represented by the embedded points.",
        "Figure 1: Proportionality Example\n\nProportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/fa7f4d1e5eec2271add1a05dfa004ed5458ca55918f4b84bc08f32bf17a5173f.jpg",
      "image_filename": "fa7f4d1e5eec2271add1a05dfa004ed5458ca55918f4b84bc08f32bf17a5173f.jpg",
      "caption": "Because $n = 6$ and $k$ =3, and there are $6 / 3 = 2$ points at $a$ and $b$ , any proportional solution must include $a$ and $b$ .",
      "context_before": "In the worst case, proportionality is incompatible with all three of the classic $k$ -center, $k$ -means, and $k$ -median objectives; i.e., there exist instances for which any proportional solution has an arbitrarily bad approximation to all objectives. We present such an instance in Example 1, and show in Section 5 that this behavior also arises in real-world datasets.\n\nExample 1. There exist problems for which any proportional clustering has an unbounded approximation to the optimal $k$ -center, $k$ -means, and $k$ -median objectives, and conversely, any clustering with bounded approximation to the optimum on these objectives is not proportional.\n\nProof. The simplest example to see this has $\\mathcal N = \\mathcal M$ , $n = 6$ , and $k = 3$ (i.e., we want to choose 3 centers from six individuals, all of which are possible cluster centers). There are two points at position a, two at position b, and one each at positions c and d. The pairwise distances are given in the following matrix.",
      "context_after": "Because $n = 6$ and $k$ =3, and there are $6 / 3 = 2$ points at $a$ and $b$ , any proportional solution must include $a$ and $b$ . Therefore, one of the points at $c$ or $d$ will have an arbitrarily large value $D _ { i } ( X )$ . The optimal solution on any of the three objectives is to instead choose $c , d$ , and one of $a$ or $b$ . □\n\nFurthermore, as we show in Section 2 and observe empirically in Section 5, proportional solutions may not always exist. We therefore consider the natural approximate notion of proportionality that relaxes the Pareto dominance condition by a multiplicative factor.\n\nDefinition 2. $X \\subseteq { \\mathcal { M } }$ with $| X | = k$ is $\\rho$ -approximate proportional (hereafter $\\rho$ -proportional) if $\\forall S \\subseteq N$ with $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and for all $y \\in \\mathcal { M }$ , there exists $i \\in S$ with $\\rho \\cdot d ( i , y ) \\geq D _ { i } ( X )$ .",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/e1336b103161abf0b5c690869dc422a54905bac4384d5e6f240e105ab3f7acd0.jpg",
      "image_filename": "e1336b103161abf0b5c690869dc422a54905bac4384d5e6f240e105ab3f7acd0.jpg",
      "caption": "Notice that the data is separate into two areas.",
      "context_before": "We begin with a negative result: in the worst case, there may not be an exact proportional solution. The impossibility remains even in the special case when $\\mathcal N = \\mathcal M$ . The latter construction is slightly more involved; we begin with the arbitrary $\\mathcal { N }$ and $\\mathcal { M }$ setting. The basic idea behind both constructions is to create two groups of points very far away from one another with $k = 3$ , ensuring that one group will be served by only one center.\n\nClaim 1. For all $\\rho < 2$ , a $\\rho$ -proportional solution is not guaranteed to exist.\n\nProof. Consider the following instance with $\\mathcal { N } = \\{ a _ { 1 } , a _ { 2 } , \\ldots , a _ { 6 } \\}$ , $\\mathcal { M } = \\{ x _ { 1 } , x _ { 2 } , \\ldots , x _ { 6 } \\}$ and $k = 3$ . Distances are specified in the following table.",
      "context_after": "Notice that the data is separate into two areas. Since $k = 3$ , in a feasible solution, we only open one center in one of these two areas. Without loss of generality, suppose that we open exactly one center among $\\{ x _ { 1 } , x _ { 2 } , x _ { 3 } \\}$ The instance is symmetric, so again suppose without loss of generality that we open $x _ { 1 }$ . Then consider the individuals in $\\{ a _ { 1 } , a _ { 2 } \\}$ . This coalition is of size $\\lceil \\frac { n } { k } \\rceil = 2$ , and both individuals would reduce their distance by a factor of 2 by switching to $x _ { 3 }$ . Thus, any solution is only 2-proportional. □\n\nClaim 2. In the special case where ${ \\mathcal { N } } = { \\mathcal { M } }$ , for all $\\rho < 1 . 5$ , a $\\rho$ -proportional solution is not guaranteed to exist.\n\nProof. Let $k = 5$ . There are three identical clusters of 303 points each (so $n = 9 0 9$ ). The pairwise distance between any two points in different clusters is $\\infty$ . Construct each cluster as follows. There are six types of points, $a _ { 1 } , a _ { 2 } , a _ { 3 } , x _ { 1 } , x _ { 2 } , x _ { 3 }$ (all feasible, since $\\mathcal { M } = \\mathcal { N }$ ). There is exactly one point of type $x _ { 1 }$ , one point of type $x _ { 2 }$ , and one point of type $x _ { 3 }$ . There are 100 points each of type $a _ { 1 }$ , $a _ { 2 }$ , and $a _ { 3 }$ (that is, there are 100 points co-located at each position). The pairwise distance between points in a cluster of given types is specified in the following table. The pairwise distance between any two points of types in $\\{ x _ { 1 } , x _ { 2 } , x _ { 3 } \\}$ is equal to the distance between any two points of types in $\\{ a _ { 1 } , a _ { 2 } , a _ { 3 } \\}$ is equal to 3, which follows from the shortest path distances on a weighted bipartite graph with weights defined by the table.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/d2d189245d47ac3dca9a35c02f957dc1fee94e955864b05113c4a8846902c754.jpg",
      "image_filename": "d2d189245d47ac3dca9a35c02f957dc1fee94e955864b05113c4a8846902c754.jpg",
      "caption": "Since $k = 5$ and there are three clusters, in a feasible solution there is a cluster in which we choose at most one center.",
      "context_before": "Notice that the data is separate into two areas. Since $k = 3$ , in a feasible solution, we only open one center in one of these two areas. Without loss of generality, suppose that we open exactly one center among $\\{ x _ { 1 } , x _ { 2 } , x _ { 3 } \\}$ The instance is symmetric, so again suppose without loss of generality that we open $x _ { 1 }$ . Then consider the individuals in $\\{ a _ { 1 } , a _ { 2 } \\}$ . This coalition is of size $\\lceil \\frac { n } { k } \\rceil = 2$ , and both individuals would reduce their distance by a factor of 2 by switching to $x _ { 3 }$ . Thus, any solution is only 2-proportional. □\n\nClaim 2. In the special case where ${ \\mathcal { N } } = { \\mathcal { M } }$ , for all $\\rho < 1 . 5$ , a $\\rho$ -proportional solution is not guaranteed to exist.\n\nProof. Let $k = 5$ . There are three identical clusters of 303 points each (so $n = 9 0 9$ ). The pairwise distance between any two points in different clusters is $\\infty$ . Construct each cluster as follows. There are six types of points, $a _ { 1 } , a _ { 2 } , a _ { 3 } , x _ { 1 } , x _ { 2 } , x _ { 3 }$ (all feasible, since $\\mathcal { M } = \\mathcal { N }$ ). There is exactly one point of type $x _ { 1 }$ , one point of type $x _ { 2 }$ , and one point of type $x _ { 3 }$ . There are 100 points each of type $a _ { 1 }$ , $a _ { 2 }$ , and $a _ { 3 }$ (that is, there are 100 points co-located at each position). The pairwise distance between points in a cluster of given types is specified in the following table. The pairwise distance between any two points of types in $\\{ x _ { 1 } , x _ { 2 } , x _ { 3 } \\}$ is equal to the distance between any two points of types in $\\{ a _ { 1 } , a _ { 2 } , a _ { 3 } \\}$ is equal to 3, which follows from the shortest path distances on a weighted bipartite graph with weights defined by the table.",
      "context_after": "Since $k = 5$ and there are three clusters, in a feasible solution there is a cluster in which we choose at most one center. In that cluster, suppose first that we choose a center of type $x _ { 1 }$ , $x _ { 2 }$ , or $x _ { 3 }$ . Since the instance is symmetric with respect to this choice, suppose without loss of generality that we choose $x _ { 1 }$ . Then the 200 points of types $a _ { 1 }$ and $a _ { 2 }$ could decrease their distance by a factor of 2 by switching to $x _ { 3 }$ . Since any $\\lceil 9 0 9 / 5 \\rceil = 1 8 2$ points are entitled to deviate, such a choice of $x _ { 1 }$ is not $\\rho$ -proportional for $\\rho < 2$ .\n\nInstead, suppose that we choose a center of type $a _ { 1 }$ , $a _ { 2 }$ , or $a _ { 3 }$ . The instance is again symmetric with respect to this choice, so suppose without loss of generality that we choose $a _ { 1 }$ . Then the 200 points of types $a _ { 2 }$ and $a _ { 3 }$ could decrease their distance by a factor of 1.5 by switching to $x _ { 1 }$ . Thus, in either case, the solution is not $\\rho$ -proportional for any $\\rho < 1 . 5$ . □\n\n2.1 Computing a $( 1 + { \\sqrt { 2 } } )$ -Approximate Proportional Clustering",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg",
      "image_filename": "1905.03674_page0_fig1.jpg",
      "caption": "Figure 2: Diagram for Proof of Theorem 1",
      "context_before": "Proof. Let $X$ be the solution computed by Algorithm 1. First note that $X$ uses at most $k$ centers, since it only opens a center when $\\lceil \\frac { n } { k } \\rceil$ unmatched points are absorbed by the ball around that center, and this can happen at most $k$ times. Now, suppose for a contradiction that $X$ is not a $( 1 + { \\sqrt { 2 } } )$ -proportional clustering. Then there exists $S \\subseteq { \\mathcal { N } }$ with $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and $y \\in \\mathcal { M }$ such that\n\n$$ \\forall i \\in S, (1 + \\sqrt {2}) \\cdot d (i, y) < D _ {i} (X). \\tag {1} $$\n\nLet be the distance of the farthest agent from in $S$ , that is, $r _ { y } : = \\operatorname* { m a x } _ { i \\in S } d ( i , y )$ , and call this agent $i ^ { * }$ . $r _ { y }$ $y$ There are two cases. In the first case, $B ( x , r _ { y } ) \\cap S = \\emptyset$ for all $x \\in X$ . This immediately yields a contradiction, because it implies that Algorithm 1 would have opened $y$ . In particular, note that $S \\subseteq B ( y , r _ { y } )$ , so if $S \\cap B ( x , r _ { y } ) = \\emptyset$ for all $x \\in X$ , then $B ( y , r _ { y } )$ would have had at least $\\lceil \\frac { n } { k } \\rceil$ unmatched points.",
      "context_after": "In the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :\n\n$$ \\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {d (i ^ {*} , x)}{d (i ^ {*} , y)}\\right) \\\\ \\leq \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {r _ {y} + d (i , x) + d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\min \\left(\\frac {r _ {y}}{d (i , y)}, 2 + \\frac {d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\max _ {z \\geq 0} \\left(\\min \\left(z, 2 + 1 / z\\right)\\right) = 1 + \\sqrt {2} \\\\ \\end{array} $$\n\nwhich violates equation 1.",
      "referring_paragraphs": [
        "In the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :",
        "Figure 2: Diagram for Proof of Theorem 1\n\nIn the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :"
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1a8cffd9414d44d51f3fd0bfc9a91001444d5623a645d91edfa245fb17c564cb.jpg",
      "image_filename": "1a8cffd9414d44d51f3fd0bfc9a91001444d5623a645d91edfa245fb17c564cb.jpg",
      "caption": "The distances satisfy the triangle inequality.",
      "context_before": "$$ \\begin{array}{l} \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {d (i ^ {*} , x)}{d (i ^ {*} , y)}\\right) \\\\ \\leq \\min \\left(\\frac {d (i , x)}{d (i , y)}, \\frac {r _ {y} + d (i , x) + d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\min \\left(\\frac {r _ {y}}{d (i , y)}, 2 + \\frac {d (i , y)}{r _ {y}}\\right) \\\\ \\leq \\max _ {z \\geq 0} \\left(\\min \\left(z, 2 + 1 / z\\right)\\right) = 1 + \\sqrt {2} \\\\ \\end{array} $$\n\nwhich violates equation 1.\n\nIt is not hard to show that there exists an instance for which Algorithm 1 yields exactly this bound. Consider the following instance with $\\mathcal { N } = \\{ a _ { 1 } , a _ { 2 } , \\ldots , a _ { 6 } \\}$ , $\\mathcal { M } = \\{ x _ { 1 } , x _ { 2 } , x _ { 3 } , x _ { 4 } \\}$ and $k = 3$ . Distances are specified in the following table, where $\\epsilon > 0$ is some small constant.",
      "context_after": "The distances satisfy the triangle inequality. Note that Algorithm 1 will open $x _ { 2 }$ and $x _ { 4 }$ . The coalition $\\{ a _ { 1 } , a _ { 2 } \\}$ can each reduce their distance by a multiplicative factor approaching $1 + { \\sqrt { 2 } }$ as $\\epsilon 0$ by deviating to x1. $x _ { 1 }$ □\n\n2.2 Local Capture Heuristic\n\nWe observe that while our Greedy Capture algorithm (Algorithm 1) always produces an approximately proportional solution, it may not produce an exactly proportional solution in practice, even on instances where such solutions exist (see Figure 4a and Figure 4b). We therefore introduce a Local Capture heuristic for searching for more proportional clusterings. Algorithm 2 takes a target value of $\\rho$ as a parameter, and proceeds by iteratively finding a center that violates $\\rho$ -fairness and swapping it for the center in the current solution that is least demanded.",
      "referring_paragraphs": [],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig2.jpg",
      "image_filename": "1905.03674_page0_fig2.jpg",
      "caption": "(a) Iris",
      "context_before": "The Diabetes data set is larger and more complex. As shown in Figure 4b, $k$ -means $^ { + + }$ no longer always finds an exact proportional solution. Local Capture always finds a better than 1.01-proportional solution. As shown in Figure 5b, the $k$ -means objectives of the solutions are separated, although generally on the same order of magnitude.\n\nFor the KDD data set, proportionality and the $k$ -means object appear to be in conflict. Greedy Capture’s performance is comparable to Local Capture on KDD, so we omit it for clarity. In Figures 4c and 5c, note that the gap between $\\rho$ and the $k$ -means objective for the $k$ -means $^ { + + }$ and Local Capture algorithms is between three and four orders of magnitude. We suspect this is due to the presence of significant outliers in the KDD data set. This is in keeping with the theoretical impossibility of simultaneously approximating the optima on both objectives, and demonstrates that this tension arises in practice as well as theory.\n\n3We run $k$ -means $^ { + + }$ on this entire 100,000 point sample. For efficiency, we run our Local Capture algorithm by further sampling 5,000 points uniformly at random to treat as $\\mathcal { N }$ and sampling 400 points via the $k$ -means++ initialization to treat as $\\mathcal { M }$ . For the sake of a fair comparison, we generate a different sample of 400 centers using the $k$ -means++ initialization that we use to determine the value of we report for both Local Capture and the $k$ -means++ algorithm. The $k$ -means objective is $\\rho$ measured on the original 100,000 points for both algorithms.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg",
      "image_filename": "1905.03674_page0_fig3.jpg",
      "caption": "(b) Diabetes",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig4.jpg",
      "image_filename": "1905.03674_page0_fig4.jpg",
      "caption": "(c) KDD, geometric scale",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_10",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig5.jpg",
      "image_filename": "1905.03674_page0_fig5.jpg",
      "caption": "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional (a) Iris",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   \n(a) Iris"
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg",
      "image_filename": "1905.03674_page0_fig6.jpg",
      "caption": "(b) Diabetes",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_12",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig7.jpg",
      "image_filename": "1905.03674_page0_fig7.jpg",
      "caption": "(c) KDD, geometric scale Figure 5: $k$ -means objective",
      "context_before": "",
      "context_after": "5.2 Proportionality and Low $k$ -means Objective\n\nNote that if one is allowed to use $2 k$ centers when $k$ is given as input, one can trivially achieve the proportionality of Local Capture and the $k$ -means objective of the $k$ -means $^ { + + }$ algorithm by taking the union of the two solutions. Thinking in this way leads to a different way of quantifying the tradeoff between proportionality and the $k$ -means objective: Given an approximately proportional solution, how many extra centers are necessary to get comparable $k$ -means objective as the $k$ -means $^ { + + }$ algorithm? For a given data set, the answer is a value between 0 and $k$ , where larger numbers indicate more incompatibility, and lower numbers indicate less incompatibility.\n\nTo answer this question, we compute the union of centers found by Local Capture and the $k$ -means $^ { + + }$ algorithm. We then greedily remove centers as long as doing so does not increase the minimum $\\rho$ such that the solution is $\\rho$ -proportional (defined on $k$ , not $2 k$ ) by more than a multiplicative factor of $\\alpha$ , and does not increase the $k$ -means objective by more than a multiplicative factor $\\beta$ .",
      "referring_paragraphs": [
        "Figure 5: $k$ -means objective"
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.03674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1905.10674": [
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "image_filename": "1905.10674_page0_fig0.jpg",
      "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "context_before": "Learning low-dimensional embeddings of the nodes in a graph is a fundamental technique underlying state-of-the-art approaches to link prediction and recommender systems (Hamilton et al., 2017b). However, in many applications— especially those involving social graphs—it is desirable to exercise control over the information contained within learned node embeddings. For instance, we may want to ensure that recommendations are fair or balanced with respect to certain attributes (e.g., that they do not depend on a user’s race or gender) or we may want to ensure privacy by not exposing certain attributes through learned node representations. In this work we investigate the feasibility of enforcing such invariance constraints on (social) graph embeddings.\n\n1McGill University 2Mila 3Facebook AI Research. Correspondence to: Avishek Joey Bose <joey.bose@mail.mcgill.ca>.\n\nProceedings of the ${ 3 6 } ^ { t h }$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).",
      "context_after": "While enforcing invariance constraints on general classification models (Chouldechova, 2017; Gajane & Pechenizkiy, 2017; Kamishima et al., 2012) and collaborative filtering algorithms (Yao & Huang, 2017) has received considerable attention in recent years, these techniques have yet to be considered within the context of graph embeddings—a setting that introduces particular challenges due to the non-i.i.d. and non-Euclidean nature of relational, graph data.\n\nMoreover, in the case of social graphs and large-scale recommender systems, it is often the case that there are many possible sensitive attributes that we may want to enforce invariance constraints over. Previous work on enforcing invariance (or “fairness”) in social applications has generally focused on situations that involve one sensitive attribute (e.g., age in the context of credit or loan decisions; Zemel et al. (2013)), but in the context of social graph embeddings there can be an extremely large number of possible sensitive attributes. In fact, in extreme settings we may even want to be fair with respect to the existence of individual edges. For instance, a user on a social networking platform might want that platform’s recommender system to ignore the fact that they are friends with a certain other user, or that they engaged with a particular piece of content.\n\nOur contributions. We introduce an adversarial framework to enforce compositional fairness constraints on graph embeddings for multiple sensitive attributes. The insight be-",
      "referring_paragraphs": [
        "We investigated the impact of enforcing invariance on graph embeddings using three datasets: Freebase $1 5 \\mathrm { k } { - } 2 3 7 ^ { 4 }$ , MovieLens- $1 \\mathbf { M } ^ { 5 }$ , and an edge-prediction dataset derived from Reddit.6 The dataset statistics are given in Table 1. Our experimental setup closely mirrors that of (Madras et al., 2018) where we jointly train the main model with adversaries, but when testing invariance, we train a new classifier (with the same capacity as the discrimi",
        "Figure 1.",
        "We investigated the impact of enforcing invariance on graph embeddings using three datasets: Freebase $1 5 \\mathrm { k } { - } 2 3 7 ^ { 4 }$ , MovieLens- $1 \\mathbf { M } ^ { 5 }$ , and an edge-prediction dataset derived from Reddit.6 The dataset statistics are given in Table 1.",
        "To select the “sensitive” subreddit communities,\n\nTable 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/780c1e61f2252b7a2e1b1dcd4c39edff255e263631a656a9a4b4b13a661bd267.jpg",
      "image_filename": "780c1e61f2252b7a2e1b1dcd4c39edff255e263631a656a9a4b4b13a661bd267.jpg",
      "caption": "Table 1. Statistics for the three datasets, including the total number of nodes $( | \\nu | )$ and number of nodes with sensitive attributes $| \\mathcal { T } ^ { * } |$ , the number of sensitive attributes and their types and the total number of edges in the graph.",
      "context_before": "To construct the edge prediction task, we examined all comments from the month of November in 2017, and we placed an edge between a user and a community if this user commented on that community at least once within this time period. We then took the 10-core of this graph to remove low-degree nodes, which resulted in a graph with approximately 366K users, 18K communities, and 7M edges. Given this graph, the main task is to train an edge-prediction model on $9 0 \\%$ of the user-subreddit edges and then predict missing edges in a held-out test set of the remaining edges.\n\nReddit is a pseudonymous website with no public user attributes. Thus, to define sensitive attributes, we treat certain subreddit nodes as sensitive nodes, and the sensitive attributes for users are whether or not they have an edge connecting to these sensitive nodes. In other words, the fairness objective in this setting is to force the model to be invariant to whether or not a user commented on a particular community. To select the “sensitive” subreddit communities,\n\n[Section: Compositional Fairness Constraints for Graph Embeddings]",
      "context_after": "",
      "referring_paragraphs": [
        "We investigated the impact of enforcing invariance on graph embeddings using three datasets: Freebase $1 5 \\mathrm { k } { - } 2 3 7 ^ { 4 }$ , MovieLens- $1 \\mathbf { M } ^ { 5 }$ , and an edge-prediction dataset derived from Reddit.6 The dataset statistics are given in Table 1. Our experimental setup closely mirrors that of (Madras et al., 2018) where we jointly train the main model with adversaries, but when testing invariance, we train a new classifier (with the same capacity as the discrimi",
        "Figure 1.",
        "We investigated the impact of enforcing invariance on graph embeddings using three datasets: Freebase $1 5 \\mathrm { k } { - } 2 3 7 ^ { 4 }$ , MovieLens- $1 \\mathbf { M } ^ { 5 }$ , and an edge-prediction dataset derived from Reddit.6 The dataset statistics are given in Table 1.",
        "To select the “sensitive” subreddit communities,\n\nTable 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg",
      "image_filename": "1905.10674_page0_fig1.jpg",
      "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).",
      "context_before": "",
      "context_after": "we randomly sampled 10 from the top-100 communities by degree.7 Note that this setting represents the extreme case where we want the model to be invariant with respect to the existence of particular edges in the input graph.\n\nAs with MovieLens-1M, we use a simple “embeddinglookup” encoder. In this case, there is only a single relation type—indicating whether a Reddit user has commented on a “subreddit” community. Thus, we employ a simple dotproduct based scoring function, $s ( \\langle u , r , v \\rangle ) = \\mathbf { z } _ { u } ^ { \\top } \\mathbf { z } _ { v }$ , and we use a max-margin loss as in Equation (9).\n\nWe now address the core experimental questions (Q1-Q3).",
      "referring_paragraphs": [
        "a baseline approach that does not include the invariance constraints. Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary. Figures 5 and 6 illustrate this tradeoff and show how the RMSE for the edge prediction task and ability to predict the sensitive attributes change as we vary ",
        "In all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2). In other words, training a model to jointly remove information about the sensitive a",
        "Figure 2.",
        "In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than\n\nTable 2.",
        "Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
        "In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg",
      "image_filename": "1905.10674_page0_fig2.jpg",
      "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
      "context_before": "We now address the core experimental questions (Q1-Q3).\n\nQ1: THE INVARIANCE-ACCURACY TRADEOFF\n\nIn order to quantify the extent to which the learned embeddings are invariant to the sensitive attributes (e.g., after adversarial training), we freeze the trained compositional encoder C-ENC and train an new MLP classifier to predict each sensitive attribute from the filtered embeddings (i.e., we train one new classifier per sensitive attribute). We also evaluate the performance of these filtered embeddings on the original prediction tasks. In the best case, a newly trained MLP classifier should have random accuracy when attempting to predict the sensitive attributes from the filtered embeddings, but these embeddings should still provide strong",
      "context_after": "",
      "referring_paragraphs": [
        "said, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
        "ADVERSARY</td><td>MAJORITY\nCLASSIFIER</td><td>RANDOM\nCLASSIFIER</td></tr><tr><td>GENDER</td><td>0.712</td><td>0.532</td><td>0.541</td><td>0.551</td><td>0.511</td><td>0.5</td><td>0.5</td></tr><tr><td>AGE</td><td>0.412</td><td>0.341</td><td>0.333</td><td>0.321</td><td>0.313</td><td>0.367</td><td>0.141</td></tr><tr><td>OCCUPATION</td><td>0.146</td><td>0.141</td><td>0.108</td><td>0.131</td><td>0.121</td><td>0.126</td><td>0.05</td></tr></table>\n\nTable 3.",
        "That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg",
      "image_filename": "1905.10674_page0_fig3.jpg",
      "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.",
      "context_before": "",
      "context_after": "performance on the main edge prediction task. Thus, for binary sensitive attributes, an ideal result is an AUC score of 0.5 when attempting to predict the sensitive attributes from the learned embeddings.\n\nOverall, we found that on the more realistic social recommendation datasets—i.e., the MovieLens-1M and Reddit datasets—our approach was able to achieve a reasonable tradeoff, with the near-complete removal of the sensitive information leading to a roughly $10 \\%$ relative error increase on the edge prediction tasks. In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than\n\n[Section: Compositional Fairness Constraints for Graph Embeddings]",
      "referring_paragraphs": [
        "said, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "We tested this phenomenon on the Reddit dataset, since it has the largest number of sensitive attributes (10, compared to 3 sensitive attributes for the other two datasets). During training we held out $1 0 \\%$ of the combinations of sensitive attributes, and we then evaluated the model’s ability to enforce invariance on this held-out set. As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of ",
        "Figure 4.",
        "That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations.",
        "The main model however\n\nTable 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/a3c4c894ae8598d88c0f82c276d5ffe1f7191e9a3253419aa3cfbeaf450db513.jpg",
      "image_filename": "a3c4c894ae8598d88c0f82c276d5ffe1f7191e9a3253419aa3cfbeaf450db513.jpg",
      "caption": "Table 2. Ability to predict sensitive attributes on the MovieLens data when using various embedding approaches. For gender attribute the score is AUC while for age and occupation attributes the score is micro averaged F1. The columns represent the different embedding approaches (e.g., with or without adversarial regularizatin) while the rows are the attribute being classified.",
      "context_before": "Overall, we found that on the more realistic social recommendation datasets—i.e., the MovieLens-1M and Reddit datasets—our approach was able to achieve a reasonable tradeoff, with the near-complete removal of the sensitive information leading to a roughly $10 \\%$ relative error increase on the edge prediction tasks. In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than\n\n[Section: Compositional Fairness Constraints for Graph Embeddings]\n\n7We excluded the top-5 highest-degree outlying communities.",
      "context_after": "",
      "referring_paragraphs": [
        "a baseline approach that does not include the invariance constraints. Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary. Figures 5 and 6 illustrate this tradeoff and show how the RMSE for the edge prediction task and ability to predict the sensitive attributes change as we vary ",
        "In all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2). In other words, training a model to jointly remove information about the sensitive a",
        "Figure 2.",
        "In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than\n\nTable 2.",
        "Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
        "In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/8b9fae945d8189edf34a51eb0c9f75df2c576695da30bf4e4d978dde89dd4774.jpg",
      "image_filename": "8b9fae945d8189edf34a51eb0c9f75df2c576695da30bf4e4d978dde89dd4774.jpg",
      "caption": "Table 3. Ability to predict sensitive attributes on the Freebase15k-237 data when using various embedding approaches. AUC scores are reported, since all the sensitive attributes are binary. The mean rank on the main edge-prediction task is also reported.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "said, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
        "ADVERSARY</td><td>MAJORITY\nCLASSIFIER</td><td>RANDOM\nCLASSIFIER</td></tr><tr><td>GENDER</td><td>0.712</td><td>0.532</td><td>0.541</td><td>0.551</td><td>0.511</td><td>0.5</td><td>0.5</td></tr><tr><td>AGE</td><td>0.412</td><td>0.341</td><td>0.333</td><td>0.321</td><td>0.313</td><td>0.367</td><td>0.141</td></tr><tr><td>OCCUPATION</td><td>0.146</td><td>0.141</td><td>0.108</td><td>0.131</td><td>0.121</td><td>0.126</td><td>0.05</td></tr></table>\n\nTable 3.",
        "That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg",
      "image_filename": "1905.10674_page0_fig4.jpg",
      "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$",
      "context_before": "",
      "context_after": "a baseline approach that does not include the invariance constraints. Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary. Figures 5 and 6 illustrate this tradeoff and show how the RMSE for the edge prediction task and ability to predict the sensitive attributes change as we vary the regularization strength, $\\lambda$ . As expected, increasing $\\lambda$ does indeed produce more invariant embeddings but leads to higher RMSE values. Figures 3 and 4 similiarly summarize these results on Reddit.\n\nInterestingly, we found that on the Freebase15k-237 dataset it was not possible to completely remove the sensitive information without incurring a significant decrease in accu-",
      "referring_paragraphs": [
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_9",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg",
      "image_filename": "1905.10674_page0_fig5.jpg",
      "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .",
      "context_before": "a baseline approach that does not include the invariance constraints. Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary. Figures 5 and 6 illustrate this tradeoff and show how the RMSE for the edge prediction task and ability to predict the sensitive attributes change as we vary the regularization strength, $\\lambda$ . As expected, increasing $\\lambda$ does indeed produce more invariant embeddings but leads to higher RMSE values. Figures 3 and 4 similiarly summarize these results on Reddit.\n\nInterestingly, we found that on the Freebase15k-237 dataset it was not possible to completely remove the sensitive information without incurring a significant decrease in accu-",
      "context_after": "racy on the original edge prediction task. This result is not entirely surprising, since for this dataset the “sensitive” attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge/relation prediction task. However, it is an interesting point of reference that demonstrates the potential limitations of removing sensitive information from learned graph embeddings.\n\nQ2: THE IMPACT OF COMPOSITIONALITY\n\nIn all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2). In other words, training a model to jointly remove information about the sensitive attributes using the compositional encoder (Equation 6) removed more information about the sensitive attributes than training separate adversarially regularized embedding models for each sensitive attribute. This result is not entirely surprising, as it essentially indicates that the different sensitive attributes (age, gender, and occupation) are correlated in this dataset. Nonetheless, it is a positive result indicating that the extra flexibility afforded by the compositional approach does not necessarily lead to a decrease in performance. That",
      "referring_paragraphs": [
        "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .\n\nracy on the original edge prediction task. This result is not entirely surprising, since for this dataset the “sensitive” attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge/relation prediction task. However, it is an interesting point of reference that demonstrates the potential limitations of removing sensitive information from learned graph embeddings."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_10",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg",
      "image_filename": "1905.10674_page0_fig6.jpg",
      "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.",
      "context_before": "We tested this phenomenon on the Reddit dataset, since it has the largest number of sensitive attributes (10, compared to 3 sensitive attributes for the other two datasets). During training we held out $1 0 \\%$ of the combinations of sensitive attributes, and we then evaluated the model’s ability to enforce invariance on this held-out set. As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations. The Appendix contains further results demonstrating how this trends scales gracefully when we increase the number of sensitive attributes from 10 to 50.\n\nIn all of the above results, we used the ability to classify the sensitive attributes as a proxy for bias being contained within the embeddings. While this is a standard approach, e.g., see Edwards & Storkey (2015), and an intuitive method for evaluating representational invariance—a natural question is whether the adversarial regularization also decreases bias in the edge prediction tasks. Ideally, after filtering the embeddings, we would have that the edge predictions themselves are not biased according to the sensitive attributes.\n\nTo quantify this issue, we computed a “prediction bias” score for the MovieLens1M dataset: For each movie, we computed the absolute difference between the average rating predicted for each possible value of a sensitive attribute and we then averaged these scores over all movies. Thus, for example, the bias score for gender corresponds to the average absolute difference in predicted ratings for male vs. female users, across all movies. From the perspective of fairness our adversary imposes a soft demographic parity constraint on the main task. A reduction in prediction bias across the different subgroups represents an empirical measure of achieving demographic parity. Figure 7 highlights these results, which show that adversarial regularization does in-",
      "context_after": "deed drastically reduce prediction bias. Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlation between sensitive attributes.\n\n6. Discussion and Conclusion\n\nOur work sheds light on how fairness can be enforced in graph representation learning—a setting that is highly relevant to large-scale social recommendation and networking platforms. We found that using our proposed compositional adversary allows us to flexibly accomodate unseen combinations of fairness constraints without explicitly training on them. This highlights how fairness could be deployed in a real-word, user-driven setting, where it is necessary to optionally enforce a large number of possible invariance constraints over learned graph representations.",
      "referring_paragraphs": [
        "To quantify this issue, we computed a “prediction bias” score for the MovieLens1M dataset: For each movie, we computed the absolute difference between the average rating predicted for each possible value of a sensitive attribute and we then averaged these scores over all movies. Thus, for example, the bias score for gender corresponds to the average absolute difference in predicted ratings for male vs. female users, across all movies. From the perspective of fairness our adversary imposes a soft",
        "Figure 7 highlights these results, which show that adversarial regularization does in-",
        "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.\n\ndeed drastically reduce prediction bias. Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlation between sensitive attributes."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_11",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/c7381818005839c7ebd67bbc37a3872e8481163ffc331c61a512d6cefbc9de93.jpg",
      "image_filename": "c7381818005839c7ebd67bbc37a3872e8481163ffc331c61a512d6cefbc9de93.jpg",
      "caption": "Table 4. Average AUC values across top-k sensitive attributes for Reddit. The results are reported on a Held Out test of different combinations of attributes.",
      "context_before": "To generate negative triplets we randomly sample either a head or tail entity during training, with a ratio of 20 negatives for each positive triplet. The TransD model is trained for 100 epochs with an embedding dimension of 20, selected using cross-validation, while the sensitive attribute classifers are trained for 50 epochs. The discriminators, sensitive attribute classifier and adversarial filters are modelled as MLP’s with 4,4 and 2 layers respectively. Lastly, we use the training, validation and testing splits provided in the datasets.\n\nAs with FB15k-237 we use model the discriminators and sensitive attribute classifiers are modelled as MLP’s but 9 layers with dropout with $p = 0 . 3$ between layers while the adversarial filter remains unchanged from FB15k-237. We found that regularization was crucial to the performance of main model and we use BatchNorm after the embedding lookup in the main model which has an embedding dimensionality of 30. As only user nodes contain sensitive attributes our discriminators do not compute losses using movie nodes. Finally, to train our sensitive attribute classifier we construct a $9 0 \\%$ split of all users while the remaining user nodes are used for test. The same ratio of train/test is used for the actual dataset which constains users,movies and corresponding ratings for said movies. Finally, we train the main model and sensitive attribute classifiers for 200 epochs.\n\nLike FB15k-237 we generate negative triplets by either sampling head or tail entities which are either users or subreddits but unnlike FB15k-237 we keep the ratio of negatives and positives the same. We also inherit the same architectures for discriminator, sensitive attribute classifier and attribute filters used in MovieLens1M. The main model however",
      "context_after": "uses an embedding dimensionality of 50. Similar to Movie-Lens1M only user nodes contain sensitive attributes and as such the discriminator and sensitive attribute classifier does not compute losses with respect to subreddit nodes. Also, our training set comprises of a $9 0 \\%$ split of all edges while the the remaining $1 0 \\%$ is used as a test set. To test compositional generalizability we held out $1 0 \\%$ of user nodes. Lastly, we train the main model for 50 epochs and the sensitive attribute classifier for 100 epochs.\n\nE. Additional Results on Reddit\n\nTo the test degree of which invariance is affected by the number of sensitive attributes we report additional results on the Reddit dataset. Specifically, we report results for the Held out set with 20, 30, 40, and 50 sensitive attributes. Overall, these results show no statistically significant degradation in terms of invariance performance or task accuracy.",
      "referring_paragraphs": [
        "said, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "We tested this phenomenon on the Reddit dataset, since it has the largest number of sensitive attributes (10, compared to 3 sensitive attributes for the other two datasets). During training we held out $1 0 \\%$ of the combinations of sensitive attributes, and we then evaluated the model’s ability to enforce invariance on this held-out set. As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of ",
        "Figure 4.",
        "That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations.",
        "The main model however\n\nTable 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.10674_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1905.11361": [
    {
      "doc_id": "1905.11361",
      "figure_id": "1905.11361_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.11361/1905.11361/hybrid_auto/images/4bfd519d4cf476219765b3fe14b5154e768742be6588dcb3df94dedd45a09170.jpg",
      "image_filename": "4bfd519d4cf476219765b3fe14b5154e768742be6588dcb3df94dedd45a09170.jpg",
      "caption": "Table 1: Confusion matrix for $\\pi _ { \\mathrm { g r e e d y } }$ assuming $\\epsilon \\leq 1 / 4$ and $\\epsilon ^ { \\prime } \\le p \\le 1 / 2$ .",
      "context_before": "We use the following parameters in the next theorems:\n\n$$ a = \\left\\lceil \\frac {\\log (\\frac {(1 - \\epsilon) (1 - \\epsilon^ {\\prime}) (1 + \\sigma)}{\\epsilon \\epsilon^ {\\prime} (1 - \\sigma)})}{\\log (\\frac {1 + \\sigma}{1 - \\sigma})} \\right\\rceil \\gg \\frac {1}{\\sigma} \\quad \\mathrm {a n d} \\quad z = \\left\\lceil \\frac {\\log (\\frac {p (1 - \\epsilon^ {\\prime}) (1 + \\sigma)}{\\epsilon^ {\\prime} (1 - p) (1 - \\sigma)})}{\\log (\\frac {1 + \\sigma}{1 - \\sigma})} \\right\\rceil $$\n\nTheorem 9 (Expected number of tests per type). The expected number of tests until a decision (namely accept or reject) for skilled candidates is $\\begin{array} { r } { \\mathbb { E } [ \\tau _ { s } ] = \\frac { 1 } { \\sigma } \\left( a \\cdot \\frac { 1 - ( \\frac { 1 - \\sigma } { 1 + \\sigma } ) ^ { z } } { 1 - ( \\frac { 1 - \\sigma } { 1 + \\sigma } ) ^ { a } } - z \\right) \\approx \\frac { 2 a } { 1 + \\sigma } - \\frac { z } { \\sigma } } \\end{array}$ and $\\begin{array} { r } { \\mathbb { E } [ \\tau _ { u } ] = \\frac { 1 } { \\sigma } \\left( z - a \\cdot \\frac { 1 - ( \\frac { 1 + \\sigma } { 1 - \\sigma } ) ^ { z } } { 1 - ( \\frac { 1 + \\sigma } { 1 - \\sigma } ) ^ { a } } \\right) \\approx \\frac { z } { \\sigma } } \\end{array}$ for unskilled candidates.",
      "context_after": "For the probabilities of the candidates to be accepted or rejected, conditioned on their true skill level, we present the results in a form of confusion matrix in Table 1.\n\nTheorem 10. The expected number of tests until deciding whether to accept or reject a candidate is $\\begin{array} { r } { \\mathbb { E } [ \\tau | \\pi ( y _ { i , \\tau } ) \\in \\{ 0 , 1 \\} ] \\approx { \\frac { a p } { \\sigma } } } \\end{array}$ , where $a \\gg \\frac { 1 } { \\sigma }$ .\n\n4 Fairness Considerations in the Two-Group Setting",
      "referring_paragraphs": [
        "For the probabilities of the candidates to be accepted or rejected, conditioned on their true skill level, we present the results in a form of confusion matrix in Table 1.",
        "Deviations for the confusion matrix (Table 1). We split the claim in the confusion matrix (Table 1) into two parts. First, using equation (2.4) from chapter XIV [page 345] in [8], we get",
        "Table 1: Confusion matrix for $\\pi _ { \\mathrm { g r e e d y } }$ assuming $\\epsilon \\leq 1 / 4$ and $\\epsilon ^ { \\prime } \\le p \\le 1 / 2$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.11361_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1905.12843": [
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig0.jpg",
      "image_filename": "1905.12843_page0_fig0.jpg",
      "caption": "Fair Regression: Quantitative Definitions and Reduction-based Algorithms",
      "context_before": "We ran Algorithm 1 on each training set over a range of constraint slack values $\\hat { \\varepsilon }$ , with a fixed discretization grid of size 40: $\\mathcal { Z } = \\{ 1 / 4 0 , 2 / 4 0 , \\dots , 1 \\}$ . Among the solutions for different $\\hat { \\varepsilon }$ , we selected the ones on the Pareto front based on their training losses and SP disparity $\\operatorname* { m a x } _ { a , z } \\left\\{ \\hat { \\gamma } _ { a , z } \\right\\}$ . We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.\n\nWe ran our algorithm with the three types of reductions\n\n[Section: Fair Regression: Quantitative Definitions and Reduction-based Algorithms]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig1.jpg",
      "image_filename": "1905.12843_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig2.jpg",
      "image_filename": "1905.12843_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "image_filename": "1905.12843_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig4.jpg",
      "image_filename": "1905.12843_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_6",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig5.jpg",
      "image_filename": "1905.12843_page0_fig5.jpg",
      "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "context_before": "",
      "context_after": "from Section 4.4: reductions to cost-sensitive (CS) oracles, least-squares (LS) oracles, and logistic-loss minimization (LR) oracles. Our CS oracle sought the linear model minimizing weighted hinge-loss (as a surrogate for weighted classification error). Because of unfavorable scaling of the cost-sensitive problem sizes (see Section 4.4), we only ran the CS oracle on the three small datasets. We considered two variants of LS and LR oracles: linear learners from scikit-learn (Pedregosa et al., 2011), and tree ensembles from XGBoost (Chen & Guestrin, 2016). Tree ensembles heavily overfitted smaller datasets, so we only show their performance on two larger datasets. We only used LR oracles when the target loss was logistic, whereas we used LS oracles across all datasets.\n\nIn addition to our algorithm, we also evaluated regression without any fairness constraints, and two baselines from the fair classification and fair regression literature.\n\nOn the three datasets where the task was least-squares regression, we evaluated the full substantive equality of opportunity (SEO) estimate of Johnson et al. (2016). It can be obtained in a closed form by solving for the linear model that minimizes least-squares error while having zero correlation with the protected attribute. In contrast, our method seeks to limit not just correlation, but statistical dependence.",
      "referring_paragraphs": [
        "We ran Algorithm 1 on each training set over a range of constraint slack values $\\hat { \\varepsilon }$ , with a fixed discretization grid of size 40: $\\mathcal { Z } = \\{ 1 / 4 0 , 2 / 4 0 , \\dots , 1 \\}$ . Among the solutions for different $\\hat { \\varepsilon }$ , we selected the ones on the Pareto front based on their training losses and SP disparity $\\operatorname* { m a x } _ { a , z } \\left\\{ \\hat { \\gamma } _ { a , z } \\right\\}$ . We then evaluated the selected predictors on the test set, ",
        "In Figure 1, we see that all of our reductions are able to significantly reduce disparity, without strongly impacting the overall loss. On communities & crime, there is a more substantial accuracy–fairness tradeoff, which can be used as a starting point to diagnose the data quality for the two racial subgroups. Our methods dominate SEO in leastsquares tasks, but are slightly worse than FC in logistic regression. The difference is statistically significant only on adult, where it points to the li",
        "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.",
        "We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.",
        "Figure 1.",
        "Table 1.",
        "The details are listed in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig6.jpg",
      "image_filename": "1905.12843_page0_fig6.jpg",
      "caption": "Fair Regression: Quantitative Definitions and Reduction-based Algorithms",
      "context_before": "$$ \\text {f o r a l l} i \\in [ n ]: t _ {i} \\geq \\frac {\\alpha}{2} - Y _ {i} \\langle \\beta , x _ {i} \\rangle , $$\n\n$$ f o r \\quad j \\in [ d ]: - 1 \\leq \\beta_ {j} \\leq 1. $$\n\n[Section: Fair Regression: Quantitative Definitions and Reduction-based Algorithms]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig7.jpg",
      "image_filename": "1905.12843_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig8.jpg",
      "image_filename": "1905.12843_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg",
      "image_filename": "1905.12843_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig10.jpg",
      "image_filename": "1905.12843_page0_fig10.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_12",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig11.jpg",
      "image_filename": "1905.12843_page0_fig11.jpg",
      "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Evaluation on the training sets. In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors. Our method generally dominated or closely matched the baseline methods. The SEO method provided solutions that were not Pareto optimal on the law school data set.",
        "In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors.",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_13",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/a45cd44bdda9f5292ddead89fd1464b57cd940bf2785836517bacce05f8ba303.jpg",
      "image_filename": "a45cd44bdda9f5292ddead89fd1464b57cd940bf2785836517bacce05f8ba303.jpg",
      "caption": "Table 1. Runtime comparison on the oracles over different model classes. We ran the oracles on a sub-sampled law school data set with 1,000 examples, using a machine with a 2.7 GHz Intel processor and 16GB memory.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We ran Algorithm 1 on each training set over a range of constraint slack values $\\hat { \\varepsilon }$ , with a fixed discretization grid of size 40: $\\mathcal { Z } = \\{ 1 / 4 0 , 2 / 4 0 , \\dots , 1 \\}$ . Among the solutions for different $\\hat { \\varepsilon }$ , we selected the ones on the Pareto front based on their training losses and SP disparity $\\operatorname* { m a x } _ { a , z } \\left\\{ \\hat { \\gamma } _ { a , z } \\right\\}$ . We then evaluated the selected predictors on the test set, ",
        "In Figure 1, we see that all of our reductions are able to significantly reduce disparity, without strongly impacting the overall loss. On communities & crime, there is a more substantial accuracy–fairness tradeoff, which can be used as a starting point to diagnose the data quality for the two racial subgroups. Our methods dominate SEO in leastsquares tasks, but are slightly worse than FC in logistic regression. The difference is statistically significant only on adult, where it points to the li",
        "Runtime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.",
        "We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.",
        "Figure 1.",
        "Table 1.",
        "The details are listed in Table 1."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg",
      "image_filename": "1905.12843_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig13.jpg",
      "image_filename": "1905.12843_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_16",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig14.jpg",
      "image_filename": "1905.12843_page0_fig14.jpg",
      "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.",
      "context_before": "",
      "context_after": "In our experiments, this optimization problem was solved with the Gurobi Optimizer (Gurobi Optimization, 2018).\n\nRuntime comparison. We performed a comparison on the running time of a single call of the three supervised learning oracles. On a subsampled law school data set with 1,000 examples, we ran the oracles to solve an instance of the $\\boldsymbol { \\mathrm { B E S T } } _ { h }$ problem, optimizing over either the linear models or tree ensemble models. The details are listed in Table 1. We also compare the number of oracle calls for different specified values of fairness slackness.\n\n[Section: Fair Regression: Quantitative Definitions and Reduction-based Algorithms]",
      "referring_paragraphs": [
        "Figure 3."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1905.12843_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1906.02589": [
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg",
      "image_filename": "1906.02589_page0_fig0.jpg",
      "caption": "(a) FFVAE learns the encoder distribution $q ( z , b | x )$ and decoder distributions $p ( x | z , b )$ , $p ( a | b )$ from inputs $x$ and multiple sensitive attributes $a$ . The disentanglement prior structures the latent space by encouraging low $\\mathrm { M I } ( b _ { i } , \\bar { a } _ { j } ) \\forall i \\neq j$ and low $\\mathrm { M I } ( b , z )$ where $\\mathrm { M I } ( \\cdot )$ denotes mutual information.",
      "context_before": "In this work, we investigate how to learn flexibly fair representations that can be easily adapted at test time to achieve fairness with respect to sets of sensitive groups or subgroups. We draw inspiration from the disentangled representation literature, where the goal is for each dimension of the representation (also called the “latent code”) to correspond to no more than one semantic factor of variation in the data (for example, independent visual features like object shape and position) (Higgins et al., 2017; Locatello et al., 2019). Our method uses multiple sensitive attribute labels at train time to induce a disentangled structure in the learned representation, which allows us to easily eliminate their influence at test time. Importantly, at test time our method does not require access to the sensitive attributes, which can be difficult to collect in practice due to legal restrictions (Elliot et al., 2008; DCCA, 1983). The trained representation permits simple and composable modifications at test time that eliminate the influence of sensitive attributes, enabling a wide variety of downstream tasks.\n\nWe first provide proof-of-concept by generating a variant of the synthetic DSprites dataset with correlated ground truth\n\narXiv:1906.02589v1 [cs.LG] 6 Jun 2019",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig1.jpg",
      "image_filename": "1906.02589_page0_fig1.jpg",
      "caption": "(b) The FFVAE latent code $[ z , b ]$ can be modified by discarding or noising out sensitive dimensions $\\{ b _ { j } \\}$ , which yields a latent code $[ z , b ^ { \\prime } ]$ independent of groups and subgroups derived from sensitive attributes $\\{ a _ { j } \\}$ . A held out label $_ y$ can then be predicted with subgroup demographic parity. Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).",
      "context_before": "",
      "context_after": "factors of variation, which is better suited to fairness questions. We demonstrate that even in the correlated setting, our method is capable of disentangling the effect of several sensitive attributes from data, and that this disentanglement is useful for fair classification tasks downstream. We then apply our method to a real-world tabular dataset (Communities & Crime) and an image dataset (Celeb-A), where we find that our method matches or exceeds the fairness-accuracy tradeoff of existing disentangled representation learning approaches on a majority of the evaluated subgroups.\n\nGroup Fair Classification In fair classification, we consider labeled examples $x , a , y \\sim p _ { \\mathrm { d a t a } }$ where $y \\in \\mathcal { V }$ are labels we wish to predict, $a \\in { \\mathcal { A } }$ are sensitive attributes, and $x \\in \\mathcal { X }$ are non-sensitive attributes. The goal is to learn a classifier ${ \\hat { y } } = g ( x , a )$ (or ${ \\hat { y } } = g ( x ) )$ which is predictive of $y$ and achieves certain group fairness criteria w.r.t. a. These criteria are typically written as independence properties of the various random variables involved. In this paper we focus on demographic parity, which is satisfied when the predictions are independent of the sensitive attributes: ${ \\hat { y } } \\perp a$ . It is often impossible or undesirable to satisfy demographic parity exactly (i.e. achieve complete independence). In this case, a useful metric is demographic parity distance:\n\n$$ \\Delta_ {D P} = | \\mathbb {E} [ \\bar {y} = 1 | a = 1 ] - \\mathbb {E} [ \\bar {y} = 1 | a = 0 ] | \\tag {1} $$",
      "referring_paragraphs": [
        "It comprises the following four terms, respectively: a reconstruction term which rewards the model for successfully modeling non-sensitive observations; a predictiveness term which rewards the model for aligning the correct latent components with the sensitive attributes; a disentanglement term which rewards the model for decorrelating the latent dimensions of $b$ from each other and $z$ ; and a dimension-wise KL term which rewards the model for matching the prior in the latent variables. We cal",
        "Figure 1.",
        "We call our model FFVAE for Flexibly Fair VAE (see Figure 1 for a schematic representation)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig2.jpg",
      "image_filename": "1906.02589_page0_fig2.jpg",
      "caption": "(a) a = Scale",
      "context_before": "DSpritesUnfair Dataset The DSprites dataset4 contains $6 4 \\times 6 4$ -pixel images of white shapes against a black background, and was designed to evaluate whether learned representations have disentangled sources of variation. The original dataset has several categorical factors of variation— Scale, Orientation, XPosition, YPosition—that combine to create 700, 000 unique images. We binarize the factors of variation to derive sensitive attributes and labels, so that many images now share any given attribute/label combination (See Appendix B for details). In the original DSprites dataset, the factors of variation are sampled uniformly. However, in fairness problems, we are often concerned with correlations between attributes and the labels we are trying to predict (otherwise, achieving low $\\Delta _ { D P }$ is aligned with standard classification objectives). Hence, we sampled an “unfair” version of this data (DSpritesUnfair) with correlated factors of variation; in particular Shape and XPosition correlate positively. Then a non-trivial fair classification task would be, for instance, learning to predict shape without discriminating against inputs from the left side of the image.\n\n[Section: Flexibly Fair Representation Learning by Disentanglement]\n\n4https://github.com/deepmind/dsprites-dataset",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig3.jpg",
      "image_filename": "1906.02589_page0_fig3.jpg",
      "caption": "(b) a = Shape",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg",
      "image_filename": "1906.02589_page0_fig4.jpg",
      "caption": "(c) a = Shape ∧ Scale",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig5.jpg",
      "image_filename": "1906.02589_page0_fig5.jpg",
      "caption": "(d) a = Shape ∨ Scale",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_7",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig6.jpg",
      "image_filename": "1906.02589_page0_fig6.jpg",
      "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot. Figure 3. Black and pink dashed lines respectively show FFVAE disentanglement audit (the higher the better) and predictiveness audit (the lower the better) as a function of $\\alpha$ . These audits use $A _ { i } { = } _ { \\backslash } ^ { \\mathscr { S } }$ Shape (see text for details). The blue line is a reference value— the log loss of a classifier that predicts $A _ { i }$ from the other 5 DSprites factors of variation (FoV) alone, ignoring the image—representing the amount of information about $A _ { i }$ inherent in the data.",
      "context_before": "",
      "context_after": "Baselines To test the utility of our predictiveness prior, we compare our model to $\\beta$ -VAE (VAE with a coefficient $\\beta \\geq 1$ on the KL term) and FactorVAE, which have disentanglement priors but no predictiveness prior. We can also think of these as FFVAE with $\\alpha = 0$ . To test the utility of our disentanglement prior, we also compare against a version of our model with $\\gamma = 0$ , denoted CVAE. This is similar to the class-conditional VAE (Kingma et al., 2014), with sensitive attributes as labels — this model encourages predictiveness but no disentanglement.\n\nFair Classification We perform the fair classification audit using several group/subgroup definitions for models trained on DSpritesUnfair (see Appendix D for training details), and report fairness-accuracy tradeoff curves in Fig. 2. In these experiments, we used Shape and Scale as our sensitive attributes during encoder training. We perform the fair classification audit by training an MLP to predict $y =$ “XPosition”—which was not used in the representa-\n\ntion learning phase—given the modified encoder outputs, and repeat for several sensitive groups and subgroups. We modify the encoder outputs as follows: When our sensitive attribute is $a _ { i }$ we remove the associated dimension $b _ { i }$ from $[ z , b ]$ ; when the attribute is a conjunction of $a _ { i }$ and $a _ { j }$ , we remove both $b _ { i }$ and $b _ { j }$ . For the baselines, we simply remove the latent dimension which is most correlated with $a _ { i }$ , or the two most correlated dimensions with the conjunction. We sweep a range of hyperparameters to produce the fairness-accuracy tradeoff curve for each model. In Fig. 2, we show the “Pareto front” of these models: points in ( $\\Delta _ { D P }$ , accuracy)-space for which no other point is better along both dimensions. The optimal result is the top left hand corner (perfect accuracy and $\\Delta _ { D P } = 0 $ ).",
      "referring_paragraphs": [
        "Fair Classification We perform the fair classification audit using several group/subgroup definitions for models trained on DSpritesUnfair (see Appendix D for training details), and report fairness-accuracy tradeoff curves in Fig. 2. In these experiments, we used Shape and Scale as our sensitive attributes during encoder training. We perform the fair classification audit by training an MLP to predict $y =$ “XPosition”—which was not used in the representa-",
        "tion learning phase—given the modified encoder outputs, and repeat for several sensitive groups and subgroups. We modify the encoder outputs as follows: When our sensitive attribute is $a _ { i }$ we remove the associated dimension $b _ { i }$ from $[ z , b ]$ ; when the attribute is a conjunction of $a _ { i }$ and $a _ { j }$ , we remove both $b _ { i }$ and $b _ { j }$ . For the baselines, we simply remove the latent dimension which is most correlated with $a _ { i }$ , or the two most correl",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig7.jpg",
      "image_filename": "1906.02589_page0_fig7.jpg",
      "caption": "(a) $a = \\mathbf { R }$",
      "context_before": "5.3. Communities & Crime\n\nDataset Communities & Crime5 is a tabular UCI dataset containing neighborhood-level population statistics. 120 such statistics are recorded for each of the 1, 994 neighborhoods. Several attributes encode demographic information that may be protected. We chose three as sensitive: racePct-Black ( $\\%$ neighborhood population which is Black), black-PerCap (avg per capita income of Black residents), and pct-NotSpeakEnglWell ( $\\%$ neighborhood population that does not speak English well). We follow the same train/eval procedure as with DSpritesUnfair - we train FFVAE with the sensitive attributes and evaluate using naive MLPs to predict a held-out label (violent crimes per capita) on held-out data.\n\nFair Classification This dataset presents a more difficult disentanglement problem than DSpritesUnfair. The three sensitive attributes we chose in Communities and Crime were somewhat correlated with each other, a natural artefact of using real (rather than simulated) data. We note that in general, the disentanglement literature does not provide much guidance in terms of disentangling correlated attributes. Despite this obstacle, FFVAE performed reasonably well in the fair classification audit (Fig. 4). It achieved",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig8.jpg",
      "image_filename": "1906.02589_page0_fig8.jpg",
      "caption": "(b) $a = \\mathbf { B }$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig9.jpg",
      "image_filename": "1906.02589_page0_fig9.jpg",
      "caption": "(c) a = P",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig10.jpg",
      "image_filename": "1906.02589_page0_fig10.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig11.jpg",
      "image_filename": "1906.02589_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig12.jpg",
      "image_filename": "1906.02589_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg",
      "image_filename": "1906.02589_page0_fig13.jpg",
      "caption": "(d) $a = \\mathbf { R }$ ∨ B",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig14.jpg",
      "image_filename": "1906.02589_page0_fig14.jpg",
      "caption": "(e) $a = \\mathbf { R }$ ∨ P",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_16",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig15.jpg",
      "image_filename": "1906.02589_page0_fig15.jpg",
      "caption": "(f) a = B ∨ P Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.",
      "context_before": "",
      "context_after": "higher accuracy than the baselines in general, likely due to its ability to incorporate side information from $a$ during training. Among the baselines, FactorVAE tended perform best, suggesting achieving a factorized aggregate posterior helps with fair classification. While our method does not outperform the baselines on each conjunction, its relatively strong performance on a difficult, tabular dataset shows the promise of using disentanglement priors in designing robust subgroup-fair machine learning models.\n\n5.4. Celebrity Faces\n\nDataset The CelebA6 dataset contains over 200, 000 images of celebrity faces. Each image is associated with 40 human-labeled binary attributes (OvalFace, HeavyMakeup, etc.). We chose three attributes, Chubby, Eyeglasses, and Male as sensitive attributes7, and report fair classification results on 3 groups and 12 two-attribute-conjunction subgroups only (for brevity we omit three-attribute conjunctions). To our knowledge this is the first exploration of fair representation learning algorithms on the Celeb-A dataset. As in the previous sections we train the encoders on the train set, then evaluate performance of MLP classifiers trained on the encoded test set.",
      "referring_paragraphs": [
        "Fair Classification This dataset presents a more difficult disentanglement problem than DSpritesUnfair. The three sensitive attributes we chose in Communities and Crime were somewhat correlated with each other, a natural artefact of using real (rather than simulated) data. We note that in general, the disentanglement literature does not provide much guidance in terms of disentangling correlated attributes. Despite this obstacle, FFVAE performed reasonably well in the fair classification audit (F",
        "Figure 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig16.jpg",
      "image_filename": "1906.02589_page0_fig16.jpg",
      "caption": "(a) a = C",
      "context_before": "6http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n7 We chose these attributes because they co-vary relatively weakly with each other (compared with other attribute triplets), but strongly with other attributes. Nevertheless the rich correlation structure amongst all attributes makes this a challenging fairness dataset; it is difficult to achieve high accuracy and low $\\Delta _ { D P }$ .\n\n5 http://archive.ics.uci.edu/ml/datasets/communities+ and+crime",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig17.jpg",
      "image_filename": "1906.02589_page0_fig17.jpg",
      "caption": "(b) a = E",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig18.jpg",
      "image_filename": "1906.02589_page0_fig18.jpg",
      "caption": "(c) $a = \\mathbf { M }$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig19.jpg",
      "image_filename": "1906.02589_page0_fig19.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig20.jpg",
      "image_filename": "1906.02589_page0_fig20.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig21.jpg",
      "image_filename": "1906.02589_page0_fig21.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig22.jpg",
      "image_filename": "1906.02589_page0_fig22.jpg",
      "caption": "(d) a = C ∧ E",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig23.jpg",
      "image_filename": "1906.02589_page0_fig23.jpg",
      "caption": "(e) a = C ∧¬ E",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig24.jpg",
      "image_filename": "1906.02589_page0_fig24.jpg",
      "caption": "(f) a = ¬ C ∧ E",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig25.jpg",
      "image_filename": "1906.02589_page0_fig25.jpg",
      "caption": "(g) a = ¬ C ∧¬ E",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_27",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig26.jpg",
      "image_filename": "1906.02589_page0_fig26.jpg",
      "caption": "(h) a = C ∧ M",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_28",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig27.jpg",
      "image_filename": "1906.02589_page0_fig27.jpg",
      "caption": "(i) $a = \\mathbf { C } \\wedge \\lnot \\mathbf { M }$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_29",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "image_filename": "1906.02589_page0_fig28.jpg",
      "caption": "(j) a = ¬ C ∧ M",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_30",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig29.jpg",
      "image_filename": "1906.02589_page0_fig29.jpg",
      "caption": "(k) a = ¬ C ∧¬ M",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_31",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig30.jpg",
      "image_filename": "1906.02589_page0_fig30.jpg",
      "caption": "(l) a = ¬ E ∧ M Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "context_before": "",
      "context_after": "Fair Classification We follow the fair classification audit procedure described above, where the held-out label HeavyMakeup—which was not used at encoder train time— is predicted by an MLP from the encoder representations. When training the MLPs we take a fresh encoder sample for each minibatch (statically encoding the dataset with one encoder sample per image induced overfitting). We found that training the MLPs on encoder means (rather than samples) increased accuracy but at the cost of very unfavorable $\\Delta _ { D P }$ . We also found that FactorVAE-style adversarial training does not scale well to this high-dimensional problem, so we instead optimize Equation 4 using the biased estimator from Chen et al. (2018). Figure 5 shows Pareto fronts that capture the fairness-accuracy tradeoff for FFVAE and $\\beta$ -VAE.\n\nWhile neither method dominates in this challenging setting, FFVAE achieves a favorable fairness-accuracy tradeoff across many of subgroups. We believe that using sensitive attributes as side information gives FFVAE an advantage over\n\n$\\beta$ -VAE in predicting the held-out label. In some cases (e.g., $a { = } { \\{ } \\mathrm { R } \\land \\mathrm { M } \\}$ FFVAE achieves better accuracy at all $\\Delta _ { D P }$ levels, while in others (e.g., $a { \\mathrm { = } } { \\mathrm { \\to } } { \\mathrm { C } } \\Lambda { \\mathrm { \\to } } { \\mathrm { E } } )$ , FFVAE did not find a low- $\\Delta _ { D P }$ solution. We believe Celeb-A–with its many high dimensional data and rich label correlations—is a useful test bed for subgroup fair machine learning algorithms, and we are encouraged by the reasonably robust performance of FFVAE in our experiments.",
      "referring_paragraphs": [
        "Fair Classification We follow the fair classification audit procedure described above, where the held-out label HeavyMakeup—which was not used at encoder train time— is predicted by an MLP from the encoder representations. When training the MLPs we take a fresh encoder sample for each minibatch (statically encoding the dataset with one encoder sample per image induced overfitting). We found that training the MLPs on encoder means (rather than samples) increased accuracy but at the cost of very u",
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_32",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg",
      "image_filename": "1906.02589_page0_fig31.jpg",
      "caption": "(a) Color is $\\gamma$ , brighter colours $\\longrightarrow$ higher values",
      "context_before": "where $j _ { k } = \\underset { j } { \\mathrm { a r g m a x } } M I ( z _ { j } ; v _ { k } ) , M I ( \\cdot ; \\cdot )$ $M I ( \\because \\cdot )$ denotes mutual information, and $K$ is the number of factors of variation. Note that we can only compute this metric in the synthetic setting where the ground truth factors of variation are known. MIG measures the difference between the latent variables which have the highest and second-highest $M I$ with each factor of variation, rewarding models which allocate one latent variable to each factor of variation. We test our disentanglement by training our models on a biased version of DSprites, and testing on a balanced version (similar to the “skewed” data in Chen et al. (2018)). This allows us to separate out two sources of correlation — the correlation existing across the data, and the correlation in the model’s learned representation.\n\nResults In Fig. 6a, we show that MIG increases with $\\alpha$ , providing more evidence that the supervised structure of the FFVAE can create disentanglement. This improvement holds across values of $\\gamma$ , except for some training instability\n\n[Section: Flexibly Fair Representation Learning by Disentanglement]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_33",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig32.jpg",
      "image_filename": "1906.02589_page0_fig32.jpg",
      "caption": "(b) Colour is $\\alpha$ , brighter colors −→ higher values Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
      "context_before": "",
      "context_after": "for the highest values of $\\gamma$ . It is harder to assess the relationship between $\\gamma$ and MIG, due to increased instability in training when $\\gamma$ is large and $\\alpha$ is small. However, in Fig. 6b, we look only at $\\alpha \\geq 3 0 0$ , and note that in this range, MIG improves as $\\gamma$ increases. See Appendix E for more details.",
      "referring_paragraphs": [
        "Evaluation Criteria Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG) (Chen et al., 2018) of FFVAE across various hyperparameter settings. With $J$ latent variables $z _ { j }$ and $K$ factors of variation $v _ { k }$ , MIG is defined as",
        "Discussion What does it mean for our model to demonstrate disentanglement on test data drawn from a new distribution? For interpretation, we can look to the causal inference literature, where one goal is to produce models that are robust to certain interventions in the data generating process (Rothenhusler et al., 2018). We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a. This property relates to counterfactual fairness",
        "Figure 6.",
        "We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_34",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg",
      "image_filename": "1906.02589_page0_fig33.jpg",
      "caption": "(a) Colour is α",
      "context_before": "for the highest values of $\\gamma$ . It is harder to assess the relationship between $\\gamma$ and MIG, due to increased instability in training when $\\gamma$ is large and $\\alpha$ is small. However, in Fig. 6b, we look only at $\\alpha \\geq 3 0 0$ , and note that in this range, MIG improves as $\\gamma$ increases. See Appendix E for more details.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_35",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig34.jpg",
      "image_filename": "1906.02589_page0_fig34.jpg",
      "caption": "(b) Colour is γ Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
      "context_before": "",
      "context_after": "In Fig. 7a, we show that for low values of $\\alpha$ , increasing $\\gamma$ leads to worse MIG, likely due to increased training instability. This is in contrast to Fig. 6b, which suggests that for high enough $\\alpha$ , increasing $\\gamma$ can improve MIG. This leads us to believe that $\\alpha$ and $\\gamma$ have a complex relationship with respect to disentanglement and MIG.\n\nTo better understand the relationship between these two hyperparameters, we examine how MIG varies with the ratio $\\textstyle { \\frac { \\gamma } { \\alpha } }$ in Fig. 7b. In We find that in general, a higher ratio yields lower MIG, but that the highest MIGs are around $\\log { \\frac { \\gamma } { \\alpha } } \\ = \\ - 2$ , with a slight tailing off for smaller ratios. This indicates there is a dependent relationship between the\n\nvalues of $\\gamma$ and $\\alpha$ .",
      "referring_paragraphs": [
        "Figure 7."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "1906.02589_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1907.06430": [
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig0.jpg",
      "image_filename": "1907.06430_page0_fig0.jpg",
      "caption": "Assume a dataset $\\varDelta = \\{ a ^ { n } , x ^ { n } = \\{ q ^ { n } , d ^ { n } \\} , y ^ { n } \\} _ { n = 1 } ^ { N }$ corresponding to a college admission scenario in which applicants are admitted based on qualificati",
      "context_before": "2 A Graphical View of (Un)fairness\n\nConsider a dataset $\\varDelta = \\{ a ^ { n } , x ^ { n } , y ^ { n } \\} _ { n = 1 } ^ { N }$ , corresponding to $N$ individuals, where $a ^ { n }$ indicates a sensitive attribute, and $x ^ { n }$ a set of observations that can be used (possibly together with $a ^ { n }$ ) to form a prediction $\\hat { y } ^ { n }$ of outcome $y ^ { n }$ . We assume a binary setting $a ^ { n } , y ^ { n } , \\hat { y } ^ { n } \\in \\{ 0 , 1 \\}$ (unless otherwise specified), and indicate with $A , { \\mathcal { X } }$ , $Y$ , and $\\hat { Y }$ the (set of) random variables1 corresponding to $a ^ { n } , x ^ { n } , y ^ { n }$ , and ${ \\hat { y } } ^ { n }$ respectively.\n\nIn this section we show at a high-level that a correct use of fairness definitions concerned with statistical properties of $\\hat { Y }$ with respect to $A$ requires an understanding of the patterns of unfairness underlying $\\varDelta$ , and therefore of the relationships among $A$ , $\\mathcal { X }$ and $Y$ . More specifically we show that:",
      "context_after": "Assume a dataset $\\varDelta = \\{ a ^ { n } , x ^ { n } = \\{ q ^ { n } , d ^ { n } \\} , y ^ { n } \\} _ { n = 1 } ^ { N }$ corresponding to a college admission scenario in which applicants are admitted based on qualifications $Q$ , choice of department $D$ , and gender $A$ ; and in which female applicants apply more often to certain departments. This scenario can be represented by\n\nthe CBN on the left (see Appendix A for an overview of BNs, and Sect. 3 for a detailed treatment of CBNs). The causal path $A Y$ represents direct influence of gender $A$ on admission $Y$ , capturing the fact that two individuals with the same qualifications and applying to the same department can be treated differently depending on their gender. The indirect causal path $A D Y$ represents influence of $A$ on $Y$ through $D$ , capturing the fact that female applicants more often apply to certain departments. Whilst the direct path $A Y$ is certainly an unfair one, the paths $A D$ and $D Y$ , and therefore $A D Y$ , could either be considered as fair or as unfair. For example, rejecting women more often due to department choice could be considered fair with respect to college\n\n[Section: S. Chiappa and W. S. Isaac]",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg",
      "image_filename": "1907.06430_page0_fig1.jpg",
      "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .",
      "context_before": "An intense ongoing debate, in which the research community has also been heavily involved, was triggered by an exposé from investigative journalists at ProPublica [5] on the COMPAS pretrial RAI developed by Equivant (formerly Northpointe) and deployed in Broward County in Florida. The COMPAS general recidivism risk scale (GRRS) and violent recidivism risk scale (VRRS), the focus of ProPublica’s investigation, sought to leverage machine learning techniques to improve the predictive accuracy of recidivism compared to older RAIs such as the level of service inventory-revised [3] which were primarily based on theories and techniques from a sub-field of psychology known as the psychology of criminal conduct [4,9]3.\n\n[Section: S. Chiappa and W. S. Isaac]\n\n3 While the exact methodology underlying GRRS and VRRS is proprietary, publicly available reports suggest that the process begins with a defendant being administered a 137 point assessment during intake. This is used to create a series of dynamic risk factor scales such as the criminal involvement scale and history of violence scale. In addition, COMPAS also includes static attributes such as the defendant’s age and prior police contact (number of prior arrests). The raw COMPAS scores are",
      "context_after": "ProPublica’s criticism of COMPAS centered on two concerns. First, the authors argued that the distribution of the risk score $R \\in \\{ 1 , \\ldots , 1 0 \\}$ exhibited discriminatory patterns, as black defendants displayed a fairly uniform distribution across each value, while white defendants exhibited a right skewed distribution, suggesting that the COMPAS recidivism risk scores disproportionately rated white defendants as lower risk than black defendants. Second, the authors claimed that the GRRS and VRRS did not satisfy EFPRs and EFNRs, as FPRs = 44.9% and FNRs $= 2 8 . 0 \\%$ for black defendants, whilst FPRs = 23.5% and FNRs = 47.7% for white defendants (see Fig. 1). This evidence led ProPublica to conclude that COMPAS had a disparate impact on black defendants, leading to public outcry over potential biases in RAIs and machine learning writ large.\n\nIn response, Equivant published a technical report [20] refuting the claims of bias made by ProPublica and concluded that COMPAS is sufficiently calibrated, in the sense that it satisfies predictive parity at key thresholds. Subsequent analyses [13,16,30] confirmed Equivant’s claims of calibration, but also demonstrated the incompatibility of EFPRs/EFNRs and calibration due to differences in base\n\n[Section: A Causal Bayesian Networks Viewpoint on Fairness]",
      "referring_paragraphs": [
        "ProPublica’s criticism of COMPAS centered on two concerns. First, the authors argued that the distribution of the risk score $R \\in \\{ 1 , \\ldots , 1 0 \\}$ exhibited discriminatory patterns, as black defendants displayed a fairly uniform distribution across each value, while white defendants exhibited a right skewed distribution, suggesting that the COMPAS recidivism risk scores disproportionately rated white defendants as lower risk than black defendants. Second, the authors claimed that the GR"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg",
      "image_filename": "1907.06430_page0_fig2.jpg",
      "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.",
      "context_before": "transformed into decile values by ranking and calibration with a normative group to ensure an equal proportion of scores within each scale value. Lastly, to aid practitioner interpretation, the scores are grouped into three risk categories. The scale values are displayed to court officials as either Low (1-4), Medium (5-7), and High (8-10) risk.\n\nrates across groups (Y ✚⊥⊥A) (see Appendix B). Moreover, the studies suggested that attempting to satisfy these competing forms of fairness force unavoidable trade-offs between criminal justice reformers’ purported goals of racial equity and public safety.\n\nAs explained in Sect. 2, by requiring the rate of (dis)agreement between $Y$ and $\\hat { Y }$ to be the same for black and white defendants (and therefore by not being concerned with dependence of $Y$ on $A$ ), EFPRs/EFNRs and calibration are inappropriate fairness criteria if dependence of $Y$ on $A$ includes influence of $A$ on $Y$ through an unfair causal path.",
      "context_after": "As previous research has shown [29,35,44], modern policing tactics center around targeting a small number of neighborhoods — often disproportionately populated by non-white and low income residents — with recurring patrols and stops. This uneven distribution of police attention, as well as other factors such as funding for pretrial services [31,46], means that differences in base rates between racial groups are not reflective of ground truth rates. We can rephrase these findings as indicating the presence\n\nof a direct path $A Y$ (through unobserved neighborhood) in the CBN representing the data-generation mechanism (Fig. 2). Such tactics also imply an influence of $A$ on $Y$ through the set of variables $\\mathcal { F }$ containing number of prior arrests. In addition, the influence of $A$ on $Y$ through $A Y$ and $A { \\mathcal { F } } Y$ could be more prominent or contain more unfairness due to racial discrimination.\n\nThese observations indicate that EFPRs/EFNRs and calibration are inappropriate criteria for this case (and therefore that their incompatibility is irrelevant), and more generally that the current fairness debate surrounding COMPAS gives insufficient consideration to the patterns of unfairness underlying the training data. Our analysis formalizes the concerns raised by social scientists and legal scholars on mismeasurement and unrepresentative data in the US criminal justice system. Multiple studies [22,34,37,46] have argued that the core premise of RAIs, to assess the likelihood a defendant reoffends, is impossible to measure and that the empirical proxy used (e.g. arrest or conviction) introduces embedded biases and norms which render existing fairness tests unreliable.",
      "referring_paragraphs": [
        "of a direct path $A Y$ (through unobserved neighborhood) in the CBN representing the data-generation mechanism (Fig. 2). Such tactics also imply an influence of $A$ on $Y$ through the set of variables $\\mathcal { F }$ containing number of prior arrests. In addition, the influence of $A$ on $Y$ through $A Y$ and $A { \\mathcal { F } } Y$ could be more prominent or contain more unfairness due to racial discrimination."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg",
      "image_filename": "1907.06430_page0_fig3.jpg",
      "caption": "(a)",
      "context_before": "[Section: S. Chiappa and W. S. Isaac]\n\nassociated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$ . The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditional distributions, i.e. $\\begin{array} { r } { p ( X _ { 1 } , \\ldots , X _ { I } ) = \\prod _ { i = 1 } ^ { I } p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) ) } \\end{array}$ (see Appendix A for more details on Bayesian networks).\n\nWhen equipped with causal semantic, namely when representing the datageneration mechanism, Bayesian networks can be used to visually express causal relationships. More specifically, CBNs enable us to give a graphical definition of causes and causal effects: if there exists a directed path from $A$ to $Y$ , then $A$ is a potential cause of $Y$ . Directed paths are also called causal paths.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig4.jpg",
      "image_filename": "1907.06430_page0_fig4.jpg",
      "caption": "(b) Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .",
      "context_before": "",
      "context_after": "The causal effect of $A$ on $Y$ can be seen as the information traveling from $A$ to $Y$ through causal paths, or as the conditional distribution of $Y$ given $A$ restricted to causal paths. This implies that, to compute the causal effect, we need to disregard the information that travels along non-causal paths, which occurs if such paths are open. Since paths with an arrow emerging from $A$ are either causal or closed (blocked) by a collider, the problematic paths are\n\nonly those with an arrow pointing into $A$ , called back-door paths, which are open if they do not contain a collider.\n\nAn example of an open back-door path is given by $A \\left. C \\right. Y$ in the CBN $\\vec { \\mathcal { G } }$ of Fig. 3(a): the variable $C$ is said to be a confounder for the effect of $A$ on $Y$ , as it confounds the causal effect with non-causal information. To understand this, assume that $A$ represents hours of exercise in a week, $Y$ cardiac health, and $C$ age: observing cardiac health conditioning on exercise level from $p ( Y | A )$ does not enable us to understand the effect of exercise on cardiac health, since $p ( Y | A )$ includes the dependence between $A$ and $Y$ induced by age.",
      "referring_paragraphs": [
        "An example of an open back-door path is given by $A \\left. C \\right. Y$ in the CBN $\\vec { \\mathcal { G } }$ of Fig. 3(a): the variable $C$ is said to be a confounder for the effect of $A$ on $Y$ , as it confounds the causal effect with non-causal information. To understand this, assume that $A$ represents hours of exercise in a week, $Y$ cardiac health, and $C$ age: observing cardiac health conditioning on exercise level from $p ( Y | A )$ does not enable us to understand the effect of exercise",
        "Each parent-child relationship in a CBN represents an autonomous mechanism, and therefore it is conceivable to change one such a relationship without changing the others. This enables us to express the causal effect of $A \\ = \\ a$ on $Y$ as the conditional distribution $p _ { A = a } ( Y | A = a )$ on the modified CBN $\\scriptstyle { \\mathcal { G } } _ { \\to A = a }$ of Fig. 3(b), resulting from replacing $p ( A | C )$ with a Dirac delta distribution $\\delta _ { A = a }$ (thereby removing the li"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg",
      "image_filename": "1907.06430_page0_fig5.jpg",
      "caption": "(a)",
      "context_before": "The equality $p _ { A = a } ( Y | A = a , \\mathcal { C } ) = p ( Y | A = a , \\mathcal { C } )$ follows from the fact that $\\mathcal { G } _ { A }$ , obtained by removing from $\\mathcal { G }$ all links emerging from $A$ , retains all (and only) the back-door paths from $A$ to $Y$ . As $\\boldsymbol { \\mathscr { C } }$ blocks all such paths, $Y \\perp A | { \\mathcal { C } }$ in $\\mathcal { G } _ { A }$ . This means that there is no non-causal information traveling from $A$ to $Y$ when conditioning on $c$ and therefore conditioning on $A$ coincides with intervening.\n\nConditioning on $C$ to block an open back-door path may open a closed path on which $C$ is a collider. For example, in the CBN of Fig. 4(a), conditioning on $C$ closes the paths $A C X Y$ and $A \\left. C \\right. Y$ , but opens the path $A E C $ $X Y$ (additional conditioning on $X$ would close $A \\left. E \\right. C \\left. X \\right. Y$ ).\n\nThe back-door criterion can also be derived from the rules of do-calculus [39,40], which indicate whether and how $p _ { A } ( Y | A )$ can be estimated using observations from $\\vec { \\mathcal { G } }$ : for many graph structures with unobserved confounders the only way to compute causal effects is by collecting obser-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig6.jpg",
      "image_filename": "1907.06430_page0_fig6.jpg",
      "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A C X Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .",
      "context_before": "",
      "context_after": "vations directly from $\\mathcal { G } _ { A }$ — in this case the effect is said to be non-identifiable.\n\nPotential Outcome Viewpoint. Let $Y _ { A = a }$ be the random variable with distribution $p ( Y _ { A = a } ) = p _ { A = a } ( Y | A = a )$ . $Y _ { A = a }$ is called potential outcome and, when not ambiguous, we will refer to it with the shorthand $Y _ { a }$ . The relation between $Y _ { a }$ and all the variables in $\\mathcal { G }$ other than $Y$ can be expressed by the graph obtained by removing from $\\vec { \\mathcal { G } }$ all the links emerging from $A$ , and by replacing $Y$ with $Y _ { a }$ . If $Y _ { a }$ is independent on $A$ in this graph, then4 $p ( Y _ { a } ) = p ( Y _ { a } | A = a ) = p ( Y | A = a )$ . If $Y _ { a }$ is independent of $A$ in this graph when conditioning on $c$ , then\n\n$$ p (Y _ {a}) = \\int_ {\\mathcal {C}} p (Y _ {a} | \\mathcal {C}) p (\\mathcal {C}) = \\int_ {\\mathcal {C}} p (Y _ {a} | A = a, \\mathcal {C}) p (\\mathcal {C}) = \\int_ {\\mathcal {C}} p (Y | A = a, \\mathcal {C}) p (\\mathcal {C}), $$",
      "referring_paragraphs": [
        "Conditioning on $C$ to block an open back-door path may open a closed path on which $C$ is a collider. For example, in the CBN of Fig. 4(a), conditioning on $C$ closes the paths $A C X Y$ and $A \\left. C \\right. Y$ , but opens the path $A E C $ $X Y$ (additional conditioning on $X$ would close $A \\left. E \\right. C \\left. X \\right. Y$ ).",
        "Consider the CBN of Fig. 4(b), containing the direct path $A Y$ and one indirect causal path through the variable $M$ . Let $Y _ { a } ( M _ { \\bar { a } } )$ be the random variable",
        "Effect of Treatment on Treated. Consider the conditional distribution $p ( Y _ { a } | A = \\bar { a } )$ . This distribution measures the information travelling from $A$ to $Y$ along all open paths, when $A$ is set to $a$ along causal paths and to $a$ along non-causal paths. The effect of treatment on treated (ETT) of $A = a$ with respect to $A = \\bar { a }$ is defined as $\\mathrm { E T T } _ { \\bar { a } a } = \\langle Y _ { a } \\rangle _ { p ( Y _ { a } | A = \\bar { a } ) } - \\langle Y _ { \\bar",
        "Consider the CBN of Fig. 4(b), containing the direct path $A  Y$ and one indirect causal path through the variable $M$ . Let $Y _ { a } ( M _ { \\bar { a } } )$ be the random variable\n\nwith distribution equal to the conditional distribution of $Y$ given $A$ restricted to causal paths, with $A = a$ along $A  Y$ and $A = { \\bar { a } }$ along $A  M  Y$ . The average direct effect (ADE) of $A = a$ with respect to $A = \\bar { a }$ , defined as"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg",
      "image_filename": "1907.06430_page0_fig7.jpg",
      "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
      "context_before": "To estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ , $A \\to M \\to , \\dots , \\to Y$ , namely along the red links. The path-specific effect (PSE) of $A = a$ with respect to $A = \\bar { a }$ for this group of paths is defined as\n\n$$ \\mathrm {P S E} _ {\\bar {a} a} = \\left\\langle Y _ {a} \\left(M _ {a}, L _ {\\bar {a}} \\left(M _ {a}\\right)\\right) \\right\\rangle - \\left\\langle Y _ {\\bar {a}} \\right\\rangle , $$\n\nwhere $p ( Y _ { a } ( M _ { a } , L _ { \\bar { a } } ( M _ { a } ) ) )$ is given by",
      "context_after": "$$ \\int_ {C, M, L} p (Y | A = a, C, M, L) p (L | A = \\bar {a}, C, M) p (M | A = a, C) p (C). $$\n\nIn the simple case in which the CBN corresponds to a linear model, e.g.\n\n$$ A \\sim \\operatorname {B e r n} (\\pi), C = \\epsilon_ {c}, $$",
      "referring_paragraphs": [
        "To estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A Y$ and the paths passing through $M$ , $A \\to M ",
        "The same conclusion could have been obtained by looking at the graph annotated with path coefficients (Fig. 5 (bottom)). The PSE is obtained by summing over the three causal paths of interest ( $A Y$ , $A M Y$ , and $A \\to M \\to L \\to Y$ ) the product of all coefficients in each path.",
        "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
        "The same conclusion could have been obtained by looking at the graph annotated with path coefficients (Fig. 5 (bottom)). The PSE is obtained by summing over the three causal paths of interest ( $A  Y$ , $A  M  Y$ , and $A \\to M \\to L \\to Y$ ) the product of all coefficients in each path.\n\nNotice that AIE $\\bar { a } a$ , given by"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig8.jpg",
      "image_filename": "1907.06430_page0_fig8.jpg",
      "caption": "To understand this, consider the hiring scenario described by the CBN on the left, where $A$ represents religious belief and $E$ educational background of the applicant, which influences religious participation ( $E A$ )",
      "context_before": "Equipped with the background on CBNs from Sect. 3, in this section we further investigate unfairness in a dataset $\\varDelta = \\{ a ^ { n } , x ^ { n } , y ^ { n } \\} _ { n = 1 } ^ { N }$ , discuss issues that might arise when building a decision system from it, and show how to measure and deal with unfairness in complex scenarios, revisiting and extending material from [11,33,48].\n\n4.1 Back-door Paths from $\\pmb { A }$ to $\\mathbf { Y }$\n\nIn Sect. 2 we have introduced a graphical interpretation of unfairness in a dataset $\\varDelta$ as the presence of an unfair causal path from $A$ to $\\mathcal { X }$ or $Y$ . More specifically, we have shown through a college admission example that unfairness can be due to an unfair link emerging (a) from $A$ or (b) from a subsequent variable in a causal path from $A$ to $Y$ (e.g. $D Y$ in the example). Our discussion did not mention paths from $A$ to $Y$ with an arrow pointing into $A$ , namely back-door paths. This is because such paths are not problematic.",
      "context_after": "To understand this, consider the hiring scenario described by the CBN on the left, where $A$ represents religious belief and $E$ educational background of the applicant, which influences religious participation ( $E A$ ). Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\n\n4.2 Opening Closed Unfair Paths from $\\pmb { A }$ to $\\mathbf { Y }$\n\nIn Sect. 2, we have seen that, in order to reason about fairness of $\\hat { Y }$ , it is necessary to question and understand unfairness in $\\varDelta$ . In this section, we warn that another crucial element needs to be considered in the fairness discussion around $\\hat { Y }$ , namely",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_10",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg",
      "image_filename": "1907.06430_page0_fig9.jpg",
      "caption": "Fig. 6. CBN underlying a music degree scenario.",
      "context_before": "(i) The variables used to form $\\hat { Y }$ could project into $\\hat { Y }$ unfair patterns in $\\mathcal { X }$ that do not concern $Y$ .\n\nThis could happen, for example, if a closed unfair path from $A$ to $Y$ is opened when conditioning on the variables used to form $\\hat { Y }$ .\n\n[Section: A Causal Bayesian Networks Viewpoint on Fairness]",
      "context_after": "As an example, assume the CBN in Fig. 6 representing the data-generation mechanism underlying a music degree scenario, where $A$ corresponds to gender, $M$ to music aptitude (unobserved, i.e. $M \\not \\in \\varDelta$ ), $X$ to the score obtained from an ability test taken at the beginning of the degree, and $Y$ to the score obtained from an ability test taken at the end of the degree. Individuals with higher music aptitude $M$ are more likely to obtain higher initial and final scores ( $M X$ , $M Y$ ). Due to discrimination occurring at the\n\ninitial testing, women are assigned a lower initial score than men for the same aptitude level ( $A X$ ). The only path from $A$ to $Y$ , $A \\to X M \\to Y$ , is closed as $X$ is a collider on this path. Therefore the unfair influence of $A$ on $X$ does not reach $Y$ $( Y \\perp \\perp A )$ . Nevertheless, as $Y { \\mathcal { A } } A | X$ , a prediction $\\hat { Y }$ based on the initial score $X$ only would contain the unfair influence of $A$ on $X$ . For example, assume the following linear model: $Y = \\gamma M , X = \\alpha A + \\beta M$ , with $\\langle A ^ { 2 } \\rangle _ { p ( A ) } = 1$ and $\\langle M ^ { 2 } \\rangle _ { p ( M ) } = 1$ . A linear predictor of the form $\\hat { Y } = \\theta _ { X } X$ minimizing $\\langle ( Y - \\hat { Y } ) ^ { 2 } \\rangle _ { p ( A ) p ( M ) }$ would have parameters $\\theta _ { X } = \\gamma \\beta / ( \\alpha ^ { 2 } + \\beta ^ { 2 } )$ , giving $\\dot { Y } = \\gamma \\beta ( \\alpha A + \\beta M ) / ( \\alpha ^ { 2 } + \\beta ^ { 2 } )$ , i.e. $\\hat { Y } .$ ✚⊥⊥A. Therefore, this predictor would be using the sensitive attribute to form a decision, although implicitly rather than explicitly. Instead, a predictor explicitly using the sensitive attribute, $\\hat { Y } = \\theta _ { X } X + \\theta _ { A } A$ , would have parameters\n\n$$ \\left( \\begin{array}{c} \\theta_ {X} \\\\ \\theta_ {A} \\end{array} \\right) = \\left( \\begin{array}{c c} \\alpha^ {2} + \\beta^ {2} & \\alpha \\\\ \\alpha & 1 \\end{array} \\right) ^ {- 1} \\left( \\begin{array}{c} \\gamma \\beta \\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c} \\gamma / \\beta \\\\ - \\alpha \\gamma / \\beta \\end{array} \\right), $$",
      "referring_paragraphs": [
        "As an example, assume the CBN in Fig. 6 representing the data-generation mechanism underlying a music degree scenario, where $A$ corresponds to gender, $M$ to music aptitude (unobserved, i.e. $M \\not \\in \\varDelta$ ), $X$ to the score obtained from an ability test taken at the beginning of the degree, and $Y$ to the score obtained from an ability test taken at the end of the degree. Individuals with higher music aptitude $M$ are more likely to obtain higher initial and final scores ( $M X$ , $M "
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_11",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
      "image_filename": "1907.06430_page0_fig10.jpg",
      "caption": "Fig. 7. CBN underlying a college admission scenario.",
      "context_before": "Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered unfair, unfairness overall population can be quantified with $\\langle Y \\rangle _ { p ( Y | a ) } - \\langle Y \\rangle _ { p ( Y | \\bar { a } ) }$\n\n[Section: S. Chiappa and W. S. Isaac]\n\n(coinciding with $\\mathrm { A T E } _ { \\bar { a } a } = \\langle Y _ { a } \\rangle _ { p ( Y _ { a } ) } - \\langle Y _ { \\bar { a } } \\rangle _ { p ( Y _ { \\bar { a } } ) } )$ where, for example, $A = a$ and $A = \\bar { a }$ indicate female and male applicants respectively.",
      "context_after": "In the more complex case in which the path $A $ $D Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A Y$ , PSE $^ { a a }$ , given by\n\n$$ \\langle Y _ {a} (D _ {\\bar {a}}) \\rangle_ {p (Y _ {a} (D _ {\\bar {a}}))} - \\langle Y _ {\\bar {a}} \\rangle_ {p (Y _ {\\bar {a}})}. $$\n\nNotice that computing $p ( Y _ { a } ( D _ { \\bar { a } } ) )$ requires knowledge of the CBN. If the CBN structure is not known or",
      "referring_paragraphs": [
        "Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A D$ , and therefore $A D Y$ , is considered unfair, unfairness overall population can be quantified with $\\langle Y \\rangle _ { p ( Y | a ) } - \\langle Y \\rangle _ { p ( Y | \\bar { a } ) }$",
        "In the college admission example of Fig. 7 in which the path $A D Y$ is considered fair, rather than measuring unfairness overall population, we might want to know e.g. whether a rejected female applicant $\\{ a ^ { n } = a , q ^ { n } , d ^ { n } , y ^ { n } = 0 \\}$ was treated unfairly. We can answer this question by estimating whether the applicant would have been admitted had she been male ( $A = \\bar { a }$ ) along the direct path $A Y$ from $p ( Y _ { \\bar { a } } ( D _ { a } ) | A = a , Q ",
        "To understand how this can be achieved, consider the following linear model associated to a CBN with the same structure as the one in Fig. 7",
        "Fig. 7. CBN underlying a college admission scenario.\n\nIn the more complex case in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$ , given by"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig11.jpg",
      "image_filename": "1907.06430_page0_fig11.jpg",
      "caption": "The relationships between $A , Q , D , Y$ and $Y _ { \\bar { a } } ( D _ { a } )$ in this model can be inferred from the twin Bayesian network [39] on the left resulting from the intervention $A \\ = \\ a$ along $A \\ \\ D$ a",
      "context_before": "$$ A \\sim \\operatorname {B e r n} (\\pi), \\quad Q = \\theta^ {q} + \\epsilon_ {q}, \\quad D = \\theta^ {d} + \\theta_ {a} ^ {d} A + \\epsilon_ {d}, $$\n\n$$ Y = \\theta^ {y} + \\theta_ {a} ^ {y} A + \\theta_ {q} ^ {y} Q + \\theta_ {d} ^ {y} D + \\epsilon_ {y}, \\tag {4} $$\n\nwhere $\\epsilon _ { q } , \\epsilon _ { d }$ and $\\epsilon _ { y }$ are unobserved independent zero-mean Gaussian variables.",
      "context_after": "The relationships between $A , Q , D , Y$ and $Y _ { \\bar { a } } ( D _ { a } )$ in this model can be inferred from the twin Bayesian network [39] on the left resulting from the intervention $A \\ = \\ a$ along $A \\ \\ D$ and $A \\ = \\ \\bar { a }$ along $A Y$ : in addition to $A , Q , D$ and $Y$ , the network contains the variables $Q ^ { * }$ , $D _ { a }$ and $Y _ { \\bar { a } } ( D _ { a } )$ corresponding to the counterfactual world in which $A = \\bar { a }$ along $A Y$ , with $Q ^ { * } = \\theta ^ { q } + \\epsilon _ { q } , D _ { a } = \\theta ^ { d } + \\theta _ { a } ^ { d } a + \\epsilon _ { d }$ , and $Y _ { \\bar { a } } ( D _ { a } ) = \\theta ^ { y } + \\theta _ { a } ^ { y } \\bar { a } + \\theta _ { q } ^ { y } Q ^ { * } + \\theta _ { d } ^ { y } D _ { a } + \\epsilon _ { y }$ . The two groups of variables are connected through $\\epsilon _ { d } , \\epsilon _ { q } , \\epsilon _ { y }$ , indicating that the factual and counterfactual worlds\n\nshare the same unobserved randomness. From this network, we can deduce that $Y _ { \\bar { a } } ( D _ { a } ) \\perp \\perp \\{ A , Q , D , Y \\} | \\epsilon = \\{ \\epsilon _ { q } , \\epsilon _ { d } , \\epsilon _ { y } \\} ^ { 6 }$ , and therefore that we can express\n\n[Section: A Causal Bayesian Networks Viewpoint on Fairness]",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg",
      "image_filename": "1907.06430_page0_fig12.jpg",
      "caption": "(a)",
      "context_before": "Appendix A Bayesian Networks\n\nA graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.\n\nA path from node $X _ { i }$ to node $X _ { j }$ is a sequence of linked nodes starting at $X _ { i }$ and ending at $X _ { j }$ . A directed path is a path whose links are directed and pointing from preceding towards following nodes in the sequence.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_14",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig13.jpg",
      "image_filename": "1907.06430_page0_fig13.jpg",
      "caption": "(b) Fig. 8. Directed (a) acyclic and (b) cyclic graph.",
      "context_before": "",
      "context_after": "A directed acyclic graph (DAG) is a directed graph with no directed paths starting and ending at the same node. For example, the directed graph in Fig. 8(a) is acyclic. The addition of a link from $X _ { 4 }$ to $X _ { 1 }$ makes the graph cyclic (Fig. 8(b)). A node $X _ { i }$ with a directed link to $X _ { j }$ is called parent of $X _ { j }$ . In this case, $X _ { j }$ is called child of $X _ { i }$ .\n\n[Section: A Causal Bayesian Networks Viewpoint on Fairness]\n\nA node is a collider on a path if it has (at least) two parents on that path. Notice that a node can be a collider on a path and a non-collider on another path. For example, in Fig. 8(a) $X _ { 3 }$ is a collider on the path $X _ { 1 } \\right. X _ { 3 } \\left. X _ { 2 }$ and a non-collider on the path $X _ { 2 } X _ { 3 } X _ { 4 }$ .",
      "referring_paragraphs": [
        "A directed acyclic graph (DAG) is a directed graph with no directed paths starting and ending at the same node. For example, the directed graph in Fig. 8(a) is acyclic. The addition of a link from $X _ { 4 }$ to $X _ { 1 }$ makes the graph cyclic (Fig. 8(b)). A node $X _ { i }$ with a directed link to $X _ { j }$ is called parent of $X _ { j }$ . In this case, $X _ { j }$ is called child of $X _ { i }$ .",
        "A node is a collider on a path if it has (at least) two parents on that path. Notice that a node can be a collider on a path and a non-collider on another path. For example, in Fig. 8(a) $X _ { 3 }$ is a collider on the path $X _ { 1 } \\right. X _ { 3 } \\left. X _ { 2 }$ and a non-collider on the path $X _ { 2 } X _ { 3 } X _ { 4 }$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.06430_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1907.09013": [
    {
      "doc_id": "1907.09013",
      "figure_id": "1907.09013_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg",
      "image_filename": "1907.09013_page0_fig0.jpg",
      "caption": "3.2 A Taxonomy of Causes",
      "context_before": "Figure 1 shows a commonly accepted process diagram for model building and deployment. This is known as the Cross Industry Standard Process for Data Mining (CRISP-DM)17, and has been a standard for well over a decade.\n\nThis diagram shows how data science tasks (particularly deployed supervised learning systems) are commonly approached as an iterative process, where the feedback from prior iterations informs models and decisions in subsequent steps. While the heart of a classification system is the training step, it is generally well understood by data scientists that training the actual algorithm comprises the minority of a data scientist’s time. The majority of time spent is generally focused on making somewhat subjective decisions, such as what events to predict, where and how to sample data, how to clean said data, how to evaluate the model and how to create a decision policy from the algorithm’s output.\n\nOur taxonomy will show that discrimination can creep in at any one of these stages, so persistent vigilance and awareness is advised throughout the process. When we discuss solutions later, we will also make recommendations on where in this process a data scientist can employ discrimination-aware unit tests to appropriately audit the need and effectiveness of their chosen discrimination-aware techniques.",
      "context_after": "3.2 A Taxonomy of Causes\n\nA classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.\n\nWe focus on building and on using the classifier. For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification. When we consider how the classifier is used, we include in our taxonomy “procedural failures” that could lead to discrimination.",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.09013_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.09013",
      "figure_id": "1907.09013_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/a2b57fd1944b782334fdab662db8d13950f8923fb4b91be8745cd3ea867b8f50.jpg",
      "image_filename": "a2b57fd1944b782334fdab662db8d13950f8923fb4b91be8745cd3ea867b8f50.jpg",
      "caption": "Figure 2: Commonly cited causes of classifier discrimination.",
      "context_before": "A classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.\n\nWe focus on building and on using the classifier. For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification. When we consider how the classifier is used, we include in our taxonomy “procedural failures” that could lead to discrimination.\n\nFigure 2 shows a high level view of the taxonomy, along with indicators showing in what parts of CRISP-DM we might expect to encounter each cause. After providing more detail on each entry in the taxonomy, we will follow with a survey of remedies for the practicing data scientist to consider when faced with these issues.",
      "context_after": "Caption{Letters detail where in the CRISP-DM process a data scientist is likely to encounter each cause, corresponding to: $\\mathbf { A } =$ Business Understanding, $\\boldsymbol { \\mathrm { B } } =$ Data Understanding, $\\mathrm { C } =$ Data Preparation, $\\mathrm { D } =$ Modeling, $\\mathrm { E } =$ Evaluation, $\\mathrm { F } =$ Deployment.}\n\n3.2.1 A Classifier’s Source of Discrimination\n\nFor this taxonomy we separate the classifier from the classification system. We define the classifier as the function that maps an input tuple $( S , X )$ into an action space $A$ (i.e., hire/don’t hire, assign a police patrol or not, etc.), and the classification system as the technology and set of processes that implement the classifier.",
      "referring_paragraphs": [
        "Figure 2 shows a high level view of the taxonomy, along with indicators showing in what parts of CRISP-DM we might expect to encounter each cause. After providing more detail on each entry in the taxonomy, we will follow with a survey of remedies for the practicing data scientist to consider when faced with these issues.",
        "Figure 2 shows a high level view of the taxonomy, along with indicators showing in what parts of CRISP-DM we might expect to encounter each cause."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.09013_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1907.09013",
      "figure_id": "1907.09013_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg",
      "image_filename": "1907.09013_page0_fig1.jpg",
      "caption": "Figure 3",
      "context_before": "While we do not claim that the above set of issues and rules is exhaustive, we feel it is a comprehensive starting point for the practicing data scientist to learn to become aware of how unintended discrimination may occur. Awareness is a necessary but not sufficient condition for preventing discrimination in classification systems.\n\n4. Auditing and Removing Unintended Discrimination\n\nIn this section we propose a discrimination-aware auditing process that mirrors the standard process for developing classifiers. It has been amended to incorporate discrimination unit tests and highlights where state-of-the-art remedies may fit. We also present an overview of prior discrimination-aware data mining work as a reference to readers, with suggestions for how a data scientist might think about incorporating different methods into their common workflows. By developing discrimination-aware systems, data scientists can incorporate ethical and legal constraints into their models, and thereby result in the intended outcomes that ethics and law hope to achieve.",
      "context_after": "Caption {We update standard a classification process diagram to reflect key testing and decision points that can inject discrimination awareness into the workflow. Dotted arrows and figures represent optional workflows, representing pre-processing, in-processing, and post-processing techniques, as suggested by the prior art}.\n\nOur process in Figure 3 begins with the raw data to be used for model development. We will assume here that the scientist has already carefully formulated the business problem and has consulted with the appropriate managers and legal team to understand the applicable antidiscrimination laws and/or ethical standards governing the application. This prerequisite work should define what data is available as a target variable and as predictive features, and what the appropriate discrimination metric should be.\n\nGiven this set up, we propose several “discrimination aware” unit tests that may guide the future development of the classifier. These tests fit into the process where exploratory analysis usually sits.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.09013_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1907.12059": [
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig0.jpg",
      "image_filename": "1907.12059_page0_fig0.jpg",
      "caption": "Remark 1. Notice that $\\begin{array} { r } { ( i i ) = \\mathbb { E } _ { \\tau \\sim U ( \\Omega ) } | \\mathbb { P } ( S _ { 1 } > \\tau ) - } \\end{array}$ $\\mathbb { P } ( S _ { 2 } > \\tau ) | = 0$ if and only if $p _ { S _",
      "context_before": "$$ \\begin{array}{l} \\mathbb{E}_{\\substack{\\tau \\sim U(\\Omega)\\\\ x\\sim p_{S_{1}}}}\\mathbb{P}\\big(\\tau \\in (m^{T}_{x},M^{T}_{x})\\big) \\\\ = \\int_ {\\tau = 0} ^ {1} \\int_ {x} | x - T (x) | p _ {S _ {1}} (x) d x d \\tau \\\\ = \\int_ {x} | x - T (x) | p _ {S _ {1}} (x) d x. \\\\ \\end{array} $$\n\n$$ \\begin{array}{l} \\mathcal {W} _ {1} \\left(p _ {S _ {1}}, p _ {S _ {2}}\\right) = \\min _ {T \\in \\mathcal {T}} \\int_ {x} | x - T (x) | p _ {S _ {1}} (x) d x \\\\ = \\int_ {x} | x - T ^ {*} (x) | p _ {S _ {1}} (x) d x \\\\ = \\mathop{\\mathbb{E}}_{\\substack{\\tau \\sim U(\\Omega)\\\\ x\\sim p_{S_{1}}}}\\mathbb{P}\\big(\\tau \\in \\big(m_{x}^{T^{*}},M_{x}^{T^{*}}\\big)\\big). \\\\ \\end{array} $$\n\nThis prove that (i) equals (iii).",
      "context_after": "Remark 1. Notice that $\\begin{array} { r } { ( i i ) = \\mathbb { E } _ { \\tau \\sim U ( \\Omega ) } | \\mathbb { P } ( S _ { 1 } > \\tau ) - } \\end{array}$ $\\mathbb { P } ( S _ { 2 } > \\tau ) | = 0$ if and only if $p _ { S _ { 1 } } = p _ { S _ { 2 } }$ . Indeed, by Proposition 1 and the property of the $\\mathcal { W } _ { 1 }$ metric, $( i i ) = 0$ $\\iff \\mathcal { W } _ { 1 } ( p _ { S _ { 1 } } , p _ { S _ { 2 } } ) = 0 \\iff p _ { S _ { 1 } } = p _ { S _ { 2 } } .$\n\nTo reach SDP, we need to achieve $p _ { S _ { a } } = p ^ { * } \\forall a \\in { \\mathcal { A } }$ , where $p ^ { * } \\in \\mathcal { P } ( \\Omega )$ , the space of pdfs on $\\Omega$ . We would like to choose transportation maps $T$ and a target distribution $p ^ { * }$ such that the transportation process from $p _ { S _ { a } }$ to $p ^ { * }$ incurs minimal total expected class prediction changes. Assume that the groups are all disjoint, so that the per-group transportation maps $T$ are independent from each other. Let $\\mathbb { T } ( p ^ { * } )$ be the set of transportation maps with elements $T$ such that, restricted to group $^ { a }$ , $T$ is a transportation map from $p _ { S _ { a } }$ to $p ^ { * }$ (i.e. $\\mathbb { T } ( p ^ { * } ) = \\{ T \\in \\mathcal { T } ( p _ { S } , p ^ { * } ) \\mid$ $T ( S ) { \\big \\vert } _ { A = a } = T _ { a } \\in { \\mathcal { T } } _ { a } = { \\mathcal { T } } ( p _ { S _ { a } } , p ^ { * } ) \\}$ where $\\boldsymbol { \\mathcal { T } } ( \\boldsymbol { p } _ { \\boldsymbol { S } } , \\boldsymbol { p } ^ { * } )$ denotes the space of transportation maps from $p _ { S }$ to $p ^ { * }$ ). We would like to obtain\n\n$$ \\begin{array}{l} \\min _ {\\left. \\begin{array}{l} T \\in \\mathbb {T} (M _ {*}) ^ {*} \\end{array} \\right.} \\quad \\mathbb {E} _ {(0)} \\mathbb {P} (\\tau \\in \\left(m _ {x} ^ {T}, M _ {x} ^ {T}\\right)) \\\\ \\begin{array}{l} I \\in \\mathbb {1} (p) \\quad \\tau \\sim U (\\Omega) \\\\ p ^ {*} \\in \\mathcal {P} (\\Omega) \\quad x \\sim p _ {\\Sigma} \\end{array} \\\\ = \\min_{\\substack{T\\in \\mathbb{T}(p^{*})\\\\ p^{*}\\in \\mathcal{P}(\\Omega)}}\\sum_{\\boldsymbol {a}\\in \\mathcal{A}}\\underbrace{p(A = \\boldsymbol{a})}_{p_{\\boldsymbol{a}}}\\underset {\\tau \\sim U(\\Omega)}{\\mathbb{E}}_{x\\sim p_{S_{\\boldsymbol{a}}}}\\mathbb{P}(\\tau \\in (m_{x}^{T},M_{x}^{T})) \\\\ = \\min_{p^{*}\\in \\mathcal{P}(\\Omega)}\\sum_{\\boldsymbol {a}\\in \\mathcal{A}}p_{\\boldsymbol{a}}\\min_{T\\in \\mathcal{T}_{\\boldsymbol{a}}}\\underset { \\begin{array}{c}\\tau \\sim U(\\Omega)\\\\ x\\sim p_{S_{\\boldsymbol{a}}} \\end{array} }{\\mathbb{E}}\\mathbb{P}(\\tau \\in (m_{x}^{T},M_{x}^{T})) \\\\ = \\min _ {p ^ {*} \\in \\mathcal {P} (\\Omega)} \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\min _ {T \\in \\mathcal {T} _ {\\boldsymbol {a}}} \\int_ {x \\in \\Omega} | x - T (x) | p _ {S _ {\\boldsymbol {a}}} (x) d x \\\\ = \\min _ {p ^ {*} \\in \\mathcal {P} (\\Omega)} \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {\\boldsymbol {a}}}, p ^ {*}\\right). \\\\ \\end{array} $$",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/bfb1d403cb4990f230659fd9fb77f12dd2f1edb741ebffdf56481dca13229b7c.jpg",
      "image_filename": "bfb1d403cb4990f230659fd9fb77f12dd2f1edb741ebffdf56481dca13229b7c.jpg",
      "caption": "Table 1: Adult Dataset – German Credit Dataset",
      "context_before": "Unconstrained: Logistic regression with no fairness constraints.\n\nHardt’s Post-Process: Post-processing of the logistic regression beliefs $s ^ { n }$ of all individuals in group $^ { a }$ by adding $0 . 5 - \\tau _ { a }$ , where the threshold $\\tau _ { a }$ is found using the method of Hardt et al. (2016). This ensures that DP is satisfied at threshold $\\tau = 0 . 5$ .\n\nConstrained Optimization: Lagrangian-based method (see e.g. Eban et al. (2017); Goh et al. (2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$",
      "context_after": "as fairness constraints with threshold $\\tau = 0$ .\n\nAdv. Constr. Opt.: The same as the previous method, but with more fairness constraints. Specifically, the fairness constraints are equal positive prediction rates for a set of thresholds from $- 2$ to 2 in increments of 0.2 on the output of the linear model.\n\n5.1 TRAINING DETAILS",
      "referring_paragraphs": [
        "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches all group histograms to the barycenter after training for 10,000 steps with $\\beta = 1 0 0$ .",
        "(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"5\">Adult</td><td colspan=\"5\">German</td></tr><tr><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td></tr><tr><td>Unconstrained</td><td>.142</td><td>.198</td><td>.413</td><td>.426</td><td>.806</td><td>.248</td><td>.319</td><td>.124</td><td>.102</td><td>.103</td></tr><tr><td>Hardt&#x27;s Post-Process</td><td>.165</td><td>.289</td><td>.327</td><td>.551</td><td>1.058</td><td>.248</td><td>.333</td><td>.056</td><td>.045</td><td>.045</td></tr><tr><td>Constrained Opt.</td><td>.205</td><td>.198</td><td>.065</td><td>.087</td><td>.166</td><td>.318</td><td>.320</td><td>.173</td><td>.149</td><td>.149</td></tr><tr><td>Adv.",
        "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset.",
        "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/7650c8f06264f67f1ff3e5b9ca8a4a9092a7e81c80adb36483bd99e4ff48b4d7.jpg",
      "image_filename": "7650c8f06264f67f1ff3e5b9ca8a4a9092a7e81c80adb36483bd99e4ff48b4d7.jpg",
      "caption": "Table 2: Bank Marketing Dataset – Community & Crime Dataset",
      "context_before": "The UCI German Credit Dataset. This dataset contains 20 attributes for 1,000 individuals applying for loans. Each applicant is classified as a good or bad credit risk, i.e. as likely or not likely to repay the loan. We randomly divided the dataset into training and test sets of sizes 670 and 330 respectively.\n\nPre-processing and Sensitive Attributes. We did not do any pre-processing. As sensitive attributes, we considered age $\\leq 3 0$ and $> 3 0$ years old), obtaining two groups.\n\nThe UCI Bank Marketing Dataset. This dataset contains 20 attributes for 41,188 individuals. Each individual is classified as subscribed or not to a term deposit. We divided the dataset into train and test sets of sizes 32,950 and 8,238 respectively.",
      "context_after": "Pre-processing and Sensitive Attributes. We preprocessed the data as for the Adult dataset. We transformed the categorical features into binary ones, and the continuous features into five binary features based on five quantile bins, obtaining a total of 60 features. We also subtracted the mean from cons.price.idx, cons.conf.idx, euribor3m, and nr.employed to make them zero-centered. As sensitive attributes, we considered age, which was discretized based on five quantiles leading to five groups.\n\nThe UCI Communities & Crime Dataset. This dataset contains 135 attributes for 1994 communities; 1495 and 499 for the training and test sets respectively. The goal is to predict whether a community has high (above the 70-th percentile) crime rate.\n\nPre-processing and Sensitive Attributes. We preprocessed the data as in Wu et al. (2018). As sensitive attributes, we considered race (Black, White, Asian and Hispanic), thresholded at the median to form height groups.",
      "referring_paragraphs": [
        "Since Wass-1 Penalty is trained by gradient descent, earlystopping can be an effective way to control trade-off between accuracy and fairness. Figure 2 shows a typical example of two trade-off curves between SDD/SPDD and",
        "Table 2: Bank Marketing Dataset – Community & Crime Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"5\">Bank Marketing</td><td colspan=\"5\">Community &amp; Crime</td></tr><tr><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td></tr><tr><td>Unconstrained</td><td>.094</td><td>.138</td><td>.135</td><td>.134</td><td>.61</td><td>.116</td><td>.195</td><td>.581</td><td>1.402</td><td>7.649</td></tr><tr><td>Hardt&#x27;s Post-Process</td><td>.097</td><td>.181</td><td>.018</td><td>.367</td><td>1.057</td><td>.321</td><td>.441</td><td>.226</td><td>.536</td><td>2.679</td></tr><tr><td>Constrained Opt.</td><td>.105</td><td>.110</td><td>.049</td><td>.026</td><td>.076</td><td>.289</td><td>.263</td><td>.193</td><td>.369</td><td>2.003</td></tr><tr><td>Adv.",
        "Figure 2 shows a typical example of two trade-off curves between SDD/SPDD and",
        "Figure 2: Err-Exp v.s."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig1.jpg",
      "image_filename": "1907.12059_page0_fig1.jpg",
      "caption": "7Given the deterministic baseline logistic regression model, all standard deviations are on the order of $1 0 ^ { - 4 }$ or below.",
      "context_before": "The main experiment results are shown in Tables 1 and $2 ^ { 7 }$ . Focusing on the three more relevant metrics – namely Err-Exp as the robust error measure, SDD as the conventional fairness comparison metric, and SPDD as the targetneural, preferred fairness metric (according to which we picked the best hyperparameter settings) – we can see that Wass-1 Penalty and Wass-1 Penalty DB have lowest SDD and SPDD (blue) on the German and Crime datasets and on the Adult and Bank datasets respectively. The fairness performance of these two methods are followed closely by the simpler Wass-1 Post-Process methods on all datasets. Hardt’s Post-Process method incurs largest errors (red) on all datasets. After the Unconstrained baseline, Constrained Optimization and Adv. Contr. Opt. give lowest error on the Adult, Bank and Crime datasets, whilst Constrained Optimization and Wass-1 Penalty (DB) give lowest error on the German dataset. Overall the Wasserstein-1 methods gave best fairness performance on all the datasets with similar or lower compromise on accuracy than the baselines.\n\nSince Wass-1 Penalty is trained by gradient descent, earlystopping can be an effective way to control trade-off between accuracy and fairness. Figure 2 shows a typical example of two trade-off curves between SDD/SPDD and\n\n7Given the deterministic baseline logistic regression model, all standard deviations are on the order of $1 0 ^ { - 4 }$ or below.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig2.jpg",
      "image_filename": "1907.12059_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig3.jpg",
      "image_filename": "1907.12059_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig4.jpg",
      "image_filename": "1907.12059_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig5.jpg",
      "image_filename": "1907.12059_page0_fig5.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig6.jpg",
      "image_filename": "1907.12059_page0_fig6.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg",
      "image_filename": "1907.12059_page0_fig7.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig8.jpg",
      "image_filename": "1907.12059_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_12",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig9.jpg",
      "image_filename": "1907.12059_page0_fig9.jpg",
      "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter. Figure 2: Err-Exp v.s. SDD, Err-Exp v.s. SPDD trade-off curves on Bank test set using Wass-1 Penalty DB, points plotted every 100 steps over 80,000 total training steps.",
      "context_before": "",
      "context_after": "Err-Exp. Though not always the case, often as the learning model moves towards the fairness goal of SDP, model accuracy decreases (Err-Exp increases).\n\nWe introduced an approach to ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance. We demon-\n\nstrated that using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions. We introduced two methods with different desirable properties, a Wasserstein-1 constrained method that does not necessarily require access to sensitive information at deployment time, and an alternative fast and practical approximation method that requires knowledge of sensitive information at test time. We showed that these methods outperform previous approaches in the literature.",
      "referring_paragraphs": [
        "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset. Wasserstein-1 Penalty effectively matches all group histograms to the barycenter after training for 10,000 steps with $\\beta = 1 0 0$ .",
        "(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"5\">Adult</td><td colspan=\"5\">German</td></tr><tr><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td></tr><tr><td>Unconstrained</td><td>.142</td><td>.198</td><td>.413</td><td>.426</td><td>.806</td><td>.248</td><td>.319</td><td>.124</td><td>.102</td><td>.103</td></tr><tr><td>Hardt&#x27;s Post-Process</td><td>.165</td><td>.289</td><td>.327</td><td>.551</td><td>1.058</td><td>.248</td><td>.333</td><td>.056</td><td>.045</td><td>.045</td></tr><tr><td>Constrained Opt.</td><td>.205</td><td>.198</td><td>.065</td><td>.087</td><td>.166</td><td>.318</td><td>.320</td><td>.173</td><td>.149</td><td>.149</td></tr><tr><td>Adv.",
        "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset.",
        "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg",
      "image_filename": "1907.12059_page0_fig10.jpg",
      "caption": "(a) Left side of Eq. (12)",
      "context_before": "$$ \\left| p _ {\\boldsymbol {a}} - \\hat {p} _ {\\boldsymbol {a}} \\right| \\leq \\frac {\\epsilon}{4 | \\mathcal {A} | \\max [ L , 1 ]}. \\tag {11} $$\n\nConsequently the desired result holds:\n\n$$ \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {\\boldsymbol {a}}}, p _ {\\bar {S}}\\right) \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(\\hat {p} _ {S _ {\\boldsymbol {a}}}, \\hat {p} _ {\\bar {S}}\\right) + \\epsilon . $$",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_14",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig11.jpg",
      "image_filename": "1907.12059_page0_fig11.jpg",
      "caption": "(b) Right side of Eq. (12) Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).",
      "context_before": "",
      "context_after": "If $p _ { \\bar { S } }$ equals the weighted barycenter of the population level distributions $\\{ p _ { S _ { a } } \\}$ , then\n\n$$ \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {a}}, p _ {\\bar {S}}\\right) \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {a}}, \\hat {p} _ {\\bar {S}}\\right). $$\n\nSince $\\hat { p } _ { \\pm } \\mathcal { W } _ { 1 } ( p _ { S _ { a } } , \\hat { p } _ { \\bar { S } } ) \\leq \\hat { p } _ { \\pm } \\mathcal { W } _ { 1 } ( \\hat { p } _ { S _ { a } } , \\hat { p } _ { \\bar { S } } ) + \\hat { p } _ { \\pm } \\mathcal { W } _ { 1 } ( \\hat { p } _ { S _ { a } } , p _ { S _ { a } } )$ , with probability $1 - \\delta$ :",
      "referring_paragraphs": [
        "Intuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3. Here is a complete proof.",
        "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).\n\nIf $p _ { \\bar { S } }$ equals the weighted barycenter of the population level distributions $\\{ p _ { S _ { a } } \\}$ , then",
        "(12) correspond to two ways of computing the same shaded area in Figure 3."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig12.jpg",
      "image_filename": "1907.12059_page0_fig12.jpg",
      "caption": "C Inverse CDFs",
      "context_before": "Since $\\hat { p } _ { \\pm } \\mathcal { W } _ { 1 } ( p _ { S _ { a } } , \\hat { p } _ { \\bar { S } } ) \\leq \\hat { p } _ { \\pm } \\mathcal { W } _ { 1 } ( \\hat { p } _ { S _ { a } } , \\hat { p } _ { \\bar { S } } ) + \\hat { p } _ { \\pm } \\mathcal { W } _ { 1 } ( \\hat { p } _ { S _ { a } } , p _ { S _ { a } } )$ , with probability $1 - \\delta$ :\n\n$$ \\begin{array}{l} \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} p _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {\\boldsymbol {a}}}, p _ {\\bar {S}}\\right) \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(p _ {S _ {\\boldsymbol {a}}}, p _ {\\bar {S}}\\right) + \\frac {\\epsilon}{2} \\\\ \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} (\\hat {p} _ {S _ {\\boldsymbol {a}}}, \\hat {p} _ {\\bar {S}}) + \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} (\\hat {p} _ {S _ {\\boldsymbol {a}}}, p _ {S _ {\\boldsymbol {a}}}) + \\frac {\\epsilon}{2} \\\\ \\leq \\sum_ {\\boldsymbol {a} \\in \\mathcal {A}} \\hat {p} _ {\\boldsymbol {a}} \\mathcal {W} _ {1} \\left(\\hat {p} _ {S _ {\\boldsymbol {a}}}, \\hat {p} _ {\\bar {S}}\\right) + \\epsilon \\\\ \\end{array} $$\n\nThe first inequality follows from Eq. (11), and the third one by Eq. (10). The result follows.",
      "context_after": "Lemma 6. Given two differentiable and invertible cumulative distribution functions $f , g$ over the probability space $\\Omega = [ 0 , 1 ]$ , thus $f , g : [ 0 , 1 ] \\to [ 0 , 1 ]$ , we have\n\n$$ \\int_ {s = 0} ^ {1} | f ^ {- 1} (s) - g ^ {- 1} (s) | d s = \\int_ {\\tau = 0} ^ {1} | f (\\tau) - g (\\tau) | d \\tau . \\tag {12} $$\n\nIntuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3. Here is a complete proof.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "1907.12059_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "1909.05088": [
    {
      "doc_id": "1909.05088",
      "figure_id": "1909.05088_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/c58dc5443f7a09d1168fae677ec50b55e78ecefd347298e2a01aee47e6b4245a.jpg",
      "image_filename": "c58dc5443f7a09d1168fae677ec50b55e78ecefd347298e2a01aee47e6b4245a.jpg",
      "caption": "Table 1: Overview of annotated parallel sentences per language pair",
      "context_before": "3.1 Analysis of the EN–FR Annotated Dataset\n\nWe first analysed the distribution of male and female sentence in our data. In the 10 different\n\n2https://github.com/evavnmssnhv/ Europarl-Speaker-Information",
      "context_after": "datasets we experimented with, the percentage of sentences uttered by female speakers is very similar, ranging between $32 \\%$ and $33 \\%$ . This similarity can be explained by the fact that Europarl is largely a multilingual corpus with a big overlap between the different language pairs.\n\nWe conducted a more focused analysis on one of the subcorpora (EN–FR) with respect to the percentage of sentences uttered by males/females for various age groups to obtain a better grasp of what kind of data we are using for training. As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 . 7 1 \\% )$ , more male data is available in all age groups. Furthermore, when looking at the entire dataset, $6 7 . 3 9 \\%$ of the sentences are produced by male speakers. Moreover, almost half of the total number of sentences are uttered by the 50–60 age group $( 4 3 . 7 6 \\% )$ .",
      "referring_paragraphs": [
        "We followed the approach described by Rabinovich et al. (2017) and tagged parallel sentences from Europarl (Koehn, 2005) with speaker information (name, gender, age, date of birth, euroID and date of the session) by retrieving speaker information provided by tags in the Europarl source files. The Europarl source files contain information about the speaker on the paragraph level and the filenames contain the data of the session. By retrieving the names of the speakers together with meta-informati",
        "We conducted a more focused analysis on one of the subcorpora (EN–FR) with respect to the percentage of sentences uttered by males/females for various age groups to obtain a better grasp of what kind of data we are using for training. As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 . 7 1 \\% )$ , more male data is available in all age groups. Furthermore, when looking at the ent",
        "An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1.",
        "As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .",
        "Figure 1: Percentage of female and male speakers per age group\n\nThe analysis shows that indeed, there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1909.05088_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1909.05088",
      "figure_id": "1909.05088_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg",
      "image_filename": "1909.05088_page0_fig0.jpg",
      "caption": "Figure 1: Percentage of female and male speakers per age group",
      "context_before": "datasets we experimented with, the percentage of sentences uttered by female speakers is very similar, ranging between $32 \\%$ and $33 \\%$ . This similarity can be explained by the fact that Europarl is largely a multilingual corpus with a big overlap between the different language pairs.\n\nWe conducted a more focused analysis on one of the subcorpora (EN–FR) with respect to the percentage of sentences uttered by males/females for various age groups to obtain a better grasp of what kind of data we are using for training. As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 . 7 1 \\% )$ , more male data is available in all age groups. Furthermore, when looking at the entire dataset, $6 7 . 3 9 \\%$ of the sentences are produced by male speakers. Moreover, almost half of the total number of sentences are uttered by the 50–60 age group $( 4 3 . 7 6 \\% )$ .",
      "context_after": "The analysis shows that indeed, there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce.\n\n4 Experimental Setup\n\nWe carried out a set of experiments on 10 language pairs (the ones for which we compiled more than 500k annotated Europarl parallel sentences): EN–DE, EN–FR, EN–ES, EN–EL, EN–PT, EN– FI, EN–IT, EN–SV, EN–NL and EN–DA. We augmented every sentence with a tag on the English source side, identifying the gender of the speaker, as illustrated in (1). This approach for encoding sentence-specific information for NMT has been successfully exploited to tackle other types of issues, multilingual NMT systems (e.g., Zero Shot Translation (Johnson et al., 2017)), domain adaptation (Sennrich et al., 2016), etc.",
      "referring_paragraphs": [
        "We followed the approach described by Rabinovich et al. (2017) and tagged parallel sentences from Europarl (Koehn, 2005) with speaker information (name, gender, age, date of birth, euroID and date of the session) by retrieving speaker information provided by tags in the Europarl source files. The Europarl source files contain information about the speaker on the paragraph level and the filenames contain the data of the session. By retrieving the names of the speakers together with meta-informati",
        "We conducted a more focused analysis on one of the subcorpora (EN–FR) with respect to the percentage of sentences uttered by males/females for various age groups to obtain a better grasp of what kind of data we are using for training. As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 . 7 1 \\% )$ , more male data is available in all age groups. Furthermore, when looking at the ent",
        "An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1.",
        "As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .",
        "Figure 1: Percentage of female and male speakers per age group\n\nThe analysis shows that indeed, there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1909.05088_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1909.05088",
      "figure_id": "1909.05088_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/92b6e00aa8b1acfb2c8ca3ee998fd39436d97b56a10dc52387e86398e1f5786a.jpg",
      "image_filename": "92b6e00aa8b1acfb2c8ca3ee998fd39436d97b56a10dc52387e86398e1f5786a.jpg",
      "caption": "Table 2: BLEU scores for the 10 baseline (denoted with EN) and the 10 gender-enhanced NMT (denoted with EN-TAG) systems. Entries labeled with * present statistically significant differences $( \\mathtt { p } < 0 . 0 5 )$ ). Statistical significance was computed with the MultEval tool (Clark et al., 2011).",
      "context_before": "In this section we discuss some of the results obtained. We hypothesized that the male/female tags would be particularly helpful for French, Portuguese, Italian, Spanish and Greek, where adjectives and even verb forms can be marked by the gender of the speaker. Since, according to the literature, women and men also make use of different syntactic constructions and make different\n\nword choices, we also tested the approach on other languages that do not have morphological agreement with the gender of the speaker such as Danish (DA), Dutch (NL), Finnish (FI), German (DE) and Swedish (SV).\n\nFirst, we wanted to see how our tagged systems performed on the general test set compared to the baseline. In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented.",
      "context_after": "While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant $+ 0 . 3 1$ BLEU improvement.\n\nWe expected to see the strongest improvements in sentences uttered by female speakers as, according to our initial analysis, the male data was overrepresented in the training. To test this hypothesis, we evaluated all systems on a male-only and female-only test set. Furthermore, we also experimented on test sets containing the pronoun of the first person singular as this form is used when a speaker refers to himself/herself. The results on the specific test set for the EN–FR dataset are presented in Table 3. As hypothesized, the biggest BLEU score improvement is observed on the female test set, particularly, the test sets containing first person singular pronouns (F1).\n\nWe had a closer look at some of the transla-",
      "referring_paragraphs": [
        "First, we wanted to see how our tagged systems performed on the general test set compared to the baseline. In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented.",
        "While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant $+ 0 . 3 1$ BLE",
        "In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1909.05088_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1909.05088",
      "figure_id": "1909.05088_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/94d54bd411e9827bdf4be7db24130f11efc48615bff8c713645096eb2a69e8c4.jpg",
      "image_filename": "94d54bd411e9827bdf4be7db24130f11efc48615bff8c713645096eb2a69e8c4.jpg",
      "caption": "Table 3: BLEU-scores on EN–FR comparing the baseline (EN) and the tagged systems (EN–TAG) on 4 different test sets: a test set containing only male data (M), only female data (F), 1st person male data (M1) and first person female data (F1). All the improvements of the EN-TAG system are statistically significant $( \\mathtt { p } < 0 . 5 )$ , as indicated by *.",
      "context_before": "While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant $+ 0 . 3 1$ BLEU improvement.\n\nWe expected to see the strongest improvements in sentences uttered by female speakers as, according to our initial analysis, the male data was overrepresented in the training. To test this hypothesis, we evaluated all systems on a male-only and female-only test set. Furthermore, we also experimented on test sets containing the pronoun of the first person singular as this form is used when a speaker refers to himself/herself. The results on the specific test set for the EN–FR dataset are presented in Table 3. As hypothesized, the biggest BLEU score improvement is observed on the female test set, particularly, the test sets containing first person singular pronouns (F1).\n\nWe had a closer look at some of the transla-",
      "context_after": "tions.3 There are cases where the gender-informed (TAG) system improves over the baseline (BASE) due to better agreement. Interestingly, in (2) the French female form of vice-president (vicepresidente) appears in the translation produced by ´ the BASE system while the male form is the correct one. The gender-informed system does make the correct agreement by using the female variant. In (3) the speaker is female but the baseline system outputs a male form of the adjective ‘happy’ (‘heureux’).\n\n(Ref) En tant que vice-president ´ ...\n\n(2) (BASE) En tant que vice-presidente ´ ...",
      "referring_paragraphs": [
        "We expected to see the strongest improvements in sentences uttered by female speakers as, according to our initial analysis, the male data was overrepresented in the training. To test this hypothesis, we evaluated all systems on a male-only and female-only test set. Furthermore, we also experimented on test sets containing the pronoun of the first person singular as this form is used when a speaker refers to himself/herself. The results on the specific test set for the EN–FR dataset are presente",
        "The results on the specific test set for the EN–FR dataset are presented in Table 3."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "1909.05088_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "1910.10872": [
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg",
      "image_filename": "1910.10872_page0_fig0.jpg",
      "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
      "context_before": "Machine learning and AI systems are becoming omnipresent in everyday lives. Recently, attention has been directed to problems concerning fairness and algorithmic bias. Some progress has been made on the analysis of gender stereotyping in different natural language processing (NLP) components, such as word embedding (Bolukbasi et al. 2016; Zhao et al. 2018b), co-reference resolution (Zhao et al. 2018a), machine translation (Font and Costa-jussa 2019) ` and sentence encoders (May et al. 2019). In this work we study bias in named entity recognition (NER) systems and show how they can propagate gender bias by analyzing the 139-year history of U.S. male and female names from census data.\n\nOur experiments show that widely used named entity recognition systems are susceptible to gender bias. We find that relatively more female names were tagged as non-PERSON than male names even though the names were used in a context where they should have been marked as PERSON. An example is “Charlotte,” ranked as the top 8th most popular female U.S. baby name in 2018. “Charlotte” is almost always tagged wrongfully as a location by the stateof-the-art NER systems despite being used in a context when it is clear that the entity should be a person. Figure 1 has more examples with names that are either not recognized as an entity or wrongfully tagged. We show that there are many\n\nNamed Entity Recognition:",
      "context_after": "instances of such cases throughout history in the real world, and that there are more female names than male names that are incorrectly tagged. Moreover, based on this same U.S. census data, we find that this miscategorization affects more women than men. This serves as our definition of bias which considers the differences between gender groups following the statistical parity notion of fairness (Kusner et al. 2017; Dwork et al. 2012).\n\nThe contributions of this paper are fourfold:\n\narXiv:1910.10872v1 [cs.IR] 24 Oct 2019",
      "referring_paragraphs": [
        "Our experiments show that widely used named entity recognition systems are susceptible to gender bias. We find that relatively more female names were tagged as non-PERSON than male names even though the names were used in a context where they should have been marked as PERSON. An example is “Charlotte,” ranked as the top 8th most popular female U.S. baby name in 2018. “Charlotte” is almost always tagged wrongfully as a location by the stateof-the-art NER systems despite being used in a context w",
        "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity. The aim was to test the performance of the NER from different perspectives. Template 1, containing just the name, purely tests the name itself and reveals something about the distribution of the training data. Template 4 is designed to direct the model to tag the name as a person. Templ",
        "Figure 1 has more examples with names that are either not recognized as an entity or wrongfully tagged.",
        "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
        "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/ea2c20b23f7c5ab05f55d12542141bb473bca2528646f38197e4fd3ace4054ec.jpg",
      "image_filename": "ea2c20b23f7c5ab05f55d12542141bb473bca2528646f38197e4fd3ace4054ec.jpg",
      "caption": "Table 1: Templates that form our benchmark with their corresponding numbers as referenced in the paper. Template 1 is the “no context” template.",
      "context_before": "The contributions of this paper are fourfold:\n\narXiv:1910.10872v1 [cs.IR] 24 Oct 2019\n\n1https://github.com/Ninarehm/NERGenderBias",
      "context_after": "4. Finally, we analyzed datasets currently used for training many NER models and found the extent of gender bias in these datasets.\n\nModels and Experiments\n\nTo measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.2 We test these models against 139 years of U.S. census data3 from years 1880 to 2018. Our benchmark evaluates these models based upon how well they recognize these names as a PERSON entity.",
      "referring_paragraphs": [
        "Our experiments show that widely used named entity recognition systems are susceptible to gender bias. We find that relatively more female names were tagged as non-PERSON than male names even though the names were used in a context where they should have been marked as PERSON. An example is “Charlotte,” ranked as the top 8th most popular female U.S. baby name in 2018. “Charlotte” is almost always tagged wrongfully as a location by the stateof-the-art NER systems despite being used in a context w",
        "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity. The aim was to test the performance of the NER from different perspectives. Template 1, containing just the name, purely tests the name itself and reveals something about the distribution of the training data. Template 4 is designed to direct the model to tag the name as a person. Templ",
        "Figure 1 has more examples with names that are either not recognized as an entity or wrongfully tagged.",
        "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
        "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig1.jpg",
      "image_filename": "1910.10872_page0_fig1.jpg",
      "caption": "(a) Error Type-1 Weightedfemale male",
      "context_before": "Error Type-3 Weighted. This type of error is similar to Error Type-3 Unweighted; however, we considered how frequent the mistaken name was while calculating the error.\n\n$$ \\frac {\\sum_ {n \\in N _ {f}} f r e q _ {f} (n _ {t y p e} = \\emptyset)}{\\sum_ {n \\in N _ {f}} f r e q _ {f} (n)} $$\n\n2https://spacy.io 3http://www.ssa.gov/oact/babynames/names. zip",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig2.jpg",
      "image_filename": "1910.10872_page0_fig2.jpg",
      "caption": "(b) Error Type-2 Weighted",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig3.jpg",
      "image_filename": "1910.10872_page0_fig3.jpg",
      "caption": "(c) Error Type-3 Weightedfemale male Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig4.jpg",
      "image_filename": "1910.10872_page0_fig4.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_7",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig5.jpg",
      "image_filename": "1910.10872_page0_fig5.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig6.jpg",
      "image_filename": "1910.10872_page0_fig6.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig7.jpg",
      "image_filename": "1910.10872_page0_fig7.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig8.jpg",
      "image_filename": "1910.10872_page0_fig8.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig9.jpg",
      "image_filename": "1910.10872_page0_fig9.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig10.jpg",
      "image_filename": "1910.10872_page0_fig10.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig11.jpg",
      "image_filename": "1910.10872_page0_fig11.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig12.jpg",
      "image_filename": "1910.10872_page0_fig12.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg",
      "image_filename": "1910.10872_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig14.jpg",
      "image_filename": "1910.10872_page0_fig14.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_17",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig15.jpg",
      "image_filename": "1910.10872_page0_fig15.jpg",
      "caption": "0 0Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.",
      "context_before": "",
      "context_after": "0.03Different types of errors allow for fine-grained analysis into 0.02the existence of different biases. Our results indicate that all models are mostly more biased toward female names vs. 1875 1895 1915 1935 1955 1975 1995 2015male names, as shown in Figures 2 and 3 over the 139-year history. The fact that all the weighted cases are biased toward female names shows that more frequent and popular female names are susceptible to bias and error in named entity recognition systems—which is a more serious type of error to consider. For space considerations, we only report the results for one of the templates (Template #4) since the results were following similar trend for all the other templates wherein the models were mostly more biased toward female\n\n0.10.04names. We have included results from other templates in 0.080.02our supplementary material to demonstrate that other tem-0.060 1875 1895 1915 1935 1955 1975 1995 2015plates also follow a similar pattern. That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend. This result shows how context helps some models over others by bringing down the error rates when sentences are added to the names (templates #2 through #9). Other models perform better on template #1, showing that context, in fact confuses the model. We observe that in contextual-based models such as Flair, context indeed helps the model make the right de-",
      "referring_paragraphs": [
        "0.03cisions by it having less error rates for templates #2 through 0.010.02#9 compared to template #1. Other models do not necessar-0ily follow this pattern. As an example, we provide the types of names and errors that can happen in these models. We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.",
        "We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.",
        "<table><tr><td>Female Name</td><td>Frequency</td><td>Error Type</td></tr><tr><td>Charlotte</td><td>12,940</td><td>Tagged as LOC</td></tr><tr><td>Sofia</td><td>7,621</td><td>Tagged as LOC</td></tr><tr><td>Victoria</td><td>7,089</td><td>Tagged as LOC</td></tr><tr><td>Madison</td><td>7,036</td><td>Tagged as LOC</td></tr><tr><td>Aurora</td><td>4,785</td><td>Tagged as LOC</td></tr></table>\n\nTable 2: Top 5 mistagged examples from the Flair model on Template #4 of female and male names from our benchmark."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig16.jpg",
      "image_filename": "1910.10872_page0_fig16.jpg",
      "caption": "(a) Error Type-1 Unweighted",
      "context_before": "0.03Different types of errors allow for fine-grained analysis into 0.02the existence of different biases. Our results indicate that all models are mostly more biased toward female names vs. 1875 1895 1915 1935 1955 1975 1995 2015male names, as shown in Figures 2 and 3 over the 139-year history. The fact that all the weighted cases are biased toward female names shows that more frequent and popular female names are susceptible to bias and error in named entity recognition systems—which is a more serious type of error to consider. For space considerations, we only report the results for one of the templates (Template #4) since the results were following similar trend for all the other templates wherein the models were mostly more biased toward female\n\n0.10.04names. We have included results from other templates in 0.080.02our supplementary material to demonstrate that other tem-0.060 1875 1895 1915 1935 1955 1975 1995 2015plates also follow a similar pattern. That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend. This result shows how context helps some models over others by bringing down the error rates when sentences are added to the names (templates #2 through #9). Other models perform better on template #1, showing that context, in fact confuses the model. We observe that in contextual-based models such as Flair, context indeed helps the model make the right de-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig17.jpg",
      "image_filename": "1910.10872_page0_fig17.jpg",
      "caption": "(b) Error Type-2 Unweightedfemale male",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_20",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig18.jpg",
      "image_filename": "1910.10872_page0_fig18.jpg",
      "caption": "(c) Error Type-3 Unweightedfemale male",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig19.jpg",
      "image_filename": "1910.10872_page0_fig19.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig20.jpg",
      "image_filename": "1910.10872_page0_fig20.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig21.jpg",
      "image_filename": "1910.10872_page0_fig21.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_24",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig22.jpg",
      "image_filename": "1910.10872_page0_fig22.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_25",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig23.jpg",
      "image_filename": "1910.10872_page0_fig23.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_26",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig24.jpg",
      "image_filename": "1910.10872_page0_fig24.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_27",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig25.jpg",
      "image_filename": "1910.10872_page0_fig25.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_28",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig26.jpg",
      "image_filename": "1910.10872_page0_fig26.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_29",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig27.jpg",
      "image_filename": "1910.10872_page0_fig27.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_30",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig28.jpg",
      "image_filename": "1910.10872_page0_fig28.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_31",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig29.jpg",
      "image_filename": "1910.10872_page0_fig29.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_32",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg",
      "image_filename": "1910.10872_page0_fig30.jpg",
      "caption": "CoreNLP 0.040 0Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.",
      "context_before": "",
      "context_after": "0.03cisions by it having less error rates for templates #2 through 0.010.02#9 compared to template #1. Other models do not necessar-0ily follow this pattern. As an example, we provide the types of names and errors that can happen in these models. We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.\n\n6668Model Version Evaluation and Comparison\n\n0.64Updates to models often lead to superior performance; however, this may come with a detriment to fairness and bias. In 0.58this section we want to see how much the version updates in",
      "referring_paragraphs": [
        "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were some cases where version updates had serious fairness-related issues. For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in",
        "As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
        "2017), machine translation (Font and Costa-jussa 2019), language models (Bordia and Bowman `\n\nTable 3: Some examples on how tagging changed during version update of the CoreNLP model."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_33",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig31.jpg",
      "image_filename": "1910.10872_page0_fig31.jpg",
      "caption": "In order to report the results of our analysis, we used four of our models mentioned in the previous section that have version updates—namely small, medium, and large models from Spacy (versions 2.0 and 2.1) and versions",
      "context_before": "models will be robust toward fairness constraints and errors defined in the previous section. Thus, we will first define this source of bias. Then we will show the results for the models from the previous section that have various versions.\n\nDefinition (Version Bias). This is a type of bias that arises from updates in the systems.\n\nIn order to report the results of our analysis, we used four of our models mentioned in the previous section that have version updates—namely small, medium, and large models from Spacy (versions 2.0 and 2.1) and versions 3.8 and 3.9 from CoreNLP. We then repeated the experiments from the previous section to report the results for Error Type-1,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_34",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig32.jpg",
      "image_filename": "1910.10872_page0_fig32.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_35",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig33.jpg",
      "image_filename": "1910.10872_page0_fig33.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_36",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg",
      "image_filename": "1910.10872_page0_fig34.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_37",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig35.jpg",
      "image_filename": "1910.10872_page0_fig35.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_38",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig36.jpg",
      "image_filename": "1910.10872_page0_fig36.jpg",
      "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.",
      "context_before": "",
      "context_after": "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were some cases where version updates had serious fairness-related issues. For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper. Not only that, the average increase rate of this error is twice that of female names as opposed to male names when updating the model version. These newer versions may try to boost their performance by trying to tag more entities, which indeed would boost their performance when having other non-PERSON entities in the test set. However, these newer tags may be tagging PER-SON entities in the relevant context to the non-PERSON counterpart, which is not desirable when evaluated on our benchmark. The reason that we separated the types of errors and created the benchmark is to show the fine-grained and sensitive issues in these systems. Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5. However, these tags would not correctly assign PERSON tags to PERSON entities, which resulted in a degraded performance in Error Type-2, as shown in Figure 5 and resulted in no change in the overall performance based on Error Type-1 in the newer version update of the CoreNLP model. The results were identical for the unweighted case, and we can see how that degrade in performance was more severe for female names on average. As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
      "referring_paragraphs": [
        "0.10.04names. We have included results from other templates in 0.080.02our supplementary material to demonstrate that other tem-0.060 1875 1895 1915 1935 1955 1975 1995 2015plates also follow a similar pattern. That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend. This result shows how context helps some models over oth",
        "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that. Unlike the census data, which is represen-0tative of real-world statistics, wherein female names have more versatility— $62 \\%$ unique names vs. $38 \\%$ unique male names—datasets used in training the NER models contain $42 \\%$ female names vs. $58 \\%$ male names from the census data. Not only do the datasets not contain more versatile female names to ",
        "That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.",
        "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years.",
        "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.",
        "We also analyzed some datasets widely used in current state-of-the-art models and showed the existence of bias in these datasets as well which\n\nTable 4: Percentage of female and male names from the census data appearing in CoNLL 2003 and OntoNotes datasets with their corresponding counts."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_39",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/d57a3d503e0e00a7eb641e373a0e9e4c04d766d9a35ad24090802458b1ead2f8.jpg",
      "image_filename": "d57a3d503e0e00a7eb641e373a0e9e4c04d766d9a35ad24090802458b1ead2f8.jpg",
      "caption": "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were so",
      "context_before": "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were some cases where version updates had serious fairness-related issues. For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper. Not only that, the average increase rate of this error is twice that of female names as opposed to male names when updating the model version. These newer versions may try to boost their performance by trying to tag more entities, which indeed would boost their performance when having other non-PERSON entities in the test set. However, these newer tags may be tagging PER-SON entities in the relevant context to the non-PERSON counterpart, which is not desirable when evaluated on our benchmark. The reason that we separated the types of errors and created the benchmark is to show the fine-grained and sensitive issues in these systems. Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5. However, these tags would not correctly assign PERSON tags to PERSON entities, which resulted in a degraded performance in Error Type-2, as shown in Figure 5 and resulted in no change in the overall performance based on Error Type-1 in the newer version update of the CoreNLP model. The results were identical for the unweighted case, and we can see how that degrade in performance was more severe for female names on average. As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_40",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/e68bf2471a1469d380bf48346172a55fc35d6b133a81d944044cb47b4ba7afea.jpg",
      "image_filename": "e68bf2471a1469d380bf48346172a55fc35d6b133a81d944044cb47b4ba7afea.jpg",
      "caption": "Table 2: Top 5 mistagged examples from the Flair model on Template #4 of female and male names from our benchmark.",
      "context_before": "",
      "context_after": "Since data plays an important role in the outcome of the model and can directly affect the fairness constraints if it contains any biases, we decided to analyze some of the datasets that are widely used in the training of NER models to determine whether they show any biases toward a specific group that could result in the biased behavior observed in those results discussed in previous sections. We used the train, test, and development sets from two widely known CoNLL- $2 0 0 3 ^ { 4 }$ (Sang and De Meulder 2003) and\n\n4https://www.clips.uantwerpen.be/ conll2003/ner/",
      "referring_paragraphs": [
        "0.03cisions by it having less error rates for templates #2 through 0.010.02#9 compared to template #1. Other models do not necessar-0ily follow this pattern. As an example, we provide the types of names and errors that can happen in these models. We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.",
        "We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.",
        "<table><tr><td>Female Name</td><td>Frequency</td><td>Error Type</td></tr><tr><td>Charlotte</td><td>12,940</td><td>Tagged as LOC</td></tr><tr><td>Sofia</td><td>7,621</td><td>Tagged as LOC</td></tr><tr><td>Victoria</td><td>7,089</td><td>Tagged as LOC</td></tr><tr><td>Madison</td><td>7,036</td><td>Tagged as LOC</td></tr><tr><td>Aurora</td><td>4,785</td><td>Tagged as LOC</td></tr></table>\n\nTable 2: Top 5 mistagged examples from the Flair model on Template #4 of female and male names from our benchmark."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_41",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig37.jpg",
      "image_filename": "1910.10872_page0_fig37.jpg",
      "caption": "(a) Error Type-2 Weighted",
      "context_before": "Since data plays an important role in the outcome of the model and can directly affect the fairness constraints if it contains any biases, we decided to analyze some of the datasets that are widely used in the training of NER models to determine whether they show any biases toward a specific group that could result in the biased behavior observed in those results discussed in previous sections. We used the train, test, and development sets from two widely known CoNLL- $2 0 0 3 ^ { 4 }$ (Sang and De Meulder 2003) and\n\n4https://www.clips.uantwerpen.be/ conll2003/ner/",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_42",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig38.jpg",
      "image_filename": "1910.10872_page0_fig38.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_43",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig39.jpg",
      "image_filename": "1910.10872_page0_fig39.jpg",
      "caption": "(b) Error Type-3 WeightedVersion 3.9",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_44",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg",
      "image_filename": "1910.10872_page0_fig40.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_45",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig41.jpg",
      "image_filename": "1910.10872_page0_fig41.jpg",
      "caption": "(c) Error Type-1 WeightedVersion 3.8Version 3.9",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_46",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig42.jpg",
      "image_filename": "1910.10872_page0_fig42.jpg",
      "caption": "0.05Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy",
      "context_before": "",
      "context_after": "0.3 Ve0.06OntoNotes- $. 5 ^ { 5 }$ 3.9 0.080.1(Weischedel et al. 2012) datasets which were 0.250.04 0.06used in the training and testing of Flair, Spacy, and many 0.20.02 0.02other models. The split of the OntoNotes-5 dataset into 0.150 0train, development, and test sets was performed according to 0.1 (Pradhan et al. 2013). We reported the percentages of male 0.05vs. female names from the census data that appeared in train, 0test, and development sets in each of the datasets and com-Male CoreNLPpared this to the percentages of male vs. female names in 0.18 Version 3.8Version 3.9reality from the census data to see how much these datasets 0.16are reflective of the reality or if they pertain to any bias to-0.12ward a specific gender group.\n\nOur results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that. Unlike the census data, which is represen-0tative of real-world statistics, wherein female names have more versatility— $62 \\%$ unique names vs. $38 \\%$ unique male names—datasets used in training the NER models contain $42 \\%$ female names vs. $58 \\%$ male names from the census data. Not only do the datasets not contain more versatile female names to reflect the reality, but instead have even less variety which can itself bias the models by not covering enough female names. Similar patterns are observable in test and development sets of datasets used in the NER systems.\n\nWe have seen large amounts of attention and work regarding fairness in machine learning and natural language processing models and methods. Recent papers observed a type of stereotyping bias in word embedding methods and tried to mitigate this type of bias by proposing a method that respects the embeddings for gender-specific words but de-",
      "referring_paragraphs": [
        "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were some cases where version updates had serious fairness-related issues. For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in",
        "Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_47",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg",
      "image_filename": "1910.10872_page0_fig43.jpg",
      "caption": "We have seen large amounts of attention and work regarding fairness in machine learning and natural language processing models and methods.",
      "context_before": "0.3 Ve0.06OntoNotes- $. 5 ^ { 5 }$ 3.9 0.080.1(Weischedel et al. 2012) datasets which were 0.250.04 0.06used in the training and testing of Flair, Spacy, and many 0.20.02 0.02other models. The split of the OntoNotes-5 dataset into 0.150 0train, development, and test sets was performed according to 0.1 (Pradhan et al. 2013). We reported the percentages of male 0.05vs. female names from the census data that appeared in train, 0test, and development sets in each of the datasets and com-Male CoreNLPpared this to the percentages of male vs. female names in 0.18 Version 3.8Version 3.9reality from the census data to see how much these datasets 0.16are reflective of the reality or if they pertain to any bias to-0.12ward a specific gender group.\n\nOur results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that. Unlike the census data, which is represen-0tative of real-world statistics, wherein female names have more versatility— $62 \\%$ unique names vs. $38 \\%$ unique male names—datasets used in training the NER models contain $42 \\%$ female names vs. $58 \\%$ male names from the census data. Not only do the datasets not contain more versatile female names to reflect the reality, but instead have even less variety which can itself bias the models by not covering enough female names. Similar patterns are observable in test and development sets of datasets used in the NER systems.\n\nWe have seen large amounts of attention and work regarding fairness in machine learning and natural language processing models and methods. Recent papers observed a type of stereotyping bias in word embedding methods and tried to mitigate this type of bias by proposing a method that respects the embeddings for gender-specific words but de-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_48",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig44.jpg",
      "image_filename": "1910.10872_page0_fig44.jpg",
      "caption": "0.1Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.",
      "context_before": "",
      "context_after": "biases embeddings for gender-neutral words (Bolukbasi et al. 2016), or by generating a gender-neutral version of Glove (called GN-Glove) that aims to preserve gender information in some directions of word vectors, while setting other dimensions free from gender influence (Zhao et al. 2018b) or other data augmentation techniques (Brunet et al. 2019; Zhao et al. 2019). Other work tried to show and address bias in co-reference resolution (Zhao et al. 2018a), semantic role labeling (Zhao et al. 2017), machine translation (Font and Costa-jussa 2019), language models (Bordia and Bowman `\n\n5https://catalog.ldc.upenn.edu/LDC2013T19",
      "referring_paragraphs": [
        "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were some cases where version updates had serious fairness-related issues. For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in",
        "For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_49",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/39fd4e344830fb06718b6258f62466c70da6e757e6cdf7c818363c02fc003716.jpg",
      "image_filename": "39fd4e344830fb06718b6258f62466c70da6e757e6cdf7c818363c02fc003716.jpg",
      "caption": "Table 3: Some examples on how tagging changed during version update of the CoreNLP model. Note how the original problem of tagging PERSON entities correctly has not been addressed.",
      "context_before": "biases embeddings for gender-neutral words (Bolukbasi et al. 2016), or by generating a gender-neutral version of Glove (called GN-Glove) that aims to preserve gender information in some directions of word vectors, while setting other dimensions free from gender influence (Zhao et al. 2018b) or other data augmentation techniques (Brunet et al. 2019; Zhao et al. 2019). Other work tried to show and address bias in co-reference resolution (Zhao et al. 2018a), semantic role labeling (Zhao et al. 2017), machine translation (Font and Costa-jussa 2019), language models (Bordia and Bowman `\n\n5https://catalog.ldc.upenn.edu/LDC2013T19",
      "context_after": "2019), and sentence embedding (May et al. 2019).\n\nAddressing fairness and bias, not only in NLP but also in general machine learning, has lately gained much attention. In Mehrabi et al. (2019b), the authors created a taxonomy on fairness and bias that discusses how researchers have addressed fairness related issues in different fields. From representation learning (Moyer et al. 2018) to graph embedding (Bose and Hamilton 2019) to community detection (Mehrabi et al. 2019a) and clustering (Backurs et al. 2019), researchers have studied biases in these areas and tried to address them by pointing out the observed problems and proposing new directions and ideas. In Buolamwini and Gebru (2018) authors show and analyze the existing gender bias in facial recognition systems, such as those used by IBM, Microsoft, and ${ \\mathrm { F a c e } } + +$ , and created a benchmark for better evaluation of bias in facial recognition systems. This is considered a significant contribution as it opens many future research questions and related papers. Paying attention to different AI applications and pointing out their issues in terms of fairness is an important issue that needs serious attention for significant future improvements to these systems.\n\nConclusion and Future Work",
      "referring_paragraphs": [
        "2, and 3 (Weighted and Unweighted) cases using template #4 from our benchmark. The results for Spacy show that al-0.05though in not all cases were updated versions worse than 0that of the previous versions, there were some cases where version updates had serious fairness-related issues. For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in",
        "As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
        "2017), machine translation (Font and Costa-jussa 2019), language models (Bordia and Bowman `\n\nTable 3: Some examples on how tagging changed during version update of the CoreNLP model."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_50",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/2d474af8421e5feca67e22af45a5e37e629750afcd18eefdeaaaf767764e41ee.jpg",
      "image_filename": "2d474af8421e5feca67e22af45a5e37e629750afcd18eefdeaaaf767764e41ee.jpg",
      "caption": "Table 4: Percentage of female and male names from the census data appearing in CoNLL 2003 and OntoNotes datasets with their corresponding counts. Both datasets fail to reflect the variety of female names.",
      "context_before": "Addressing fairness and bias, not only in NLP but also in general machine learning, has lately gained much attention. In Mehrabi et al. (2019b), the authors created a taxonomy on fairness and bias that discusses how researchers have addressed fairness related issues in different fields. From representation learning (Moyer et al. 2018) to graph embedding (Bose and Hamilton 2019) to community detection (Mehrabi et al. 2019a) and clustering (Backurs et al. 2019), researchers have studied biases in these areas and tried to address them by pointing out the observed problems and proposing new directions and ideas. In Buolamwini and Gebru (2018) authors show and analyze the existing gender bias in facial recognition systems, such as those used by IBM, Microsoft, and ${ \\mathrm { F a c e } } + +$ , and created a benchmark for better evaluation of bias in facial recognition systems. This is considered a significant contribution as it opens many future research questions and related papers. Paying attention to different AI applications and pointing out their issues in terms of fairness is an important issue that needs serious attention for significant future improvements to these systems.\n\nConclusion and Future Work\n\nIn this work we not only performed a historical analysis of named entity recognition systems and showed the existence of bias, but we also introduced a benchmark that can help future models evaluate the extent of gender bias in their systems. We then performed a cross version analysis of models and showed that model updates can sometimes amplify the existing bias in previous versions. We also analyzed some datasets widely used in current state-of-the-art models and showed the existence of bias in these datasets as well which",
      "context_after": "can directly affect the biased performance of models. Named entity recognition systems are extensively used in different downstream tasks and having biased NER systems can have implications beyond just the NER task. We believe that using our benchmark for evaluation of future named entity recognition systems can help mitigate the gender bias issue in these applications.\n\nThis work identifies an important problem with the current state-of-the-art in named entity recognition. Nevertheless, this measure is a glimpse into the many possible biases that NER may contain, and there are some key limitations that we plan to address in future work. First, the nine templates used to test the models are not necessarily representative of real-world text. There is a limitless supply of sentences that could be fed to the model. Moving forward, we seek to generate a sentence corpora that is based on realworld text. Second, our approach is based upon names taken from United States census data. This work can be extended to different languages to demonstrate the biases they pertain.\n\nThrough our analysis, we provide some suggestions for avoiding gender bias in NER systems, as listed below:",
      "referring_paragraphs": [
        "0.10.04names. We have included results from other templates in 0.080.02our supplementary material to demonstrate that other tem-0.060 1875 1895 1915 1935 1955 1975 1995 2015plates also follow a similar pattern. That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend. This result shows how context helps some models over oth",
        "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that. Unlike the census data, which is represen-0tative of real-world statistics, wherein female names have more versatility— $62 \\%$ unique names vs. $38 \\%$ unique male names—datasets used in training the NER models contain $42 \\%$ female names vs. $58 \\%$ male names from the census data. Not only do the datasets not contain more versatile female names to ",
        "That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.",
        "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years.",
        "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.",
        "We also analyzed some datasets widely used in current state-of-the-art models and showed the existence of bias in these datasets as well which\n\nTable 4: Percentage of female and male names from the census data appearing in CoNLL 2003 and OntoNotes datasets with their corresponding counts."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_51",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig45.jpg",
      "image_filename": "1910.10872_page0_fig45.jpg",
      "caption": "(a) Error Type-1 Weightedfemale male",
      "context_before": "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under\n\nAgreement No. HR0011890019. We would want to thank Kai-Wei Chang and Jieyu Zhao for their constructive feedbacks.\n\nWeighted Results from Template # 50.15",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_52",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig46.jpg",
      "image_filename": "1910.10872_page0_fig46.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_53",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig47.jpg",
      "image_filename": "1910.10872_page0_fig47.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_54",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig48.jpg",
      "image_filename": "1910.10872_page0_fig48.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_55",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig49.jpg",
      "image_filename": "1910.10872_page0_fig49.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_56",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig50.jpg",
      "image_filename": "1910.10872_page0_fig50.jpg",
      "caption": "(b) Error Type-2 Weighted",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_57",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig51.jpg",
      "image_filename": "1910.10872_page0_fig51.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_58",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig52.jpg",
      "image_filename": "1910.10872_page0_fig52.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_59",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig53.jpg",
      "image_filename": "1910.10872_page0_fig53.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_60",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig54.jpg",
      "image_filename": "1910.10872_page0_fig54.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_61",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig55.jpg",
      "image_filename": "1910.10872_page0_fig55.jpg",
      "caption": "(c) Error Type-3 Weightedfemale male",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_62",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig56.jpg",
      "image_filename": "1910.10872_page0_fig56.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_63",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig57.jpg",
      "image_filename": "1910.10872_page0_fig57.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_64",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig58.jpg",
      "image_filename": "1910.10872_page0_fig58.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_65",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg",
      "image_filename": "1910.10872_page0_fig59.jpg",
      "caption": "CoreNLP 0 0Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08 0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.",
      "context_before": "",
      "context_after": "[Section: Appendices]",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_66",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig60.jpg",
      "image_filename": "1910.10872_page0_fig60.jpg",
      "caption": "Appendices",
      "context_before": "[Section: Appendices]",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_67",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig61.jpg",
      "image_filename": "1910.10872_page0_fig61.jpg",
      "caption": "(a) Error Type-1 Unweighted Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_68",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig62.jpg",
      "image_filename": "1910.10872_page0_fig62.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_69",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig63.jpg",
      "image_filename": "1910.10872_page0_fig63.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_70",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig64.jpg",
      "image_filename": "1910.10872_page0_fig64.jpg",
      "caption": "Flair",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_71",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig65.jpg",
      "image_filename": "1910.10872_page0_fig65.jpg",
      "caption": "Small Spacy maleMedium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_72",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig66.jpg",
      "image_filename": "1910.10872_page0_fig66.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_73",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig67.jpg",
      "image_filename": "1910.10872_page0_fig67.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_74",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig68.jpg",
      "image_filename": "1910.10872_page0_fig68.jpg",
      "caption": "Small Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_75",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig69.jpg",
      "image_filename": "1910.10872_page0_fig69.jpg",
      "caption": "Medium Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_76",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig70.jpg",
      "image_filename": "1910.10872_page0_fig70.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_77",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig71.jpg",
      "image_filename": "1910.10872_page0_fig71.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_78",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig72.jpg",
      "image_filename": "1910.10872_page0_fig72.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_79",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig73.jpg",
      "image_filename": "1910.10872_page0_fig73.jpg",
      "caption": "Large Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_80",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig74.jpg",
      "image_filename": "1910.10872_page0_fig74.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_81",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig75.jpg",
      "image_filename": "1910.10872_page0_fig75.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_82",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig76.jpg",
      "image_filename": "1910.10872_page0_fig76.jpg",
      "caption": "maleLarge Spacy",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_83",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig77.jpg",
      "image_filename": "1910.10872_page0_fig77.jpg",
      "caption": "CoreNLP",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_84",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg",
      "image_filename": "1910.10872_page0_fig78.jpg",
      "caption": "0.10 1875 1895 1915 1935 1955 1975 1995 2015 0 1875 1895 1915 1935 1955 1975 1995 2015Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
      "context_before": "",
      "context_after": "[Section: Unweighted Results from Template # 50.1 0.20 1875 1895 1]",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "1910.10872_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "2005.07293": [
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "image_filename": "2005.07293_page0_fig0.jpg",
      "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "context_before": "Fred Morstatter University of Southern California Information Sciences Institute morstatt@usc.edu\n\nMachine learning systems have been shown to propagate the societal errors of the past. In light of this, a wealth of research focuses on designing solutions that are “fair.” Even with this abundance of work, there is no singular definition of fairness, mainly because fairness is subjective and context dependent. We propose a new fairness definition, motivated by the principle of equity, that considers existing biases in the data and attempts to make equitable decisions that account for these previous historical biases. We formalize our definition of fairness, and motivate it with its appropriate contexts. Next, we operationalize it for equitable classification. We perform multiple automatic and human evaluations to show the effectiveness of our definition and demonstrate its utility for aspects of fairness, such as the feedback loop.\n\nWith the omnipresent use of machine learning in different decision and policy making environments, fairness has gained significant importance. This became the case when researchers noticed that an AI system used to measure recidivism risk in bail decisions was biased against certain racial groups [Angwin et al., 2016]. As a reaction to the disclosure of this issue and various others, the AI community has made efforts to mitigate biased and unfair outcomes in decision making processes. Many researchers have proposed definitions of algorithmic fairness, while others have tried to use these definitions in different down-stream tasks in an effort to overcome unfair outcomes. Despite the abundance of fairness definitions, the majority of them are not complete [Gajane and Pechenizkiy, 2018]. Moreover,",
      "context_after": "theoretical analysis of these definitions have found that many at the forefront are incompatible with each other [Kleinberg, Mullainathan, and Raghavan, 2016]. For now at least, fairness remains a philosophical question that is not yet answered in the computational domain. In light of that, we propose and mathematically formalize the equity notion of fairness in which resources and outcomes are distributed to overcome obstacles experienced by groups in order to maximize their opportunities [Schement, 2001]. In this work we take the perspective that historical biases should be compensated and disadvantaged groups should be leveraged. We then introduce a data-driven classification objective function that operationalizes the notion of equity in which existing historical biases in the training data are compensated through predictions on the test data. This approach will not only target fixing biases but it will also target minimizing the feedback loop phenomenon in which the biased data contaminates the decision making outcome, and it continues to stay and grow through the system.\n\nOur definition of fairness is an augmented version of statistical parity [Dwork et al., 2012] that we adapt to mea-\n\narXiv:2005.07293v1 [cs.LG] 14 May 2020",
      "referring_paragraphs": [
        "Two different fairness realizations are depicted in Figure 1. On the left side there is the notion of equality in which every group is given an equal amount of resources, which is too much for some members and insufficient for others. This is the problem that motivates this work: how can a classifier produce predictions that are good for the majority of a group or society? This leads us to the right picture which depicts equity where leverage is given through the model to give the groups appropr",
        "Note that there is a difference between the notation of historical and predictive (future) outcomes. By equalizing the sum of historical plus future outcomes of one group to another, we are enforcing affirmative action and try to compensate for observed historical biases in the data by correcting and adjusting the predictive outcome so that eventually all the groups reach an equilibrium in our objective function. This equilibrium can be in terms of all the groups satisfying their goals, e.g. tha",
        "From results shown in Figure 2, we can observe that classifier trained on our Equity loss is able to achieve higher fairness gain for all $\\beta$ values. We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid",
        "As shown in Figure 2, we can observe that for all $\\beta$ values our definition was able to achieve higher fairness gain. We also show the significance of these results in Table 1. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness. Especially for mid $\\beta$ values in whic",
        "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
        "that in Figure 1.",
        "We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values.",
        "We also show the significance of these results in Table 1.",
        "Through MannWhitney U significance\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"2\">COMPAS Dataset</td><td colspan=\"2\">Adult Dataset</td></tr><tr><td colspan=\"2\"></td><td colspan=\"2\">p-value</td><td colspan=\"2\">p-value</td></tr><tr><td>Beta</td><td></td><td>Parity</td><td>Classifier</td><td>Parity</td><td>Classifier</td></tr><tr><td>0.1</td><td rowspan=\"9\">Equity</td><td>0.0003</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.2</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.3</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.4</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.5</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.6</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.7</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.8</td><td>0.0001</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.9</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr></table>\n\nTable 1: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets.",
        "• Scenario 1 (Equality vs Equity): We asked workers to rate pictures of equity and equality in Figure 1 and chose their preferred picture."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_2",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig1.jpg",
      "image_filename": "2005.07293_page0_fig1.jpg",
      "caption": "1https://github.com/Ninarehm/Fairness",
      "context_before": "$$ \\min _ {\\theta} L (\\theta). \\tag {3} $$\n\nIn our results, Equity corresponds to a classifier using the equity loss function defined in Equation 1, Parity using the parity loss function defined in Equation 2, and\n\n1https://github.com/Ninarehm/Fairness",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg",
      "image_filename": "2005.07293_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig3.jpg",
      "image_filename": "2005.07293_page0_fig3.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig4.jpg",
      "image_filename": "2005.07293_page0_fig4.jpg",
      "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.",
      "context_before": "",
      "context_after": "Classifier using the cross-entropy loss only in Equation 3. We tested these classifiers on two benchmark datasets in fairness, COMPAS and Adult datasets, and reported the performance accuracy and fairness gain as defined below.\n\nDefinition 2 (Fairness Gain) For a given loss function $\\ell \\in \\ \\{ E q u i t y , P a r i t y , C l a s s i f i e r \\}$ , we define the fairness gain relative to a simple classifier with no fairness constraint for demographic groups a and $b$ on the $D \\cup M$ set as:\n\nFairness $G a i n = [ | p ( Y | A = a ) - p ( Y | A = b ) | ] _ { c l a s s i f i e l }$",
      "referring_paragraphs": [
        "The COMPAS dataset contains information about defendants from Broward County. The labels in our prediction classification task were weather or not a criminal will re-offend within two years. The sensitive attribute in our experiments was gender. Among features in this dataset we used features as listed in Table 2. We split the dataset into 10 different random 80-10-10 splits for train, test, and validation sets. The averaged accuracy and fairness gain results obtained from applying different los",
        "classification task over 10 experiments on different splits with different $\\beta$ values on the COMPAS dataset is shown in Figure 2.",
        "From results shown in Figure 2, we can observe that classifier trained on our Equity loss is able to achieve higher fairness gain for all $\\beta$ values. We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid",
        "The Adult dataset contains information about individuals with a label corresponding if an individual’s income exceeds 50k per year or not. We utilized all the features from the dataset in our classification task for predicting the label. We considered gender as the protected attribute in our classification loss. The data was split into 10 different random 80-10-10 splits for train, test, and validation sets for each set of experiments. The averaged test accuracy and fairness gain results over 10",
        "As shown in Figure 2, we can observe that for all $\\beta$ values our definition was able to achieve higher fairness gain. We also show the significance of these results in Table 1. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness. Especially for mid $\\beta$ values in whic",
        "test we show that this gain is significant for all the $\\beta$ values for both of the datasets. With regards to degrade in test accuracy, as expected, larger $\\beta$ values resulted in more loss in test accuracy, while more gain in fairness. However, this loss was shown to be non-significant for one of our datasets, the COMPAS dataset, for low to mid $\\beta$ values which we recommend using. For the Adult dataset, although the loss was shown to be statistically significant, the test accuracy loss",
        "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values.",
        "The averaged accuracy and fairness gain results obtained from applying different losses in our\n\nclassification task over 10 experiments on different splits with different $\\beta$ values on the COMPAS dataset is shown in Figure 2.",
        "The averaged test accuracy and fairness gain results over 10 different splits for each $\\beta$ value obtained from applying different losses in our classification task on the Adult dataset is shown in Figure 2.",
        "Figure 2, demonstrates the behavior of different losses over different $\\beta$ values in terms of test accuracy and fairness gain for the COMPAS and Adult datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/4d5947ae89148fbb4a74866a301c38724aec0f806837795d1cf3125a6b1d6279.jpg",
      "image_filename": "4d5947ae89148fbb4a74866a301c38724aec0f806837795d1cf3125a6b1d6279.jpg",
      "caption": "As expected in our initial hypothesis, through experimentation and hypothesis testing, we were able to gain knowledge that using the Equity loss in classification will result in gain in fairness.",
      "context_before": "As shown in Figure 2, we can observe that for all $\\beta$ values our definition was able to achieve higher fairness gain. We also show the significance of these results in Table 1. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness. Especially for mid $\\beta$ values in which the degrade can be perceived negligible when considering the gain in fairness. As with the COMPAS dataset, we recommend a $\\beta$ value around 0.3-0.5 which balances the fairness gain and test accuracy for this dataset as well.\n\n3.4 Overall Results Discussion\n\nAs expected in our initial hypothesis, through experimentation and hypothesis testing, we were able to gain knowledge that using the Equity loss in classification will result in gain in fairness. Through MannWhitney U significance",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_7",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/5bca1eeb81ee4d3cf6bc71a795a6e5468579757886ed0a934f2427d076b5b651.jpg",
      "image_filename": "5bca1eeb81ee4d3cf6bc71a795a6e5468579757886ed0a934f2427d076b5b651.jpg",
      "caption": "Table 1: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets. The results show the statistical significance of experiments performed for evaluation of fairness gain amongst different losses over different $\\beta$ values. The assumed test hypothesis was whether Equity will have greater fairness gain compared to Parity and Classifier losses. Table 2: Features used in the experiments from the COM-PAS dataset.",
      "context_before": "",
      "context_after": "test we show that this gain is significant for all the $\\beta$ values for both of the datasets. With regards to degrade in test accuracy, as expected, larger $\\beta$ values resulted in more loss in test accuracy, while more gain in fairness. However, this loss was shown to be non-significant for one of our datasets, the COMPAS dataset, for low to mid $\\beta$ values which we recommend using. For the Adult dataset, although the loss was shown to be statistically significant, the test accuracy loss was reasonable considering the price of fairness we get through the gain in fairness. Figure 2, demonstrates the behavior of different losses over different $\\beta$ values in terms of test accuracy and fairness gain for the COMPAS and Adult datasets. Tables 1 and 3 indicate the significance of our hypothesis in terms of Equity loss being able to gain highest gain in fairness and also the significance of its degrade in performance in terms of test accuracy over other baselines for the COMPAS and Adult datasets respectively. From the overall results, we suggest use of $\\beta$ values between 0.3-0.5 when using our Equity objective as they are shown to be the most effective in terms of gain in fairness and maintaining a reasonable test accuracy.",
      "referring_paragraphs": [
        "Two different fairness realizations are depicted in Figure 1. On the left side there is the notion of equality in which every group is given an equal amount of resources, which is too much for some members and insufficient for others. This is the problem that motivates this work: how can a classifier produce predictions that are good for the majority of a group or society? This leads us to the right picture which depicts equity where leverage is given through the model to give the groups appropr",
        "Note that there is a difference between the notation of historical and predictive (future) outcomes. By equalizing the sum of historical plus future outcomes of one group to another, we are enforcing affirmative action and try to compensate for observed historical biases in the data by correcting and adjusting the predictive outcome so that eventually all the groups reach an equilibrium in our objective function. This equilibrium can be in terms of all the groups satisfying their goals, e.g. tha",
        "From results shown in Figure 2, we can observe that classifier trained on our Equity loss is able to achieve higher fairness gain for all $\\beta$ values. We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid",
        "As shown in Figure 2, we can observe that for all $\\beta$ values our definition was able to achieve higher fairness gain. We also show the significance of these results in Table 1. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness. Especially for mid $\\beta$ values in whic",
        "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
        "that in Figure 1.",
        "We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values.",
        "We also show the significance of these results in Table 1.",
        "Through MannWhitney U significance\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"2\">COMPAS Dataset</td><td colspan=\"2\">Adult Dataset</td></tr><tr><td colspan=\"2\"></td><td colspan=\"2\">p-value</td><td colspan=\"2\">p-value</td></tr><tr><td>Beta</td><td></td><td>Parity</td><td>Classifier</td><td>Parity</td><td>Classifier</td></tr><tr><td>0.1</td><td rowspan=\"9\">Equity</td><td>0.0003</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.2</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.3</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.4</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.5</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.6</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.7</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.8</td><td>0.0001</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.9</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr></table>\n\nTable 1: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets.",
        "• Scenario 1 (Equality vs Equity): We asked workers to rate pictures of equity and equality in Figure 1 and chose their preferred picture."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/e39ebfc88e4a25a35fc3ed6beb3e0461d6f3402ecc9793b01955c1719673bdce.jpg",
      "image_filename": "e39ebfc88e4a25a35fc3ed6beb3e0461d6f3402ecc9793b01955c1719673bdce.jpg",
      "caption": "Table 3: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets. The results test the statistical significance of experiments performed for evaluation of test accuracy amongst different losses over different $\\beta$ values. The test reports the significance of degrade in performance of Equity loss over the other two losses in terms of test accuracy.",
      "context_before": "test we show that this gain is significant for all the $\\beta$ values for both of the datasets. With regards to degrade in test accuracy, as expected, larger $\\beta$ values resulted in more loss in test accuracy, while more gain in fairness. However, this loss was shown to be non-significant for one of our datasets, the COMPAS dataset, for low to mid $\\beta$ values which we recommend using. For the Adult dataset, although the loss was shown to be statistically significant, the test accuracy loss was reasonable considering the price of fairness we get through the gain in fairness. Figure 2, demonstrates the behavior of different losses over different $\\beta$ values in terms of test accuracy and fairness gain for the COMPAS and Adult datasets. Tables 1 and 3 indicate the significance of our hypothesis in terms of Equity loss being able to gain highest gain in fairness and also the significance of its degrade in performance in terms of test accuracy over other baselines for the COMPAS and Adult datasets respectively. From the overall results, we suggest use of $\\beta$ values between 0.3-0.5 when using our Equity objective as they are shown to be the most effective in terms of gain in fairness and maintaining a reasonable test accuracy.",
      "context_after": "4 Effect of Equity on Feedback Loop\n\nAn important and major concern in the fairness community is the feedback loop phenomenon [Chouldechova and Roth, 2018]. Since biased data is generated by humans, these biases are perpetuated after the models make biased decisions based on the historical biased data. The bias originates from humans, the models amplify these biases, and they loop back biased results back to the humans. This loop gets repeated and continues to carry the initial existing biases. This phenomenon is called the feedback loop phenomenon.\n\nWe hope that since our notion considers and compensates the historical biases in the training set, which might have come from humans in initial phases, and attempts to fix them by achieving an ultimate equilibrium considering the past and future decisions, it may help with the mitigation of the feedback loop phenomenon.",
      "referring_paragraphs": [
        "From results shown in Figure 2, we can observe that classifier trained on our Equity loss is able to achieve higher fairness gain for all $\\beta$ values. We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid",
        "As shown in Figure 2, we can observe that for all $\\beta$ values our definition was able to achieve higher fairness gain. We also show the significance of these results in Table 1. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness. Especially for mid $\\beta$ values in whic",
        "Figure 3 reports $| p ( Y | A = { \\mathrm { f e m a l e } } ) - p ( Y | A = { \\mathrm { m a l e } } )$ |, averaged across 10 runs, as a measure for disparity for both predicted class labels $Y ~ = ~ 0$ and $Y ~ = ~ 1$ in each of the datasets for each of the fairness notions for each $\\beta$ value. These results demonstrate that our notion of fairness was able to minimize the gap between $p ( Y | A \\ =$ female) and $p ( Y | A \\ = \\ { \\mathrm { m a l e } } )$ in all of the datasets. The results sh",
        "equality, equity, and fairness in long run and mitigate the negative effects of the feedback loop phenomenon. As expected and shown in Figure 3, higher $\\beta$ values resulted in achieving more fair outcomes which resulted in reduction of bias. In addition, we reported the MannWhitney U test results to show the significance of our results. Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 T",
        "Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid $\\beta$ values in this dataset.",
        "Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness.",
        "Table 3: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets.",
        "The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.5.",
        "Figure 3 reports $| p ( Y | A = { \\mathrm { f e m a l e } } ) - p ( Y | A = { \\mathrm { m a l e } } )$ |, averaged across 10 runs, as a measure for disparity for both predicted class labels $Y ~ = ~ 0$ and $Y ~ = ~ 1$ in each of the datasets for each of the fairness notions for each $\\beta$ value.",
        "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.",
        "The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/328d67e868f4fc560580db7a11ffd090f810a9b28b2225d0705eb9ff00788a88.jpg",
      "image_filename": "328d67e868f4fc560580db7a11ffd090f810a9b28b2225d0705eb9ff00788a88.jpg",
      "caption": "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets. The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.5.",
      "context_before": "An important and major concern in the fairness community is the feedback loop phenomenon [Chouldechova and Roth, 2018]. Since biased data is generated by humans, these biases are perpetuated after the models make biased decisions based on the historical biased data. The bias originates from humans, the models amplify these biases, and they loop back biased results back to the humans. This loop gets repeated and continues to carry the initial existing biases. This phenomenon is called the feedback loop phenomenon.\n\nWe hope that since our notion considers and compensates the historical biases in the training set, which might have come from humans in initial phases, and attempts to fix them by achieving an ultimate equilibrium considering the past and future decisions, it may help with the mitigation of the feedback loop phenomenon.\n\nIn order to observe the effect of our new equity notion on fixing the historical biases in the training sets and effectively fixing the feedback loop as a consequence, we conducted experiments on datasets used in the previous section and recorded averaged results over the 10 experiments on random splits along with their MannWhitney U significance tests in this section with the following hypothesis. The model architecture remains the same as experiments conducted in the previous section. In addition the experiments are performed on the Equity, Parity, and Classification losses for comparison purposes.",
      "context_after": "Hypothesis 2 The Equity classification objective can be the most effective in terms of reducing the disparities (bias) defined as $| p ( Y | A = a ) -$ $p ( Y | A = b ) |$ between demographic groups a and b over some iterations when predictive outcomes on the test sets are accumulated over time on the historical train sets.\n\n4.1 Experimental Design and Results\n\nHerein, we answer the question of what will happen if the equity classifier is allowed to play out in a realistic environment. We simulate the feedback loop as an iterative training-predicting cycle. We train our model in sequential chunks, splitting the test data into 10 equalsized chunks. At the first iteration, we train the model using the train data. At each subsequent iteration, we take one of the chunks from our test data adding it to the previous train data alongside its predicted labels and retrain the model for the next iteration. We then deleted this chunk from the test set and keep it in the train set. Each experiment was repeated 10 times with different random splits.",
      "referring_paragraphs": [
        "equality, equity, and fairness in long run and mitigate the negative effects of the feedback loop phenomenon. As expected and shown in Figure 3, higher $\\beta$ values resulted in achieving more fair outcomes which resulted in reduction of bias. In addition, we reported the MannWhitney U test results to show the significance of our results. Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 T",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4. In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of the scenarios. Similar to findings in [Saxena et al., 2019], we also observed the support for the principle of affirmative action in our experiments which relates to our notion. From the results it is evident that strong preference is given to our notion introduced in this paper f",
        "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.",
        "Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 This is consistent with our earlier finding that $\\beta = 0 .",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4.",
        "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_10",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig5.jpg",
      "image_filename": "2005.07293_page0_fig5.jpg",
      "caption": "(a) β = 0.1",
      "context_before": "4.1 Experimental Design and Results\n\nHerein, we answer the question of what will happen if the equity classifier is allowed to play out in a realistic environment. We simulate the feedback loop as an iterative training-predicting cycle. We train our model in sequential chunks, splitting the test data into 10 equalsized chunks. At the first iteration, we train the model using the train data. At each subsequent iteration, we take one of the chunks from our test data adding it to the previous train data alongside its predicted labels and retrain the model for the next iteration. We then deleted this chunk from the test set and keep it in the train set. Each experiment was repeated 10 times with different random splits.\n\nFigure 3 reports $| p ( Y | A = { \\mathrm { f e m a l e } } ) - p ( Y | A = { \\mathrm { m a l e } } )$ |, averaged across 10 runs, as a measure for disparity for both predicted class labels $Y ~ = ~ 0$ and $Y ~ = ~ 1$ in each of the datasets for each of the fairness notions for each $\\beta$ value. These results demonstrate that our notion of fairness was able to minimize the gap between $p ( Y | A \\ =$ female) and $p ( Y | A \\ = \\ { \\mathrm { m a l e } } )$ in all of the datasets. The results show that using our notion can bring",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig6.jpg",
      "image_filename": "2005.07293_page0_fig6.jpg",
      "caption": "(b) β = 0.5",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig7.jpg",
      "image_filename": "2005.07293_page0_fig7.jpg",
      "caption": "(c) β = 0.9",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg",
      "image_filename": "2005.07293_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig9.jpg",
      "image_filename": "2005.07293_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_15",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig10.jpg",
      "image_filename": "2005.07293_page0_fig10.jpg",
      "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.",
      "context_before": "",
      "context_after": "equality, equity, and fairness in long run and mitigate the negative effects of the feedback loop phenomenon. As expected and shown in Figure 3, higher $\\beta$ values resulted in achieving more fair outcomes which resulted in reduction of bias. In addition, we reported the MannWhitney U test results to show the significance of our results. Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 This is consistent with our earlier finding that $\\beta = 0 . 5$ is the most effective and reasonable with significant impact in gaining fairness, reducing bias, and balancing the fairness-accuracy trade-off.\n\n5 Public Perception of Equity\n\nIn order to understand the public’s perception of equity (via our proposed definition) and its comparison to equality in different real life scenarios, we conducted surveys on Amazon Mechanical Turk in the vein of [Saxena et al., 2019].",
      "referring_paragraphs": [
        "From results shown in Figure 2, we can observe that classifier trained on our Equity loss is able to achieve higher fairness gain for all $\\beta$ values. We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid",
        "As shown in Figure 2, we can observe that for all $\\beta$ values our definition was able to achieve higher fairness gain. We also show the significance of these results in Table 1. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness. Especially for mid $\\beta$ values in whic",
        "Figure 3 reports $| p ( Y | A = { \\mathrm { f e m a l e } } ) - p ( Y | A = { \\mathrm { m a l e } } )$ |, averaged across 10 runs, as a measure for disparity for both predicted class labels $Y ~ = ~ 0$ and $Y ~ = ~ 1$ in each of the datasets for each of the fairness notions for each $\\beta$ value. These results demonstrate that our notion of fairness was able to minimize the gap between $p ( Y | A \\ =$ female) and $p ( Y | A \\ = \\ { \\mathrm { m a l e } } )$ in all of the datasets. The results sh",
        "equality, equity, and fairness in long run and mitigate the negative effects of the feedback loop phenomenon. As expected and shown in Figure 3, higher $\\beta$ values resulted in achieving more fair outcomes which resulted in reduction of bias. In addition, we reported the MannWhitney U test results to show the significance of our results. Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 T",
        "Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid $\\beta$ values in this dataset.",
        "Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness.",
        "Table 3: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets.",
        "The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.5.",
        "Figure 3 reports $| p ( Y | A = { \\mathrm { f e m a l e } } ) - p ( Y | A = { \\mathrm { m a l e } } )$ |, averaged across 10 runs, as a measure for disparity for both predicted class labels $Y ~ = ~ 0$ and $Y ~ = ~ 1$ in each of the datasets for each of the fairness notions for each $\\beta$ value.",
        "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.",
        "The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_16",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/0a67ab02538002abba9d0a51e025d28cd1928b8c4ca2d01c300ef31b2c8b8acc.jpg",
      "image_filename": "0a67ab02538002abba9d0a51e025d28cd1928b8c4ca2d01c300ef31b2c8b8acc.jpg",
      "caption": "Table 5: Number of people preferring solutions provided by the equity vs solutions provided by the parity notions of fairness in different scenarios.",
      "context_before": "equality/parity. For each scenario, we asked workers to rate how fair they think each solution is on a scale of zero to four. At the end of each scenario, we asked workers to select their preferred fairness solution for each scenario. We asked workers to provide written justification for their responses. In addition, we had a “sanity check” question at the end of our survey to discover and remove workers behaving randomly. The screenshot from our questionnaire is included in the Appendixes section for more detailed information.\n\nA summary of the scenarios are as follows. Note that the experimental results follow the same numbering convention as listed below.\n\n2Results for other $\\beta$ values can be found in the supplementary material.",
      "context_after": "uity). The other proposes to equally distribute houses across different racial categories (parity).\n\n• Scenario 4 (College Admission): We asked respondents to rate college admission systems—one based on equity considering if the student is a first generation college student (equity). The other equally admits students from first generation and non-first generation backgrounds (parity).\n\nAfter gathering and analyzing responses from mechanical turk workers, we observed that there are some cases in which our notion of fairness is strongly preferred by a large margin, and some other cases where preference is given to the parity notion. Fairness is subjective and different people may have different takes on what would be a fair solution to a particular case. That is the main reason why we introduce this notion as not only in some scenarios our definition will be over-preferred but also in some non-preferred scenarios it will get some preference from certain groups of people.",
      "referring_paragraphs": [
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4. In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of the scenarios. Similar to findings in [Saxena et al., 2019], we also observed the support for the principle of affirmative action in our experiments which relates to our notion. From the results it is evident that strong preference is given to our notion introduced in this paper f",
        "• Scenario 3 (Government Subsidized Housing): We asked respondents to rate the government subsidized housing distribution systems proposed in the survey— one based on equity considering how houses were historically distributed across different races (eq-\n\nTable 5: Number of people preferring solutions provided by the equity vs solutions provided by the parity notions of fairness in different scenarios.",
        "In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of the scenarios."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_17",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig11.jpg",
      "image_filename": "2005.07293_page0_fig11.jpg",
      "caption": "The statistics of ratings for each of the 4 scenarios is shown in Figure 4. In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of t",
      "context_before": "• Scenario 4 (College Admission): We asked respondents to rate college admission systems—one based on equity considering if the student is a first generation college student (equity). The other equally admits students from first generation and non-first generation backgrounds (parity).\n\nAfter gathering and analyzing responses from mechanical turk workers, we observed that there are some cases in which our notion of fairness is strongly preferred by a large margin, and some other cases where preference is given to the parity notion. Fairness is subjective and different people may have different takes on what would be a fair solution to a particular case. That is the main reason why we introduce this notion as not only in some scenarios our definition will be over-preferred but also in some non-preferred scenarios it will get some preference from certain groups of people.\n\nThe statistics of ratings for each of the 4 scenarios is shown in Figure 4. In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of the scenarios. Similar to findings in [Saxena et al., 2019], we also observed the support for the principle of affirmative action in our experiments which relates to our notion. From the results it is evident that strong preference is given to our notion introduced in this paper for scenarios 1 and 2, and despite the fact that scenarios 3 and 4 are not over-preferred for our notion, there are still considerable number of people who gave preference to our notion in these scenarios. All the justifications written down by the respondents were analyzed. For each preference recorded in this paper, respondents gave justifications that cover a wide range of perspectives. The dataset can be found in 1.",
      "context_after": "",
      "referring_paragraphs": [
        "equality, equity, and fairness in long run and mitigate the negative effects of the feedback loop phenomenon. As expected and shown in Figure 3, higher $\\beta$ values resulted in achieving more fair outcomes which resulted in reduction of bias. In addition, we reported the MannWhitney U test results to show the significance of our results. Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 T",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4. In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of the scenarios. Similar to findings in [Saxena et al., 2019], we also observed the support for the principle of affirmative action in our experiments which relates to our notion. From the results it is evident that strong preference is given to our notion introduced in this paper f",
        "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.",
        "Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 This is consistent with our earlier finding that $\\beta = 0 .",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4.",
        "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg",
      "image_filename": "2005.07293_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_19",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig13.jpg",
      "image_filename": "2005.07293_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_20",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig14.jpg",
      "image_filename": "2005.07293_page0_fig14.jpg",
      "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.",
      "context_before": "",
      "context_after": "With relatively recent popularity of fairness in machine learning and natural language processing domains, the need to find a universal and a more complete fairness definition and measure is crucial. Although finding such definition and measure is a challenge not only in machine learning but also in social and political sciences, steps need to be taken to make current definitions evolve and cover more real world cases. In light of this many fairness definitions have been proposed. Some tried to complement others and some starting a new direction and view-point on their own. Different body of work tried to incorporate the proposed definitions in different downstream tasks such as classification and regression [Menon and Williamson, 2018, Berk et al., 2017, Krasanakis et al., 2018, Agarwal, Dudik, and Wu, 2019, Goel, Yaghini, and Faltings, 2018].\n\n6.1 Fairness Definitions\n\nFor a more complete list of existing fairness definitions there exists papers that survey [Mehrabi et al., 2019] and explain [Verma and Rubin, 2018] proposed definitions. Here we will elaborate some important and widely known definitions related to our work introduced in this paper.",
      "referring_paragraphs": [
        "equality, equity, and fairness in long run and mitigate the negative effects of the feedback loop phenomenon. As expected and shown in Figure 3, higher $\\beta$ values resulted in achieving more fair outcomes which resulted in reduction of bias. In addition, we reported the MannWhitney U test results to show the significance of our results. Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 T",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4. In addition, Table 5 depicts the number of mechanical turk workers who preferred a certain solution following a fairness definition in each of the scenarios. Similar to findings in [Saxena et al., 2019], we also observed the support for the principle of affirmative action in our experiments which relates to our notion. From the results it is evident that strong preference is given to our notion introduced in this paper f",
        "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.",
        "Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 This is consistent with our earlier finding that $\\beta = 0 .",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4.",
        "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_21",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/c94ac656b59b21e7a5c59b972efb866b9ec0edeaf1b03f51772ac58d0c01d2bf.jpg",
      "image_filename": "c94ac656b59b21e7a5c59b972efb866b9ec0edeaf1b03f51772ac58d0c01d2bf.jpg",
      "caption": "Table 6: Architecture of model used in our experiments.",
      "context_before": "to a wide audience, and formalized it for classification. We tested this approach in a traditional cross validation setup, and demonstrated how it can be used in a real-world environment, such as unfairness that can arise from the feedback loop. Our results show the effectiveness of our method in mitigating bias and achieving fairness. We also performed human evaluation to evaluate our notion in different scenarios with the equality/parity notion of fairness. As a future direction, our definition can be utilized to achieve and study the effects of equity in classification with different techniques. In this work, we provide a framework for equity to be formalized; however, there is still work to be done in the area of fairness with regards to equity. Future work is to further study how the equity notion interacts with other existing definitions of fairness, such as equality of opportunity, equalized odds or other definitions in the equality domain other than statistical parity. It can also be extended to other machine learning tasks such as regression.\n\nWe wanted to thank Hrayr Harutyunyan and Mozhdeh Gheini for their help and comments.\n\nIn this section we are going to report some additional and detailed numbers reported in the main paper, such as detailed averaged values shown in Figures 2 and 3 for the 10 conduced experiments on different splits of data along with the corresponding standard deviations in parenthesis. As also mentioned in the main text, due to the existing variance in different random splits of the dataset, we found reporting the $p$ -values with Mann-Whitney U test more suitable; however, here we also report detailed standard deviations for the sake of completeness. We would also include details of our model architecture and also the mechanical turk survey conducted and discussed in the main paper in this section.",
      "context_after": "",
      "referring_paragraphs": [
        "Table 6: Architecture of model used in our experiments."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_22",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/96ffb84cc8d9cb51110f181f111d228559c65b258f54c3539985b0d76c79ba8f.jpg",
      "image_filename": "96ffb84cc8d9cb51110f181f111d228559c65b258f54c3539985b0d76c79ba8f.jpg",
      "caption": "Table 7: Averaged percent accuracy and fairness gain for the Adult dataset along with the standard deviation numbers reported in parenthesis for different $\\beta$ values.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td>Layer Type</td><td>Parameters</td></tr><tr><td>dense</td><td>256 hidden dimension, tanh activation</td></tr><tr><td>dense</td><td>2 output dimension</td></tr></table>\n\nTable 7: Averaged percent accuracy and fairness gain for the Adult dataset along with the standard deviation numbers reported in parenthesis for different $\\beta$ values."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig15.jpg",
      "image_filename": "2005.07293_page0_fig15.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_24",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/e81acadfcbda34d8fc29b241fb57d44018dcedd869c5e7915979e3faea100b90.jpg",
      "image_filename": "e81acadfcbda34d8fc29b241fb57d44018dcedd869c5e7915979e3faea100b90.jpg",
      "caption": "Table 8: Averaged percent accuracy and fairness gain for the COMPAS dataset along with the standard deviation numbers reported in parenthesis for different $\\beta$ values. Table 9: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets. The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td></td><td>Beta</td><td>Equity</td><td>Parity</td><td>Classifier</td></tr><tr><td rowspan=\"10\">Accuracy</td><td>0.0</td><td>84.76%(0.41)</td><td>84.76%(0.41)</td><td>84.76%(0.41)</td></tr><tr><td>0.1</td><td>84.68%(0.42)</td><td>84.83%(0.46)</td><td>NA</td></tr><tr><td>0.2</td><td>84.29%(0.51)</td><td>84.89%(0.45)</td><td>NA</td></tr><tr><td>0.3</td><td>83.51%(0.45)</td><td>84.73%(0.50)</td><td>NA</td></tr><tr><td>0.4</td><td>82.86%(0.45)</td><td>84.55%(0.51)</td><td>NA</td></tr><tr><td>0.5</td><td>82.00%(0.49)</td><td>84.36%(0.57)</td><td>NA</td></tr><tr><td>0.6</td><td>81.48%(0.43)</td><td>84.14%(0.49)</td><td>NA</td></tr><tr><td>0.7</td><td>80.81%(0.47)</td><td>83.97%(0.57)</td><td>NA</td></tr><tr><td>0.8</td><td>80.45%(0.63)</td><td>83.74%(0.44)</td><td>NA</td></tr><tr><td>0.9</td><td>79.38%(1.12)</td><td>83.71%(0.54)</td><td>NA</td></tr><tr><td rowspan=\"10\">Fairness Gain</td><td>0.0</td><td>0.00%(0.00)</td><td>0.00%(0.00)</td><td>0.00%(0.00)</td></tr><tr><td>0.1</td><td>0.61%(0.06)</td><td>0.30%(0.04)</td><td>NA</td></tr><tr><td>0.2</td><td>1.24%(0.10)</td><td>0.58%(0.06)</td><td>NA</td></tr><tr><td>0.3</td><td>1.80%(0.16)</td><td>0.84%(0.07)</td><td>NA</td></tr><tr><td>0.4</td><td>2.25%(0.16)</td><td>1.08%(0.09)</td><td>NA</td></tr><tr><td>0.5</td><td>2.61%(0.17)</td><td>1.30%(0.13)</td><td>NA</td></tr><tr><td>0.6</td><td>2.83%(0.21)</td><td>1.42%(0.17)</td><td>NA</td></tr><tr><td>0.7</td><td>3.12%(0.24)</td><td>1.58%(0.11)</td><td>NA</td></tr><tr><td>0.8</td><td>3.33%(0.28)</td><td>1.67%(0.13)</td><td>NA</td></tr><tr><td>0.9</td><td>3.77%(0.42)</td><td>1.86%(0.12)</td><td>NA</td></tr><tr><td rowspan=\"10\">Accuracy</td><td>0.0</td><td>66.80%(2.19)</td><td>66.80%(2.19)</td><td>66.80%(2.19)</td></tr><tr><td>0.1</td><td>66.60%(1.59)</td><td>67.04%(1.90)</td><td>NA</td></tr><tr><td>0.2</td><td>66.23%(1.78)</td><td>66.47%(2.08)</td><td>NA</td></tr><tr><td>0.3</td><td>65.66%(2.04)</td><td>66.48%(1.87)</td><td>NA</td></tr><tr><td>0.4</td><td>65.39%(1.82)</td><td>66.30%(1.74)</td><td>NA</td></tr><tr><td>0.5</td><td>65.17%(1.84)</td><td>66.41%(1.22)</td><td>NA</td></tr><tr><td>0.6</td><td>64.61%(2.12)</td><td>66.51%(1.43)</td><td>NA</td></tr><tr><td>0.7</td><td>64.25%(2.37)</td><td>66.51%(1.49)</td><td>NA</td></tr><tr><td>0.8</td><td>64.10%(2.08)</td><td>66.61%(1.49)</td><td>NA</td></tr><tr><td>0.9</td><td>64.06%(1.62)</td><td>66.39%(1.17)</td><td>NA</td></tr><tr><td rowspan=\"10\">Fairness Gain</td><td>0.0</td><td>0.0%(0.00)</td><td>0.0%(0.00)</td><td>0.0%(0.00)</td></tr><tr><td>0.1</td><td>1.80%(0.52)</td><td>0.77%(0.32)</td><td>NA</td></tr><tr><td>0.2</td><td>3.26%(0.57)</td><td>1.38%(0.53)</td><td>NA</td></tr><tr><td>0.3</td><td>4.07%(0.61)</td><td>1.72%(0.44)</td><td>NA</td></tr><tr><td>0.4</td><td>4.48%(0.48)</td><td>1.94%(0.43)</td><td>NA</td></tr><tr><td>0.5</td><td>4.81%(0.65)</td><td>2.26%(0.59)</td><td>NA</td></tr><tr><td>0.6</td><td>5.11%(0.72)</td><td>2.53%(0.50)</td><td>NA</td></tr><tr><td>0.7</td><td>5.38%(0.80)</td><td>2.07%(0.38)</td><td>NA</td></tr><tr><td>0.8</td><td>5.37%(1.06)</td><td>2.78%(0.70)</td><td>NA</td></tr><tr><td>0.9</td><td>6.05%(1.40)</td><td>2.91%(0.36)</td><td>NA</td></tr></table>\n\nTable 8: Averaged percent accuracy and fairness gain for the COMPAS dataset along with the standard deviation numbers reported in parenthesis for different $\\beta$ values."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_25",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/892c1426cfcb6798569783f05135747b5f5584714af2e5d3703f4d8e13bbfe28.jpg",
      "image_filename": "892c1426cfcb6798569783f05135747b5f5584714af2e5d3703f4d8e13bbfe28.jpg",
      "caption": "Table 10: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets. The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.9.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td colspan=\"2\"></td><td colspan=\"2\">COMPAS Dataset</td><td colspan=\"2\">Adult Dataset</td></tr><tr><td colspan=\"2\"></td><td colspan=\"2\">p-value</td><td colspan=\"2\">p-value</td></tr><tr><td>Iter</td><td></td><td>Parity</td><td>Classifier</td><td>Parity</td><td>Classifier</td></tr><tr><td>1</td><td rowspan=\"9\">Equity</td><td>0.3387</td><td>0.2854</td><td>0.3115</td><td>0.2603</td></tr><tr><td>2</td><td>0.2248</td><td>0.2137</td><td>0.2137</td><td>0.0520</td></tr><tr><td>3</td><td>0.1365</td><td>0.1207</td><td>0.1365</td><td>0.0106</td></tr><tr><td>4</td><td>0.1207</td><td>0.0320</td><td>0.1061</td><td>0.0011</td></tr><tr><td>5</td><td>0.0929</td><td>0.0070</td><td>0.0445</td><td>0.0018</td></tr><tr><td>6</td><td>0.0269</td><td>0.0008</td><td>0.0226</td><td>0.0006</td></tr><tr><td>7</td><td>0.0378</td><td>0.0004</td><td>0.0445</td><td>0.0004</td></tr><tr><td>8</td><td>0.0106</td><td>0.0004</td><td>0.0226</td><td>0.0004</td></tr><tr><td>9</td><td>0.0106</td><td>0.0004</td><td>0.0106</td><td>0.0001</td></tr></table>\n\nTable 10: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_26",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/8e7a78d2344b139a16107ab790233c265cad156a26b609f3845111e5d6e18b10.jpg",
      "image_filename": "8e7a78d2344b139a16107ab790233c265cad156a26b609f3845111e5d6e18b10.jpg",
      "caption": "Table 11: Detailed averaged percent biases and standard deviation results in parenthesis for the COMPAS dataset shown in Figure 3 for $\\beta$ value of 0.1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td colspan=\"2\"></td><td colspan=\"2\">COMPAS Dataset</td><td colspan=\"2\">Adult Dataset</td></tr><tr><td colspan=\"2\"></td><td colspan=\"2\">p-value</td><td colspan=\"2\">p-value</td></tr><tr><td>Iter</td><td></td><td>Parity</td><td>Classifier</td><td>Parity</td><td>Classifier</td></tr><tr><td>1</td><td rowspan=\"9\">Equity</td><td>0.1537</td><td>0.0606</td><td>0.0156</td><td>0.0004</td></tr><tr><td>2</td><td>0.0606</td><td>0.0056</td><td>0.0005</td><td>9.1e-05</td></tr><tr><td>3</td><td>0.0156</td><td>0.0003</td><td>0.0001</td><td>9.1e-05</td></tr><tr><td>4</td><td>0.0045</td><td>0.0001</td><td>9.1e-05</td><td>9.1e-05</td></tr><tr><td>5</td><td>0.0018</td><td>9.1e-05</td><td>9.1e-05</td><td>9.1e-05</td></tr><tr><td>6</td><td>0.0005</td><td>9.1e-05</td><td>9.1e-05</td><td>9.1e-05</td></tr><tr><td>7</td><td>0.0005</td><td>9.1e-05</td><td>9.1e-05</td><td>9.1e-05</td></tr><tr><td>8</td><td>0.0002</td><td>9.1e-05</td><td>9.1e-05</td><td>9.1e-05</td></tr><tr><td>9</td><td>0.0002</td><td>9.1e-05</td><td>9.1e-05</td><td>9.1e-05</td></tr></table>\n\nTable 11: Detailed averaged percent biases and standard deviation results in parenthesis for the COMPAS dataset shown in Figure 3 for $\\beta$ value of 0.1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_27",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/212f08feace5304b23f819d3e428f2a57604e1b5c4078e03abad8c939ab2902d.jpg",
      "image_filename": "212f08feace5304b23f819d3e428f2a57604e1b5c4078e03abad8c939ab2902d.jpg",
      "caption": "Table 12: Detailed averaged percent biases and standard deviation results in parenthesis for the COMPAS dataset shown in Figure 3 for $\\beta$ value of 0.5.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td></td><td>Beta</td><td>Equity</td><td>Parity</td><td>Classifier</td></tr><tr><td rowspan=\"10\">Bias</td><td>0</td><td>11.82%(0.78)</td><td>11.82%(0.78)</td><td>11.82%(0.78)</td></tr><tr><td>1</td><td>11.79%(0.74)</td><td>11.87%(0.77)</td><td>11.93%(0.79)</td></tr><tr><td>2</td><td>11.80%(0.74)</td><td>12.04%(0.79)</td><td>12.10%(0.76)</td></tr><tr><td>3</td><td>11.82%(0.75)</td><td>12.18%(0.80)</td><td>12.30%(0.72)</td></tr><tr><td>4</td><td>11.79%(0.69)</td><td>12.25%(0.76)</td><td>12.47%(0.67)</td></tr><tr><td>5</td><td>11.84%(0.69)</td><td>12.40%(0.77)</td><td>12.69%(0.69)</td></tr><tr><td>6</td><td>11.70%(0.60)</td><td>12.39%(0.73)</td><td>12.77%(0.64)</td></tr><tr><td>7</td><td>11.67%(0.70)</td><td>12.47%(0.85)</td><td>12.98%(0.75)</td></tr><tr><td>8</td><td>11.67%(0.67)</td><td>12.55%(0.82)</td><td>13.16%(0.70)</td></tr><tr><td>9</td><td>11.66%(0.63)</td><td>12.59%(0.81)</td><td>13.24%(0.70)</td></tr></table>\n\nTable 12: Detailed averaged percent biases and standard deviation results in parenthesis for the COMPAS dataset shown in Figure 3 for $\\beta$ value of 0.5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_28",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/6cde76da10b6968f8b579c707ceecc290199865713210edc9c72c0e8de8f3540.jpg",
      "image_filename": "6cde76da10b6968f8b579c707ceecc290199865713210edc9c72c0e8de8f3540.jpg",
      "caption": "Table 13: Detailed averaged percent biases and standard deviation results in parenthesis for the COMPAS dataset shown in Figure 3 for $\\beta$ value of 0.9.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td></td><td>Beta</td><td>Equity</td><td>Parity</td><td>Classifier</td></tr><tr><td rowspan=\"10\">Bias</td><td>0</td><td>11.18%(0.78)</td><td>11.82%(0.78)</td><td>11.82%(0.78)</td></tr><tr><td>1</td><td>11.45%(0.73)</td><td>11.75%(0.73)</td><td>11.93%(0.79)</td></tr><tr><td>2</td><td>11.18%(0.70)</td><td>11.77%(0.77)</td><td>12.10%(0.76)</td></tr><tr><td>3</td><td>10.90%(0.69)</td><td>11.74%(0.77)</td><td>12.30%(0.72)</td></tr><tr><td>4</td><td>10.63%(0.75)</td><td>11.68%(0.77)</td><td>12.47%(0.67)</td></tr><tr><td>5</td><td>10.40%(0.76)</td><td>11.71%(0.75)</td><td>12.69%(0.69)</td></tr><tr><td>6</td><td>10.11%(0.73)</td><td>11.52%(0.64)</td><td>12.77%(0.64)</td></tr><tr><td>7</td><td>9.85%(0.81)</td><td>11.46%(0.77)</td><td>12.98%(0.75)</td></tr><tr><td>8</td><td>9.66%(0.75)</td><td>11.40%(0.77)</td><td>13.16%(0.70)</td></tr><tr><td>9</td><td>9.35%(0.78)</td><td>11.36%(0.72)</td><td>13.24%(0.70)</td></tr></table>\n\nTable 13: Detailed averaged percent biases and standard deviation results in parenthesis for the COMPAS dataset shown in Figure 3 for $\\beta$ value of 0.9."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_29",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2e6a33fd42cff2f11f9bb3407dca29edd22c9ff2c3772b5d5c4fed31a71b18f7.jpg",
      "image_filename": "2e6a33fd42cff2f11f9bb3407dca29edd22c9ff2c3772b5d5c4fed31a71b18f7.jpg",
      "caption": "Table 14: Detailed averaged percent biases and standard deviation results in parenthesis for the Adult dataset shown in Figure 3 for $\\beta$ value of 0.1.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td></td><td>Beta</td><td>Equity</td><td>Parity</td><td>Classifier</td></tr><tr><td rowspan=\"10\">Bias</td><td>0</td><td>11.82%(0.78)</td><td>11.82%(0.78)</td><td>11.82%(0.78)</td></tr><tr><td>1</td><td>11.38%(0.86)</td><td>11.69%(0.75)</td><td>11.93%(0.79)</td></tr><tr><td>2</td><td>10.97%(0.84)</td><td>11.64%(0.76)</td><td>12.10%(0.76)</td></tr><tr><td>3</td><td>10.59%(0.91)</td><td>11.57%(0.75)</td><td>12.30%(0.72)</td></tr><tr><td>4</td><td>10.20%(1.03)</td><td>11.48%(0.71)</td><td>12.47%(0.67)</td></tr><tr><td>5</td><td>9.92%(1.04)</td><td>11.41%(0.73)</td><td>12.69%(0.69)</td></tr><tr><td>6</td><td>9.53%(1.08)</td><td>11.21%(0.63)</td><td>12.77%(0.64)</td></tr><tr><td>7</td><td>9.19%(1.18)</td><td>11.12%(0.74)</td><td>12.98%(0.75)</td></tr><tr><td>8</td><td>8.91%(1.16)</td><td>11.06%(0.71)</td><td>13.16%(0.70)</td></tr><tr><td>9</td><td>8.52%(1.21)</td><td>11.01%(0.74)</td><td>13.24%(0.70)</td></tr></table>\n\nTable 14: Detailed averaged percent biases and standard deviation results in parenthesis for the Adult dataset shown in Figure 3 for $\\beta$ value of 0.1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_30",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/3d1576a517cdfb17a2bfd82e672943446e91be0bee35fee4d18aa0cdbb4d2c1c.jpg",
      "image_filename": "3d1576a517cdfb17a2bfd82e672943446e91be0bee35fee4d18aa0cdbb4d2c1c.jpg",
      "caption": "Table 15: Detailed averaged percent biases and standard deviation results in parenthesis for the Adult dataset shown in Figure 3 for $\\beta$ value of 0.5.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td></td><td>Beta</td><td>Equity</td><td>Parity</td><td>Classifier</td></tr><tr><td rowspan=\"10\">Bias</td><td>0</td><td>19.91%(0.18)</td><td>19.91%(0.18)</td><td>19.91%(0.18)</td></tr><tr><td>1</td><td>19.83%(0.17)</td><td>19.86%(0.17)</td><td>19.89%(0.16)</td></tr><tr><td>2</td><td>19.75%(0.17)</td><td>19.81%(0.16)</td><td>19.88%(0.16)</td></tr><tr><td>3</td><td>19.67%(0.17)</td><td>19.76%(0.17)</td><td>19.87%(0.17)</td></tr><tr><td>4</td><td>19.59%(0.16)</td><td>19.70%(0.16)</td><td>19.86%(0.16)</td></tr><tr><td>5</td><td>19.51%(0.19)</td><td>19.67%(0.18)</td><td>19.85%(0.18)</td></tr><tr><td>6</td><td>19.43%(0.20)</td><td>19.63%(0.19)</td><td>19.83%(0.19)</td></tr><tr><td>7</td><td>19.34%(0.22)</td><td>19.58%(0.21)</td><td>19.81%(0.20)</td></tr><tr><td>8</td><td>19.26%(0.23)</td><td>19.53%(0.21)</td><td>19.79%(0.21)</td></tr><tr><td>9</td><td>19.18%(0.23)</td><td>19.48%(0.21)</td><td>19.77%(0.21)</td></tr></table>\n\nTable 15: Detailed averaged percent biases and standard deviation results in parenthesis for the Adult dataset shown in Figure 3 for $\\beta$ value of 0.5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_31",
      "figure_number": 16,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/ad33fafc6f7aca52dfacde917855164a67b5ed73c74c7624e7632b340a916cb5.jpg",
      "image_filename": "ad33fafc6f7aca52dfacde917855164a67b5ed73c74c7624e7632b340a916cb5.jpg",
      "caption": "Table 16: Detailed averaged percent biases and standard deviation results in parenthesis for the Adult dataset shown in Figure 3 for $\\beta$ value of 0.9.",
      "context_before": "",
      "context_after": "Survey Instructions (Click to expand)\n\nIn this task, you will be given 4 different scenarios and we would ask you to rate how much proposed solutions to each of the scenarios would be fair on a scale of 0 to 4 (0 meaning completely unfair and 4 meaning completely fair). We would also ask you to pick one of the solutions and tell us why you picked your preferred solution. Attention: You should provide a justification in text boxes 1,2, 3, and 4 or you would not be paid. In other words, you should tell us why you chose your preferred picture/solution to each of the scenarios.\n\n1. Scenario 1 ss degree",
      "referring_paragraphs": [
        "<table><tr><td></td><td>Beta</td><td>Equity</td><td>Parity</td><td>Classifier</td></tr><tr><td rowspan=\"10\">Bias</td><td>0</td><td>19.91%(0.18)</td><td>19.91%(0.18)</td><td>19.91%(0.18)</td></tr><tr><td>1</td><td>19.61%(0.16)</td><td>19.76%(0.17)</td><td>19.89%(0.16)</td></tr><tr><td>2</td><td>19.32%(0.18)</td><td>19.59%(0.17)</td><td>19.88%(0.16)</td></tr><tr><td>3</td><td>19.01%(0.17)</td><td>19.42%(0.17)</td><td>19.87%(0.17)</td></tr><tr><td>4</td><td>18.73%(0.15)</td><td>19.30%(0.16)</td><td>19.86%(0.16)</td></tr><tr><td>5</td><td>18.44%(0.16)</td><td>19.12%(0.17)</td><td>19.85%(0.18)</td></tr><tr><td>6</td><td>18.71%(0.16)</td><td>18.97%(0.16)</td><td>19.83%(0.19)</td></tr><tr><td>7</td><td>17.90%(0.18)</td><td>18.82%(0.18)</td><td>19.81%(0.20)</td></tr><tr><td>8</td><td>17.63%(0.20)</td><td>18.66%(0.18)</td><td>19.79%(0.21)</td></tr><tr><td>9</td><td>17.38%(0.21)</td><td>18.51%(0.19)</td><td>19.77%(0.21)</td></tr></table>\n\nTable 16: Detailed averaged percent biases and standard deviation results in parenthesis for the Adult dataset shown in Figure 3 for $\\beta$ value of 0.9."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_32",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig16.jpg",
      "image_filename": "2005.07293_page0_fig16.jpg",
      "caption": "Picture A",
      "context_before": "Survey Instructions (Click to expand)\n\nIn this task, you will be given 4 different scenarios and we would ask you to rate how much proposed solutions to each of the scenarios would be fair on a scale of 0 to 4 (0 meaning completely unfair and 4 meaning completely fair). We would also ask you to pick one of the solutions and tell us why you picked your preferred solution. Attention: You should provide a justification in text boxes 1,2, 3, and 4 or you would not be paid. In other words, you should tell us why you chose your preferred picture/solution to each of the scenarios.\n\n1. Scenario 1 ss degree",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_33",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig17.jpg",
      "image_filename": "2005.07293_page0_fig17.jpg",
      "caption": "PictureB",
      "context_before": "",
      "context_after": "Picture A (Picture on the left):\n\n2-Neither Fair Nor Unfair\n\nPicture B (Picture on the right):",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {
        "source": "content_list",
        "source_file": "2005.07293_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ],
  "2103.11320": [
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/025a16163ff5c7cd16a3334c0db238728264a7d9f86f0dcce0cdce842980756e.jpg",
      "image_filename": "025a16163ff5c7cd16a3334c0db238728264a7d9f86f0dcce0cdce842980756e.jpg",
      "caption": "Table 1: Biased cases in ConceptNet and GenericsKB.",
      "context_before": "Warning: this paper contains content that may be offensive or upsetting.\n\nCommonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not conflated with the notion of commonsense. Here we focus on two widely used CSKBs, ConceptNet and GenericsKB, and establish the presence of bias in the form of two types of representational harms, overgeneralization of polarized perceptions and representation disparity across different demographic groups in both CSKBs. Next, we find similar representational harms for downstream models that use ConceptNet. Finally, we propose a filtering-based approach for mitigating such harms, and observe that our filtered-based approach can reduce the issues in both resources and models but leads to a performance drop, leaving room for future work to build fairer and stronger commonsense models.\n\nCommonsense knowledge is important for a wide range of natural language processing (NLP) tasks as a way to incorporate information about everyday situations necessary for human language understanding. Numerous models have included knowledge resources such as ConceptNet (Speer et al., 2017) for question answering (Lin et al., 2019), sarcasm generation (Chakrabarty et al., 2020), and dialogue response generation (Zhou et al., 2018, 2021), among others. However, commonsense knowledge resources are mostly human-generated, either crowdsourced from the public (Speer et al., 2017; Sap et al., 2019) or crawled from massive web corpora (Bhakthavatsalam et al., 2020). For example, ConceptNet originated from the Open",
      "context_after": "Mind Common Sense project that collects commonsense statements online from web users (Singh et al., 2002)1 and GenericsKB consists of crawled text from public websites. One issue with this approach is that the crowdsourcing workers and web page writers may conflate their own prejudices with the notion of commonsense. For instance, we have found that querying for some target words such as “church” as shown in Table 1 in ConceptNet, results in biased triples.\n\nThe potentially biased nature of commonsense knowledge bases (CSKB), given their increasing popularity, raises the urgent need to quantify biases both in the knowledge resources and in the downstream models that use these resources. We present the first study on measuring bias in two large CSKBs, namely ConceptNet (Speer et al., 2017), the most widely used knowledge graph in commonsense reasoning tasks, and GenericsKB (Bhakthavatsalam et al., 2020), which expresses knowledge in the form of natural language sentences and has gained increasing usage. We formalize a new quantification of “representational harms,” i.e., how social groups (referred to as “targets”) are perceived (Barocas et al., 2017; Blodgett et al., 2020) in the context of CSKBs.\n\nWe consider two types of such harms in the context of CSKBs. One is intra-target overgeneralization, indicating that “common sense” in these resources may unfairly attribute a polarized (nega-",
      "referring_paragraphs": [
        "Mind Common Sense project that collects commonsense statements online from web users (Singh et al., 2002)1 and GenericsKB consists of crawled text from public websites. One issue with this approach is that the crowdsourcing workers and web page writers may conflate their own prejudices with the notion of commonsense. For instance, we have found that querying for some target words such as “church” as shown in Table 1 in ConceptNet, results in biased triples.",
        "To adapt the definition of representational harms to a sentence set, we define two sub-types of harms, intra-target overgeneralization and inter-target disparity, aiming to cover different categories of representational harms (Barocas et al., 2017; Crawford, 2017). We consider overgeneralization that directly examines whether targets such as “lawyer” or “lady” are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, ",
        "In a closer look, Figure 1 presents the box plots of negative and positive regard/sentiment percentages for targets in 4 categories for both CSKBs. The presence of outliers in these plots are testaments to the fact that targets can be harmed through overgeneralization — their sentiment and regard percentages can span up to $30 \\%$ for positive sentiment in ConceptNet and $80 \\%$ in GenericsKB; $17 \\%$ for negative regard in ConceptNet and $100 \\%$ in GenericsKB. We again find some similar trends",
        "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4. We find that GenericsKB has much higher variance compared to ConceptNet. To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories. These plots illustrate the dispersion of negative sentiment/regard percentages which represent",
        "For example, ConceptNet originated from the Open\n\nTable 1: Biased cases in ConceptNet and GenericsKB.",
        "We consider overgeneralization that directly examines whether targets such as “lawyer” or “lady” are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.",
        "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
        "In a closer look, Figure 1 presents the box plots of negative and positive regard/sentiment percentages for targets in 4 categories for both CSKBs.",
        "To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/7b47d3e29f73ab68cdd5c4d63258f9e40f12646b570b557c6a295c71e7905d19.jpg",
      "image_filename": "7b47d3e29f73ab68cdd5c4d63258f9e40f12646b570b557c6a295c71e7905d19.jpg",
      "caption": "Table 2: Agreement of sentiment and regard labels with human annotators in terms of accuracy.",
      "context_before": "$$ D _ {R} (\\mathbb {S}, \\mathbb {T}) = \\mathbb {E} \\left[ \\left(\\left| \\mathbb {S} _ {t _ {j}} \\right| - \\left| \\overline {{\\mathbb {S} _ {t}}} \\right|\\right) ^ {2} \\right], \\tag {3} $$\n\n$$ D _ {O} ^ {+ / -} (\\mathbb {S}, \\mathbb {T}) = \\mathbb {E} \\left[ \\left(O ^ {+ / -} (\\mathbb {S}, t _ {j}) - \\overline {{O ^ {+ / -} (\\mathbb {S} , t _ {j})}}\\right) ^ {2} \\right], \\tag {4} $$\n\nwhere $| \\overline { { \\mathbb { S } _ { t } } } |$ indicates the average number of statements for targets in $\\mathbb { T }$ and $\\overline { { O ^ { + / - } ( \\mathbb { S } , t _ { j } ) } }$ is the average overgeneralization bias for targets, $\" < '$ for favoritism and “-” for prejudice. The expectation $\\mathbb { E }$ is taken over all targets $t _ { j } \\in \\mathbb { T }$ .",
      "context_after": "2.3 Measuring Polarized Perceptions\n\nPrior work (Sheng et al., 2019) demonstrated that sentiment and regard are effective measures of bias (polarized views toward a target group). Although this is still an active area of research, for now, these are promising proxies that many works in ethical NLP also have used to measure bias (e.g. Sheng et al. (2019); Li et al. (2020); Brown et al. (2020); Sheng et al. (2020); Dhamala et al. (2021)). However, we acknowledge that there still exist problems with these measures as proxies for measuring bias and acknowledge the existence of noisy labels using these measures as proxies. To put this into test and to show that these measures can still be reliable proxies despite the aforementioned problems, we perform studies both including human evaluators in the loop as well as comparison of these measures with a keyword-based approach in this section.\n\nIn order to determine the polarization of perception associated to a statement toward a group, we apply sentiment and regard classifiers on the statement containing the target group and obtain the corresponding labels from each of the classifiers. We then categorize the statement into favoritism, prejudice, or neutral based on the positive, negative, or neutral labels obtained from each of the classifiers.",
      "referring_paragraphs": [
        "in Table 2, we found reasonable agreement in terms of accuracy for sentiment and regard with human labels. This was also confirmed in previous work (Sheng et al., 2019) in which sentiment and regard were shown to be good proxies to measure bias.",
        "Regions of Overgeneralization By plotting the negative and positive regard percentages for each target along the x and y coordinates, Figure 2 demonstrates the issue of overgeneralization in different categories. For example, for “Profession,” some target professions such as “CEO” are associated with a higher positive regard percentage (blue region) and thus a higher overgenaralization in terms of favoritism. In contrast, some professions, such as “politician” are associated with a higher negati",
        "prejudices against targets as well as positive sentiment/regard percentages for favoritism toward targets. We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others. The same trend also holds for positive sentiment and regard scores. Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions.",
        "Table 2: Agreement of sentiment and regard labels with human annotators in terms of accuracy.",
        "<table><tr><td>CSKB</td><td>Method</td><td>Favoritism R/P/F1</td><td>Prejudice R/P/F1</td></tr><tr><td rowspan=\"3\">GenericsKB</td><td>Regard</td><td>0.551/0.579/0.565</td><td>0.809/0.333/0.472</td></tr><tr><td>Sentiment</td><td>0.441/0.622/0.516</td><td>0.432/0.541/0.480</td></tr><tr><td>Keyword</td><td>0.268/0.643/0.379</td><td>0.276/0.539/0.365</td></tr><tr><td rowspan=\"3\">ConceptNet</td><td>Regard</td><td>0.436/0.383/0.408</td><td>0.698/0.342/0.459</td></tr><tr><td>Sentiment</td><td>0.378/0.528/0.440</td><td>0.264/0.531/0.353</td></tr><tr><td>Keyword</td><td>0.201/0.556/0.295</td><td>0.105/0.470/0.172</td></tr></table>\n\nin Table 2, we found reasonable agreement in terms of accuracy for sentiment and regard with human labels.",
        "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al.",
        "Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/e926981b10473306511687907481003ef94478586f0d188edc77ed9cb0214ac7.jpg",
      "image_filename": "e926981b10473306511687907481003ef94478586f0d188edc77ed9cb0214ac7.jpg",
      "caption": "Table 3: Comparison of sentiment, regard, and baseline keyword-based approach in terms of favoritism and prejudice recall/precision/F1 scores.",
      "context_before": "In order to determine the polarization of perception associated to a statement toward a group, we apply sentiment and regard classifiers on the statement containing the target group and obtain the corresponding labels from each of the classifiers. We then categorize the statement into favoritism, prejudice, or neutral based on the positive, negative, or neutral labels obtained from each of the classifiers.\n\nCrowdsourcing Human Labels To validate the quality of these polarity proxies, we conduct crowdsourcing to solicit human labels on the statement polarity. We asked Amazon Mechanical Turk workers to label provided knowledge from GenericsKB (Bhakthavatsalam et al., 2020) and Concept-Net (Speer et al., 2017) with regards to favoritism, prejudice, and neutral toward a target group. 3,000 instances were labeled from ConceptNet and more than 1,500 from GenericsKB. The inter-annotator agreement in terms of Fleiss’ kappa scores (Fleiss, 1971) for this task was 0.5007 and 0.3827 for GenericsKB and ConceptNet respectively.\n\nAlignment with Human Labels We compare human labels with those obtained from sentiment and regard classifiers to check the validity of these measures as proxies for overgeneralization. As shown",
      "context_after": "in Table 2, we found reasonable agreement in terms of accuracy for sentiment and regard with human labels. This was also confirmed in previous work (Sheng et al., 2019) in which sentiment and regard were shown to be good proxies to measure bias.\n\nComparison with Keyword-based Approach We also compare the sentiment and regard classifiers to a keyword-based baseline, in which we collect a list of biased words that could represent favoritism and prejudice from LIWC (Tausczik and Pennebaker, 2010) and Empath (Fast et al., 2016). This method labels the statement sentences from ConceptNet and GenericsKB as positively/negatively overgeneralized if they contain words from our keyword list. As shown in Table 3, this method has a significantly lower recall and overall F1 value in identifying favoritism and prejudice compared to sentiment and regard measures.\n\n3 Representational Harms in CSKBs",
      "referring_paragraphs": [
        "Comparison with Keyword-based Approach We also compare the sentiment and regard classifiers to a keyword-based baseline, in which we collect a list of biased words that could represent favoritism and prejudice from LIWC (Tausczik and Pennebaker, 2010) and Empath (Fast et al., 2016). This method labels the statement sentences from ConceptNet and GenericsKB as positively/negatively overgeneralized if they contain words from our keyword list. As shown in Table 3, this method has a significantly low",
        "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgen-",
        "prejudices against targets as well as positive sentiment/regard percentages for favoritism toward targets. We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others. The same trend also holds for positive sentiment and regard scores. Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions.",
        "As shown\n\nTable 3: Comparison of sentiment, regard, and baseline keyword-based approach in terms of favoritism and prejudice recall/precision/F1 scores.",
        "Teacher causes the desire to study.Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias.",
        "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgen-\n\neralization can be, along with some concrete examples.",
        "We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_4",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig0.jpg",
      "image_filename": "2103.11320_page0_fig0.jpg",
      "caption": "4https://github.com/commonsense/ conceptnet5/wiki/Downloads",
      "context_before": "3.2 Analysis of Representational Harms\n\nResults on Overgeneralization We quantify overgeneralization using Eq. (1) and (2) in Section 2. The overall average percentage of overgeneralized\n\n4https://github.com/commonsense/ conceptnet5/wiki/Downloads",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg",
      "image_filename": "2103.11320_page0_fig1.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_6",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig2.jpg",
      "image_filename": "2103.11320_page0_fig2.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_7",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig3.jpg",
      "image_filename": "2103.11320_page0_fig3.jpg",
      "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Mind Common Sense project that collects commonsense statements online from web users (Singh et al., 2002)1 and GenericsKB consists of crawled text from public websites. One issue with this approach is that the crowdsourcing workers and web page writers may conflate their own prejudices with the notion of commonsense. For instance, we have found that querying for some target words such as “church” as shown in Table 1 in ConceptNet, results in biased triples.",
        "To adapt the definition of representational harms to a sentence set, we define two sub-types of harms, intra-target overgeneralization and inter-target disparity, aiming to cover different categories of representational harms (Barocas et al., 2017; Crawford, 2017). We consider overgeneralization that directly examines whether targets such as “lawyer” or “lady” are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, ",
        "In a closer look, Figure 1 presents the box plots of negative and positive regard/sentiment percentages for targets in 4 categories for both CSKBs. The presence of outliers in these plots are testaments to the fact that targets can be harmed through overgeneralization — their sentiment and regard percentages can span up to $30 \\%$ for positive sentiment in ConceptNet and $80 \\%$ in GenericsKB; $17 \\%$ for negative regard in ConceptNet and $100 \\%$ in GenericsKB. We again find some similar trends",
        "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4. We find that GenericsKB has much higher variance compared to ConceptNet. To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories. These plots illustrate the dispersion of negative sentiment/regard percentages which represent",
        "For example, ConceptNet originated from the Open\n\nTable 1: Biased cases in ConceptNet and GenericsKB.",
        "We consider overgeneralization that directly examines whether targets such as “lawyer” or “lady” are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.",
        "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
        "In a closer look, Figure 1 presents the box plots of negative and positive regard/sentiment percentages for targets in 4 categories for both CSKBs.",
        "To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_8",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg",
      "image_filename": "2103.11320_page0_fig4.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_9",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig5.jpg",
      "image_filename": "2103.11320_page0_fig5.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_10",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig6.jpg",
      "image_filename": "2103.11320_page0_fig6.jpg",
      "caption": "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral. Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning. We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the “Religion” category and find similar polarized perceptions of certain groups, despite a much larger percentage range for GenericsKB.",
      "context_before": "",
      "context_after": "triples in ConceptNet is $4 . 5 \\%$ (4.6k triples) for sentiment and $3 . 4 \\%$ (3.6k triple) for regard. For GenericsKB, the percentages are $3 6 . 5 \\%$ for sentiment (11k triples) and $3 8 . 6 \\%$ for regard (11k triples). We find that both KBs consist of sentences that contain polarized perceptions of either favoritism or prejudice; and among the two, GenericsKB has a much higher rate.\n\nIn a closer look, Figure 1 presents the box plots of negative and positive regard/sentiment percentages for targets in 4 categories for both CSKBs. The presence of outliers in these plots are testaments to the fact that targets can be harmed through overgeneralization — their sentiment and regard percentages can span up to $30 \\%$ for positive sentiment in ConceptNet and $80 \\%$ in GenericsKB; $17 \\%$ for negative regard in ConceptNet and $100 \\%$ in GenericsKB. We again find some similar trends of representational harms across the two KBs qualitatively, such as the box shapes for “Gender” and “Religion” categories, indicating common biases in\n\nknowledge resources. Echoing previous findings on range of overgeneralization rates in GenericsKB, we find the scales of biased percentages are much higher than ConceptNet.",
      "referring_paragraphs": [
        "in Table 2, we found reasonable agreement in terms of accuracy for sentiment and regard with human labels. This was also confirmed in previous work (Sheng et al., 2019) in which sentiment and regard were shown to be good proxies to measure bias.",
        "Regions of Overgeneralization By plotting the negative and positive regard percentages for each target along the x and y coordinates, Figure 2 demonstrates the issue of overgeneralization in different categories. For example, for “Profession,” some target professions such as “CEO” are associated with a higher positive regard percentage (blue region) and thus a higher overgenaralization in terms of favoritism. In contrast, some professions, such as “politician” are associated with a higher negati",
        "prejudices against targets as well as positive sentiment/regard percentages for favoritism toward targets. We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others. The same trend also holds for positive sentiment and regard scores. Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions.",
        "Table 2: Agreement of sentiment and regard labels with human annotators in terms of accuracy.",
        "<table><tr><td>CSKB</td><td>Method</td><td>Favoritism R/P/F1</td><td>Prejudice R/P/F1</td></tr><tr><td rowspan=\"3\">GenericsKB</td><td>Regard</td><td>0.551/0.579/0.565</td><td>0.809/0.333/0.472</td></tr><tr><td>Sentiment</td><td>0.441/0.622/0.516</td><td>0.432/0.541/0.480</td></tr><tr><td>Keyword</td><td>0.268/0.643/0.379</td><td>0.276/0.539/0.365</td></tr><tr><td rowspan=\"3\">ConceptNet</td><td>Regard</td><td>0.436/0.383/0.408</td><td>0.698/0.342/0.459</td></tr><tr><td>Sentiment</td><td>0.378/0.528/0.440</td><td>0.264/0.531/0.353</td></tr><tr><td>Keyword</td><td>0.201/0.556/0.295</td><td>0.105/0.470/0.172</td></tr></table>\n\nin Table 2, we found reasonable agreement in terms of accuracy for sentiment and regard with human labels.",
        "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al.",
        "Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_11",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig7.jpg",
      "image_filename": "2103.11320_page0_fig7.jpg",
      "caption": "ConceptNet vs GenericsKB We compare Con-",
      "context_before": "knowledge resources. Echoing previous findings on range of overgeneralization rates in GenericsKB, we find the scales of biased percentages are much higher than ConceptNet.\n\nRegions of Overgeneralization By plotting the negative and positive regard percentages for each target along the x and y coordinates, Figure 2 demonstrates the issue of overgeneralization in different categories. For example, for “Profession,” some target professions such as “CEO” are associated with a higher positive regard percentage (blue region) and thus a higher overgenaralization in terms of favoritism. In contrast, some professions, such as “politician” are associated with a higher negative regard percentage (red region) representing a higher overgenaralization in terms of prejudice. In addition, some professions, such as “psychologist” are associated with both high negative and positive regard percentages (purple region) and high positive and negative overgenaralization.\n\nConceptNet vs GenericsKB We compare Con-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_12",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg",
      "image_filename": "2103.11320_page0_fig8.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_13",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig9.jpg",
      "image_filename": "2103.11320_page0_fig9.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_14",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig10.jpg",
      "image_filename": "2103.11320_page0_fig10.jpg",
      "caption": "Saffron terror is related to hindu. Teacher causes the desire to study.Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Comparison with Keyword-based Approach We also compare the sentiment and regard classifiers to a keyword-based baseline, in which we collect a list of biased words that could represent favoritism and prejudice from LIWC (Tausczik and Pennebaker, 2010) and Empath (Fast et al., 2016). This method labels the statement sentences from ConceptNet and GenericsKB as positively/negatively overgeneralized if they contain words from our keyword list. As shown in Table 3, this method has a significantly low",
        "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgen-",
        "prejudices against targets as well as positive sentiment/regard percentages for favoritism toward targets. We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others. The same trend also holds for positive sentiment and regard scores. Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions.",
        "As shown\n\nTable 3: Comparison of sentiment, regard, and baseline keyword-based approach in terms of favoritism and prejudice recall/precision/F1 scores.",
        "Teacher causes the desire to study.Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias.",
        "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgen-\n\neralization can be, along with some concrete examples.",
        "We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_15",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig11.jpg",
      "image_filename": "2103.11320_page0_fig11.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg",
      "image_filename": "2103.11320_page0_fig12.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_17",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig13.jpg",
      "image_filename": "2103.11320_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_18",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig14.jpg",
      "image_filename": "2103.11320_page0_fig14.jpg",
      "caption": "(c) ConceptNet Profession (d) GenericsKB Profession Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB. We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.",
      "context_before": "",
      "context_after": "ceptNet and GenericsKB on the “Religion” category and see certain targets contain similar biases, such as “christian” contains both biases and “sharia” is prejudiced against in both KBs. Furthermore, we find interesting discrepancies between the two KBs: GenericsKB’s overall percentages of positive and negative biases are much higher than ConceptNet, indicated by the scale on x and y axis ( $0 \\%$ for GenericsKB and $0 . 1 6 \\%$ for ConceptNet). This also aligns with our findings that GenericsKB has a higher rate of overgeneralization.\n\nSeverity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgen-\n\neralization can be, along with some concrete examples. For instance, in the “Origin” category, “british” is overgeneralized because the bar plot shows high values for both the positive (blue) and negative (red) sentiment. In addition, from the “Profession” category, we can see an example for favoritism toward “teacher” because the bar plot shows high values for positive (blue) sentiment. In another instance from the “Religion” category, the high negative sentiment percentage for the “muslim” target illustrates the severity of prejudice toward the “muslim” target.",
      "referring_paragraphs": [
        "Representation Disparity We first quantify the disparity in terms of the number of triples for each target (word) in the 4 categories, using Eq. (3). Table 4 shows extremely high variance in both CSKBs. Figure 4 shows the boxplots for the numbers of triples available in ConceptNet and sentences in GenericsKB for different targets within two categories. We can see that the number ranges from 0 to thousands triples for different targets in two KBs, and GenericsKB has more severe outliers that have",
        "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4. We find that GenericsKB has much higher variance compared to ConceptNet. To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories. These plots illustrate the dispersion of negative sentiment/regard percentages which represent",
        "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB.",
        "(4), shown in Table 4."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_19",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2a00a60b9344a87838ad3286c514250a1180628926214e895e09a02f4a2f6841.jpg",
      "image_filename": "2a00a60b9344a87838ad3286c514250a1180628926214e895e09a02f4a2f6841.jpg",
      "caption": "Table 4: Disparity results quantified by variance across all targets on two CSKBs as shown in Equations 3 (statement #) and 4.",
      "context_before": "Representation Disparity We first quantify the disparity in terms of the number of triples for each target (word) in the 4 categories, using Eq. (3). Table 4 shows extremely high variance in both CSKBs. Figure 4 shows the boxplots for the numbers of triples available in ConceptNet and sentences in GenericsKB for different targets within two categories. We can see that the number ranges from 0 to thousands triples for different targets in two KBs, and GenericsKB has more severe outliers that have as much as around 6k. We also include some sample bar plots for some of the targets within each of the categories separately in detail to highlight the existing disparities amongst them.\n\nOvergeneralization Disparity\n\nWe further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4. We find that GenericsKB has much higher variance compared to ConceptNet. To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories. These plots illustrate the dispersion of negative sentiment/regard percentages which represent",
      "context_after": "prejudices against targets as well as positive sentiment/regard percentages for favoritism toward targets. We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others. The same trend also holds for positive sentiment and regard scores. Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions.\n\n4 Analysis on Downstream Applications\n\nAs a popular downstream application, we first consider the task of commonsense knowledge base completion which looks to automatically augment a CSKB with generated facts (Li et al., 2016). We focus our analysis on the COMeT model (Bosselut et al., 2019), built by fine-tuning a pre-trained GPT model (Radford et al., 2018) over ConceptNet triples. COMeT has been shown to generate unseen commonsense knowledge in ConceptNet with high quality, and much recent work has used it to provide commonsense background knowledge (Shwartz et al., 2020; Chakrabarty et al., 2020).",
      "referring_paragraphs": [
        "Representation Disparity We first quantify the disparity in terms of the number of triples for each target (word) in the 4 categories, using Eq. (3). Table 4 shows extremely high variance in both CSKBs. Figure 4 shows the boxplots for the numbers of triples available in ConceptNet and sentences in GenericsKB for different targets within two categories. We can see that the number ranges from 0 to thousands triples for different targets in two KBs, and GenericsKB has more severe outliers that have",
        "We further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4. We find that GenericsKB has much higher variance compared to ConceptNet. To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories. These plots illustrate the dispersion of negative sentiment/regard percentages which represent",
        "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB.",
        "(4), shown in Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_20",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/73496994a1140388a1c68bc0501fc131ca0a6facbb40ca4d842002229bfcb758.jpg",
      "image_filename": "73496994a1140388a1c68bc0501fc131ca0a6facbb40ca4d842002229bfcb758.jpg",
      "caption": "Table 5: Qualitative examples of existing biases in a downstream knowledge generation model COMeT. We can observe how destructive biases also exist in these models. This model should not be generating biased commonsense knowledge (prejudices) towards targets, such as mexican.",
      "context_before": "As a popular downstream application, we first consider the task of commonsense knowledge base completion which looks to automatically augment a CSKB with generated facts (Li et al., 2016). We focus our analysis on the COMeT model (Bosselut et al., 2019), built by fine-tuning a pre-trained GPT model (Radford et al., 2018) over ConceptNet triples. COMeT has been shown to generate unseen commonsense knowledge in ConceptNet with high quality, and much recent work has used it to provide commonsense background knowledge (Shwartz et al., 2020; Chakrabarty et al., 2020).\n\nData We collect statements in COMeT as follows: we input the same target words used in ConceptNet as prompts and collect triples by following all relations existing in the model. Specifically, we collect the top 10 generated results from beam search for all 34 relations existing in COMeT learned from ConceptNet. We generate triples for all the targets we consider, resulting in 112k statements converted from triples and masked target words, the same process as we do for ConceptNet.\n\nOvergeneralization From the results of the analysis on statements generated by COMeT, one can observe that the overgeneralization issue still exists in the generated statements. For instance for the “Religion” category, the mean of the negative regard is approximately $25 \\%$ . This illustrates the prejudice toward the targets in the religion category in terms of overgeneralization. In addition, senti-",
      "context_after": "ment scores as high as $50 \\%$ for some of the targets in some categories represent the severity of overgeneralization bias. Some additional qualitative examples are also included in Table 5.\n\nDisparity in Overgeneralization Notice that in COMeT we do not have the data imbalance problem since COMeT is a generative model, and we generate an equal number of statements for each target. Disparity in number of triples is not an issue for this task. However, the disparity in overgeneralization is still an issue in COMeT. For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization. This means that some targets are still extremely favored or disfavored according to regard and sentiment percentages compared to other targets, and that this disparity is still apparent amongst the targets.\n\n4.2 Neural Story Generation",
      "referring_paragraphs": [
        "ment scores as high as $50 \\%$ for some of the targets in some categories represent the severity of overgeneralization bias. Some additional qualitative examples are also included in Table 5.",
        "Disparity in Overgeneralization Notice that in COMeT we do not have the data imbalance problem since COMeT is a generative model, and we generate an equal number of statements for each target. Disparity in number of triples is not an issue for this task. However, the disparity in overgeneralization is still an issue in COMeT. For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity i",
        "Overgeneralization From Figure 5, we observe similar patterns in terms of the existence of the overgeneralization issue. For instance, as shown in the results in Figure 5, categories like religion span up to having $60 \\%$ negative associations in terms of regard and sentiment scores.",
        "Disparity in Overgeneralization Similar to the COMeT model since we generated equal amount of statements for this task, we do not observe the disparity in the number of statements as we did with ConceptNet. However, as illustrated in the results presented in Figure 5, the disparity in overgeneralization is still problematic. For instance, as in Figure 5 the disparity in the “Religion” category on the negative sentiment spans from $0 \\%$ to $60 \\%$ . In addition, the “Origin” category for the CSG",
        "For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.",
        "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_21",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig15.jpg",
      "image_filename": "2103.11320_page0_fig15.jpg",
      "caption": "Data To analyze bias in the story output for CSG, we prompt the CSG model using sentences that are about the social perception of a certain target. We split our targets into: people, locations, professions, and others. N",
      "context_before": "4.2 Neural Story Generation\n\nAs our second downstream task, we consider Commonsense Story Generation (CSG) (Guan et al., 2020): given a prompt, the model will generate 3 to 5 sentences to tell a story. The CSG model augments GPT-2 (Radford et al., 2019) with external commonsense knowledge by training on the CSKB examples constructed from ConceptNet and ATOMIC (Sap et al., 2019).\n\nData To analyze bias in the story output for CSG, we prompt the CSG model using sentences that are about the social perception of a certain target. We split our targets into: people, locations, professions, and others. Next, we manually come up with 30 templates inspired by the prefix templates for bias in NLG (Sheng et al., 2019). Some examples are listed in Table 6. We then generate prompts by filling the corresponding templates with target names,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_22",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg",
      "image_filename": "2103.11320_page0_fig16.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_23",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig17.jpg",
      "image_filename": "2103.11320_page0_fig17.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_24",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig18.jpg",
      "image_filename": "2103.11320_page0_fig18.jpg",
      "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "ment scores as high as $50 \\%$ for some of the targets in some categories represent the severity of overgeneralization bias. Some additional qualitative examples are also included in Table 5.",
        "Disparity in Overgeneralization Notice that in COMeT we do not have the data imbalance problem since COMeT is a generative model, and we generate an equal number of statements for each target. Disparity in number of triples is not an issue for this task. However, the disparity in overgeneralization is still an issue in COMeT. For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity i",
        "Overgeneralization From Figure 5, we observe similar patterns in terms of the existence of the overgeneralization issue. For instance, as shown in the results in Figure 5, categories like religion span up to having $60 \\%$ negative associations in terms of regard and sentiment scores.",
        "Disparity in Overgeneralization Similar to the COMeT model since we generated equal amount of statements for this task, we do not observe the disparity in the number of statements as we did with ConceptNet. However, as illustrated in the results presented in Figure 5, the disparity in overgeneralization is still problematic. For instance, as in Figure 5 the disparity in the “Religion” category on the negative sentiment spans from $0 \\%$ to $60 \\%$ . In addition, the “Origin” category for the CSG",
        "For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.",
        "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_25",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/c7fbc34be2d43f199af138aba898016036237321cab6407885eae3dbfa0e9292.jpg",
      "image_filename": "c7fbc34be2d43f199af138aba898016036237321cab6407885eae3dbfa0e9292.jpg",
      "caption": "Table 6: Example prompt templates for story generation for different targets inspired by (Sheng et al., 2019). XYZ is replaced with the target name.",
      "context_before": "",
      "context_after": "resulting in around 3k prompts for CSG. CSG generates a total of $1 2 \\mathrm { k }$ sentences and we calculate regard and sentiment percentages based on all the sentences for a given story.\n\nOvergeneralization From Figure 5, we observe similar patterns in terms of the existence of the overgeneralization issue. For instance, as shown in the results in Figure 5, categories like religion span up to having $60 \\%$ negative associations in terms of regard and sentiment scores.\n\nDisparity in Overgeneralization Similar to the COMeT model since we generated equal amount of statements for this task, we do not observe the disparity in the number of statements as we did with ConceptNet. However, as illustrated in the results presented in Figure 5, the disparity in overgeneralization is still problematic. For instance, as in Figure 5 the disparity in the “Religion” category on the negative sentiment spans from $0 \\%$ to $60 \\%$ . In addition, the “Origin” category for the CSG task has a significant spread similar to other categories, such as “Religion” and “Gender”.",
      "referring_paragraphs": [
        "Data To analyze bias in the story output for CSG, we prompt the CSG model using sentences that are about the social perception of a certain target. We split our targets into: people, locations, professions, and others. Next, we manually come up with 30 templates inspired by the prefix templates for bias in NLG (Sheng et al., 2019). Some examples are listed in Table 6. We then generate prompts by filling the corresponding templates with target names,",
        "Some examples are listed in Table 6.",
        "Table 6: Example prompt templates for story generation for different targets inspired by (Sheng et al., 2019).",
        "Figure 6: Examples from ConceptNet.   \nFigure 7: Data filtering bias mitigation framework."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_26",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/50a68cfbc7db1da961e6e5f8c01fcf6ab03601b272031bd6e63e8fbb33d1faa7.jpg",
      "image_filename": "50a68cfbc7db1da961e6e5f8c01fcf6ab03601b272031bd6e63e8fbb33d1faa7.jpg",
      "caption": "Table 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT. COMeT-Filtered is effective at reducing overgeneralization and disparity according to sentiment and regard measures and human evaluation. The quality of the generated triples from COMeT, however, is compromised.",
      "context_before": "Disparity in Overgeneralization Similar to the COMeT model since we generated equal amount of statements for this task, we do not observe the disparity in the number of statements as we did with ConceptNet. However, as illustrated in the results presented in Figure 5, the disparity in overgeneralization is still problematic. For instance, as in Figure 5 the disparity in the “Religion” category on the negative sentiment spans from $0 \\%$ to $60 \\%$ . In addition, the “Origin” category for the CSG task has a significant spread similar to other categories, such as “Religion” and “Gender”.\n\n4.3 Bias Mitigation on CSKB Completion\n\nTo mitigate the observed representational harms in ConceptNet and their effects on downstream tasks, we propose a pre-processing data filtering technique that reduces the effect of existing representational harms in ConceptNet. We apply our mitigation technique on COMeT as a case study.",
      "context_after": "Mitigation Approach Our pre-processing technique relies on data filtering. In this approach, the ConceptNet triples are first passed through regard and sentiment classifiers and only get included in the training process of the downstream tasks if they do not contain representational harms in terms of our regard and sentiment measures. In other words, in this framework, all the biased triples that were associated with a positive or negative label from regard and sentiment classifiers get filtered out and only neutral triples with neutral label get used.\n\nResults on Overgeneralization To measure effectiveness of mitigation over overgeneralization, we consider increasing the overall mean of neutral triples which is indicative of reducing the overall favoritism and prejudice according to sentiment and regard measures. We report the effects on overgenaralization on sentiment as Neutral Sentiment Mean (NSM) and regard measure as Neutral Regard Mean (NRM). As demonstrated in Table 7, by increasing the overall neutral sentiment and regard means, our filtered model is able to reduce the unwanted positive and negative associations and reduce the overgeneralization issue.\n\nResults on Disparity in Overgeneralization To measure effectiveness of mitigation over disparity in overgeneralization, we consider reducing the existing variance amongst different targets. We report the disparity in overgeneralization on sentiment as Neutral Sentiment Variance (NSV) and on regard as Neutral Regard Variance (NRV). Shown in Table 7, our filtered technique reduces the variance and dis-",
      "referring_paragraphs": [
        "Results on Overgeneralization To measure effectiveness of mitigation over overgeneralization, we consider increasing the overall mean of neutral triples which is indicative of reducing the overall favoritism and prejudice according to sentiment and regard measures. We report the effects on overgenaralization on sentiment as Neutral Sentiment Mean (NSM) and regard measure as Neutral Regard Mean (NRM). As demonstrated in Table 7, by increasing the overall neutral sentiment and regard means, our fi",
        "Results on Disparity in Overgeneralization To measure effectiveness of mitigation over disparity in overgeneralization, we consider reducing the existing variance amongst different targets. We report the disparity in overgeneralization on sentiment as Neutral Sentiment Variance (NSV) and on regard as Neutral Regard Variance (NRV). Shown in Table 7, our filtered technique reduces the variance and dis-",
        "Human Evaluation of Mitigation Results In addition to reporting regard and sentiment scores, we perform human evaluation on 3,000 generated triples from standard COMeT and COMeT-Filtered models to evaluate both the quality of the generated triples and the bias aspect of it from the human perspective on Amazon Mechanical Turk. From the results in Table 7, one can observe that COMeT-Filtered is construed to have less overall overgeneralization harm since humans rated more of the triples generated ",
        "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.",
        "Table 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT.",
        "Figure 6: Examples from ConceptNet.   \nFigure 7: Data filtering bias mitigation framework."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_27",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg",
      "image_filename": "2103.11320_page0_fig19.jpg",
      "caption": "We include details in the appendix section both in terms of providing more qualitative analysis and also some detailed experimental results that we could not include in the main text due to the space limitation.",
      "context_before": "2020) and made extensions to fill gaps in these groups. Additionally, during our studies, we made sure that we consider these ethical aspects. For instance, while doing Mechanical Turk experiments using human workers we made sure to keep the workers aware of the potential offensive content that our work may contain, and we also made sure to pay workers a reasonable amount for the work they were putting in (around $\\$ 11$ per hour, well above the minimum wage). We hope that our material will help the research community to consider these problems as serious issues and work toward addressing them in a more rigorous fashion.\n\nA Qualitative Examples\n\nWe include details in the appendix section both in terms of providing more qualitative analysis and also some detailed experimental results that we could not include in the main text due to the space limitation. For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet. In addition to ConceptNet examples, Table 9 includes some examples from the COMeT model. Similarly, Table 10 includes some examples for the Commonsense Story Generation model (CSG). Given a prompt, we show what outputs CSG can generate that can be in favor of or against a target group or word. Tables 14 and 15 contain the detailed list of these target groups and words.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_28",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig20.jpg",
      "image_filename": "2103.11320_page0_fig20.jpg",
      "caption": "Figure 6: Examples from ConceptNet. Figure 7: Data filtering bias mitigation framework.",
      "context_before": "",
      "context_after": "B Mitigation Framework\n\nIn addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.\n\nCOMeT vs Filtered-COMeT For human evaluations, we sample the top 3 generated triples for each of the “CapableOf ”, “Causes”, and “HasProperty” relations for all the groups in each category resutling in around 1,000 triples for each model and ask three mecahnical turk workers to rate each of the triples in terms of their quality (whether a triple is a valid commonsense or not) and bias (whether a triple shows favoritism or prejudice or is neutral toward the demographic groups). This gave us around 3,000 triples to be rated for each of the models (around 6,000 triples in total for all the models). Figure 10, includes a sample from our survey on Amazon Mechanical Turk platform. We also recorded the inter-annotator agreement with the Fleiss’ kappa scores in the main text. These numbers are reasonable agreements. Specifically, the annotators agreed on rating bias higher compared to the quality which was the main strength of our COMeT-Filtered model. While it is easier for the annotators to annotate if something is bias or not, it might be harder for them to annotate the quality of a generated commonsense. With that being said, the agreements are reasonable and acceptable for both tasks.",
      "referring_paragraphs": [
        "Data To analyze bias in the story output for CSG, we prompt the CSG model using sentences that are about the social perception of a certain target. We split our targets into: people, locations, professions, and others. Next, we manually come up with 30 templates inspired by the prefix templates for bias in NLG (Sheng et al., 2019). Some examples are listed in Table 6. We then generate prompts by filling the corresponding templates with target names,",
        "Some examples are listed in Table 6.",
        "Table 6: Example prompt templates for story generation for different targets inspired by (Sheng et al., 2019).",
        "Figure 6: Examples from ConceptNet.   \nFigure 7: Data filtering bias mitigation framework."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.75,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_29",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/ceb382903c4e9e2efbf09a8330f230227a1500d3acbdf84042229c0bab791fd0.jpg",
      "image_filename": "ceb382903c4e9e2efbf09a8330f230227a1500d3acbdf84042229c0bab791fd0.jpg",
      "caption": "Table 8: Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans. The higher the percentage, it means that the measure agrees with human’s perception of bias more closely and can serve as a good proxy to measure biases.",
      "context_before": "B Mitigation Framework\n\nIn addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.\n\nCOMeT vs Filtered-COMeT For human evaluations, we sample the top 3 generated triples for each of the “CapableOf ”, “Causes”, and “HasProperty” relations for all the groups in each category resutling in around 1,000 triples for each model and ask three mecahnical turk workers to rate each of the triples in terms of their quality (whether a triple is a valid commonsense or not) and bias (whether a triple shows favoritism or prejudice or is neutral toward the demographic groups). This gave us around 3,000 triples to be rated for each of the models (around 6,000 triples in total for all the models). Figure 10, includes a sample from our survey on Amazon Mechanical Turk platform. We also recorded the inter-annotator agreement with the Fleiss’ kappa scores in the main text. These numbers are reasonable agreements. Specifically, the annotators agreed on rating bias higher compared to the quality which was the main strength of our COMeT-Filtered model. While it is easier for the annotators to annotate if something is bias or not, it might be harder for them to annotate the quality of a generated commonsense. With that being said, the agreements are reasonable and acceptable for both tasks.",
      "context_after": "ConceptNet vs GenericsKB For this task we also asked three mechanical turk workers to rate 1,000 instances from ConceptNet and more than 500 instances from GenericsKB. The statement sentence triples were chosen randomly. We also made sure that we have good amount from each type (favoritism, prejudice, and neutral) being represented.\n\nD Experimental Details\n\nSentiment Analysis For sentiment analysis, we used a threshold value of greater than or equal",
      "referring_paragraphs": [
        "Table 8: Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
        "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_30",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig21.jpg",
      "image_filename": "2103.11320_page0_fig21.jpg",
      "caption": "Sentiment Analysis For sentiment analysis, we used a threshold value of greater than or equal",
      "context_before": "ConceptNet vs GenericsKB For this task we also asked three mechanical turk workers to rate 1,000 instances from ConceptNet and more than 500 instances from GenericsKB. The statement sentence triples were chosen randomly. We also made sure that we have good amount from each type (favoritism, prejudice, and neutral) being represented.\n\nD Experimental Details\n\nSentiment Analysis For sentiment analysis, we used a threshold value of greater than or equal",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_31",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg",
      "image_filename": "2103.11320_page0_fig22.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_32",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig23.jpg",
      "image_filename": "2103.11320_page0_fig23.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_33",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig24.jpg",
      "image_filename": "2103.11320_page0_fig24.jpg",
      "caption": "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 8: Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
        "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_34",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg",
      "image_filename": "2103.11320_page0_fig25.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_35",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig26.jpg",
      "image_filename": "2103.11320_page0_fig26.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_36",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig27.jpg",
      "image_filename": "2103.11320_page0_fig27.jpg",
      "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions. Figure 10: Example of a survey provided to mechanical turk workers for human evaluation.",
      "context_before": "",
      "context_after": "to 0.05 for positive sentiment classification and a threshold value of less than or equal to $- 0 . 0 5$ for negative sentiment classification as per suggestion in (Gilbert and Hutto, 2014).\n\nFiltered-COMeT and COMeT We used the same configurations for training Filtered-COMeT as config_0.json in the COMeT repository5 (details for training COMet can be obtained from the same\n\nrepository as well). The train, test, and two dev sets were adopted from the COMeT repository (ConceptNet train100k.txt, test.txt, dev1.txt, and dev2.txt) and augmented according to our filtering approach. Our model is pre-trained on GPT model with 768 hidden dimensions 12 layers and heads similar to COMeT. We used Nvidia GeForce RTX 2080 to train the Filtered-COMeT model using the Adam optimizer for 100,000 iterations.",
      "referring_paragraphs": [
        "We include details in the appendix section both in terms of providing more qualitative analysis and also some detailed experimental results that we could not include in the main text due to the space limitation. For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet. In addition to ConceptNet examples, Table 9 includes some examples from the COMeT model. Similarly, Table 10 includes some examples for the Commonsense Story G",
        "For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.",
        "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_37",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/139f3652c76ce6d5a98dc0b8791bc1c6cdbc5a2c32852e796e385a36ab4f7280.jpg",
      "image_filename": "139f3652c76ce6d5a98dc0b8791bc1c6cdbc5a2c32852e796e385a36ab4f7280.jpg",
      "caption": "Table 9: More qualitative results from ConceptNet and COMeT.",
      "context_before": "Commonsense Story Generation Experimental details can be found at CommonsenseStoryGen repository 6\n\n5https://github.com/atcbosselut/ comet-commonsense\n\n6https://github.com/thu-coai/ CommonsenseStoryGen",
      "context_after": "",
      "referring_paragraphs": [
        "We include details in the appendix section both in terms of providing more qualitative analysis and also some detailed experimental results that we could not include in the main text due to the space limitation. For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet. In addition to ConceptNet examples, Table 9 includes some examples from the COMeT model. Similarly, Table 10 includes some examples for the Commonsense Story G",
        "For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.",
        "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_38",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/01a444e037b7752752fc95dc94f31136ba91fb80506461d32a2b66d38576d36f.jpg",
      "image_filename": "01a444e037b7752752fc95dc94f31136ba91fb80506461d32a2b66d38576d36f.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_39",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/9bcd5e9fc5d49451092d83b74f9ca897a156a069d00766170164fb40f23e5d2e.jpg",
      "image_filename": "9bcd5e9fc5d49451092d83b74f9ca897a156a069d00766170164fb40f23e5d2e.jpg",
      "caption": "Table 10: Qualitative results from CSG. Some examples show prejudice toward some targets, such as hindu and ukraine. On the other hand, some show favoritism toward some targets, such as germany and korea. Table 11: Detailed mitigation results for filtering technique compared to vanilla COMeT for each category.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We include details in the appendix section both in terms of providing more qualitative analysis and also some detailed experimental results that we could not include in the main text due to the space limitation. For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet. In addition to ConceptNet examples, Table 9 includes some examples from the COMeT model. Similarly, Table 10 includes some examples for the Commonsense Story G",
        "COMeT vs Filtered-COMeT For human evaluations, we sample the top 3 generated triples for each of the “CapableOf ”, “Causes”, and “HasProperty” relations for all the groups in each category resutling in around 1,000 triples for each model and ask three mecahnical turk workers to rate each of the triples in terms of their quality (whether a triple is a valid commonsense or not) and bias (whether a triple shows favoritism or prejudice or is neutral toward the demographic groups). This gave us aroun",
        "Similarly, Table 10 includes some examples for the Commonsense Story Generation model (CSG).",
        "Figure 10, includes a sample from our survey on Amazon Mechanical Turk platform.",
        "Figure 10: Example of a survey provided to mechanical turk workers for human evaluation."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_40",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/a11f120cfbf9ce3cc95ec391f167354f5dd83d38481384d0a5368b21738d6db8.jpg",
      "image_filename": "a11f120cfbf9ce3cc95ec391f167354f5dd83d38481384d0a5368b21738d6db8.jpg",
      "caption": "Table 12: Detailed human annotator results for each category.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.",
        "<table><tr><td>Measure</td><td>Model</td><td>Origin</td><td>Religion</td><td>Gender</td><td>Profession</td></tr><tr><td rowspan=\"2\">Neutral Sentiment Mean ↑</td><td>COMeT</td><td>64.527</td><td>58.578</td><td>59.169</td><td>61.610</td></tr><tr><td>COMeT-Filtered</td><td>65.257</td><td>59.485</td><td>59.272</td><td>62.105</td></tr><tr><td rowspan=\"2\">Neutral Sentiment Variance ↓</td><td>COMeT</td><td>18.875</td><td>69.043</td><td>15.432</td><td>44.415</td></tr><tr><td>COMeT-Filtered</td><td>17.660</td><td>104.284</td><td>15.190</td><td>37.222</td></tr><tr><td rowspan=\"2\">Neutral Regard Mean ↑</td><td>COMeT</td><td>79.630</td><td>68.775</td><td>76.074</td><td>78.946</td></tr><tr><td>COMeT-Filtered</td><td>80.009</td><td>71.618</td><td>76.471</td><td>79.120</td></tr><tr><td rowspan=\"2\">Neutral Regard Variance ↓</td><td>COMeT</td><td>36.848</td><td>108.086</td><td>19.319</td><td>72.088</td></tr><tr><td>COMeT-Filtered</td><td>33.532</td><td>97.282</td><td>18.162</td><td>67.261</td></tr></table>\n\nTable 12: Detailed human annotator results for each category.",
        "female-negative regard   \nFigure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_41",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/547e309e12edce069d1bbfefe78e086e52504b243e21e8465ed6147ded23c2c7.jpg",
      "image_filename": "547e309e12edce069d1bbfefe78e086e52504b243e21e8465ed6147ded23c2c7.jpg",
      "caption": "Table 13: Additional results on neutral triples from ConceptNet.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td>Measure</td><td>Model</td><td>Origin</td><td>Religion</td><td>Gender</td><td>Profession</td><td>Overall</td></tr><tr><td rowspan=\"2\">Neutral Mean ↑</td><td>COMeT</td><td>55.7</td><td>43.5</td><td>56.4</td><td>57.0</td><td>55.8</td></tr><tr><td>COMeT-Filtered</td><td>60.2</td><td>51.8</td><td>58.9</td><td>62.2</td><td>60.5</td></tr><tr><td rowspan=\"2\">Quality ↑</td><td>COMeT</td><td>41.0</td><td>55.5</td><td>63.9</td><td>72.7</td><td>55.8</td></tr><tr><td>COMeT-Filtered</td><td>30.1</td><td>45.4</td><td>60.3</td><td>73.0</td><td>49.9</td></tr></table>\n\nTable 13: Additional results on neutral triples from ConceptNet."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_42",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig28.jpg",
      "image_filename": "2103.11320_page0_fig28.jpg",
      "caption": "(a) ConceptNet Origin",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_43",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg",
      "image_filename": "2103.11320_page0_fig29.jpg",
      "caption": "(b) ConceptNet Religion",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_44",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig30.jpg",
      "image_filename": "2103.11320_page0_fig30.jpg",
      "caption": "(c) GenericsKB Origin",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_45",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig31.jpg",
      "image_filename": "2103.11320_page0_fig31.jpg",
      "caption": "(d) GenericsKB Religion Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.",
        "Table 11: Detailed mitigation results for filtering technique compared to vanilla COMeT for each category.",
        "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_46",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig32.jpg",
      "image_filename": "2103.11320_page0_fig32.jpg",
      "caption": "british-negative sentiment",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_47",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig33.jpg",
      "image_filename": "2103.11320_page0_fig33.jpg",
      "caption": "british-negative regard",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_48",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig34.jpg",
      "image_filename": "2103.11320_page0_fig34.jpg",
      "caption": "female-negative sentiment",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_49",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig35.jpg",
      "image_filename": "2103.11320_page0_fig35.jpg",
      "caption": "female-negative regard Figure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.",
        "<table><tr><td>Measure</td><td>Model</td><td>Origin</td><td>Religion</td><td>Gender</td><td>Profession</td></tr><tr><td rowspan=\"2\">Neutral Sentiment Mean ↑</td><td>COMeT</td><td>64.527</td><td>58.578</td><td>59.169</td><td>61.610</td></tr><tr><td>COMeT-Filtered</td><td>65.257</td><td>59.485</td><td>59.272</td><td>62.105</td></tr><tr><td rowspan=\"2\">Neutral Sentiment Variance ↓</td><td>COMeT</td><td>18.875</td><td>69.043</td><td>15.432</td><td>44.415</td></tr><tr><td>COMeT-Filtered</td><td>17.660</td><td>104.284</td><td>15.190</td><td>37.222</td></tr><tr><td rowspan=\"2\">Neutral Regard Mean ↑</td><td>COMeT</td><td>79.630</td><td>68.775</td><td>76.074</td><td>78.946</td></tr><tr><td>COMeT-Filtered</td><td>80.009</td><td>71.618</td><td>76.471</td><td>79.120</td></tr><tr><td rowspan=\"2\">Neutral Regard Variance ↓</td><td>COMeT</td><td>36.848</td><td>108.086</td><td>19.319</td><td>72.088</td></tr><tr><td>COMeT-Filtered</td><td>33.532</td><td>97.282</td><td>18.162</td><td>67.261</td></tr></table>\n\nTable 12: Detailed human annotator results for each category.",
        "female-negative regard   \nFigure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_50",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/159998c58d69f08e1020d8afd7b35556eb6ee987dfd8ea87a538ef19710d46ba.jpg",
      "image_filename": "159998c58d69f08e1020d8afd7b35556eb6ee987dfd8ea87a538ef19710d46ba.jpg",
      "caption": "Table 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "may not necessarily represent a race but more of the origin or nationality of a person. Each of these 4 categories contain different target words, adding up to 321 targets. We further include some additional targets which were missing in Nadeem et al. (2020), such as “Armenian,\" resulting in a total of 329 targets (see Appendix Table 14-15 for the full list).",
        "(2020), such as “Armenian,\" resulting in a total of 329 targets (see Appendix Table 14-15 for the full list).",
        "Table 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020)."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_51",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/8600b039b425f1b05e1167456f055bb4069182836dfc455cc5111dc4b728e38b.jpg",
      "image_filename": "8600b039b425f1b05e1167456f055bb4069182836dfc455cc5111dc4b728e38b.jpg",
      "caption": "Table 15: Targets from origin, gender, and religion categories in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "<table><tr><td></td><td></td><td colspan=\"3\">Profession</td></tr><tr><td>barber</td><td>coach</td><td>businessperson</td><td>football player</td><td>construction worker</td></tr><tr><td>manager</td><td>CEO</td><td>accountant</td><td>commander</td><td>firefighter</td></tr><tr><td>mover</td><td>software developer</td><td>guard</td><td>baker</td><td>doctor</td></tr><tr><td>athlete</td><td>artist</td><td>dancer</td><td>mathematician</td><td>janitor</td></tr><tr><td>carpenter</td><td>mechanic</td><td>actor</td><td>handyman</td><td>musician</td></tr><tr><td>detective</td><td>politician</td><td>entrepreneur</td><td>model</td><td>opera singer</td></tr><tr><td>chief</td><td>lawyer</td><td>farmer</td><td>writer</td><td>librarian</td></tr><tr><td>army</td><td>real estate developer</td><td>broker</td><td>scientist</td><td>butcher</td></tr><tr><td>electrician</td><td>prosecutor</td><td>banker</td><td>cook</td><td>hairdresser</td></tr><tr><td>prisoner</td><td>plumber</td><td>attorney</td><td>boxer</td><td>chess player</td></tr><tr><td>priest</td><td>swimmer</td><td>tennis player</td><td>supervisor</td><td>attendant</td></tr><tr><td>housekeeper</td><td>maid</td><td>producer</td><td>researcher</td><td>midwife</td></tr><tr><td>judge</td><td>lampire</td><td>bartender</td><td>economist</td><td>physicist</td></tr><tr><td>psychologist</td><td>theologian</td><td>salesperson</td><td>physician</td><td>sheriff</td></tr><tr><td>cashier</td><td>assistant</td><td>receptionist</td><td>editor</td><td>engineer</td></tr><tr><td>comedian</td><td>painter</td><td>civil servant</td><td>diplomat</td><td>guitarist</td></tr><tr><td>linguist</td><td>poet</td><td>laborer</td><td>teacher</td><td>delivery man</td></tr><tr><td>realtor</td><td>pilot</td><td>professor</td><td>chemist</td><td>historian</td></tr><tr><td>pensioner</td><td>performing artist</td><td>singer</td><td>secretary</td><td>auditor</td></tr><tr><td>counselor</td><td>designer</td><td>soldier</td><td>journalist</td><td>dentist</td></tr><tr><td>analyst</td><td>nurse</td><td>tailor</td><td>waiter</td><td>author</td></tr><tr><td>architect</td><td>academic</td><td>director</td><td>illustrator</td><td>clerk</td></tr><tr><td>policeman</td><td>chef</td><td>photographer</td><td>drawing</td><td>cleaner</td></tr><tr><td>pharmacist</td><td>pianist</td><td>composer</td><td>handball player</td><td>sociologist</td></tr></table>\n\nTable 15: Targets from origin, gender, and religion categories in our dataset (most of them borrowed from Nadeem et al 2020)."
      ],
      "figure_type": "table_img",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2103.11320_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    }
  ],
  "2109.03952": [
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "image_filename": "2109.03952_page0_fig0.jpg",
      "caption": "(a) Classification model.",
      "context_before": "We consider each feature value as an individual entity (like the words are considered in text-classification) and learn a fixed-size embedding $\\{ e _ { k } \\} _ { k = 1 } ^ { m }$ , $e _ { k } \\in \\mathbb { R } ^ { d ^ { e } }$ for each feature, $\\{ f _ { k } \\} _ { k = 1 } ^ { m }$ . These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is described in Eq. 1. $E = [ e _ { 1 } \\ldots e _ { m } ]$ , $E \\in$ $\\mathbb { R } ^ { d ^ { e } \\times m }$ is the concatenation of all the embeddings, $w \\in \\mathbb { R } ^ { d ^ { e } }$ is a learnable parameter, $r \\in \\mathbb { R } ^ { d ^ { e } }$ denotes the overall sample representation, and $\\alpha \\in \\mathbb { R } ^ { m }$ denotes the attention weights.\n\n$$ H = \\tanh (E) \\quad \\alpha = \\operatorname {s o f t m a x} \\left(w ^ {T} H\\right) \\quad r = \\tanh \\left(E \\alpha^ {T}\\right) \\tag {1} $$\n\nThe resulting representation, $r$ , is passed to the feed-forward layers for classification. In this work, we have used two feedforward layers (See Fig. 1 for overall architecture).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig1.jpg",
      "image_filename": "2109.03952_page0_fig1.jpg",
      "caption": "(b) Attribution framework. Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "context_before": "",
      "context_after": "2.2 Fairness Attribution with Attention Weights\n\nThe aforementioned classification model with the attention mechanism combines input feature embeddings by taking a weighted combination. By manipulating the weights, we can intuitively capture the effects of specific features on the output. To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change. Other works have used similar ideas to understand the effect of attention weights on accuracy and evaluate interpretability of the attention weights by comparing the difference in outcomes in terms of measures such as Jensen-Shannon Divergence (Serrano and Smith 2019) but not for fairness. We are interested in the effect of features on fairness measures. Thus, we measure the difference in fairness of the outcomes based on the desired fairness measure. A large change in fairness measure and a small change in performance of the model would indicate that this feature is mostly responsible for unfairness, and it can be dropped without causing large impacts on performance. The overall framework is shown in Fig. 1. First, the outcomes are recorded with the original attention weights intact (Fig. 1a). Next, attention weights corresponding to a particular feature are zeroed out, and the difference in performance and fairness measures is recorded (Fig. 1b). Based on the observed differences, one may conclude how incorporating this feature contributes to fairness/unfairness.\n\nTo measure the effect of the $k ^ { t h }$ feature on different fairness measures, we consider the difference in the fairness of outcomes of the original model and model with $k ^ { t h }$ feature’s effect removed. For example, for statistical parity difference, we will consider $\\mathrm { S P D } ( \\hat { \\mathbf { y } } _ { o } , \\mathbf { a } ) - \\mathrm { S P D } ( \\hat { \\mathbf { y } } _ { z } ^ { k } , \\mathbf { a } )$ . A negative value will indicate that the $\\dot { k } ^ { t h }$ feature helps mitigate unfairness,",
      "referring_paragraphs": [
        "The resulting representation, $r$ , is passed to the feed-forward layers for classification. In this work, we have used two feedforward layers (See Fig. 1 for overall architecture).",
        "The aforementioned classification model with the attention mechanism combines input feature embeddings by taking a weighted combination. By manipulating the weights, we can intuitively capture the effects of specific features on the output. To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change. Other works have used similar ideas to understand the effect of attention weights on accuracy and evalu",
        "bias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the ",
        "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned.",
        "As shown in Table 1, our post-processing mitigation technique provides lower TRPD while being more accurate, followed by the technique that masks the gendered words before training.",
        "We also used different bias mitigation strategies to compare against our mitigation strategy, such\n\nTable 1: Difference of the True Positive Rates (TPRD) amongst different genders for the dentist and nurse occupations on the biosbias dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_3",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg",
      "image_filename": "2109.03952_page0_fig2.jpg",
      "caption": "For all the baselines described in Sec.",
      "context_before": "4.2 Attention as a Mitigation Technique\n\nAs we have highlighted earlier, understanding how the information within features interact and contribute to the decision making can be used to design effective bias mitigation strategies. One such example was shown in Sec. 4.1. Often realworld datasets have features which cause indirect discrimination, due to which fairness can not be achieved by simply eliminating the sensitive feature from the decision process. Using the attributions derived from our attention-based attribution framework, we propose a post-processing mitigation strategy. Our strategy is to intervene on attention weights as discussed in Sec. 2.3. We first attribute and identify the features responsible for the unfairness of the outcomes, i.e., all the features whose exclusion will decrease the bias compared to the original model’s outcomes and gradually decrease their attention weights to zero as also outlined in Algorithm 1. We do this by first using the whole fraction of the attention weights learned and gradually use less fraction of the weights until the weights are completely zeroed out.\n\nFor all the baselines described in Sec. 3.3, we used the approach outlined in Gupta et al. (2021) for training a downstream classifier and evaluating the accuracy/fairness tradeoffs. The downstream classifier was a 1-hidden-layer MLP with 50 neurons along with ReLU activation function. Our experiments were performed on Nvidia GeForce RTX 2080. Each method was trained with five different seeds, and we report the average accuracy and fairness measure as statistical parity difference (SPD). Results for other fairness notions can be found in the appendix. CVIB, MaxEnt-ARL,",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig3.jpg",
      "image_filename": "2109.03952_page0_fig3.jpg",
      "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
      "context_before": "",
      "context_after": "Adversarial Forgetting and FCRL are designed for statistical parity notion of fairness and are not applicable for other measures like Equalized Odds and Equality of Opportunity. LAFTR can only deal with binary sensitive attributes and thus not applicable for Heritage Health dataset. Notice that our approach does not have these limitations. For our approach, we vary the attention weights and report the resulting fairness-accuracy trade offs.\n\nFig. 3 compares fairness-accuracy trade-offs of different bias mitigation approaches. We desire outcomes to be fairer, i.e., lower values of SPD and to be more accurate, i.e., towards the right. The results show that using attention attributions can indeed be beneficial for reducing bias. Moreover, our mitigation framework based on the manipulation of the attention weights is competitive with state-of-the-art mitigation strategies. However, most of these approaches are specifically designed and optimized to achieve parity and do not provide any interpretability. Our model can not only achieve comparable and competitive results, but it is also able to provide explanation such that the users exactly know what feature and by how much it was manipulated to get the corresponding outcome. Another advantage of our model is that it needs only one round of training. The adjustments to attention weights are made post-training; thus, it is possi-",
      "referring_paragraphs": [
        "found in the appendix. Fig. 2 summarizes our results by visualizing the attributions, which we now discuss.",
        "ble to achieve different trade-offs. Moreover, our approach does not need to know sensitive attributes while training; thus, it could work with other sensitive attributes not known beforehand or during training. Lastly, here we merely focused on mitigating bias (as our goal was to show that the attribution framework can identify problematic features and their removal would result in bias mitigation) and did not focus too much on improving accuracy and achieving the best trade-off curve which can",
        "More details about each of the datasets along with the descriptions of each feature for the Adult dataset can be found $\\mathrm { a t } ^ { 6 }$ and for the Heritage Health dataset can be found at 7. In our qualitative results, we used the feature names as marked in these datasets. If the names or acronyms are unclear kindly reference to the references mentioned for more detailed description for each of the features. Although most of the features in the Adult datasets are self-descriptive, Herit",
        "Figure 2: Results from the synthetic datasets.",
        "Although most of the features in the Adult datasets are self-descriptive, Heritage Health dataset includes some abbreviations that we list in Table 2 for the ease of interpreting each feature’s meaning."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_5",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg",
      "image_filename": "2109.03952_page0_fig4.jpg",
      "caption": "Fig. 3 compares fairness-accuracy trade-offs of different bias mitigation approaches. We desire outcomes to be fairer, i.e., lower values of SPD and to be more accurate, i.e., towards the right. The results show that usi",
      "context_before": "Adversarial Forgetting and FCRL are designed for statistical parity notion of fairness and are not applicable for other measures like Equalized Odds and Equality of Opportunity. LAFTR can only deal with binary sensitive attributes and thus not applicable for Heritage Health dataset. Notice that our approach does not have these limitations. For our approach, we vary the attention weights and report the resulting fairness-accuracy trade offs.\n\nFig. 3 compares fairness-accuracy trade-offs of different bias mitigation approaches. We desire outcomes to be fairer, i.e., lower values of SPD and to be more accurate, i.e., towards the right. The results show that using attention attributions can indeed be beneficial for reducing bias. Moreover, our mitigation framework based on the manipulation of the attention weights is competitive with state-of-the-art mitigation strategies. However, most of these approaches are specifically designed and optimized to achieve parity and do not provide any interpretability. Our model can not only achieve comparable and competitive results, but it is also able to provide explanation such that the users exactly know what feature and by how much it was manipulated to get the corresponding outcome. Another advantage of our model is that it needs only one round of training. The adjustments to attention weights are made post-training; thus, it is possi-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "neighbor_text",
        "anchor_strength": "medium",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig5.jpg",
      "image_filename": "2109.03952_page0_fig5.jpg",
      "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.",
      "context_before": "",
      "context_after": "ble to achieve different trade-offs. Moreover, our approach does not need to know sensitive attributes while training; thus, it could work with other sensitive attributes not known beforehand or during training. Lastly, here we merely focused on mitigating bias (as our goal was to show that the attribution framework can identify problematic features and their removal would result in bias mitigation) and did not focus too much on improving accuracy and achieving the best trade-off curve which can be considered as the current limitation of our work. We manipulated attention weights of all the features that contributed to unfairness irrespective of if they helped maintaining high accuracy or not. However, the trade-off results can be improved by carefully considering the trade-off each feature contributes to with regards to both accuracy and fairness (e.g., using results from Fig. 2) to achieve better trade-off results which can be investigated as a future direction (e.g., removing problematic features that contribute to unfairness only if their contribution to accuracy is below a certain threshold value). The advantage of our work is that this trade-off curve can be controlled by controlling how many features and by how much to be manipulated which is not the case for most existing work.\n\n4.3 Experiments with Non-Tabular Data\n\nIn addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets. To put this to the test, we applied our model to mitigate bias in text-based data. We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset. We compare our approach with the debiasing technique proposed in the original paper (De-Arteaga et al. 2019), which works by masking the gender-related words and then training the model on this masked data. As discussed earlier, such a method is computationally inefficient. It requires re-training the model or creating a new masked dataset, each time it is required to de-",
      "referring_paragraphs": [
        "Fig. 3 compares fairness-accuracy trade-offs of different bias mitigation approaches. We desire outcomes to be fairer, i.e., lower values of SPD and to be more accurate, i.e., towards the right. The results show that using attention attributions can indeed be beneficial for reducing bias. Moreover, our mitigation framework based on the manipulation of the attention weights is competitive with state-of-the-art mitigation strategies. However, most of these approaches are specifically designed and ",
        "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.",
        "Table 3: Adult results on post-processing approach from Hardt et al."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_7",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/bfaa91540e94ca2032a9e2a511b21f164f9c101bead9c08ff4cf3fb2e57f4c2c.jpg",
      "image_filename": "bfaa91540e94ca2032a9e2a511b21f164f9c101bead9c08ff4cf3fb2e57f4c2c.jpg",
      "caption": "Table 1: Difference of the True Positive Rates (TPRD) amongst different genders for the dentist and nurse occupations on the biosbias dataset. Our introduced post-processing method is the most effective in reducing the disparity for both occupations compared to the pre-processing technique.",
      "context_before": "In addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets. To put this to the test, we applied our model to mitigate bias in text-based data. We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset. We compare our approach with the debiasing technique proposed in the original paper (De-Arteaga et al. 2019), which works by masking the gender-related words and then training the model on this masked data. As discussed earlier, such a method is computationally inefficient. It requires re-training the model or creating a new masked dataset, each time it is required to de-\n\nbias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the bias, we used the same measure as in (De-Arteaga et al. 2019) which is based on the equality of opportunity notion of fairness (Hardt et al. 2016) and reported the True Positive Rate Difference (TPRD) for each occupation amongst different genders. As shown in Table 1, our post-processing mitigation technique provides lower TRPD while being more accurate, followed by the technique that masks the gendered words before training. Although both methods reduce the bias compared to a model trained on raw bios without applying any mask or invariance to gendered words, our post-processing method is more effective. Fig. 4 also highlights qualitative differences between models in terms of their most attentive features for the prediction task. As shown in the results, our post-processing technique is able to use more meaningful words, such as R.N. (registered nurse) to predict the outcome label nurse compared to both baselines, while the non-debiased model focuses on gendered words.\n\nFairness. The research in fairness concerns itself with various topics, such as defining fairness metrics, proposing solutions for bias mitigation, and analyzing existing harms in various systems (Mehrabi et al. 2021). In this work, we utilized different metrics that were introduced previously, such as statistical parity (Dwork et al. 2012), equality of opportunity and equalized odds (Hardt et al. 2016), to measure the amount of bias. We also used different bias mitigation strategies to compare against our mitigation strategy, such",
      "context_after": "",
      "referring_paragraphs": [
        "The resulting representation, $r$ , is passed to the feed-forward layers for classification. In this work, we have used two feedforward layers (See Fig. 1 for overall architecture).",
        "The aforementioned classification model with the attention mechanism combines input feature embeddings by taking a weighted combination. By manipulating the weights, we can intuitively capture the effects of specific features on the output. To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change. Other works have used similar ideas to understand the effect of attention weights on accuracy and evalu",
        "bias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the ",
        "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned.",
        "As shown in Table 1, our post-processing mitigation technique provides lower TRPD while being more accurate, followed by the technique that masks the gendered words before training.",
        "We also used different bias mitigation strategies to compare against our mitigation strategy, such\n\nTable 1: Difference of the True Positive Rates (TPRD) amongst different genders for the dentist and nurse occupations on the biosbias dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_8",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg",
      "image_filename": "2109.03952_page0_fig6.jpg",
      "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”",
      "context_before": "",
      "context_after": "as FCRL (Gupta et al. 2021), CVIB (Moyer et al. 2018), MIFR (Song et al. 2019), adversarial forgetting (Jaiswal et al. 2020), MaxEnt-ARL (Roy and Boddeti 2019), and LAFTR (Madras et al. 2018). We also utilized concepts and datasets that were analyzing existing biases in NLP systems, such as (De-Arteaga et al. 2019) which studied the existing biases in NLP systems on the occupation classification task on the bios dataset.\n\nInterpretability. In this work, we introduced an attribution framework based on the attention weights that can analyze fairness and accuracy of the models at the same time and reason about the importance of each feature on fairness and accuracy. There is a body of work in NLP literature that tried to analyze the effect of the attention weights on interpretability of the model (Wiegreffe and Pinter 2019; Jain and Wallace 2019; Serrano and Smith 2019). Other work also utilized attention weights to define an attribution score to be able to reason about how transformer models such as BERT work (Hao et al. 2021). Notice that although Jain and Wallace (2019) claim that attention might not be explanation, a body of work has proved otherwise including (Wiegreffe and Pinter 2019) in which authors directly target the work in Jain and Wallace (2019) and analyze in detail the problems associated with this study. In our work, we also find that attention can be useful and can extract meaningful information which can be beneficial in many aspects. In addition, Vig et al. (2020) analyze the effect of the attention weights in transformer models for bias analysis in language models. However, their approach is different and has a more causal take on investigating the bias. Their study is specific to language models and does not necessarily apply to broader tasks and\n\nexisting fairness definitions. Aside from interpretability and fairness, we utilized concepts from the NLP literature for designing our attention-based model that can be applicable to tabular data (Vaswani et al. 2017; Zhou et al. 2016).",
      "referring_paragraphs": [
        "bias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the ",
        "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts.",
        "<table><tr><td></td><td>Accuracy</td><td>SPD</td><td>Accuracy</td><td>EQOP</td><td>Accuracy</td><td>EQOD</td></tr><tr><td>Attention (Ours)</td><td>0.77 (0.006)</td><td>0.012 (0.003)</td><td>0.81 (0.013)</td><td>0.020 (0.019)</td><td>0.81 (0.021)</td><td>0.027 (0.023)</td></tr><tr><td>Hardt et al.</td><td>0.77 (0.012)</td><td>0.013 (0.005)</td><td>0.83 (0.005)</td><td>0.064 (0.016)</td><td>0.81 (0.007)</td><td>0.047 (0.014)</td></tr></table>\n\nTable 4: Heritage Health results on post-processing approach from Hardt et al."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_9",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/4e1c17194cf759adbc783dca48daf90a0edd66fd43ca02a9aa736aecf203a87b.jpg",
      "image_filename": "4e1c17194cf759adbc783dca48daf90a0edd66fd43ca02a9aa736aecf203a87b.jpg",
      "caption": "Table 2: Some abbreviations used in Heritage Health dataset’s feature names. These abbreviations are listed for clarity of interpreting each feature’s meaning specifically in our qualitative analysis or attribution visualizations.",
      "context_before": "Fig. 9 shows results on a subset of the features from the UCI Adult and Heritage Health datasets (to keep the plots uncluttered and readable, we incorporated the most interesting features in the plot), and provide some intuition about how different features in these datasets contribute to the model fairness and accuracy. While features such as capital gain and capital loss in the UCI Adult dataset are responsible for improving accuracy and reducing bias, we can observe that features such as relationship or marital status, which can be indirectly correlated with the feature sex, have a negative impact on fairness. For the Heritage Health dataset, including the features drugCount ave and dsfs max provide accuracy gains but at the expense of fairness, while including no Claims and no Specialities negatively impact both accuracy and fairness.\n\nA.4 Information on Datasets and Features\n\nMore details about each of the datasets along with the descriptions of each feature for the Adult dataset can be found $\\mathrm { a t } ^ { 6 }$ and for the Heritage Health dataset can be found at 7. In our qualitative results, we used the feature names as marked in these datasets. If the names or acronyms are unclear kindly reference to the references mentioned for more detailed description for each of the features. Although most of the features in the Adult datasets are self-descriptive, Heritage Health dataset includes some abbreviations that we list in Table 2 for the ease of interpreting each feature’s meaning.",
      "context_after": "5https://fairlearn.org\n\n6https://archive.ics.uci.edu/ml/datasets/adult\n\n3RVW\u00103URFHVVLQJ\u0003 2XUV",
      "referring_paragraphs": [
        "found in the appendix. Fig. 2 summarizes our results by visualizing the attributions, which we now discuss.",
        "ble to achieve different trade-offs. Moreover, our approach does not need to know sensitive attributes while training; thus, it could work with other sensitive attributes not known beforehand or during training. Lastly, here we merely focused on mitigating bias (as our goal was to show that the attribution framework can identify problematic features and their removal would result in bias mitigation) and did not focus too much on improving accuracy and achieving the best trade-off curve which can",
        "More details about each of the datasets along with the descriptions of each feature for the Adult dataset can be found $\\mathrm { a t } ^ { 6 }$ and for the Heritage Health dataset can be found at 7. In our qualitative results, we used the feature names as marked in these datasets. If the names or acronyms are unclear kindly reference to the references mentioned for more detailed description for each of the features. Although most of the features in the Adult datasets are self-descriptive, Herit",
        "Figure 2: Results from the synthetic datasets.",
        "Although most of the features in the Adult datasets are self-descriptive, Heritage Health dataset includes some abbreviations that we list in Table 2 for the ease of interpreting each feature’s meaning."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_10",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg",
      "image_filename": "2109.03952_page0_fig7.jpg",
      "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights. Accuracy vs EQOP (UCI Adult)",
      "context_before": "0HJDQ\u0003 ZDV\u0003 ERUQ\u0003 DQG\u0003 UDLVHG\u0003 LQ\u0003 $UOLQJWRQ\u000f\u0003 7H[DV\u0011\u0003 6KH\u0003 JUDGXDWHG\u0003IURP\u0003%D\\ORU\u00038QLYHUVLW\\\u0003/RXLVH\u0003+HUULQJWRQ\u00036FKRRO\u0003 RI\u0003 1XUVLQJ\u0003 LQ\u0003 \u0015\u0013\u0014\u0018\u0003 ZLWK\u0003 KHU\u0003 %DFKHORUV\u0003 RI\u0003 6FLHQFH\u0003 LQ\u0003 1XUVLQJ\u0011\u0003 )ROORZLQJ\u0003 JUDGXDWLRQ\u000f\u0003 VKH\u0003 FRPSOHWHG\u0003 D\u0003 FDUGLDF\u0003 WHOHPHWU\\\u0003LQWHUQVKLS\u0003DW\u00037H[DV\u0003+HDOWK\u00035HVRXUFHV\u0003)RUW\u0003:RUWK\u0011 6KH\u0003 FRQWLQXHG\u0003 WR\u0003 ZRUN\u0003 WKHUH\u0003 IRU\u0003 WZR\u0003 \\HDUV\u0003 WDNLQJ\u0003 FDUH\u0003 RI\u0003 SRVWVXUJLFDO\u0003 FDUGLDF\u0003 SDWLHQWV\u0003 DQG\u0003 WDNLQJ\u0003 DQ\u0003 DFWLYH\u0003 UROH\u0003 LQ\u0003 SDWLHQW\u0003 HGXFDWLRQ\u0003 FRPPLWWHHV\u0011\u0003 6LQFH\u0003 WKDW\u0003 WLPH\u000f\u0003 VKH\u0003 KDV\u0003 OHIW\u0003 WKH\u0003 KRVSLWDO\u0003 VHWWLQJ\u0003 DQG\u0003 KDV\u0003 MRLQHG\u0003 WKH\u0003 SUDFWLFH\u0011\u0003 0HJDQ\u0003 LV\u0003 UHFHQWO\\\u0003 PDUULHG\u0003 DQG\u0003 KDV\u0003 D\u0003 UHVFXH\u0003 &KLKXDKXD\u0003 7HUULHU\u0003 PL[\u0003 SXS\u0011\u0003 ,Q\u0003 KHU\u0003 IUHH\u0003 WLPH\u000f\u0003 VKH\u0003 ORYHV\u0003 VSHQGLQJ\u0003 WLPH\u0003 ZLWK\u0003 IDPLO\\\u0003 DQG\u0003 IULHQGV\u000f\u0003 WU\\LQJ\u0003 QHZ\u0003 IRRGV\u000f\u0003 DQG\u0003 FDWFKLQJ\u0003 XS\u0003 RQ\u0003 D\u0003 JRRG\u0003 ERRN\u0011\n\n0HJDQ\u0003 ZDV\u0003 ERUQ\u0003 DQG\u0003 UDLVHG\u0003 LQ\u0003 $UOLQJWRQ\u000f\u0003 7H[DV\u0011\u0003 6KH\u0003 JUDGXDWHG\u0003IURP\u0003%D\\ORU\u00038QLYHUVLW\\\u0003/RXLVH\u0003+HUULQJWRQ\u00036FKRRO\u0003 RI\u0003 1XUVLQJ\u0003 LQ\u0003 \u0015\u0013\u0014\u0018\u0003 ZLWK\u0003 KHU\u0003 %DFKHORUV\u0003 RI\u0003 6FLHQFH\u0003 LQ\u0003 1XUVLQJ\u0011\u0003 )ROORZLQJ\u0003 JUDGXDWLRQ\u000f\u0003 VKH\u0003 FRPSOHWHG\u0003 D\u0003 FDUGLDF\u0003 WHOHPHWU\\\u0003LQWHUQVKLS\u0003DW\u00037H[DV\u0003+HDOWK\u00035HVRXUFHV\u0003)RUW\u0003:RUWK\u0011\u0003 6KH\u0003 FRQWLQXHG\u0003 WR\u0003 ZRUN\u0003 WKHUH\u0003 IRU\u0003 WZR\u0003 \\HDUV\u0003 WDNLQJ\u0003 FDUH\u0003 RI\u0003 SRVWVXUJLFDO\u0003 FDUGLDF\u0003 SDWLHQWV\u0003 DQG\u0003 WDNLQJ\u0003 DQ\u0003 DFWLYH\u0003 UROH\u0003 LQ\u0003 SDWLHQW\u0003 HGXFDWLRQ\u0003 FRPPLWWHHV\u0011\u0003 6LQFH\u0003 WKDW\u0003 WLPH\u000f\u0003 VKH\u0003 KDV\u0003 OHIW\u0003 WKH\u0003 KRVSLWDO\u0003 VHWWLQJ\u0003 DQG\u0003 KDV\u0003 MRLQHG\u0003 WKH\u0003 SUDFWLFH\u0011\u0003 0HJDQ\u0003 LV\u0003 UHFHQWO\\\u0003 PDUULHG\u0003 DQG\u0003 KDV\u0003 D\u0003 UHVFXH\u0003 &KLKXDKXD\u0003 7HUULHU\u0003 PL[\u0003 SXS\u0011\u0003 ,Q\u0003 KHU\u0003 IUHH\u0003 WLPH\u000f\u0003 VKH\u0003 ORYHV\u0003 VSHQGLQJ\u0003 WLPH\u0003 ZLWK\u0003 IDPLO\\\u0003 DQG\u0003 IULHQGV\u000f\u0003 WU\\LQJ\u0003 QHZ\u0003 IRRGV\u000f\u0003 DQG\u0003 FDWFKLQJ\u0003 XS\u0003 RQ\u0003 D\u0003 JRRG ERRN\u0011\n\n0HJDQ\u0003 ZDV\u0003 ERUQ\u0003 DQG\u0003 UDLVHG\u0003 LQ\u0003 $UOLQJWRQ\u000f\u0003 7H[DV\u0011\u0003 6KH\u0003 JUDGXDWHG\u0003IURP\u0003%D\\ORU\u00038QLYHUVLW\\\u0003/RXLVH\u0003+HUULQJWRQ\u00036FKRRO\u0003 RI\u0003 1XUVLQJ\u0003 LQ\u0003 \u0015\u0013\u0014\u0018\u0003 ZLWK\u0003 KHU\u0003 %DFKHORUV\u0003 RI\u0003 6FLHQFH\u0003 LQ\u0003 1XUVLQJ\u0011\u0003 )ROORZLQJ\u0003 JUDGXDWLRQ\u000f\u0003 VKH\u0003 FRPSOHWHG\u0003 D\u0003 FDUGLDF\u0003 WHOHPHWU\\\u0003LQWHUQVKLS\u0003DW\u00037H[DV\u0003+HDOWK\u00035HVRXUFHV\u0003)RUW\u0003:RUWK\u0011\u0003 6KH\u0003 FRQWLQXHG\u0003 WR\u0003 ZRUN\u0003 WKHUH\u0003 IRU\u0003 WZR\u0003 \\HDUV\u0003 WDNLQJ\u0003 FDUH\u0003 RI\u0003 SRVWVXUJLFDO\u0003 FDUGLDF\u0003 SDWLHQWV\u0003 DQG\u0003 WDNLQJ\u0003 DQ\u0003 DFWLYH\u0003 UROH\u0003 LQ\u0003 SDWLHQW\u0003 HGXFDWLRQ\u0003 FRPPLWWHHV\u0011\u0003 6LQFH\u0003 WKDW\u0003 WLPH\u000f\u0003 VKH\u0003 KDV\u0003 OHIW\u0003 WKH\u0003 KRVSLWDO\u0003 VHWWLQJ\u0003 DQG\u0003 KDV\u0003 MRLQHG\u0003 WKH\u0003 SUDFWLFH\u0011\u0003 0HJDQ\u0003 LV\u0003 UHFHQWO\\\u0003 PDUULHG\u0003 DQG\u0003 KDV\u0003 D\u0003 UHVFXH\u0003 &KLKXDKXD\u0003 7HUULHU\u0003 PL[\u0003 SXS\u0011\u0003 ,Q\u0003 KHU\u0003 IUHH\u0003 WLPH\u000f\u0003 VKH\u0003 ORYHV\u0003 VSHQGLQJ\u0003 WLPH\u0003 ZLWK\u0003 IDPLO\\\u0003 DQG\u0003 IULHQGV\u000f\u0003 WU\\LQJ\u0003 QHZ\u0003 IRRGV\u000f\u0003 DQG\u0003 FDWFKLQJ\u0003 XS\u0003 RQ\u0003 D\u0003 JRRG\u0003 ERRN\u0011",
      "context_after": "",
      "referring_paragraphs": [
        "We also included some additional qualitative results from the experiments on non-tabular data in Fig. 5.",
        "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   \nAccuracy vs EQOP (UCI Adult)"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_11",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg",
      "image_filename": "2109.03952_page0_fig8.jpg",
      "caption": "Accuracy vs EQOP (Heritage Health) Figure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Accuracy vs EQOP (Heritage Health)   \nFigure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_12",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/bc9f023086698d4e8b8597bed4c45c3e19333e4b674ef0374d2228cbd45b123b.jpg",
      "image_filename": "bc9f023086698d4e8b8597bed4c45c3e19333e4b674ef0374d2228cbd45b123b.jpg",
      "caption": "Table 3: Adult results on post-processing approach from Hardt et al. vs our attention method when all problematic features are zeroed out.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Fig. 3 compares fairness-accuracy trade-offs of different bias mitigation approaches. We desire outcomes to be fairer, i.e., lower values of SPD and to be more accurate, i.e., towards the right. The results show that using attention attributions can indeed be beneficial for reducing bias. Moreover, our mitigation framework based on the manipulation of the attention weights is competitive with state-of-the-art mitigation strategies. However, most of these approaches are specifically designed and ",
        "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.",
        "Table 3: Adult results on post-processing approach from Hardt et al."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_13",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/043bba14147b424e218e3df7811e820efdc24d1fa8e88d8647807af4f5e22500.jpg",
      "image_filename": "043bba14147b424e218e3df7811e820efdc24d1fa8e88d8647807af4f5e22500.jpg",
      "caption": "Table 4: Heritage Health results on post-processing approach from Hardt et al. vs our attention method when all problematic features are zeroed out.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "bias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the ",
        "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts.",
        "<table><tr><td></td><td>Accuracy</td><td>SPD</td><td>Accuracy</td><td>EQOP</td><td>Accuracy</td><td>EQOD</td></tr><tr><td>Attention (Ours)</td><td>0.77 (0.006)</td><td>0.012 (0.003)</td><td>0.81 (0.013)</td><td>0.020 (0.019)</td><td>0.81 (0.021)</td><td>0.027 (0.023)</td></tr><tr><td>Hardt et al.</td><td>0.77 (0.012)</td><td>0.013 (0.005)</td><td>0.83 (0.005)</td><td>0.064 (0.016)</td><td>0.81 (0.007)</td><td>0.047 (0.014)</td></tr></table>\n\nTable 4: Heritage Health results on post-processing approach from Hardt et al."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "table",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "content_list"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_14",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig9.jpg",
      "image_filename": "2109.03952_page0_fig9.jpg",
      "caption": "Accuracy vs EQOD (UCI Adult)",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_15",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg",
      "image_filename": "2109.03952_page0_fig10.jpg",
      "caption": "Accuracy vs EQOD (Heritage Health) Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_16",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig11.jpg",
      "image_filename": "2109.03952_page0_fig11.jpg",
      "caption": "Top Problematic Features from Adult",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_17",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig12.jpg",
      "image_filename": "2109.03952_page0_fig12.jpg",
      "caption": "Top Problematic Features from Health Figure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In addition, we show how different features contribute differently under different fairness notions. Fig. 8 demonstrates the top three features that contribute to unfairness the most along with the percentages of the fairness improvement upon their removal for each of the fairness notions. As observed from the results, while equality of opportunity and equalized odds are similar in terms of their problematic features, statistical parity has different trends. This is also expected as equality of ",
        "Top Problematic Features from Health   \nFigure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_18",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg",
      "image_filename": "2109.03952_page0_fig13.jpg",
      "caption": "",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.1,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "none",
        "anchor_strength": "weak",
        "image_source": "markdown_fallback"
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_19",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig14.jpg",
      "image_filename": "2109.03952_page0_fig14.jpg",
      "caption": "Figure 9: Results from the real-world datasets. Note that in our $\\hat { y } _ { z }$ notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema, but the feature names are more universal and discriptive for this case. Labels on the points represent the feature name that was removed (zeroed out) according to our $\\hat { y } _ { z }$ notation. The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Fig. 9 shows results on a subset of the features from the UCI Adult and Heritage Health datasets (to keep the plots uncluttered and readable, we incorporated the most interesting features in the plot), and provide some intuition about how different features in these datasets contribute to the model fairness and accuracy. While features such as capital gain and capital loss in the UCI Adult dataset are responsible for improving accuracy and reducing bias, we can observe that features such as rela",
        "Figure 9: Results from the real-world datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {
        "source": "content_list",
        "source_file": "2109.03952_content_list.json",
        "source_type": "image",
        "caption_source": "content_list",
        "anchor_strength": "strong",
        "image_source": "markdown_fallback"
      }
    }
  ]
}