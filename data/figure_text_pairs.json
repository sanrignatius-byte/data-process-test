{
  "1104.3913": [
    {
      "doc_id": "1104.3913",
      "figure_id": "1104.3913_fig_1",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg",
      "image_filename": "1104.3913_page0_fig0.jpg",
      "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$",
      "context_before": "Similar to the proof of Theorem 3.3, we may interpret this program as a flow problem. The variables $f ( x , y ) , g ( x , y )$ represent a nonnegative flow from $x$ to $y$ and $\\epsilon _ { x }$ are slack variables. Note that the variables $\\epsilon _ { x }$ are unrestricted as they correspond to an equality constraint. The first constraint requires that $x$ \u000fhas at least $S \\left( x \\right) - T \\left( x \\right)$ outgoing units of flow in $f .$ The RHS of the constraints states that the penalty for receiving a unit of flow from $y$ is $e ^ { d ( x , y ) }$ . However, it is no longer clear that we can get rid of the variables $\\epsilon _ { x } , g ( x , y )$\n\nOpen Question 3.1. Can we achieve a tight characterization of when $( D _ { \\infty } , d )$ -Lipschitz implies statistical parity?\n\n[Section: 4 Fair Affirmative Action]\n\nIn this section, we explore how to implement what may be called fair affirmative action. Indeed, a typical question when we discuss fairness is, “What if we want to ensure statistical parity between two groups S and T but members of S are less likely to be “qualified”? In Section 3, we have seen that when $S$ and $T$ are “similar” then the Lipschitz condition implies statistical parity. Here we consider the complementary case where $S$ and $T$ are very different and imposing statistical parity corresponds to preferential treatment. This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.\n\nFor simplicity, let $T = S ^ { c }$ . Assume $| S | / | T \\cup S | = 1 / 1 0$ , so S is only $10 \\%$ of the population. Suppose that our task-specific metric partitions $S \\cup T$ into two groups, call them $G _ { 0 }$ and $G _ { 1 }$ , where members of $G _ { i }$ are very close to one another and very far from all members of $G _ { 1 - i }$ . Let $S _ { i }$ , respectively $T _ { i }$ , denote the intersection $S \\cap G _ { i }$ , respectively $T \\cap G _ { i }$ , for $i = 0$ 1. Finally, assume $| S _ { 0 } | = | T _ { 0 } | = 9 | S | / 1 0 .$ Thus, $G _ { 0 }$ contains less than $20 \\%$ of the total population, and is equally divided between S and $T$ .\n\nThe Lipschitz condition requires that members of each $G _ { i }$ be treated similarly to one another, but there is no requirement that members of $G _ { 0 }$ be treated similarly to members of $G _ { 1 }$ . The treatment of",
      "context_after": "Remark 5.1. If $( V , d )$ is not well-separated, then for every constant $\\epsilon > 0$ it must contain a wellseparated subset $V ^ { \\prime } \\subseteq V$ such that every point $x \\in V$ has a neighbor $x ^ { \\prime } \\in V ^ { \\prime }$ such that $d ( x , x ^ { \\prime } ) \\leq \\epsilon$ A Lipschitz mapping $M ^ { \\prime }$ defined on $V ^ { \\prime }$ naturally extends to all of $V$ by putting $M ( x ) = M ^ { \\prime } ( x ^ { \\prime } )$\n\nwhere $x ^ { \\prime }$ is the nearest neighbor of $x$ in $V ^ { \\prime }$ It is easy to see that the expected loss of $M$ is only an additive $\\epsilon$ worse than that of $M ^ { \\prime }$ . Similarly, the Lipschitz condition deteriorates by an additive $2 \\epsilon$ ， i.e., $D _ { \\infty } ( M ( x ) , M ( y ) ) \\leq d ( x , y ) + 2 \\epsilon$ Indeed, denoting the nearest neighbors in $V ^ { \\prime }$ of $x , y$ by $x ^ { \\prime } , y ^ { \\prime }$ , yrespectively, we have $D _ { \\infty } ( M ( x ) , M ( y ) ) = D _ { \\infty } ( M ^ { \\prime } ( x ^ { \\prime } ) , M ^ { \\prime } ( y ^ { \\prime } ) ) \\leq d ( x ^ { \\prime } , y ^ { \\prime } ) \\leq d ( x , y ) + d ( x , x ^ { \\prime } ) + d ( y , y ^ { \\prime } ) \\leq \\frac { 1 - \\gamma } { 2 } .$ $d ( x , y ) + 2 \\epsilon$ , y Here, we used the triangle inequality.\n\nThe proof of Theorem 5.2 shows an exponential dependence on the doubling dimension $k$ of the underlying space in the error of the exponential mechanism. The next theorem shows that the loss of any Lipschitz mapping has to scale at least linearly with $k$ The proof follows from a packing argument .similar to that in [HT10]. The argument is slightly complicated by the fact that we need to give a lower bound on the average error (over $x \\in V .$ ) of any mechanism.\n\nDefinition 5.4. A set $B \\subseteq V$ is called an $R$ -packing if $d ( x , y ) > R$ for all $x , y \\in B$\n\nHere we give a lower bound using a metric space that may not be well-separated. However, following Remark 5.1, this also shows that any mapping defined on a well-separated subset of the metric space must have large error up to a small additive loss.\n\nTheorem 5.3. For every $k \\geq 2$ and every large enough $n \\geq n _ { 0 } ( k )$ there exists an n-point metric space of doubling dimension $O ( k )$ such that any $( D _ { \\infty } , d )$ -Lipschitz mapping $M \\colon V \\to \\Delta ( V )$ must satisfy\n\nProof. Construct $V$ by randomly picking $n$ points from a $r$ -dimensional sphere of radius $1 0 0 k$ We will choose $n$ sufficiently large and $r = O ( k )$ Endow $V$ with the Euclidean distance $d$ Since $V \\subseteq \\mathbb { R } ^ { r }$ and $r = O ( k )$ it follows from a well-known fact that the doubling dimension of $( V , d )$ is bounded by $O ( k )$\n\nClaim 5.4. Let X be the distribution obtained by choosing a random $x \\in V$ and outputting a random $y \\in B ( x , k )$ Then, for sufficiently large n the distribution $X$ has statistical distance at most 1 100 from the uniform distribution over V\n\nProof. The claim follows from standard arguments showing that for large enough n every point $y \\in V$ is contained in approximately equally many balls of radius $k$ \u0003\n\nLet M denote any $( D _ { \\infty } , d )$ -Lipschitz mapping and denote its error on a point $x \\in V$ by\n\nand put $R = \\mathbb { E } _ { x \\in V } R ( x )$ Let $G = \\{ x \\in V \\colon R ( x ) \\leq 2 R \\}$ By Markov’s inequality $| G | \\geq n / 2$\n\nNow, pick $x \\in V$ . .uniformly at random and choose a set $P _ { x }$ of $2 ^ { 2 k }$ / .random points (with replacement) from $B ( x , k )$ For sufficiently large dimension $r = O ( k )$ it follows from concentration of measure on the sphere that $P _ { x }$ forms a $k / 2$ -packing with probability, say, 1 10\n\n/Moreover, by Claim 5.4, for random $x \\in V$ and random $y \\in B ( x , k )$ the probability that $y \\in G$ is at least $| G | / | V | - 1 / 1 0 0 \\geq 1 / 3$ Hence, with high probability,",
      "referring_paragraphs": [
        "This is a cardinal question, which we examine with a concrete example illustrated in Figure 2.",
        "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$\n\nmembers of S , on average, may therefore be very different from the treatment, on average, of members of $T$ , since members of S are over-represented in $G _ { 0 }$ and under-represented in $G _ { 1 }$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1104.3913_page0_fig1.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1306.5204": [
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg",
      "image_filename": "1306.5204_page0_fig0.jpg",
      "caption": "Figure 1: Tag cloud of top terms from each dataset.",
      "context_before": "Twitter is a social media giant famous for the exchange of short, 140-character messages called “tweets”. In the scientific community, the microblogging site is known for openness in sharing its data. It provides a glance into its millions of users and billions of tweets through a “Streaming API” which provides a sample of all tweets matching some parameters preset by the API user. The API service has been used by many researchers, companies, and governmental institutions that want to extract knowledge in accordance with a diverse array of questions pertaining to social media. The essential drawback of the Twitter API is the lack of documentation concerning what and how much data users get. This leads researchers to question whether the sampled data is a valid representation of the overall activity on Twitter. In this work we embark on answering this question by comparing data collected using Twitter’s sampled API service with data collected using the full, albeit costly, Firehose stream that includes every single published tweet. We compare both datasets using common statistical metrics as well as metrics that allow us to compare topics, networks, and locations of tweets. The results of our work will help researchers and practitioners understand the implications of using the Streaming API.\n\n[Section: Introduction]\n\nTwitter is a microblogging site where users exchange short, 140-character messages called “tweets”. Ranking as the 10th most popular site in the world by the Alexa rank in January of $2 0 \\dot { 1 } 3 \\dot { 1 }$ , the site boasts 500 million registered users publishing 400 million tweets per day. Twitter’s platform for rapid communication is said to be a vital communication platform in recent events including Hurricane Sandy2, the Arab Spring of 2011 (Campbell 2011), and several political campaigns (Tumasjan et al. 2010; Gayo-Avello, Metaxas, and Mustafaraj 2011). As a result, Twitter’s data has been coveted by both computer and social scientists to better understand human behavior and dynamics.\n\nSocial media data is often difficult to obtain, with most social media sites restricting access to their data. Twitter’s policies lie opposite to this. The “Twitter Streaming $\\mathbf { A P I } ^ { \\mathbf { \\vec { \\sigma } } , \\vec { \\mathbf { \\sigma } } _ { 3 } ^ { - } }$ is a capability provided by Twitter that allows anyone to retrieve at most a $1 \\%$ sample of all the data by providing some parameters. According to the documentation, the sample will return at most $1 \\%$ of all the tweets produced on Twitter at a given time. Once the number of tweets matching the given parameters eclipses $1 \\%$ of all the tweets on Twitter, Twitter will begin to sample the data returned to the user. The methods that Twitter employs to sample this data is currently unknown. The Streaming API takes three parameters: keywords (words, phrases, or hashtags), geographical boundary boxes, and user ID.\n\nOne way to overcome the $1 \\%$ limitation is to use the Twitter Firehose—a feed provided by Twitter that allows access to $100 \\%$ of all public tweets. A very substantial drawback of the Firehose data is the restrictive cost. Another drawback is the sheer amount of resources required to retain the Firehose data (servers, network availability, and disk space). Consequently, researchers as well as decision makers in companies and government institutions are forced to decide between two versions of the API: the freely-available but limited Streaming, and the very expensive but comprehensive Firehose version. To the best of our knowledge, no research has been done to assist those researchers and decision makers by answering the following: How does the use of the Streaming API affect common measures and metrics performed on the data? In this article we answer this question from different perspectives.\n\nWe begin the analysis by employing classic statistical measures commonly used to compare two sets of data. Based on unique characteristics of tweets, we design and conduct additional comparative analysis. By extracting topics using a frequently used algorithm, we compare how topics differ between the two datasets. As tweets are linked data, we perform network measures of the two datasets. Because tweets can be geo-tagged, we compare the geographical distribution of geolocated tweets to better understand how sampling affects aggregated geographic information.",
      "context_after": "[Section: Related Work]\n\nTwitter’s Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010), among others. Researchers’ reliance upon this data source is significant, and these examples only provide a cursory glance at the tip of the iceberg. Due to the widespread use of Twitter’s Streaming API in various scientific fields, it is important that we understand how using a sub-sample of the data generated affects these results.\n\nFrom a statistical point of view, the “law of large numbers” (mean of a sample converges to the mean of the entire population) and the Glivenko-Cantelli theorem (the unknown distribution $X$ of an attribute in a population can be approximated with the observed distribution $x$ ) guarantee satisfactory results from sampled data when the randomly selected sub-sample is big enough. From network algorithmic (Wasserman and Faust 1994) perspective the question is more complicated. Previous efforts have delved into the topic of network sampling and how working with a restricted set of data can affect common network measures. The problem was studied earlier in (Granovetter 1976), where the author proposes an algorithm to sample networks in a way that allows one to estimate basic network properties. More recently, (Costenbader and Valente 2003) and (Borgatti, Carley, and Krackhardt 2006) have studied the affect of data error on common network centrality measures by randomly deleting and adding nodes and edges. The authors discover that centrality measures are usually most resilient on dense networks. In (Kossinets 2006), the authors study global properties of simulated random graphs to better understand data error in social networks. (Leskovec and Faloutsos 2006)\n\nproposes a strategy for sampling large graphs to preserve network measures.\n\nIn this work we compare the datasets by analyzing facets commonly used in the literature. We start by comparing the top hashtags found in the tweets, a feature of the text commonly used for analysis. In (Tsur and Rappoport 2012), the authors try to predict the magnitude of the number of tweets mentioning a particular hashtag. Using a regression model trained with features extracted from the text, the authors find that the content of the idea behind the tag is vital to the count of the tweets employing it. Tweeting a hashtag automatically adds a tweet to a page showing tweets published by other tweeters containing that hashtag. In (Yang et al. 2012), the authors find that this communal property of hashtags along with the meaning of the tag itself drive the adoption of hashtags on Twitter. (De Choudhury et al. 2010) studies the propagation patterns of URLs on sampled Twitter data.\n\nTopic analysis can also be used to better understand the content of tweets. (Kireyev, Palen, and Anderson 2009) drills the problem down to disaster-related tweets, discovering two main types of topics: informational and emotional. Finally, (Yin et al. 2011; Hong et al. 2012; Pozdnoukhov and Kaiser 2011) all study the problem of identifying topics in geographical Twitter datasets, proposing models to extract topics relevant to different geographical areas in the data. (Joseph, Tan, and Carley 2012) studies how the topics users discuss drive their geolocation.\n\nGeolocation has become a prominent area in the study of social media data. In (Wakamiya, Lee, and Sumiya 2011) the authors try to classify towns based upon the content of the geotagged tweets that originate from within the town. (De Longueville, Smith, and Luraschi 2009) studies Twitter’s use as a sensor for disaster information by studying the geographical properties of users tweets. The authors discover that Twitter’s information is accurate in the later stages of a crisis for information dissemination and retrieval.",
      "referring_paragraphs": [
        "Figure 1: Tag cloud of top terms from each dataset.",
        "To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.",
        "We start by grouping the locations of tweets by continent and can find a strong Asian bias due to the boundary box we used to collect the data from both sources, shown in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) Firehose",
        "(b) Streaming API",
        "1306.5204_page0_fig1.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg",
      "image_filename": "1306.5204_page0_fig2.jpg",
      "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
      "context_before": "[Section: Related Work]\n\nTwitter’s Streaming API has been used throughout the domain of social media and network analysis to generate understanding of how users behave on these platforms. It has been used to collect data for topic modeling (Hong and Davison 2010; Pozdnoukhov and Kaiser 2011), network analysis (Sofean and Smith 2012), and statistical analysis of content (Mathioudakis and Koudas 2010), among others. Researchers’ reliance upon this data source is significant, and these examples only provide a cursory glance at the tip of the iceberg. Due to the widespread use of Twitter’s Streaming API in various scientific fields, it is important that we understand how using a sub-sample of the data generated affects these results.\n\nFrom a statistical point of view, the “law of large numbers” (mean of a sample converges to the mean of the entire population) and the Glivenko-Cantelli theorem (the unknown distribution $X$ of an attribute in a population can be approximated with the observed distribution $x$ ) guarantee satisfactory results from sampled data when the randomly selected sub-sample is big enough. From network algorithmic (Wasserman and Faust 1994) perspective the question is more complicated. Previous efforts have delved into the topic of network sampling and how working with a restricted set of data can affect common network measures. The problem was studied earlier in (Granovetter 1976), where the author proposes an algorithm to sample networks in a way that allows one to estimate basic network properties. More recently, (Costenbader and Valente 2003) and (Borgatti, Carley, and Krackhardt 2006) have studied the affect of data error on common network centrality measures by randomly deleting and adding nodes and edges. The authors discover that centrality measures are usually most resilient on dense networks. In (Kossinets 2006), the authors study global properties of simulated random graphs to better understand data error in social networks. (Leskovec and Faloutsos 2006)\n\nproposes a strategy for sampling large graphs to preserve network measures.\n\nIn this work we compare the datasets by analyzing facets commonly used in the literature. We start by comparing the top hashtags found in the tweets, a feature of the text commonly used for analysis. In (Tsur and Rappoport 2012), the authors try to predict the magnitude of the number of tweets mentioning a particular hashtag. Using a regression model trained with features extracted from the text, the authors find that the content of the idea behind the tag is vital to the count of the tweets employing it. Tweeting a hashtag automatically adds a tweet to a page showing tweets published by other tweeters containing that hashtag. In (Yang et al. 2012), the authors find that this communal property of hashtags along with the meaning of the tag itself drive the adoption of hashtags on Twitter. (De Choudhury et al. 2010) studies the propagation patterns of URLs on sampled Twitter data.\n\nTopic analysis can also be used to better understand the content of tweets. (Kireyev, Palen, and Anderson 2009) drills the problem down to disaster-related tweets, discovering two main types of topics: informational and emotional. Finally, (Yin et al. 2011; Hong et al. 2012; Pozdnoukhov and Kaiser 2011) all study the problem of identifying topics in geographical Twitter datasets, proposing models to extract topics relevant to different geographical areas in the data. (Joseph, Tan, and Carley 2012) studies how the topics users discuss drive their geolocation.\n\nGeolocation has become a prominent area in the study of social media data. In (Wakamiya, Lee, and Sumiya 2011) the authors try to classify towns based upon the content of the geotagged tweets that originate from within the town. (De Longueville, Smith, and Luraschi 2009) studies Twitter’s use as a sensor for disaster information by studying the geographical properties of users tweets. The authors discover that Twitter’s information is accurate in the later stages of a crisis for information dissemination and retrieval.",
      "context_after": "[Section: The Data]\n\nFrom December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown in Figure 2. One of the more interesting results in this dataset is that as the data in the Firehose spikes, the Streaming API coverage is reduced. One possible explanation for this phenomenon could be that due to the Western holidays observed at this time, activity on Twitter may have reduced causing the $1 \\%$ threshold to go down.\n\nOne of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the Firehose on any given day. While this is much better than just $1 \\%$ of the tweets promised by the Streaming API, we have no reference point for the data in the tweets we received.\n\nThe most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.\n\nTable 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.   \n\n<table><tr><td>Keywords</td><td>Geoboxes</td><td>Users</td></tr><tr><td>#syria, #assad, #aleppovolcano, #alawite, #homs, #hama, #tartous, #idlib, #damascus, #daraa, #aleppo, ####*, #houla</td><td>(32.8, 35.9), (37.3, 42.3)</td><td>@SyrianRevo</td></tr></table>\n\n* Arabic word for “Syria”",
      "referring_paragraphs": [
        "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.",
        "The raw counts of tweets we received each day from both sources are shown in Figure 2.",
        "Table 2 shows the results for the average of 28 daily networks, the min-max range, as well as the aggregated network including all 28 days."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg",
      "image_filename": "1306.5204_page0_fig3.jpg",
      "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
      "context_before": "[Section: The Data]\n\nFrom December 14th, 2011 - January 10th, 2012 we collected tweets from the Twitter Firehose matching any of the keywords, geographical bounding boxes, and users in Table 1. During the same time period, we collected tweets from the Streaming API using TweetTracker (Kumar et al. 2011) with exactly the same parameters. During the time we collected 528,592 tweets from the Streaming API and 1,280,344 tweets from the Firehose. The raw counts of tweets we received each day from both sources are shown in Figure 2. One of the more interesting results in this dataset is that as the data in the Firehose spikes, the Streaming API coverage is reduced. One possible explanation for this phenomenon could be that due to the Western holidays observed at this time, activity on Twitter may have reduced causing the $1 \\%$ threshold to go down.\n\nOne of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on Twitter data. Here we define coverage as the ratio of data from the Streaming API to data from the Firehose. To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3. In this period of time the Streaming API receives, on average, $4 3 . 5 \\%$ of the data available on the Firehose on any given day. While this is much better than just $1 \\%$ of the tweets promised by the Streaming API, we have no reference point for the data in the tweets we received.\n\nThe most striking observation is the range of coverage rates (see Figure 3). Increase of absolute importance (more global awareness) or relative importance (the overall number of tweets decreases) result in lower coverage as well as fewer tweets. To give the reader a sense for the top words in both datasets, we include tag clouds for the top words in the Streaming API and the Firehose, shown in Figure 1.\n\nTable 1: Parameters used to collect data from Syria. Coordinates below the boundary box indicate the Southwest and Northeast corner, respectively.   \n\n<table><tr><td>Keywords</td><td>Geoboxes</td><td>Users</td></tr><tr><td>#syria, #assad, #aleppovolcano, #alawite, #homs, #hama, #tartous, #idlib, #damascus, #daraa, #aleppo, ####*, #houla</td><td>(32.8, 35.9), (37.3, 42.3)</td><td>@SyrianRevo</td></tr></table>\n\n* Arabic word for “Syria”",
      "context_after": "[Section: Statistical Measures]\n\nWe investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose. We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic. We continue to extract topics from the text, matching topical content and comparing topical distribution to better understand how sampling affects the results of this common process performed on Twitter data. In both cases we compare our streaming data to random datasets obtained by sampling the data obtained through the Firehose.\n\n[Section: Top Hashtag Analysis]",
      "referring_paragraphs": [
        "To better understand the coverage of the Streaming API for each day, we construct a box-and-whisker plot to visualize the distribution of daily coverage, shown in Figure 3.",
        "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.",
        "To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th).",
        "These metrics are reported in Table 3 and are calculated as follows.",
        "After removing these tweets,\n\nTable 3: Comparison of Network-Level Social Network Analysis Metrics."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg",
      "image_filename": "1306.5204_page0_fig4.jpg",
      "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
      "context_before": "We investigate the statistical properties of the two datasets with the intent of understanding how well the characteristics of the sampled data match those of the Firehose. We begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic. We continue to extract topics from the text, matching topical content and comparing topical distribution to better understand how sampling affects the results of this common process performed on Twitter data. In both cases we compare our streaming data to random datasets obtained by sampling the data obtained through the Firehose.\n\n[Section: Top Hashtag Analysis]\n\nHashtags are an important communication device on Twitter. Users employ them to annotate the content they produce, allowing for other users to find their tweets and to facilitate interaction on the platform. Also, adding a hashtag to a tweet is equivalent to joining a community of users discussing the same topic (Yang et al. 2012). In addition, hashtags are also used by Twitter to calculate the trending topics of the day, which encourages the user to post in these communities.\n\nRecently, hashtags have become an important part of Twitter analysis (Efron 2010; Tsur and Rappoport 2012; Recuero and Araujo 2012). For both the purpose of community formation and trend analysis it is important that our Streaming dataset convey the same importance for hashtags as the Firehose data. Here we compare the top hashtags in",
      "context_after": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total. The $\\tau _ { \\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.\n\nTo understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th). The results of this experiment are shown in Figure 4. Here we see mixed results at small values of $n$ , indicating that the Streaming data may not be good for finding the top hashtags. At larger values of $n$ , we see that the Streaming API does a better job of estimating the top hashtags in the Firehose data.\n\nComparison with Random Samples After seeing the results from the previous section, we are left to wonder if the results are an artifact of using the Streaming API or if we",
      "referring_paragraphs": [
        "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .",
        "The results of this experiment are shown in Figure 4.",
        "%</td><td>84.6%</td><td>95.5%</td><td>82.5%</td><td>92.9%</td></tr><tr><td>Clust.Cofef.</td><td>0.029</td><td>0.053</td><td>0.033</td><td>0.050</td></tr><tr><td>DCinCentr.</td><td>0.059</td><td>0.042</td><td>0.085</td><td>0.043</td></tr><tr><td>BCCentr.</td><td>0.010</td><td>0.053</td><td>0.010</td><td>0.050</td></tr><tr><td>PReach Centr.</td><td>0.130</td><td>0.240</td><td>0.156</td><td>0.205</td></tr></table>\n\nTable 4: Geotagged Tweet Location by Continent."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_6",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg",
      "image_filename": "1306.5204_page0_fig5.jpg",
      "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.",
      "context_before": "where $P _ { C }$ is the set of concordant pairs, $P _ { D }$ is the set of discordant pairs, $T _ { F }$ is the set of ties in the Firehose data, but not in the Streaming data, $T _ { S }$ is the number of ties found in the Streaming data, but not in the Firehose, and $n$ is the number of pairs in total. The $\\tau _ { \\beta }$ value ranges from -1, perfect negative correlation, to 1, perfect positive correlation.\n\nTo understand the relationship between $n$ and the resulting correlation, $\\tau _ { \\beta }$ , we construct a chart showing the value of $\\tau _ { \\beta }$ for $n$ between 10 and 1000 in steps of 10. To get an accurate representation of the differences in correlation at each level of Streaming coverage, we select five days with different levels of coverage as motivated by Figure 3: The minimum (December 27th), lower quartile (December 24th), median (December 29th), upper quartile (December 18th), and the maximum (December 19th). The results of this experiment are shown in Figure 4. Here we see mixed results at small values of $n$ , indicating that the Streaming data may not be good for finding the top hashtags. At larger values of $n$ , we see that the Streaming API does a better job of estimating the top hashtags in the Firehose data.\n\nComparison with Random Samples After seeing the results from the previous section, we are left to wonder if the results are an artifact of using the Streaming API or if we",
      "context_after": "[Section: Topic Analysis]\n\nTopic models are statistical models which discover topics in a corpus. Topic modeling is especially useful in large data, where it is too cumbersome to extract the topics manually. Due to the large volume of tweets published on Twitter, topic modeling has become central to many content-based studies using Twitter data (Kireyev, Palen, and Anderson 2009; Pozdnoukhov and Kaiser 2011; Hong et al. 2012; Yin et al. 2011; Chae et al. 2012). We compare the topics drawn from the Streaming data with those drawn from the Firehose data using a widely-used topic modeling algorithm, latent Dirichlet allocation (LDA) (Blei, $\\mathrm { N g }$ , and Jordan 2003). Latent Dirichlet allocation is an algorithm for the automated discovery of topics. LDA treats documents as a mixture of topics, and topics as a mixture of words. Each topic discovered\n\nby LDA is represented by a probability distribution which conveys the affinity for a given word to that particular topic. We analyze these distributions to understand the differences between the topics discovered in the two datasets. To get a sense of how the topics found in the Streaming data compare with those found with random samples, we compare with topics found by running LDA on random subsamples of the Firehose data.\n\nTopic Discovery Here we compare the topics generated using the Firehose corpus with those generated using the Streaming corpus. LDA takes, in addition to the corpus, three parameters as its input: $K$ - the number of topics, $\\alpha$ - a hyperparameter for the Dirichlet prior topic distribution, and $\\eta$ - a hyperparameter for the Dirichlet prior word distribution. Choosing optimal parameters is a very challenging problem, and is not the focus of this work. Instead we focus on the similarity of the results given by LDA using identical parameters on both the Streaming and Firehose corpus. We set $K = 1 0 0$ as suggested by (Dumais et al. 1988) and use priors of $\\alpha = 5 0 / \\bar { K }$ , and $\\eta = 0 . 0 1$ . The software we used to discover the topics is the gensim software package (Reh ˇ u˚ˇrek and Sojka 2010). To get an understanding of the topics discovered at each level of Streaming coverage, we select the same days as we did for the comparison of Kendall’s $\\tau$ .\n\nTopic Comparison To understand the differences between the topics generated by LDA, we compute the distance in their probability distribution using the Jensen-Shannon divergence metric (Lin Jan). Since LDA’s topics have no implicit orderings we first must match them based upon the similarity of the words in the distribution. To do the matching we construct a weighted bipartite graph between the topics from the Streaming API and the Firehose. Treating each topic as a bag of words, we use the Jaccard score between the words in a Streaming topic $T _ { i } ^ { S }$ and a Firehose topic $T _ { j } ^ { F }$ as the weight of the edges in the graph,\n\nAfter constructing the graph we use the maximum weight matching algorithm proposed in (Galil 1986) to find the best matches between topics from the Streaming and Firehose data. After making the ideal matches, we then compute the Jensen-Shannon divergence between the two topics. Treating each topic as a probability distribution, we compute this as follows:",
      "referring_paragraphs": [
        "Figure 5: Random sampling of Firehose data."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Min. $\\mu = 0 . 0 2 4$ , $\\sigma = 0 . 0 1 9$",
        "1306.5204_page0_fig6.jpg",
        "(b) Q1. $\\mu = 0 . 0 1 8$ , $\\sigma = 0 . 0 1 8$ .",
        "1306.5204_page0_fig7.jpg",
        "(c) Median. $\\mu = 0 . 0 1 8$ , σ = 0.020.",
        "1306.5204_page0_fig8.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig8.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_10",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg",
      "image_filename": "1306.5204_page0_fig9.jpg",
      "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "We compute the Jensen-Shannon divergence for each matched pair and plot a histogram of the values in Figure 6.",
        "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   \n(a) Min. $S = 0 . 0 2 4$ , $\\hat { \\mu } = 0 . 0 1 7$ $\\hat { \\sigma } = 0 . 0 0 2$ $z = 3 . 5 0 0$ ."
      ],
      "figure_type": "other",
      "sub_figures": [
        "(d) Q3. $\\mu = 0 . 0 1 4$ , $\\sigma = 0 . 0 1 6$",
        "(e) Max. µ = 0.016, σ = 0.018.",
        "1306.5204_page0_fig10.jpg",
        "1306.5204_page0_fig11.jpg",
        "(b) Q1. $S = 0 . 0 1 8$ , $\\hat { \\mu } = 0 . 0 1 2$ $\\hat { \\sigma } = 0 . 0 0 1$ $z = 6 . 0 0 0$",
        "1306.5204_page0_fig12.jpg"
      ],
      "quality_score": 0.55,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig12.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1306.5204",
      "figure_id": "1306.5204_fig_14",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg",
      "image_filename": "1306.5204_page0_fig13.jpg",
      "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.",
      "context_before": "",
      "context_after": "[Section: Network-Level Measures]\n\nWe complement our node-level analysis by comparing various metrics at the network level. These metrics are reported in Table 3 and are calculated as follows. Since retweet networks create a lot of small disconnected components, we focus only on the size of the largest component. The size of the main component and the fact that all smaller components contain less than $1 \\%$ of the nodes justify our focus on the main component for this data. Therefore, we reduce the networks to their largest component before we proceed with\n\nthe calculations. To describe the structure of the retweet networks we calculate the clustering coefficient, a measure for local density (Watts and Strogatz 1998). We do not take all possible triads of directed networks into account, but treat the networks as undirected when calculating the clustering coefficient. $D _ { i n } > 0$ shows the proportion of nodes in the largest component that are retweeted and $m a x ( D _ { i n } )$ shows the value of the highest unscaled In-Degree value, i.e., number of unique users retweeting the same single user. The final three lines of Table 3 are network centralization indexes based on the node-level measures that have been introduced in the previous paragraph. Freeman (Freeman 1979) describes the centralization $C _ { X }$ of a network for any given metric as the difference of the value $C _ { X } ( p * )$ of the most central node to all other node values compared to the maximum possible difference:\n\nHigh centralization indicates a network with some nodes having very high node-level values and many nodes with low values while low centralization is the result of evenly distributed node-level measures.\n\nWe do not discuss all details of the individual results but focus on the differences between the two data sources. First, the coverage of nodes and links is similar to the coverage of tweets. This is a good indicator that the sub-sample is not biased to the specific Twitter user (e.g. high activity). The smaller proportion of nodes with non-zero In-Degree for the Firehose shows us that the larger number of nodes includes many more peripheral nodes. A low Clustering Coefficient implies that networks are hierarchical rather than interacting communities. Even though the centralization indexes are rather similar, there is one very interesting result when looking at the individual days: The range of values is much higher for the Streaming data as a result of the high coverage fluctuation. Further research will analyze whether we can use network metrics to better estimate how sufficient the sampled Streaming data is.",
      "referring_paragraphs": [
        "Results of this experiment, including $z$ -Scores are shown in Figure 7.",
        "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(c) Median. $S = 0 . 0 1 8$ $\\hat { \\mu } = 0 . 0 1 3$ , $\\hat { \\sigma } = 0 . 0 0 1$ , $z = 5 . 0 0 0$ .",
        "(d) Q3. $S = 0 . 0 1 4$ , $\\hat { \\mu } = 0 . 0 1 3$ $\\hat { \\sigma } = 0 . 0 0 1$ $z = 1 . 0 0 0$ .",
        "1306.5204_page0_fig14.jpg",
        "(e) Max. $S = 0 . 0 1 6$ , $\\hat { \\mu } = 0 . 0 1 3$ , $\\hat { \\sigma } = 0 . 0 0 1$ , $z = 3 . 0 0 0$ .",
        "1306.5204_page0_fig15.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig15.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1403.7400": [
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg",
      "image_filename": "1403.7400_page0_fig0.jpg",
      "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)",
      "context_before": "While there are many social media platforms, big data research focuses disproportionately on Twitter. For example, ICWSM, perhaps the leading selective conference in the field of social media, had 72 full papers last year (2013), almost half of which presented data that was primarily or solely drawn from Twitter. (Disclosure: I’ve long been involved in this conference in multiple capacities and think highly of it. The point isn’t about any one paper’s worth or quality but rather about the prevalence of attention to a single platform).\n\nThis preponderance of Twitter studies is mostly due to availability of data, tools and ease of analysis. Very large data sets, millions or billions of points, are available from this source. In contrast to Facebook, the largest social media platform, almost all Twitter activity, other than direct messages and private profiles, is visible on the public Internet. More Facebook users (estimated to be more than $50 \\%$ ) have made their profiles “private”, i.e. not accessible on the public Internet, as compared with Twitter users (estimated as less than $10 \\%$ ). While Twitter has been closing some of the easier means of access, the bulk of Facebook is largely inaccessible except by Facebook’s own data scientists. (Though Facebook public pages are available through its API). Unsurprisingly, only about $5 \\%$ of the papers presented in ICWSM 2013 were about Facebook, and nearly all of them were co-authored with Facebook data scientists.\n\nTwitter data also has a simple and clean structure. In contrast to the finer-grained privacy settings on other social media platforms, Twitter profiles are either “all public” or “all private.” With only a few basic functions (retweet, mention, and hashtags) to map, and a maximum of 140 characters per tweet, the datasets generated by Twitter are relatively easy to structure, process and analyze as compared with most other platforms. Consequently, Twitter has emerged as a “model organism” of big data.\n\nIn biology, “model organisms” refer to species which have been selected for intensive examination by the research community in order to shed light on key biological processes such as fundamental properties of living cells. Focusing on model organisms is conducive to progress in basic questions underlying the entire field, and this approach has been spectacularly successful in molecular biology (Fields and Johnson, 2007; Geddes, 1990). However, this investigative path is not without tradeoffs.\n\nBiases in “model-organism” research programs are not the same as sample bias in survey research, which also impact social media big data studies. The focus on just a few\n\nplatforms introduces non-representativeness at the level of mechanisms, not just at the level of samples. Thus, drawing a Twitter sample that is representative of adults in the target population would not solve the problem.\n\nTo explain the issue, consider that all dominant biological model organisms – such as the fruit fly Drosophila melanogaster, the bacterium Escherichia coli, the nematode worm Caenorhabditis elegans, and the mouse Mus musculus – were selected for rapid life cycles (quick results), ease of breeding in artificial settings and small adult size (lab-friendliness), “rapid and stereotypical” development (making experimental comparisons easier), and “early separation of germ line and soma,” (reducing certain kinds of variability) (Bolker, 1995; Jenner and Wills, 2007). However, the very characteristics that make them useful for studying certain biological mechanisms come at the expense of illuminating others (Gilbert, 2001, Jenner and Wills, 2007). Being easy to handle in the laboratory, in effect, implies “relative insensitivity to environmental influences” (Bolker, 1995, p:451–2) and thus unsuitability for the study of environmental interactions. The rapid development cycle depresses mechanisms present in slowergrowing species, and small adult size can imply “simplification or loss of structures, the evolution of morphological novelties, and increased morphological variability” (Bolker, 1995, p:451).\n\nIn other words, model organisms can be unrepresentative of their taxa, and more importantly, may be skewed with regard to the importance of mechanisms in their taxa. They are chosen because they don’t die easily in confinement and therefore encapsulate mechanisms specific to surviving in captivity—a trait that may not be shared with species not chosen as model organisms. The fruit fly, which breeds easily and with relative insensitivity to the environment, may lead to an emphasis on genetic factors over environmental influences. In fact, this appears to be a bias common to most model organisms used in biology.\n\nBarbara McClintock’s discovery of transposable genes, though much later awarded the Nobel prize, was initially disbelieved and disregarded partly because the organism she used, maize, was not a model organism at the time (Pray and Zhaurova, 2008). Yet the novel mechanisms which she discovered were not as visible in the more accepted model organisms, and would not have been found had she kept to those. Consequently, biologists interested in the roles of ecology and development have widened the range of organisms studied in order to uncover a wider range of mechanisms.\n\nThe dominance of Twitter as the “model organism” for social media big data analyses similarly skews analyses of mechanisms. Each social media platform carries with it a suite of affordances - things that it allows and makes easy\n\nversus things that are not possible or difficult - which help structure behavior on the platform. For Twitter, the key characteristics are short message length, rapid turnover, public visibility, and a directed network graph (“follow” relationships do not need to be mutual.) It lacks some of the characteristics that blogs, LiveJournal communities, or Facebook possess, such as longer texts, lengthier reaction times, stronger integration of visuals with text, the mutual nature of “friending” and the evolution of conversations over longer periods of time.\n\nTwitter’s affordances and the mechanisms it engenders interact in multiple ways. Its lightweight interface, suitable to mobile devices and accessible via texting, means that it is often the platform of choice when on the move, in lowbandwidth environments or in high-tension events such as demonstrations. The retweet mechanism also generates its own complex set of status-related behaviors and norms that do not necessarily translate to other platforms. Also, crucially, Twitter is a directed graph in that one person can “follow” another without mutuality. In contrast, Facebook’s backbone is mostly an undirected graph in which “friending” is a two-way relationship and requires mutual consent. Similarly, Livejournal’s core interactions tend to occur within “friends lists”. Consequently, Twitter is more likely than other platforms to sustain bridge mechanisms between communities and support connections between densely interconnected clusters that are otherwise sparsely connected to each other.\n\nTo see the implications for analysis and interpretation of big data, let’s look at bridging as a mechanism and consider a study which shows that bit.ly shortened links, distributed on Twitter using revolutionary hashtags during the Arab Spring, played a key role as an information conduit from within the Arab uprisings to the outside world—in other words, as bridges (Aday et al, 2012). Given its dependence on Twitter’s affordances, this finding should not be generalized to mean that social media as a whole also acted as a bridge, that social media was primarily useful as a bridging mechanism, nor that Twitter was solely a bridge mechanism (since this analyzed only of tweets containing bitl.ly links). Rather, this finding speaks to a convergence of user needs and certain affordances in a subset of cases which fueled one mechanism: in this case, bridging.\n\nFinally, there is indeed a sample bias problem. Twitter is used by less than $20 \\%$ of the US population (Mitchell and Hitlin, 2014), and that is not a randomly selected group. While Facebook has wider diffusion, its use is also structured by race, gender, class and other factors (Hargittai, 2008). Thus, the use of a few online platforms as “big data” model organisms raises important questions of representation and visibility, as different demographic or social groups may have different behavior—online and offline—\n\nand may not be fully represented or even sampled via current methods.\n\nAll this is not to say that Twitter is an inappropriate platform to study. Research in the model organism paradigm allows a large community to coalesce around shared datasets, tools and problems, and can produce illuminating results. However, the specifics of the “model organism” and biases that may result should not be overlooked.\n\n[Section: 2. Hashtag Analyses, Selecting on the Dependent Variable, Selection Effects and User Choices.]\n\nThe inclusion of hashtags in tweets is a Twitter convention for marking a tweet as part of a particular conversation or topic, and many social media studies rely on them for sample extraction. For example, the Tunisian uprising was associated with the hashtag #sidibouzid while the initial Egyptian protests of January 25, 2011, with #jan25. Facebook’s adoption of hashtags makes the methodological specifics of this convention even more important. While hashtag studies can be a powerful for examining network structure & information flows, all hashtag analyses, by definition, select on a dependent variable, and hence display the concomitant features and weaknesses of this methodological path.\n\n“Selecting on the dependent variable” occurs when inclusion of a case in a sample depends on the very variable being examined. Such samples have specific limits to their analytic power. For example, analyses that only examine revolutions or wars that have occurred will overlook cases where the causes and correlates of revolution and war have been present but in which there have been no resulting wars or revolutions (Geddes, 2010). Thus, selecting on the dependent variable (the occurrence of war or revolution) can help identify necessary conditions, but those may not be sufficient. Selecting on the dependent variable can introduce a range of errors specifics of which depend on the characteristics of the uncorrelated sample.\n\nIn hashtag datasets, a tweet is included because the user chose to use it, a clear act of self-selection. Self-selected samples often will not only have different overall characteristics than the general population, they may also exhibit significantly different correlational tendencies which create thorny issues of confounding variables. Famous examples include the hormone replacement therapy (HRT) controversy in which researchers had, erroneously, believed that HRT conferred health benefits to post-menopausal women based on observational studies of women who self-selected to take HRT. In reality, HRT therapy was adopted by healthier women. Later randomized double-blind studies showed that HRT was, in fact, harmful—so harmful that the researchers stopped the study in its tracks to reverse advice that had been given to women for a decade.",
      "context_after": "[Section: 3. The Missing Denominator: We Know Who Clicked But We Don’t Know Who Saw Or Could:]\n\nOne of the biggest methodological dangers of big data analyses is insufficient understanding of the denominator. It’s not enough to know how many people have “liked” a Facebook status update, clicked on a link, or “retweeted” a message without knowing how many people saw the item and chose not to take any action. We rarely know the characteristics of the sub-population that sees the content even though that is the group, and not the entire population, from which we are sampling. Normalization is rarely done, or may even be actively decided against because the results start appearing more complex or more trivial (Cha, 2008).\n\nWhile the denominator is often not calculable, it may be possible to estimate. One measure might be “potential exposure,” corresponding to the maximum number of people who may have seen a message. However, this highlights another key issue: the data is often proprietary (Boyd and Crawford, 2012). It might be possible to work with the platforms to get estimates of visibility, click-through and availability. For example, Facebook researchers have disclosed that the mean and median fraction of a user’s friends that see status update posts is about 34 to $3 5 \\%$ though the distribution of the variable seems to have a large spread (Bernstein et al., 2013).\n\nWith some disclosure from proprietary platforms, it may be possible to calculate “likely” exposure numbers based on “potential” exposure - similar to the way election polls model “likely” voters or TV ratings try to measure people watching a show rather than just being in the room where the TV is on. Steps in this direction are likely to be complex and difficult, but without such efforts, our ability to interpret raw numbers will remain limited. The academic community should ask for more disclosure and access from the commercial platforms.\n\nIt’s also important to normalize underlying populations when comparing “clicks,” “links,” or tweets. For example, Aday et al. (2012) compares numbers of clicks on bit.ly links in tweets containing hashtags associated with the Ar-\n\nab uprisings and concludes that “new media outlets that that use bit.ly are more likely to spread information outside the region than inside it.” This is an important finding. However, interpretation of this finding should take into account the respective populations of Twitter users in the countries in question. Egypt’s population is about 80 million, about 1 percent of the global population. Any topic of global interest about Egypt could very easily generate more absolute number of clicks outside the country even if the activity within the country remained much more concentrated in relative proportions. Finally, the size of these datasets makes traditional measures like statistical significance less valuable (Meiman and Freund, 2012), a problem exacerbated by lack of information about the denominator.\n\n[Section: 4. Missing the Ecology for the Platform:]",
      "referring_paragraphs": [
        "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig1.jpg",
      "image_filename": "1403.7400_page0_fig1.jpg",
      "caption": "Figure 2: Retweeted widely, but mostly in disgust",
      "context_before": "The question of inference from analyses of social media big data remains underconceptualized and underexamined. What’s a click? What does a retweet mean? In what context? By whom? How do different communities interpret these interactions? As with all human activities, interpreting online imprints engages layers of complexity.\n\n[Section: 1. What’s in a Retweet? Understanding our Data:]\n\nThe same act can have multiple, even contradictory meanings. In many studies, for example, retweets or mentions are used as proxies for influence or agreement. This may hold in some contexts; however, there are many conceptual steps and implicit assumptions embedded in this analysis. It is clear that a retweet is information exposure and/or reaction; however, after that, its meaning could range from affirmation to denunciation to sarcasm to approval to disgust. In fact, many social media acts which are designed as “positive” interactions by the platform engineers, ranging from Retweets on Twitter to even “Likes” on Facebook can carry a range of meanings, some quite negative.",
      "context_after": "[Section: 2. Engagement Invisible to Machines: Subtweets, Hate-Links, Screen Captures and Other Methods:]\n\nSocial media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person’s tweets via “screen captures,” and hatelinking. All these practices can blind big data analyses to this mode of activity and engagement.\n\nSubtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and consequently to automated data collection—even as the reference remains clear to those “in the know.” This manipulation of visibility can be achieved by referring to a person who has a twitter handle without either “mentioning” this handle, or by inserting a space between the $@$\n\nssign and the hhandle, or by uusing their reggular name or  a nnickname ratheer than the hanndle, or even soometimes delibbeerately misspellling the name . In some casees, the referencce ccan only be unnderstood in coontext, as theree is no mentioon oof the target inn any form. Thhese many formms of subtwee tinng come with ddifferent impli cations for bigg data analyticss.\n\nFor examplle, a controvversial article  by Egyptiann-AAmerican Monna El Eltahawyy sparked a masssive discussioon inn Egypt’s sociial media. In aa valuable anallysis of this dissccussion, socioloogists Alex Haanna and Marcc Smith extrac teed the tweets wwhich mentioneed her or linkeed to the articlee. TTheir network  analysis reveaaled great pol arization in thhe ddiscussion, wit h two distinctlly clustered gr oups. Howeveer, wwhile watchingg this discussioon online, I nooticed that manny oof the high-proofile bloggers  and young Eggyptian activistts ddiscussing the  article - and g reatly influenccing the converrssation - were iindeed subtweeeting. Later ddiscussions witth thhis communityy revealed thatt this was a deeliberate choicce thhat had been mmade because  many people  did not want tto ggive Eltahawy  attention,” evven as they waanted to discusss thhe topic and hher work.",
      "referring_paragraphs": [
        "Figure 2: Retweeted widely, but mostly in disgust\n\nAs an example, take the recent case of the twitter account of fashion store @celebboutique."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg",
      "image_filename": "1403.7400_page0_fig2.jpg",
      "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.",
      "context_before": "[Section: 2. Engagement Invisible to Machines: Subtweets, Hate-Links, Screen Captures and Other Methods:]\n\nSocial media users engage in practices that alter their visibility to machine algorithms, including subtweeting, discussing a person’s tweets via “screen captures,” and hatelinking. All these practices can blind big data analyses to this mode of activity and engagement.\n\nSubtweeting is the practice of making a tweet referring to a person algorithmically invisible to that person—and consequently to automated data collection—even as the reference remains clear to those “in the know.” This manipulation of visibility can be achieved by referring to a person who has a twitter handle without either “mentioning” this handle, or by inserting a space between the $@$\n\nssign and the hhandle, or by uusing their reggular name or  a nnickname ratheer than the hanndle, or even soometimes delibbeerately misspellling the name . In some casees, the referencce ccan only be unnderstood in coontext, as theree is no mentioon oof the target inn any form. Thhese many formms of subtwee tinng come with ddifferent impli cations for bigg data analyticss.\n\nFor examplle, a controvversial article  by Egyptiann-AAmerican Monna El Eltahawyy sparked a masssive discussioon inn Egypt’s sociial media. In aa valuable anallysis of this dissccussion, socioloogists Alex Haanna and Marcc Smith extrac teed the tweets wwhich mentioneed her or linkeed to the articlee. TTheir network  analysis reveaaled great pol arization in thhe ddiscussion, wit h two distinctlly clustered gr oups. Howeveer, wwhile watchingg this discussioon online, I nooticed that manny oof the high-proofile bloggers  and young Eggyptian activistts ddiscussing the  article - and g reatly influenccing the converrssation - were iindeed subtweeeting. Later ddiscussions witth thhis communityy revealed thatt this was a deeliberate choicce thhat had been mmade because  many people  did not want tto ggive Eltahawy  attention,” evven as they waanted to discusss thhe topic and hher work.",
      "context_after": "FFigure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.\n\nIn another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context. Whille eeach person is  referring to thhe other, theree are no name s, nnicknames, or hhandles. In adddition, neither follows the othheer on Twitter.  It is, howeverr, clearly a dirrect engagemennt aand conversatioon, if a negativve one. A broaad discussion oof thhis “Twitter sppat” on Turkis h Twitter provved people werre aaware of this ass a two-way coonversation. It  was so well unndderstood that it  was even repoorted in newspaapers.\n\nWhile the truue prevalence  of this behavioor is hard to esstaablish, exactlyy because the aactivity is hiddden from largeesscale, machine--led analyses,  observations oof Turkish Twiitter during the GGezi protests off June 2013 revvealed that succh ssubtweets weree common. Inn order to gett a sense of itts sscale, I underttook an onlinee ethnographyy in Decembeer, 22013, during wwhich two hunndred Twitter uusers from Turrkkey, assembledd as a purposivve sample inc luding ordinarry uusers as well a s journalists annd pundits, weere followed foor\n\nan houur at a time inn, totaling at leeast 10 hours oof observation deedicated to cattching subtweeets. This resulteed in a collectionn of 100 unmmistakable subttweets; many mmore were undouubtedly missedd because theyy are not alwayys obvious to obsservers. In fact,, the subtweetss were widely uunderstood and reetweeted, whiich increases  the importancce of such practicces. Overall, thhe practice apppears commonn enough to be desscribed as routiine, at least in  Turkey.",
      "referring_paragraphs": [
        "In another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg",
      "image_filename": "1403.7400_page0_fig3.jpg",
      "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”",
      "context_before": "FFigure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.\n\nIn another exxample drawn  from my primmary research oon TTurkey, figure  3 shows a subbtweet exchangge between twwo pprominent indi viduals that wwould be uninteelligible to anyyoone who did nnot already follow the broadder conversatioon aand was not inntimately familiar with the context. Whille eeach person is  referring to thhe other, theree are no name s, nnicknames, or hhandles. In adddition, neither follows the othheer on Twitter.  It is, howeverr, clearly a dirrect engagemennt aand conversatioon, if a negativve one. A broaad discussion oof thhis “Twitter sppat” on Turkis h Twitter provved people werre aaware of this ass a two-way coonversation. It  was so well unndderstood that it  was even repoorted in newspaapers.\n\nWhile the truue prevalence  of this behavioor is hard to esstaablish, exactlyy because the aactivity is hiddden from largeesscale, machine--led analyses,  observations oof Turkish Twiitter during the GGezi protests off June 2013 revvealed that succh ssubtweets weree common. Inn order to gett a sense of itts sscale, I underttook an onlinee ethnographyy in Decembeer, 22013, during wwhich two hunndred Twitter uusers from Turrkkey, assembledd as a purposivve sample inc luding ordinarry uusers as well a s journalists annd pundits, weere followed foor\n\nan houur at a time inn, totaling at leeast 10 hours oof observation deedicated to cattching subtweeets. This resulteed in a collectionn of 100 unmmistakable subttweets; many mmore were undouubtedly missedd because theyy are not alwayys obvious to obsservers. In fact,, the subtweetss were widely uunderstood and reetweeted, whiich increases  the importancce of such practicces. Overall, thhe practice apppears commonn enough to be desscribed as routiine, at least in  Turkey.",
      "context_after": "Figuree 4: Algorithmiccally Invisible Enngagement: A coolumnist responds  to critics by screeen captures.\n\nUsiing screen capttures rather thaan quotes is an other practice thhat adds to thhe invisibility  of engagemennt to algorithmss. A “caps” is ddone when Twwitter users refeerence each other’s tweets throuugh screen caaptures rather than links, mentioons or quotes . An examplee is shown onn Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”\n\nYett another praactice, colloquuially known  as “hatelinkingg,” limits the  algorithmic vvisibility of enngagement, althouugh this one i s potentially ttraceable. “Haate-linking” occurss when a user llinks to anotheer user’s tweet rather than mentiooning or quotiing the user. TThis practice, ttoo, would skew  analyses baseed on mention s or retweets,  though in this caase, it is at leasst possible to loook for such linnks.\n\nSubbtweeters, “capps” users, and  hate-linkers arre obviously a smmaller commuunity than tweeeters as a wholle. While it is uncclear how wideespread these ppractices truly  are, studying Tuurkish Twitter  shows that theey are not unccommon, at least iin that contextt. Other counttries might havve specific\n\nssocial media p ractices that c onfound big ddata analytics iin ddifferent ways.. Overall, a ssimple “scrapiing” of Turkissh TTwitter might pproduce a polaarized map of ggroups not talkkinng to each othher, whereas thhe reality is a  polarized situaatiion in which ccontentious grooups are engagging each otheer bbut without thee conventionall means that mmake such connvversations visibble to algorithmms and to reseaarchers.\n\n[Section: 33. Limits of MMethodologiccal Analogiess and Importti ng Network  Methods froom Other Fieelds:]\n\nDDo social mediia networks opperate through similar mechaannisms as netwoorks of airliness? Does informmation work thhe wway germs doo? Such questiions are rarelyy explicitly addddressed even tthough many  papers importt methods fromm oother fields on  the implicit asssumption that  the answer is  a yyes. Studies thaat do look at tthis question, ssuch as Romerro eet al. (2011) annd Lerman et aal. (2010), are  often limited tto ssingle, or few , platforms, wwhich limit thheir explanatorry ppower becausee information aamong humanns does not difffuse in a singlee platform whhereas viruses  do, indeed, di fffuse in a well-defined manneer. To step bacck further, eveen rrepresenting soocial media innteractions as  a network reeqquires a whole  host of impliccit and importaant assumptionns thhat should be  considered exxplicitly ratherr than assumeed aaway (Butts, 20009).\n\nEpidemiologgical or contaagion-inspired  analyses ofteen trreat connectedd edges in sociial media netwworks as if theey wwere “neighborrs” in physicall proximity. Inn epidemiologyy, itt is reasonablee to treat “physsical proximityy” as a key varriaable, assuming  that adjacent nnodes are “suscceptible” to disseease transmiss ion for very  good reason:  the underlyinng mmodel is a wwell-developedd, empirically--verified germmthheory of diseaase in which smmall microbes  travel in actuaal sspace (where ddistance matter s) to infect thee next person bby eentering their  body. This  physical proccess has wellluunderstood prooperties, and unnderlying probbabilities can o ft en be calculateed with precisioon.\n\nCreating an analogy fromm social mediaa interactions tto pphysical proximmity may be a  reasonable andd justified undeer ccertain conditioons and assummptions, but th is step is rarelly ssubjected to ccritical examinnation (Salathéé et al, 2013 ). TThere are signnificant differeences between  germs and innfformation traveeling in social  media networrks. Adjacenccy inn social meddia is multi-faaceted; it cannnot always bbe mmapped to physsical proximityy; and human ““nodes” are subbj ect to informaation from a wwide range of ssources, not ju st thhose they are  connected to  in a particulaar social mediia pplatform. Finallly, whether thhere is a straigghtforward relaatiion between innformation expposure and thee rate of “influueence,” as there  often is for exxposure to a diisease agent annd thhe rate of infecction, is sometthing that shou ld be empiricaallyy investigated,, not assumed.\n\nTheere are clearly  similar dynammics in differennt types of netwoorks, human annd otherwise, and the diffeerent fields can leearn much fromm each other.  However, impportation of methoods needs to reely on more th an some putatiive universal, coontext-indepenndent property  of networked  interaction simplyy by virtue of tthe existence oof a network.",
      "referring_paragraphs": [
        "An examplee is shown onn Figure 4."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg",
      "image_filename": "1403.7400_page0_fig4.jpg",
      "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.",
      "context_before": "DDo social mediia networks opperate through similar mechaannisms as netwoorks of airliness? Does informmation work thhe wway germs doo? Such questiions are rarelyy explicitly addddressed even tthough many  papers importt methods fromm oother fields on  the implicit asssumption that  the answer is  a yyes. Studies thaat do look at tthis question, ssuch as Romerro eet al. (2011) annd Lerman et aal. (2010), are  often limited tto ssingle, or few , platforms, wwhich limit thheir explanatorry ppower becausee information aamong humanns does not difffuse in a singlee platform whhereas viruses  do, indeed, di fffuse in a well-defined manneer. To step bacck further, eveen rrepresenting soocial media innteractions as  a network reeqquires a whole  host of impliccit and importaant assumptionns thhat should be  considered exxplicitly ratherr than assumeed aaway (Butts, 20009).\n\nEpidemiologgical or contaagion-inspired  analyses ofteen trreat connectedd edges in sociial media netwworks as if theey wwere “neighborrs” in physicall proximity. Inn epidemiologyy, itt is reasonablee to treat “physsical proximityy” as a key varriaable, assuming  that adjacent nnodes are “suscceptible” to disseease transmiss ion for very  good reason:  the underlyinng mmodel is a wwell-developedd, empirically--verified germmthheory of diseaase in which smmall microbes  travel in actuaal sspace (where ddistance matter s) to infect thee next person bby eentering their  body. This  physical proccess has wellluunderstood prooperties, and unnderlying probbabilities can o ft en be calculateed with precisioon.\n\nCreating an analogy fromm social mediaa interactions tto pphysical proximmity may be a  reasonable andd justified undeer ccertain conditioons and assummptions, but th is step is rarelly ssubjected to ccritical examinnation (Salathéé et al, 2013 ). TThere are signnificant differeences between  germs and innfformation traveeling in social  media networrks. Adjacenccy inn social meddia is multi-faaceted; it cannnot always bbe mmapped to physsical proximityy; and human ““nodes” are subbj ect to informaation from a wwide range of ssources, not ju st thhose they are  connected to  in a particulaar social mediia pplatform. Finallly, whether thhere is a straigghtforward relaatiion between innformation expposure and thee rate of “influueence,” as there  often is for exxposure to a diisease agent annd thhe rate of infecction, is sometthing that shou ld be empiricaallyy investigated,, not assumed.\n\nTheere are clearly  similar dynammics in differennt types of netwoorks, human annd otherwise, and the diffeerent fields can leearn much fromm each other.  However, impportation of methoods needs to reely on more th an some putatiive universal, coontext-indepenndent property  of networked  interaction simplyy by virtue of tthe existence oof a network.\n\n[Section: 4. Fieeld Effects: NNon-Networkks Interactionns]\n\nAnothher difference  between spaatial or epideemiological netwoorks and humann social netwoorks is that humman social informmation flows ddo not occur onnly through noode-to-node netwoorks but also thhrough field efffects, large-scaale societal eventss which impacct a large grouup of actors coontemporaneouslly. Big events,, weather occuurrences, etc. aall have society-wwide field efffects and ofteen do not difffuse solely througgh interpersonnal interaction  (although theey also do greatlyy impact interrpersonal inte raction by afffecting the agendda, mood and ddisposition of inndividuals).\n\nForr example, moost studies agrree that Egypptian social mediaa networks playyed a role in tthe uprising whhich began in Egyypt in January  2011 and weree a key condui t of protest informmation (Lynch,, 2012; Aday  et al, 2012; TTufekci and Wilsoon, 2012). Howwever, there waas almost certa inly another impportant informaation diffusionn dynamic. Thee influence of the e Tunisian revoolution itself o n the expectattions of the Egypttian populace was a major  turning pointt (Ghonim, 2012;  Lynch, 2012) . While analyysis of networkks in Egypt might not have revvealed a majorr difference beetween the secondd and third weeek of Januaryy of 2011, sommething major haad changed in tthe field. To trranslate it into epidemiologicaal language, duue to the Tunnisian revolutioon and the exampple it presentedd, all the “noddes” in the netwwork had a differeent “susceptibiility” and “recceptivity” to innformation about  an uprising. TThe downfall oof the Tunisiann president, whichh showed that eeven an enduriing autocracy iin the Middle Eaast was suscepptible to streeet protests, eneergized the oppossition and channged the politiical calculationn in Egypt. This iinformation wwas diffused thhrough multiplle methods and brroadcast mediaa played a keyy role. Thus, thhe communicatioon of the Tuniisia effect to thhe Egyptian neetwork was not neecessarily depeendent on the  network struccture of social mmedia.",
      "context_after": "[Section: 55. You Namee It, Humans  Will Game iit: Reflexivityy aand Humans :]\n\nUUnlike disease  vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same  metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg “trend.” Indeeed, “to trend”” is increasinglly uused as a transsitive verb ammong these useers, as in: “let’s trrend #Hungry44BH”. These eefforts are not mmere blind stabbs aat massive tweeeting; they ofteen display a finne-tuned underrsstanding of Twwitter’s trendiing topics alggorithm, whichh, wwhile not publiic, is widely unnderstood throough reverse ennggineering (Lotaan, 2012). In  coordinated caampaigns, Bahhrrain activistss have tre nded hashtaags such aas ##100strikedays , #Bloodyf1,  #KillingKhaw aja, #StopTearr-GGasBahrain, annd #F1DontRacceBahrain, amoong others.",
      "referring_paragraphs": [
        "Figure 5: Cleaar meaning onlyy in context and ttime."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1403.7400",
      "figure_id": "1403.7400_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg",
      "image_filename": "1403.7400_page0_fig5.jpg",
      "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]",
      "context_before": "[Section: 55. You Namee It, Humans  Will Game iit: Reflexivityy aand Humans :]\n\nUUnlike disease  vectors or gasses in a chambber, humans unndderstand, evaluuate and responnd to the same  metrics that biig ddata researcherrs are measurinng. For exampple, political acctiivists, especiallly in countriees such as Bahhrain, where thhe uunrest and reprression have re ceived less maainstream globaal mmedia attentio n, often undeertake deliberaate attempts tto mmake a hashtagg “trend.” Indeeed, “to trend”” is increasinglly uused as a transsitive verb ammong these useers, as in: “let’s trrend #Hungry44BH”. These eefforts are not mmere blind stabbs aat massive tweeeting; they ofteen display a finne-tuned underrsstanding of Twwitter’s trendiing topics alggorithm, whichh, wwhile not publiic, is widely unnderstood throough reverse ennggineering (Lotaan, 2012). In  coordinated caampaigns, Bahhrrain activistss have tre nded hashtaags such aas ##100strikedays , #Bloodyf1,  #KillingKhaw aja, #StopTearr-GGasBahrain, annd #F1DontRacceBahrain, amoong others.",
      "context_after": "[Section: Connclusion: Praactical Stepps—a Call too Action]\n\nSociall media big daata is a powerfful addition too the scientific ttoolkit. Howevver, this emerrgent field neeeds to be placedd on firmer mmethodological  and conceptuual footing. Meaniing of social mmedia imprints , context of huuman communiccations, and naature of socio -cultural interaactions are multi--faceted and ccomplex. Peopple’s behavior differs in signifificant dimensioons from other objects of nettwork analyses suuch as germs oor airline flightts.\n\nThee challenges ouutlined in this  paper are resuults of systematiic qualitative aand quantitativ e inquiry over  two years; howevver, this paper  cannot be connclusive in idenntifying the scale  and the scope  these issues——in fact, the chhallenge is exactlly that such isssues are resiistant to large -scale machine--analyses that  allows us to exactly pinppoint them. Whilee the challengees are thorny,  there are manny practical steps.  These includee:\n\n1- Taarget non-soccial dependeent variables.. No data sourcee is perfect, annd every dataseet is imperfect  in its own ways.  Most big dataa analyses remmains within thhe confines the daataset, with litttle means to  probe validityy. Seeking depenndent variabless outside big ddata, especiallyy those for whichh there are otheer measures obbtained using  traditional, tested  and fairly rreliable methoods, and lookiing at the conveergent and diveergent points, wwill provide muuch needed clarityy to strengths aand weaknessees of these dataasets. Such depenndent variables  could range ffrom , election s results to unempployment numbbers.   \n2- Q ualitative puull-outs. Reseearchers can  include a “qualiitative pull-ouut” from theeir sample too examine variatiions in behavvior. For exaample, what ppercent of retweeets are “hate-rretweets”? A ssmall random  subsample can prrovide a checkk. This may invvolve asking qquestions to peoplee. For examplee: did the somme people hear of X from TV ass well as fromm Twitter? Thhese qualitativee pull-outs need nnot be huge to  help interpretaation.   \n2. Basseline panels. Establishing aa panel to studdy peoples’ digitall behavior, simmilar to panel sstudies in sociaal sciences, could  develop “baseelines” and “gguidelines” for  the whole\n\ncommunity. Data sought could include those in this paper.   \n3. Industry Outreach. The field should solicit cooperation from the industry for data such as “denominators”, similar to Facebook’s recent release of what percent of a Facebook network sees status updates. Industry scientists who participate in the research community can be conduits.   \n4. Convergent answers and complimentary methods. Multi-method, multi-platform analyses should be sought and rewarded. As things stand, these exist (Adar et al., 2007 or Kairam, 2013) but are rare. Whenever possible, social media big data studies should be paired with surveys, interviews, ethnographies, and other methods so that biases and short-comings of each method can be used to balance each other to arrive at richer answers.   \n5. Multi-disciplinary teams. Scholars from fields where network methods are shared should cooperate to study the scope, differences and utility of common methods.   \n6. Methodological awareness in review. These issues should be incorporated into the review process and go beyond soliciting “limitations” sections.   \nA future study that recruited a panel of ordinary users, from multiple countries, and examined their behavior online and offline, and across multiple platforms to detect the frequency of behaviors outlined here, and those not detected yet, would be a path-breaking next step for understanding and grounding our social media big data.\n\n[Section: References]",
      "referring_paragraphs": [
        "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1409.0575": [
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_1",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg",
      "image_filename": "1409.0575_page0_fig0.jpg",
      "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.",
      "context_before": "The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.\n\nWe describe the choices we made in constructing the ILSVRC image classification dataset: selecting the target object categories from ImageNet (Section 3.1.1), collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Section 3.1.2), and finally filtering the millions of collected images using the carefully designed crowdsourcing strategy of ImageNet (Deng et al., 2009) (Section 3.1.3).\n\n[Section: 3.1.1 Defining object categories for the image classification dataset]\n\nThe 1000 categories used for the image classification task were selected from the ImageNet (Deng et al., 2009) categories. The 1000 synsets are selected such that there is no overlap between synsets: for any synsets $i$ and $j$ , i is not an ancestor of $j$ in the ImageNet hierarchy. These synsets are part of the larger hierarchy and may have children in ImageNet; however, for ILSVRC we do not consider their child subcategories. The synset hierarchy of ILSVRC can be thought of as a “trimmed” version of the complete ImageNet hierarchy. Figure 1 visualizes the diversity of the ILSVRC2012 object categories.\n\nThe exact 1000 synsets used for the image classification and single-object localization tasks have changed over the years. There are 639 synsets which have been used in all five ILSVRC challenges so far. In the first year of the challenge synsets were selected randomly from the available ImageNet synsets at the time, followed by manual filtering to make sure the object categories were not too obscure. With the introduction of",
      "context_after": "Fig. 5 Consider the problem of binary multi-label annotation. For each input (e.g., image) and each label (e.g., object), the goal is to determine the presence or absense $\\cdot +$ or -) of the label (e.g., decide if the object is present in the image). Multilabel annotation becomes much more efficient when considering real-world structure of data: correlation between labels, hierarchical organization of concepts, and sparsity of labels.\n\n[Section: 3.3.3 Complete image-object annotation for the object detection dataset]\n\nThe key challenge in annotating images for the object detection task is that all objects in all images need to be labeled. Suppose there are N inputs (images) which need to be annotated with the presence or absence of K labels (objects). A na¨ıve approach would query humans for each combination of input and label, requiring $N K$ queries. However, N and K can be very large and the cost of this exhaustive approach quickly becomes prohibitive. For example, annotating 60, 000 validation and test images with the presence or absence of 200 object classes for the detection task na¨ıvely would take 80 times more effort than annotating $1 5 0 , 0 0 0$ validation and test images with 1 object each for the classification task – and this is not even counting the additional cost of collecting bounding box annotations around each object instance. This quickly becomes infeasible.\n\nIn (Deng et al., 2014) we study strategies for scalable multilabel annotation, or for efficiently acquiring multiple labels from humans for a collection of items. We exploit three key observations for labels in real world applications (illustrated in Figure 5):\n\n1. Correlation. Subsets of labels are often highly correlated. Objects such as a computer keyboard, mouse and monitor frequently co-occur in images. Similarly, some labels tend to all be absent at the same time. For example, all objects that require electricity are usually absent in pictures taken outdoors. This suggests that we could potentially fill in the values of multiple labels by grouping them into only one query for humans. Instead of checking if dog, cat, rabbit etc. are present in the photo, we just check\n\nabout the “animal” group If the answer is no, then this implies a no for all categories in the group.\n\n2. Hierarchy. The above example of grouping dog, cat, rabbit etc. into animal has implicitly assumed that labels can be grouped together and humans can efficiently answer queries about the group as a whole. This brings up our second key observation: humans organize semantic concepts into hierarchies and are able to efficiently categorize at higher semantic levels (Thorpe et al., 1996), e.g. humans can determine the presence of an animal in an image as fast as every type of animal individually. This leads to substantial cost savings.\n\n3. Sparsity. The values of labels for each image tend to be sparse, i.e. an image is unlikely to contain more than a dozen types of objects, a small fraction of the hundreds of object categories. This enables rapid elimination of many objects by quickly filling in no. With a high degree of sparsity, an efficient algorithm can have a cost which grows logarithmically with the number of objects instead of linearly.\n\nWe propose algorithmic strategies that exploit the above intuitions. The key is to select a sequence of queries for humans such that we achieve the same labeling results with only a fraction of the cost of the na¨ıve approach. The main challenges include how to measure cost and utility of queries, how to construct good queries, and how to dynamically order them. A detailed description of the generic algorithm, along with theoretical analysis and empirical evaluation, is presented in (Deng et al., 2014).\n\nApplication of the generic multi-class labeling algorithm to our setting. The generic algorithm automatically selects the most informative queries to ask based on object label statistics learned from the training set. In our case of 200 object classes, since obtaining the training set was by itself challenging we chose to design the queries by hand. We created a hierarchy of queries of the type “is there a... in the image?” For example, one of the high-level questions was “is there an animal in the image?” We ask the crowd workers this question about every image we want to label. The children of the “animal” question would correspond to specific examples of animals: for example, “is there a mammal in the image?” or “is there an animal with no legs?” To annotate images efficiently, these questions are asked only on images determined to contain an animal. The 200 leaf node questions correspond to the 200 target objects, e.g., “is there a cat in the image?”. A few sample iterations of the algorithm are shown in Figure 6.\n\nAlgorithm 1 is the formal algorithm for labeling an image with the presence or absence of each target object",
      "referring_paragraphs": [
        "In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Figure 2.",
        "Fig. 2 The ILSVRC dataset contains many more fine-grained classes compared to the standard PASCAL VOC benchmark; for example, instead of the PASCAL “dog” category there are 120 different breeds of dogs in ILSVRC2012-2014 classification and single-object localization tasks.\n\nare 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table 2 (top) documents the size of the dataset over the years of the challenge.",
        "The goal is to build a system that is fully automated,\n\nImage classification annotations (1000 object classes)   \nAdditional annotations for single-object localization (1000 object classes)   \n\n<table><tr><td>Year</td><td>Train images (per class)</td><td>Val images (per class)</td><td>Test images (per class)</td></tr><tr><td>ILSVRC2010</td><td>1,261,406 (668-3047)</td><td>50,000 (50)</td><td>150,000 (150)</td></tr><tr><td>ILSVRC2011</td><td>1,229,413 (384-1300)</td><td>50,000 (50)</td><td>100,000 (100)</td></tr><tr><td>ILSVRC2012-14</td><td>1,281,167 (732-1300)</td><td>50,000 (50)</td><td>100,000 (100)</td></tr></table>\n\n<table><tr><td>Year</td><td>Train images withbbox annotations (per class)</td><td>Train bboxes annotated (per class)</td><td>Val images withbbox annotations (per class)</td><td>Val bboxes annotated (per class)</td><td>Test images withbbox annotations</td></tr><tr><td>ILSVRC2011</td><td>315,525 (104-1256)</td><td>344,233 (114-1502)</td><td>50,000 (50)</td><td>55,388 (50-118)</td><td>100,000</td></tr><tr><td>ILSVRC2012-14</td><td>523,966 (91-1268)</td><td>593,173 (92-1418)</td><td>50,000 (50)</td><td>64,058 (50-189)</td><td>100,000</td></tr></table>\n\nTable 2 Scale of ILSVRC image classification task (top) and single-object localization task (bottom).",
        "Table 2 (bottom) documents the size of this dataset.",
        "Even though the exact object categories have changed (Section 3.1.1), the large scale of the dataset has remained the same (Table 2), making the results comparable across the years."
      ],
      "figure_type": "example",
      "sub_figures": [
        "1409.0575_page0_fig1.jpg",
        "1409.0575_page0_fig2.jpg",
        "1409.0575_page0_fig3.jpg",
        "1409.0575_page0_fig4.jpg",
        "1409.0575_page0_fig5.jpg",
        "1409.0575_page0_fig6.jpg",
        "1409.0575_page0_fig7.jpg",
        "1409.0575_page0_fig8.jpg",
        "1409.0575_page0_fig9.jpg",
        "1409.0575_page0_fig10.jpg",
        "1409.0575_page0_fig11.jpg",
        "1409.0575_page0_fig12.jpg",
        "1409.0575_page0_fig13.jpg",
        "1409.0575_page0_fig14.jpg",
        "1409.0575_page0_fig15.jpg",
        "1409.0575_page0_fig16.jpg",
        "1409.0575_page0_fig17.jpg",
        "1409.0575_page0_fig18.jpg",
        "1409.0575_page0_fig19.jpg",
        "1409.0575_page0_fig20.jpg",
        "1409.0575_page0_fig21.jpg",
        "1409.0575_page0_fig22.jpg",
        "1409.0575_page0_fig23.jpg",
        "1409.0575_page0_fig24.jpg",
        "1409.0575_page0_fig25.jpg",
        "1409.0575_page0_fig26.jpg",
        "1409.0575_page0_fig27.jpg",
        "1409.0575_page0_fig28.jpg",
        "1409.0575_page0_fig29.jpg",
        "1409.0575_page0_fig30.jpg",
        "1409.0575_page0_fig31.jpg",
        "1409.0575_page0_fig32.jpg",
        "1409.0575_page0_fig33.jpg",
        "1409.0575_page0_fig34.jpg",
        "1409.0575_page0_fig35.jpg",
        "1409.0575_page0_fig36.jpg",
        "1409.0575_page0_fig37.jpg",
        "1409.0575_page0_fig38.jpg",
        "1409.0575_page0_fig39.jpg",
        "1409.0575_page0_fig40.jpg",
        "1409.0575_page0_fig41.jpg",
        "1409.0575_page0_fig42.jpg",
        "1409.0575_page0_fig43.jpg",
        "1409.0575_page0_fig44.jpg",
        "1409.0575_page0_fig45.jpg",
        "1409.0575_page0_fig46.jpg",
        "1409.0575_page0_fig47.jpg",
        "1409.0575_page0_fig48.jpg",
        "1409.0575_page0_fig49.jpg",
        "1409.0575_page0_fig50.jpg",
        "1409.0575_page0_fig51.jpg",
        "1409.0575_page0_fig52.jpg",
        "1409.0575_page0_fig53.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig24.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig25.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig26.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig27.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig28.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig29.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig30.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig31.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig32.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig33.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig34.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig35.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig36.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig37.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig38.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig39.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig40.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig41.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig42.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig43.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig44.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig45.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig46.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig47.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig48.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig49.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig50.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig51.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig52.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig53.jpg"
        ],
        "panel_count": 54
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_55",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig54.jpg",
      "image_filename": "1409.0575_page0_fig54.jpg",
      "caption": "Figure 12.",
      "context_before": "",
      "context_after": "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The numbers in parentheses indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with $1 0 0 \\%$ accuracy. Object detection results are shown in Figure 12.\n\n[Section: Object detection]",
      "referring_paragraphs": [
        "Object detection results are shown in Figure 12.",
        "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The numbers in parentheses indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with $1 0 0 \\%$ accuracy. Object detection results are shown in Figure 12.",
        "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst results. The numbers in parentheses indicate average precision. Image classification and single-object localization results are shown in Figure 11."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1409.0575_page0_fig55.jpg",
        "1409.0575_page0_fig56.jpg",
        "1409.0575_page0_fig57.jpg",
        "1409.0575_page0_fig58.jpg",
        "1409.0575_page0_fig59.jpg",
        "1409.0575_page0_fig60.jpg",
        "1409.0575_page0_fig61.jpg",
        "1409.0575_page0_fig62.jpg",
        "1409.0575_page0_fig63.jpg",
        "1409.0575_page0_fig64.jpg",
        "1409.0575_page0_fig65.jpg",
        "1409.0575_page0_fig66.jpg",
        "1409.0575_page0_fig67.jpg",
        "1409.0575_page0_fig68.jpg",
        "1409.0575_page0_fig69.jpg",
        "1409.0575_page0_fig70.jpg",
        "1409.0575_page0_fig71.jpg",
        "1409.0575_page0_fig72.jpg",
        "1409.0575_page0_fig73.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig54.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig55.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig56.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig57.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig58.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig59.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig60.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig61.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig62.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig63.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig64.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig65.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig66.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig67.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig68.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig69.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig70.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig71.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig72.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig73.jpg"
        ],
        "panel_count": 20
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_75",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg",
      "image_filename": "1409.0575_page0_fig74.jpg",
      "caption": "Figure 11.",
      "context_before": "",
      "context_after": "Object detection   \nAverage scale of object   \nFig. 13 Performance of the “optimistic” method as a function of object scale in the image, on each task. Each dot corresponds to one object class. Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. “Optimistic” performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014 (including entries with additional training data). The test set has remained the same over these three years. We see that accuracy tends to increase as the objects get bigger in the image. However, it is clear that far from all the variation in accuracy on these classes can be accounted for by scale alone.\n\nThe image classification and single-object localization “optimistic” models performs better on large and extra large real-world objects than on smaller ones. The “optimistic” object detection model surprisingly performs better on extra small objects than on small or medium ones.   \n– Deformability within instance: Rigid (e.g., mug) or deformable (e.g., water snake)\n\nThe “optimistic” model on each of the three tasks performs statistically significantly better on deformable objects compared to rigid ones. However, this effect disappears when analyzing natural objects separately from man-made objects.\n\n– Amount of texture: none (e.g. punching bag), low (e.g. horse), medium (e.g. sheep) or high (e.g. honeycomb)\n\nThe “optimistic” model on each of the three tasks is significantly better on objects with at least low level of texture compared to untextured objects.\n\nThese and other findings are justified and discussed in detail below.\n\nExperimental setup. We observed in Section 6.3.3 that objects that occupy a larger area in the image tend to be somewhat easier to recognize. To make sure that differences in object scale are not influencing results in this section, we normalize each bin by object scale. We discard object classes with the largest scales from each bin as needed until the average object scale of object classes in each bin across one property is the same (or as close as possible). For real-world size property for example, the resulting average object scale in each of the five bins is 31.6%−31.7% in the image classification and single-object localization tasks, and $1 2 . 9 \\% - 1 3 . 4 \\%$ in the object detection task.11\n\nFigure 14 shows the average performance of the “optimistic” model on the object classes that fall into each bin for each property. We analyze the results in detail below. Unless otherwise specified, the reported accuracies below are after the scale normalization step.\n\nTo evaluate statistical significance, we compute the 95% confidence interval for accuracy using bootstrapping: we repeatedly sample the object classes within the bin with replacement, discard some as needed to normalize by scale, and compute the average accuracy of the “optimistic” model on the remaining classes. We report the $9 5 \\%$ confidence intervals (CI) in parentheses.\n\nReal-world size. In Figure 14(top, left) we observe that in the image classification task the “optimistic” model tends to perform significantly better on objects which are larger in the real-world. The classification accuracy is $9 3 . 6 \\% - 9 3 . 9 \\%$ on XS, S and M objects compared to $9 7 . 0 \\%$ on L and $9 6 . 4 \\%$ on XL objects. Since this is after normalizing for scale and thus can’t be explained by the objects’ size in the image, we conclude that either (1) larger real-world objects are easier for the model to recognize, or (2) larger real-world objects usually occur in images with very distinctive backgrounds.\n\nTo distinguish between the two cases we look Figure 14(top, middle). We see that in the single-object localization task, the L objects are easy to localize at $8 2 . 4 \\%$ localization accuracy. XL objects, however, tend to be the hardest to localize with only 73.4% localization accuracy. We conclude that the appearance of L objects must be easier for the model to learn, while XL objects tend to appear in distinctive backgrounds. The image background make these XL classes easier for the image-level classifier, but the individual instances are difficult to accurately localize. Some examples of L objects are “killer whale,” “schooner,” and “lion,” and some examples of XL objects are “boathouse,” “mosque,” “toyshop” and “steel arch bridge.”\n\nIn Figure 14(top,right) corresponding to the object detection task, the influence of real-world object size is not as apparent. One of the key reasons is that many of the XL and L object classes of the image classification and single-object localization datasets were removed in constructing the detection dataset (Section 3.3.1) since they were not basic categories well-suited for detection. There were only 3 XL object classes remaining in the dataset (“train,” “airplane” and “bus”), and none after scale normalization.We omit them from the analysis. The average precision of XS, S, M objects (44.5%, $3 9 . 0 \\%$ , and 38.5% mAP respectively) is statistically insignificant from average precision on L objects: 95% confidence interval of L objects is $3 7 . 5 \\% - 5 9 . 5 \\%$ . This may be due to the fact that there are only 6 L object classes remaining after scale normalization; all other real-world size bins have at least 18 object classes.\n\nFinally, it is interesting that performance on XS objects of $4 4 . 5 \\%$ mAP (CI 40.5% − 47.6%) is statistically significantly better than performance on S or M objects with 39.0% mAP and 38.5% mAP respectively. Some examples of XS objects are “strawberry,” “bow tie” and “rugby ball.”",
      "referring_paragraphs": [
        "Figure 11 (top) shows a random set of 10 of them.",
        "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The numbers in parentheses indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with $1 0 0 \\%$ accuracy. Object detection results are shown in Figure 12.",
        "Fig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst results. The numbers in parentheses indicate average precision. Image classification and single-object localization results are shown in Figure 11."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1409.0575_page0_fig75.jpg",
        "1409.0575_page0_fig76.jpg",
        "1409.0575_page0_fig77.jpg",
        "1409.0575_page0_fig78.jpg",
        "1409.0575_page0_fig79.jpg",
        "1409.0575_page0_fig80.jpg",
        "1409.0575_page0_fig81.jpg",
        "1409.0575_page0_fig82.jpg",
        "1409.0575_page0_fig83.jpg",
        "1409.0575_page0_fig84.jpg",
        "1409.0575_page0_fig85.jpg",
        "1409.0575_page0_fig86.jpg",
        "1409.0575_page0_fig87.jpg",
        "1409.0575_page0_fig88.jpg",
        "1409.0575_page0_fig89.jpg",
        "1409.0575_page0_fig90.jpg",
        "1409.0575_page0_fig91.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig75.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig76.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig77.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig78.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig79.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig80.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig81.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig82.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig83.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig84.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig85.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig86.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig87.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig88.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig89.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig90.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig91.jpg"
        ],
        "panel_count": 18
      }
    },
    {
      "doc_id": "1409.0575",
      "figure_id": "1409.0575_fig_93",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg",
      "image_filename": "1409.0575_page0_fig92.jpg",
      "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.",
      "context_before": "",
      "context_after": "Fig. 16 Distribution of various measures of localization difficulty on the ILSVRC2012-2014 single-object localization (dark green) and PASCAL VOC 2012 (light blue) validation sets. Object scale is fraction of image area occupied by an average object instance. Chance performance of localization and level of clutter are defined in Appendix B. The plots on top contain the full ILSVRC validation set with 1000 classes; the plots on the bottom contain 200 ILSVRC classes with the lowest chance performance of localization. All plots contain all 20 classes of PASCAL VOC.\n\n◦ (21) flute: a high-pitched musical instrument that looks like a straight tube and is usually played sideways (please do not confuse with oboes, which have a distinctive straw-like mouth piece and a slightly flared end)   \n◦ (22) oboe: a slender musical instrument roughly 65cm long with metal keys, a distinctive straw-like mouthpiece and often a slightly flared end (please do not confuse with flutes)   \n◦ (23) saxophone: a musical instrument consisting of a brass conical tube, often with a u-bend at the end\n\n• food: something you can eat or drink (includes growing fruit, vegetables and mushrooms, but does not include living animals)\n\n◦ food with bread or crust: pretzel, bagel, pizza, hotdog, hamburgers, etc\n\n◦ (24) pretzel   \n◦ (25) bagel, beigel   \n◦ (26) pizza, pizza pie   \n◦ (27) hotdog, hot dog, red hot   \n◦ (28) hamburger, beefburger, burger\n\n◦ (29) guacamole   \n◦ (30) burrito   \n◦ (31) popsicle (ice cream or water ice on a small wooden stick)\n\n◦ fruit\n\n◦ (32) fig   \n。(33)pineapple,ananas   \n(34)   \n◦ (34) banana （34)Danana   \n◦ (35) pome （(so)ponc   \n◦ (36) apple（27）  \n◦ (37) strawberry (3)   \n◦ (38) or   \n◦ (39) lemon\n\n◦ vegetables\n\n◦ (40) cucumber, cuke   \n◦ (41) artichoke, globe artichoke   \n◦ (42) bell pepper   \n◦ (43) head cabbage\n\n◦ (44) mushroom\n\n• items that run on electricity (plugged in or using batteries); including clocks, microphones, traffic lights, computers, etc\n\n◦ (45) remote control, remote   \n◦ electronics that blow air\n\n◦ (46) hair dryer, blow dryer   \n◦ (47) electric fan: a device for creating a current of air by movement of a   \nsurface or surfaces (please do not consider hair dryers)\n\n◦ electronics that can play music or amplify sound\n\n◦ (48) tape player   \n。(49)tape\n\n◦ (50) microphone, mike   \n◦ computer and computer peripherals: mouse, laptop, printer, keyboard, etc ◦ (51) computer mouse\n\n◦ (52) laptop, laptop computer   \n◦ (53) printer (please do not consider typewriters to be printers)   \n◦ (54) computer keyboard\n\n◦ (55) lamp\n\n◦ electric cooking appliance (an appliance which generates heat to cook food or boil water)\n\n◦ (56) microwave, microwave oven   \n◦ (57) toaster   \n◦ (58) waffle iron   \n0 (59) coffee maker:a kitchen appliance used for brewing coffee automati-\n\n(59) coff cally\n\n◦ (60) vacuum, vacuum cleaner   \n◦ (61) dishwasher, dish washer, dishwashing machine   \n◦ (62) washer, washing machine: an electric appliance for washing clothes   \n◦ (63) traffic light, traffic signal, stoplight   \n◦ (64) tv or monitor: an electronic device that represents information in visual form\n\n◦ (65) digital clock: a clock that displays the time of day digitally\n\nkitchen items: tools,utensils and appliances usually found in the kitchen\n\n◦ electric cooking appliance (an appliance which generates heat to cook food or boil water)\n\n◦ (56) microwave, microwave oven   \n◦ (57) toaster   \n◦ (58) waffle iron   \n◦ (59) coffee maker: a kitchen appliance used for brewing coffee automati-\n\ncally\n\n◦ (61) dishwasher, dish washer, dishwashing machine\n\n◦ (66) stove\n\n◦ things used to open cans/bottles: can opener or corkscrew\n\n◦ (68) corkscrew\n\n◦ (69) cocktail shaker\n\n◦ non-electric item commonly found in the kitchen: pot, pan, utensil, bowl, etc\n\n◦ (70) strainer   \n◦ (71) frying pan (skillet)   \n◦ (72) bowl: a dish for serving food that is round, open at the top, and has no handles (please do not confuse with a cup, which usually has a handle and is used for serving drinks)   \n◦ (73) salt or pepper shaker: a shaker with a perforated top for sprinkling salt or pepper   \n◦ (74) plate rack   \n◦ (75) spatula: a turner with a narrow flexible blade   \n◦ (76) ladle: a spoon-shaped vessel with a long handle; frequently used to   \ntransfer liquids from one container to another\n\n◦ (77) refrigerator, icebox\n\n• furniture (including benches)\n\n◦ (78) bookshelf: a shelf on which to keep books\n\n◦ (79) baby bed: small bed for babies, enclosed by sides to prevent baby from falling\n\n◦ (80) filing cabinet: office furniture consisting of a container for keeping papers in order\n\n◦ (81) bench (a long seat for several people, typically made of wood or stone) ◦ (82) chair: a raised piece of furniture for one person to sit on; please do not confuse with benches or sofas, which are made for more people\n\n◦ (83) sofa, couch: upholstered seat for more than one person; please do not confuse with benches (which are made of wood or stone) or with chairs (which are for just one person)\n\n◦ (84) table\n\n• clothing, article of clothing: a covering designed to be worn on a person’s body ◦ (85) diaper: Garment consisting of a folded cloth drawn up between the legs and fastened at the waist; worn by infants to catch excrement\n\n◦ swimming attire: clothes used for swimming or bathing (swim suits, swim trunks, bathing caps)\n\n◦ (86) swimming trunks: swimsuit worn by men while swimming ◦ (87) bathing cap, swimming cap: a cap worn to keep hair dry while swimming or showering\n\n◦ (88) maillot: a woman’s one-piece bathing suit\n\n◦ necktie: a man’s formal article of clothing worn around the neck (including bow ties)\n\n◦ (89) bow tie: a man’s tie that ties in a bow\n\n◦ (90) tie: a long piece of cloth worn for decorative purposes around the neck or shoulders, resting under the shirt collar and knotted at the throat (NOT a bow tie)\n\n◦ headdress, headgear: clothing for the head (hats, helmets, bathing caps, etc) ◦ (87) bathing cap, swimming cap: a cap worn to keep hair dry while swimming or showering\n\n◦ (91) hat with a wide brim\n\n◦ (92) helmet: protective headgear made of hard material to resist blows\n\n◦ (93) miniskirt, mini: a very short skirt\n\n◦ (94) brassiere, bra: an undergarment worn by women to support their breasts\n\n◦ (95) sunglasses\n\n• living organism (other than people): dogs, snakes, fish, insects, sea urchins, starfish, etc.\n\n◦ living organism which can fly\n\n◦ (96) bee\n\n◦ (97) dragonfly\n\n◦ (99) butterfly\n\no(99)butte ◦ (100) bird\n\n◦ living organism which cannot fly (please don’t include humans)\n\n◦ living organism with 2 or 4 legs (please don’t include humans):\n\n◦ mammals (but please do not include humans)\n\nfeline (cat-like) animal: cat, tiger or lion\n\n◦ (101) domestic cat\n\n◦ (102) tiger\n\n◦ (103) lion\n\n◦ canine (dog-like animal): dog, hyena, fox or wolf\n\n◦ (104) dog, domestic dog, canis familiaris\n\n◦ (105) fox: wild carnivorous mammal with pointed muzzle and ears and a bushy tail (please do not confuse with dogs)\n\n◦ animals with hooves: camels, elephants, hippos, pigs, sheep, etc\n\n◦ (106) elephant\n\n◦ (107) hippopotamus, hippo\n\n◦ (108) camel\n\n◦ (109) swine: pig or boar\n\n◦ (110) sheep: woolly animal, males have large spiraling horns (please\n\ndo not confuse with antelope which have long legs)\n\n◦ (111) cattle: cows or oxen (domestic bovine animals)\n\n◦ (112) zebra\n\n◦ (113) horse\n\n◦ (114) antelope: a graceful animal with long legs and horns directed\n\nupward and backward\n\n◦ (115) squirrel\n\n◦ (116) hamster: short-tailed burrowing rodent with large cheek pouches ◦ (117) otter\n\n◦ (118) monkey\n\n◦ (119) koala\n\n◦ (120) bear (other than pandas)\n\n◦ (121) skunk (mammal known for its ability fo spray a liquid with a strong odor; they may have a single thick stripe across back and tail, two thinner stripes, or a series of white spots and broken stripes\n\n◦ (122) rabbit\n\n◦ (123) giant panda: an animal characterized by its distinct black and white markings\n\n◦ (124) red panda: Reddish-brown Old World raccoon-like carnivore\n\n◦ (125) frog, toad\n\n◦ (126) lizard: please do not confuse with snake (lizards have legs)\n\n◦ (127) turtle\n\n◦ (128) armadillo\n\n◦ (129) porcupine, hedgehog\n\n◦ living organism with 6 or more legs: lobster, scorpion, insects, etc.\n\n◦ (130) lobster: large marine crustaceans with long bodies and muscular tails; three of their five pairs of legs have claws\n\n◦ (131) scorpion\n\n◦ (132) centipede: an arthropod having a flattened body of 15 to 173 segments each with a pair of legs, the foremost pair being modified as prehensors\n\n◦ (133) tick (a small creature with 4 pairs of legs which lives on the blood of mammals and birds)\n\n◦ (134) isopod: a small crustacean with seven pairs of legs adapted for crawling\n\n◦ (135) ant\n\n◦ living organism without legs: fish, snake, seal, etc. (please don’t include plants)\n\n◦ living organism that lives in water: seal, whale, fish, sea cucumber, etc. ◦ (136) jellyfish\n\n◦ (137) starfish, sea star (1.37\n\n0(138)seal\n\n（(1oo)scar\n\n◦ (139) whale 0(140)rav:a marine animal with a horizontally flattened body and\n\n◦ (140) ray: a marine animal with a horizontally flattene enlargedray:glikeareetralfinswithaiorizotheyndarside\n\nenlarged winglike pectoral fins with gills on the un ◦ (141) goldfish: small golden or orange-red fishes or orange-red fishes\n\ngol olivingOrganismthatslidesonland:wormCsnailsnake\n\nnv(142)ignail ◦ (142)\n\n◦ (143) snake: please do not confuse with lizard (snakes do not have legs)\n\n• vehicle: any object used to move people or objects from place to place\n\n◦ a vehicle with wheels\n\n◦ (144) golfcart, golf cart\n\n◦ (145) snowplow: a vehicle used to push snow from roads\n\n◦ (146) motorcycle (or moped)\n\n◦ (147) car, automobile (not a golf cart or a bus)\n\n◦ (148) bus: a vehicle carrying many passengers; used for public transport\n\n◦ (149) train\n\n◦ (150) cart: a heavy open wagon usually having two wheels and drawn by an animal\n\n◦ (151) bicycle, bike: a two wheeled vehicle moved by foot pedals\n\n◦ (152) unicycle, monocycle\n\n◦ a vehicle without wheels (snowmobile, sleighs)\n\n◦ (153) snowmobile: tracked vehicle for travel on snow\n\n◦ (154) watercraft (such as ship or boat): a craft designed for water trans-\n\nportatio\n\n◦ (155) airplane: an aircraft powered by propellers or jets\n\n• cosmetics: toiletry designed to beautify the body\n\n◦ (156) face powder\n\n◦ (157) perfume, essence (usually comes in a smaller bottle than hair spray\n\n◦ (158) hair spray\n\n◦ (159) cream, ointment, lotion\n\n◦ (160) lipstick, lip rouge\n\n• carpentry items: items used in carpentry, including nails, hammers, axes, screwdrivers, drills, chain saws, etc\n\n◦ (161) chain saw, chainsaw\n\n◦ (162) nail: pin-shaped with a head on one end and a point on the other\n\n◦ (163) axe: a sharp tool often used to cut trees/ logs\n\n◦ (164) hammer: a blunt hand tool used to drive nails in or break things apart\n\n(please do not confuse with axe, which is sharp)\n\n◦ (165) screwdriver\n\n◦ (166) power drill: a power tool for drilling holes into hard materials\n\n• school supplies: rulers, erasers, pencil sharpeners, pencil boxes, binders\n\n◦ (167) ruler,rule: measuring stick consisting of a strip of wood or metal or plastic with a straight edge that is used for drawing straight lines and measuring lengths\n\n◦ (168) rubber eraser, rubber, pencil eraser\n\n◦ (169) pencil sharpener\n\n◦ (170) pencil box, pencil case\n\n◦ (171) binder, ring-binder\n\n• sports items: items used to play sports or in the gym (such as skis, raquets, gymnastics bars, bows, punching bags, balls)\n\n◦ (172) bow: weapon for shooting arrows, composed of a curved piece of resilient wood with a taut cord to propel the arrow\n\n◦ (173) puck, hockey puck: vulcanized rubber disk 3 inches in diameter that\n\nis used instead of a ball in ice hockey\n\n◦ (174) ski\n\n◦ (175) racket, racquet\n\n◦ gymnastic equipment: parallel bars, high beam, etc\n\n◦ (176) balance beam: a horizontal bar used for gymnastics which is raised from the floor and wide enough to walk on\n\n◦ (177) horizontal bar, high bar: used for gymnastics; gymnasts grip it with g\n\ntheir hands (please do not confuse with balance beam, which is wide enough to walk on)\n\n◦ ball\n\n◦ (178) golf ball\n\n0(179)baseball\n\no(180)basketball\n\n2(181) Lel\n\n◦ (181) croquet ba ◦ (182) soccer ball\n\no（182）soccer ba （183)\n\n◦ (183) ping-pong ball\n\n◦ (184) rugby ball\n\n◦ (185) volleyball\n\n◦ (186) tennis ball\n\n◦ (187) punching bag, punch bag, punching ball, punchball\n\n◦ (188) dumbbell: An exercising weight; two spheres connected by a short bar that serves as a handle\n\n• liquid container: vessels which commonly contain liquids such as bottles, cans, etc.\n\n◦ (189) pitcher: a vessel with a handle and a spout for pouring\n\n◦ (190) beaker: a flatbottomed jar made of glass or plastic; used for chemistry\n\n◦ (191) milk can\n\n◦ (192) soap dispenser\n\n◦ (193) wine bottle\n\n◦ (194) water bottle\n\n(195)cup or mug (usually with a handle and usually cylindrical)\n\n• bag\n\n◦ (196) backpack: a bag carried by a strap on your back or shoulder\n\n◦ (197) purse: a small bag for carrying money\n\n◦ (198) plastic bag\n\n(199) person\n\n• (200) flower pot: a container in which plants are cultivated\n\n[Section: Appendix E Modification to bounding box system for object detection]\n\nThe bounding box annotation system described in Section 3.2.1 is used for annotating images for both the single-object localization dataset and the object detection dataset. However, two additional manual postprocessing are needed to ensure accuracy in the object detection scenario:\n\nAmbiguous objects. The first common source of error was that workers were not able to accurately differentiate some object classes during annotation. Some commonly confused labels were seal and sea otter, backpack and purse, banjo and guitar, violin and cello, brass instruments (trumpet, trombone, french horn and brass), flute and oboe, ladle and spatula. Despite our best efforts (providing positive and negative example images in the annotation task, adding text explanations to alert the user to the distinction between these categories) these errors persisted.\n\nIn the single-object localization setting, this problem was not as prominent for two reasons. First, the way the data was collected imposed a strong prior on the object class which was present. Second, since only one object category needed to be annotated per image, ambiguous images could be discarded: for example, if workers couldn’t agree on whether or not a trumpet was in fact present, this image could simply be removed. In contrast, for the object detection setting consensus had to be reached for all target categories on all images.\n\nTo fix this problem, once bounding box annotations were collected we manually looked through all cases where the bounding boxes for two different object classes had significant overlap with each other (about 3% of the collected boxes). About a quarter of these boxes were found to correspond to incorrect objects and were removed. Crowdsourcing this post-processing step (with very stringent accuracy constraints) would be possible but it occurred in few enough cases that it was faster (and more accurate) to do this in-house.\n\nDuplicate annotations. The second common source of error were duplicate bounding boxes drawn on the same object instance. Despite instructions not to draw more than one bounding box around the same object instance and constraints in the annotation UI enforcing at least a 5 pixel difference between different bounding boxes, these errors persisted. One reason was that sometimes the initial bounding box was not perfect and subsequent labelers drew a slightly improved alternative.\n\nThis type of error was also present in the singleobject localization scenario but was not a major cause for concern. A duplicate bounding box is a slightly perturbed but still correct positive example, and singleobject localization is only concerned with correctly localizing one object instance. For the detection task algorithms are evaluated on the ability to localize every object instance, and penalized for duplicate detections, so it is imperative that these labeling errors are corrected (even if they only appear in about $0 . 6 \\%$ of cases).\n\nApproximately 1% of bounding boxes were found to have significant overlap of more than 50% with an-\n\nother bounding box of the same object class.We again manually verified all of these cases in-house. In approximately 40% of the cases the two bounding boxes correctly corresponded to different people in a crowd, to stacked plates, or to musical instruments nearby in an orchestra. In the other 60% of cases one of the boxes was randomly removed.\n\nThese verification steps complete the annotation procedure of bounding boxes around every instance of every object class in validation, test and a subset of training images for the detection task.\n\nTraining set annotation. With the optimized algorithm of Section 3.3.3 we fully annotated the validation and test sets. However, annotating all training images with all target object classes was still a budget challenge. Positive training images taken from the single-object localization dataset already had bounding box annotations of all instances of one object class on each image. We extended the existing annotations to the detection dataset by making two modification. First, we corrected any bounding box omissions resulting from merging fine-grained categories: i.e., if an image belonged to the ”dalmatian” category and all instances of ”dalmatian” were annotated with bounding boxes for single-object localization, we ensured that all remaining ”dog” instances are also annotated for the object detection task. Second, we collected significantly more training data for the person class because the existing annotation set was not diverse enough to be representative (the only people categories in the single-object localization task are scuba diver, groom, and ballplayer). To compensate, we additionally annotated people in a large fraction of the existing training set images.",
      "referring_paragraphs": [
        "Table 1 shows summary statistics.",
        "Table 1 Overview of the provided annotations for each of the tasks in ILSVRC.",
        "Figure 1 visualizes the diversity of the ILSVRC2012 object categories.",
        "These properties are illustrated in Figure 1.",
        "The x-axis corresponds to object properties annotated by human labelers for each object class (Russakovsky et al., 2013) and illustrated in Figure 1."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1409.0575_page0_fig93.jpg",
        "1409.0575_page0_fig94.jpg",
        "1409.0575_page0_fig95.jpg",
        "1409.0575_page0_fig96.jpg",
        "1409.0575_page0_fig97.jpg",
        "1409.0575_page0_fig98.jpg",
        "1409.0575_page0_fig99.jpg",
        "1409.0575_page0_fig100.jpg",
        "1409.0575_page0_fig101.jpg",
        "1409.0575_page0_fig102.jpg",
        "1409.0575_page0_fig103.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig93.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig94.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig95.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig96.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig97.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig98.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig99.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig100.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig101.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig102.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig103.jpg"
        ],
        "panel_count": 12
      }
    }
  ],
  "1412.3756": [
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg",
      "image_filename": "1412.3756_page0_fig0.jpg",
      "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.",
      "context_before": "[Section: 5 Removing disparate impact]\n\nOnce Bob’s certification procedure has made a determination of (potential) disparate impact on $D$ , Alice might request a repaired version $\\bar { D }$ of $D$ , where any attributes in $D$ that could be used to predict X have been changed so that $\\bar { D }$ would be certified as $\\epsilon$ -fair. We now describe how to construct such a set $\\breve { \\bar { D } } = ( X , \\bar { Y } , C )$ such that $\\bar { D }$ does not have disparate impact in terms of protected attribute X. While for notational simplicity we will assume that X is used directly in what follows, in practice the attribute used to stratify the data for repair need not directly be the protected attribute or even a single protected attribute. In the case of the Texas Top $1 0 \\%$ Rule that admits the top ten percent of every high school class in Texas to the University of Texas [23], the attribute used to stratify is the high school attended, which is an attribute that correlates with race. If repair of multiple protected attributes is desired, the joint distribution can be used to stratify the data. (We will look into the effects of this experimentally in Section 6.2.)\n\nOf course, it is important to change the data in such a way that predicting the class is still possible. Specifically, our goal will be to preserve the relative per-attribute ordering as follows. Given protected attribute X and a single numerical attribute $Y$ , let $Y _ { x } = \\mathrm { P r } ( Y | \\bar { X } =$ x) denote the marginal distribution on Y conditioned on $X = x$ . Let $F _ { x } : Y _ { x }  [ 0 , 1 ]$ be the cumulative distribution function for values $y \\in Y _ { x }$ and let $F _ { x } ^ { - 1 } : [ 0 , 1 ] \\to Y _ { x }$ be the associated quantile function (i.e $F _ { x } ^ { - 1 } ( 1 / 2 )$ is the value of $y$ such that $\\operatorname* { P r } ( Y \\geq y | X = x ) =$ 1/2). We will say that $F _ { x }$ ranks the values of $Y _ { x }$ .\n\nLet $\\bar { Y }$ be the repaired version of $Y$ in $\\bar { D }$ . We will say that $\\bar { D }$ strongly preserves rank if for any $y \\in Y _ { x }$ and $x \\in X .$ , its “repaired” counterpart $\\bar { y } \\in \\bar { Y } _ { x }$ has $F _ { x } ( y ) = F _ { x } ( \\bar { y } )$ . Strongly preserving rank in this way, despite changing the true values of Y, appears to allow Alice’s algorithm to continue choosing stronger (higher ranked) applicants over weaker ones. We present experimental evidence for this in Section 6.\n\nWith this motivation, we now give a repair algorithm that strongly preserves rank and ensures that $\\bar { D } \\ = \\ ( X , \\bar { Y } , C )$ is fair (i.e., is $\\epsilon$ -fair for $\\epsilon = 1 / 2 $ ). In the discussion that follows, for the sake of clarity we will treat $Y$ as a single attribute over a totallyordered domain. To handle multiple totally-ordered attributes $Y _ { 1 } , \\dots , Y _ { k }$ we will repair each attribute individually.\n\nWe define a “median” distribution $A$ in terms of its quantile function $F _ { A } ^ { - 1 } \\colon F _ { A } ^ { - 1 } ( u ) =$ median $_ { x \\in X } F _ { x } ^ { - 1 } ( u )$ . The choice of the term “median” is not accidental.\n\nLemma 5.1. Let A be a distribution such that $F _ { A } ^ { - 1 } ( u ) = m e d i a n _ { x \\in X } F _ { x } ^ { - 1 } ( u )$ $\\boldsymbol { x } \\in \\boldsymbol { X } ^ { F _ { x } ^ { - 1 } } \\left( u \\right)$ . Then A is also the distribution minimizing $\\Sigma _ { x \\in X } d ( Y _ { x } , C )$ over all distributions C, where $d ( \\cdot , \\cdot )$ is the earthmover distance on R.\n\nProof. For any two distributions $P$ and $Q$ on the line, the earthmover distance (using the underlying Euclidean distance $d ( x , y ) = | x - y |$ as the metric) can be written as\n\nIn other words, the map $P \\mapsto F _ { P } ^ { - 1 }$ is an isometric embedding of the earthmover distance into $\\ell _ { 1 }$ .\n\nConsider now a set of points $p _ { 1 } , \\ldots , p _ { n } \\in \\ell _ { 1 }$ . Their 1-median – the point minimizing $\\textstyle \\sum _ { i } \\| p _ { i } - c \\| _ { 1 } - \\mathrm { i } s$ the point whose $j ^ { \\mathrm { t h } }$ coordinate is the median of the $j ^ { \\mathrm { t h } }$ coordinates of the $p _ { i }$ . This is precisely the definition of the distribution $A$ (in terms of $\\dot { F } _ { A } ^ { - 1 }$ ). □\n\nAlgorithm. Our repair algorithm creates $\\bar { Y } _ { \\cdot }$ , such that for all $y \\in Y _ { x } ,$ the corresponding $\\bar { y } = F _ { A } ^ { - 1 } ( F _ { x } ( y ) )$ . The resulting $\\bar { D } = ( X , \\bar { Y } , C )$ changes only Y while the protected attribute and class remain the same as in the original data, thus preserving the ability to predict the class. See Figure 1 for an example.\n\nNotes. We note that this definition is reminiscent of the method by which partial rankings are combined to form a total ranking. The rankings are “Kemeny”-ized by finding a ranking that minimizes the sum of distances to the original rankings. However, there is a crucial difference in our procedure. Rather than merely reorganizing the data into a total ranking, we are modifying the data to construct this consensus distribution.\n\nTheorem 5.1. $\\bar { D }$ is fair and strongly preserves rank.\n\nProof. In order to show that $\\bar { D }$ strongly preserves rank, recall that we would like to show that $F _ { x } ( y ) = F _ { x } ( \\bar { y } )$ for all $x \\in X , \\bar { y } \\in \\bar { Y } _ { x } ,$ , and $y \\in Y _ { x }$ . Since, by definition of our algorithm, $\\bar { y } = F _ { A } ^ { - 1 } ( F _ { x } ( y ) ) .$ , we know that $F _ { x } ( \\bar { y } ) = F _ { x } ( F _ { A } ^ { - 1 } ( F _ { x } ( y ) ) )$ , so we would like to show that $F _ { x } ( F _ { A } ^ { - 1 } ( z ) ) = z$ for all $z \\in [ 0 , 1 ]$ and for all $x$ . Recall that $F _ { A } ^ { - 1 } ( z ) = \\mathrm { m e d i a n } _ { x \\in X } F _ { x } ^ { - 1 } ( z )$ .\n\nSuppose the above claim is not true. Then there are two values $z _ { 1 } < z _ { 2 }$ and some value $x$ such that $F _ { x } ( F _ { A } ^ { - 1 } ( z _ { 1 } ) ) > F _ { x } ( F _ { A } ^ { - 1 } ( z _ { 2 } ) )$ . That is, there is some $x$ and two elements $y _ { 1 } = F _ { A } ^ { - 1 } ( z _ { 1 } ) , y _ { 2 } = F _ { A } ^ { - \\bar { 1 } } \\bar { ( z _ { 2 } ) }$ such that $y _ { 1 } > y _ { 2 }$ . Now we know that y1 = median $_ { x \\in X } F _ { x } ^ { - 1 } \\left( z _ { 1 } \\right)$",
      "context_after": "[Section: 5.1 Partial Repair]\n\nSince the repair process outlined above is likely to degrade Alice’s ability to classify accurately, she might want a partially repaired data set instead. This in effect creates a tradeoff between the ability to classify accurately and the fairness of the resulting data. This tradeoff can be achieved by simply moving each inverse quantile distribution only part way towards the median distribution. Let $\\lambda \\in [ 0 , 1 ]$ be the amount of repair desired, where $\\lambda = 0$ yields the unmodified data set and $\\lambda = 1$ is the fully repaired version described above. Recall that $F _ { x } : Y _ { x }  [ 0 , 1 ]$ is the function giving the rank of $y$ . The repair algorithm for $\\lambda = 1$ creates $\\bar { Y }$ such that $\\bar { y } = F _ { A } ^ { - 1 } ( F _ { x } ( y ) )$ where $A$ is the median distribution.\n\nIn the partial repair setting we will be creating a different distribution $A _ { x }$ for each protected value $x \\in X$ and setting $\\hat { y } = F _ { A _ { x } } ^ { - 1 } ( F _ { x } ( y ) )$ . Consider the ordered set of all $y$ at rank $u$ in their respective conditional distributions i.e the set $U ( u ) = \\{ F _ { x } ^ { - 1 } ( u ) | x \\in X \\}$ . We can associate with $U$ the cumulant function $U F ( u , y ) = | \\{ y ^ { \\prime } \\geq y | y \\in U ( u ) \\} | / | U ( u ) |$ and define the associated quantile function $U F ^ { - 1 } ( u , \\alpha ) = y$ where $U F ( u , y ) = \\alpha$ . We can restate the full repair algorithm in this formulation as follows: for any (x $\\cdot , y ) , \\bar { y } = U F ^ { - 1 } ( F _ { x } ( y ) , 1 / 2 )$ .\n\nWe now describe two different approaches to performing a partial repair, each with their own advantages and disadvantages. Intuitively, these repair methods differ in which space they operate in: the combinatorial space of ranks or the geometric space of values.\n\n[Section: 5.1.1 A Combinatorial Repair]",
      "referring_paragraphs": [
        "Table 1 describes the confusion matrix for a classification with respect to the above attributes where each entry is the probability of that particular pair of outcomes for data sampled from the input distribution (we use the empirical distribution when referring to a specific data set).",
        "See Figure 1 for an example."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg",
      "image_filename": "1412.3756_page0_fig1.jpg",
      "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.",
      "context_before": "The goal in this section is to experimentally validate our certification algorithm, described in subsection 4.2. On each of the data sets described above, we attempt to predict the protected attribute from the remaining attributes. The resulting BER is compared to $\\mathsf { D } \\mathsf { I } ( g )$ where $g : Y \\to C ,$ i.e., the disparate impact value as measured when some classifier attempts to predict the class given the non-protected attributes. From the underlying data, we can calculate the BER threshold $\\epsilon = 1 / 2 - \\beta / 8$ . Above this threshold, any classifier applied to the data will have disparate impact. The threshold is chosen conservatively so as to preclude false positives (times when we falsely declare the data to be safe from disparate impact).\n\nIn Figure 2 we can see that there are no data points greater than the BER threshold and also much below $\\tau = 0 . 8$ , the threshold for legal disparate impact. The only false positives are a few points very close to the line. This is likely because the $\\beta$ value, as measured from the data, has some error. We can also see, from the points close to the BER threshold line on its left but below τ that while we chose the threshold conservatively, we were not overly conservative. Still, using a classifier other than the purely biased one in the certification algorithm analysis might allow this threshold to be tightened.\n\nThe points in the upper left quadrant of these charts represent false negatives of our certification algorithm on a specific data set and a specific classifier. However, our certification algorithm guarantees lack of disparate impact over any classifier, so these are not false negatives in the traditional sense. In fact, when a single data set is considered over all classifiers, we see that all such data sets below the BER threshold have some classifier that has DI close to or below $\\tau = 0 . 8$ .\n\nOne seemingly surprising artifact in the charts is the vertical line in the Adult Income data chart at $\\mathrm { B E R } = 0 . 5$ for the GNB repair. Recall that the chart is based off of two different confusion matrices - the BER comes from predicting gender while the disparate impact is calculated when predicting the class. In a two class system, the BER cannot be any higher than 0.5, so while the ability to predict the gender cannot get any worse, the resulting fairness of the class predictions can still improve, thus causing the vertical line in the chart.\n\n[Section: 6.2 Fairness / Utility Tradeoff]\n\nThe goal in this section is to determine how much the partial repair procedure degrades utility. Using the same data sets as described above, we will examine how the utility (see Definition 5.3) changes DI (measuring fairness) increases. Utility will be defined with respect to the data labels. Note that this may itself be faulty data, in that the labels may not themselves provide the best possible utility based on the underlying, but perhaps",
      "context_after": "",
      "referring_paragraphs": [
        "Consider the confusion matrix associated with $g .$ , depicted in Table 2.",
        "In Figure 2 we can see that there are no data points greater than the BER threshold and also much below $\\tau = 0 ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg",
      "image_filename": "1412.3756_page0_fig2.jpg",
      "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.",
      "context_before": "",
      "context_after": "[Section: 6.3 Comparison to previous work]\n\nHere, we compare our results to related work on the German credit data and Adult income data sets. Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.’s solution [26]. All comparison data is taken from Zemel et al.’s implementations [26]. Zemel et al. define discrimination as $( 1 - \\alpha ) - \\beta$ . So that increasing Zemel scores mean that fairness has increased, as is the case with DI, we will look at the Zemel fairness score which we define as $1 - \\left( \\left( 1 - \\alpha \\right) - \\beta \\right) = 2$ · BER. Accuracy is the usual rate of successful classification. Unlike the compared works, we do not choose a single partial repair point. Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.",
      "referring_paragraphs": [
        "The confusion matrix for $\\phi$ is depicted in Table 3.",
        "The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg",
      "image_filename": "1412.3756_page0_fig3.jpg",
      "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.",
      "context_before": "[Section: 6.3 Comparison to previous work]\n\nHere, we compare our results to related work on the German credit data and Adult income data sets. Logistic regression is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al.’s solution [26]. All comparison data is taken from Zemel et al.’s implementations [26]. Zemel et al. define discrimination as $( 1 - \\alpha ) - \\beta$ . So that increasing Zemel scores mean that fairness has increased, as is the case with DI, we will look at the Zemel fairness score which we define as $1 - \\left( \\left( 1 - \\alpha \\right) - \\beta \\right) = 2$ · BER. Accuracy is the usual rate of successful classification. Unlike the compared works, we do not choose a single partial repair point. Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.",
      "context_after": "[Section: 7 Limitations and Future Work]\n\nOur experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.",
      "referring_paragraphs": [
        "The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1412.3756",
      "figure_id": "1412.3756_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg",
      "image_filename": "1412.3756_page0_fig4.jpg",
      "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.",
      "context_before": "[Section: 7 Limitations and Future Work]\n\nOur experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algorithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.",
      "context_after": "0.65acc RLR0.40.65 0.80 0.85 0.90 0.95 1.00Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.\n\nIn addition, our discussion of disparate impact is necessarily tied to the legal framework as defined in United States law. It would be valuable in future work to collect the legal frameworks of different jurisdictions, and investigate whether a single unifying formulation is possible.\n\nFinally, we note that the algorithm we present operates only on numerical attributes. Although we are satisfied with its performance, we chose this setting mostly for its relative theoretical simplicity. A natural avenue for future work is to investigate generalizations of our repair procedures for datasets with different attribute types, such as categorical data, vector-valued attributes, etc.\n\n[Section: 8 Acknowledgments]\n\nThis research was funded in part by the NSF under grant BIGDATA-1251049. Thanks to Deborah Karpatkin, David Robinson, and Natalie Shapero for helping us understand the legal interpretation of disparate impact. Any misunderstandings about these issues in this paper are our own. Thanks also to Mark Gould for pointing us to the Griggs v. Duke decision, which helped to set us down this path in the first place.",
      "referring_paragraphs": [
        "Figure 5 shows our fairness and accuracy results for both combinatorial and geometric partial repairs for values of $\\lambda \\in [ 0 , 1 ]$ at increments of 0.1 using all three classifiers described above.",
        "Figure 5 shows that our method can be flexible with respect to the chosen classifier."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    }
  ],
  "1511.00830": [
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg",
      "image_filename": "1511.00830_page0_fig0.jpg",
      "caption": "Figure 1: Unsupervised model",
      "context_before": "[Section: 1 INTRODUCTION]\n\nIn “Representation Learning” one tries to find representations of the data that are informative for a particular task while removing the factors of variation that are uninformative and are typically detrimental for the task under consideration. Uninformative dimensions are often called “noise” or “nuisance variables” while informative dimensions are usually called latent or hidden factors of variation. Many machine learning algorithms can be understood in this way: principal component analysis, nonlinear dimensional reduction and latent Dirichlet allocation are all models that extract informative factors (dimensions, causes, topics) of the data which can often be used to visualize the data. On the other hand, linear discriminant analysis and deep (convolutional) neural nets learn representations that are good for classification.\n\nIn this paper we consider the case where we wish to learn latent representations where (almost) all of the information about certain known factors of variation are purged from the representation while still retaining as much information about the data as possible. In other words, we want a latent representation z that is maximally informative about an observed random variable y (e.g., class label) while minimally informative about a sensitive or nuisance variable s. By treating s as a sensitive variable, i.e. s is correlated with our objective, we are dealing with “fair representations”, a problem previously considered by Zemel et al. (2013). If we instead treat s as a nuisance variable we are dealing with “domain adaptation”, in other words by removing the domain s from our representations we will obtain improved performance.\n\nIn this paper we introduce a novel model based on deep variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014). These models can naturally encourage separation between latent variables z and sensitive variables s by using factorized priors $p ( \\mathbf { s } ) p ( \\mathbf { z } )$ . However, some dependencies may still remain when mapping data-cases to their hidden representation using the variational posterior $q ( \\mathbf { z } | \\mathbf { x } , \\mathbf { s } )$ , which we stamp out using a “Maximum Mean Discrepancy” (Gretton et al., 2006) term that penalizes differences between all order moments of the marginal posterior distributions $q ( \\mathbf { z } | \\mathbf { s } = k )$ and $q ( \\mathbf { z } | \\mathbf { s } = k ^ { \\prime } )$ (for a discrete RV s). In experiments we show that this combined approach is highly successful in learning representations that are devoid of unwanted information while retaining as much information as possible from what remains.\n\n[Section: 2 LEARNING INVARIANT REPRESENTATIONS]",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1: Unsupervised model",
        "As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {}
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg",
      "image_filename": "1511.00830_page0_fig1.jpg",
      "caption": "Figure 2: Semi-supervised model",
      "context_before": "",
      "context_after": "[Section: 2.1 UNSUPERVISED MODEL]\n\nFactoring out undesired variations from the data can be easily formulated as a general probabilistic model which admits two distinct (independent) “sources”; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information. This generative process can be formally defined as:\n\nwhere $p _ { \\theta } ( \\mathbf { x } | \\mathbf { z } , \\mathbf { s } )$ is an appropriate probability distribution for the data we are modelling. With this formulation we explicitly encode a notion of ‘invariance’ in our model, since the latent representation is marginally independent of the factors of variation s. Therefore the problem of finding an invariant representation for a data point x and variation s can be cast as performing inference on this graphical model and obtaining the posterior distribution of z, $p ( \\mathbf { z } | \\mathbf { x } , \\mathbf { s } )$ .\n\nFor our model we will employ a variational autoencoder architecture (Kingma & Welling, 2014; Rezende et al., 2014); namely we will parametrize the generative model (decoder) $p _ { \\theta } ( \\mathbf { x } | \\mathbf { z } , \\mathbf { s } )$ and the variational posterior (encoder) $q _ { \\phi } ( \\mathbf { z } | \\mathbf { x } , \\mathbf { s } )$ as (deep) neural networks which accept as inputs z, s and x, s respectively and produce the parameters of each distribution after a series of non-linear transformations. Both the model $\\mathbf { \\eta } ^ { ( \\theta ) }$ and variational $( \\phi )$ parameters will be jointly optimized with the SGVB (Kingma & Welling, 2014) algorithm according to a lower bound on the log-likelihood. This parametrization will allow us to capture most of the salient information of $\\mathbf { x }$ in our embedding z. Furthermore the distributed representation of a neural network would allow us to better resolve the dependencies between x and s thus yielding a better disentangling between the independent factors z and s. By choosing a Gaussian posterior $q _ { \\phi } ( \\mathbf { z } | \\mathbf { x } , \\mathbf { s } )$ and standard isotropic Gaussian prior $p ( \\mathbf { z } ) = \\mathcal { N } ( \\mathbf { 0 } , \\mathbf { I } )$ we can obtain the following lower bound:",
      "referring_paragraphs": [
        "Figure 2: Semi-supervised model",
        "Table 2: Results on the Extended Yale B dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1511.00830_page0_fig2.jpg",
        "1511.00830_page0_fig3.jpg",
        "1511.00830_page0_fig4.jpg",
        "(a) Adult dataset",
        "1511.00830_page0_fig5.jpg",
        "1511.00830_page0_fig6.jpg",
        "(b) German dataset",
        "1511.00830_page0_fig7.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig7.jpg"
        ],
        "panel_count": 7
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_9",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg",
      "image_filename": "1511.00830_page0_fig8.jpg",
      "caption": "Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "The results for all three datasets can be seen in Figure 3.",
        "Figure 3: Fair classification results."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1511.00830_page0_fig9.jpg",
        "(c) Health dataset",
        "1511.00830_page0_fig10.jpg",
        "(a)",
        "1511.00830_page0_fig11.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig11.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_13",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg",
      "image_filename": "1511.00830_page0_fig12.jpg",
      "caption": "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.",
      "context_before": "",
      "context_after": "[Section: 3.3.2 DOMAIN ADAPTATION]\n\nAs for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also observed by Ganin et al. (2015) and Chen et al. (2012). As far as the accuracy on y\n\nis concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) (Ganin et al., 2015). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture.\n\nTable 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).   \n\n<table><tr><td rowspan=\"2\">Source - Target</td><td colspan=\"2\">S</td><td colspan=\"2\">Y</td></tr><tr><td>RF</td><td>LR</td><td>VFAE</td><td>DANN</td></tr><tr><td>books -DVD</td><td>0.535</td><td>0.564</td><td>0.799</td><td>0.784</td></tr><tr><td>books -electronics</td><td>0.541</td><td>0.562</td><td>0.792</td><td>0.733</td></tr><tr><td>books -kitchen</td><td>0.537</td><td>0.583</td><td>0.816</td><td>0.779</td></tr><tr><td>dvd -books</td><td>0.537</td><td>0.563</td><td>0.755</td><td>0.723</td></tr><tr><td>dvd -electronics</td><td>0.538</td><td>0.566</td><td>0.786</td><td>0.754</td></tr><tr><td>dvd -kitchen</td><td>0.543</td><td>0.589</td><td>0.822</td><td>0.783</td></tr><tr><td>electronics -books</td><td>0.562</td><td>0.590</td><td>0.727</td><td>0.713</td></tr><tr><td>electronics -DVD</td><td>0.556</td><td>0.586</td><td>0.765</td><td>0.738</td></tr><tr><td>electronics -kitchen</td><td>0.536</td><td>0.570</td><td>0.850</td><td>0.854</td></tr><tr><td>kitchen -books</td><td>0.560</td><td>0.593</td><td>0.720</td><td>0.709</td></tr><tr><td>kitchen -DVD</td><td>0.561</td><td>0.599</td><td>0.733</td><td>0.740</td></tr><tr><td>kitchen -electronics</td><td>0.533</td><td>0.565</td><td>0.838</td><td>0.843</td></tr></table>\n\n[Section: 3.4 LEARNING INVARIANT REPRESENTATIONS]",
      "referring_paragraphs": [
        "In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the $\\mathbf { z } _ { 1 }$ representations, obtained from the model trained on the Adult dataset, in Figure 4.",
        "Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ without s and MMD, (c): latent $\\mathbf { z } _ { 1 }$ with s and without MMD, (d): latent $\\mathbf { z } _ { 1 }$ with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(b)",
        "(c)",
        "1511.00830_page0_fig13.jpg",
        "(d)",
        "1511.00830_page0_fig14.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig14.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_16",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg",
      "image_filename": "1511.00830_page0_fig15.jpg",
      "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.",
      "context_before": "Regarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions. As soon as we utilize our VFAE model we simultaneously decrease the accuracy on s, from $96 \\%$ to about $50 \\%$ , and increase our accuracy on y, from $78 \\%$ to about $85 \\%$ . This effect can also be seen in Figure 5b: the images are now mostly clustered according to the person ID (the label y). It is clear that in this scenario the information about s is purely “nuisance” with respect to the labels y. Therefore, by using our VFAE model we are able to obtain improved generalization and classification performance by effectively removing s from our representations.\n\nTable 2: Results on the Extended Yale B dataset. We also included the best result from Li et al. (2014) under the $\\mathrm { N N } + \\mathrm { M M D }$ row.   \n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">S</td><td rowspan=\"2\">Y</td></tr><tr><td>RF</td><td>LR</td></tr><tr><td>Original x</td><td>0.952</td><td>0.961</td><td>0.78</td></tr><tr><td>NN + MMD</td><td>-</td><td>-</td><td>0.82</td></tr><tr><td>VFAE</td><td>0.435</td><td>0.565</td><td>0.846</td></tr></table>\n\n[Section: 4 RELATED WORK]\n\nMost related to our “fair” representations view is the work from Zemel et al. (2013). They proposed a neural network based semi-supervised clustering model for learning fair representations. The idea is to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster gets assigned roughly equal proportions of data from each group in s. Although their approach was successfully applied on several datasets, the restriction to clustering means that it cannot leverage the representational power of a distributed representation. Furthermore, this penalty does not account for higher order moments in the latent distribution. For example, if $p ( z _ { k } ^ { - } = 1 | x _ { i } , s = 0 )$ always",
      "context_after": "[Section: 5 CONCLUSION]\n\nWe introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible. We further use a Maximum Mean Discrepancy regularizer in order to further promote invariance in the posterior distribution over latent variables. We apply this model to tasks involving developing fair classifiers that are invariant to sensitive demographic information and show that it produces a better tradeoff with respect to accuracy and invariance. As a second application, we consider the task of domain adaptation, where the goal is to improve classification by training a classifier that is invariant to the domain. We find that our model is competitive with recently proposed adversarial approaches. Finally, we also consider the more general task of learning invariant representations. We can observe that our model provides a clear improvement against a neural network that incorporates a Maximum Mean Discrepancy penalty.\n\n[Section: REFERENCES]",
      "referring_paragraphs": [
        "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1511.00830_page0_fig16.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig16.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1511.00830",
      "figure_id": "1511.00830_fig_18",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg",
      "image_filename": "1511.00830_page0_fig17.jpg",
      "caption": "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))",
      "context_before": "[Section: B PROXY A-DISTANCE (PAD) FOR AMAZON REVIEWS DATASET]\n\nSimilarly to Ganin et al. (2015), we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the $\\mathbf { z } _ { 1 }$ representations of VFAE. Briefly, Proxy Adistance is an approximation to the $\\mathcal { H }$ -divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010). To compute it we first need to train a learning algorithm on the task of discriminating examples from the source and target domain. Afterwards we can use the test error $\\epsilon$ of that algorithm in the following formula:\n\nIt is clear that low PAD scores correspond to low discrimination of the source and target domain examples from the classifier. To obtain $\\epsilon$ for our model we used Logistic Regression. The resulting plot can be seen in Figure 6, where we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference. It can be seen that our VFAE model can factor out the information about s better, since the PAD scores on our new representation are, overall, lower than the ones obtained from the DANN architecture.",
      "context_after": "",
      "referring_paragraphs": [
        "The resulting plot can be seen in Figure 6, where we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classifier, as a reference.",
        "Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))"
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1511.00830_page0_fig18.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig18.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1602.05352": [
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg",
      "image_filename": "1602.05352_page0_fig0.jpg",
      "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .",
      "context_before": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missingdata imputation based on the joint likelihood of the missing data model and the rating model (Marlin et al., 2007; Marlin & Zemel, 2009; Hernandez-Lobato et al. ´ , 2014). This has led to sophisticated and highly complex methods. We take a fundamentally different approach that treats both models separately, making our approach modular and scalable. Furthermore, our approach is robust to mis-specification of the rating model, and we characterize how the overall learning process degrades gracefully under a mis-specified missing-data model. We empirically compare against the state-of-the-art joint likelihood model (Hernandez-Lobato et al. ´ , 2014) in this paper.\n\nRelated but different from the problem we consider is recommendation from positive feedback alone (Hu et al., 2008; Liang et al., 2016). Related to this setting are also alternative approaches to learning with MNAR data (Steck, 2010; 2011; Lim et al., 2015), which aim to avoid the problem by considering performance measures less affected by selection bias under mild assumptions. Of these works, the approach of Steck (2011) is most closely related to ours, since it defines a recall estimator that uses item popularity as a proxy for propensity. Similar to our work, Steck (2010; 2011) and Hu et al. (2008) also derive weighted matrix factorization methods, but with weighting schemes that are either heuristic or need to be tuned via cross validation. In contrast, our weighted matrix factorization method enjoys rigorous learning guarantees in an ERM framework.\n\nPropensity-based approaches have been widely used in causal inference from observational studies (Imbens & Rubin, 2015), as well as in complete-case analysis for missing data (Little & Rubin, 2002; Seaman & White, 2013) and in survey sampling (Thompson, 2012). However, their use in matrix completion is new to our knowledge. Weighting approaches are also widely used in domain adaptation and covariate shift, where data from one source is used to train for a different problem (e.g., Huang et al., 2006; Bickel et al., 2009; Sugiyama & Kawanabe, 2012). We will draw upon this work, especially the learning theory of weighting approaches in (Cortes et al., 2008; 2010).\n\n[Section: 3. Unbiased Performance Estimation for Recommendation]\n\nConsider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings. Denote with $u ~ \\in ~ \\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }$ for our toy example, where a sub-",
      "context_after": "[Section: 3.1. Task 1: Estimating Rating Prediction Accuracy]\n\nFor the first task, we want to evaluate how well a predicted rating matrix $\\hat { Y }$ reflects the true ratings in $Y$ . Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as:\n\nfor an appropriately chosen $\\delta _ { u , i } ( Y , \\hat { Y } )$ .\n\nMAE: (2)\n\nMSE: (3)\n\nAccuracy: (4)\n\nSince $Y$ is only partially known, the conventional practice is to estimate $\\dot { R ( Y ) }$ using the average over only the observed entries,",
      "referring_paragraphs": [
        "Consider a toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of held-out ratings. Denote with $u ~ \\in ~ \\{ 1 , . . . , U \\}$ the users and with $i \\in \\{ 1 , . . . , I \\}$ the movies. Figure 1 shows the matrix of true ratings $\\dot { Y } \\in \\mathfrak { R } ^ { U \\times I }$ for our toy example, where a sub-",
        "Figure 1.",
        "We call this the naive estimator, and its naivety leads to a gross misjudgment for the $\\hat { Y } _ { 1 }$ and $\\hat { Y } _ { 2 }$ given in Figure 1.",
        "An example is $\\hat { Y } _ { 3 }$ in Figure 1.",
        "Note that the IPS estimator only requires the marginal probabilities $P _ { u , i }$ and unbiased-ness is not affected by dependencies within $O$ :\n\nTable 1.",
        "Table 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via $\\operatorname { D C G } @ 5 0$ for the following five prediction matrices $\\hat { Y _ { i } }$ .",
        "Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when $\\alpha = 0 ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1602.05352_page0_fig1.jpg",
        "1602.05352_page0_fig2.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg",
      "image_filename": "1602.05352_page0_fig3.jpg",
      "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.",
      "context_before": "In all experiments, we perform model selection for the regularization parameter $\\lambda$ and/or the rank of the factorization $d$ via cross-validation as follows. We randomly split the observed MNAR ratings into $k$ folds $k = 4$ in all experiments), training on $k - 1$ and evaluating on the remaining one using the IPS estimator. Reflecting this additional split requires scaling the propensities in the training folds by $\\textstyle { \\frac { k - 1 } { k } }$ and those in the validation fold by $\\frac { 1 } { k }$ . The parameters with the best validation set performance are then used to retrain on all MNAR data. We finally report performance on the MCAR test set for the real-world datasets, or using Eq. (1) for our semi-synthetic dataset.\n\n[Section: 6.2. How does sampling bias severity affect evaluation?]\n\nFirst, we evaluate how different observation models impact the accuracy of performance estimates. We compare the Naive estimator of Eq. (5) for MSE, MAE and DCG with their propensity-weighted analogues, IPS using Eq. (10) and SNIPS using Eq. (11) respectively. Since this experiment requires experimental control of sampling bias, we created a semi-synthetic dataset and observation model.\n\nML100K Dataset. The ML100K dataset4 provides 100K MNAR ratings for 1683 movies by 944 users. To allow ground-truth evaluation against a fully known rating matrix, we complete these partial ratings using standard matrix factorization. The completed matrix, however, gives",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2.",
        "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation\n\nTable 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg",
      "image_filename": "1602.05352_page0_fig4.jpg",
      "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).",
      "context_before": "",
      "context_after": "[Section: 6.3. How does sampling bias severity affect learning?]\n\nNow we explore whether these gains in risk estimation accuracy translate into improved learning via ERM, again in the Experimental Setting. Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model with separate $\\lambda$ selected via cross-validation and $d = 2 0$ . The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials. The propensity-weighted matrix factorization MF-IPS consistently outperforms conventional matrix factorization in terms of MSE. We also conducted experiments for MAE, with similar results.\n\n[Section: 6.4. How robust is evaluation and learning to inaccurately learned propensities?]",
      "referring_paragraphs": [
        "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).",
        "The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials.",
        "Figure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1602.05352_page0_fig5.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig5.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1602.05352",
      "figure_id": "1602.05352_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg",
      "image_filename": "1602.05352_page0_fig6.jpg",
      "caption": "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.",
      "context_before": "Now we explore whether these gains in risk estimation accuracy translate into improved learning via ERM, again in the Experimental Setting. Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MF-IPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model with separate $\\lambda$ selected via cross-validation and $d = 2 0$ . The results are plotted in Figure 3 (left), where shaded regions indicate $9 5 \\%$ confidence intervals over 30 trials. The propensity-weighted matrix factorization MF-IPS consistently outperforms conventional matrix factorization in terms of MSE. We also conducted experiments for MAE, with similar results.\n\n[Section: 6.4. How robust is evaluation and learning to inaccurately learned propensities?]\n\nWe now switch from the Experimental Setting to the Observational Setting, where propensities need to be estimated. To explore robustness to propensity estimates of varying accuracy, we use the ML100K data and observation model with $\\alpha = 0 . 2 5$ . To generate increasingly bad propensity estimates, we use the Naive Bayes model from Section 5.1, but vary the size of the MCAR sample for estimating the marginal ratings $P ( Y = r )$ via the Laplace estimator.\n\nFigure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perform worse than Naive. Interestingly, IPS-NB with estimated propensities can perform even better than IPS-KNOWN with known propensities, as can be seen for MSE. This is a known effect, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).\n\nFigure 3 (right) shows how learning performance is affected by inaccurate propensities using the same setup as in Section 6.3. We compare the MSE prediction error of MF-IPS-NB with estimated propensities to that of MF-Naive and MF-IPS with known propensities. The shaded area shows the $9 5 \\%$ confidence interval over 30 trials. Again, we see that MF-IPS-NB outperforms MF-Naive even for",
      "context_after": "[Section: 6.5. Performance on Real-World Data]\n\nOur final experiment studies performance on real-world datasets. We use the following two datasets, which both have a separate test set where users were asked to rate a uniformly drawn sample of items.\n\nYahoo! R3 Dataset. This dataset5 (Marlin & Zemel, 2009) contains user-song ratings. The MNAR training set provides over 300K ratings for songs that were selfselected by 15400 users. The test set contains ratings by a subset of 5400 users who were asked to rate 10 randomly chosen songs. For this data, we estimate propensities via Naive Bayes. As a MCAR sample for eliciting the marginal rating distribution, we set aside $5 \\%$ of the test set and only report results on the remaining $9 5 \\%$ of the test set.\n\nCoat Shopping Dataset. We collected a new dataset6 simulating MNAR data of customers shopping for a coat in an online store. The training data was generated by giving Amazon Mechanical Turkers a simple web-shop interface with facets and paging. They were asked to find the coat in the store that they wanted to buy the most. Afterwards, they had to rate 24 of the coats they explored (self-selected) and 16 randomly picked ones on a five-point scale. The dataset contains ratings from 290 Turkers on an inventory of 300 items. The self-selected ratings are the training set and the uniformly selected ratings are the test set. We learn propensities via logistic regression based on user covariates (gender, age group, location, and fashion-awareness) and item covariates (gender, coat type, color, and was it promoted). A standard regularized logistic regression (Pedregosa et al., 2011) was trained using all pairs of user and item covariates as features and cross-validated to optimize log-likelihood of the self-selected observations.\n\nResults. Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation\n\nTable 2. Test set MAE and MSE on the Yahoo and Coat datasets.   \n\n<table><tr><td></td><td colspan=\"2\">YAHOO</td><td colspan=\"2\">COAT</td></tr><tr><td></td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>MF-IPS</td><td>0.810</td><td>0.989</td><td>0.860</td><td>1.093</td></tr><tr><td>MF-Naive</td><td>1.154</td><td>1.891</td><td>0.920</td><td>1.202</td></tr><tr><td>HL MNAR</td><td>1.177</td><td>2.175</td><td>0.884</td><td>1.214</td></tr><tr><td>HL MAR</td><td>1.179</td><td>2.166</td><td>0.892</td><td>1.220</td></tr></table>\n\nmodels from (Hernandez-Lobato et al. ´ , 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, $p < 0 . 0 0 1$ for all). This holds for both MAE and MSE. Furthermore, the performance of MF-IPS beats the best published results for Yahoo in terms of MSE (1.115) and is close in terms of MAE (0.770) (the CTP-v model of (Marlin & Zemel, 2009) as reported in the supplementary material of Hernandez- ´ Lobato et al. (2014)). For MF-IPS and MF-Naive all hyperparameters (i.e., $\\lambda \\in \\{ 1 0 ^ { - 6 } , . . . , 1 \\}$ and $d \\in \\{ 5 , 1 0 , 2 0 , 4 0 \\} )$ were chosen by cross-validation. For the HL baselines, we explored $d \\in \\{ 5 , 1 0 , 2 0 , 4 0 \\}$ using software provided by the authors7 and report the best performance on the test set for efficiency reasons. Note that our performance numbers for HL on Yahoo closely match the values reported in (Hernandez-Lobato et al.´ , 2014).\n\nCompared to the complex generative HL models, we conclude that our discriminative MF-IPS performs robustly and efficiently on real-world data. We conjecture that this strength is a result of not requiring any generative assumptions about the validity of the rating model. Furthermore, note that there are several promising directions for further improving performance, like propensity clipping (Strehl et al., 2010), doubly-robust estimation (Dud´ık et al., 2011), and the use of improved methods for propensity estimation (McCaffrey et al., 2004).\n\n[Section: 7. Conclusions]",
      "referring_paragraphs": [
        "Figure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2.",
        "Figure 4. RMSE of IPS and SNIPS as propensity estimates degrade. IPS with true propensities and Naive are given as reference.\n\nseverely degraded propensity estimates, demonstrating the robustness of the approach."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1602.05352_page0_fig7.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig7.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1603.07025": [
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg",
      "image_filename": "1603.07025_page0_fig0.jpg",
      "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
      "context_before": "[Section: 3 Data: Reddit as a community]\n\n[Section: 3.1 What is Reddit, briefly]\n\nReddit is one of the largest sharing and discussion communities on the Web. According to Alexa, as of late 2015 Reddit is in the top 15 sites in the U.S. and the top 35 in the world in terms of monthly unique visitors. It consists of a large number of subreddits (853,000 as of June 21st, 20152), each of which focuses on a particular purpose. Many subreddits are primarily about sharing web content from other sites: in “Pics”, “News”, “Funny”, “Gaming”, and many other communities, users (“Redditors”) make “submissions” of links posted at other sites that they think are interesting. In other subreddits, Redditors primarily write text-based “self-posts”: “AskReddit”, “IAmA”, and “ShowerThoughts” are places where people can ask questions and share stories of their own lives. Generically, we will refer to submissions and text posts as “submissions”.\n\nEach submission can be imagined as the root of a threaded comment tree, in which Redditors can comment on submissions or each other’s comments. Redditors can also vote on both submissions and comments; these votes affect the order in which submissions and comments are displayed and also form the basis of “karma”, a reputation system that tracks how often people upvote a given Redditor’s comments and submissions. We can observe these elements in Figure 1.\n\nWe choose Reddit as our target community for a number of reasons. It has existed since 2005, meaning that there has been ample time for the community to evolve and for differences in user cohorts to appear. Second, it is one of the most popular online communities, allowing different types of contributions—comments and original submissions—across many different subreddits. Third, a number of Reddit users believe that it is, in fact, getting worse over time [1, 14, 20, 46, 50, 67]. Finally, Reddit data are publicly available through an API.",
      "context_after": "",
      "referring_paragraphs": [
        "We can observe these elements in Figure 1.",
        "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.",
        "Table 1: Evolution of the average throughout the years for each cohort."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg",
      "image_filename": "1603.07025_page0_fig1.jpg",
      "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.",
      "context_before": "",
      "context_after": "[Section: 3.2 The dataset]\n\nRedditor Stuck In The Matrix used Reddit’s API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable. He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions. These datasets contain the JSON data objects returned by Reddit’s API for comments and submissions3; for our purposes, the main items of interest were the UTC creation date, the username, the subreddit, and for comments, the comment text.\n\nWe focus on submissions and comments in the dataset because they have timestamps and can be tied to specific users and subreddits, allowing us to perform time-based analyses. In some analyses, we look only at comments; in some, we combine comments and submissions, calling them “posts”. We would also like to have looked at voting behavior as a measure of user activity4, but individual votes with timestamps and usernames are not available through the API, only the aggregate number of votes that posts receive.",
      "referring_paragraphs": [
        "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month."
      ],
      "figure_type": "other",
      "sub_figures": [
        "(a)",
        "(b)",
        "1603.07025_page0_fig2.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg",
      "image_filename": "1603.07025_page0_fig3.jpg",
      "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.",
      "context_before": "[Section: 3.2 The dataset]\n\nRedditor Stuck In The Matrix used Reddit’s API to compile a dataset of almost every publicly available comment [65] from October 2007 until May 2015. The dataset is composed of 1.65 billion comments, although due to API call failures, about 350,000 comments are unavailable. He also compiled a submissions dataset for the period of October 2007 until December 2014 (made available for us upon request) containing a total of 114 million submissions. These datasets contain the JSON data objects returned by Reddit’s API for comments and submissions3; for our purposes, the main items of interest were the UTC creation date, the username, the subreddit, and for comments, the comment text.\n\nWe focus on submissions and comments in the dataset because they have timestamps and can be tied to specific users and subreddits, allowing us to perform time-based analyses. In some analyses, we look only at comments; in some, we combine comments and submissions, calling them “posts”. We would also like to have looked at voting behavior as a measure of user activity4, but individual votes with timestamps and usernames are not available through the API, only the aggregate number of votes that posts receive.",
      "context_after": "[Section: 3.3 Preprocessing the dataset]\n\nTo analyze the data, we used Google BigQuery [21], a big data processing tool. Redditor fhoffa imported the comments into BigQuery and made them publicly available [16]. We uploaded the submission data ourselves using Google’s SDK.\n\nFor the analysis in the paper, we did light preprocessing to filter out posts by deleted users, posts with no creation time, and posts by authors with bot-like names5.\n\nWe also considered only comment data from October 2007 until December 2014 in order to have a matching period for comments and submissions. After this process, we had a total of 1.17 billion comments and 114 million submissions.\n\n[Section: 3.4 An overview of the dataset]",
      "referring_paragraphs": [
        "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1603.07025_page0_fig4.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg",
      "image_filename": "1603.07025_page0_fig5.jpg",
      "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.",
      "context_before": "Here we present an overview of the dataset that shows Reddit’s overall growth. Figure 2a presents the cumulative number of user accounts and subreddits created as of the last day of every month. After an initial extremely rapid expansion from 2008–2009, the number of users and subreddits have grown exponentially. As of the end of 2014, about 16.2 million distinct users and 327 thousand subreddits made/received at least one post based on our data.\n\nHowever, as with many other online sites, most users [25, 27, 62] and communities [2] do not stay active. We define as an “active user” one that made at least one post in the month in question. Similarly, an “active subreddit” is one that received at least one post in the month. In December 2014, about 2.7 million users and 66 thousand subreddits were active, both around a fifth of the cumulative numbers. Figure 2b shows the monthly number of active users and subreddits.\n\nOur interest in this paper is not so much whether users survive as it is about the behavior of active users. Thus, in general our analysis will look only at active users and subreddits in each month; those that are temporarily or permanently gone from Reddit are not included.\n\n[Section: 3.5 Identifying cohorts]\n\nWe define the “user’s creation time” as the time of the first post made by that user. Throughout this paper, we will use the notion of user cohorts, which will consist of users created in the same calendar year.\n\nIn many cases, we will look at the evolution of these cohorts. Since users can be created at any time during their cohort year, and our dataset ends in 2014, we are likely to have a variation on the data available for each user of up to one year, even though they are in the same cohort. To deal with this, some of our cohorted analyses will consider only the overlapping time window for which we collect data for all users in a cohort. This means that we are normally not going to include the 2014 cohort in our analyses.\n\nOur data starts in October 2007, but Reddit existed before that. That means that, not only do we have incomplete data for the 2007 year (which compromises this cohort), but there might also be users and subreddits",
      "context_after": "[Section: 4 Average posts per user]\n\nOne common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks. In this section, we use this measure to address our first research question (RQ1): how does the amount of users’ activity change over time?\n\nAs we will see, both visualizing behavior relative to a user’s creation time and using cohorts provide additional insight into posting activity in Reddit compared to a straightforward aggregate analysis based on calendar time.\n\n[Section: 4.1 Calendar versus user-relative time]",
      "referring_paragraphs": [
        "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1603.07025_page0_fig6.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig6.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg",
      "image_filename": "1603.07025_page0_fig7.jpg",
      "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
      "context_before": "One common way to represent user activity in online communities is quantity: the number of posts people make over time. Approaches that consider the total number of posts per user in a particular dataset [23] and that analyze the variation of the number of posts per user over time [24] have been applied to online social networks. In this section, we use this measure to address our first research question (RQ1): how does the amount of users’ activity change over time?\n\nAs we will see, both visualizing behavior relative to a user’s creation time and using cohorts provide additional insight into posting activity in Reddit compared to a straightforward aggregate analysis based on calendar time.\n\n[Section: 4.1 Calendar versus user-relative time]\n\nFigure 3a shows that aggregate analysis, presenting the average number of posts per month by active users in that month. Taken at face value, this suggests that over the first few years of Reddit, users became more active in posting, with per-user activity remaining more or less steady since mid-2011.\n\nThis average view hides several important aspects of users’ activity dynamics. Previous work has looked into behavior relative to the user creation time. It has been shown that edge creation time in a social network relative to the user creation follows an exponential distribution [36]. User lifetime, however, does not follow a exponential distribution and some types of user content generation follow a stretched exponential distribution [24]. Throw-away accounts are one example of very short-lived users in Reddit [6], for example.\n\nTo address these characteristics, Figure 3b shows a view that emphasizes the trajectory over a user’s lifespan rather than the community’s. To do this, we scale the x-axis not by clock time, as in Figure 3a, but by time since the user’s first post: “1” on the x-axis refers to one year since the user’s account first post, and so on. We call this the time in the user referential. One caution about interpreting graphs with time in the user referential is that the amount of data available rapidly decreases over time as users leave the community, meaning that values toward the right side of an individual data series are more subject to individual variation.\n\nThe evidence at this point supports the tempting hypothesis that the longer a user survives, the more posts they make (H1). This hypothesis, however, is incorrect; we will present a more nuanced description of what is happening informed by cohort-based analyses.",
      "context_after": "[Section: 4.2 New cohorts do not catch up]\n\nFigure 3b suggests that older users are more active than newer ones, raising the question of whether new users eventually follow in older users’ footsteps (RQ1a).\n\nAnalyzing users’ behavior by cohort is a reasonable way to address this question, and Figure 4a shows a first attempt at this analysis. We can already observe a significant cohort effect: users from later cohorts appear to level off at significantly lower posting averages than users from earlier ones. It suggests that newer users likely will never be as active as older ones on average. It also shows that surviving users are significantly more active than the overall average (the black line in the figure) would suggest.\n\nHowever, Figure 4a also has an awkward anomaly: a rapid rise in the average number of posts during each cohort’s first calendar year, especially in December. Combining cohort segmentation with user-referential analysis, as in Figure 4b, helps smooth out this anomaly and aligns cohorts with each other. Doing this alignment makes clear that differences between earlier and later cohorts are apparent early on.\n\n[Section: 4.3 Does tenure predict activity, or vice versa?]",
      "referring_paragraphs": [
        "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.",
        "Figure 5 shows this analysis for the 2010, 2011 and 2012 cohorts6.",
        "Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network.",
        "Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived.",
        "For post length, Figure 5 shows that even the most active users come in at a certain activity level and stay there, perhaps even slowly declining over time."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) 2010 cohort",
        "(b) 2011 cohort",
        "1603.07025_page0_fig8.jpg",
        "(c) 2012 cohort",
        "1603.07025_page0_fig9.jpg",
        "(a)",
        "1603.07025_page0_fig10.jpg",
        "1603.07025_page0_fig11.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig11.jpg"
        ],
        "panel_count": 5
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_13",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg",
      "image_filename": "1603.07025_page0_fig12.jpg",
      "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.",
      "context_before": "",
      "context_after": "[Section: 5.1 Comment length drops over time]\n\nFigure 6a shows the overall comment length in Reddit over time (the darker line) and the overall length per cohort. Based on the downwards tendency of the overall comment length in Figure 6a, one might hypothesize that users’ commitment to the network is decreasing over time (H3), or that there is some community-wide norm toward shorter commenting (H4).\n\nHowever, this might not be the best way to interpret this information. Figure 6b shows the comment length per cohort in the user referential time. An important observation here is that younger users start from a lower baseline comment length than older ones. Considering the fact that Reddit has experienced exponential growth, the overall average for Figures 6a and 6b is heavily influenced by the ever-growing younger generations, who are more numerous than older survivors and who post shorter comments.\n\n[Section: 5.2 Simpson’s Paradox: the length also rises]",
      "referring_paragraphs": [
        "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(c) 2010 cohort",
        "(d) 2011 cohort",
        "1603.07025_page0_fig13.jpg",
        "(e) 2012 cohort",
        "1603.07025_page0_fig14.jpg",
        "(a)",
        "1603.07025_page0_fig15.jpg",
        "1603.07025_page0_fig16.jpg",
        "(c) 2008 cohort",
        "1603.07025_page0_fig17.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig17.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "1603.07025",
      "figure_id": "1603.07025_fig_19",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg",
      "image_filename": "1603.07025_page0_fig18.jpg",
      "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).",
      "context_before": "",
      "context_after": "[Section: 6.2 Comment early, comment often]\n\nFigures 7c-f shows the cohorts from 2008 to 2011 segmented by surviving year. Three interesting observations arise from these data. First, we see that just as in the analysis of average posts per user, the users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-tosubmission ratio relative to users in their cohort who abandon Reddit more quickly. This suggests that both the count of posts and the propensity to comment might be a useful early predictor of user survival.\n\nSecond, and unlike the case for average post length, surviving users’ behavior changes over time. For post length, Figure 5 shows that even the most active users come in at a certain activity level and stay there, perhaps even slowly declining over time. Here, Figures 7c-f show that the ratio of comments to submissions increases over time. Combined with the observation that overall activity stays steady, this suggests that the ratio is changing because people substitute making their own submissions for commenting on others’ posts.\n\nFinally, this increase is most pronounced in the earlier cohorts of 2008 and 2009, with ratios more than doubling over their first year, much more than for later cohorts.\n\n[Section: 7 Discussion and Future Work]",
      "referring_paragraphs": [
        "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average."
      ],
      "figure_type": "other",
      "sub_figures": [
        "(d) 2009 cohort",
        "(e) 2010 cohort",
        "1603.07025_page0_fig19.jpg",
        "(f) 2011 cohort",
        "1603.07025_page0_fig20.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig20.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1607.06520": [
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_1",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg",
      "image_filename": "1607.06520_page0_fig0.jpg",
      "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).",
      "context_before": "This normalized similarity between vectors $u$ and $v$ is written as cos because it is the cosine of the angle between the two vectors. Since words are normalized $\\cos ( \\vec { w } _ { 1 } , \\vec { w } _ { 2 } ) = \\vec { w } _ { 1 } \\cdot \\vec { w } _ { 2 }$ .\n\nEmbedding. Unless otherwise stated, the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a $d = 3 0 0$ -dimensional word2vec [24, 25] embedding, which has proven to be immensely useful since it is high quality, publicly available, and easy to incorporate into any application. In particular, we downloaded the pre-trained embedding on the Google News corpus,4 and normalized each word to unit length as is common. Starting with the 50,000 most frequent words, we selected only lower-case words and phrases consisting of fewer than 20 lower-case characters (words with upper-case letters, digits, or punctuation were discarded). After this filtering, 26,377 words remained. While we focus on w2vNEWS, we show later that gender stereotypes are also present in other embedding data-sets.\n\nCrowd experiments. All human experiments were performed on the Amazon Mechanical Turk crowdsourcing platform. We selected for U.S.-based workers to maintain homogeneity and reproducibility to the extent possible with crowdsourcing. Two types of experiments were performed: ones where we solicited words from the crowd (to see if the embedding biases contain those of the crowd) and ones where we solicited ratings on words or analogies generated from our embedding (to see if the crowd’s biases contain those from the embedding). These two types of experiments are analogous to experiments performed in rating results in information retrieval to evaluate precision and recall. When we speak of the majority of 10 crowd judgments, we mean those annotations made by 5 or more independent workers.\n\nSince gender associations vary by culture and person, we ask for ratings of stereotypes rather than bias. In addition to possessing greater consistency than biases, people may feel more comfortable rating the stereotypes of their culture than discussing their own gender biases. The Appendix contains the questionnaires that were given to the crowd-workers to perform these tasks.\n\n[Section: 4 Gender stereotypes in word embeddings]\n\nOur first task is to understand the biases present in the word-embedding (i.e. which words are closer to she than to he, etc.) and the extent to which these geometric biases agree with human notion of gender stereotypes. We use two simple methods to approach this problem: 1) evaluate whether the embedding has",
      "context_after": "where $\\delta$ is a threshold for similarity. The intuition of the scoring metric is that we want a good analogy pair to be close to parallel to the seed direction while the two words are not too far apart in order to be semantically coherent. The parameter $\\delta$ sets the threshold for semantic similarity. In all the experiments, we take $\\delta = 1$ as we find that this choice often works well in practice. Since all embeddings are normalized, this threshold corresponds to an angle $\\leq \\pi / 3$ , indicating that the two words are closer to each other than they are to the origin. In practice, it means that the two words forming the analogy are significantly closer together than two random embedding vectors. Given the embedding and seed words, we output the top analogous pairs with the largest positive $S _ { ( a , b ) }$ scores. To reduce redundancy, we do not output multiple analogies sharing the same word $x$ .\n\nSince analogies, stereotypes, and biases are heavily influenced by culture, we employed U.S. based crowdworkers to evaluate the analogies output by the analogy generating algorithm described above. For each analogy, we asked the workers two yes/no questions: (a) whether the pairing makes sense as an analogy, and (b) whether it reflects a gender stereotype. Every analogy is judged by 10 workers, and we used the number of workers that rated this pair as stereotyped to quantify the degree of bias of this analogy. Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8). Examples of analogies generated from w2vNEWS that were rated as stereotypical are shown at the top of Figure 2, and examples of analogies that make sense and are rated as gender-appropriate are shown at the bottom of Figure 2. The full list of analogies and crowd ratings are in Appendix G.\n\nIndirect gender bias. The direct bias analyzed above manifests in the relative similarities between genderspecific words and gender neutral words. Gender bias could also affect the relative geometry between gender neutral words themselves. To test this indirect gender bias, we take pairs of words that are gender-neutral, for example softball and football. We project all the occupation words onto the  softball − football direction and looked at the extremes words, which are listed in Figure 3. For instance, the fact that the words bookkeeper and receptionist are much closer to softball than football may result indirectly from female associations with bookkeeper, receptionist and softball. It’s important to point out that that many pairs of male-biased (or female-biased) words have legitimate associations having nothing to do with gender. For example, while both footballer and football have strong male biases, their similarity is justified by factors other than gender. In Section 5, we define a metric to more rigorously quantify these indirect effects of gender bias.\n\n[Section: 5 Geometry of Gender and Bias]\n\nIn this section, we study the bias present in the embedding geometrically, identifying the gender direction and quantifying the bias independent of the extent to which it is aligned with the crowd bias. We develop metrics of direct and indirect bias that more rigorously quantify the observations of the previous section.",
      "referring_paragraphs": [
        "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_2",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg",
      "image_filename": "1607.06520_page0_fig1.jpg",
      "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).",
      "context_before": "However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences. Figure 5 illustrates ten possible gender pairs, $\\left\\{ \\left( x _ { i } , y _ { i } \\right) \\right\\} _ { i = 1 } ^ { 1 0 }$\n\nWe experimentally verified that the pairs of vectors corresponding to these words do agree with the crowd concept of gender. On Amazon Mechanical Turk, we asked crowdworkers to generate two lists of words: one list corresponding to words that they think are gendered by definition (waitress, menswear) and a separate list corresponding to words that they believe captures gender stereotypes (e.g., sewing, football). From this we generated the most frequently suggested 50 male and 50 female words for each list to be used for a classification task. For each candidate pair, for example she, he, we say that it accurately classifies a crowd suggested female definition (or stereotype) word if that word vector is closer to she than to he. Table 5 reports the classification accuracy for definition and stereotype words for each gender pair. The accuracies are high, indicating that these pairs capture the intuitive notion of gender.\n\nTo identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest. Note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues. However, as also illustrated in 6, the decrease one observes due to random sampling is much more gradual and uniform. Therefore we hypothesize that the top PC, denoted by the unit vector $g$ , captures the gender subspace. In general, the gender subspace could be higher dimensional and all of our analysis and algorithms (described below) work with general subspaces.\n\n[Section: 5.2 Direct bias]\n\nTo measure direct bias, we first identify words that should be gender-neutral for the application in question. How to generate this set of gender-neutral words is described in Section 7. Given the gender neutral words, denoted by $N$ , and the gender direction learned from above, $g$ , we define the direct gender bias of an",
      "context_after": "where $c$ is a parameter that determines how strict do we want to in measuring bias. If $c$ is $0$ , then $| \\mathrm { c o s } ( \\vec { w } - g ) | ^ { c } = 0$ only if $\\vec { w }$ has no overlap with $g$ and otherwise it is 1. Such strict measurement of bias might be desirable in settings such as the college admissions example from the Introduction, where it would be unacceptable for the embedding to introduce a slight preference for one candidate over another by gender. A more gradual bias would be setting $c = 1$ . The presentation we have chosen favors simplicity – it would be natural to extend our definitions to weight words by frequency. For example, in w2vNEWS, if we take $N$ to be the set of 327 occupations, then DirectBias1 = 0.08, which confirms that many occupation words have substantial component along the gender direction.\n\n[Section: 5.3 Indirect bias]\n\nUnfortunately, the above definitions still do not capture indirect bias. To see this, imagine completely removing from the embedding both words in gender pairs (as well as words such as beard or uterus that are arguably gender-specific but which cannot be paired). There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3). As discussed in the Introduction, it can be subtle to obtain the ground truth of the extent to which such similarities is due to gender.\n\nThe gender subspace $g$ that we have identified allows us to quantify the contribution of $g$ to the similarities between any pair of words. We can decompose a given word vector $w \\in \\mathbb { R } ^ { d }$ as $w = w _ { g } + w _ { \\perp }$ , where $w _ { g } = ( w \\cdot g ) g$ is the contribution from gender and $w _ { \\perp } = w - w _ { g }$ . Note that all the word vectors are normalized to have unit length. We define the gender component to the similarity between two word vectors $w$ and $v$ as",
      "referring_paragraphs": [
        "As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors.",
        "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).\n\nembedding to be"
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1607.06520_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_4",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg",
      "image_filename": "1607.06520_page0_fig3.jpg",
      "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.",
      "context_before": "[Section: 5.3 Indirect bias]\n\nUnfortunately, the above definitions still do not capture indirect bias. To see this, imagine completely removing from the embedding both words in gender pairs (as well as words such as beard or uterus that are arguably gender-specific but which cannot be paired). There would still be indirect gender association in that a word that should be gender neutral, such as receptionist, is closer to softball than football (see Figure 3). As discussed in the Introduction, it can be subtle to obtain the ground truth of the extent to which such similarities is due to gender.\n\nThe gender subspace $g$ that we have identified allows us to quantify the contribution of $g$ to the similarities between any pair of words. We can decompose a given word vector $w \\in \\mathbb { R } ^ { d }$ as $w = w _ { g } + w _ { \\perp }$ , where $w _ { g } = ( w \\cdot g ) g$ is the contribution from gender and $w _ { \\perp } = w - w _ { g }$ . Note that all the word vectors are normalized to have unit length. We define the gender component to the similarity between two word vectors $w$ and $v$ as\n\nThe intuition behind this metric is as follow: $\\frac { \\textbf {  { w } } _ { \\perp } \\cdot \\boldsymbol { v } _ { \\perp } } { \\| \\boldsymbol { w } _ { \\perp } \\| _ { 2 } \\| \\boldsymbol { v } _ { \\perp } \\| _ { 2 } }$ is the inner product between the two vectors if we project out the gender subspace and renormalize the vectors to be of unit length. The metric quantifies how much this inner product changes (as a fraction of the original inner product value) due to this operation of removing the gender subspace. Because of noise in the data, every vector has some non-zero component $w _ { \\perp }$ and $\\beta$ is well-defined. Note that $\\beta ( w , w ) = 0$ , which is reasonable since the similarity of a word to itself should not depend on gender contribution. If $w _ { g } = 0 = v _ { g }$ , then $\\beta ( w , v ) = 0$ ; and if $w _ { \\perp } = 0 = v _ { \\perp }$ , then $\\beta ( w , v ) = 1$ .\n\nIn Figure 3, as a case study, we examine the most extreme words on the  softball $-$ football direction. The five most extreme words (i.e. words with the highest positive or the lowest negative projections onto",
      "context_after": "[Section: 6 Debiasing algorithms]\n\nThe debiasing algorithms are defined in terms of sets of words rather than just pairs, for generality, so that we can consider other biases such as racial or religious biases. We also assume that we have a set of words to neutralize, which can come from a list or from the embedding as described in Section 7. (In many cases it may be easier to list the gender specific words not to neutralize as this set can be much smaller.)\n\nThe first step, called Identify gender subspace, is to identify a direction (or, more generally, a subspace) of the embedding that captures the bias. For the second step, we define two options: Neutralize and Equalize or Soften. Neutralize ensures that gender neutral words are zero in the gender subspace. Equalize perfectly equalizes sets of words outside the subspace and thereby enforces the property that any neutral word is equidistant to all words in each equality set. For instance, if {grandmother, grandfather} and $\\{ { \\mathrm { g u y } } , { \\mathrm { g a l } } \\}$ were two equality sets, then after equalization babysit would be equidistant to grandmother and grandfather and also equidistant to gal and guy, but presumably closer to the grandparents and further from the gal and guy. This is suitable for applications where one does not want any such pair to display any bias with respect to neutral words.\n\nThe disadvantage of Equalize is that it removes certain distinctions that are valuable in certain applications. For instance, one may wish a language model to assign a higher probability to the phrase to grandfather a regulation) than to grandmother a regulation since grandfather has a meaning that grandmother does not – equalizing the two removes this distinction. The Soften algorithm reduces the differences between these sets\n\nwhile maintaining as much similarity to the original embedding as possible, with a parameter that controls this trade-off.\n\nTo define the algorithms, it will be convenient to introduce some further notation. A subspace $B$ is defined by $k$ orthogonal unit vectors $B = \\{ b _ { 1 } , \\ldots , b _ { k } \\} \\subset \\mathbb { R } ^ { d }$ . In the case $k = 1$ , the subspace is simply a direction. We denote the projection of a vector $v$ onto $B$ by,\n\nThis also means that $v - v _ { B }$ is the projection onto the orthogonal subspace.\n\nStep 1: Identify gender subspace. Inputs: word sets $W$ , defining sets $D _ { 1 } , D _ { 2 } , \\dotsc , D _ { n } \\subset W$ as well as embedding $\\{ \\vec { w } \\in \\mathbb { R } ^ { d } \\} _ { w \\in W }$ and integer parameter $k \\geq 1$ . Let",
      "referring_paragraphs": [
        "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line.",
        "Figure 7 illustrates the results of the classifier for separating gender-specific words from gender-neutral words."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1607.06520",
      "figure_id": "1607.06520_fig_5",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg",
      "image_filename": "1607.06520_page0_fig4.jpg",
      "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.",
      "context_before": "We evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases.\n\n<table><tr><td></td><td>RG</td><td>WS</td><td>analogy</td></tr><tr><td>Before</td><td>62.3</td><td>54.5</td><td>57.0</td></tr><tr><td>Hard-debiased</td><td>62.4</td><td>54.1</td><td>57.0</td></tr><tr><td>Soft-debiased</td><td>62.4</td><td>54.2</td><td>56.8</td></tr></table>\n\nTable 1: The columns show the performance of the original w2vNEWS embedding (“before”) and the debiased w2vNEWS on the standard evaluation metrics measuring coherence and analogy-solving abilities: RG [32], WS [12], MSR-analogy [26]. Higher is better. The results show that the performance does not degrade after debiasing. Note that we use a subset of vocabulary in the experiments. Therefore, the performances are lower than the previously published results.\n\nDirect Bias. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-workers to evaluate whether these pairs reflect gender stereotypes. Figure 8 shows the results. On the initial w2vNEWS embedding, 19% of the top 150 analogies were judged as showing gender stereotypes by a majority of the ten workers. After applying our hard debiasing algorithm, only 6% of the new embedding were judged as stereotypical. As an example, consider the analogy puzzle, he to doctor is as she to $X$ . The original embedding returns $X = n u r s e$ while the hard-debiased embedding finds X = physician. Moreover the hard-debiasing algorithm preserved gender appropriate analogies such as she to ovarian cancer is as he to prostate cancer. This demonstrates that the hard-debiasing has effectively reduced the gender stereotypes in the word embedding. Figure 8 also shows that the number of appropriate analogies remains similar as in the original embedding after executing hard-debiasing. This demonstrates that that the quality of the embeddings is preserved. The details results are in Appendix G. Soft-debiasing was less effective in removing gender bias.\n\nTo further confirms the quality of embeddings after debiasing, we tested the debiased embedding on several standard benchmarks that measure whether related words have similar embeddings as well as how well the embedding performs in analogy tasks. Table 1 shows the results on the original and the new embeddings and the transformation does not negatively impact the performance.\n\nIndirect bias. We also investigated how the strict debiasing algorithm affects indirect gender bias. Because we do not have the ground truth on the indirect effects of gender bias, it is challenging to quantify the performance of the algorithm in this regard. However we do see promising qualitative improvements, as shown in Figure 3 in the softball, football example. After applying the strict debias algorithm, we repeated the experiment and show the most extreme words in the $\\overrightarrow { \\mathrm { s o f t b a l l } } - \\overrightarrow { \\mathrm { f o o t b a l l } }$ direction. The most extreme words closer to softball are now infielder and major leaguer in addition to pitcher, which are more relevant and do not exhibit gender bias. Gender stereotypic associations such are receptionist, waitress and homemaker are moved down the list. Similarly, words that clearly show male bias, e.g. businessman, are also no longer at the top of the list. Note that the two most extreme words in the  softball − football direction are pitcher and footballer. The similarities between pitcher and softball and between footballer and football comes from the actual functions of these words and hence have little gender contribution. These two words are essentially unchanged by the debiasing algorithm.\n\n[Section: 9 Discussion]\n\nWord embeddings help us further our understanding of bias in language. We find a single direction that largely captures gender, that helps us capture associations between gender neutral words and gender as well as indirect inequality.The projection of gender neutral words on this direction enables us to quantify their degree of female- or male-bias.\n\nTo reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing",
      "context_after": "[Section: References]\n\n[1] J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks., 2016.   \n[2] C. Aschwanden. The finkbeiner test: What matters in stories about women scientists? DoubleXScience, 2013.   \n[3] S. Barocas and A. D. Selbst. Big data’s disparate impact. Available at SSRN 2477899, 2014.   \n[4] E. Beigman and B. B. Klebanov. Learning with annotation noise. In ACL, 2009.   \n[5] A. J. Cuddy, E. B. Wolf, P. Glick, S. Crotty, J. Chong, and M. I. Norton. Men as cultural ideals: Cultural values moderate gender stereotype content. Journal of personality and social psychology, 109(4):622, 2015.   \n[6] A. Datta, M. C. Tschantz, and A. Datta. Automated experiments on ad privacy settings. Proceedings on Privacy Enhancing Technologies, 2015.   \n[7] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science Conference, 2012.   \n[8] J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing. Diffusion of lexical change in social media. PLoS ONE, pages 1–13, 2014.   \n[9] M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and N. A. Smith. Retrofitting word vectors to semantic lexicons. In NAACL, 2015.   \n[10] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and removing disparate impact. In KDD, 2015.   \n[11] C. Fellbaum, editor. Wordnet: An Electronic Lexical Database. The MIT Press, Cambridge, MA, 1998.   \n[12] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search in context: The concept revisited. In WWW. ACM, 2001.   \n[13] P. Glick and S. T. Fiske. The ambivalent sexism inventory: Differentiating hostile and benevolent sexism. Journal of personality and social psychology, 70(3):491, 1996.   \n[14] J. Gordon and B. Van Durme. Reporting bias and knowledge extraction. Automated Knowledge Base Construction (AKBC), 2013.   \n[15] A. G. Greenwald, D. E. McGhee, and J. L. Schwartz. Measuring individual differences in implicit cognition: the implicit association test. Journal of personality and social psychology, 74(6):1464, 1998.   \n[16] C. Hansen, M. Tosik, G. Goossen, C. Li, L. Bayeva, F. Berbain, and M. Rotaru. How to get the best word vectors for resume parsing. In SNN Adaptive Intelligence / Symposium: Machine Learning 2015, Nijmegen.   \n[17] J. Holmes and M. Meyerhoff. The handbook of language and gender, volume 25. John Wiley & Sons, 2008.   \n[18] O. İrsoy and C. Cardie. Deep recursive neural networks for compositionality in language. In NIPS. 2014.   \n[19] R. Jakobson, L. R. Waugh, and M. Monville-Burston. On language. Harvard Univ Pr, 1990.   \n[20] J. T. Jost and A. C. Kay. Exposure to benevolent sexism and complementary gender stereotypes: consequences for specific and diffuse forms of system justification. Journal of personality and social psychology, 88(3):498, 2005.\n\n[21] M. Kay, C. Matuszek, and S. A. Munson. Unequal representation and gender stereotypes in image search results for occupations. In Human Factors in Computing Systems. ACM, 2015.   \n[22] T. Lei, H. Joshi, R. Barzilay, T. Jaakkola, A. M. Katerina Tymoshenko, and L. Marquez. Semi-supervised question retrieval with gated convolutions. In NAACL. 2016.   \n[23] O. Levy and Y. Goldberg. Linguistic regularities in sparse and explicit word representations. In CoNLL, 2014.   \n[24] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013.   \n[25] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS.   \n[26] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746–751, 2013.   \n[27] E. Nalisnick, B. Mitra, N. Craswell, and R. Caruana. Improving document ranking with dual word embeddings. In www, April 2016.   \n[28] B. A. Nosek, M. Banaji, and A. G. Greenwald. Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics: Theory, Research, and Practice, 6(1):101, 2002.   \n[29] D. Pedreshi, S. Ruggieri, and F. Turini. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560–568. ACM, 2008.   \n[30] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014.   \n[31] K. Ross and C. Carter. Women and news: A long and winding road. Media, Culture & Society, 33(8):1148–1165, 2011.   \n[32] H. Rubenstein and J. B. Goodenough. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633, 1965.   \n[33] E. Sapir. Selected writings of Edward Sapir in language, culture and personality, volume 342. Univ of California Press, 1985.   \n[34] B. Schmidt. Rejecting the gender binary: a vector-space operation. http://bookworm.benschmidt. org/posts/2015-10-30-rejecting-the-gender-binary.html, 2015.   \n[35] J. P. Stanley. Paradigmatic woman: The prostitute. Papers in language variation, pages 303–321, 1977.   \n[36] L. Sweeney. Discrimination in online ad delivery. Queue, 11(3):10, 2013.   \n[37] A. Torralba and A. Efros. Unbiased look at dataset bias. In CVPR, 2012.   \n[38] P. D. Turney. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, pages 533–585, 2012.   \n[39] C. Wagner, D. Garcia, M. Jadidi, and M. Strohmaier. It’s a man’s wikipedia? assessing gender inequality in an online encyclopedia. In Ninth International AAAI Conference on Web and Social Media, 2015.   \n[40] D. Yogatama, M. Faruqui, C. Dyer, and N. A. Smith. Learning word representations with hierarchical sparse coding. In ICML, 2015.   \n[41] I. Zliobaite. A survey on measuring indirect discrimination in machine learning. arXiv preprint arXiv:1511.00148, 2015.\n\n[Section: A Generating analogies]",
      "referring_paragraphs": [
        "Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure 8).",
        "Figure 8 shows the results.",
        "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1607.06520_page0_fig5.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig5.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1608.07187": [
    {
      "doc_id": "1608.07187",
      "figure_id": "1608.07187_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg",
      "image_filename": "1608.07187_page0_fig0.jpg",
      "caption": "",
      "context_before": "In another laboratory study, Nosek et al. (2002b) found that female terms are less associated with the sciences, and male terms less associated with the arts.\n\nOriginal Finding: 83 subjects took the IAT with a combination of math/science and art/language attributes, and the expected associations were observed with an effect size of 1.47 and a $p$ -value of $1 0 ^ { - 2 4 }$ , Nosek et al. (2002b, p. 51).\n\nOur Finding: By examining only arts and sciences attributes, we found that female terms were associated more with arts and male terms with science with an effect size of 1.24 and a $p$ -value of $1 0 ^ { - 2 }$ .\n\nStimuli: We use the stimuli found in Nosek et al. (2002b, p. 59).\n\n• Science words : science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy.   \n• Arts words : poetry, art, Shakespeare, dance, literature, novel, symphony, drama.   \n• Male attributes: brother, father, uncle, grandfather, son, he, his, him.   \n• Female attributes: sister, mother, aunt, grandmother, daughter, she, hers, her.\n\n[Section: Comparison to Real-World Data: Occupational Statistics]\n\nIt has been suggested that implicit gender-occupation biases are linked to gender gaps in occupational participation (Nosek et al., 2009); however the relationship between these is complex and may be mutually reinforcing. Here we examine the correlation between the gender association of occupation words and labor-force participation data.\n\nOriginal Data: The x-axis of Figure 1 is derived from the 2015 U.S. Bureau of Labor Statistics3, which provides information about occupational categories and the percentage of women that have certain occupations under these categories. We generated single word occupation names (as explained in the Methods section) based on the available data and calculated the percentage of women for the set of single word occupation names.\n\nOur Finding: By applying WEFAT, we are able to use word embeddings to predict the percentage of women in the 50 most relevant occupations with a Pearson’s correlation coefficient of $\\rho = 0 . 9 0$ with $p$ -value $< 1 0 ^ { - 1 8 }$ .\n\nStimuli: We use the gender stimuli found in Nosek et al. (2002a, p. 114) along with the occupation attributes we derived from labor statistics.\n\n• Careers : technician, accountant, supervisor, engineer, worker, educator, clerk, counselor, inspector, mechanic, manager, therapist, administrator, salesperson, receptionist, librarian, advisor, pharmacist, janitor, psychologist, physician, carpenter, nurse, investigator, bartender, specialist, electrician, officer, pathologist, teacher, lawyer, planner, practitioner, plumber, instructor, surgeon, veterinarian, paramedic, examiner, chemist, machinist, appraiser, nutritionist, architect, hairdresser, baker, programmer, paralegal, hygienist, scientist.   \n• Female attributes: female, woman, girl, sister, she, her, hers, daughter.   \n• Male attributes: male, man, boy, brother, he, him, his, son.\n\nFigure 1. Occupation-gender association",
      "context_after": "Pearson’s correlation coefficient $\\rho = 0 . 9 0$ with $p$ -value $< 1 0 ^ { - 1 8 }$ .\n\n[Section: Comparison to Real-World Data: Androgynous Names]\n\nSimilarly, we looked at the veridical association of gender to androgynous names, that is, names sometimes used by either gender. In this case, the most recent information we were able to find was the 1990 census name and gender statistics. Perhaps because of the age of our name data, our correlation was weaker than for the 2015 occupation statistics, but still strikingly significant.\n\nFigure 2. People with androgynous names",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "1608.07187_page0_fig1.jpg"
      ],
      "quality_score": 0.5,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1608.07187",
      "figure_id": "1608.07187_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg",
      "image_filename": "1608.07187_page0_fig2.jpg",
      "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.",
      "context_before": "Pearson’s correlation coefficient $\\rho = 0 . 8 4$ with $p$ -value $< 1 0 ^ { - 1 3 }$ .\n\nOriginal Data: The x-axis of Figure 2 is derived from the 1990 U.S. census data4 that provides first name and gender information in population.\n\nOur Finding: The y-axis reflects our calculation of the bias for how male or female each of the names is. By applying WEFAT, we are able to predict the percentage of people with a name who were women with Pearson’s correlation coefficient of $\\rho = 0 . 8 4$",
      "context_after": "[Section: Methods]\n\n[Section: Data and training]\n\nA word embedding is a representation of words as points in a vector space. Loosely, embeddings satisfy the property that vectors that are close to each other represent semantically “similar” words. Word embeddings derive their power from the discovery that vector spaces with around 300 dimensions suffice to capture most aspects of similarity, enabling a computationally tractable representation of all or most words in large corpora of text (Bengio et al., 2003; Lowe, 1997). Starting in 2013, the word2vec family of word embedding techniques has gained popularity due to a new set of computational techniques for generating word embeddings from large training corpora of text, with superior speed and predictive performance in various natural-language processing tasks (Mikolov et al., 2013; Mikolov and Dean, 2013).\n\nMost famously, word embeddings excel at solving “word analogy” tasks because the algebraic relationships between vectors capture syntactic and semantic relationships between words (Figure 3). In addition, word embeddings have found use in natural-language processing tasks such as web search and document classification. They have also found use in cognitive science for understanding human memory and recall (Zaromb et al., 2006; McDonald and Lowe, 1998).\n\nFor all results in this paper we use the state-of-the-art GloVe word embedding method, in which, at a high level, the similarity between a pair of vectors is related to the probability that the words co-occur close to each other in text (Pennington et al., 2014). Word embedding algorithms such as GloVe substantially amplify the signal found in simple co-occurrence probabilities using dimensionality reduction. In pilot-work experiments along the lines of those presented here (on free associations rather than implicit associations) raw co-occurrence probabilities were shown to lead to substantially weaker results (Macfarlane, 2013).\n\nRather than train the embedding ourselves, we use pre-trained GloVe embeddings distributed by its authors. We aim to replicate the effects that may be found in real applications to the extent possible, and using pre-trained embeddings minimizes\n\nthe choices available to us and simplifies reproducing our results. We pick the largest of the four corpora for which the GloVe authors provide trained embeddings, which is a “Common Crawl” corpus obtained from a large-scale crawl of the web, containing 840 billion tokens (roughly, words). Tokens in this corpus are case-sensitive and there are 2.2 million different ones, each corresponding to a 300-dimensional vector. The large size of this corpus and the resulting model is important to us, since it enables us to find word vectors for even relatively uncommon names. An important limitation is that there are no vectors for multi-word phrases.\n\nWe can expect similar results to the ones presented here if we used other corpora and/or embedding algorithms. For example, we repeated all the WEAT and WEFAT experiments presented above using a different pre-trained embedding: word2vec on a Google News corpus (Mikolov and Dean, 2013). In all experiments, we observed statistically significant effects and high effect sizes. Further, we found that the gender association strength of occupation words is highly correlated between the GloVe embedding and the word2vec embedding (Pearson $\\rho = 0 . 8 8$ ; Spearman $\\rho = 0 . 8 6 )$ ). In concurrent work, Bolukbasi et al. (2016) compared the same two embeddings, using a different measure of the gender bias of occupation words, also finding a high correlation (Spearman $\\rho = 0 . 8 1$ ).",
      "referring_paragraphs": [
        "Figure 3.",
        "Most famously, word embeddings excel at solving “word analogy” tasks because the algebraic relationships between vectors capture syntactic and semantic relationships between words (Figure 3)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    }
  ],
  "1609.05807": [
    {
      "doc_id": "1609.05807",
      "figure_id": "1609.05807_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1609.05807/1609.05807/hybrid_auto/images/1609.05807_page0_fig0.jpg",
      "image_filename": "1609.05807_page0_fig0.jpg",
      "caption": "",
      "context_before": "[Section: Appendix: NP-Completeness of Non-Trivial Integral Fair Risk Assignments]\n\nWe can reduce to the integral assignment problem, parameterized by $a _ { 1 \\sigma } , a _ { 2 \\sigma }$ , and $p _ { \\sigma }$ , from subset sum as follows.\n\nSuppose we have an instance of the subset sum problem specified by $m$ numbers $w _ { 1 } , \\ldots , w _ { m }$ and a target $T$ ; the goal is to determine whether a subset of the $w _ { i }$ add up to $T$ . We create an instance of the integral assignment problem with $\\sigma _ { 1 } , \\ldots , \\sigma _ { 2 m + 2 }$ . $a _ { 1 , \\sigma _ { i } } = 1 / 2$ if $i \\in \\{ 2 m + 1 , 2 m + 2 \\}$ and 0 otherwise. $a _ { 2 , \\sigma _ { i } } =$ $1 / ( 2 m )$ if $i \\le 2 m$ and 0 otherwise. We make the following definitions:\n\nWith this definition, the subset sum instance has a solution if and only if the integral assignment instance given by $a _ { 1 , \\sigma } , a _ { 2 , \\sigma } , p _ { \\sigma _ { 1 } } , . . . , p _ { \\sigma _ { 2 m + 2 } }$ has a solution.\n\nBefore we prove this, we need the following lemma.\n\nLemma 5.1 For any $z _ { 1 } , \\ldots , z _ { k } \\in \\mathbb { R } ,$ ,",
      "context_after": "Now, we can prove that the integral assignment problem is NP-hard.\n\nProof: First, we observe that for any nontrivial solution to the integral assignment instance, there must be two bins $b \\ne b ^ { \\prime }$ such that $X _ { \\sigma _ { 2 m + 1 } , b } = 1$ and $X _ { \\sigma _ { 2 m + 2 } , b ^ { \\prime } } = 1$ . In other words, the people with $\\sigma _ { 2 m + 1 }$ and $\\sigma _ { 2 m + 2 }$ must be split up. If not, then all the people of group 1 would be in the same bin, meaning that bin must be labeled with the base rate $\\rho _ { 1 } = 1 / 2$ . In order to maintain fairness, the same would have to be done for all the people of group 2, resulting in the trivial solution. Moreover, $b$ and $b ^ { \\prime }$ must be labeled $( 1 \\pm \\sqrt { 2 \\gamma - 1 } ) / 2$ respectively because those are the fraction of people of group 1 in those bins who belong to the positive class.\n\nThis means that $\\gamma _ { 1 } = 1 / \\rho \\cdot ( a _ { 1 , \\sigma _ { 2 m + 1 } } p _ { \\sigma _ { 2 m + 1 } } ^ { 2 } + a _ { 1 , \\sigma _ { 2 m + 2 } } p _ { \\sigma _ { 2 m + 2 } } ^ { 2 } ) = p _ { \\sigma _ { 2 m + 1 } } ^ { 2 } + p _ { \\sigma _ { 2 m + 2 } } ^ { 2 } = \\gamma$ as defined above. We know that a well-calibrated assignment is fair only if $\\gamma _ { 1 } = \\gamma _ { 2 }$ , so we know $\\gamma _ { 2 } = \\gamma$ .\n\nNext, we observe that $\\rho _ { 2 } = \\rho _ { 1 } = 1 / 2$ because all of the positive $a _ { 2 , \\sigma }$ ’s are $1 / ( 2 m )$ , so $\\rho _ { 2 }$ is just the average of $\\{ p _ { \\sigma _ { 1 } } , \\hdots , p _ { \\sigma _ { 2 m } } \\}$ , which is $1 / 2$ by symmetry.\n\nLet $Q$ be the partition of $[ 2 m ]$ corresponding to the assignment, meaning that for a given $q \\in Q$ , there is a bin $b _ { q }$ containing all people with $\\sigma _ { i }$ such that $i \\in q$ . The label on that bin is\n\nFurthermore, bin $b _ { q }$ contains $\\textstyle \\sum _ { i \\in q } a _ { 2 , \\sigma _ { i } } p _ { \\sigma _ { i } } = 1 / ( 2 m ) \\sum _ { i \\in q } p _ { \\sigma _ { i } }$ positive fraction. Using this, we can come up with an expression for $\\gamma _ { 2 }$ .\n\nSetting this equal to $\\gamma$ , we have",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.4,
      "metadata": {}
    }
  ],
  "1610.02413": [
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg",
      "image_filename": "1610.02413_page0_fig0.jpg",
      "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).",
      "context_before": "We will first develop an intuitive geometric solution in the case where we adjust a binary predictor $\\widehat { Y }$ and $A$ is a binary protected attribute The proof generalizes directly to the case of a discrete protected attribute with more than two values. For convenience, we introduce the notation\n\nThe first component of $\\gamma _ { a } ( \\widehat { Y } )$ is the false positive rate of $\\widehat { Y }$ within the demographic satisfying $A = a$ . Similarly, the second component is the true positive rate of $\\widehat { Y }$ within $A = a$ . Observe that we can calculate $\\gamma _ { a } ( \\widehat { Y } )$ given the joint distribution of $( \\widehat { Y } , A , Y )$ . The definitions of equalized odds and equal opportunity can be expressed in terms of $\\gamma ( \\widehat { Y } ) .$ , as formalized in the following straight-forward Lemma:\n\nLemma 4.2. A predictor $\\widehat { Y }$ satisfies:\n\n1. equalized odds if and only if $\\gamma _ { 0 } ( \\widehat { Y } ) = \\gamma _ { 1 } ( \\widehat { Y } ) .$ , and   \n2. equal opportunity if and only if $\\gamma _ { 0 } ( \\widehat { Y } )$ and $\\gamma _ { 1 } ( \\widehat { Y } )$ agree in the second component, i.e., $\\gamma _ { 0 } ( \\widehat { Y } ) _ { 2 } =$ $\\dot { \\gamma _ { 1 } } ( \\widehat { Y } ) _ { 2 }$ .\n\nFor $a \\in \\{ 0 , 1 \\}$ , consider the two-dimensional convex polytope defined as the convex hull of four vertices:\n\nOur next lemma shows that $P _ { 0 } ( \\widehat { Y } )$ and $P _ { 1 } ( \\widehat { Y } )$ characterize exactly the trade-offs between false positives and true positives that we can achieve with any derived classifier. The polytopes are visualized in Figure 1.\n\nLemma 4.3. A predictor $\\widetilde { Y }$ is derived if and only if for all $a \\in \\{ 0 , 1 \\}$ , we have $\\gamma _ { a } ( \\widetilde { Y } ) \\in P _ { a } ( \\widehat { Y } )$ .\n\nProof. Since a derived predictor $\\widetilde { Y }$ can only depend on $( { \\widehat { Y } } , A )$ and these variables are binary, the predictor $\\widetilde { Y }$ is completely described by four parameters in [0,1] corresponding to the probabilities $\\operatorname* { P r } \\left\\{ { \\widetilde { Y } } = 1 \\mid { \\widehat { \\overline { { Y } } } } = { \\widehat { y } } , { \\dot { A } } = a \\right\\}$ for $\\widehat { y , a } \\in \\{ 0 , 1 \\}$ . Each of these parameter choices leads to one of the points in $P _ { a } ( \\widehat { Y } )$ and every point in the convex hull can be achieved by some parameter setting. □",
      "context_after": "Figure 1 gives a simple geometric picture for the solution of the linear program whose guarantees are summarized next.\n\nFigure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).\n\nCombining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds:\n\nProposition 4.4. The optimization problem (4.3) is a linear program in four variables whose coefficients can be computed from the joint distribution of $( \\widehat { Y } , A , \\grave { Y } )$ . Moreover, its solution is an optimal equalized odds predictor derived from $\\widehat { Y }$ and A.\n\nProof of Proposition 4.4. The second claim follows by combining Lemma 4.2 with Lemma 4.3. To argue the first claim, we saw in the proof of Lemma 4.3 that a derived predictor is specified by four parameters and the constraint region is an intersection of two-dimensional linear constraints. It remains to show that the objective function is a linear function in these parameters. Writing out the objective, we have\n\nAll probabilities in the last line that do not involve $\\widetilde { Y }$ can be computed from the joint distribution. The probabilities that do involve $\\widetilde { Y }$ are each a linear function of the parameters that specify $\\widetilde { Y }$ .\n\n□\n\nThe corresponding optimization problem for equation opportunity is the same except that it has a weaker constraint $\\gamma _ { 0 } ( \\widetilde { Y } ) _ { 2 } = \\dot { \\gamma } _ { 1 } ( \\widetilde { Y } ) _ { 2 }$ . The proof is analogous to that of Proposition 4.4. Figure 1 explains the solution geometrically.",
      "referring_paragraphs": [
        "The polytopes are visualized in Figure 1.",
        "Figure 1 gives a simple geometric picture for the solution of the linear program whose guarantees are summarized next.\n\nFigure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).\n\nCombining Lemma 4.2 with Lemma 4.3, we see that the following optimization problem gives the optimal derived predictor with equalized odds:",
        "Figure 1 explains the solution geometrically."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.02413_page0_fig1.jpg",
        "1610.02413_page0_fig2.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
      "image_filename": "1610.02413_page0_fig3.jpg",
      "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.",
      "context_before": "[Section: 4.2 Deriving from a score function]\n\nWe now consider deriving non-discriminating predictors from a real valued score $R \\in [ 0 , 1 ]$ . The motivation is that in many realistic scenarios (such as FICO scores), the data are summarized by a one-dimensional score function and a decision is made based on the score, typically by thresholding it. Since a continuous statistic can carry more information than a binary outcome Y , we can hope to achieve higher utility when working with $R$ directly, rather then with a binary predictor $\\widehat { \\boldsymbol { Y } }$ .\n\nA “protected attribute blind” way of deriving a binary predictor from $R$ would be to threshold it, i.e. using $\\widehat { Y } = \\mathbb { I } \\{ R > t \\}$ . If $R$ satisfied equalized odds, then so will such a predictor, and the optimal threshold should be chosen to balance false positive and false negatives so as to minimize the expected loss. When R does not already satisfy equalized odds, we might need to use different thresholds for different values of $A$ (different protected groups), i.e. $\\widetilde { Y } = \\mathbb { I } \\left\\{ R > t _ { A } \\right\\}$ . As we will see, even this might not be sufficient, and we might need to introduce additional randomness as in the preceding section.\n\nCentral to our study is the ROC (Receiver Operator Characteristic) curve of the score, which captures the false positive and true positive (equivalently, false negative) rates at different thresholds. These are curves in a two dimensional plane, where the horizontal axes is the false positive rate of a predictor and the vertical axes is the true positive rate. As discussed in the previous section, equalized odds can be stated as requiring the true positive and false positive rates, $( \\operatorname* { P r } { \\Bigl \\{ } { \\widehat { Y } } = 1 \\mid { \\dot { Y } } = 0 , A = a { \\Bigr \\} } , \\operatorname* { P r } { \\Bigl \\{ } { \\widehat { Y } } = 1 \\mid Y = 1 , A = a { \\Bigr \\} } )$ , agree between different values of a of the protected attribute. That is, that for all values of the protected attribute, the conditional behavior of the predictor is at exactly the same point in this space. We will therefor consider the $A$ -conditional ROC curves\n\nSince the ROC curves exactly specify the conditional distributions $R | A , Y$ , a score function obeys equalized odds if and only if the ROC curves for all values of the protected attribute agree, that is $C _ { a } ( t ) = C _ { a ^ { \\prime } } ( t )$ for all values of a and $t$ . In this case, any thresholding of $R$ yields an equalized odds predictor (all protected groups are at the same point on the curve, and the same point in false/true-positive plane).\n\nWhen the ROC curves do not agree, we might choose different thresholds $t _ { a }$ for the different protected groups. This yields different points on each $A$ -conditional ROC curve. For the resulting predictor to satisfy equalized odds, these must be at the same point in the false/truepositive plane. This is possible only at points where all $A$ -conditional ROC curves intersect. But the ROC curves might not all intersect except at the trivial endpoints, and even if they do, their point of intersection might represent a poor tradeoff between false positive and false negatives.\n\nAs with the case of correcting a binary predictor, we can use randomization to fill the span of possible derived predictors and allow for significant intersection in the false/true-positive plane. In particular, for every protected group $a$ , consider the convex hull of the image of the conditional ROC curve:",
      "context_after": "where $T _ { a }$ is a randomized threshold assuming the value ${ \\underline { { t } } } _ { a }$ with probability ${ \\underline { { p } } } _ { a }$ and the value $\\overline { { t } } _ { a }$ with probability $\\overline { { p } } _ { a }$ . In other words, to construct an equalized odds predictor, we should choose a point in the intersection of these convex hulls, $\\gamma = ( \\gamma _ { 0 } , \\gamma _ { 1 } ) \\in \\cap _ { a } D _ { a } ,$ , and then for each protected group realize the true/false-positive rates $\\gamma$ with a (possible randomized) predictor $\\mathcal { \\overline { { Y } } } | ( A = a ) = \\mathbb { I } \\{ R > T _ { a } \\}$ resulting in the predictor $\\widetilde { Y } = \\operatorname* { P r } \\mathbb { I } \\{ R > T _ { A } \\}$ . For each group $a$ , we either use a fixed threshold $T _ { a } = t _ { a }$ or a mixture of two thresholds ${ \\underline { { t } } } _ { a } < { \\overline { { t } } } _ { a }$ . In the latter case, if $A = a$ and $R < \\underline { { t } } _ { a }$ we always set $\\widetilde { Y } = 0$ , if $R > \\overline { { t } } _ { a }$ we always set $\\widetilde { Y } = 1$ , but if $\\underline { { t } } _ { a } < R < \\overline { { t } } _ { a . }$ , we flip a coin and set Ye = 1 with probability p . $\\widetilde { Y } = \\overset { \\cdots } { 1 }$ ${ \\underline { { p } } } _ { a }$\n\nThe feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal (see Figure 2). Since for any loss function the optimal false/true-positive rate will always be on the upper-left boundary of this feasible set, this is effectively the ROC curve of the equalized odds predictors. This ROC curve is the pointwise minimum of all $A$ -conditional ROC curves. The performance of an equalized odds predictor is thus determined by the minimum performance among all protected groups. Said differently, requiring equalized odds incentivizes the learner to build good predictors for all classes. For a given loss function, finding the optimal tradeoff\n\namounts to optimizing (assuming w.l.o.g. $\\ell ( 0 , 0 ) = \\ell ( 1 , 1 ) = 0$ ):\n\nThis is no longer a linear program, since $D _ { a }$ are not polytopes, or at least are not specified as such. Nevertheless, (4.5) can be efficiently optimized numerically using ternary search.\n\nDeriving an optimal equal opportunity threshold predictor. The construction follows the same approach except that there is one fewer constraint. We only need to find points on the conditional ROC curves that have the same true positive rates in both groups. Assuming continuity of the conditional ROC curves, this means we can always find points on the boundary of the conditional ROC curves. In this case, no randomization is necessary. The optimal solution corresponds to two deterministic thresholds, one for each group. As before, the optimization problem can be solved efficiently using ternary search over the target true positive value. Here we use, as Figure 2 illustrates, that the cost of the best solution is convex as a function of its true positive rate.\n\n[Section: 5 Bayes optimal predictors]",
      "referring_paragraphs": [
        "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right).",
        "$\\widetilde { Y } = \\overset { \\cdots } { 1 }$ ${ \\underline { { p } } } _ { a }$\n\nThe feasible set of false/true positive rates of possible equalized odds predictors is thus the intersection of the areas under the $A$ -conditional ROC curves, and above the main diagonal (see Figure 2).",
        "Here we use, as Figure 2 illustrates, that the cost of the best solution is convex as a function of its true positive rate."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.02413_page0_fig4.jpg",
        "1610.02413_page0_fig5.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig5.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg",
      "image_filename": "1610.02413_page0_fig6.jpg",
      "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.",
      "context_before": "This is no longer a linear program, since $D _ { a }$ are not polytopes, or at least are not specified as such. Nevertheless, (4.5) can be efficiently optimized numerically using ternary search.\n\nDeriving an optimal equal opportunity threshold predictor. The construction follows the same approach except that there is one fewer constraint. We only need to find points on the conditional ROC curves that have the same true positive rates in both groups. Assuming continuity of the conditional ROC curves, this means we can always find points on the boundary of the conditional ROC curves. In this case, no randomization is necessary. The optimal solution corresponds to two deterministic thresholds, one for each group. As before, the optimization problem can be solved efficiently using ternary search over the target true positive value. Here we use, as Figure 2 illustrates, that the cost of the best solution is convex as a function of its true positive rate.\n\n[Section: 5 Bayes optimal predictors]\n\nIn this section, we develop the theory a theory for non-discriminating Bayes optimal classification. We will first show that a Bayes optimal equalized odds predictor can be obtained as an derived threshold predictor of the Bayes optimal regressor. Second, we quantify the loss of deriving an equalized odds predictor based on a regressor that deviates from the Bayes optimal regressor. This can be used to justify the approach of first training classifiers without any fairness constraint, and then deriving an equalized odds predictor in a second step.\n\nDefinition 5.1 (Bayes optimal regressor). Given random variables $( X , A )$ and a target variable Y , the Bayes optimal regressor is $\\begin{array} { r } { R = \\arg \\operatorname* { m i n } _ { r ( x , a ) } \\mathbb { E } \\left[ ( Y - r ( X , A ) ) ^ { 2 } \\right] = r ^ { * } ( X , A ) } \\end{array}$ with $r ^ { * } ( x , a ) = \\mathbb { E } [ Y \\mid X =$ $x , A = a ]$ .\n\nThe Bayes optimal classifier, for any proper loss, is then a threshold predictor of $R$ , where the threshold depends on the loss function (see, e.g., [Was10]). We will extend this result to the case where we additionally ask the classifier to satisfy an oblivious property, such as our non-discrimination properties.\n\nProposition 5.2. For any source distribution over $( Y , X , A )$ with Bayes optimal regressor $R ( X , A )$ , any loss function, and any oblivious property C, there exists a predictor $Y ^ { * } ( R , A )$ such that:\n\n1. $Y ^ { \\ast }$ is an optimal predictor satisfying C. That is, $\\mathbb { E } \\ell ( Y ^ { * } , Y ) \\leqslant \\mathbb { E } \\ell ( { \\widehat { Y } } , Y )$ for any predictor ${ \\widehat { Y } } ( X , A )$ which satisfies C.   \n2. $Y ^ { \\ast }$ is derived from $( R , A )$ .\n\nProof. Consider an arbitrary classifier $\\widehat { Y }$ on the attributes $( X , A )$ , defined by a (possibly randomized) function ${ \\widehat { Y } } = f ( X , A )$ . Given $R = r , A = a$ ), we can draw a fresh $X ^ { \\prime }$ from the distribution $( X \\mid R = r , A = a )$ , and set $Y ^ { * } = f ( X ^ { \\prime } , a )$ . This satisfies (2). Moreover, since Y is binary with expectation R, Y is independent of $X$ conditioned on $( R , A )$ . Hence $( Y , X , R , A )$ and $( Y , X ^ { \\prime } , R , A )$ have identical distributions, so $( Y ^ { * } , A , Y )$ and $( \\widehat { Y } , A , Y )$ also have identical distributions. This implies $Y ^ { \\ast }$ satisfies (1) as desired. □",
      "context_after": "[Section: 5.1 Near optimality]\n\nWe can furthermore show that if we can approximate the (unconstrained) Bayes optimal regressor well enough, then we can also construct a nearly optimal non-discriminating classifier.\n\nTo state the result, we introduce the following distance measure on random variables.\n\nDefinition 5.4. We define the conditional Kolmogorov distance between two random variables $R , R ^ { \\prime } \\in [ 0 , 1 ]$ in the same probability space as $A$ and Y as:\n\nWithout the conditioning on $A$ and Y , this definition coincides with the standard Kolmogorov distance. Closeness in Kolmogorov distance is a rather weak requirement. We need the slightly stronger condition that the Kolmogorov distance is small for each of the four conditionings on A and Y . This captures the distance between the restricted ROC curves, as formalized next.\n\nLemma 5.5. Let $R , R ^ { \\prime } \\in [ 0 , 1 ]$ be random variables in the same probability space as A and Y . Then, for any point $p$ on a restricted ROC curve of √ $R _ { * }$ , there is a point q on the corresponding restricted ROC curve of $R ^ { \\prime }$ such that $\\| p - q \\| _ { 2 } \\leqslant \\sqrt { 2 } \\cdot d _ { K } ( \\dot { R , R ^ { \\prime } } )$ .\n\nProof. Assume the point $p$ is achieved by thresholding $R$ at $t \\in [ 0 , 1 ]$ . Let $q$ be the point on the ROC curve achieved by thresholding $R ^ { \\prime }$ at the same threshold $t ^ { \\prime }$ . After applying the definition to bound the distance in each coordinate, the claim follows from Pythagoras’ theorem. □\n\nWe can now show that an equalized odds predictor derived from a nearly optimal regressor is still nearly optimal among all equal odds predictors, while quantifying the loss in terms of the conditional Kolmogorov distance.\n\nTheorem 5.6 (Near optimality). Assume that ` is a bounded loss function, and let $\\widehat { R } \\in [ 0 , 1 ]$ be an arbitrary random variable. Then, there is an optimal equalized odds predictor $Y ^ { \\ast }$ and an equalized odds predictor $\\widehat { Y }$ derived from $( \\widehat { R } , A )$ such that",
      "referring_paragraphs": [
        "Figure 3: Graphical model for the proof of Proposition 5.2.\n\nCorollary 5.3 (Optimality characterization). An optimal equalized odds predictor can be derived from the Bayes optimal regressor R and the protected attribute A. The same is true for an optimal equal opportunity predictor."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_8",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig7.jpg",
      "image_filename": "1610.02413_page0_fig7.jpg",
      "caption": "Figure 4: Graphical model for Scenario I.",
      "context_before": "This completes the proof.\n\n[Section: 6 Oblivious identifiability of discrimination]\n\nBefore turning to analyzing data, we pause to consider to what extent “black box” oblivious tests like ours can identify discriminatory predictions. To shed light on this issue, we introduce two possible scenarios for the dependency structure of the score, the target and the protected attribute. We will argue that while these two scenarios can have fundamentally different interpretations from the point of view of fairness, they can be indistinguishable from their joint distribution. In particular, no oblivious test can resolve which of the two scenarios applies.\n\nScenario I Consider the dependency structure depicted in Figure 4. Here, $X _ { 1 }$ is a feature highly (even deterministically) correlated with the protected attribute $A$ , but independent of the target Y given A. For example, $X _ { 1 }$ might be “languages spoken at home” or “great great grandfather’s profession”. The target Y has a statistical correlation with the protected attribute. There’s a second real-valued feature $X _ { 2 }$ correlated with Y , but only related to A through Y . For example, $X _ { 2 }$",
      "context_after": "",
      "referring_paragraphs": [
        "Scenario I Consider the dependency structure depicted in Figure 4.",
        "Figure 4: Graphical model for Scenario I."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {}
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_9",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg",
      "image_filename": "1610.02413_page0_fig8.jpg",
      "caption": "Figure 5: Graphical model for Scenario II.",
      "context_before": "",
      "context_after": "[Section: 6.1 Unidentifiability]\n\nThe above two scenarios seem rather different. The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protected attribute and the score would give different indications for the optimal score $R ^ { * }$ in the two scenarios. If it were judged unfair in one scenario, it would also be judged unfair in the other.\n\nWe will show this by constructing specific instantiations of the two scenarios where the joint distributions over $( Y , \\overbrace { A } , R ^ { * } , \\widetilde { R } )$ are identical. The scenarios are thus unidentifiable based only on these joint distributions.\n\nWe will consider binary targets and protected attributes taking values in $A , Y \\in \\{ - 1 , 1 \\}$ and real valued features. We deviate from our convention of {0,1}-values only to simplify the resulting expressions. In Scenario I, let:\n\n• $\\operatorname* { P r } \\left\\{ A = 1 \\right\\} = 1 / 2$ , and $X _ { 1 } = A$   \n• Y follows a logistic model parametrized based on A: Pr {Y = y | A = a} = 11+exp(−2ay) , $\\begin{array} { r } { A \\colon \\operatorname* { P r } \\left\\{ Y = y \\mid A = a \\right\\} = \\frac { 1 } { 1 + \\exp ( - 2 a y ) } , } \\end{array}$   \n• $X _ { 2 }$ is Gaussian with mean $Y \\colon X _ { 2 } = Y + { \\mathcal { N } } ( 0 , 1 )$   \n• Optimal unconstrained and equalized odds scores are given by: $R ^ { * } = X _ { 1 } + X _ { 2 } = A + X _ { 2 } ,$ and $\\widetilde { R } = X _ { 2 }$",
      "referring_paragraphs": [
        "Scenario II Now consider the dependency structure depicted in Figure 5.",
        "Figure 5: Graphical model for Scenario II."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {}
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_10",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg",
      "image_filename": "1610.02413_page0_fig9.jpg",
      "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.",
      "context_before": "[Section: 6.1 Unidentifiability]\n\nThe above two scenarios seem rather different. The optimal score $R ^ { * }$ is in one case based directly on $A$ or its surrogate, and in another only on a directly predictive feature, but this is not apparent by considering the equalized odds criterion, suggesting a possible shortcoming of equalized odds. In fact, as we will now see, the two scenarios are indistinguishable using any oblivious test. That is, no test based only on the target labels, the protected attribute and the score would give different indications for the optimal score $R ^ { * }$ in the two scenarios. If it were judged unfair in one scenario, it would also be judged unfair in the other.\n\nWe will show this by constructing specific instantiations of the two scenarios where the joint distributions over $( Y , \\overbrace { A } , R ^ { * } , \\widetilde { R } )$ are identical. The scenarios are thus unidentifiable based only on these joint distributions.\n\nWe will consider binary targets and protected attributes taking values in $A , Y \\in \\{ - 1 , 1 \\}$ and real valued features. We deviate from our convention of {0,1}-values only to simplify the resulting expressions. In Scenario I, let:\n\n• $\\operatorname* { P r } \\left\\{ A = 1 \\right\\} = 1 / 2$ , and $X _ { 1 } = A$   \n• Y follows a logistic model parametrized based on A: Pr {Y = y | A = a} = 11+exp(−2ay) , $\\begin{array} { r } { A \\colon \\operatorname* { P r } \\left\\{ Y = y \\mid A = a \\right\\} = \\frac { 1 } { 1 + \\exp ( - 2 a y ) } , } \\end{array}$   \n• $X _ { 2 }$ is Gaussian with mean $Y \\colon X _ { 2 } = Y + { \\mathcal { N } } ( 0 , 1 )$   \n• Optimal unconstrained and equalized odds scores are given by: $R ^ { * } = X _ { 1 } + X _ { 2 } = A + X _ { 2 } ,$ and $\\widetilde { R } = X _ { 2 }$",
      "context_after": "[Section: 6.2 Comparison of different oblivious measures]\n\nIt is interesting to consider how different oblivious measures apply to the scores $\\widetilde { R }$ and $R ^ { * }$ in these two scenarios.\n\nAs discussed in Section 4.2, a score satisfies equalized odds iff the conditional ROC curves agree for both values of $A$ , which we refer to as having identical ROC curves.\n\nDefinition 6.2 (Identical ROC Curves). We say that a score $R$ has identical conditional ROC curves if $C _ { a } ( t ) = C _ { a ^ { \\prime } } ( t )$ for all groups of $a , a ^ { \\prime }$ and all $t \\in \\mathbb { R }$ .\n\nIn particular, this property is achieved by an equalized odds score $\\widetilde { R }$ . Within each protected group, i.e. for each value $A = a$ , the score $R ^ { * }$ differs from $\\widetilde { R }$ by a fixed monotone transformation, namely an additive shift $R ^ { * } = \\widetilde { R } + A$ . Consider a derived threshold predictor $\\widehat { Y } ( \\widetilde { R } ) = \\mathbb { I } \\big \\{ \\widetilde { R } > t \\big \\}$ based on ${ \\widetilde { R } }$ . Any such predictor obeys equalized odds. We can also derive the same predictor\n\ndeterministically from $R ^ { * }$ and $A$ as $\\widehat { Y } ( R ^ { * } , A ) = \\mathbb { I } \\left\\{ R ^ { * } > t _ { A } \\right\\}$ where $t _ { A } = t { - } A$ . That is, in our particular example, $R ^ { * }$ is special in that optimal equalized odds predictors can be derived from it (and the protected attribute $A$ ) deterministically, without the need to introduce randomness as in Section 4.2. In terms of the $A$ -conditional ROC curves, this happens because the images of the conditional ROC curves $C _ { 0 }$ and $C _ { 1 }$ overlap, making it possible to choose points in the true/false-positive rate plane that are on both ROC curves. However, the same point on the conditional ROC curves correspond to different thresholds! Instead of $C _ { 0 } ( t ) = C _ { 1 } ( t )$ , for $R ^ { * }$ we have $C _ { 0 } ( t ) = C _ { 1 } ( t - 1 )$ . We refer to this property as “matching” conditional ROC curves:\n\nDefinition 6.3 (Matching ROC curves). We say that a score R has matching conditional ROC curves if the images of all $A$ -conditional ROC curves are the same, i.e., for all groups $a , a ^ { \\prime }$ , $\\{ C _ { a } ( t ) \\colon t \\in \\mathbb { R } \\} = \\{ C _ { a ^ { \\prime } } ( t ) \\colon t \\in \\mathbb { R } \\}$ .\n\nHaving matching conditional ROC curves corresponds to being deterministically correctable to be non-discriminating: If a predictor $R$ has matching conditional ROC curves, then for any loss function the optimal equalized odds derived predictor is a deterministic function of $R$ and A. But as our examples show, having matching ROC curves does not at all mean the score is itself non-discriminatory: it can be biased according to $A$ , and a (deterministic) correction might be necessary in order to ensure equalized odds.\n\nHaving identical or matching ROC curves are properties of the conditional distribution $R | Y , A ,$ also referred to as “model errors”. Oblivious measures can also depend on the conditional distribution $Y | R , A ,$ also referred to as “target population errors”. In particular, one might consider the following property:\n\nDefinition 6.4 (Matching frequencies). We say that a score $R$ has matching conditional frequencies, if for all groups $a , a ^ { \\prime }$ and all scores $t$ , we have\n\nMatching conditional frequencies state that at a given score, both groups have the same probability of being labeled positive. The definition can also be phrased as requiring that the conditional distribution $Y | R , A$ be independent of $A$ . In other words, having matching conditional frequencies is equivalent to $A$ and Y being independent conditioned on R. The corresponding dependency structure is $Y - R - A$ . That is, the score $R$ includes all possible information the protected attribute can provide on the target Y . Indeed having matching conditional frequencies means that the score is in a sense “optimally dependent” on the protected attribute A. Formally, for any loss function the optimal (unconstrained, possibly discriminatory) derived predictor $\\overrightharpoon { Y } ( R , A )$ would be a function of $R$ alone, since $R$ already includes all relevant information about $A$ . In particular, an unconstrained optimal score, like $R ^ { * }$ in our constructions, would satisfy matching conditional frequencies. Having matching frequencies can therefore be seen as a property indicating utilizing the protected attribute for optimal predictive power, rather then protecting discrimination based on it.\n\nIt is also worth noting the similarity between matching frequencies and a binary predictor $\\widehat { Y }$ having equal conditional precision, that is $\\operatorname* { P r } \\left\\{ Y = 1 \\mid { \\widehat { Y } } = { \\widehat { y } } , { \\widehat { A } } = a \\right\\} = \\operatorname* { P r } \\left\\{ Y = 1 \\mid { \\widehat { Y } } = { \\widehat { y } } , { \\widehat { A } } = a ^ { \\prime } \\right\\}$ . Viewing $\\widehat { Y }$ as a score that takes two possible values, the notions agree. But $R$ having matching conditional frequencies does not imply the threshold predictors $\\widehat { Y } ( \\bar { R } ) = \\mathbb { I } \\{ R > t \\}$ will have matching precision—the conditional distributions $R | A$ might be different, and these are involved in marginalizing over $R > t$ and $R \\leqslant t$ .\n\nTo summarize, the properties of the scores in our scenarios are:\n\n• $R ^ { * }$ is optimal based on the features and protected attribute, without any constraints.   \n• $\\widetilde { R }$ is optimal among all equalized odds scores.   \n• $\\widetilde { R }$ does satisfy equal odds, $R ^ { * }$ does not satisfy equal odds.   \n• $\\widetilde { R }$ has identical (thus matching) ROC curves, $R ^ { * }$ has matching but non-identical ROC curves.   \n• $R ^ { * }$ has matching conditional frequencies, while $\\widetilde { R }$ does not.",
      "referring_paragraphs": [
        "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.02413_page0_fig10.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig10.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_12",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg",
      "image_filename": "1610.02413_page0_fig11.jpg",
      "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.",
      "context_before": "That is, optimal classification only depends on $x _ { 1 } + x _ { 2 }$ and so $R ^ { * } = X _ { 1 } + X _ { 2 }$ is optimal.\n\nTurning to scenario II, since $P ( Y | X _ { 3 } )$ is monotone in $X _ { 3 }$ , any monotone function of it is optimal (unconstrained), and the dependency structure implies its optimal even if we allow dependence on A. Furthermore, the conditional distribution $Y | X _ { 3 }$ matched that of $Y | R ^ { * }$ from scenario I since again we have $\\mathrm { P r } \\left\\{ Y = y \\vert X _ { 3 } = x _ { 3 } \\right\\} \\propto \\exp ( 2 y x _ { 3 } )$ by construction. Since we defined $R ^ { * } = X _ { 3 }$ , we have that the conditionals $R ^ { * } | Y$ match. We can also verify that by construction $X _ { 3 } | A$ matches $R ^ { * } | A$ in scenario I. Since in scenario I, $R ^ { * }$ is optimal even dependent on $A$ , we have that $A$ is independent of $Y$ conditioned on $R ^ { * }$ , as in scenario II when we condition on $X _ { 3 } = R ^ { * }$ . This establishes the joint distribution over $( A , Y , R ^ { * } )$ is the same in both scenarios. Since $\\widetilde { R }$ is the same deterministic function of $A$ and $R ^ { * }$ in both scenarios, we can further conclude the joint distributions over $A , Y , R ^ { * }$ and $\\widetilde { R }$ are the same. Since equalized odds is an oblivious property, once these distributions match, if $\\widetilde { R }$ obeys equalized odds in scenario I, it also obeys it in scenario II.\n\n[Section: 7 Case study: FICO scores]\n\nWe examine various fairness measures in the context of FICO scores with the protected attribute of race. FICO scores are a proprietary classifier widely used in the United States to predict credit worthiness. Our FICO data is based on a sample of 301536 TransUnion TransRisk scores from 2003 [Res07]. These scores, ranging from 300 to 850, try to predict credit risk; they form our score R. People were labeled as in default if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; this gives an outcome Y . Our protected attribute $A$ is race, which is restricted to four values: Asian, white non-Hispanic (labeled “white”\n\nin figures), Hispanic, and black. FICO scores are complicated proprietary classifiers based on features, like number of bank accounts kept, that could interact with culture—and hence race—in unfair ways. A credit score cutoff of 620 is commonly used for prime-rate loans1,",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 7: These two marginals, and the number of people per group, constitute our input data."
      ],
      "figure_type": "example",
      "sub_figures": [
        "1610.02413_page0_fig12.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig12.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_14",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg",
      "image_filename": "1610.02413_page0_fig13.jpg",
      "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Hence it will pick the single threshold at which $8 2 \\%$ of people do not default overall, shown in Figure 8.",
        "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.02413_page0_fig14.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig14.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_16",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg",
      "image_filename": "1610.02413_page0_fig15.jpg",
      "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.",
      "context_before": "",
      "context_after": "[Section: 8 Conclusions]\n\nWe proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion. Second, it is fully",
      "referring_paragraphs": [
        "As shown in Section 4, the optimal thresholds can be computed efficiently; the results are shown in Figure 9.",
        "Figure 9: FICO thresholds for various definitions of fairness."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.02413_page0_fig16.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig16.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_18",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg",
      "image_filename": "1610.02413_page0_fig17.jpg",
      "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.",
      "context_before": "[Section: 8 Conclusions]\n\nWe proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of demographic parity as a fairness notion. Second, it is fully",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 10 plots the ROC curves for each group.",
        "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.02413_page0_fig18.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig18.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1610.02413",
      "figure_id": "1610.02413_fig_20",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg",
      "image_filename": "1610.02413_page0_fig19.jpg",
      "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.",
      "context_before": "",
      "context_after": "[Section: References]\n\n[BS16] Solon Barocas and Andrew Selbst. Big data’s disparate impact. California Law Review, 104, 2016.   \n[BZVGRG15] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Learning fair classifiers. CoRR, abs:1507.05259, 2015.   \n[CKP09] T. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints. In In Proc. IEEE International Conference on Data Mining Workshops, pages 13–18, 2009.   \n$\\left[ \\mathrm { D H P ^ { + } 1 } 2 \\right]$ Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness through awareness. In Proc. ACM ITCS, pages 214–226, 2012.   \n$\\left[ \\mathrm { F F M ^ { + } } 1 5 \\right]$ Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proc. 21st ACM SIGKDD, pages 259–268. ACM, 2015.   \n[KMR16] Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent tradeoffs in the fair determination of risk scores. CoRR, abs/1609.05807, 2016.   \n[LSL+15] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair autoencoder. CoRR, abs/1511.00830, 2015.   \n$\\left[ \\mathrm { P P M ^ { + } 1 4 } \\right]$ John Podesta, Penny Pritzker, Ernest J. Moniz, John Holdren, and Jefrey Zients. Big data: Seizing opportunities and preserving values. Executive Office of the President, May 2014.   \n[PRT08] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Proc. 14th ACM SIGKDD, 2008.   \n[Res07] US Federal Reserve. Report to the congress on credit scoring and its effects on the availability and affordability of credit, 2007.   \n[RR14] Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. The Knowledge Engineering Review, 29:582–638, 11 2014.   \n[Was10] Larry Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer, 2010.   \n[Whi16] Big data: A report on algorithmic systems, opportunity, and civil rights. Executive Office of the President, May 2016.   \n[Zli15] Indre Zliobaite. On the relation between accuracy and fairness in binary classification. CoRR, abs/1505.05723, 2015.   \n$\\left[ Z \\mathrm { W } S ^ { + } 1 3 \\right]$ Richard S. Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. Learning fair representations. In Proc. 30th ICML, 2013.",
      "referring_paragraphs": [
        "The left side of Figure 11 shows the fraction of people that wouldn’t default that would qualify for loans by the various metrics.",
        "Figure 11: On the left, we see the fraction of non-defaulters that would get loans."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1610.02413_page0_fig20.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig20.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1610.07524": [
    {
      "doc_id": "1610.07524",
      "figure_id": "1610.07524_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg",
      "image_filename": "1610.07524_page0_fig0.jpg",
      "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.",
      "context_before": "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.\n\n[Section: 2.1 Implied constraints on the false positive and false negative rates]\n\nTo facilitate a simpler discussion of error rates, we introduce the coarsened score $S _ { c }$ , which is obtained",
      "context_after": "The coarsened score simply assesses each defendant as being at high-risk or low-risk of recidivism. For the purpose of our discussion, we will think of $S _ { c }$ as a classifier used to predict the binary outcome $Y$ . This allows us to summarize $S _ { c }$ in terms of a confusion matrix, as shown below.\n\n<table><tr><td></td><td>Sc=Low-Risk</td><td>Sc=High-Risk</td></tr><tr><td>Y=0</td><td>TN</td><td>FP</td></tr><tr><td>Y=1</td><td>FN</td><td>TP</td></tr></table>\n\nIt is easily verified that test fairness of $S$ implies that the positive predictive value of the coarsened score $S _ { c }$ does not depend on $R$ . More precisely, it implies that that the quantity\n\ndoes not depend on $r$ . Equation (2.3) thus forms a necessary condition for the test fairness of $S$ . We can think of this as a constraint on the values of the confusion matrix. A second constraint—one that we have no direct control over—is the recidivism prevalence within groups, which we denote here by $p _ { r } \\equiv \\mathbb { P } ( Y = 1 \\mid R = r )$ .\n\nGiven values of the $\\mathrm { P P V } \\in ( 0 , 1 )$ and prevalence $p \\in ( 0 , 1 )$ , it is straightforward to show that the false negative rate $\\mathrm { F N R } = \\mathbb { P } ( S _ { c } = \\mathrm { L R } \\mid Y = 1 )$ and false positive rate $\\mathrm { F P R } = \\mathbb { P } ( S _ { c } = \\mathrm { H R } \\mid Y = 0 )$ are related\n\nvia the equation\n\nA direct implication of this simple expression is that when the recidivism prevalence differs between two groups, a test-fair score $S _ { c }$ cannot have equal false positive and negative rates across those groups.3\n\nThis observation enables us to better understand why the ProPublica authors observed large discrepancies in FPR and FNR between Black and White defendants.4 The recidivism rate among back defendants in the data is $5 1 \\%$ , compared to $3 9 \\%$ for White defendants. Since the COMPAS RPI approximately satisfies test fairness, we know that some level of imbalance in the error rates must exist.",
      "referring_paragraphs": [
        "Figure 1 shows a plot of the observed recidivism rates across all possible values of the COMPAS score. We can see that the COMPAS RPI appears to adhere well to the test fairness condition. In their response to the ProPublica investigation, Flores et al. [10] further verify this adherence using logistic regression.",
        "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.\n\nby thresholding $S$ at some cutoff $s H R$"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1610.07524",
      "figure_id": "1610.07524_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
      "image_filename": "1610.07524_page0_fig1.jpg",
      "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ",
      "context_before": "When using a test-fair RPI in populations where recidivism prevalence differs across groups, it will generally be the case that the higher recidivism prevalence group will have a higher FPR and lower FNR. From equations (3.2) and (3.3), we can see that this would result in greater penalties for defendants in the higher prevalence group, both among recidivating and non-recidivating offenders.\n\nAn interesting special case to consider is one where $t _ { L } = 0$ . This could arise in sentencing decisions for\n\noffenders convicted of low-severity crimes who have good prior records. In such cases, so-called restorative sanctions may be imposed as an alternative to a period of incarceration. If we further take $t _ { H } = 1$ , then $\\mathbb { E } T = \\mathbb { P } ( T \\neq 0 )$ , which can be interpreted as the probability that a defendant receives a sentence imposing some period of incarceration.\n\nIt’s easy to see that in such settings a nonrecidivating defendant in group $b$ is FPRb/FPRw times more likely to be incarcerated compared to a non-recidivating defendant in group $w$ .5 This naturally raises the question of whether overall differences in error rates are observed to persist across more granular subgroups.\n\nOne might expect that differences in false positive rates are largely attributable to the subset of defendants who are charged with more serious offenses and who have a larger number of prior arrests/convictions. While it is true that the false positive rates within both racial groups are higher for defendants with worse criminal histories, considerable between-group differences in these error rates persist across low prior count subgroups. Figure 2 shows a plot of false positive rates across different ranges of prior count for defendants charged with a misdemeanor offense, which is the lowest severity criminal offense category. As one can see, differences in false positive rates between Black defendants and White defendants persist across prior record subgroups.\n\n[Section: 3.1 Connections to measures of effect size]\n\nA natural question to ask is whether the level of disparate impact, $\\Delta$ , is related to some measures of effect size commonly used in scientific reporting. With a small generalization of the $\\%$ non-overlap measure, we can answer this question in the affirmative.\n\nThe $\\%$ non-overlap of two distributions is generally calculated assuming both distributions are normal, and thus has a one-to-one correspondence to Cohen’s $d$ [12].6 Figure 3 shows that the COMPAS decile score is far from being normally distributed. A more reasonable way to calculate $\\%$ non-overlap is to note that in the Gaussian case $\\%$ non-overlap",
      "context_after": "[Section: 4 Discussion]\n\nThe primary contribution of this paper was to show how disparate impact can result from the use of a recidivism prediction instrument that is known to be free from predictive bias. Our analysis focussed on the simple setting where a binary risk assessment\n\nwas used to inform a binary penalty policy. While all of the formulas have natural analogs in the nonbinary score and penalty setting, we find that all of the salient features are already present in the analysis of the simpler binary-binary problem.\n\nOur analysis indicates that there are risk assessment use cases in which it is desirable to balance error rates across different groups, even though this will generally result in risk assessments that are not free from predictive bias. However, balancing error rates overall may not be sufficient, as this does not guarantee balance at finer levels of granularity. That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2). One needs to decide the level of granularity at which error rate balance is desirable to achieve.\n\nIn closing, we would like to note that there is a large body of literature showing that data-driven risk assessment instruments tend to be more accurate than professional human judgements [13, 14], and investigating whether human-driven decisions are themselves prone to exhibiting racial bias [15, 16]. We should not abandon the data-driven approach on the basis of negative headlines. Rather, we need to work to ensure that the instruments we use are demonstrably free from the kinds of quantifiable biases that could lead to disparate impact in the specific contexts in which they are to be applied.\n\n[Section: References]",
      "referring_paragraphs": [
        "Figure 2 shows a plot of false positive rates across different ranges of prior count for defendants charged with a misdemeanor offense, which is the lowest severity criminal offense category.",
        "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense.",
        "That is, even if $\\mathrm { F P R } _ { b } = \\mathrm { F P R } _ { w }$ , we may still see differences in error rates within prior record score categories (see e.g., Figure 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1610.07524_page0_fig2.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1610.08452": [
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg",
      "image_filename": "1610.08452_page0_fig0.jpg",
      "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.",
      "context_before": "Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.\n\n[Section: 1. INTRODUCTION]\n\nThe emergence and widespread usage of automated datadriven decision making systems in a wide variety of applications, ranging from content recommendations to pretrial risk assessment, has raised concerns about their potential unfairness towards people with certain traits [8, 22, 24, 27]. Anti-discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits, also called sensitive attributes (e.g., gender, race). These laws typically distinguish between two different notions of unfairness [5] namely, disparate treatment and disparate impact. More specifically, there is disparate treatment when the decisions an individual user receives change with changes\n\nAn open-source code implementation of our scheme is available at: http://fate-computing.mpi-sws.org/\n\n$\\textcircled { \\mathrm { c } } 2 0 1 7$ International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW 2017, April 3–7, 2017, Perth, Australia. ACM 978-1-4503-4913-0/17/04. http://dx.doi.org/10.1145/3038912.3052660",
      "context_after": "to her sensitive attribute information, and there is disparate impact when the decision outcomes disproportionately benefit or hurt members of certain sensitive attribute value groups. A number of recent studies [10, 21, 29], including our own prior work [28], have focused on designing decision making systems that avoid one or both of these types of unfairness.\n\nThese prior designs have attempted to tackle unfairness in decision making scenarios where the historical decisions in the training data are biased (i.e., groups of people with certain sensitive attributes may have historically received unfair treatment) and there is no ground truth about the correctness of the historical decisions (i.e., one cannot tell whether a historical decision used during the training phase was right or wrong). However, when the ground truth for historical decisions is available, disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by means of the ground truth. Therefore, disparate impact would not be a suitable notion of unfairness in such scenarios.\n\nIn this paper, we propose an alternative notion of unfairness, disparate mistreatment , especially well-suited for scenarios where ground truth is available for historical decisions used during the training phase. We call a decision making process to be suffering from disparate mistreatment with respect to a given sensitive attribute (e.g., race) if the misclassification rates differ for groups of people having different values of that sensitive attribute (e.g., blacks and whites). For example, in the case of the NYPD Stopquestion-and-frisk program (SQF) [1], where pedestrians are stopped on the suspicion of possessing an illegal weapon [12], having different weapon discovery rates for different races would constitute a case of disparate mistreatment.\n\nIn addition to all misclassifications in general, depending on the application scenario, one might want to measure disparate mistreatment with respect to different kinds of misclassifications. For example, in pretrial risk assessments, the decision making process might only be required to ensure that the false positive rates are equal for all groups, since it may be more acceptable to let a guilty person go, rather than incarcerate an innocent person. 1 On the other hand, in loan approval systems, one might instead favor a decision making process in which the false negative rates are equal, to ensure that deserving (positive class) people with a certain sensitive attribute value are not denied (negative class) loans disproportionately. Similarly, depending on the application\n\n<table><tr><td colspan=\"3\">User Attributes</td><td rowspan=\"2\">Ground Truth (Has Weapon)</td><td colspan=\"3\">Classifier&#x27;s Decision to Stop</td><td></td><td>Disp. Treat.</td><td>Disp. Imp.</td><td>Disp. Mist.</td></tr><tr><td>Sensitive</td><td colspan=\"2\">Non-sensitive</td><td>C1</td><td>C2</td><td>C3</td><td></td><td></td><td></td><td></td></tr><tr><td>Gender</td><td>Clothing Bulge</td><td>Prox. Crime</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C1</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>Male 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>0</td><td>C2</td><td>✓</td><td>X</td><td>✓</td></tr><tr><td>Male 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Male 3</td><td>0</td><td>1</td><td>✓</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 3</td><td>0</td><td>0</td><td>✓</td><td>0</td><td>1</td><td>0</td><td></td><td></td><td></td><td></td></tr></table>\n\nFigure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.\n\nscenario at hand, and the cost of the type of misclassification, one may choose to measure disparate mistreatment using false discovery and false omission rates, instead of false positive and false negative rates (see Table 1).\n\nIn the remainder of the paper, we first formalize disparate treatment, disparate impact and disparate mistreatment in the context of (binary) classification. Then, we introduce intuitive measures of disparate mistreatment for decision boundary-based classifiers and show that, for a wide variety of linear and nonlinear classifiers, these measures can be incorporated into their formulation as convex-concave constraints. The resulting formulation can be solved efficiently using recent advances in convex-concave programming [26]. Finally, we experiment with synthetic as well as real world datasets and show that our methodology can be effectively used to avoid disparate mistreatment.\n\n[Section: 2. BACKGROUND AND RELATED WORK]\n\nIn this section, we first elaborate on the three different notions of unfairness in automated decision making systems using an illustrative example and then provide an overview of the related literature.\n\nDisparate mistreatment. Intuitively, disparate mistreatment can arise in any automated decision making system whose outputs (or decisions) are not perfectly (i.e., $1 0 0 \\%$ ) accurate. For example, consider a decision making system that uses a logistic regression classifier to provide binary outputs (say, positive and negative) on a set of people. If the items in the training data with positive and negative class labels are not linearly separable, as is often the case in many real-world application scenarios, the system will misclassify (i.e., produce false positives, false negatives, or both, on) some people. In this context, the misclassification rates may be different for groups of people having different values of sensitive attributes (e.g., males and females; blacks and whites) and thus disparate mistreatment may arise.\n\nFigure 1 provides an example of decision making systems (classifiers) with and without disparate mistreatment. In all cases, the classifiers need to decide whether to stop a pedestrian—on the suspicion of possessing an illegal weapon— using a set of features such as bulge in clothing and proximity to a crime scene. The “ground truth” on whether a pedestrian actually possesses an illegal weapon is also shown. We show decisions made by three different classifiers $\\mathbf { C _ { 1 } }$ , $\\mathbf { C _ { 2 } }$ and ${ \\bf C 3 }$ . We deem $\\mathbf { C _ { 1 } }$ and $\\mathbf { C _ { 2 } }$ as unfair due to disparate mistreatment because their rate of erroneous decisions for males and females are different: $\\mathbf { C _ { 1 } }$ has different false negative rates for males and females (0.0 and 0.5, respectively),\n\nwhereas $\\mathbf { C _ { 2 } }$ has different false positive rates (0.0 and 1.0) as well as different false negative rates (0.0 and 0.5) for males and females.\n\nDisparate treatment. In contrast to disparate mistreatment, disparate treatment arises when a decision making system provides different outputs for groups of people with the same (or similar) values of non-sensitive attributes (or features) but different values of sensitive attributes.\n\nIn Figure 1, we deem $\\mathbf { C _ { 2 } }$ and ${ \\bf C 3 }$ to be unfair due to disparate treatment since C2’s (C3’s) decisions for M ale 1 and F emale 1 (Male 2 and F emale 2) are different even though they have the same values of non-sensitive attributes. Here, disparate treatment corresponds to the very intuitive notion of fairness: two otherwise similar persons should not be treated differently solely because of a difference in gender.\n\nDisparate impact. Finally, disparate impact arises when a decision making system provides outputs that benefit (hurt) a group of people sharing a value of sensitive attribute more frequently than other groups of people.\n\nIn Figure 1, assuming that a pedestrian benefits from a decision of not being stopped, we deem $\\mathbf { C _ { 1 } }$ as unfair due to disparate impact because the fraction of males and females that were stopped are different (1.0 and 0.66, respectively).\n\nApplication scenarios for disparate impact vs. disparate mistreatment. Note that unlike in the case of disparate mistreatment, the notion of disparate impact is independent of the “ground truth” information about the decisions, i.e., whether or not the decisions are correct or valid. Thus, the notion of disparate impact is particularly appealing in application scenarios where ground truth information for decisions does not exist and the historical decisions used during training are not reliable and thus cannot be trusted. Unreliability of historical decisions for automated decision making systems is particularly concerning in scenarios like recruiting or loan approvals, where biased judgments by humans in the past may be used when training classifiers for the future. In such application scenarios, it is hard to distinguish correct and incorrect decisions, making it hard to assess or use disparate mistreatment as a notion of fairness.\n\nHowever, in scenarios where ground truth information for decisions can be obtained, disparate impact can be quite misleading as a notion of fairness. That is, in scenarios where the validity of decisions can be reliably ascertained, it would be possible to distinguish disproportionality in decision outcomes for sensitive groups that arises from justifiable reasons (e.g., qualification of the candidates) and disproportionality that arises for non-justifiable reasons (i.e.,\n\ndiscrimination against certain groups). By requiring decision outcomes to be proportional, disparate impact risks introducing reverse-discrimination against qualified candidates. Such practices have previously been deemed unlawful by courts (Ricci vs. DeStefano, 2009 ). In contrast, when the correctness of decisions can be determined, disparate mistreatment can not only be accurately assessed, but also avoids reverse-discrimination, making it a more appealing notion of fairness.\n\nRelated Work. There have been a number of studies, including our own prior work [28], proposing methods for detecting [10, 21, 23, 25] and removing [9, 10, 13, 16, 17, 23, 28, 29] unfairness when it is defined in terms of disparate treatment, disparate impact or both. However, as pointed out earlier, the disparate impact notion might be less meaningful in scenarios where ground truth decisions are available.\n\nA number of previous studies have pointed out racial disparities in both automated [4] as well as human [12, 14] decision making systems related to criminal justice. For example, a recent work by Goel et al. [12] detects racial disparities in NYPD SQF program, inspired by a notion of unfairness similar to our notion of disparate mistreatment. More specifically, it uses ground truth (stops leading to successful discovery of an illegal weapon on the suspect) to show that blacks were treated unfairly since false positive rates in stops were higher for them than for whites. The study’s findings provide further justification for the need for data-driven decision making systems without disparate mistreatment.\n\nA recent work by Hardt et al. [15] (concurrently conducted with our work) proposes a method to achieve a fairness notion equivalent to our notion of disparate mistreatment. This method works by post-processing the probability estimates of an unfair classifier to learn different decision thresholds for different sensitive attribute value groups, and applying these group-specific thresholds at decision making time. Since this method requires the sensitive attribute information at decision time, it cannot be used in cases where sensitive attribute information is unavailable (e.g., due to privacy reasons) or prohibited from being used due to disparate treatment laws [5].",
      "referring_paragraphs": [
        "Crime</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C1</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>Male 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>0</td><td>C2</td><td>✓</td><td>X</td><td>✓</td></tr><tr><td>Male 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Male 3</td><td>0</td><td>1</td><td>✓</td><td>1</td><td>0</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 1</td><td>1</td><td>1</td><td>✓</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 2</td><td>1</td><td>0</td><td>X</td><td>1</td><td>1</td><td>1</td><td>C3</td><td>✓</td><td>X</td><td>X</td></tr><tr><td>Female 3</td><td>0</td><td>0</td><td>✓</td><td>0</td><td>1</td><td>0</td><td></td><td></td><td></td><td></td></tr></table>\n\nFigure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon.",
        "Figure 1 provides an example of decision making systems (classifiers) with and without disparate mistreatment.",
        "Table 1: In addition to the overall misclassification rate, error rates can be measured in two different ways: false negative rate and false positive rate are defined as fractions over the class distribution in the ground truth labels, or true labels.",
        "Table 1 describes various ways of measuring misclassification rates.",
        "These results suggest that satisfying all five criterion of disparate mistreatment (Table 1) simultaneously is impossible when the underlying distribution of data is different for different groups."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {}
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg",
      "image_filename": "1610.08452_page0_fig1.jpg",
      "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.",
      "context_before": "[Section: 5.1.1 Disparate mistreatment on only false positive rate or false negative rate]\n\nThe first scenario considers a case where a classifier trained on the ground truth data leads to disparate mistreatment in terms of only the false positive rate (false negative rate), while being fair with respect to false negative rate (false positive rate), i.e., $D _ { F P R } \\neq 0$ and $D _ { F N R } = 0$ (or, alternatively, $D _ { F P R } = 0$ and $D _ { F N R } \\neq 0$ ).\n\nExperimental setup. We first generate 10,000 binary class labels $y \\in \\{ - 1 , 1 \\}$ ) and corresponding sensitive attribute values $( z \\in \\{ 0 , 1 \\}$ ), both uniformly at random, and assign a two-dimensional user feature vector (x) to each of the points. To ensure different distributions for negative classes of the two sensitive attribute value groups (so that the two groups have different false positive rates), the user feature vectors are sampled from the following distributions (we sample 2500 points from each distribution):\n\nNext, we train a (unconstrained) logistic regression classifier on this data. The classifier is able to achieve an accuracy of 0.85. However, due to difference in feature distributions for the two sensitive attribute value groups, it achieves $D _ { F N R } =$ $0 . 1 4 - 0 . 1 4 = 0$ and $D _ { F P R } = 0 . 2 5 - 0 . 0 6 = 0 . 1 9$ , which constitutes a clear case of disparate mistreatment in terms of false positive rate.\n\nWe then train several logistic regression classifiers on the same training data subject to fairness constraints on false positive rate, i.e., we train a logistic regressor by solving problem (18), where $g _ { \\boldsymbol { \\theta } } ( y , \\mathbf { x } )$ is given by Eq. (11). Each classifier constrains the false positive rate covariance (c) with a multiplicative factor $\\left( m \\in \\left[ 0 , 1 \\right] \\right)$ ) of the covariance of the unconstrained classifier (c∗), that is, $c = m c ^ { * }$ . Ideally, a smaller $m$ , and hence a smaller $c$ , would result in more fair outcomes.\n\nResults. Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier",
      "context_after": "[Section: 5.1.2 Disparate mistreatment on both false positive rate and false negative rate]\n\nIn this section, we consider a more complex scenario, where the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive rate and false negative rate, i.e., both $D _ { F P R }$ and $D _ { F N R }$ are non-zero. This scenario can in turn be split into two cases:\n\nI. $D _ { F P R }$ and $D _ { F N R }$ have opposite signs, i.e., the decision boundary disproportionately favors subjects from a certain sensitive attribute value group to be in the positive class (even when such assignments are misclassifications) while disproportionately assigning the subjects from the other group to the negative class. As a result, false positive rate for one group is higher than the other, while the false negative rate for the same group is lower.\n\nII. $D _ { F P R }$ and $D _ { F N R }$ have the same sign, i.e., both false positive as well as false negative rate are higher for a certain sensitive attribute value group. These cases might arise in scenarios when a certain group is harder to classify than the other.\n\nNext, we experiment with each of the above cases separately.\n\n— Case I: To simulate this scenario, we first generate 2,500 samples from each of the following distributions:\n\nAn unconstrained logistic regression classifier on this dataset attains an overall accuracy of 0.78 but leads to a false positive rate of 0.14 and 0.30 (i.e., $D _ { F P R } = 0 . 1 4 - 0 . 3 0 = - 0 . 1 6$ ) for the sensitive attribute groups $z ~ = ~ 0$ and $z ~ = ~ 1$ , respectively; and false negative rates of 0.31 and 0.12 (i.e., $D _ { F N R } = 0 . 3 1 - 0 . 1 2 = 0 . 1 9$ ). Finally, we train three different fair classifiers, with fairness constraints on (i) false positive rates— $g _ { \\boldsymbol { \\theta } } ( y , \\mathbf { x } )$ given by Eq. (11), (ii) false nega-\n\ntive rates— ${ \\bf \\nabla } _ { \\boldsymbol { g } \\boldsymbol { \\theta } } ( y , { \\bf x } )$ given by Eq. (12) and (iii) on both false positive and false negative rates—separate constraints for $g _ { \\boldsymbol { \\theta } } ( y , \\mathbf { x } )$ given by Eq. (11) and Eq. (12).\n\nResults. Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers. Here, we can observe several interesting patterns. First, removing disparate mistreatment on only false positive rate causes a rotation in the decision boundary to move previously misclassified examples with $z = 1$ into the negative class, decreasing their false positive rate. However, in the process, it also moves previously well-classified examples with $z = 1$ into the negative class, increasing their false negative rate. As a consequence, controlling disparate mistreatment on false positive rate (Figure 3(a)), also removes disparate mistreatment on false negative rate. A similar effect occurs when we control disparate mistreatment only with respect to the false negative rate (Figure 3(b)), and therefore, provides similar results as the constrained classifier for both false positive and false negative rates (Figure 3(c)). This effect is explained by the distribution of the data, where the centroids of the clusters for the group with $z = 0$ are shifted with respect to the ones for the group $z = 1$ .\n\nCase II: To simulate the scenario where both $D _ { F P R }$ and $D _ { F N R }$ have the same sign, we generate 2,500 samples from each of the following distributions:",
      "referring_paragraphs": [
        "Figure 2 summarizes the results for this scenario by showing (a) the relation between decision-boundary covariance and the false positive rates for both sensitive attribute values; (b) the trade-off between accuracy and fairness; and (c) the decision boundaries for both the unconstrained classifier (solid) and the fair constrained classifier",
        "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar.",
        "Table 2 shows the performance comparison for all the methods on the three synthetic datasets described above.",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Cov. vs FPR",
        "(b) Fairness vs. Acc.",
        "1610.08452_page0_fig2.jpg",
        "(c) Boundaries",
        "1610.08452_page0_fig3.jpg",
        "(a) FPR constraints",
        "1610.08452_page0_fig4.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig4.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg",
      "image_filename": "1610.08452_page0_fig5.jpg",
      "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3 summarizes the results for this scenario by showing the decision boundaries for the unconstrained classifier (solid) and the constrained fair classifiers.",
        "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   \n(a) FPR constraints",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) FNR constraints",
        "(c) Both constraints",
        "1610.08452_page0_fig6.jpg",
        "1610.08452_page0_fig7.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig7.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1610.08452",
      "figure_id": "1610.08452_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
      "image_filename": "1610.08452_page0_fig8.jpg",
      "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.",
      "context_before": "",
      "context_after": "[Section: 5.1.3 Performance Comparison]\n\nIn this section, we compare the performance of our scheme with two different methods on the synthetic datasets described above. In particular, we compare the performance of the following approaches:\n\nOur method: implements our scheme to avoid disparate treatment and disparate mistreatment simultaneously. Disparate mistreatment is avoided by using fairness constraints (as described in Sections 5.1.1 and 5.1.2). Disparate treatment is avoided by ensuring that sensitive attribute information is not used while making decisions, i.e., by keeping user feature vectors (x) and the sensitive features (z) disjoint. All the explanatory simulations on synthetic data shown earlier (Sections 5.1.1 and 5.1.2) implement this scheme.\n\nOur methodsen: implements our scheme to avoid disparate mistreatment only. The user feature vectors (x) and the sensitive features $( z )$ are not disjoint, that is, $z$ is used as a learnable feature. Therefore, the sensitive attribute information is used for decision making, resulting in disparate treatment.\n\nHardt et al. [15]: operates by post-processing the outcomes of an unfair classifier (logistic regression in this case) and using different decision thresholds for different sensitive attribute value groups to achieve fairness. By construction, it needs the sensitive attribute information while making decisions, and hence cannot avoid disparate treatment.\n\nBaseline: tries to remove disparate mistreatment by introducing different penalties for misclassified data points with different sensitive attribute values during training phase. Specifically, it proceeds in two steps. First, it trains an (unfair) classifier minimizing a loss function (e.g., logistic loss) over the training data. Next, it selects the set of misclassified data points from the sensitive attribute value group that presents the higher error rate. For example, if one wants to remove disparate mistreatment with respect to false positive rate and $D _ { F P R } > 0$ (which means the false positive rate for points with $z = 0$ is higher than that of $z = 1$ ), it selects the set of misclassified data points in the training set having $z = 0$ and $y = - 1$ . Next, it iteratively re-trains the classifier with increasingly higher penalties on this set of data points until a certain fairness level is achieved in the training set (until $D _ { F P R } \\leq \\epsilon$ ). The algorithm is summarized in Figure 5, particularized to ensure fairness in terms of false positive rate. This process can be intuitively ex-\n\nInput: Training set $\\mathcal { D } = \\{ ( \\mathbf { x } _ { i } , y _ { i } , z _ { i } ) \\} _ { i = 1 } ^ { N }$ , $\\Delta > 0 \\epsilon > 0$\n\nOutput: Fair baseline decision boundary $\\pmb { \\theta }$\n\nInitialize: Penalty $C = 1$\n\nTrain (unfair) classifier $\\begin{array} { r } { \\pmb { \\theta } = \\mathrm { a r g m i n } _ { \\pmb { \\theta } } \\sum _ { \\mathbf { d } \\in \\mathcal { D } } L ( \\pmb { \\theta } , \\mathbf { d } ) } \\end{array}$ Compute $\\hat { y } _ { i } = \\mathrm { s i g n } ( d _ { \\pmb { \\theta } } ( \\mathbf { x } _ { i } ) )$ and $D _ { F P }$ on $\\mathcal { D }$ .\n\nif $D _ { F P } > 0$ then $s = 0$\n\nelse $s = 1$\n\n$\\mathcal { P } = \\{ \\mathbf { x } _ { i } , y _ { i } , z _ { i } \\vert \\hat { y } \\neq y _ { i } , z _ { i } = s \\} ,$ $\\mathcal { \\hat { P } } = \\mathcal { D } \\setminus \\mathcal { P }$ .\n\nwhile $D _ { F P } > \\epsilon$ do\n\n[Section: 5.2 Real world dataset: ProPublica COMPAS]",
      "referring_paragraphs": [
        "Figure 4 summarizes the results by showing the decision boundaries for both the unconstrained classifiers (solid) and the fair constrained classifier (dashed) when controlling for disparate mistreatment with respect to false positive rate, false negative rate and both, respectively.",
        "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign.",
        "In scenarios with sufficiently large training datasets, we expect more reliable estimates of covariance,\n\n<table><tr><td rowspan=\"4\">Synthetic setting 1\n(Figure 2)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td colspan=\"3\">FPR constraints</td><td colspan=\"3\">FNR constraints</td><td colspan=\"3\">Both constraints</td></tr><tr><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td><td>Acc.</td><td>DFPR</td><td>DFNR</td></tr></table>\n\n<table><tr><td>0.80</td><td>0.02</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.25</td><td>-</td><td>-</td><td>-</td><td>0.83</td><td>0.07</td><td>0.01</td></tr><tr><td>0.65</td><td>0.00</td><td>0.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0.85</td><td>0.00</td><td>0.21</td><td>-</td><td>-</td><td>-</td><td>0.80</td><td>0.00</td><td>0.02</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 2\n(Figure 3)</td><td>Our method</td></tr><tr><td>Our method sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td><td>0.75</td><td>-0.01</td><td>0.01</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.02</td><td>0.01</td><td>0.80</td><td>0.01</td><td>0.02</td></tr><tr><td>0.59</td><td>-0.01</td><td>0.15</td><td>0.59</td><td>-0.15</td><td>0.01</td><td>0.76</td><td>-0.04</td><td>0.03</td></tr><tr><td>0.80</td><td>0.00</td><td>0.03</td><td>0.80</td><td>0.03</td><td>0.00</td><td>0.79</td><td>0.00</td><td>-0.01</td></tr></table>\n\n<table><tr><td rowspan=\"4\">Synthetic setting 3\n(Figure 4)</td><td>Our method</td></tr><tr><td>Our method Sen</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\n<table><tr><td>0.77</td><td>0.00</td><td>0.19</td><td>0.77</td><td>0.55</td><td>0.04</td><td>0.69</td><td>-0.01</td><td>0.06</td></tr><tr><td>0.78</td><td>0.00</td><td>0.42</td><td>0.79</td><td>0.38</td><td>0.03</td><td>0.77</td><td>0.14</td><td>0.06</td></tr><tr><td>0.57</td><td>0.01</td><td>0.09</td><td>0.67</td><td>0.44</td><td>0.01</td><td>0.38</td><td>-0.43</td><td>0.01</td></tr><tr><td>0.78</td><td>0.01</td><td>0.44</td><td>0.79</td><td>0.41</td><td>0.02</td><td>0.67</td><td>0.02</td><td>0.00</td></tr></table>\n\n<table><tr><td rowspan=\"3\">ProPuclica\nCOMPAS\n(Section 5.2)</td><td>Our methodSEN</td></tr><tr><td>Baseline</td></tr><tr><td>Hardt et al.</td></tr></table>\n\nTable 2: Performance of different methods while removing disparate mistreatment with respect to false positive rate, false negative rate and both."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) FNR constraints",
        "(c) Both constraints",
        "1610.08452_page0_fig9.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig9.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1611.07438": [
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg",
      "image_filename": "1611.07438_page0_fig0.jpg",
      "caption": "Figure 1: Causal graph of an example university admission system.",
      "context_before": "[Section: 2 Modeling Discrimination Using Causal Graph]\n\nConsider a dataset $\\mathcal { D }$ which may contain discrimination against a certain protected group. Each individual in $\\mathcal { D }$ is specified by a set of attributes V, which includes the protected attribute (e.g., gender), the decision attribute (e.g., admission), and the non-protected attributes (e.g., major). Throughout this paper, we use an uppercase alphabet, e.g., $X$ , to represent an attribute; a bold uppercase alphabet, e.g., X, to represent a subset of attributes, e.g., {gender, major}. We use a lowercase alphabet, e.g., $x ,$ , to represent a domain value of attribute $X$ ; a bold lowercase alphabet, e.g., x, to represent a value assignment to X. We denote the decision attribute by $E$ , associated with domain values of positive decision $e ^ { + }$ and negative decision $e ^ { - }$ ; denote the protected attribute by $C$ , associated with two domain values $c ^ { - }$ (e.g., female) and $c ^ { + }$ (e.g., male).\n\nFor the measurement of discrimination, we use risk difference [24] to measure the difference in the the proportion of positive decisions between the protected group and nonprotected group. Formally, by assuming $c ^ { - }$ is the protected group, risk difference is defined as $\\Delta P | _ { s } ~ = ~ \\mathrm { P r } ( e ^ { + } | c ^ { + } , s ) -$ $\\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { s } )$ ,, where s denotes a specified subpopulation pro-,duced by a partition S. We say that the protected group is treated less favorably within subpopulation s if $\\Delta P | _ { \\textbf { s } } \\ge \\tau$ , where $\\tau > 0$ τ is a threshold for discrimination depending on τ >law. For instance, the 1975 British legislation for sex discrimination sets $\\tau = 0 . 0 5$ , namely a $5 \\%$ difference. To avoid reverse discrimination, we do not specify which group is the protected group. Thus, we use $| \\Delta P | _ { \\mathbf { s } } |$ to deal with both scenarios where either $c ^ { - }$ or $c ^ { + }$ is designated as the protected group.\n\nA DAG $\\mathcal { G }$ is represented by a set of nodes and a set of arcs. Each node represents an attribute in $\\mathcal { D }$ . Each arc, denoted by an arrow in the graph, connects a pair of nodes where the node emanating the arrow is called the parent of the other node. The DAG is assumed to satisfy the Markov condition, i.e., each node $X$ is independent of all\n\nits non-descendants conditioning on its parents $\\operatorname { P a r } ( X )$ . Each node is associated with a conditional probability table (CPT) specified by $\\mathrm { P r } ( X | \\mathrm { P a r } ( X ) )$ . The joint probability distribution can be computed using the factorization formula [16]:\n\nSpirtes et al. [26] have shown that, when causal interpretations are given to the DAG, i.e., each node’s parents are this node’s direct causes, the DAG represents a correct causal structure of the underlying data. In particular, the causation among the attributes are encoded in the missing arcs in the DAG: if there is no arc between two nodes in $\\mathcal { G }$ , then it guarantees no direct causal effect between the two attributes in $\\mathcal { D }$ . The DAG with the causal interpretation is called the causal graph.\n\nIn this paper, we assume that we have a causal graph $\\mathcal { G }$ that correctly represents the causal structure of the dataset. We also assume that the arc $C  E$ is present in $\\mathcal { G }$ , since the absence of the arc represents a zero direct effect of $C$ on $E$ . A causal DAG can be learned from data and domain knowledge. In the past decades, many algorithms have been proposed to learn causal DAGs, and they are proved to be quite successful [26, 21, 5, 12]. In our implementation, we use the original PC algorithm [26] to build the causal graph.\n\n2.1 Identifying Meaningful Partition Discrimination occurs due to difference decisions made explicitly based on the membership in the protected group. As stated in [9], all discrimination claims require plaintiffs to demonstrate the existence of a causal connection between the decision and the protected attribute. Given a partition S, for $\\Delta P \\vert _ { \\mathbf { s } }$ to capture the discriminatory effect and be considered as a quantitative evidence of discrimination, one needs to prove that this difference is directly caused by the protected attribute. In causal graphs, causal effects are carried by the paths that trace arrows pointing from the cause to the effect. Specially, the direct causal effect of $C$ on $E$ , if exists, is carried by the direct arc from $C$ to $E$ , i.e., $C  E$ in the causal graph. In the following, we show that $\\Delta P \\vert _ { \\mathbf { s } }$ captures the direct causal effect of $C$ on $E$ if S satisfies certain requirements, which we refer to as the block set.\n\nWe use the graphical criterion $d \\cdot$ -separation. Consider a dataset $\\mathcal { D }$ and its represented causal graph $\\mathcal { G }$ . Nodes $X$ and Y are said to be $d$ -separated by a set of attributes $\\mathbf { Z }$ in $\\mathcal { G }$ , denoted by $( X \\perp \\perp Y \\mid \\mathbf { Z } ) _ { \\mathcal { G } }$ , if following requirement is met:\n\nDefinition 2.1. ( $^ { d }$ -Separation [26]) Nodes $X$ and $Y$ are dseparated by Z if and only if Z blocks all paths from $X$ t o Y. A path $p$ is said to be blocked by a set of nodes Z if and only if\n\n1. p contains a chain $i  m  j$ or a fork $i \\left. m \\right. j$ such that the middle node m is in Z, or\n\n2. p contains an collider $i \\right. m \\left. j$ such that the middle node m is not in Z and no descendant of m is in Z.\n\nAttributes $X$ and $Y$ are said to be conditionally independent given a set of attributes $\\mathbf { Z }$ , denoted by $( X \\mathrm { ~ \\bf ~ \\underline { ~ } { ~ \\underline { ~ } { ~ \\bf ~ Y ~ } ~ } ~ } | \\mathrm { ~ \\bf ~ Z ) _ { \\mathcal { D } } ~ }$ , if $\\mathrm { P r } ( x | y , \\mathbf { z } ) ~ = ~ \\mathrm { P r } ( x | \\mathbf { z } )$ holds for all values $x , y , \\mathbf { z }$ . With ,the Markov condition, the $d$ , ,-separation criterion in $\\mathcal { G }$ and the conditional independence relations in $\\mathcal { D }$ are connected such that, if we have $( X \\ \\perp \\mid Y \\ \\mid \\ Z ) _ { \\mathcal { G } }$ , then we must have $( X \\perp \\perp Y \\mid \\mathbf { Z } ) _ { \\mathcal { D } }$ .\n\nWe make use of the above connection to identify the direct causal effect of $C$ on $E$ . We construct a new DAG $\\scriptstyle { G ^ { \\prime } }$ by deleting the arc $C  E$ from $\\mathcal { G }$ and keeping everything else unchanged. Thus, the possible difference between the causal relationships represented by $\\scriptstyle { G ^ { \\prime } }$ and $\\mathcal { G }$ lies merely in the presence of the direct causal effect of $C$ on $E$ . We consider a node set B such that $( E \\mathrm { ~ \\bf ~ \\underline { ~ } { ~ \\underline { ~ } { ~ \\bf ~ U ~ } ~ } ~ } C \\mathrm { ~ \\bf ~ \\underline { ~ } { ~ \\bf ~ \\mathsf { ~ B ~ } ~ } ~ } ) _ { \\mathcal { G } ^ { \\prime } }$ , and use B to examine the conditional independence relations in $\\mathcal { D }$ . If there is no direct causal effect of $C$ on $E$ in $\\mathcal { G }$ , we should also obtain $( E ~ \\texttt { l l } C \\texttt { l } \\mathbf { B } ) _ { \\mathcal { G } }$ , which entails $( E \\perp \\perp C \\mid \\mathbf { B } ) _ { \\mathcal { D } }$ , i.e., $\\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) = \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } ) = \\operatorname* { P r } ( e ^ { + } | \\mathbf { b } )$ for each value assignment $\\mathbf { b }$ , ,of B. However, if we observe $\\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) \\neq \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } )$ , the difference must be due to the , ,existence of the direct causal effect of $C$ on $E$ . Therefore, $\\Delta P | _ { \\mathbf { b } } = \\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { b } ) - \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { b } )$ can be used to measure the ,direct causal effect of $C$ on $E$ ,. On the other hand, if a node set S does not satisfy $( E \\perp \\perp C \\mid \\mathbf { S } ) _ { \\mathcal { G } ^ { \\prime } }$ , then this conditional dependence between $C$ and $E$ given S that is not caused by the direct causal effect will also exist in $\\Delta P \\vert _ { \\mathbf { s } }$ . As a result, $\\Delta P \\vert _ { \\mathbf { s } }$ cannot accurately measure the direct causal effect.\n\nIt must be noted that, for $\\Delta P \\vert _ { \\mathbf { b } }$ to correctly measure the direct causal effect, set B cannot contain any descendant of $E$ even when it satisfies the requirement $( E \\perp \\perp C \\mid \\mathbf { B } ) _ { \\mathcal { G } ^ { \\prime } }$ . This is because when conditioning on $E$ ’s descendants, part of the knowledge of $E$ is already given since the consequences caused by $E$ is known.\n\nBased on the above analysis, we measure the direct causal effect of $C$ on $E$ using $\\Delta P \\vert _ { \\mathbf { b } }$ where the node set B satisfies the following requirements: (1) $( C \\parallel E \\mid \\mathbf { B } ) _ { \\mathcal { G } ^ { \\prime } }$ holds; (2) B contains none of $E$ ’s decedents. We call such set B a block set. By treating the direct causal effect of $C$ one $E$ as the effect of direct discrimination, $\\Delta P \\vert _ { \\mathbf { b } }$ is considered as a correct measure for direct discrimination if and only if B is a block set. Thus, if $| \\Delta P | _ { \\mathbf { b } } | \\ge \\tau$ , it is a quantitative evidence τof discrimination against either $c ^ { - }$ or $c ^ { + }$ for subpopulation b. As can be seen, each block set B forms a meaningful partition on the dataset where the direct causal effect of $C$ on $E$ within each subpopulation b can be correctly measured by $\\Delta P \\vert _ { \\mathbf { b } }$ . On the other hand, for any partition that is not defined by a block set, the measured differences, which either contain spurious influences or have been explained by the consequences of the decisions, cannot accurately measure the direct causal effect and hence cannot be used to prove discrimination or non-discrimination. Therefore, we",
      "context_after": "[Section: 3 Discrimination Discovery and Prevention]\n\n3.1 Non-Discrimination Criterion Now we develop the criterion that ensures non-discrimination for a dataset. Based on Theorem 2.1, if each block set B is examined and ensures that $| \\Delta P | _ { \\bf b } | < \\tau$ holds for each subpopulation b, we can guarantee that the dataset contains no discriminatory effect and is not liable for any claim of direct discrimination. Otherwise, there exists a subpopulation b of a block set such that $| \\Delta P | _ { \\mathbf { b } } | \\ge \\tau$ , which implies that subpopulation b suffers τthe risk of being accused of discrimination. Therefore, we give the following theorem.\n\nTheorem 3.1. Non-discrimination is claimed for D if and only if inequality $| \\Delta P | _ { \\mathbf { b } } | < \\tau$ holds for each value assignment b of each block set B.\n\nWe use the illustrative examples in Section 1 to show how the criterion works. The causal graph of the examples is shown in Figure 1. There are two block sets in this graph: {major}, and {major,test score}. Note that test score alone is not a block set. That is why conditioning on it will produce misleading results. For the example shown in Table 1, examining both block sets shows no discriminatory effect. Thus, non-discrimination can be claimed. For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 . 0 6$ , $| \\Delta P | _ { \\mathrm { \\{ m a t h , A \\} } } | = 0 . 1 0 , | \\Delta P | _ { \\mathrm { \\{ b i o l o g y , B \\} } } | = 0 . 0 5$ , and $| \\Delta P | _ { \\{ { \\mathrm { b i o l o g y } } , \\mathrm { A } \\} } | =$ . .0 10. Thus, the evidences of discrimination for four subpop-.ulations are identified.\n\nAlthough Theorem 3.1 provides a clear criterion for non-discrimination, it requires examining each subpopulation of each block set. A brute force algorithm may have an exponential complexity. Instead of examining all block sets,\n\nthe following theorem shows that we only need to examine one node set Q, which is the set of all $E$ ’s parents except $C$ , i.e., $\\mathbf { Q } = \\operatorname { P a r } ( E ) \\backslash \\{ C \\}$ .\n\nTheorem 3.2. Non-discrimination is claimed if and only if inequality $| \\Delta P | _ { \\mathbf { q } } | < \\tau$ holds for each value assignment q of set Q where $\\mathbf { Q } = \\operatorname { P a r } ( E ) \\backslash \\{ C \\}$ .\n\nProof. We first give two lemmas and their proofs are given Appendices A and B.\n\nLemma 3.1. Given a value assignment b of a block set B, $\\mathbf { Q } = \\operatorname { P a r } ( E ) \\backslash \\{ C \\}$ , we have\n\nwhere ${ \\bf q } ^ { \\prime }$ goes through all the possible combinations of the values of non-overlapping attributes $\\mathbf { Q } ^ { \\prime } \\ = \\ \\mathbf { Q } \\backslash \\mathbf { B }$ . For overlapping attributes $\\mathbf { Q } \\cap \\mathbf { B }$ , q and b have the same values.\n\nLemma 3.2. Node set Q of all E’s parents except C, i.e., $\\mathbf { Q } = \\operatorname { P a r } ( E ) \\backslash \\{ C \\}$ , must be a block set.\n\nLemma 3.1 indicates that, for each value assignment b of each block set, $\\Delta P \\vert _ { \\mathbf { b } }$ can be expressed by a weighted average of $\\Delta P \\vert _ { \\mathbf { q } }$ . If $| \\Delta P | _ { \\mathbf { q } } | < \\tau$ for each subpopulation of <Q, then it is guaranteed that $| \\Delta P | _ { \\bf b } | < \\tau$ holds for each < τsubpopulation of each block set. According to Theorem 3.1, non-discrimination is claimed. Otherwise, Lemma 3.2 means that $| \\Delta P | _ { \\mathbf { b } } | \\ge \\tau$ for at least one block set Q, which τprovides the evidence of discrimination.\n\n3.2 Discrimination Discovery We present the nondiscrimination certifying algorithm based on Theorem 3.2. The algorithm first finds set $\\mathbf { Q }$ in the graph. Then, the algorithm computes $| \\Delta P | _ { \\mathbf { q } } | ~ = ~ | \\operatorname* { P r } ( e ^ { + } | c ^ { + } , \\mathbf { q } ) - \\operatorname* { P r } ( e ^ { + } | c ^ { - } , \\mathbf { q } ) |$ , ,for each subpopulation q, and makes the judgment of nondiscrimination based on the criterion. The procedure of the algorithm is shown in Algorithm 1.\n\nAlgorithm 1 Certifying of Non-Discrimination (Certify)   \nInput: dataset $\\mathcal{D}$ protected attribute $C$ , decision $E$ , user-defined parameter $\\tau$ Output: judgment of non-discrimination judge, parents of $E$ except $C\\mathbf{Q}$ 1: $\\mathbf{Q} =$ findParent(E)\\{C}   \n2: for all value assignment q of Q do   \n3: $|\\Delta P|_{\\mathbf{q}}| = |\\operatorname *{Pr}(e^{+}|c^{+},\\mathbf{q}) - \\operatorname *{Pr}(e^{+}|c^{-},\\mathbf{q})|$ 4: if $|\\Delta P|_{\\mathbf{q}}|\\geq \\tau$ then   \n5: return [false,Q]   \n6: end if   \n7: end for   \n8: return [true,Q]\n\nThe complexity from Line 2 to 8 is $O ( | \\mathbf { Q } | )$ , where |Q| is the number of value assignments of Q. The function\n\nf indParent $( E )$ (Line 1) finds the parents of $E$ in a causal graph. A straightforward way is to first build a causal graph from the dataset using a structure learning algorithm (e.g., the classic PC algorithm), then find the parents of $E$ in the graph. The complexity of the PC algorithm is bounded by the largest degree in the undirected graph. In the worst case, the number of conditional independence tests required by the algorithm is bounded by $\\frac { n ^ { 2 } ( n - 1 ) ^ { k - 1 } } { ( k - 1 ) ! }$ where $k$ is the maximal degree of any vertex and $n$ is the number of vertices. However, in our algorithm we only need to identify the parents of $E$ without the need of building the complete network. Thus, we can use local causal discovery algorithms such as the Markov blanket [27] to determine the local structure for the decision attribute $E$ . We leave this part as our future work.\n\n3.3 Discrimination Removal When non-discrimination is not claimed, the discriminatory effects need to be removed by modifying the data before it is used for predictive analysis (e.g., building a discrimination-free classifier). Since the modification makes the data distorted, it may cause losses in data utility when compared with the original data. Thus, a general requirement in discrimination removal is to maximize the utility of the modified data while achieving nondiscrimination. A naive approach such as used in [8] would be totally removing the protected attribute from the dataset to eliminates discrimination. However, as we shall show in the experiments, in this way the data utility would be greatly suffered. In this section, we propose two strategies that exactly remove discrimination while retaining good data utility.\n\n3.3.1 Discrimination Removal by Modifying Causal Graph The first strategy modifies the constructed causal graph and uses it to generate a new dataset. Specifically, it modifies the CPT of $E$ , i.e., $\\mathrm { P r } ( e | c , \\mathbf { q } )$ , to obtain a new CPT $\\mathrm { P r } ^ { \\prime } ( e | c , \\mathbf { q } )$ ,, to meet the non-discrimination criterion given by ,Theorem 3.2, i.e., $| \\mathrm { P r } ^ { \\prime } ( e ^ { + } | c ^ { + } , { \\bf q } ) - \\mathrm { P r } ^ { \\prime } ( e ^ { + } | c ^ { - } , { \\bf q } ) | < \\tau$ for all , , < τsubpopulations q. The CPTs of all the other nodes are kept unchanged. The joint distribution of the causal graph after the modification can be calculated using the factorization formula (2.1). After that, the algorithm generates a new dataset based on the modified joint distribution. Since the structure of the causal graph is not changed after the modification, Q is still the parent set of $E$ excluding $C$ . Thus, according to Theorem 3.2, the newly generated dataset satisfies the nondiscrimination criterion.\n\nTo achieve a good data utility, we minimize the difference between the original distribution (denoted by $P$ ) and the modified distribution (denoted by $P ^ { \\prime }$ ). We use the Euclidean distance, i.e., $d ( P ^ { \\prime } , P ) = \\sqrt { \\scriptstyle \\sum _ { \\mathbf { v } } ( \\mathbf { P r } ^ { \\prime } ( \\mathbf { v } ) - \\mathbf { P r } ( \\mathbf { v } ) ) ^ { 2 } }$ , to measure ,the distance between the two distributions. We sort the nodes according to the topological ordering of the graph, and represent the sorted nodes as $\\{ C , { \\bf X } , E , { \\bf Y } \\}$ . Note that we must have\n\n$\\mathbf { Q } \\subseteq \\mathbf { X }$ . Then, using the factorization formula (2.1), $d ( P ^ { \\prime } , P )$ can be formulated as",
      "referring_paragraphs": [
        "Table 1: Summary statistics of Example 1.",
        "Figure 1: Causal graph of an example university admission system.",
        "The causal graph of the examples is shown in Figure 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {}
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg",
      "image_filename": "1611.07438_page0_fig1.jpg",
      "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.",
      "context_before": "where $\\begin{array} { r } { \\hat { \\mu } _ { \\mathbf { B } } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) \\cdot \\Delta P | _ { \\mathbf { b } } } \\end{array}$ and $\\begin{array} { r } { \\hat { \\sigma } _ { \\mathbf { B } } ^ { 2 } = \\sum _ { \\mathbf { B } } \\operatorname* { P r } ( \\mathbf { b } ) ( \\Delta P | _ { \\mathbf { b } } - \\hat { \\mu } _ { \\mathbf { B } } ) ^ { 2 } , } \\end{array}$ .\n\nProof. The proof is straightforward by giving two lemmas:\n\nLemma 4.1. For each block set B, $\\hat { \\mu } _ { \\bf B } ~ = ~ \\hat { \\mu } _ { \\bf Q }$ , where ${ \\textbf { Q } } =$ $\\operatorname { P a r } ( E ) \\backslash \\{ C \\}$ .\n\nLemma 4.2. For each block set B, $\\begin{array} { r } { \\hat { \\sigma } _ { \\mathbf { B } } ^ { 2 } \\ \\le \\ \\hat { \\sigma } _ { \\mathbf { Q } } ^ { 2 } , } \\end{array}$ where ${ \\textbf { Q } } =$ $\\operatorname { P a r } ( E ) \\backslash \\{ C \\}$ .\n\nRefer to the Appendices C and D for proof details.\n\n[Section: 5 Experiments]\n\nIn this section, we conduct experiments for discrimination discovery and removal algorithms by using two real data sets: the Adult dataset [18] and the Dutch Census of 2001 [22], and compare our algorithms with the conditional discrimination removal methods proposed in [28].\n\nThe causal graphs are constructed by utilizing an opensource software TETRAD [10], which is a widely used platform for causal modeling. We employ the original PC algorithm and set the significance threshold 0 01 used for con-.ditional independence testing in causal graph construction. The quadratic programming is solved using CVXOPT [2]. All experiments were conducted with a PC workstation with 16GB RAM and Intel Core i7-4770 CPU.\n\n5.1 Discrimination Discovery The Adult dataset consists of 65123 tuples with 11 attributes such as age, eduation, sex, occupation, income, etc.. Since the computational complexity of the PC algorithm is an exponential function of the number of attributes and their domain sizes, for computational feasibility we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the first tier,\n\neducation is defined in the second tier, and all other attributes are defined in the third tier. The constructed causal graph is shown in Figure 2. We treat sex (female and male) as the protected attribute and income (low income and high income) as the decision. An arc pointing from sex to income is observed. We first find set Q of income, which contains all the non-protected attributes. There are 512 subpopulations specified by Q, and 376 subpopulations with non-zero number of tuples. Then, we compute $\\Delta P \\vert _ { \\mathbf { q } }$ for the 376 subpopulations. The value of $\\Delta P \\vert _ { \\mathbf { q } }$ ranges from $- 0 . 8 5$ to 0 67 across all subpopulations. Among them, 90 . .subpopulations have $\\Delta P \\vert _ { \\mathbf { q } } ~ > ~ 0 . 0 5$ and 49 subpopulations have $\\Delta P | _ { \\mathbf { q } } ~ < ~ - 0 . 0 5$ > ., indicating the existence of discrimi-< .nation in the Adult dataset. Moreover, the mean and the standard variance of $\\Delta P \\vert _ { \\mathbf { q } }$ are 0 004 and 0 129, which has small $\\mathrm { P r } ( | \\Delta P | _ { \\mathbf { q } } | < \\tau )$ . . based on the Chebyshev’s inequality, e.g., $\\mathrm { P r } ( | \\Delta P | _ { \\mathbf { q } } | < 0 . 1 5 ) \\ge 2 5 . 9 7 \\%$ . It indicates that the non-< . .discrimination cannot be claimed for the Adult dataset even under the relaxed $\\alpha$ -non-discrimination model with large $\\tau$ and small $\\alpha$ .\n\nαAnother dataset Dutch census consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, education level is defined in the second tire, and all other attributes are defined in the third tire. The constructed causal graph is shown in Figure 3. We treat sex (female and male) as the protected attribute and occupation (occupation w low income, occupation w high income) as the decision. An arc from sex to occupation is observed in the causal graph. Set Q of occupation is $\\mathbf { Q } =$ {edu level age}. The value of $\\Delta P \\vert _ { \\mathbf { q } }$ ranges from 0 062 , .to 0 435 across all the 12 subpopulations specified by Q. .Thus, discrimination against females is detected in the Dutch dataset based on the non-discrimination criterion. Moreover, the mean and the standard variance of $\\Delta P \\vert _ { \\mathbf { q } }$ are 0 222 and 0 125, which has small $\\mathrm { P r } ( | \\Delta P | _ { \\mathbf { q } } | < \\tau )$ . based on the Cheby-.shev’s inequality, e.g., $\\mathrm { P r } ( | \\Delta P | _ { \\mathbf { q } } | < 0 . 3 0 ) \\ge 2 7 . 9 4 \\%$ . Hence, < . .the Dutch census dataset still contains discrimination based on the relaxed $\\alpha$ -non-discrimination criterion.\n\nαOur current implementation uses the PC algorithm to construct the complete causal graph. In our experiment, the PC algorithm with the default significance threshold 0 01 .takes 51.59 seconds to build the graph for the binarized Adult dataset and 139.96 seconds for the binarized Dutch census dataset. We also run the PC algorithm on the original Adult dataset, which incurs 4492.36 seconds. In our future work, we will explore the use of the local causal discovery algorithms to improve the efficiency.\n\n5.2 Discrimination Removal The performance of our two proposed discrimination removal algorithms, MGraph",
      "context_after": "",
      "referring_paragraphs": [
        "applicants</td><td>450</td><td>150</td><td>150</td><td>450</td><td>300</td><td>100</td><td>100</td><td>300</td></tr><tr><td>admission rate</td><td>20%</td><td>40%</td><td>20%</td><td>40%</td><td>50%</td><td>70%</td><td>50%</td><td>70%</td></tr><tr><td></td><td colspan=\"2\">25%</td><td colspan=\"2\">35%</td><td colspan=\"2\">55%</td><td colspan=\"2\">65%</td></tr></table>\n\nTable 2: Summary statistics of Example 2.",
        "For the example shown in Table 2, although examining {major} shows no discriminatory effect, when examining {major,test score} we observe $| \\Delta P | _ { \\mathrm { \\{ m a t h , B \\} } } | = 0 .",
        "The constructed causal graph is shown in Figure 2.",
        "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1611.07438",
      "figure_id": "1611.07438_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg",
      "image_filename": "1611.07438_page0_fig2.jpg",
      "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.",
      "context_before": "",
      "context_after": "[Section: 6 Related Work]\n\nA number of data mining techniques have been proposed to discover discrimination in the literature. Classification rule-based methods such as elift [23] and belift [20] were proposed to represent certain discrimination patterns. In [19, 29], the authors dealt with the individual discrimination by finding a group of similar individuals. Zliobaitˇ e et˙ al. [28] proposed conditional discrimination. However, their approaches cannot determine whether a partition is meaningful and hence cannot achieve non-discrimination guarantee. Our work showed that the causal discriminatory effect through $C  E$ can only be correctly measured under the partition specified by the block set. Recently, the authors in [3] proposed a framework based on the Suppes-Bayes causal network and developed several random-walk-based methods to detect different types of discrimination. However, the construction of the Suppes-Bayes causal network is impractical with the large number of attribute-value pairs. In addition, it is unclear how the number of random walks is related to practical discrimination metrics, e.g., the difference in positive decision rates.\n\nProposed methods for discrimination removal are either based on data preprocessing [13, 28] or algorithm tweaking [14, 4, 15]. The authors [7] addressed the problem of fair classification that achieves both group fairness, i.e., the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole, and individual fairness, i.e., similar individuals should be treated similarly. A recent work [8] studies how to remove disparate impact, i.e., indirect discrimination, from\n\nthe data. The authors first ensures no direct discrimination by completely removing the protected attribute $C$ from the data. Then, they proposed to test disparate impact based on how well $C$ can be predicted with the non-protected attributes, and remove disparate impact by modifying the non-protected attributes. As shown by our experiments, removing $C$ from the data would significantly damage the data utility. Still, their work is based on correlations rather than the causation.\n\n[Section: 7 Conclusions and Future Work]",
      "referring_paragraphs": [
        "Table 3: Contingency table within subpopulation q.",
        "The constructed causal graph is shown in Figure 3.",
        "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1611.07509": [
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg",
      "image_filename": "1611.07509_page0_fig0.jpg",
      "caption": "Figure 1: The toy model.",
      "context_before": "Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discovering both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data is used for predictive analysis (e.g., building classifiers). We make use of the causal network to capture the causal structure of the data. Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish the two types of discrimination as the causal effects transmitted along different paths in the network. Based on that, we propose an effective algorithm for discovering direct and indirect discrimination, as well as an algorithm for precisely removing both types of discrimination while retaining good data utility. Different from previous works, our approaches can ensure that the predictive models built from the modified data will not incur discrimination in decision making. Experiments using real datasets show the effectiveness of our approaches.\n\n[Section: Introduction]\n\nDiscrimination refers to unjustified distinctions in decisions against individuals based on their membership in a certain group. Federal Laws and regulations (e.g., the Equal Credit Opportunity Act of 1974) have been established to prohibit discrimination on several grounds, such as gender, age, sexual orientation, race, religion or belief, and disability or illness, which are referred to as the protected attributes. Nowadays various predictive models have been built around the collection and use of historical data to make important decisions like employment, credit and insurance. If the historical data contains discrimination, the predictive models are likely to learn the discriminatory relationship present in the data and apply it when making new decisions. Therefore, it is imperative to ensure that the data goes into the predictive models and the decisions made with its assistance are not subject to discrimination.\n\nIn the legal field, discrimination is usually divided into direct and indirect discrimination. Direct discrimination occurs when individuals receive less favorable treatment explicitly based on the protected attributes. An example would be rejecting a qualified female applicant in applying a university just because of her gender. Indirect discrimination\n\nrefers to the situation where the treatment is based on apparently neutral non-protected attributes but still results in unjustified distinctions against individuals from the protected group. A well-known example of indirect discrimination is redlining, where the residential Zip code of the individual is used for making decisions such as granting a loan. Although Zip code is apparently a neutral attribute, it correlates with race due to the racial makeups of certain areas. Thus, Zip code can indirectly lead to racial discrimination if there is no good reason to justify its use in making decisions.\n\nDiscrimination discovery and removal has received an increasing attention over the past few years in data science (Hajian and Domingo-Ferrer 2013; Kamiran and Calders 2012; Ruggieri, Pedreschi, and Turini 2010; Romei and Ruggieri 2014; Feldman et al. 2015). Many approaches have been proposed to deal with both direct and indirect discrimination. However, significant issues exist in current techniques. For discrimination discovery, the difference in decisions across the protected and non-protected groups is a combined (not necessarily linear) effect of direct discrimination, indirect discrimination, and other effects which are objectively explainable and should not be considered as discrimination. However, few works have explicitly identified the three different effects when measuring discrimination. For example, the classic metrics risk difference, risk ratio, relative chance, odds ratio, etc. (Romei and Ruggieri 2014) treat all the difference in decisions as discrimination. (Zliobait ˇ e, Kamiran, and Calders 2011) considered the ex- ˙ plainable effect, but failed to distinguish the effects of direct and indirect discrimination. For discrimination removal, an algorithm must ensure that the predictive models built from the historical data do not incur discrimination in decision making. However, as we will show in our experiments, previous works in removing discrimination cannot guarantee that the predictive models are not subject to discrimination even though they attempt to modify the historical data to contain no discrimination. In addition, it is a general requirement is to preserve the data utility while achieving non-discrimination. As will also shown in the experiments, totally removing all connections between the protected attribute and decision as proposed in (Feldman et al. 2015) may suffer significant utility loss.\n\nThe causal modeling based discrimination detection has been proposed most recently (Bonchi et al. 2015; Zhang,",
      "context_after": "[Section: Preliminary Concepts]\n\nA causal network is a DAG $\\mathcal { G } = ( \\mathbf { V } , \\mathbf { A } )$ where V is a set ,of nodes and A is a set of arcs. Each node represents an attribute. Each arc, denoted by an arrow pointing from the cause to the effect represents the direct causal relationship. Throughout the paper, we denote an attribute by an uppercase alphabet, e.g., $X$ ; denote a subset of attributes by a bold uppercase alphabet, e.g., X. We denote a domain value of attribute $X$ by a lowercase alphabet, e.g., x; denote a value assignment of attributes $\\mathbf { X }$ by a bold lowercase alphabet, e.g.,\n\nx. For a node $X$ , its parents are denoted by $P a ( X )$ , and its children are denoted by $C h ( X )$ . Each node is associated with a conditional probability table (CPT), i.e., $P ( x | P a ( X ) )$ . We also use $P a ( X )$ to represent a value assignment of $X$ ’s parents if no unambiguity occurs in the context. The joint distribution over all attributes $P ( \\mathbf { v } )$ can be computed using the factorization formula (Koller and Friedman 2009)\n\nwhere $P ( \\nu | P a ( V ) )$ is the observational distribution.\n\nIn the causal network, the measuring of causal effects is facilitated with the $d o$ -calculus (Pearl 2009), which simulates the physical interventions that force some attributes X to take certain values x. The post-intervention distributions, which represent the effect of the intervention, can be estimated from the observational data. Formally, the intervention that sets the value of $\\mathbf { X }$ to $\\mathbf { X }$ is denoted by $d o ( \\mathbf { X } = \\mathbf { x } )$ . The post-intervention distribution of all other attributes $\\mathbf { Y } =$ ${ \\bf V } \\backslash { \\bf X }$ , i.e., $P ( \\mathbf { Y } = \\mathbf { y } | d o ( \\mathbf { X } = \\mathbf { x } ) )$ or simply $P ( \\mathbf { y } | d o ( \\mathbf { x } ) )$ , can be expressed as a truncated factorization formula (Pearl 2009)",
      "referring_paragraphs": [
        "Figure 1: The toy model.",
        "When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white).",
        "When applied to the example in Figure 1, it means the expected change in loan approval of the disadvantage group if they had the same racial makeups shown in the Zip code as the advantage group.",
        "χThe results are shown in Table 1.",
        "Table 1: Direct/indirect discriminatory effects in the modified training data and predictions for the testing data."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.9,
      "metadata": {}
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg",
      "image_filename": "1611.07509_page0_fig1.jpg",
      "caption": "Figure 2: An example with the recanting witness criterion satisfied.",
      "context_before": "where $\\delta \\mathbf { { x } } = \\mathbf { { x } }$ means assigning any attributes in $\\mathbf { X }$ involved in δthe term ahead with the corresponding values in x. Specifically, the post-intervention distribution of a single attribute Y given an intervention on a single attribute $X$ is given by\n\nwhere the summation is a marginalization that traverses all value combinations of ${ \\bf V } \\backslash \\{ X , Y \\}$ .\n\n,By using the do-calculus, the total causal effect of $X$ on Y is defined as follows (Pearl 2009). Note that in this definition, the effect of the intervention is transmitted along all causal paths from the cause $X$ to the effect Y.\n\nDefinition 1 (Total causal effect) The total causal effect of the change of X from $x _ { 1 }$ to $x _ { 2 }$ on $Y = y$ is given by\n\nThe path-specific effect is an extension to the total causal effect in the sense that the effect of the intervention is transmitted only along a subset of causal paths from $X$ to Y. Denote a subset of causal paths by $\\pi$ . The $\\pi$ -specific effect conπ πsiders a counterfactual situation where the effect of $X$ on $Y$ with the intervention is transmitted along $\\pi$ , while the effect of $X$ πon Y without the intervention is transmitted along paths not in $\\pi$ . We denote by $P ( y \\mid d o ( x _ { 2 } | \\pi ) )$ the distribution of $Y$ πafter an intervention of changing $X$ πfrom $x _ { 1 }$ to $x _ { 2 }$ with the effect transmitted along $\\pi$ . Then, the $\\pi$ -specific effect of $X$ π πon Y is defined as follows (Avin, Shpitser, and Pearl 2005).\n\nDefinition 2 (Path-specific effect) Given a path set $\\pi$ , the $\\pi$ -specific effect of the value change of $X$ from $x _ { 1 }$ to $x _ { 2 }$ on $Y = y$ is given by",
      "context_after": "[Section: Modeling Direct and Indirect Discrimination as Path-Specific Effects]\n\nConsider a historical dataset $\\mathcal { D }$ that contains a group of tuples, each of which describes the profile of an individual. Each tuple is specified by a set of attributes V, including the protected attributes, the decision, and the nonprotected attributes. Among the non-protected attributes, assume there is a set of attributes that cannot be objectively justified if used in the decision making process, which we refer to as the redlining attributes denoted by R. For ease of presentation, we assume that there is only one protected attribute with binary values. We denote the protected attribute by $C$ associated with two domain values $c ^ { - }$ (e.g., female) and $c ^ { + }$ (e.g., male); denote the decision by $E$ associated with two domain values $e ^ { - }$ (i.e., negative decision) and $e ^ { + }$ (i.e., positive decision). Our approach can extend to handling multiple domain values of $C$ and even multiple Cs. We assume that a causal graph $\\mathcal { G }$ can be built\n\nto correctly represent the causal structure of dataset $\\mathcal { D }$ . In the past decades, many algorithms have been proposed to learn the causal network from data and they are proved to be quite successful (Spirtes, Glymour, and Scheines 2000; Neapolitan and others 2004; Colombo and Maathuis 2014; Kalisch and Buhlmann 2007). We also make a reasonable ¨ assumption that $C$ has no parent in $\\mathcal { G }$ , as the protected attribute is always an inherent nature of an individual.\n\nDiscrimination can be captured by the causal effect of $C$ on $E$ . In our context, the causal effect includes direct/indirect discrimination and other explainable effects. We model direct discrimination as the causal effect transmitted along the direct path from $C$ to $E$ , i.e., $C  E$ . Define $\\pi _ { d }$ as the path set that contains only $C  E$ . The $\\pi _ { d }$ π-specific effect of the change of $C$ from $c ^ { - }$ to $c ^ { + }$ on $E = e ^ { + }$ is given by\n\nThe physical meaning of $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } )$ is the expected π ,change in decisions (in term of the probability of $E = e ^ { + }$ ) of individuals from protected group $c ^ { - }$ , if it is told that these individuals were from the other group $c ^ { + }$ and everything else remains unchanged. When applied to the example in Figure 1, it means the expected change in loan approval of applications actually from the disadvantage group (e.g., black), when the bank is instructed to treat the applicants as from the advantage group (e.g., white). Thus, the measurement of the $\\pi _ { d }$ -specific effect exactly follows the definition of direct πdiscrimination and is appropriate for measuring the discriminatory effect.\n\nSimilarly, we model indirect discrimination as the causal effect transmitted along all the indirect paths from $C$ to $E$ that contain the redlining attributes. Given the set of redlining attributes R, define $\\pi _ { i }$ as the path set that contains all the causal paths from $C$ to $E$ which pass through R, i.e., each of the paths includes at least one node in R. Then, the $\\pi _ { i }$ - specific effect, which is given by",
      "referring_paragraphs": [
        "Figure 2: An example with the recanting witness criterion satisfied."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1611.07509_page0_fig2.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg",
      "image_filename": "1611.07509_page0_fig3.jpg",
      "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
      "context_before": "[Section: Experiments]\n\nIn this section, we conduct experiments using two real datasets: the Adult dataset (Lichman 2013) and the Dutch Census of 2001 (Netherlands 2001). We compare our algorithms with the local massaging (LMSG) and local preferential sampling (LPS) algorithms proposed in (Zliobaitˇ e,˙ Kamiran, and Calders 2011) and disparate impact removal algorithm (DI) proposed in (Feldman et al. 2015; Adler et al. 2016). The causal networks are constructed and presented by utilizing an open-source software TETRAD (Glymour and others 2004). We employ the original PC algorithm (Spirtes, Glymour, and Scheines 2000) and set the significance threshold 0 01 for conditional independence testing in .causal network construction. The quadratic programming is solved using CVXOPT (Dahl and Vandenberghe 2006).",
      "context_after": "[Section: Discrimination Discovery]\n\nThe Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc. Since the computational complexity of the PC algorithm is an exponential function of the number of attributes and their domain sizes, for computational feasibility we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the first tier, edu level and marital status are defined in the second tier, and all other attributes are defined in the third tier. The causal network is shown in Figure 3. We treat sex as the protected attribute, income as the decision, and marital status as the redlining attribute. The green path represents the direct path from sex to income, and the blue paths represent the indirect paths passing through marital status. We set the discrimination threshold $\\tau$ as τ0.05. By computing the path-specific effects, we obtain that $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) = 0 . 0 2 5$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) = 0 . 1 7 5$ , which indiπ , . π , .cate no direct discrimination but significant indirect discrimination against females according to our criterion.\n\nThe Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the decision, and marital status as the redlining attribute. For this dataset, $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) =$ 0 220 and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 0 1$ π ,, indicating significant di-. π , .rect discrimination but no indirect discrimination against females.\n\n[Section: Discrimination Removal]",
      "referring_paragraphs": [
        "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
        "The causal network is shown in Figure 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1611.07509",
      "figure_id": "1611.07509_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg",
      "image_filename": "1611.07509_page0_fig4.jpg",
      "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.",
      "context_before": "The Adult dataset consists of 65123 tuples with 11 attributes such as age, education, sex, occupation, income, marital status etc. Since the computational complexity of the PC algorithm is an exponential function of the number of attributes and their domain sizes, for computational feasibility we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the first tier, edu level and marital status are defined in the second tier, and all other attributes are defined in the third tier. The causal network is shown in Figure 3. We treat sex as the protected attribute, income as the decision, and marital status as the redlining attribute. The green path represents the direct path from sex to income, and the blue paths represent the indirect paths passing through marital status. We set the discrimination threshold $\\tau$ as τ0.05. By computing the path-specific effects, we obtain that $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) = 0 . 0 2 5$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) = 0 . 1 7 5$ , which indiπ , . π , .cate no direct discrimination but significant indirect discrimination against females according to our criterion.\n\nThe Dutch dataset consists of 60421 tuples with 12 attributes. Similarly, we binarize the domain values of attribute age due to its large domain size. Three tiers are used in the partial order for temporal priority: sex, age, country birth are defined in the first tire, edu level and marital status are defined in the second tire, and all other attributes are defined in the third tire. The causal graph is shown in Figure 4. Similarly we treat sex as the protected attribute, occupation as the decision, and marital status as the redlining attribute. For this dataset, $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) =$ 0 220 and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 0 1$ π ,, indicating significant di-. π , .rect discrimination but no indirect discrimination against females.\n\n[Section: Discrimination Removal]\n\nWe run the removal algorithm PSE-DR to remove discrimination from the Adult and Dutch datasets. Then, we run the discovery algorithm PSE-DD to further examine whether discrimination is truly removed in the modified datasets. For the modified Adult dataset we have $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) = 0 . 0 1 3$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 4 9$ π , ., and for the modified Dutch π ,dataset we have $S E _ { \\pi _ { d } } ( c ^ { + } , c ^ { - } ) ~ = ~ 0 . 0 5 0$ and $S E _ { \\pi _ { i } } ( c ^ { + } , c ^ { - } ) =$ π , . π ,0 001. The results show that the modified datasets contain .no direct and indirect discrimination.\n\nDiscrimination in predictive models. We aim to examine whether the predictive models built from the modified dataset incur discrimination in decision making. We use the Adult dataset where indirect discrimination is detected, and divide the original dataset into the training and testing datasets. First, we remove discrimination from the training dataset to obtain the modified training dataset. Then, we build the predictive models from the modified training dataset, and use them to make predictive decisions over the testing data. Two classifiers, SVM and Decision Tree, are used for prediction with five-fold cross-validation. Finally, we run PSE-DD to examine whether the predictions for the testing data contain discrimination. We also examine the data utility $( \\chi ^ { 2 } )$ and the prediction accuracy.\n\nχThe results are shown in Table 1. As shown in the column “PSE-DD”, both the modified training data and the predictions for the testing data contain no direct and indirect discrimination. In addition, PSE-DD produces relatively small data utility loss in term of $\\chi ^ { 2 }$ and good prediction accuχracy. For comparison, we include algorithms from previous works: LMSG, LPS and DI. For LMSG and LPS, discrimination is not removed even from the training data, and hence also exists in the predictions. The $D I$ algorithm provides a parameter  to indicate the amount of discrimination to be λremoved, where $\\lambda = 0$ represents no modification and $\\lambda = 1$",
      "context_after": "[Section: Related Work]\n\nA number of techniques have been proposed to discover discrimination in the literature. Classification rule-based methods such as elift (Pedreshi, Ruggieri, and Turini 2008) and belift (Mancuhan and Clifton 2014) were proposed to represent certain discrimination patterns. (Luong, Ruggieri, and\n\nTurini 2011; Zhang, Wu, and Wu 2016b) dealt with the individual discrimination by finding a group of similar individuals. (Zliobait ˇ e, Kamiran, and Calders 2011) proposed ˙ conditional discrimination which considers some part of the discrimination may be explainable by certain attributes. None of these work explicitly identifies direct discrimination, indirect discrimination, and explainable effects. In (Bonchi et al. 2015), the authors proposed a framework based on the Suppes-Bayes causal network and developed several random-walk-based methods to detect different types of discrimination. However, the construction of the Suppes-Bayes causal network is impractical with the large number of attribute-value pairs. In addition, it is unclear how the number of random walks is related to practical discrimination metrics, e.g., the difference in positive decision rates.\n\nProposed methods for discrimination removal are either based on data preprocessing (Kamiran and Calders 2012; Zliobait ˇ e, Kamiran, and Calders 2011) or algorithm tweak- ˙ ing (Kamiran, Calders, and Pechenizkiy 2010; Calders and Verwer 2010; Kamishima, Akaho, and Sakuma 2011). In a recent work (Feldman et al. 2015), the authors first ensure no direct discrimination by completely removing the protected attribute $C$ from data, and then modify all the non-protected attributes to ensure that $C$ cannot be predicted from the nonprotected attributes. As a result, indirect discrimination is removed since the decision $E$ has no connection with $C$ via the non-protected attributes. However, as shown in our experiment results, this approach cannot ensure that predictions made by the classifier built on the modified data do not contain discrimination. In addition, it suffers significant utility loss as it removes all the connections between $C$ and $E$ .\n\n[Section: Conclusions]",
      "referring_paragraphs": [
        "The causal graph is shown in Figure 4.",
        "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    }
  ],
  "1701.08230": [
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig0.jpg",
      "image_filename": "1701.08230_page0_fig0.jpg",
      "caption": "",
      "context_before": "As shown above, the optimal algorithms under past notions of fairness dier from the unconstrained solution.9 Consequently, satisfying common denitions of fairness means one must in theory sacrice some degree of public safety. We turn next to the question of how great this public safety loss might be in practice.\n\nWe use data from Broward County, Florida originally compiled by ProPublica [30]. Following their analysis, we only consider black and white defendants who were assigned COMPAS risk scores within 30 days of their arrest, and were not arrested for an ordinary trac crime. We further restrict to only those defendants who spent at least two years (aer their COMPAS evaluation) outside a correctional facility without being arrested for a violent crime, or were arrested for a violent crime within this two-year period. Following standard practice, we use this two-year violent recidivism metric to approximate the benet $y _ { i }$ of detention: we set $y _ { i } = 1$ for those who reoended, and $y _ { i } = 0$ i i for those who did not. For the yi3,377 defendants satisfying these criteria, the dataset includes race, age, sex, number of prior convictions, and COMPAS violent crime risk score (a discrete score between 1 and 10).\n\ne COMPAS scores may not be the most accurate estimates of risk, both because the scores are discretized and because they are not trained specically for Broward County. erefore, to estimate ${ p / X | X }$ we re-train a risk assessment model that predicts two-year Y Xviolent recidivism using $L ^ { 1 }$ -regularized logistic regression followed Lby Pla scaling [35]. e model is based on all available features for each defendant, excluding race. Our risk scores achieve higher AUC on a held-out set of defendants than the COMPAS scores (0.75 vs. 0.73). We note that adding race to this model does not improve performance, as measured by AUC on the test set.\n\nWe investigate the three past fairness denitions previously discussed: statistical parity, conditional statistical parity, and predictive equality. For each denition, we nd the set of thresholds that produce a decision rule that: (1) satises the fairness denition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public\n\nTable 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety. For each fairness constraint, we estimate the increase in violent crime committed by released defendants, relative to a rule that optimizes for public safety alone; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety).   \n\n<table><tr><td>Constraint</td><td>Percent of detainees that are low risk</td><td>Estimated increase in violent crime</td></tr><tr><td>Statistical parity</td><td>17%</td><td>9%</td></tr><tr><td>Predictive equality</td><td>14%</td><td>7%</td></tr><tr><td>Cond. stat. parity</td><td>10%</td><td>4%</td></tr></table>\n\nsafety subject to (1) and (2). e proportion of defendants detained is chosen to match the fraction of defendants classied as medium or high risk by COMPAS (scoring 5 or greater). Conditional statistical parity requires that one dene the “legitimate” factors $\\ell ( X )$ , and ` Xthis choice signicantly impacts results. For example, if all variables are deemed legitimate, then this fairness condition imposes no constraint on the algorithm. In our application, we consider only a defendant’s number of prior convictions to be legitimate; to deal with sparsity in the data, we partition prior convictions into four bins: 0, 1–2, 3–4, and 5 or more.\n\nWe estimate two quantities for each decision rule: the increase in violent crime commied by released defendants, relative to a rule that optimizes for public safety alone, ignoring formal fairness requirements; and the proportion of detained defendants that are low risk (i.e., would be released if we again considered only public safety). We compute these numbers on 100 random train-test splits of the data. On each iteration, we train the risk score model and nd the optimal thresholds using $7 0 \\%$ of the data, and then calculate the two statistics on the remaining $3 0 \\%$ . Ties are broken randomly when they occur, and we report results averaged over all runs.\n\nFor each fairness constraint, Table 1 shows that violent recidivism increases while low risk defendants are detained. For example, when we enforce statistical parity, $1 7 \\%$ of detained defendants are relatively low risk. An equal number of high-risk defendants are thus released (because we hold xed the number of individuals detained), leading to an estimated $9 \\%$ increase in violent recidivism among released defendants. ere are thus tangible costs to satisfying popular notions of algorithmic fairness.\n\n[Section: 5 THE COST OF PUBLIC SAFETY]\n\nA decision rule constrained to satisfy statistical parity, conditional statistical parity, or predictive equality reduces public safety. However, a single-threshold rule that maximizes public safety generally violates all of these fairness denitions. For example, in the Broward County data, optimally detaining $3 0 \\%$ of defendants with a singlethreshold rule means that $4 0 \\%$ of black defendants are detained, compared to $1 8 \\%$ of white defendants, violating statistical parity. And among defendants who ultimately do not go on to commit a violent crime, $1 4 \\%$ of whites are detained compared to $3 2 \\%$ of blacks, violating predictive equality.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {}
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
      "image_filename": "1701.08230_page0_fig1.jpg",
      "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For each denition, we nd the set of thresholds that produce a decision rule that: (1) satises the fairness denition; (2) detains $3 0 \\%$ of defendants; and (3) maximizes expected public\n\nTable 1: Based on the Broward County data, satisfying common fairness denitions results in detaining low-risk defendants while reducing public safety.",
        "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right).",
        "ough intuitively appealing, detention rates and false positive rates are poor proxies for the thresholds: these infra-marginal statistics consider average risk above the thresholds, and so can dier even if the thresholds are identical (as shown in Figure 1)."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1701.08230_page0_fig2.jpg",
        "1701.08230_page0_fig3.jpg"
      ],
      "quality_score": 0.6,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig3.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg",
      "image_filename": "1701.08230_page0_fig4.jpg",
      "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.",
      "context_before": "",
      "context_after": "[Section: 6 DETECTING DISCRIMINATION]\n\ne algorithms we have thus far considered output a decision $d ( x )$ d xfor each individual. In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant’s risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).\n\nIn some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory. To address this concern, researchers oen examine whether scores are calibrated [29], as dened by Eq. (4).10 Since the true probabilities ${ p / X | X }$ are necessarily calipY Xbrated, it is reasonable to expect risk estimates that approximate these probabilities to be calibrated as well. Figure 2 shows that the COMPAS scores indeed satisfy this property. For example, among defendants who scored a seven on the COMPAS scale, $6 0 \\%$ of white defendants reoended, which is nearly identical to the $6 1 \\%$ percent of black defendants who reoended.\n\nHowever, given only scores $s ( x )$ and outcomes $y$ , it is impossible s x yto determine whether the scores are accurate estimates of ${ p / X | X }$ Y Xor have been strategically designed to produce racial disparities.\n\nHardt et al. [22] make a similar observation in their discussion of “oblivious” measures. Consider a hypothetical situation where a malicious decision maker wants to release all white defendants, even if they are high risk. To shield himself from claims of discrimination, he applies a facially neutral $3 0 \\%$ threshold to defendants regardless of race. Suppose that $2 0 \\%$ of blacks recidivate, and the decision-maker’s algorithm uses additional information, such as prior arrests, to partition blacks into three risk categories: low risk ( $1 0 \\%$ chance of reoending), average risk $2 0 \\%$ chance), and high risk ( $4 0 \\%$ chance). Further suppose that whites are just as risky as blacks overall ( $2 0 \\%$ of them reoend), but the decision maker ignores individual characteristics and labels every white defendant average risk. is algorithm is calibrated, as both whites and blacks labeled average risk reoend $2 0 \\%$ of the time. However, all white defendants fall below the decision threshold, so none are detained. By systematically ignoring information that could be used to distinguish between white defendants, the decision maker has succeeded in discriminating while using a single threshold applied to calibrated scores.\n\nFigure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates. We start by adding noise to the true scores (black curve) of the group that we wish to treat favorably—in the gure we use $\\mathrm { N } ( 0 , 0 . 5 )$ noise. We then , .use the perturbed scores to predict the outcomes $y _ { i }$ via a logistic yiregression model. e resulting model predictions (red curve) are more tightly clustered around their mean, since adding noise removes information. Consequently, under the transformed scores, no one in the group lies above the decision threshold, indicated by the vertical line. e key point is that the red curve is a perfectly plausible distribution of risk: without further information, one cannot determine whether the risk model was t on input data that were truly noisy, or whether noise was added to the inputs to produce disparities.\n\nese examples relate to the historical practice of redlining, in which lending decisions were intentionally based only on coarse information—usually neighborhood—in order to deny loans to wellqualied minorities [11]. Since even creditworthy minorities oen resided in neighborhoods with low average income, lenders could deny their applications by adhering to a facially neutral policy of not serving low-income areas. In the case of redlining, one discriminates by ignoring information about the disfavored group; in the pretrial seing, one ignores information about the favored group. Both strategies, however, operate under the same general principle.\n\nere is no evidence to suggest that organizations have intentionally ignored relevant information when constructing risk scores. Similar eects, however, may also arise through negligence or unintentional oversights. Indeed, we found in Section 4 that we could improve the predictive power of the Broward County COMPAS scores with a standard statistical model. To ensure an algorithm is equitable, it is thus important to inspect the algorithm itself and not just the decisions it produces.\n\n[Section: 7 DISCUSSION]",
      "referring_paragraphs": [
        "Figure 2: Recidivism rate by COMPAS risk score and race.",
        "Figure 2 shows that the COMPAS scores indeed satisfy this property."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1701.08230",
      "figure_id": "1701.08230_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg",
      "image_filename": "1701.08230_page0_fig5.jpg",
      "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).",
      "context_before": "e algorithms we have thus far considered output a decision $d ( x )$ d xfor each individual. In practice, however, algorithms like COMPAS typically output a score $s ( x )$ that is claimed to indicate a defendant’s risk ${ p / X | X }$ ; decision makers then use these risk estimates to select Y Xan action (e.g., release or detain).\n\nIn some cases, neither the procedure nor the data used to generate these scores is disclosed, prompting worry that the scores are themselves discriminatory. To address this concern, researchers oen examine whether scores are calibrated [29], as dened by Eq. (4).10 Since the true probabilities ${ p / X | X }$ are necessarily calipY Xbrated, it is reasonable to expect risk estimates that approximate these probabilities to be calibrated as well. Figure 2 shows that the COMPAS scores indeed satisfy this property. For example, among defendants who scored a seven on the COMPAS scale, $6 0 \\%$ of white defendants reoended, which is nearly identical to the $6 1 \\%$ percent of black defendants who reoended.\n\nHowever, given only scores $s ( x )$ and outcomes $y$ , it is impossible s x yto determine whether the scores are accurate estimates of ${ p / X | X }$ Y Xor have been strategically designed to produce racial disparities.\n\nHardt et al. [22] make a similar observation in their discussion of “oblivious” measures. Consider a hypothetical situation where a malicious decision maker wants to release all white defendants, even if they are high risk. To shield himself from claims of discrimination, he applies a facially neutral $3 0 \\%$ threshold to defendants regardless of race. Suppose that $2 0 \\%$ of blacks recidivate, and the decision-maker’s algorithm uses additional information, such as prior arrests, to partition blacks into three risk categories: low risk ( $1 0 \\%$ chance of reoending), average risk $2 0 \\%$ chance), and high risk ( $4 0 \\%$ chance). Further suppose that whites are just as risky as blacks overall ( $2 0 \\%$ of them reoend), but the decision maker ignores individual characteristics and labels every white defendant average risk. is algorithm is calibrated, as both whites and blacks labeled average risk reoend $2 0 \\%$ of the time. However, all white defendants fall below the decision threshold, so none are detained. By systematically ignoring information that could be used to distinguish between white defendants, the decision maker has succeeded in discriminating while using a single threshold applied to calibrated scores.\n\nFigure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates. We start by adding noise to the true scores (black curve) of the group that we wish to treat favorably—in the gure we use $\\mathrm { N } ( 0 , 0 . 5 )$ noise. We then , .use the perturbed scores to predict the outcomes $y _ { i }$ via a logistic yiregression model. e resulting model predictions (red curve) are more tightly clustered around their mean, since adding noise removes information. Consequently, under the transformed scores, no one in the group lies above the decision threshold, indicated by the vertical line. e key point is that the red curve is a perfectly plausible distribution of risk: without further information, one cannot determine whether the risk model was t on input data that were truly noisy, or whether noise was added to the inputs to produce disparities.\n\nese examples relate to the historical practice of redlining, in which lending decisions were intentionally based only on coarse information—usually neighborhood—in order to deny loans to wellqualied minorities [11]. Since even creditworthy minorities oen resided in neighborhoods with low average income, lenders could deny their applications by adhering to a facially neutral policy of not serving low-income areas. In the case of redlining, one discriminates by ignoring information about the disfavored group; in the pretrial seing, one ignores information about the favored group. Both strategies, however, operate under the same general principle.\n\nere is no evidence to suggest that organizations have intentionally ignored relevant information when constructing risk scores. Similar eects, however, may also arise through negligence or unintentional oversights. Indeed, we found in Section 4 that we could improve the predictive power of the Broward County COMPAS scores with a standard statistical model. To ensure an algorithm is equitable, it is thus important to inspect the algorithm itself and not just the decisions it produces.\n\n[Section: 7 DISCUSSION]\n\nMaximizing public safety requires detaining all individuals deemed suciently likely to commit a violent crime, regardless of race.",
      "context_after": "[Section: ACKNOWLEDGMENTS]\n\nWe thank Imanol Arrieta Ibarra, Angele Christin, Alexander Gelber, ` Andrew Gelman, Jan Overgoor, Ravi Shro, and Jennifer Skeem for their helpful comments. is work was supported in part by the John S. and James L. Knight Foundation, and by the Hellman Fellows Fund. Data and code to reproduce our results are available at hps://github.com/5harad/cost-of-fairness.\n\n[Section: REFERENCES]",
      "referring_paragraphs": [
        "Figure 3 illustrates a general method for constructing such discriminatory scores from true risk estimates.",
        "Figure 3: Calibration is insucient to assess discrimination."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1701.08230_page0_fig6.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig6.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1703.06856": [
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig0.jpg",
      "image_filename": "1703.06856_page0_fig0.jpg",
      "caption": "",
      "context_before": "[Section: 3 Counterfactual Fairness]\n\nGiven a predictive problem with fairness considerations, where $A$ , $X$ and $Y$ represent the protected attributes, remaining attributes, and output of interest respectively, let us assume that we are given a causal model $( U , V , F )$ , where $V \\equiv A \\cup X$ . We postulate the following criterion for predictors of $Y$ .\n\nDefinition 5 (Counterfactual fairness). Predictor $\\hat { Y }$ is counterfactually fair if under any context $X = x$ and $A = a$ ,\n\nfor all y and for any value $a ^ { \\prime }$ attainable by $A$ .\n\nThis notion is closely related to actual causes [13], or token causality in the sense that, to be fair, $A$ should not be a cause of $\\hat { Y }$ in any individual instance. In other words, changing $A$ while holding things which are not causally dependent on $A$ constant will not change the distribution of $\\hat { Y }$ . We also emphasize that counterfactual fairness is an individual-level definition. This is substantially different from comparing different individuals that happen to share the same “treatment” $A = a$ and coincide on the values of $X$ , as discussed in Section 4.3.1 of [29] and the Supplementary Material. Differences between $X _ { a }$ and $X _ { a ^ { \\prime } }$ must be caused by variations on $A$ only. Notice also that this definition is agnostic with respect to how good a predictor $\\hat { Y }$ is, which we discuss in Section 4.\n\nRelation to individual fairness. IF is agnostic with respect to its notion of similarity metric, which is both a strength (generality) and a weakness (no unified way of defining similarity). Counterfactuals and similarities are related, as in the classical notion of distances between “worlds” corresponding to different counterfactuals [23]. If $\\hat { Y }$ is a deterministic function of $W \\subset A \\cup X \\cup U$ , as in several of",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)"
      ],
      "quality_score": 0.25,
      "metadata": {}
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg",
      "image_filename": "1703.06856_page0_fig1.jpg",
      "caption": "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios. See Section 3.1 for discussion. (c) The graph corresponding to a causal model with $A$ being the protected attribute and $Y$ some outcome of interest, with background variables assumed to be independent. (d) Expanding the model to include an intermediate variable indicating whether the individual is employed with two (latent) background variables Prejudiced (if the person offering the job is prejudiced) and Qualifications (a measure of the individual’s qualifications). (e) A twin network representation of this system [28] under two different counterfactual levels for $A$ . This is created by copying nodes descending from $A$ , which inherit unaffected parents from the factual world.",
      "context_before": "",
      "context_after": "[Section: 3.1 Examples]\n\nTo provide an intuition for counterfactual fairness, we will consider two real-world fair prediction scenarios: insurance pricing and crime prediction. Each of these correspond to one of the two causal graphs in Figure 1(a),(b). The Supplementary Material provides a more mathematical discussion of these examples with more detailed insights.\n\nScenario 1: The Red Car. A car insurance company wishes to price insurance for car owners by predicting their accident rate $Y$ . They assume there is an unobserved factor corresponding to aggressive driving $U$ , that (a) causes drivers to be more likely have an accident, and (b) causes individuals to prefer red cars (the observed variable $X$ ). Moreover, individuals belonging to a certain race $A$ are more likely to drive red cars. However, these individuals are no more likely to be aggressive or to get in accidents than any one else. We show this in Figure 1(a). Thus, using the red car feature $X$ to predict accident rate $Y$ would seem to be an unfair prediction because it may charge individuals of a certain race more than others, even though no race is more likely to have an accident. Counterfactual fairness agrees with this notion: changing $A$ while holding $U$ fixed will also change $X$ and, consequently, $\\hat { Y }$ . Interestingly, we can show (Supplementary Material) that in a linear model, regressing $Y$ on $A$ and $X$ is equivalent to regressing on $U$ , so off-the-shelf regression here is counterfactually fair. Regressing $Y$ on $X$ alone obeys the FTU criterion but is not counterfactually fair, so omitting A (FTU) may introduce unfairness into an otherwise fair world.\n\nScenario 2: High Crime Regions. A city government wants to estimate crime rates by neighborhood to allocate policing resources. Its analyst constructed training data by merging (1) a registry of residents containing their neighborhood $X$ and race $A$ , with (2) police records of arrests, giving each resident a binary label with $Y = 1$ indicating a criminal arrest record. Due to historically segregated housing, the location $X$ depends on $A$ . Locations $X$ with more police resources have larger numbers of arrests $Y$ . And finally, $U$ represents the totality of socioeconomic factors and policing practices that both influence where an individual may live and how likely they are to be arrested and charged. This can all be seen in Figure 1(b).\n\nIn this example, higher observed arrest rates in some neighborhoods are due to greater policing there, not because people of different races are any more or less likely to break the law. The label $Y = 0$\n\ndoes not mean someone has never committed a crime, but rather that they have not been caught. If individuals in the training data have not already had equal opportunity, algorithms enforcing EO will not remedy such unfairness. In contrast, a counterfactually fair approach would model differential enforcement rates using $U$ and base predictions on this information rather than on $X$ directly.\n\nIn general, we need a multistage procedure in which we first derive latent variables $U$ , and then based on them we minimize some loss with respect to $Y$ . This is the core of the algorithm discussed next.\n\n[Section: 3.2 Implications]",
      "referring_paragraphs": [
        "Figure 1: (a), (b) Two causal models for different real-world fair prediction scenarios.",
        "Each of these correspond to one of the two causal graphs in Figure 1(a),(b).",
        "Consider the DAG $A  Y$ , shown in Figure 1(c) with the explicit inclusion of set $U$ of independent background variables.",
        "This model is shown\n\nTable 1: Prediction results using logistic regression.",
        "We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1.",
        "Consider a linear model with the structure in Figure 1(a).",
        "□\n\nNote that if Figure 1(a) is the true model for the real world then ${ \\hat { Y } } ( X , A )$ will also satisfy demographic parity and equality of opportunity as $\\hat { Y }$ will be unaffected by $A$ .",
        "A contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figure 1(d), where many of the mediators themselves are considered to be unfairly affected by the protected attribute, and independence constraints among counterfactuals alone are less likely to be useful in identifying constraints for the fitting of a fair predictor."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(b)",
        "1703.06856_page0_fig2.jpg",
        "(e)",
        "1703.06856_page0_fig3.jpg",
        "1703.06856_page0_fig4.jpg",
        "1703.06856_page0_fig5.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig5.jpg"
        ],
        "panel_count": 5
      }
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_7",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig6.jpg",
      "image_filename": "1703.06856_page0_fig6.jpg",
      "caption": "Figure 2: Left: A causal model for the problem of predicting law school success fairly. Right: Density plots of predicted $\\mathrm { F Y A } _ { a }$ and $\\mathrm { F Y A } _ { a ^ { \\prime } }$ .",
      "context_before": "",
      "context_after": "We estimate the error terms $\\epsilon _ { G } , \\epsilon _ { L }$ by first fitting two models that each use race and sex to individually predict GPA and LSAT. We then compute the residuals of each model (e.g., $\\epsilon _ { G } = \\mathrm { G P A } - \\hat { Y } _ { \\mathrm { G P A } } ( R , S ) )$ . We use these residual estimates of $\\epsilon _ { G } , \\epsilon _ { L }$ to predict FYA. We call this Fair Add.\n\nAccuracy. We compare the RMSE achieved by logistic regression for each of the models on the test set in Table 1. The Full model achieves the lowest RMSE as it uses race and sex to more accurately reconstruct FYA. Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3. The (also unfair) Unaware model still uses the unfair variables GPA and LSAT, but because it does not use race and sex it cannot match the RMSE of the Full model. As our models satisfy counterfactual fairness, they trade off some accuracy. Our first model Fair $K$ uses weaker assumptions and thus the RMSE is highest. Using the Level 3 assumptions, as in Fair Add we produce a counterfactually fair model that trades slightly stronger assumptions for lower RMSE.\n\nCounterfactual fairness. We would like to empirically test whether the baseline methods are counterfactually fair. To do so we will assume the true model of the world is given by Figure 2, (Level 2). We can fit the parameters of this model using the observed data and evaluate counterfactual fairness by sampling from it. Specifically, we will generate samples from the model given either the observed race and sex, or counterfactual race and sex variables. We will fit models to both the original and counterfactual sampled data and plot how the distribution of predicted FYA changes for both baseline models. Figure 2 shows this, where each row corresponds to a baseline predictor and each column corresponds to the counterfactual change. In each plot, the blue distribution is density of predicted FYA for the original data and the red distribution is this density for the counterfactual data. If a model is counterfactually fair we would expect these distributions to lie exactly on top of each other. Instead, we note that the Full model exhibits counterfactual unfairness for all counterfactuals except sex. We see a similar trend for the Unaware model, although it is closer to being counterfactually fair. To see why these models seem to be fair w.r.t. to sex we can look at weights of the DAG which generates the counterfactual data. Specifically the DAG weights from (male,female) to GPA are (0.93,1.06) and from (male,female) to LSAT are (1.1,1.1). Thus, these models are fair w.r.t. to sex simply because of a very weak causal link between sex and GPA/LSAT.\n\n[Section: 6 Conclusion]\n\nWe have presented a new model of fairness we refer to as counterfactual fairness. It allows us to propose algorithms that, rather than simply ignoring protected attributes, are able to take into account the different social biases that may arise towards individuals based on ethically sensitive attributes and compensate for these biases effectively. We experimentally contrasted our approach with previous fairness approaches and show that our explicit causal models capture these social biases and make clear the implicit trade-off between prediction accuracy and fairness in an unfair world. We propose that fairness should be regulated by explicitly modeling the causal structure of the world. Criteria based purely on probabilistic independence cannot satisfy this and are unable to address how unfairness is occurring in the task at hand. By providing such causal tools for addressing fairness questions we hope we can provide practitioners with customized techniques for solving a wide array of fairness modeling problems.",
      "referring_paragraphs": [
        "The causal graph corresponding to this model is shown in Figure 2, (Level 2).",
        "Figure 2: Left: A causal model for the problem of predicting law school success fairly.",
        "Note that in this case, this model is not fair even if the data was generated by one of the models shown in Figure 2 as it corresponds to Scenario 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_8",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg",
      "image_filename": "1703.06856_page0_fig7.jpg",
      "caption": "Figure 3: A causal model for the stop and frisk dataset.",
      "context_before": "for all y and for any value $a ^ { \\prime }$ attainable by A.\n\nThis notion is related to controlled direct effects [29], where we intervene on some paths from $A$ to $Y$ , but not others. Paths in $\\mathcal { P } _ { \\mathcal { G } _ { A } }$ are considered here to be the “direct” paths, and we condition on $X$ and $A$ similarly to the definition of probability of sufficiency (3). This definition is the same as the original counterfactual fairness definition for the case where $\\mathcal { P } _ { \\mathcal { G } _ { A } } ^ { c } = \\emptyset$ . Its interpretation is analogous to the original, indicating that for any $X _ { 0 } \\in X _ { \\mathcal { P } _ { \\mathcal { G } _ { A } } ^ { c } }$ we are allowed to propagate information from the factual assigment $A = a$ , along with what we learned about the background causes $U _ { X _ { 0 } }$ , in order to reconstruct $X _ { 0 }$ . The contribution of $A$ is considered acceptable in this case and does not need to be “deconvolved.” The implication is that any member of $X _ { { \\mathcal P } _ { \\mathcal G _ { A } } ^ { c } }$ can be included in the definition of $\\hat { Y }$ . In the example of college applications, we are allowed to use the choice of course $X$ even though $A$ is a confounder for $X$ and $Y$ . We are still not allowed to use $A$ directly, bypassing the background variables.\n\nAs discussed by [27], there are some counterfactual manipulations usable in a causal definition of fairness that can be performed by exploiting only independence constraints among the counterfactuals: that is, without requiring the explicit description of structural equations or other models for latent variables. A contrast between the two approaches is left for future work, although we stress that they are in some sense complementary: we are motivated mostly by problems such as the one in Figure 1(d), where many of the mediators themselves are considered to be unfairly affected by the protected attribute, and independence constraints among counterfactuals alone are less likely to be useful in identifying constraints for the fitting of a fair predictor.\n\n[Section: S5 The Multifaceted Dynamics of Fairness]\n\nOne particularly interesting question was raised by one of the reviewers: what is the effect of continuing discrimination after fair decisions are made? For instance, consider the case where banks enforce a fair allocation of loans for business owners regardless of, say, gender. This does not mean such businesses will thrive at a balanced rate if customers continue to avoid female owned business at a disproportionate rate for unfair reasons. Is there anything useful that can be said about this issue from a causal perspective?\n\nThe work here proposed regards only what we can influence by changing how machine learningaided decision making takes place at specific problems. It cannot change directly how society as a whole carry on with their biases. Ironically, it may sound unfair to banks to enforce the allocation of resources to businesses at a rate that does not correspond to the probability of their respective success, even if the owners of the corresponding businesses are not to be blamed by that. One way of conciliating the different perspectives is by modeling how a fair allocation of loans, even if it does not come without a cost, can nevertheless increase the proportion of successful female businesses",
      "context_after": "[Section: S6 Case Study: NYC Stop-and-Frisk Data]\n\nSince 2002, the New York Police Department (NYPD) has recorded information about every time a police officer has stopped someone. The officer records information such as if the person was searched or frisked, if a weapon was found, their appearance, whether an arrest was made or a summons issued, if force was used, etc. We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking at just males stopped as this accounts for more than $9 0 \\%$ of the data. We fit a model which postulates that police interactions is caused by race and a single latent factor labeled Criminality that is meant to index other aspects of the individual that have been used by the police and which are independent of race. We do not claim that this model has a solid theoretical basis, we use it below as an illustration on how to carry on an analysis of counterfactually fair decisions. We also describe a spatial analysis of the estimated latent factors.\n\nModel. We model this stop-and-frisk data using the graph in Figure 3. Specifically, we posit main causes for the observations: Arrest (if an individual was arrested), Force (some sort of force was used during the stop), Frisked, and Searched. The first cause of these observations is some measure of an individual’s latent Criminality, which we do not observe. We believe that Criminality also directly affects Weapon (an individual was found to be carrying a weapon). For all of the features previously mentioned we believe there is an additional cause, an individual’s Race which we do observe. This factor is introduced as we believe that these observations may be biased based on an officer’s perception of whether an individual is likely a criminal or not, affected by an individual’s Race. Thus note that, in this model, Criminality is counterfactually fair for the prediction of any characteristic of the individual for problems where Race is a protected attribute.\n\nVisualization on a map of New York City. Each of the stops can be mapped to longitude and latitude points for where the stop occurred7. This allows us to visualize the distribution of two distinct populations: the stops of White and Black Hispanic individuals, shown in Figure 4. We note that there are more White individuals stopped (4492) than Black Hispanic individuals (2414). However, if we look at the arrest distribution (visualized geographically in the second plot) the rate of arrest for White individuals is lower $( 1 2 . 1 \\% )$ than for Black Hispanic individuals $( 1 9 . 8 \\%$ , the highest rate for any race in the dataset). Given our model we can ask: “If every individual had been White,",
      "referring_paragraphs": [
        "Figure 3: A causal model for the stop and frisk dataset.",
        "We model this stop-and-frisk data using the graph in Figure 3."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1703.06856",
      "figure_id": "1703.06856_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg",
      "image_filename": "1703.06856_page0_fig8.jpg",
      "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.",
      "context_before": "[Section: S6 Case Study: NYC Stop-and-Frisk Data]\n\nSince 2002, the New York Police Department (NYPD) has recorded information about every time a police officer has stopped someone. The officer records information such as if the person was searched or frisked, if a weapon was found, their appearance, whether an arrest was made or a summons issued, if force was used, etc. We consider the data collected on males stopped during 2014 which constitutes 38,609 records. We limit our analysis to looking at just males stopped as this accounts for more than $9 0 \\%$ of the data. We fit a model which postulates that police interactions is caused by race and a single latent factor labeled Criminality that is meant to index other aspects of the individual that have been used by the police and which are independent of race. We do not claim that this model has a solid theoretical basis, we use it below as an illustration on how to carry on an analysis of counterfactually fair decisions. We also describe a spatial analysis of the estimated latent factors.\n\nModel. We model this stop-and-frisk data using the graph in Figure 3. Specifically, we posit main causes for the observations: Arrest (if an individual was arrested), Force (some sort of force was used during the stop), Frisked, and Searched. The first cause of these observations is some measure of an individual’s latent Criminality, which we do not observe. We believe that Criminality also directly affects Weapon (an individual was found to be carrying a weapon). For all of the features previously mentioned we believe there is an additional cause, an individual’s Race which we do observe. This factor is introduced as we believe that these observations may be biased based on an officer’s perception of whether an individual is likely a criminal or not, affected by an individual’s Race. Thus note that, in this model, Criminality is counterfactually fair for the prediction of any characteristic of the individual for problems where Race is a protected attribute.\n\nVisualization on a map of New York City. Each of the stops can be mapped to longitude and latitude points for where the stop occurred7. This allows us to visualize the distribution of two distinct populations: the stops of White and Black Hispanic individuals, shown in Figure 4. We note that there are more White individuals stopped (4492) than Black Hispanic individuals (2414). However, if we look at the arrest distribution (visualized geographically in the second plot) the rate of arrest for White individuals is lower $( 1 2 . 1 \\% )$ than for Black Hispanic individuals $( 1 9 . 8 \\%$ , the highest rate for any race in the dataset). Given our model we can ask: “If every individual had been White,",
      "context_after": "",
      "referring_paragraphs": [
        "This allows us to visualize the distribution of two distinct populations: the stops of White and Black Hispanic individuals, shown in Figure 4.",
        "Figure 4: How race affects arrest."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    }
  ],
  "1705.10378": [
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
      "image_filename": "1705.10378_page0_fig0.jpg",
      "caption": "Figure 1: (a) A causal graph with a single mediator. (b) A causal graph with two mediators, one confounded with the outcome via an unobserved common cause. (c) A causal graph with a single mediator where the natural direct effect is not identified.",
      "context_before": "Causal relationships are often represented by graphical causal models (Spirtes, Glymour, and Scheines 2001; Pearl 2009). Such models generalize independence models on directed acyclic graphs, also known as Bayesian networks (Pearl 1988), to also encode conditional independence statements on counterfactual random variables (Richardson and Robins 2013). In such graphs, vertices represent observed random variables, and absence of directed edges represents absence of direct causal relationships. As an example, in Fig. 1 (a), $C$ is potentially a direct cause of $A$ , while $M$ mediates a part of the causal influence of $A$ on $Y$ , represented by all directed paths from $A$ to $Y$ .\n\n[Section: Mediation Analysis]\n\nA natural step in causal inference is understanding the mechanism by which $A$ influences $Y$ . A simple form of understanding mechanisms is via mediation analysis, where the causal influence of $A$ on $Y$ , as quantified by the ACE,",
      "context_after": "which may be estimated by plug in estimators, or other methods (Tchetgen and Shpitser 2012a).\n\n[Section: Path-Specific Effects]\n\nIn general, we may be interested in decomposing the ACE into effects along particular causal pathways. For example in Fig. 1 (b), we may wish to decompose the effect of $A$ on $Y$ into the contribution of the path $A  W  Y$ , and the path bundle $A  Y$ and $A \\to M \\to W \\to Y .$ Effects along paths, such as an effect along the path $A  W  Y$ , are known as path-specific effects (Pearl 2001). Just as the NDE and NIE, path-specific effects (PSEs) can be formulated as nested counterfactuals (Shpitser 2013). The general idea is that along the pathways of interest, variables behave as if the treatment variable $A$ were set to the “active value” $a$ , and along other pathways, variables behave as if the treatment variable $A$ were set to the “baseline value” $a ^ { \\prime }$ , thus “turning the treatment off.” Using this scheme, the path-specific effect of $A$ on $Y$ along the path $A  W  Y$ , on the mean difference scale, can be formulated as",
      "referring_paragraphs": [
        "Figure 1: (a) A causal graph with a single mediator."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1705.10378_page0_fig1.jpg",
        "(c)",
        "1705.10378_page0_fig2.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1705.10378",
      "figure_id": "1705.10378_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg",
      "image_filename": "1705.10378_page0_fig3.jpg",
      "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
      "context_before": "We first illustrate our approach to fair inference via two datasets: the COMPAS dataset (Angwin et al. 2016) and the Adult dataset (Lichman 2013). We also illustrate how a part of the model involving the outcome $Y$ may be regularized without compromising fair inferences if the NDE quantifying discrimination is estimated using methods that are robust to misspecification of the $Y$ model.\n\n[Section: The COMPAS Dataset]\n\nCorrectional Offender Management Profiling for Alternative Sanctions, or COMPAS, is a risk assessment tool, created by the company Northpointe, that is being used across the US to determine whether to release or detain a defendant before his or her trial. Each pretrial defendant receives several COMPAS scores based on factors including but not limited to demographics, criminal history, family history, and social status. Among these scores, we are primarily interested in “Risk of Recidivism\". Propublica (Angwin et al. 2016) has obtained two years worth of COMPAS scores from the Broward County Sheriff’s Office in Florida that contains scores for over 11000 people who were assessed at the pretrial stage and scored in 2013 and 2014. COMPAS score for each defendant ranges from 1 to 10, with 10 being the highest risk. Besides the COMPAS score, the data also includes records on defendant’s age, gender, race, prior convictions, and whether or not recidivism occurred in a span of two years. We limited our attention to the cohort consisting of African-Americans and Caucasians.\n\nWe are interested in predicting whether a defendant would reoffend using the COMPAS data. For illustration, we assume the use of prior convictions, possibly influenced by race, is fair for determining recidivism. Thus, we defined discrimination as effect along the direct path from race to the recidivism prediction outcome. The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism. The “disallowed\" path in this problem is drawn in green in Figure 2(a). The effect along this path is the NDE. The objective is to learn a fair model for $Y$ . i.e. a model where NDE is minimized.\n\nWe obtained the posterior sample representation of $\\mathbb { E } [ Y | A , M , \\mathbf { C } ]$ via both regular and constrained BART. Under the unconstrained posterior, the NDE (on the odds ratio scale) was equal to 1.3. This number is interpreted to mean that the odds of recidivism would have been 1.3 times higher",
      "context_after": "[Section: The Adult Dataset]\n\nThe “adult” dataset from the UCI repository has records on 14 attributes such as demographic information, level of education, and job related variables such as occupation and work class on 48842 instances along with their income that is recorded as a binary variable denoting whether individuals have income above or below $5 0 k$ – high vs low income. The objective is to learn a statistical model that predicts the class of income for a given individual. Suppose banks are interested in using this model to identify reliable candidates for loan application. Raw use of data might construct models that are biased towards females who are perceived to have lower income in general compared to males. The causal model for this dataset is drawn in Figure 2(b). Gender is the sensitive variable in this example denoted by $A$ in figure 2(b) and income class is denoted by Y . M denotes the marital\n\nstatus, $L$ denotes the level of education, and R consists of three variables, occupation, hours per week, and work class. The baseline variables including age and nationality are collected in C. $U _ { 1 }$ and $U _ { 2 }$ capture the unobserved confounders between $M , Y$ and $L , \\mathbf { R }$ , respectively.\n\nHere, besides the direct effect $A  Y$ ), we would like to remove the effect of gender on income through marital status $( A \\to M \\to \\ldots \\to Y )$ ). The “disallowed\" paths are drawn in green in Figure 2(b). The PSE along the green paths is identifiable via the recanting district criterion in (Shpitser 2013), and can be computed by calculating odds ratio or contrast comparison of the counterfactual variable $Y ( a , M ( a ) , L ( a ^ { \\prime } , M ( a ) ) , { \\bf R } ( a ^ { \\prime } , M ( a ) , L ( a ^ { \\prime } , M ( a ) ) ) , { \\bf C } )$ , where $a ^ { \\prime }$ is set to a baseline value, $a \\ = \\ 1$ in one counterfactual, and $ { \\boldsymbol { a } } \\ = \\  { \\boldsymbol { 0 } }$ in the other. The counterfactual distribution can be estimated from the following functional:\n\nIf we use logistic regression to model $Y$ and linear regression to model other variables given their past, and compute the PSE on the odds ratio scale, it is straightforward to show that the PSE simplifies to $\\exp \\left( \\theta _ { a } ^ { y } + \\theta _ { m } ^ { \\bar { y } } \\theta _ { a } ^ { m } + \\theta _ { l } ^ { y } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m } + \\right.$ $\\begin{array} { r } { \\sum _ { i } \\theta _ { r _ { i } } ^ { y } ( \\theta _ { m } ^ { r _ { i } } \\theta _ { a } ^ { m } + \\theta _ { l } ^ { r _ { i } } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m } ) ) } \\end{array}$ , where $\\theta _ { i } ^ { j }$ denotes the coefficient associated with variable $i$ in modeling the variable $j$ , (VanderWeele and Vansteelandt 2010). Therefore, the constraint in (4) is an easy function to compute, and the resulting constrained optimization problem relatively easy to solve.\n\nWe trained two models for $Y$ , one by maximizing the constrained likelihood in (4) using the R package nloptr, and the other by using the full model with no constrain. For performance evaluation on test set, we should use $\\mathbb { E } [ Y | A , \\mathbf { C } ]$ in constrained model and $\\mathbb { E } [ Y | A , M , L , \\mathbf { R } , \\mathbf { C } ]$ in unconstrained model.\n\nThe PSE in the unconstrained model is 3.16. This means, the odds of having a high income would have been more than 3 times higher for a female if her sex and marital status would have been the same as if she was a male. We solve the constrained problem by restricting the PSE, as estimated by (9), to lie between 0.95 and 1.05. Accuracy in the unconstrained model is $8 2 \\%$ , and drops to $7 2 \\%$ in the constrained model while assuring that the constrained model is fair.\n\nA very simple and naive approach to the problem that, superficially, might appear to be sensible for removing discrimination, is to drop the sensitive feature, which in this case results in the same accuracy as the unconstrained model. However, we believe dropping the sensitive feature from the model is a poor choice in our setting, both because sensitive features are often highly predictive of the outcome in interesting (and politicized) cases, and because dropping the sensitive feature does not in fact remove discrimination (as we defined it)!",
      "referring_paragraphs": [
        "The simplified causal graph model for this task is given in Figure 2 (a), where $A$ denotes race, prior convictions is the mediator $M$ , demographic information such as age and gender are collected in C, and $Y$ is recidivism.",
        "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.",
        "The causal model for this dataset is drawn in Figure 2(b)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1705.10378_page0_fig4.jpg"
      ],
      "quality_score": 0.9,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1706.02409": [
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig0.jpg",
      "image_filename": "1706.02409_page0_fig0.jpg",
      "caption": "",
      "context_before": "In this section we describe an empirical case study in which we apply our regularization framework to six different datasets in which fairness is a central concern. These datasets include cases in which the observed labels are real-valued, and cases in which they are binary-valued. For the real-valued datasets, we apply standard linear regression with our various fairness regularizers. For the binary-valued datasets, we apply standard logistic regression, again along with fairness regularizers. For datasets with real-valued targets we normalized the inputs and outputs to be zero mean and unit variance, and we set the cross-group fairness weights as $d ( y _ { i } , y _ { j } ) = e ^ { - ( y _ { i } - y _ { j } ) ^ { 2 } }$ ; for datasets with binary targets we set $d ( y _ { i } , y _ { j } ) = \\mathbb { 1 } [ y _ { i } = y _ { j } ]$ .\n\nFor each dataset $S$ , our framework requires that we solve optimization problems of the form $\\mathrm { m i n } _ { \\mathbf { w } } \\ell ( \\mathbf { w } , S ) + \\lambda f ( \\mathbf { w } , S ) + \\gamma | | \\mathbf { w } | | _ { 2 }$ for variable values of $\\lambda$ , where $\\ell ( \\mathbf { w } , S )$ is either MSE (linear regression) or the logistic regression loss. For each $\\lambda$ we picked $\\gamma$ as a function of this $\\lambda$ by cross validation (see Appendix A.1 for more details). All optimization problems are solved using the CVX solver in Matlab (for real-valued datasets) or python (for binary-valued datasets).4 Furthermore, all the results are reported using 10-fold cross validation (see more details in Appendix A.2).\n\nThe datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protected” feature (race or gender) that thus defines the subgroups across which we apply our fairness criteria (see Appendix A.3 for more details). The datasets vary considerably in the number of observations, their dimensionality, and the relative size of the minority subgroup.\n\nThe Adult dataset [23, 24] from the UC Irvine Repository contains 1994 Census data, and the goal is to predict whether the income of an individual in the dataset is more than 50K per year or not. The sensitive or protected attribute is gender.5 The Communities and Crime dataset [24] includes features relevant to per capita violent crime rates in different communities in the United States, and the goal is to predict this crime rate; race is the protected variable. The COMPAS dataset contains data from Broward County, Florida, originally compiled by ProPublica [2], in which the goal is to predict whether a convicted individual would commit a violent crime in the following two years or not. The protected attribute is race, and the data was filtered in a fashion similar to that of Corbett-Davies et al. [8]. The Default dataset [24, 32] contains data from Taiwanese credit card users, and the goal is to predict whether an individual will default on payments. The protected attribute is gender. The Law School dataset6 consists of the records of law students who went on to take the bar exam. The goal is to predict whether a student will pass the exam based on features such as LSAT score and undergraduate GPA. The protected attribute is gender. The Sentencing dataset contains information from a state department of corrections regarding inmates in 2010. The\n\ngoal is to predict the sentence length given by the judge based on factors such as previous criminal records and the crimes for which the conviction was obtained. The protected attribute is gender.\n\nTable 1: Summary of datasets. Type indicates whether regression is logistic or linear; $n$ is total number of data points; $d$ is dimensionality; Minority $n$ is the number of data points in the smaller population; Protected indicates which feature is protected or fairness-sensitive.   \n\n<table><tr><td>Data Set</td><td>Type</td><td>n</td><td>d</td><td>Minority n</td><td>Protected</td></tr><tr><td>Adult</td><td>logit</td><td>32561</td><td>14</td><td>10771</td><td>gender</td></tr><tr><td>Communities and Crime</td><td>linear</td><td>1994</td><td>128</td><td>227</td><td>race</td></tr><tr><td>COMPAS</td><td>logit</td><td>3373</td><td>19</td><td>1455</td><td>race</td></tr><tr><td>Default</td><td>logit</td><td>30000</td><td>24</td><td>11888</td><td>gender</td></tr><tr><td>Law School</td><td>logit</td><td>27478</td><td>36</td><td>12079</td><td>gender</td></tr><tr><td>Sentencing</td><td>linear</td><td>5969</td><td>17</td><td>385</td><td>gender</td></tr></table>\n\n[Section: 4.1 Accuracy-Fairness Efficient Frontiers]\n\nWe begin by examining the efficient frontier of accuracy vs. fairness for the six datasets. These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function. For the logistic regression cases, we extract probabilities from the learned model w as ${ \\mathrm { P r } } [ y _ { i } = 1 ] =$ $\\exp ( \\mathbf { w } \\cdot x _ { i } ) / ( 1 + \\exp ( \\mathbf { w } \\cdot x _ { i } ) )$ and evaluate these probabilities as predictions for the binary labels using MSE. 7 In all of the datasets, as $\\lambda$ increases, the models converge to the best constant predictor, which minimizes the fairness penalties.\n\nPerhaps the most striking aspect of Figure 1 is the great diversity of tradeoffs across different datasets and different fairness regularizers. For instance, if we examine the individual fairness regularizer, on four of the datasets (Adult, Communities and Crime, Law School and Sentencing), the curvature is relatively mild and constant — there is an approximately fixed rate at which fairness can be traded for accuracy. In contrast, on COMPAS and Default, fairness loss can be reduced almost for “free” until some small threshold value, at which point the accuracy cost increases dramatically. Similar comments can be made regarding hybrid fairness in the logistic regression cases.\n\nIndividual fairness appears to be strictly more costly than group fairness for the entire regime between the extremes of $\\lambda = 0$ and $\\lambda \\to \\infty$ for a majority of these datasets (with the exception of COMPAS and Default datasets). In COMPAS and Default, for small amounts of unfairness, group unfairness may be more costly than the same level of individual unfairness.\n\nPerhaps surprisingly, building separate models for each population barely improves the tradeoff (and in some cases, hurts the tradeoff for some values of $\\lambda$ ) across almost all datasets and fairness regularizers. This suggests that the academic discussion about whether to allow disparate treatment (as explicitly allowed in e.g. [9, 17]) – i.e. whether sensitive attributes such as race and gender should be “forbidden” vs. used in building more accurate models for each subpopulation, is perhaps less consequential than expected (at least on these datasets and using linear or logistic regression models).\n\nNote that in theory group fairness is strictly less costly than individual fairness for any particular model w (by Jensen’s inequality), and using separate models (one for each group) should strictly\n\nimprove the fairness/accuracy trade-off for any of these notions of fairness. However, both group fairness and separate models are more prone to overfitting (than individual fairness and single model), and hence a larger $\\ell _ { 2 }$ -regularization parameter $\\gamma$ tends to be selected in cross validation in these settings (see Appendix A.1 for more details). This is a surprising interaction between the strength of the fairness penalty and the generalization ability of the model, and results in group fairness sometimes having a more severe tradeoff with accuracy when compared to individual fairness, and separate models having little benefit out of sample, although they can appear to have a large effect in-sample (because of the effects of over-fitting).",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "1706.02409_page0_fig1.jpg",
        "1706.02409_page0_fig2.jpg"
      ],
      "quality_score": 0.35,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg",
      "image_filename": "1706.02409_page0_fig3.jpg",
      "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
      "context_before": "",
      "context_after": "[Section: 4.2 Price of Fairness]\n\nThe efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g. to see that in some datasets, the fairness penalty can be substantially decreased with little cost to accuracy. However, they are difficult to compare quantitatively, because the scale of the fairness loss differs substantially from data set to data set. In this section, we give a cross-dataset comparison using a measure (which we call Price of Fairness) which has the effect of normalizing the fairness loss across data sets to lie on the same scale.\n\nFor a given data set and regression type (linear or logistic), let w∗ be the optimal model absent any fairness penalty (i.e. the empirical risk minimizer when the fairness “regularization” weight $\\lambda = 0$ ). This model will suffer some fairness penalty: it represents the “maximally unfair” point on the fairness/accuracy frontiers from Figure 1. For each dataset, we will fix a normalization such that this fairness penalty is rescaled to be 1, and ask for the cost (in terms of the relative increase in mean squared error) of constraining our predictor to have fairness penalty $\\alpha \\leq 1$ . Equivalently, this is measuring the relative increase in MSE that results from constraining a predictor to have fairness\n\npenalty that is no more than an $\\alpha$ fraction of the fairness penalty of the unconstrained optimal predictor.\n\nMore formally, let $\\mathbf { w } ^ { * } = \\arg \\operatorname* { m i n } _ { \\mathbf { w } } \\ell _ { \\mathcal { P } } ( \\mathbf { w } )$ . For any value of $\\alpha \\in [ 0 , 1 ]$ we define the price of fairness (PoF) as follows:\n\nNote that by definition, $\\mathrm { P o F } ( \\alpha ) \\ge 1$ , $\\mathrm { P o F } ( 1 ) = 1$ , and that $\\mathrm { P o F } ( \\alpha )$ increases monotonically as $\\alpha$ decreases. Larger values represent more severe costs for imposing fairness constraints that ask that the measure of unfairness be small relative to the unconstrained optimum. It is important to note that because this measure asks for the cost of relative improvements over the unconstrained optimum, it can be, for example, that the PoF for one fairness penalty case is larger than for another, even if the absolute fairness loss for both the numerator and the denominator is smaller in the second case. With this observation in mind, we can move to the empirical findings.\n\nFigure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case. We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1. For some datasets (e.g. COMPAS and Sentencing), increasing the fairness constraint by decreasing $\\alpha$ has only a mild cost in terms of error. For others (e.g. Communities and Crime, and Law School), the cost increases steadily as we decrease $\\alpha$ .\n\nNext, we observe that with this normalization, although the difference between separate and single models remains small on most datasets, on two datasets, differences emerge. In the Law School dataset, restricting to a single model leads to a significantly higher PoF when considering the group fairness metric, compared to allowing separate models. In contrast, on the Adult dataset, restricting to a single model substantially reduces the PoF when considering the individual fairness metric.\n\nFinally, this normalization allows us to observe variation across fairness penalties in the rate of change in the PoF as $\\alpha$ is decreased. In some datasets (e.g. Communities and Crime, and Sentencing), the PoF changes in lock-step across all measures of unfairness. However, for others (e.g. Default), the PoF increases substantially with $\\alpha$ when we consider group or hybrid fairness measures, but is much more stable for individual fairness.",
      "referring_paragraphs": [
        "The datasets themselves are summarized in Table 1, where we specify the size and dimensionality of each, along with the “protected” feature (race or gender) that thus defines the subgroups across which we apply our fairness criteria (see Appendix A.3 for more details).",
        "These curves are shown in Figure 1, and are obtained by varying the weight $\\lambda$ on the fairness regularizer, and for each value of $\\lambda$ finding the model which minimizes the associated regularized loss function.",
        "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.",
        "The efficient fairness/accuracy frontiers pictured in Figure 1 can be compared across data sets in a qualitative sense — e.g.",
        "We first note that even when normalized on a common scale, we continue to see the diversity across datasets that was apparent in Figure 1.",
        "This is because: (1) using more cross pairs did not substantially improve the efficiency curves in Figure 1, (2) the CVXPY solver for binary-valued problems would become unstable when using individual fairness if we increase the number of cross pairs significantly."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1706.02409_page0_fig4.jpg",
        "1706.02409_page0_fig5.jpg",
        "1706.02409_page0_fig6.jpg",
        "1706.02409_page0_fig7.jpg",
        "1706.02409_page0_fig8.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig8.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "1706.02409",
      "figure_id": "1706.02409_fig_10",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
      "image_filename": "1706.02409_page0_fig9.jpg",
      "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.",
      "context_before": "",
      "context_after": "[Section: References]\n\n[1] Philip Adler, Casey Falk, Sorelle Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. In Proceedings of the 16th International Conference on Data Mining, pages 1–10, 2016.   \n[2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica, 2016.   \n[3] Anna Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing. The Marshall Project, August 8 2015. URL https://www.themarshallproject.org/2015/08/04/ the-new-science-of-sentencing. Retrieved 4/28/2016.   \n[4] Nanette Byrnes. Artificial intolerance. MIT Technology Review, March 28 2016. URL https: //www.technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016.   \n[5] Toon Calders and Sicco Verwer. Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2):277–292, 2010.   \n[6] Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang. Controlling attribute effect in linear regression. In Proceedings of the 13th International Conference on Data Mining, pages 71–80, 2013.   \n[7] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. coRR, abs/1703.00056, 2017.   \n[8] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. CoRR, abs/1701.08230, 2017.   \n[9] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Conference on Innovations in Theoretical Computer Science, pages 214–226, 2012.\n\n[10] Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259–268, 2015.   \n[11] Benjamin Fish, Jeremy Kun, and Ádám Dániel Lelkes. A confidence-based approach for balancing fairness and accuracy. In Proceedings of the 16th SIAM International Conference on Data Mining, pages 144–152, 2016.   \n[12] Sorelle Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility of fairness. CoRR, abs/1609.07236, 2016.   \n[13] Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. IEEE Transactions on Knowledge and Data Engineering, 25(7): 1445–1459, 2013.   \n[14] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In Proceedings of the 30th Annual Conference on Neural Information Processing Systems, pages 3315–3323, 2016.   \n[15] Zubin Jelveh and Michael Luca. Towards diagnosing accuracy loss in discrimination-aware classification: An application to predictive policing. In 2nd Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2015.   \n[16] Kory Johnson, Dean Foster, and Robert Stine. Impartial predictive modeling: Ensuring fairness in arbitrary models. CoRR, abs/1608.00528, 2016.   \n[17] Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in learning: classic and contextual bandits. In Proceedings of The 30th Annual Conference on Neural Information Processing Systems, pages 325–333, 2016.   \n[18] Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1–33, 2011.   \n[19] Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learning. In Proceedings of the 10th IEEE International Conference on Data Mining, pages 869–874, 2010.   \n[20] Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for discrimination-aware classification. In Proceedings of the 12th IEEE International Conference on Data Mining, pages 924–929, 2012.   \n[21] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice remover regularizer. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, pages 35–50, 2012.   \n[22] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Proceedings of the 8th Conference on Innovations in Theoretical Computer Science, 2017.   \n[23] Ron Kohavi. Scaling up the accuracy of naive Bayes classifiers: A decision-tree hybrid. In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, pages 202–207, 1996.\n\n[24] Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ ml.   \n[25] Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. k-NN as an implementation of situation testing for discrimination discovery and prevention. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 502–510. ACM, 2011.   \n[26] Clair Miller. Can an algorithm hire better than a human? The New York Times, June 25 2015. URL http://www.nytimes.com/2015/06/26/upshot/ can-an-algorithm-hire-better-than-a-human.html/. Retrieved 4/28/2016.   \n[27] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560–568. ACM, 2008.   \n[28] Michael Redmond and Alok Baveja. A data-driven software tool for enabling cooperative information sharing among police departments. European Journal of Operational Research, 141 (3):660–678, 2002.   \n[29] Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired Magazine, August 2013. URL http://www.wired.com/insights/2013/ 08/predictive-policing-using-machine-learning-to-detect-patterns-of-crime/. Retrieved 4/28/2016.   \n[30] Latanya Sweeney. Discrimination in online ad delivery. Communications of the ACM, 56(5): 44–54, 2013.   \n[31] Blake Woodworth, Suriya Gunasekar, Mesrob Ohannessian, and Nathan Srebro. Learning non-discriminatory predictors. CoRR, abs/1702.06081, 2017.   \n[32] I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36 (2):2473–2480, 2009.   \n[33] Muhammad Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna Gummadi. Fairness constraints: A mechanism for fair classification. CoRR, abs/1507.05259, 2015.   \n[34] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pages 1171–1180, 2017.   \n[35] Richard Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Proceedings of the 30th International Conference on Machine Learning, pages 325–333, 2013.\n\n[Section: A Missing Details from the Experiments]",
      "referring_paragraphs": [
        "Figure 2 displays the PoF on each of the 6 datasets we study, for each fairness regularizer (individual, hybrid, and group), and for the single and separate model case.",
        "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1706.02409_page0_fig10.jpg",
        "1706.02409_page0_fig11.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig11.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1706.02744": [
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg",
      "image_filename": "1706.02744_page0_fig0.jpg",
      "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .",
      "context_before": "Causal graphs are a convenient way of organizing assumptions about the data generating process. We will generally consider causal graphs involving a protected attribute $A$ , a set of proxy variables $P _ { - }$ , features $X$ , a predictor $R$ and sometimes an observed outcome Y. For background on causal graphs see [11]. In the present paper a causal graph is a directed, acyclic graph whose nodes represent random variables. A directed path is a sequence of distinct nodes $V _ { 1 } , \\ldots , V _ { k }$ , for $k \\geq 2$ , such that $V _ { i } \\to V _ { i + 1 }$ for all $i \\in \\{ 1 , \\ldots , k - 1 \\}$ . We say a directed path is blocked by a set of nodes $Z$ , where $V _ { 1 } , V _ { k } \\notin Z$ , if $V _ { i } \\in Z$ for some $i \\in \\{ 2 , . . . , \\dot { k } - 1 \\}$ . 1\n\nA structural equation model is a set of equations $V _ { i } ~ = ~ f _ { i } ( p a ( V _ { i } ) , N _ { i } )$ , for $i \\in \\{ 1 , \\ldots , n \\}$ , where $p a ( V _ { i } )$ are the parents of $V _ { i }$ , i.e. its direct causes, and the $N _ { i }$ are independent noise variables. We interpret these equations as assignments. Because we assume acyclicity, starting from the roots of the graph, we can recursively compute the other variables, given the noise variables. This leads us to view the structural equation model and its corresponding graph as a data generating model. The predictor $R$ maps inputs, e.g., the features $X$ , to a predicted output. Hence we model it as a childless node, whose parents are its input variables. Finally, note that given the noise variables, a structural equation model entails a unique joint distribution; however, the same joint distribution can usually be entailed by multiple structural equation models corresponding to distinct causal structures.\n\n[Section: 2 Unresolved discrimination and limitations of observational criteria]\n\nTo bear out the limitations of observational criteria, we turn to Pearl’s commentary on claimed gender discrimination in Berkeley college admissions [11, Section 4.5.3]. Bickel [20] had shown earlier that a lower college-wide admission rate for women than for men was explained by the fact that women applied in more competitive departments. When adjusted for department choice, women experienced a slightly higher acceptance rate compared with men. From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$ , see Figure 1. We shall use the term resolving variable for any variable in the causal graph that is influenced by $A$ in a manner that we accept as nondiscriminatory. With this convention, the criterion can be stated as follows.",
      "context_after": "",
      "referring_paragraphs": [
        "From the causal point of view, what matters is the direct effect of the protected attribute (here, gender $A$ ) on the decision (here, college admission $R$ ) that cannot be ascribed to a resolving variable such as department choice $X$ , see Figure 1.",
        "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {}
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg",
      "image_filename": "1706.02744_page0_fig1.jpg",
      "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.",
      "context_before": "",
      "context_after": "[Section: 3 Proxy discrimination and interventions]\n\nWe now turn to an important aspect of our framework. Determining causal effects in general requires modeling interventions. Interventions on deeply rooted individual properties such as gender or race are notoriously difficult to conceptualize—especially at an individual level, and impossible to perform in a randomized trial. VanderWeele et al. [16] discuss the problem comprehensively in an epidemiological setting. From a machine learning perspective, it thus makes sense to separate the protected attribute $A$ from its potential proxies, such as name, visual features, languages spoken at home, etc. Intervention based on proxy variables poses a more manageable problem. By deciding on a suitable proxy we can find an adequate mounting point for determining and removing its influence on the prediction. Moreover, in practice we are often limited to imperfect measurements of $A$ in any case, making the distinction between root concept and proxy prudent.\n\nAs was the case with resolving variables, a proxy is a priori nothing more than a descendant of $A$ in the causal graph that we choose to label as a proxy. Nevertheless in reality we envision the proxy\n\nto be a clearly defined observable quantity that is significantly correlated with $A$ , yet in our view should not affect the prediction.\n\nDefinition 2 (Potential proxy discrimination). A variable $V$ in a causal graph exhibits potential proxy discrimination, if there exists a directed path from $A$ to $V$ that is blocked by a proxy variable and $V$ itself is not a proxy.\n\nPotential proxy discrimination articulates a causal criterion that is in a sense dual to unresolved discrimination. From the benevolent viewpoint, we allow any path from $A$ to $R$ unless it passes through a proxy variable, which we consider worrisome. This viewpoint acknowledges the fact that the influence of $A$ on the graph may be complex and it can be too restraining to rule out all but a few designated features. In practice, as with unresolved discrimination, we can naively build an unconstrained predictor based only on those features that do not exhibit potential proxy discrimination. Then we must not provide $P$ as input to $R$ ; unawareness, i.e. excluding $P$ from the inputs of $R$ , suffices. However, by granting $R$ access to $P$ , we can carefully tune the function $R ( P , X )$ to cancel the implicit influence of $P$ on features $X$ that exhibit potential proxy discrimination by the explicit dependence on $P$ . Due to this possible cancellation of paths, we called the path based criterion potential proxy discrimination. When building predictors that exhibit no overall proxy discrimination, we precisely aim for such a cancellation.\n\nFortunately, this idea can be conveniently expressed by an intervention on $P$ , which is denoted by $d o ( P \\doteq p )$ [11]. Visually, intervening on $P$ amounts to removing all incoming arrows of $P$ in the graph; algebraically, it consists of replacing the structural equation of $P$ by $P = p$ , i.e. we put point mass on the value $p$ .\n\nDefinition 3 (Proxy discrimination). A predictor $R$ exhibits no proxy discrimination based on a proxy $P$ if for all $p , p ^ { \\prime }$\n\nThe interventional characterization of proxy discrimination leads to a simple procedure to remove it in causal graphs that we will turn to in the next section. It also leads to several natural variants of the definition that we discuss in Section 4.3. We remark that Equation (1) is an equality of probabilities in the “do-calculus” that cannot in general be inferred by an observational method, because it depends on an underlying causal graph, see the discussion in [11]. However, in some cases, we do not need to resort to interventions to avoid proxy discrimination.\n\nProposition 1. If there is no directed path from a proxy to a feature, unawareness avoids proxy discrimination.",
      "referring_paragraphs": [
        "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ .",
        "Let us consider the two graphs in Figure 2."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1706.02744_page0_fig2.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg",
      "image_filename": "1706.02744_page0_fig3.jpg",
      "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.",
      "context_before": "[Section: 4.1 Avoiding proxy discrimination]\n\nWhile presenting the general procedure, we illustrate each step in the example shown in Figure 3. A protected attribute $A$ affects a proxy $P$ as well as a feature $X$ . Both $P$ and $X$ have additional unobserved causes $N _ { P }$ and $N _ { X }$ , where $N _ { P } , N _ { X } , A$ are pairwise independent. Finally, the proxy also has an effect on the features $X$ and the predictor $R$ is a function of $P$ and $X$ . Given labeled training data, our task is to find a good predictor that exhibits no proxy discrimination within a hypothesis class of functions $R _ { \\theta } ( P , X )$ parameterized by a real valued vector $\\theta$ .\n\nWe now work out a formal procedure to solve this task under specific assumptions and simultaneously illustrate it in a fully linear example, i.e. the structural equations are given by\n\nNote that we choose linear functions parameterized by $\\theta \\ = \\ ( \\lambda _ { P } , \\lambda _ { X } )$ as the hypothesis class for $R _ { \\theta } ( P , X )$ .",
      "context_after": "",
      "referring_paragraphs": [
        "While presenting the general procedure, we illustrate each step in the example shown in Figure 3.",
        "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.",
        "For the example in Figure 3,",
        "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations",
        "Figure 3 is an example thereof.",
        "In the scenario of Figure 3, the direct effect of $P$ on $X$ along the arrow $P  X$ in the left graph cannot be estimated by $\\mathbb { E } [ X | P ]$ , because of the common confounder $A$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig4.jpg",
      "image_filename": "1706.02744_page0_fig4.jpg",
      "caption": "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the skeptical viewpoint we generically do not want $A$ to influence $R$ , we first intervene on $E$ interrupting all paths through $E$ and only cancel the remaining influence on $A$ to $R$ .",
      "context_before": "",
      "context_after": "2. Iteratively substitute variables in the equation for $R _ { \\theta }$ from their structural equations until only root nodes of the intervened graph are left, i.e. write $R _ { \\theta } ( P , X )$ as $R _ { \\theta } ( P , g ( t \\bar { a } ^ { \\mathcal { G } } ( X ) ) )$ for some function $g$ . In the example, $t \\bar { a ( X ) } = \\{ A , P , N _ { X } \\}$ and\n\n3. We now require the distribution of $R _ { \\theta }$ in (3) to be independent of $p$ , i.e. for all $p , p ^ { \\prime }$\n\nWe seek to write the predictor as a function of $P$ and all the other roots of $\\mathcal { G }$ separately. If our hypothesis class is such that there exists $\\tilde { \\theta }$ such that $R _ { \\theta } ( P , g ( t a ( X ) ) ) = R _ { \\tilde { \\theta } } ( P , \\tilde { g } ( t a ( X ) \\backslash \\{ P \\} ) )$ , we call the structural equation model and hypothesis class specified in (2) expressible. In our example, this is possible with $\\tilde { \\theta } = ( \\lambda _ { P } + \\lambda _ { X } \\beta , \\lambda _ { X } )$ and $\\tilde { g } = \\alpha _ { X } A + N _ { X }$ . Equation (4) then yields the non-discrimination constraint $\\tilde { \\theta } = \\theta _ { 0 }$ . Here, a possible $\\theta _ { 0 }$ is $\\theta _ { 0 } = ( 0 , \\lambda _ { X } )$ , which simply yields $\\lambda _ { P } = - \\lambda _ { X } \\beta$ .\n\n4. Given labeled training data, we can optimize the predictor $R _ { \\theta }$ within the hypothesis class as given in (2), subject to the non-discrimination constraint. In the example",
      "referring_paragraphs": [
        "Figure 4: A template graph $\\tilde { \\mathcal { G } }$ for unresolved discrimination (left) with its intervened version $\\mathcal { G }$ (right).",
        "We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of the proxy, we consider a resolving variable $E$ . The causal dependences are equivalent to the ones in Figure 3 and we again assume linear structural equations",
        "Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination.\n\n1. Intervene on $E$ by fixing it to a random variable $\\eta$ with $\\mathbb { P } ( \\eta ) = \\mathbb { P } ( E )$ , the marginal distribution of $E$ in $\\tilde { \\mathcal { G } }$ , see Figure 4. In the example we find"
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1706.02744",
      "figure_id": "1706.02744_fig_6",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg",
      "image_filename": "1706.02744_page0_fig5.jpg",
      "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
      "context_before": "3. We now require the distribution of $R _ { \\theta }$ in (3) to be independent of $p$ , i.e. for all $p , p ^ { \\prime }$\n\nWe seek to write the predictor as a function of $P$ and all the other roots of $\\mathcal { G }$ separately. If our hypothesis class is such that there exists $\\tilde { \\theta }$ such that $R _ { \\theta } ( P , g ( t a ( X ) ) ) = R _ { \\tilde { \\theta } } ( P , \\tilde { g } ( t a ( X ) \\backslash \\{ P \\} ) )$ , we call the structural equation model and hypothesis class specified in (2) expressible. In our example, this is possible with $\\tilde { \\theta } = ( \\lambda _ { P } + \\lambda _ { X } \\beta , \\lambda _ { X } )$ and $\\tilde { g } = \\alpha _ { X } A + N _ { X }$ . Equation (4) then yields the non-discrimination constraint $\\tilde { \\theta } = \\theta _ { 0 }$ . Here, a possible $\\theta _ { 0 }$ is $\\theta _ { 0 } = ( 0 , \\lambda _ { X } )$ , which simply yields $\\lambda _ { P } = - \\lambda _ { X } \\beta$ .\n\n4. Given labeled training data, we can optimize the predictor $R _ { \\theta }$ within the hypothesis class as given in (2), subject to the non-discrimination constraint. In the example\n\nwith the free parameter $\\lambda _ { X } \\in \\mathbb { R }$ .\n\nIn general, the non-discrimination constraint (4) is by construction just $\\mathbb { P } ( R | d o ( P ~ = ~ p ) ) ~ =$ $\\mathbb { P } ( \\boldsymbol { \\bar { R } } | d o ( \\boldsymbol { P } = \\boldsymbol { p ^ { \\prime } } ) )$ , coinciding with Definition 3. Thus Proposition 2 holds by construction of the procedure. The choice of $\\theta _ { 0 }$ strongly influences the non-discrimination constraint. However, as the example shows, it allows $R _ { \\theta }$ to exploit features that exhibit potential proxy discrimination.",
      "context_after": "[Section: Proof of Corollary 1]\n\nCorollary. Under the assumptions of Theorem 2, if all directed paths from any ancestor of $P$ to $X$ in the graph $\\mathcal { G }$ are blocked by $P$ , then any predictor based on the adjusted features ${ \\tilde { X } } : = X - \\mathbb { E } [ X | P ]$ exhibits no proxy discrimination and can be learned from the observational distribution $\\mathbb { P } ( P , X , Y )$ when target labels $Y$ are available.\n\nProof. Let $Z$ denote the set of ancestors of $P$ . Under the given assumptions $Z \\cap t a ^ { \\mathcal { G } } ( X ) = \\varnothing$ , because in $\\mathcal { G }$ all arrows into $P$ are removed, which breaks all directed paths from any variable in $Z$ to $X$ by assumption. Hence the distribution of $X$ under an intervention on $P$ in $\\tilde { \\mathcal { G } }$ , where the influence of potential ancestors of $P$ on $X$ that does not go through $P$ would not be affected, is the same as simply conditioning on $P$ . Therefore $\\mathbb { E } [ X | d o ( \\bar { P } ) ] = \\bar { \\mathbb { E } } [ X | P ]$ , which can be computed from the joint observational distribution, since we observe $X$ and $P$ as generated by $\\tilde { \\mathcal { G } }$ . □\n\n[Section: Proof of Proposition 3]",
      "referring_paragraphs": [
        "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.",
        "We consider a generic graph structure as shown on the left in Figure 5.",
        "Individual proxy discrimination aims at comparing examples with the same features $X$ , for different values of $P$ . Note that this can be individuals with different values for the unobserved non-feature variables. A true individual-level comparison of the form “What would have happened to me, if I had always belonged to another group” is captured by counterfactuals and discussed in [15, 19].\n\nFor an analysis of proxy discrimination, we need the structural equations for $P , X , R$ in Figure 5",
        "We can find $f _ { X } , f _ { R }$ from $\\hat { f } _ { X } , \\hat { f } _ { R }$ by first rewriting the functions in terms of root nodes of the intervened graph, shown on the right side of Figure 5, and then assigning the overall dependence on $P$ to the first argument."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1706.02744_page0_fig6.jpg",
        "1706.02744_page0_fig7.jpg",
        "1706.02744_page0_fig8.jpg",
        "1706.02744_page0_fig9.jpg"
      ],
      "quality_score": 0.9,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig9.jpg"
        ],
        "panel_count": 5
      }
    }
  ],
  "1707.00574": [
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig0.jpg",
      "image_filename": "1707.00574_page0_fig0.jpg",
      "caption": "",
      "context_before": "Our model considers a fixed number $N$ of items. These represent transmissible units of information, sometimes referred to as memes [7], such as music tracks, videos, books, fashion products, or links to news articles. Items are selected sequentially at discrete times. Each item $i$ has an intrinsic quality value $q _ { i }$ drawn uniformly at random from $[ 0 , 1 ]$ . Quality is operationally defined as the probability that an item is selected by a user when not exposed to the popularity of the item. The popularity of item $i$ at time $t$ , $p _ { i } ( t )$ , is simply the number of times $_ i$ has been selected until $t$ . At the beginning each item is equally popular: $p _ { i } ( 0 ) = 1 , i = 1 \\ldots N$ .\n\nAt each time step, with probability $\\beta$ , an item is selected based on its popularity. All items are first ranked by their popularity, and then an item is drawn with probability proportional to its rank raised to some power:\n\nwhere the rank $r _ { i } ( t )$ is the number of items that, at time $t$ , have been selected at least as many times as $i$ . The exponent $\\alpha$ regulates the decay of selection probability for lower-ranked items. This schema is inspired by the ranking model, which allows for the emergence of scale-free popularity distributions with arbitrary power-law exponents [11]; it is consistent with empirical data about how people click search engine results [10] and scroll through social media feeds [26]. This model could accurately capture aggregate behavior even if individuals followed different selection schemes [8].\n\nAlternatively, with probability $1 - \\beta$ , an item is drawn with probability proportional to its quality:\n\nAfter an item $i$ has been selected, we update its popularity $( p _ { i } ( t { + } 1 ) = p _ { i } ( t ) + 1 )$ and the ranking. Two items will have the same rank $r$ if they have been selected the same number of times. If $k$ item are all at the same rank $r$ , then the next rank will be $r + k$ .\n\nThe model has two parameters: $\\beta$ regulates the importance of popularity over quality and thus represents the popularity bias of the algorithm. When $\\beta = 0$ , choices are entirely driven by quality (no popularity bias). When $\\beta = 1$ , only popularity choices are allowed, yielding a type of Polya urn model [22]. The parameter $\\alpha$ can be thought of as an exploration cost. A large $\\alpha$ implies that users are likely to consider only one or a few most popular items, whereas a small $\\alpha$ allows users to explore less popular choices. In the limit $\\alpha  0$ , the selection no longer depends on popularity, yielding the uniform probability across the discrete set of $N$ items.\n\nWe vary $\\beta$ systematically in $\\lfloor 0 , 1 \\rfloor$ and consider different values of $\\alpha$ between 0 and 3. We simulate 1,000 realizations for each parameter configuration. In each realization we perform $T = 1 0 ^ { 6 }$ selections using Eqs. 1 and 2 and store the final popularity values.\n\nWe characterize two properties of the final distribution of popularity $\\{ p _ { i } \\} _ { i = 1 } ^ { N }$ with respect to the intrinsic quality distribution $\\left\\{ q _ { i } \\right\\} _ { i = 1 } ^ { N }$ . For brevity, we pose $p _ { i } ~ = ~ p _ { i } ( T )$ The first quantity we meand the second property ure is the average quality is the faithfulness of the a $\\bar { q } \\ =$ $\\textstyle \\sum _ { i = 1 } ^ { N } p _ { i } q _ { i } / \\sum _ { i = 1 } ^ { N } p _ { i }$ $\\tau$ faithfulness using Kendall’s rank correlation between popularity and quality [17]. We can derive the values of both properties in the extreme cases of maximum or no popularity bias. When $\\beta = 0$ , selections are made exclusively on the basis of quality and therefore one expects $p _ { i } \\to q _ { i }$ as $T \\to \\infty$ . The rankings by quality and popularity are therefore perfectly aligned, and $\\tau = 1$ . In the limit of large $N$ we can make a continuous approximation $\\begin{array} { r } { \\bar { q }  \\int _ { 0 } ^ { 1 } q ^ { 2 } d q / \\int _ { 0 } ^ { 1 } q d q = 2 / 3 } \\end{array}$ . When $\\beta = 1$ , quality never enters the picture and any permutation of the items is an equally likely popularity ranking, which translates into $\\tau = 0$ . Also $p _ { i } \\to 1 / N$ and in the continuous approximation $\\begin{array} { r } { \\bar { q }  \\int _ { 0 } ^ { 1 } q d q = 1 / 2 } \\end{array}$ . What happens for intermediate values of popularity bias is harder to predict. The question we ask is whether it is possible to leverage some popularity bias to obtain a higher",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(c)"
      ],
      "quality_score": 0.35,
      "metadata": {}
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg",
      "image_filename": "1707.00574_page0_fig1.jpg",
      "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1: Effects of popularity bias on average quality and faithfulness.."
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(b)",
        "1707.00574_page0_fig2.jpg",
        "1707.00574_page0_fig3.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig3.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1707.00574",
      "figure_id": "1707.00574_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg",
      "image_filename": "1707.00574_page0_fig4.jpg",
      "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.",
      "context_before": "",
      "context_after": "[Section: Discussion]\n\nCultural markets like social media and the music and fashion industry account for multi-billion businesses with worldwide social and economic impact [25]. Success in these markets may strongly depend on structural or subjective features, like competition for limited attention [34, 26]. The inherent quality of cultural products is often difficult to establish, therefore relying on measurable quantitative features like the popularity of an item is hugely advantageous in terms of cognitive processing and scalability.\n\nYet, previous literature has shown that recommending already popular choices can be detrimental to the predictability and overall quality of a cultural market [29]. This left open the question of whether there exist situations in which a bit of popularity bias can help high-quality items bubble up in a cultural market.\n\nIn this paper we answered this question using an extremely simplified abstraction of cultural market, in which items are endowed with inherent quality. The model could be extended in many directions, for example assuming a population of networked agents with heterogeneous parameters. However, our approach leads to very general findings about the effects of popularity bias. While we confirmed that such a bias distorts assessments of quality, the scenario emerging from our analysis is less dire than suggested by prior literature. First, it is possible to maintain a good correspondence between popularity and quality rankings of consumed items even when our reliance on popularity for our choices is relatively high. Second, it is possible to carefully tune the popularity mechanisms that drive our choices to effectively leverage the wisdom of the crowd and boost the average quality of consumed items.\n\nFrom a normative perspective, our results provide a recipe for improving the quality of content in techno-social cultural markets driven by engagement metrics, such as social media platforms. It is possible in these systems to estimate the exponent $\\alpha$ empirically, by measuring the probability that a user engages with an item as a function of the item’s position in the feed. Given a statistical characterization (e.g., average or distribution) of the exploration cost, the bias $\\beta$ of the ranking algorithm can be tuned to maximize expected average quality.\n\nThese findings are important because in our information-flooded world we increasingly rely on algorithms to help us make consumption choices. Platforms such as search engines, shopping sites, and mobile news feeds save us time but also bias our choices. Their algorithms are affected by and in turn affect the popularity of products, and ultimately drive what we collectively consume in ways that we do not entirely comprehend. It has been argued, for example, that the engagement bias of social media ranking algorithms is partly responsible for the spread of low-quality content over high-quality material [4]. Evaluating such a claim is challenging, but the present results may lead to a better understanding of algorithmic bias.\n\n[Section: Acknowledgements]",
      "referring_paragraphs": [
        "Figure 2: Temporal evolution of average quality."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1707.00574_page0_fig5.jpg",
        "1707.00574_page0_fig6.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig6.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1707.09457": [
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg",
      "image_filename": "1707.09457_page0_fig0.jpg",
      "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.",
      "context_before": "Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over $33 \\%$ more likely to involve females than males in a training set, and a trained model further amplifies the disparity to $68 \\%$ at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by $4 7 . 5 \\%$ and $40 . 5 \\%$ for multilabel classification and visual semantic role labeling, respectively.\n\n[Section: 1 Introduction]\n\nVisual recognition tasks involving language, such as captioning (Vinyals et al., 2015), visual question answering (Antol et al., 2015), and visual semantic role labeling (Yatskar et al., 2016), have emerged as avenues for expanding the diversity of information that can be recovered from images. These tasks aim at extracting rich seman-\n\ntics from images and require large quantities of labeled data, predominantly retrieved from the web. Methods often combine structured prediction and deep learning to model correlations between labels and images to make judgments that otherwise would have weak visual support. For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking. Yet such methods run the risk of discovering and exploiting societal biases present in the underlying web corpora. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models can have the inadvertent effect of magnifying stereotypes.\n\nIn this paper, we develop a general framework for quantifying bias and study two concrete tasks, visual semantic role labeling (vSRL) and multilabel object classification (MLC). In vSRL, we use the imSitu formalism (Yatskar et al., 2016, 2017), where the goal is to predict activities, objects and the roles those objects play within an activity. For MLC, we use MS-COCO (Lin et al., 2014; Chen et al., 2015), a recognition task covering 80 object classes. We use gender bias as a running example and show that both supporting datasets for these tasks are biased with respect to a gender binary1.\n\nOur analysis reveals that over $45 \\%$ and $37 \\%$ of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1. For example, as seen in Figure 1, the cooking activity in imSitu is a heavily biased verb. Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by $5 . 0 \\%$ for vSRL, and $3 . 6 \\%$ in MLC.",
      "context_after": "[Section: 2 Related Work]\n\nAs intelligence systems start playing important roles in our daily life, ethics in artificial intelligence research has attracted significant interest. It is known that big-data technologies sometimes inadvertently worsen discrimination due to implicit biases in data (Podesta et al., 2014). Such issues have been demonstrated in various learning systems, including online advertisement systems (Sweeney, 2013), word embedding models (Bolukbasi et al., 2016; Caliskan et al., 2017), online news (Ross and Carter, 2011), web search (Kay et al., 2015), and credit score (Hardt et al., 2016). Data collection biases have been discussed in the context of creating image corpus (Misra et al., 2016; van Miltenburg, 2016) and text corpus (Gordon and Van Durme, 2013; Van Durme, 2010). In contrast, we show that given a gender biased corpus, structured models such as conditional random fields, amplify the bias.\n\nThe effect of the data imbalance can be easily detected and fixed when the prediction task is simple. For example, when classifying binary data with unbalanced labels (i.e., samples in the majority class dominate the dataset), a classifier trained exclusively to optimize accuracy learns to always predict the majority label, as the cost of making mistakes on samples in the minority class can be neglected. Various approaches have been proposed to make a “fair” binary classification (Barocas and Selbst, 2014; Dwork et al., 2012; Feldman\n\net al., 2015; Zliobaite, 2015). For structured prediction tasks the effect is harder to quantify and we are the first to propose methods to reduce bias amplification in this context.\n\nLagrangian relaxation and dual decomposition techniques have been widely used in NLP tasks (e.g., (Sontag et al., 2011; Rush and Collins, 2012; Chang and Collins, 2011; Peng et al., 2015)) for dealing with instance-level constraints. Similar techniques (Chang et al., 2013; Dalvi, 2015) have been applied in handling corpus-level constraints for semi-supervised multilabel classification. In contrast to previous works aiming for improving accuracy performance, we incorporate corpus-level constraints for reducing gender bias.\n\n[Section: 3 Visualizing and Quantifying Biases]",
      "referring_paragraphs": [
        "For example, in the first image of Figure 1, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking.",
        "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset.",
        "For example, one can represent the problem as an\n\nTable 1: Statistics for the two recognition problems.",
        "In this section, we provide details about the two visual recognition tasks we evaluated for bias: visual semantic role labeling (vSRL), and multi-label classification (MLC). We focus on gender, defining $G = \\{ \\mathrm { m a n } , \\mathrm { w o m a n } \\}$ and focus on the agent\n\nrole in vSRL, and any occurrence in text associated with the images in MLC. Problem statistics are summarized in Table 1. We also provide setup details for our calibration method."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg",
      "image_filename": "1707.09457_page0_fig1.jpg",
      "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.",
      "context_before": "imSitu is gender biased In Figure 2(a), along the x-axis, we show the male favoring bias of im-Situ verbs. Overall, the dataset is heavily biased toward male agents, with $6 4 . 6 \\%$ of verbs favoring a male agent by an average bias of 0.707 (roughly 3:1 male). Nearly half of verbs are extremely biased in the male or female direction: $4 6 . 9 5 \\%$ of verbs favor a gender with a bias of at least 0.7.6 Figure 2(a) contains several activity labels revealing problematic biases. For example, shopping, microwaving and washing are biased toward a female agent. Furthermore, several verbs such as driving, shooting, and coaching are heavily biased toward a male agent.\n\nTraining on imSitu amplifies bias In Figure 2(a), along the y-axis, we show the ratio of male agents $\\%$ of total people) in predictions on an unseen development set. The mean bias amplification in the development set is high, 0.050 on average, with $4 5 . 7 5 \\%$ of verbs exhibiting amplification. Biased verbs tend to have stronger amplification: verbs with training bias over 0.7 in either the male or female direction have a mean amplification of 0.072. Several already problematic biases have gotten much worse. For example, serving, only had a small bias toward females in the training set, 0.402, is now heavily biased toward females, 0.122. The verb tuning, originally heavily biased toward males, 0.878, now has exclusively male agents.\n\n[Section: 6.2 Multilabel Classification]\n\nMS-COCO is gender biased In Figure 2(b) along the x-axis, similarly to imSitu, we analyze bias of objects in MS-COCO with respect to males. MS-COCO is even more heavily biased toward men than imSitu, with $8 6 . 6 \\%$ of objects biased toward men, but with smaller average magnitude, 0.65. One third of the nouns are extremely biased toward males, $3 7 . 9 \\%$ of nouns favor men with a bias of at least 0.7. Some problematic examples include kitchen objects such as knife, fork, or spoon being more biased toward woman. Outdoor recreation related objects such tennis racket, snowboard and boat tend to be more biased toward men.",
      "context_after": "[Section: 6.3 Discussion]\n\nWe confirmed our hypothesis that (a) both the im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased and that (b) models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data. Furthermore, across both datasets, we showed that the degree of bias amplification was related to the size of the initial bias, with highly biased object and verb categories exhibiting more bias amplification. Our results demonstrate that care needs be taken in deploying such uncalibrated systems otherwise they could not only reinforce existing social bias but actually make them worse.\n\n[Section: 7 Calibration Results]",
      "referring_paragraphs": [
        "imSitu is gender biased In Figure 2(a), along the x-axis, we show the male favoring bias of im-Situ verbs.",
        "MS-COCO is gender biased In Figure 2(b) along the x-axis, similarly to imSitu, we analyze bias of objects in MS-COCO with respect to males.",
        "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC.",
        "We test our methods for reducing bias amplification in two problem settings: visual semantic role labeling in the imSitu dataset (vSRL) and multilabel image classification in MS-COCO (MLC). In all settings we derive corpus constraints using the training set and then run our calibration method in batch on either the development or testing set. Our results are summarized in Table 2 and Figure 3.",
        "Our quantitative results are summarized in the first two sections of Table 2.",
        "(%)</td></tr><tr><td colspan=\"4\">vSRL: Development Set</td></tr><tr><td>CRF</td><td>154</td><td>0.050</td><td>24.07</td></tr><tr><td>CRF + RBA</td><td>107</td><td>0.024</td><td>23.97</td></tr><tr><td colspan=\"4\">vSRL: Test Set</td></tr><tr><td>CRF</td><td>149</td><td>0.042</td><td>24.14</td></tr><tr><td>CRF + RBA</td><td>102</td><td>0.025</td><td>24.01</td></tr><tr><td colspan=\"4\">MLC: Development Set</td></tr><tr><td>CRF</td><td>40</td><td>0.032</td><td>45.27</td></tr><tr><td>CRF + RBA</td><td>24</td><td>0.022</td><td>45.19</td></tr><tr><td colspan=\"4\">MLC: Test Set</td></tr><tr><td>CRF</td><td>38</td><td>0.040</td><td>45.40</td></tr><tr><td>CRF + RBA</td><td>16</td><td>0.021</td><td>45.38</td></tr></table>\n\nTable 2: Number of violated constraints, mean amplified bias, and test performance before and after calibration using RBA.",
        "Our quantitative results on MS-COCO RBA are summarized in the last two sections of Table 2."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Bias analysis on imSitu vSRL",
        "(b) Bias analysis on MS-COCO MLC",
        "1707.09457_page0_fig2.jpg",
        "(a) Bias analysis on imSitu vSRL without RBA",
        "1707.09457_page0_fig3.jpg",
        "(b) Bias analysis on MS-COCO MLC without RBA",
        "1707.09457_page0_fig4.jpg",
        "(c) Bias analysis on imSitu vSRL with RBA",
        "1707.09457_page0_fig5.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig5.jpg"
        ],
        "panel_count": 5
      }
    },
    {
      "doc_id": "1707.09457",
      "figure_id": "1707.09457_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg",
      "image_filename": "1707.09457_page0_fig6.jpg",
      "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.",
      "context_before": "",
      "context_after": "[Section: 7.2 Multilabel Classification]\n\nOur quantitative results on MS-COCO RBA are summarized in the last two sections of Table 2. Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by $5 \\%$ , by $40 \\%$ (Viol.). Bias amplification was reduced by $3 1 . 3 \\%$ on the development set (Amp. bias). The underlying recognition system was evaluated by the standard measure: top-1 mean average precision, the precision averaged across object categories. Our calibration method results in a negligible loss in performance. In Figure 3(d), we demonstrate that we substantially reduce the distance between training bias and bias in the development set. Finally, in Figure 3(f) we demonstrate that we decrease bias amplification for all initial training bias settings. Results on the test set support our development results: we decrease bias amplification by $4 7 . 5 \\%$ (Amp. bias).\n\n[Section: 7.3 Discussion]",
      "referring_paragraphs": [
        "We test our methods for reducing bias amplification in two problem settings: visual semantic role labeling in the imSitu dataset (vSRL) and multilabel image classification in MS-COCO (MLC). In all settings we derive corpus constraints using the training set and then run our calibration method in batch on either the development or testing set. Our results are summarized in Table 2 and Figure 3.",
        "In Figure 3(c) we can see that the overall distance to the training set distribution after applying RBA decreased significantly, over $39 \\%$ .",
        "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC.",
        "In Figure 3(d), we demonstrate that we substantially reduce the distance between training bias and bias in the development set."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(d) Bias analysis on MS-COCO MLC with RBA",
        "(e) Bias in vSRL with (blue) / without (red) RBA",
        "1707.09457_page0_fig7.jpg",
        "(f) Bias in MLC with (blue) / without (red) RBA",
        "1707.09457_page0_fig8.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig8.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1709.02012": [
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig0.jpg",
      "image_filename": "1709.02012_page0_fig0.jpg",
      "caption": "",
      "context_before": "The setup of our framework most follows the Equalized Odds framework [19, 37]; however, we extend their framework for use with probabilistic classifiers. Let $P \\subset \\mathbb { R } ^ { k } \\times \\{ \\bar { 0 } , 1 \\}$ be the input space of a binary classification task. In our criminal justice example, $( \\mathbf { x } , y ) \\sim P$ represents a person, with $\\mathbf { x }$ representing the individual’s history and $y$ representing whether or not the person will commit another crime. Additionally, we assume the presence of two groups $G _ { 1 } , G _ { 2 } \\subset P$ , which represent disjoint population subsets, such as different races. We assume that the groups have different base rates $\\mu _ { t }$ , or probabilities of belonging to the positive class: $\\mu _ { 1 } = \\mathrm { P } _ { ( \\mathbf { x } , y ) \\sim G _ { 1 } } \\boldsymbol { \\left[ y ^ { \\cdot } = 1 \\right] } \\neq \\mathrm { P } _ { ( \\mathbf { x } , y ) \\sim G _ { 2 } } \\boldsymbol { \\left[ y = 1 \\right] } = \\mu _ { 2 }$ .\n\nFinally, let $h _ { 1 } , h _ { 2 } : \\mathbb { R } ^ { k }  [ 0 , 1 ]$ be binary classifiers, where $h _ { 1 }$ classifies samples from $G _ { 1 }$ and $h _ { 2 }$ classifies samples from $G _ { 2 }$ .2 Each classifier outputs the probability that a given sample $\\mathbf { x }$ belongs to the positive class. The notion of Equalized Odds non-discrimination is based on the false-positive and false-negative rates for each group, which we generalize here for use with probabilistic classifiers:\n\nDefinition 1. The generalized false-positive rate of classifier $h _ { t }$ for group $G _ { t }$ is $c _ { f p } ( h _ { t } ) \\ =$ $\\mathbb { E } _ { ( \\mathbf { x } , y ) \\sim G _ { t } } \\left[ h _ { t } ( \\mathbf { x } ) \\ \\mathrm { ~ \\left| ~ \\ \\right]} y \\ = \\ 0  \\right.$ . Similarly, the generalized false-negative rate of classifier $h _ { t }$ is $c _ { f n } ( h _ { t } ) = \\mathbb { E } _ { ( \\mathbf { x } , y ) \\sim G _ { t } } \\left[ ( 1 - h _ { t } ( \\mathbf { x } ) ) \\mid y { = } 1 \\right]$ .\n\nIf the classifier were to output either 0 or 1, this represents the standard notions of false-positive and false-negative rates. We now define the Equalized Odds framework (generalized for probabilistic classifiers), which aims to ensure that errors of a given type are not biased against any group.\n\nDefinition 2 (Probabilistic Equalized Odds). Classifiers $h _ { 1 }$ and $h _ { 2 }$ exhibit Equalized Odds for groups $G _ { 1 }$ and $G _ { 2 }$ if $c _ { f p } ( h _ { 1 } ) = c _ { f p } ( h _ { 2 } )$ and $c _ { f n } ( h _ { 1 } ) = c _ { f n } ( h _ { 2 } )$ .\n\nCalibration Constraints. As stated in the introduction, these two conditions do not necessarily prevent discrimination if the classifier predictions do not represent well-calibrated probabilities. Recall that calibration intuitively says that probabilities should carry semantic meaning: if there are 100 people in $G _ { 1 }$ for whom $h _ { 1 } ( \\mathbf { \\bar { x } } ) \\stackrel { - } { = } 0 . 6$ , then we expect 60 of them to belong to the positive class.\n\nDefinition 3. A classifier $h _ { t }$ is perfectly calibrated $i f \\forall p \\in [ 0 , 1 ] , \\operatorname { P } _ { ( \\mathbf { x } , y ) \\sim G _ { t } } \\left[ y = 1 \\mid h _ { t } ( \\mathbf { x } ) = p \\right] = p .$\n\nIt is commonly accepted amongst practitioners that both classifiers $h _ { 1 }$ and $h _ { 2 }$ should be calibrated with respect to groups $G _ { 1 }$ and $G _ { 2 }$ to prevent discrimination [4, 10, 12, 16]. Intuitively, this prevents the probability scores from carrying group-specific information. Unfortunately, Kleinberg et al. [26] (as well as [8], in a binary setting) prove that a classifier cannot achieve both calibration and Equalized Odds, even in an approximate sense, except in the most trivial of cases.\n\n[Section: 3.1 Geometric Characterization of Constraints]\n\nWe now will characterize the calibration and error-rate constraints with simple geometric intuitions. Throughout the rest of this paper, all of our results can be easily derived from this interpretation. We begin by defining the region of classifiers which are trivial, or those that output a constant value for all inputs (i.e. $h ^ { c } ( \\mathbf { x } ) = c$ , where $0 \\leq c \\leq 1$ is a constant). We can visualize these classifiers on a graph with generalized false-positive rates on one axis and generalized false-negatives on the other. It follows from the definitions of generalized false-positive/false-negative rates and calibration that all trivial classifiers $h$ lie on the diagonal defined by $c _ { f p } ( h ) + c _ { f n } ( h ) = 1$ (Figure 1a). Therefore, all classifiers that are “better than random” must lie below this diagonal in false-positive/false-negative space (the gray triangle in the figure). Any classifier that lies above the diagonal performs “worse than random,” as we can find a point on the trivial classifier diagonal with lower false-positive and false-negative rates.\n\nNow we will characterize the set of calibrated classifiers for groups $G _ { 1 }$ and $G _ { 2 }$ , which we denote as $\\mathcal { H } _ { 1 } ^ { \\ast }$ and $\\mathcal { H } _ { 2 } ^ { * }$ . Kleinberg et al. show that the generalized false-positive and false-negative rates of a calibrated classifier are linearly related by the base rate of the group:3",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Possible cal. classifiers $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ (blue/red).",
        "(b) Satisfying cal. and equal F.P. rates.",
        "1709.02012_page0_fig1.jpg"
      ],
      "quality_score": 0.35,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg",
      "image_filename": "1709.02012_page0_fig2.jpg",
      "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   \n(a) Level-order curves of cost. Low cost implies low error rates."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(c) Satisfying cal. and equal F.N. rates.",
        "(d) Satisfying cal. and a general constraint.",
        "1709.02012_page0_fig3.jpg",
        "1709.02012_page0_fig4.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig4.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg",
      "image_filename": "1709.02012_page0_fig5.jpg",
      "caption": "Figure 2: Calibration-Preserving Parity through interpolation.",
      "context_before": "",
      "context_after": "[Section: 4 Relaxing Equalized Odds to Preserve Calibration]\n\nIn this section, we show that a substantially simplified notion of Equalized Odds is compatible with calibration. We introduce a general relaxation that seeks to satisfy a single equal-cost constraint while maintaining calibration for each group $G _ { t }$ . We begin with the observation that Equalized\n\nOdds sets constraints to equalize false-positives $c _ { f p } ( h _ { t } )$ and false-negatives $c _ { f n } ( h _ { t } )$ . To capture and generalize this, we define a cost function $g _ { t }$ to be a linear function in $c _ { f p } ( h _ { t } )$ and $c _ { f n } ( h _ { t } )$ with arbitrary dependence on the group’s base rate $\\mu _ { t }$ . More formally, a cost function for group $G _ { t }$ is\n\nwhere $a _ { t }$ and $b _ { t }$ are non-negative constants that are specific to each group (and thus may depend on $\\mu _ { t }$ ): see Figure 1d. We also make the assumption that for any $\\mu _ { t }$ , at least one of $a _ { t }$ and $b _ { t }$ is nonzero, meaning $g _ { t } ( h _ { t } ) = 0$ if and only if $c _ { f p } ( \\bar { h } _ { t } ) = c _ { f n } ( h _ { t } ) \\stackrel { . } { = } \\stackrel { . } { 0 }$ .4 This class of cost functions encompasses a variety of scenarios. As an example, imagine an application in which the equal false-positive condition is essential but not the false-negative condition. Such a scenario may arise in our recidivism-prediction example, if we require that non-repeat offenders of any race are not disproportionately labeled as high risk. If we plot the set of calibrated classifiers $\\mathcal { H } _ { 1 } ^ { \\ast }$ and $\\mathcal { H } _ { 2 } ^ { * }$ on the false-positive/false-negative plane, we can see that ensuring the false-positive condition requires finding classifiers $h _ { 1 } \\in \\mathcal { H } _ { 1 } ^ { * }$ and $h _ { 2 } \\in \\mathcal { H } _ { 2 } ^ { * }$ that fall on the same vertical line (Figure 1b). Conversely, if we instead choose to satisfy only the false-negative condition, we would find classifiers $h _ { 1 }$ and $h _ { 2 }$ that fall on the same horizontal (Figure 1c). Finally, if both false-positive and false-negative errors incur a negative cost on the individual, we may choose to equalize a weighted combination of the error rates [3, 4, 8], which can be graphically described by the classifiers lying on a convex and negatively-sloped level set (Figure 1d). With these definitions, we can formally define our relaxation:\n\nDefinition 4 (Relaxed Equalized Odds with Calibration). Given a cost function $g _ { t }$ of the form in (2), classifiers $h _ { 1 }$ and $h _ { 2 }$ achieve Relaxed Equalized Odds with Calibration for groups $G _ { 1 }$ and $G _ { 2 }$ if both classifiers are calibrated and satisfy the constraint $g _ { 1 } ( h _ { 1 } ) = g _ { 2 } ( h _ { 2 } )$ .\n\nIt is worth noting that, for calibrated classifiers, an increase in cost strictly corresponds to an increase in both the false-negative and false-positive rate. This can be interpreted graphically, as the level-order cost curves lie further away from the origin as cost increases (Figure 2a). In other words, the cost function can always be used as a proxy for either error rate.5\n\nFeasibility. It is easy to see that Definition 4 is always satisfiable – in Figures 1b, 1c, and 1d we see that there are many such solutions that would lie on a given level-order cost curve while maintaining calibration, including the case in which both classifiers are perfect. In practice, however, not all classifiers are achievable. For the rest of the paper, we will assume that we have access to “optimal” (but possibly discriminatory) calibrated classifiers $h _ { 1 }$ and $h _ { 2 }$ such that, due to whatever limitations there are on the predictability of the task, we are unable to find other classifiers that have lower cost with respect to $g _ { t }$ . We allow $h _ { 1 }$ and $h _ { 2 }$ to be learned in any way, as long as they are calibrated. Without loss of generality, for the remainder of the paper, we will assume that $g _ { 1 } ( h _ { 1 } ) \\ge g _ { 2 } ( h _ { 2 } )$ .\n\nSince by assumption we have no way to find a classifier for $G _ { 1 }$ with lower cost than $h _ { 1 }$ , our goal is therefore to find a classifier $\\tilde { h } _ { 2 }$ with cost equal to $h _ { 1 }$ . This pair of classifiers would represent the lowest cost (and therefore optimal) set of classifiers that satisfies calibration and the equal cost constraint. For a given base rate $\\mu _ { t }$ and value of the cost function $g _ { t }$ , a calibrated classifier’s position in the generalized false-positive/false-negative plane is uniquely determined (Figure 2a). This is because each level-order curve of the cost function $g _ { t }$ has negative slope in this plane, and each level order curve only intersects a group’s calibrated classifier line once. In other words, there is a unique solution in the false-positive/false-negative plane for classifier $\\tilde { h } _ { 2 }$ (Figure 2b).\n\nConsider the range of values that $g _ { t }$ can take. As noted above, $g _ { t } ( h _ { t } ) \\geq 0$ , with equality if and only if $h _ { t }$ is the perfect classifier. On the other hand, the trivial classifier (again, which outputs the constant $\\mu _ { t }$ for all inputs) is the calibrated classifier that achieves maximum cost for any $g _ { t }$ (see Lemma 3 in Section S2). As a result, the cost of a classifier for group $G _ { t }$ is between 0 and $g _ { t } ( h ^ { \\mu _ { t } } )$ . This naturally leads to a characterization of feasibility: Definition 4 can be achieved if and only if $h _ { 1 }$ incurs less cost than group $G _ { 2 }$ ’s trivial classifier $h ^ { \\mu _ { 2 } }$ ; i.e. if $g _ { 1 } ( h _ { 1 } ) \\leq g _ { 2 } ( h ^ { \\mu _ { 2 } } )$ . This can be seen graphically in Figure 2c, in which the level-order curve for $g _ { 1 } ( h _ { 1 } )$ does not intersect the set of calibrated classifiers for $G _ { 2 }$ . Since, by assumption, we cannot find a calibrated classifier for $G _ { 1 }$ with strictly smaller cost than $h _ { 1 }$ , there is no feasible solution. On the other hand, if $h _ { 1 }$ incurs less cost than $h ^ { \\mu _ { 2 } }$ , then we will show feasibility by construction with a simple algorithm.\n\nAn Algorithm. While it may be possible to encode the constraints of Definition 4 into the training procedure of $h _ { 1 }$ and $h _ { 2 }$ , it is not immediately obvious how to do so. Even naturally probabilistic\n\nalgorithms, such as logistic regression, can become uncalibrated in the presence of optimization constraints (as is the case in [37]). It is not straightforward to encode the calibration constraint if the probabilities are assumed to be continuous, and post-processing calibration methods [31, 35] would break equal-cost constraints by modifying classifier scores. Therefore, we look to achieve the calibrated Equalized Odds relaxation by post-processing existing calibrated classifiers.\n\nAgain, given $h _ { 1 }$ and $h _ { 2 }$ with $g _ { 1 } ( h _ { 1 } ) \\ge g _ { 2 } ( h _ { 2 } )$ , we want to arrive at a calibrated classifier $\\tilde { h } _ { 2 }$ for group $G _ { 2 }$ such that $g _ { 1 } ( h _ { 1 } ) = g _ { 2 } ( \\tilde { h } _ { 2 } )$ . Recall that, under our assumptions, this would be the best possible solution with respect to classifier cost. We show that this cost constraint can be achieved by withholding predictive information for a randomly chosen subset of group $G _ { 2 }$ . In other words, rather than always returning $h _ { 2 } ( \\mathbf { x } )$ for all samples, we will occasionally return the group’s mean probability (i.e. the output of the trivial classifier $h ^ { \\mu _ { 2 } }$ ). In Lemma 4 in Section S2, we show that if",
      "referring_paragraphs": [
        "Figure 2: Calibration-Preserving Parity through interpolation."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) Usually, there is a calibrated classifier $\\tilde { h } _ { 2 }$ with the same cost of $h _ { 1 }$ .",
        "(c) Cal. and equal-cost are incompatible if $h _ { 1 }$ has high error.",
        "1709.02012_page0_fig6.jpg",
        "(d) Possible cal. classifiers for $G _ { 2 }$ (bold red) by mixing $h _ { 2 }$ and $h ^ { \\mu _ { 2 } }$ .",
        "1709.02012_page0_fig7.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig7.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1709.02012",
      "figure_id": "1709.02012_fig_9",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg",
      "image_filename": "1709.02012_page0_fig8.jpg",
      "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.",
      "context_before": "then the cost of $\\tilde { h } _ { 2 }$ is a linear interpolation between the costs of $h _ { 2 }$ and $h ^ { \\mu _ { 2 } }$ (Figure 2d). More formally, we have that $g _ { 2 } ( \\tilde { h } _ { 2 } ) = ( 1 - \\alpha ) g _ { 2 } ( h _ { 2 } ) + \\alpha g _ { 2 } ( h ^ { \\mu _ { 2 } } ) )$ , and thus setting $\\begin{array} { r } { \\alpha = \\frac { \\ J _ { 1 } \\left( h _ { 1 } \\right) - g _ { 2 } \\left( h _ { 2 } \\right) } { g _ { 2 } \\left( h ^ { \\mu _ { 2 } } \\right) - g _ { 2 } \\left( h _ { 2 } \\right) } } \\end{array}$ ensures that $g _ { 2 } ( { \\tilde { h } } _ { 2 } ) = g _ { 1 } ( h _ { 1 } )$ as desired (Figure 2b). Moreover, this randomization preserves calibration (see Section S4). Algorithm 1 summarizes this method.\n\n[Section: Algorithm 1 Achieving Calibration and an Equal-Cost Constraint via Information Withholding]\n\nInput: classifiers $h _ { 1 }$ and $h _ { 2 }$ s.t. $g _ { 2 } ( h _ { 2 } ) \\leq g _ { 1 } ( h _ { 1 } ) \\leq g _ { 2 } ( h ^ { \\mu _ { 2 } } )$ , holdout set $P _ { v a l i d }$\n\n• Determine base rate $\\mu _ { 2 }$ of $G _ { 2 }$ (using $P _ { v a l i d } )$ to produce trivial classifier $h ^ { \\mu _ { 2 } }$ .   \n• Construct $\\tilde { h } _ { 2 }$ using with $\\begin{array} { r } { \\alpha = \\frac { g _ { 1 } ( h _ { 1 } ) - g _ { 2 } ( h _ { 2 } ) } { g _ { 2 } ( h ^ { \\mu _ { 2 } } ) - g _ { 2 } ( h _ { 2 } ) } } \\end{array}$ , where $\\alpha$ is the interpolation parameter.\n\nreturn h1, h˜2 — which are calibrated and satisfy $g _ { 1 } ( h _ { 1 } ) = g _ { 2 } ( \\tilde { h } _ { 2 } )$\n\nImplications. In a certain sense, Algorithm 1 is an “optimal” method because it arrives at the unique false-negative/false-positive solution for $\\tilde { h } _ { 2 }$ , where $\\tilde { h } _ { 2 }$ is calibrated and has cost equal to $h _ { 1 }$ . Therefore (by our assumptions) we can find no better classifiers that satisfy Definition 4. This simple result has strong consequences, as the tradeoffs to satisfy both calibration and the equal-cost constraint are often unsatisfactory — both intuitively and experimentally (as we will show in Section 5).\n\nWe find two primary objections to this solution. First, it equalizes costs simply by making a classifier strictly worse for one of the groups. Second, it achieves this cost increase by withholding information on a randomly chosen population subset, making the outcome inequitable within the group (as measured by a standard measure of inequality like the Gini coefficient). Due to the optimality of the algorithm, the former of these issues is unavoidable in any solution that satisfies Definition 4. The latter, however, is slightly more subtle, and brings up the question of individual fairness (what guarantees we would like an algorithm to make with respect to each individual) and how it interacts with group fairness (population-level guarantees). While this certainly is an important issue for future work, in this particular setting, even if one could find another algorithm that distributes the burden of additional cost more equitably, any algorithm will make at least as many false-positive/false-negative errors as Algorithm 1, and these misclassifications will always be tragic to the individuals whom they affect. The performance loss across the entire group is often significant enough to make this combination of constraints somewhat worrying to use in practice, regardless of the algorithm.\n\nImpossibility of Satisfying Multiple Equal-Cost Constraints. It is natural to argue there might be multiple cost functions that we would like to equalize across groups. However, satisfying more than one distinct equal-cost constraint (i.e. different curves in the F.P./F.N. plane) is infeasible.\n\nTheorem 1 (Generalized impossibility result). Let $h _ { 1 }$ and $h _ { 2 }$ be calibrated classifiers for $G _ { 1 }$ and $G _ { 2 }$ with equal cost with respect to $g _ { t }$ . If $\\mu _ { 1 } \\neq \\mu _ { 2 }$ , and if $h _ { 1 }$ and $h _ { 2 }$ also have equal cost with respect to a different cost function $g _ { t } ^ { \\prime }$ , then $h _ { 1 }$ and $h _ { 2 }$ must be perfect classifiers.\n\n(Proof in Section S5). Note that this is a generalization of the impossibility result of [26]. Furthermore, we show in Theorem 9 (in Section S5) that this holds in an approximate sense: if calibration and multiple distinct equal-cost constraints are approximately achieved by some classifier, then that classifier must have approximately zero generalized false-positive and false-negative rates.",
      "context_after": "[Section: 5 Experiments]\n\nIn light of these findings, our goal is to understand the impact of imposing calibration and an equalcost constraint on real-world datasets. We will empirically show that, in many cases, this will result in performance degradation, while simultaneously increasing other notions of disparity. We perform experiments on three datasets: an income-prediction, a health-prediction, and a criminal recidivism dataset. For each task, we choose a cost function within our framework that is appropriate for the given scenario. We begin with two calibrated classifiers $h _ { 1 }$ and $h _ { 2 }$ for groups $G _ { 1 }$ and $G _ { 2 }$ . We assume that these classifiers cannot be significantly improved without more training data or features. We then derive $\\tilde { h } _ { 2 }$ to equalize the costs while maintaining calibration. The original classifiers are trained on a portion of the data, and then the new classifiers are derived using a separate holdout set. To compare against the (uncalibrated) Equalized Odds framework, we derive F.P./F.N. matching classifiers using the post-processing method of [19] (EO-Derived). On the criminal recidivism dataset, we additionally learn classifiers that directly encode the Equalized Odds constraints, using the methods of [37] (EO-Trained). (See Section S6 for detailed training and post-processing procedures.) We visualize model error rates on the generalized F.P. and F.N. plane. Additionally, we plot the calibrated classifier lines for $G _ { 1 }$ and $G _ { 2 }$ to visualize model calibration.\n\nIncome Prediction. The Adult Dataset from UCI Machine Learning Repository [28] contains 14 demographic and occupational features for various people, with the goal of predicting whether a person’s income is above $\\$ 50$ , 000. In this scenario, we seek to achieve predictions with equalized cost across genders ( $G _ { 1 }$ represents women and $G _ { 2 }$ represents men). We model a scenario where the primary concern is ensuring equal generalized F.N. rates across genders, which would, for example, help job recruiters prevent gender discrimination in the form of underestimated salaries. Thus, we choose our cost constraint to require equal generalized F.N. rates across groups. In Figure 3a, we see that the original classifiers $h _ { 1 }$ and $h _ { 2 }$ approximately lie on the line of calibrated classifiers. In the left plot (EO-Derived), we see that it is possible to (approximately) match both error rates of the classifiers at the cost of $h _ { 1 } ^ { e o }$ deviating from the set of calibrated classifiers. In the right plot, we see that it is feasible to equalize the generalized F.N. rates while maintaining calibration. $h _ { 1 }$ and $\\tilde { h } _ { 2 }$ lie on the same level-order curve of $g _ { t }$ (represented by the dashed-gray line), and simultaneously remain on the “line” of calibrated classifiers. It is worth noting that achieving either notion of non-discrimination requires some cost to at least one of the groups. However, maintaining calibration further increases the difference in F.P. rates between groups. In some sense, the calibrated framework trades off one notion of disparity for another while simultaneously increasing the overall error rates.\n\nHealth Prediction. The Heart Dataset from the UCI Machine Learning Repository contains 14 processed features from 906 adults in 4 geographical locations. The goal of this dataset is to accurately predict whether or not an individual has a heart condition. In this scenario, we would like to reduce disparity between middle-aged adults $( G _ { 1 } )$ and seniors $\\left( G _ { 2 } \\right)$ . In this scenario, we consider F.P. and F.N. to both be undesirable. A false prediction of a heart condition could result in unnecessary medical attention, while false negatives incur cost from delayed treatment. We therefore utilize the following cost function $g _ { t } ( h _ { t } ) = \\bar { r _ { f p } } h _ { t } ( \\mathbf { x } ) \\left( 1 - y \\right) + r _ { f n } \\left( 1 - h _ { t } ( \\mathbf { x } ) \\right) y$ , which essentially assigns a weight to both F.N. and F.P. predictions. In our experiments, we set $r _ { f p } = 1$ and $r _ { f n } = 3$ . In the right plot of Figure 3b, we can see that the level-order curves of the cost function form a curved line in the generalized F.P./F.N. plane. Because our original classifiers lie approximately on the same level-order curve, little change is required to equalize the costs of $h _ { 1 }$ and $\\tilde { h } _ { 2 }$ while maintaining calibration. This is the only experiment in which the calibrated framework incurs little additional cost, and therefore could be considered a viable option. However, it is worth noting that, in this example, the equal-cost constraint does not explicitly match either of the error types, and therefore the two groups will in expectation experience different types of errors. In the left plot of Figure 3b (EO-Derived), we see that it is alternatively feasible to explicitly match both the F.P. and F.N. rates while sacrificing calibration.\n\nCriminal Recidivism Prediction. Finally, we examine the frameworks in the context of our motivating example: criminal recidivism. As mentioned in the introduction, African Americans $( G _ { 1 } )$ receive a disproportionate number of F.P. predictions as compared with Caucasians $\\left( G _ { 2 } \\right)$ when automated risk tools are used in practice. Therefore, we aim to equalize the generalized F.P. rate. In this experiment, we modify the predictions made by the COMPAS tool [12], a risk-assessment tool used in practice by the American legal system. Additionally, we also see if it is possible to improve the classifiers with training-time Equalized Odds constraints using the methods of Zafar et al. [37] (EO-Trained). In Figure 3c, we first observe that the original classifiers $h _ { 1 }$ and $h _ { 2 }$ have large generalized F.P. and F.N. rates. Both methods of achieving Equalized Odds — training constraints (left plot) and postprocessing (middle plot) match the error rates while sacrificing calibration. However, we observe that, assuming $h _ { 1 }$ and $h _ { 2 }$ cannot be improved, it is infeasible to achieve the calibrated relaxation (Figure 3c right). This is an example where matching the F.P. rate of $h _ { 1 }$ would require a classifier worse than the trivial classifier $h ^ { \\mu _ { 2 } }$ . This example therefore represents an instance in which calibration is completely incompatible with any error-rate constraints. If the primary concern of criminal justice practitioners is calibration [12, 16], then there will inherently be discrimination in the form of F.P. and F.N. rates. However, if the Equalized Odds framework is adopted, the miscalibrated risk scores inherently cause discrimination to one group, as argued in the introduction. Therefore, the most meaningful change in such a setting would be an improvement to $h _ { 2 }$ (the classifier for African Americans) either through the collection of more data or the use of more salient features. A reduction in overall error to the group with higher cost will naturally lead to less error-rate disparity.\n\n[Section: 6 Discussion and Conclusion]",
      "referring_paragraphs": [
        "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Income Prediction.",
        "(b) Health Prediction.",
        "1709.02012_page0_fig9.jpg",
        "(c) Recidivism Prediction.",
        "1709.02012_page0_fig10.jpg",
        "1709.02012_page0_fig11.jpg",
        "1709.02012_page0_fig12.jpg",
        "1709.02012_page0_fig13.jpg",
        "1709.02012_page0_fig14.jpg",
        "1709.02012_page0_fig15.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig15.jpg"
        ],
        "panel_count": 8
      }
    }
  ],
  "1710.08615": [
    {
      "doc_id": "1710.08615",
      "figure_id": "1710.08615_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg",
      "image_filename": "1710.08615_page0_fig0.jpg",
      "caption": "",
      "context_before": "Kristina Lerman\n\nReceived: date / Accepted: date\n\nAbstract Observational data about human behavior is often heterogeneous, i.e., generated by subgroups within the population under study that vary in size and behavior. Heterogeneity predisposes analysis to Simpson’s paradox, whereby the trends observed in data that has been aggregated over the entire population may be substantially different from those of the underlying subgroups. I illustrate Simpson’s paradox with several examples coming from studies of online behavior and show that aggregate response leads to wrong conclusions about the underlying individual behavior. I then present a simple method to test whether Simpson’s paradox is affecting results of analysis. The presence of Simpson’s paradox in social data suggests that important behavioral differences exist within the population, and failure to take these differences into account can distort the studies’ findings.\n\n[Section: 1 Introduction]\n\nThe landscape of social science changed dramatically in the 21st century, when large volumes of social and behavioral data created the field of computational social science [16]. While the bulk of the data is now digital traces of online behaviors, the accelerating instrumentation of our physical spaces is opening offline behaviors to analysis. The new data has vastly expanded the opportunities for discovery in the social sciences [18]. Algorithms have mined behavioral data to validate theories of individual decision-making and social interaction [13,5] and produce new insights into first principles of human behavior. These insights help to better explain and predict human behavior, and eventually even help policy makers devise more effective interventions to improve wellbeing by steering behaviors towards desirable outcomes, for example, by fostering behaviors that promote healthy habits, reduce substance abuse and social isolation, improve learning, etc.\n\nComputational social scientists, however, are facing challenges, some of which were rarely encountered by past researchers. Although behavioral data is usually massive, it is also often extremely sparse and noisy at the individual level. To uncover hidden patterns, scientists might choose to aggregate data over the entire population. For example, diurnal cycles of mood (cf Fig. 1 in [8]) and online activity (cf Fig. 2 in [12]) only become apparent once the activity of tens of thousands or even millions of people is aggregated. In the past, when behavioral data came from populations that were carefully crafted to address specific research questions [18], aggregation helped improve signal-to-noise ratio and uncover weak effects. Today, however, the same strategy can lead researchers to wrong conclusions. The reason for this is that current behavioral data is highly heterogeneous: it is collected from subgroups that vary widely in size and behavior. Heterogeneity is evident in practically all social data sets and can be easily recognized by its hallmark, the long-tailed distribution. The prevalence of some trait in these systems, whether the number of followers in an online social network, or the number of words used in an email, can vary by many orders of magnitude, making it difficult to compare users with small values of the trait to those with large values. As shown in this paper, heterogeneity can dramatically distort conclusions of analysis.\n\nSimpson’s paradox [4, 19] is one important phenomenon confounding analysis of heterogeneous social data. According to the paradox, an association observed in data that has been aggregated over an entire population may be quite different from, and even opposite to, associations found in the underlying subgroups. A notorious example of Simpson’s paradox comes from the gender bias lawsuit against UC Berkeley [3]. Analysis of graduate school admissions data seemingly revealed a statistically significant bias against women: a smaller fraction of female applicants were admitted for graduate studies. However, when admissions data was disaggregated by department, women had parity and even a slight advantage over men in some departments. The paradox arose because departments preferred by female applicants have lower admissions rates for both genders.\n\nSimpson’s paradox also affects analysis of trends. When measuring how an outcome variable changes as a function of some independent variable, the characteristics of the population over which the trend is measured may change with the independent variable. As a result, the data may appear to exhibit a trend, which disappears or reverses when the data is disaggregated by subgroups [1]. Vaupel and Yashin [23] give several illustrations of this effect. For example, a study of recidivism among convicts released from prison showed that the rate at which they return to prison declines over time. From this, policy makers concluded that age has a pacifying effect, with older convicts less likely to commit crimes. In reality, this is not the case. Instead, the population of ex-convicts is composed of two subgroups with nearly constant, but very different recidivism rates. The first subgroup—the “reformed”—will never commit a crime once released from prison. The other subgroup—the “incorrigibles”—are highly likely commit a crime. Over time, as “incorrigibles” commit offenses and return to prison, there are fewer of them left in the population. Survivor bias changes the composition of the population, creating an illusion of an overall decline in recidivism. As Vaupel and Yashin warn, “unsuspecting researchers who are not wary of heterogeneity’s ruses may fallaciously assume that observed patterns for the population as a whole also hold on the sub-population or individual level.”",
      "context_after": "Fig. 2 Rate of content consumption during a session. Average time spent viewing each item in a social feed appears to increase over the course of a session when looking at all the data (a) but decreases within sessions of the same length (b). This indicates that users speed up near the end of the session, taking less and less time to view each item.\n\nresponsive users (with fewer friends) quickly drop out of analysis (since they are generally exposed fewer times), leaving only the highly connected, but less responsive users behind. Their reduced susceptibility biases aggregate response, leading to wrong conclusions about individual behavior. Once data is disaggregated based on the volume of information individuals receive [20], a clearer pattern of response emerges, one that is more predictive of behavior [11].\n\nContent Consumption in Social Media. A study of content consumption on a popular social networking site Facebook examined the time users devote to viewing each item in their social feed [15]. The study segmented each user’s activity into sessions, defined as sequences of activity without a prolonged break (see Fig. 4 for an explanation). At a population level, it looks as if users slow down over the course of a session, taking more and more time to view each item (Fig. 2 (a)). However, when looking at user activity within sessions of the same length, e.g., sessions that are 30 minutes long, it appears that individuals speed up instead (Fig. 2 (b)). As the session progresses, they spend less and less time viewing each item, which suggests that they begin to skim posts.\n\nThe difference in trends arises because users who have longer sessions also tend to spend more time viewing each item in their feed. When calculating how long users view items as a function of time, the faster users drop out of analysis of aggregate data, leaving the slower, users who tend to have longer sessions. Therefore, stratifying data by session length removes the confounding factor and allows us to study behavior within a similar cohort.\n\nAnswer Quality on Stack Exchange. Stack Exchange is a popular question-answering platform where users ask and answer questions. Askers can also “accept” an answer as the best answer to their question. A study of dynamics of user performance on Stack Exchange found that answer quality, as measured by the probability that it will be accepted by the asker as the best answer, declines steadily over the course of a session, with each successive answer written by a user ever less likely to get accepted [7]. However, this trend is seen only when comparing sessions of the same length, for example, sessions where exactly four answers were written (Fig. 3 (b)). When calculating answer acceptance probability over all the data, it looks as though answers written later in a session are more likely to get accepted",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1710.08615_page0_fig1.jpg",
        "(a)",
        "1710.08615_page0_fig2.jpg",
        "(b)",
        "1710.08615_page0_fig3.jpg",
        "1710.08615_page0_fig4.jpg",
        "(a)",
        "(b)",
        "1710.08615_page0_fig5.jpg",
        "1710.08615_page0_fig6.jpg",
        "1710.08615_page0_fig7.jpg",
        "(a)",
        "(b)",
        "1710.08615_page0_fig8.jpg",
        "1710.08615_page0_fig9.jpg",
        "(a)",
        "(b)",
        "1710.08615_page0_fig10.jpg",
        "1710.08615_page0_fig11.jpg"
      ],
      "quality_score": 0.5,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig11.jpg"
        ],
        "panel_count": 12
      }
    }
  ],
  "1710.11214": [
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg",
      "image_filename": "1710.11214_page0_fig0.jpg",
      "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.",
      "context_before": "Recommendation systems; algorithmic confounding.\n\n[Section: 1 INTRODUCTION]\n\nRecommendation systems are designed to help people make decisions. These systems are commonly used on online platforms for video, music, and product purchases through service providers such as Netflix, Pandora, and Amazon. Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1). While the broad notion of confounding from the data collection process has been studied extensively, we seek to characterize the impact of this feedback loop in the context of recommendation systems, demonstrating the unintended consequences of algorithmic confounding. As recommendation systems become increasingly important in decision-making, we have an ethical responsibility to understand the idiosyncrasies of these systems and consider their implications for individual and societal welfare [18].\n\nIndividual decisions can aggregate to have broad political and economic consequences. Recommendation systems can influence how users perceive the world by filtering access to media; pushing political dialog towards extremes [59] or filtering out contrary opinions [25]. Even more gravely, these systems impact crucial decision-making processes, such as loan approvals, criminal profiling, and medical interventions. As recommendation systems shape access to goods and resources, issues of fairness and transparency need to be considered. For example, if a distinct group of minoritypreference users exists, a system may deferentially undermine the",
      "context_after": "[Section: 2 CONSEQUENCES OF THE FEEDBACK LOOP]\n\nReal-world recommendation systems are often part of a feedback loop (figure 1): the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system. We attempt to characterize the impact of this feedback loop through three claims.\n\nThe recommendation feedback loop causes homogenization of user behavior (section 5.2), which is amplified with more cycles through the loop. Homogenization occurs at both a population level (all users behave more similarly) and at an individual level (each user behaves more like its nearest neighbors).\n\nUsers experience losses in utility due to homogenization effects; these losses are distributed unequally (section 5.3). Across multiple recommendation system types, users with lower relative utility have higher homogenization, indicating that homogenization can be detrimental for subsets of users by encouraging them to make sub-optimal choices.\n\nThe feedback loop amplifies the impact of recommendation systems on the distribution of item consumption (section 5.4), irrespective of homogenization effects. Two recommendation systems can produce similar amounts of user homogenization with different distributions of item consumption.\n\n[Section: 3 RELATED WORK]",
      "referring_paragraphs": [
        "Live systems are updated or retrained regularly to incorporate new data that was influenced by the recommendation system itself, forming a feedback loop (figure 1).",
        "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems.",
        "Real-world recommendation systems are often part of a feedback loop (figure 1): the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system.",
        "[52] note that users introduce selection bias; this occurs during the interaction component of the feedback loop shown in figure 1.",
        "We consider two cases of observing user interactions with items: a simple case where each recommendation algorithm is trained once, and a more complicated case of repeated training; this allows us to compare a single cycle of the feedback loop (figure 1) to multiple cycles.",
        "As the number of cycles in the feedback loop (figure 1) increases, we observe homogenization effects continue to increase without corresponding increases in utility."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg",
      "image_filename": "1710.11214_page0_fig1.jpg",
      "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.",
      "context_before": "[Section: 5.1 Simulation Procedure]\n\nWe consider six recommendation algorithms: popularity, content filtering (“content”), matrix factorization (“MF”), social filtering (“social”), random, and ideal. Appendix A provides further details on the first four approaches and describes our general recommendation framework. The core idea of this framework is that each recommendation system provides some underlying score $s _ { u i } ( t )$ of how much a user $u$ uiwill enjoy item  at time . These scores are constructed using user preferences $\\theta _ { u } ( t )$ and item attributes $\\beta _ { i } ( t )$ :\n\nand each recommendation approach has a different way of constructing or modeling these preferences and attributes.\n\nFor our simulations, all of the six approaches recommend from the set of items that exist in the system at the time of training; random recommends these items in random order. Ideal recommends items for each user $u$ based on the user’s true utility $V _ { u i }$ for those u Vuiitems. Comparison with these two approaches minimizes the impact of the interaction model assumptions (section 4) on our results.\n\nIn all of our simulations, a community consists of 100 users and is run for 1,000 time intervals with ten new items being introduced at each interval; each simulation is repeated with ten random seeds and all our results are averages over these ten “worlds.” We generate the distributions of user preference and item attribute popularity, as used in equation (3), in $K = 2 0$ dimensions; we generate uneven user preferences, but approximately even item attributes. The user preference parameter is generated as follows: ˜ ∼ Dirichlet(1) and $\\mu _ { \\rho } = 1 0 \\cdot \\tilde { \\mu } _ { \\rho }$ ρ. This mirrors the real world where preferences are ρ ρunevenly distributed, which allows us to expose properties of the recommendation algorithms. Item attribute popularity is encouraged to be more even in aggregate, but still be sparse for individual items; we draw: ˜ ∼ Dirichlet(100) and $\\mu _ { \\alpha } = 0 . 1 \\cdot \\tilde { \\mu } _ { \\alpha }$ . While item µα µα . µαattributes are not evenly distributed in the real world, this ensures that all users will be able to find items that match their preferences. With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior. We generate social networks (used by social filtering only) using the covariance matrix of user preferences; we impose that each user must have at least one network connection and binarize the covariance matrix using this criteria. This procedure enforces homophily between connected users, which is generally (but not always) true in the real world.\n\nWe consider two cases of observing user interactions with items: a simple case where each recommendation algorithm is trained once, and a more complicated case of repeated training; this allows us to compare a single cycle of the feedback loop (figure 1) to multiple cycles. In the simple paradigm, we run 50 iterations of “start-up” (new items only each iteration), train the algorithms, and then observe 50 iterations of confounded behavior. In the second paradigm, we have ten iterations of “start-up,” then train the algorithms every iteration for the remaining 90 iterations using all previous data.",
      "context_after": "[Section: 5.2 Homogenization Effects]\n\nRecommendation systems may not change the underlying preferences of user (especially not when used on short time scales), but they do impact user behavior, or the collection of items with which users interact. Recommendation algorithms encourage similar users to interact with the same set of items, therefore homogenizing their behavior, relative to the same platform without recommended content. For example, Popularity-based systems represent all users in the same way; this homogenizes all users, as seen in previous work [11, 58]. Social recommendation systems homogenize connected users or within cliques, and matrix factorization homogenizes users along learned latent factors.\n\nHomogenizing effects are not inherently bad as they indicate that the models are learning patterns from the data, as intended; when achieving optimum utility, users will have some degree of homogenization. However, homogenization of user behavior does not correspond directly with an increase in utility: we can observe an increase in homogenization without a corresponding increase in utility. This is related to the explore/exploit paradigm, where we wish to exploit the user representation to maximize utility, but not to homogenize users more than necessary. When a representation of users is over-exploited, users are being pushed to be have more similar behaviors than their underlying preferences would optimally dictate. This suggests that the “tyranny of majority” and niche “echo chamber” effects may both be manifestations of the same problem: over-exploitation of recommendation models. While concerns about maximizing utility are not new to the recommendation system literature, there are also grave social consequences from homogenization that have received less consideration.\n\nWe measure homogenization of behavior by first pairing each user with their most similar user according to the recommendation system, or user $u$ is partnered with user $^ { v }$ that maximizes the ucosine similarity of $\\theta _ { u }$ and $\\theta _ { v }$ v. Next, we compute the Jaccard index θu θvof the sets of observed items for these users. If at time $t$ , user $u$ has interacted with a set if items ${ \\mathcal { D } } _ { u } ( t ) = \\{ i _ { u } ( 1 ) , \\ldots , i _ { u } ( t ) \\}$ uthe Jaccard u t iu , . . . , iu tindex of the two users’ interactions can be written as\n\nWe compared the Jaccard index for paired users against the Jaccard index of the same users exposed to ideal recommendations; this difference captures how much the behavior has homogenized relative",
      "referring_paragraphs": [
        "With these settings for user preferences and item attributes, the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior.",
        "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig2.jpg",
      "image_filename": "1710.11214_page0_fig2.jpg",
      "caption": "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.",
      "context_before": "[Section: 5.2 Homogenization Effects]\n\nRecommendation systems may not change the underlying preferences of user (especially not when used on short time scales), but they do impact user behavior, or the collection of items with which users interact. Recommendation algorithms encourage similar users to interact with the same set of items, therefore homogenizing their behavior, relative to the same platform without recommended content. For example, Popularity-based systems represent all users in the same way; this homogenizes all users, as seen in previous work [11, 58]. Social recommendation systems homogenize connected users or within cliques, and matrix factorization homogenizes users along learned latent factors.\n\nHomogenizing effects are not inherently bad as they indicate that the models are learning patterns from the data, as intended; when achieving optimum utility, users will have some degree of homogenization. However, homogenization of user behavior does not correspond directly with an increase in utility: we can observe an increase in homogenization without a corresponding increase in utility. This is related to the explore/exploit paradigm, where we wish to exploit the user representation to maximize utility, but not to homogenize users more than necessary. When a representation of users is over-exploited, users are being pushed to be have more similar behaviors than their underlying preferences would optimally dictate. This suggests that the “tyranny of majority” and niche “echo chamber” effects may both be manifestations of the same problem: over-exploitation of recommendation models. While concerns about maximizing utility are not new to the recommendation system literature, there are also grave social consequences from homogenization that have received less consideration.\n\nWe measure homogenization of behavior by first pairing each user with their most similar user according to the recommendation system, or user $u$ is partnered with user $^ { v }$ that maximizes the ucosine similarity of $\\theta _ { u }$ and $\\theta _ { v }$ v. Next, we compute the Jaccard index θu θvof the sets of observed items for these users. If at time $t$ , user $u$ has interacted with a set if items ${ \\mathcal { D } } _ { u } ( t ) = \\{ i _ { u } ( 1 ) , \\ldots , i _ { u } ( t ) \\}$ uthe Jaccard u t iu , . . . , iu tindex of the two users’ interactions can be written as\n\nWe compared the Jaccard index for paired users against the Jaccard index of the same users exposed to ideal recommendations; this difference captures how much the behavior has homogenized relative",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3: Change in Jaccard index of user behavior relative to ideal behavior; users paired by cosine similarity of . On the left, mild homogenization of behavior occurs soon after a single training, but then diminishes. On the right, recommendation systems that include repeated training homogenize user behavior more than is needed for ideal utility.",
        "Figure 3 shows these results for both the single training and the repeated training cases."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg",
      "image_filename": "1710.11214_page0_fig3.jpg",
      "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.",
      "context_before": "",
      "context_after": "[Section: 5.3 Loss of Utility]\n\nChanges in utility due to these effects are not necessarily born equally across all users. For example, users whose true preferences are not captured well by the low dimensional representation of user preferences may be disproportionately impacted. These minority users may see lesser improvements or even decreases in utility when homogenization occurs. Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor.\n\nNote that we have assumed that each user/item pair has fixed utility (assumption 1). In reality, a collection of similar items is probably less useful than a collection of diverse items [19]. With a more nuanced representation of utility that includes the collection of interactions as a whole, these effects would likely increase in magnitude.\n\n[Section: 5.4 Shifts in Item Consumption]",
      "referring_paragraphs": [
        "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig4.jpg",
      "image_filename": "1710.11214_page0_fig4.jpg",
      "caption": "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization.",
      "context_before": "[Section: 5.4 Shifts in Item Consumption]\n\nTo explore the impacts of these systems on item consumption, we computed the Gini coefficient on the distribution of consumed items, which measures the inequality of the distribution (0 being all items are consumed at equal rates and 1 indicating maximal inequality between the consumption rate of items). We use the following definition of Gini inequality, which relies on the relative popularity $( \\mathrm { R P } ) ^ { 5 }$ of each item at time :\n\nWe found that the feedback loop amplifies the impact of recommendation systems on the distribution of item consumption,",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 5 breaks down the relationship between homogenization and utility by user; for all recommendation algorithms, we find that users who experience lower utility generally have higher homogenization with their nearest neighbor.",
        "Figure 5: For the repeated training case, change in Jaccard index of user behavior, relative to ideal behavior, and shown as a function of utility relative to the ideal platform; users paired by cosine similarity of . Each user is shown as a point, with a θlinear fit to highlight the general trend that users who experience losses in utility have higher homogenization."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1710.11214",
      "figure_id": "1710.11214_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig5.jpg",
      "image_filename": "1710.11214_page0_fig5.jpg",
      "caption": "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items). Each point is a single simulation. Similar homogenization can result in different item consumption distributions.",
      "context_before": "",
      "context_after": "[Section: 6 ACCOUNTING FOR CONFOUNDING]\n\nRecommendation system are often evaluated using confounded held-out data. This form of offline evaluation is used by both researchers and practitioners alike, but the choice is not as innocuous\n\nas one might expect. When a recommendation system is evaluated using confounded held-out data, results are biased toward recommendation systems similar to the confounding algorithm. Researchers can, intentionally or unintentionally, select data sets that highlight their proposed model, thus overstating its performance.\n\nThere are specific instances in recommendation system literature where this may be troubling. For example, MovieLens is a research website that uses collaborative filtering to make movie recommendations [45]; the researchers behind it have released several public datasets6 that are used prolifically in evaluation recommendation systems [23]. Recommendation systems based on collaborative filtering will likely perform better on this data. Similarly, the Douban service contains popularity features; algorithms that include a notion of popularity should perform better on the publicly available data from this platform [42]. Many algorithms use social network information to improve recommendations [13, 22, 42] but when training and evaluating these algorithms on data from social platforms, it is not clear if the models capture the true preferences of users or the biasing effects of platform features. These biases are problematic for researchers and platform developers who attempt to evaluate models offline or rely on academic publications that do.\n\nResearchers and practitioners alike would benefit from methods to address these concerns. Weighting techniques, such as those proposed by Schnabel, et al. [52] and De Myttenaere, et al. [17] for offline evaluation seem promising. We performed a preliminary exploration of weighting techniques and found that in the repeated training case, weighting can simultaneously increase utility and decrease homogenization. These weighting techniques could also be of use when attempting to answer social science questions using algorithmically confounded user behavior data. We leave a full exploration of these methods for future work.\n\nAs proposed by Bottou, et al. [8], formal causal inference techniques can assist in the design of deployed learning systems to avoid confounding. This would likely reduce the effects we have described (section 5), but needs to be studied in greater depth. In particular, the distribution of impact across users should be explored, as well as non-accuracy metrics. Regardless, practitioners would do well to incorporate measures to avoid confounding, such as these. At least they should log information about the recommendation system in\n\ndeployment along with observations of behavior; this would be useful in disentangling recommendation system influence from true preference signal as weighting and other techniques are refined.\n\n[Section: 7 CONCLUSION]",
      "referring_paragraphs": [
        "Figure 6: For the repeated training case, change in Jaccard index of user behavior (higher is more homogeneous), relative to the Gini coefficient of the item consumption distribution (higher is more unequal consumption of items)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    }
  ],
  "1711.05144": [
    {
      "doc_id": "1711.05144",
      "figure_id": "1711.05144_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg",
      "image_filename": "1711.05144_page0_fig0.jpg",
      "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.",
      "context_before": "The main details in the implementation of FairFictPlay are the identification of the model classes for Learner and Auditor, the implementation of the cost sensitive classification oracle and auditing oracle, and the identification of the protected features for Auditor. For our experiments, at each round Learner chooses a linear threshold function over all 122 features. We implement the cost sensitive classification oracle via a two stage regression procedure. In particular, the inputs to the cost sensitive classification oracle are cost vectors $c _ { 0 } , c _ { 1 }$ , where the $i ^ { t h }$ element of $c _ { k }$ is the cost of predicting $k$ on datapoint i. We train two linear regression models $r _ { 0 } , r _ { 1 }$ to predict $c _ { 0 }$ and $c _ { 1 }$ respectively, using all 122 features. Given a new point $x$ , we predict the cost of classifying $x$ as 0 and 1 using our regression models: these predictions are $r _ { 0 } ( x )$ and $r _ { 1 } ( x )$ respectively. Finally we output the prediction $\\hat { y }$ corresponding to lower predicted cost: $\\begin{array} { r } { \\hat { y } = \\mathrm { a r g m i n } _ { i \\in \\{ 0 , 1 \\} } r _ { i } ( x ) } \\end{array}$ .\n\nAuditor’s model class consists of all linear threshold functions over just the 18 aforementioned protected race-based attributes. As per the algorithm, at each iteration $t$ Auditor attempts to find a subgroup on which the false positive rate is substantially different than the base rate, given\n\nthe Learner’s randomized classifier so far. We implement the auditing oracle by treating it as a weighted regression problem in which the goal is find a linear function (which will be taken to define the subgroup) that on the negative examples, can predict the Learner’s probabilistic classification on each point. We use the same regression subroutine as Learner does, except that Auditor only has access to the 18 sensitive features, rather than all 122.\n\nRecall that in addition to the choices of protected attributes and model classes for Learner and Auditor, FairFictPlay has a parameter C, which is a bound on the norm of the dual variables for Auditor (the dual player). While the theory does not provide an explicit bound or guide for choosing C, it needs to be large enough to permit the dual player to force the minmax value of the game. For our experiments we chose $C = 1 0$ , which despite being a relatively small value seems to suffice for (approximate) convergence.\n\nThe other and more meaningful parameter of the algorithm is the bound $\\gamma$ in the Fair ERM optimization problem implemented by the game, which controls the amount of unfairness permitted. If on a given round the subgroup disparity found by the Auditor is greater than $\\gamma$ , the Learner must react by adding a fairness penalty for this subgroup to its objective function; if it is smaller than $\\gamma$ , the Learner can ignore it and continue to optimize its previous objective function. Ideally, and as we shall see, varying γ allows us to trace out a menu of trade-offs between accuracy and fairness.\n\n[Section: 5.4 Results]\n\nParticularly in light of the gaps between the idealized theory and the actual implementation, the most basic questions about FairFictPlay are whether it converges at all, and if so, whether it converges to “interesting” models — that is, models with both nontrivial classification error (much better than the $3 0 \\%$ or 0.3 baserate), and nontrivial subgroup fairness (much better than ignoring fairness altogether). We shall see that at least for the C&C dataset, the answers to these questions is strongly affirmative.",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ.",
        "For any choice of the parameter $\\gamma$ , and each iteration $t$ , the two panels of Figure 1 yield a pair of realized values $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ from the experiment, corresponding to a Learner model whose error is $\\varepsilon _ { t } .$ , and for which the worst subgroup the Auditor was able to find had unfairness $\\gamma _ { t }$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1711.05144_page0_fig1.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1711.05144",
      "figure_id": "1711.05144_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
      "image_filename": "1711.05144_page0_fig2.jpg",
      "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.",
      "context_before": "",
      "context_after": "[Section: References]\n\nAlekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, and John Langford. A reductions approach to fair classification. Fairness, Accountability, and Transparency in Machine Learning (FATML), 2017.   \nJulia Angwin and Hannes Grassegger. Facebooks secret censorship rules protect white men from hate speech but not black children. Propublica, 2017.   \nAnna Maria Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing. The Marshall Project, August 8 2015. Retrieved 4/28/2016.   \nGeorge W. Brown. Some notes on computation of games solutions, Jan 1949.   \nAlexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. arXiv preprint arXiv:1703.00056, 2017.   \nConstantinos Daskalakis and Qinxuan Pan. A counter-example to Karlin’s strong conjecture for fictitious play. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 11–20. IEEE, 2014.   \nIlias Diakonikolas, Ryan O’Donnell, Rocco A. Servedio, and Yi Wu. Hardness results for agnostically learning low-degree polynomial threshold functions. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California, USA, January 23-25, 2011, pages 1590–1606, 2011.   \nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214–226. ACM, 2012.   \nVitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of monomials by halfspaces is hard. SIAM J. Comput., 41(6):1558–1590, 2012.\n\nYoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, COLT 1996, Desenzano del Garda, Italy, June 28-July 1, 1996., pages 325–332, 1996.   \nSorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility of fairness. arXiv preprint arXiv:1609.07236, 2016.   \nSara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445– 1459, 2013.   \nMoritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. Advances in Neural Information Processing Systems, 2016.   \nUrsula H ´ ebert-Johnson, Michael P Kim, Omer Reingold, and Guy N Rothblum. Calibration for ´ the (computationally-identifiable) masses. arXiv preprint arXiv:1711.08513, 2017.   \nMatthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325–333, 2016.   \nAdam Tauman Kalai and Santosh Vempala. Efficient algorithms for online decision problems. J. Comput. Syst. Sci., 71(3):291–307, 2005.   \nAdam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learning. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008, pages 629–638, 2008.   \nFaisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1–33, 2012.   \nMichael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. An Empirical Study of Rich Subgroup Fairness for Machine Learning. ArXiv e-prints, art. arXiv:1808.08166, August 2018.   \nMichael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning Theory. MIT press, 1994.   \nMichael J Kearns, Robert E Schapire, and Linda M Sellie. Toward efficient agnostic learning. Machine Learning, 17(2-3):115–141, 1994.   \nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Proceedings of the 2017 ACM Conference on Innovations in Theoretical Computer Science, Berkeley, CA, USA, 2017, 2017.   \nJames Rufus Koren. What does that web search say about your credit? Los Angeles Times, July 16 2016. Retrieved 9/15/2016.   \nJulia Robinson. An iterative method of solving a game. Annals of Mathematics, pages 10–2307, 1951.\n\nCynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired Magazine, August 2013. Retrieved 4/28/2016.   \nMaurice Sion. On general minimax theorems. Pacific J. Math., 8(1):171–176, 1958.   \nBlake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning nondiscriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.   \nBianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM 2003), 19-22 December 2003, Melbourne, Florida, USA, page 435, 2003.   \nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pages 1171–1180. International World Wide Web Conferences Steering Committee, 2017.   \nZhe Zhang and Daniel B Neill. Identifying significant predictive bias in classifiers. arXiv preprint arXiv:1611.08292, 2016.\n\n[Section: A Chernoff-Hoeffding Bound]",
      "referring_paragraphs": [
        "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ."
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(a)",
        "(b)",
        "1711.05144_page0_fig3.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig3.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1711.07076": [
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg",
      "image_filename": "1711.07076_page0_fig0.jpg",
      "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.",
      "context_before": "Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs.\n\n[Section: 1 Introduction]\n\nEffective decision-making requires choosing among options given the available information. That much is unavoidable, unless we wish to make trivial decisions. In selection processes, such as hiring, university admissions, and loan approval, the options are people; the available features include (but are rarely limited to) direct evidence of qualifications; and decisions impact lives.\n\nLaws in many countries restrict the ways in which certain decisions can be made. For example, Title VII of the US Civil Rights Act [1], forbids employment decisions that discriminate on the basis of certain protected characteristics. Interpretation of this law has led to two notions of discrimination: disparate treatment and disparate impact. Disparate treatment addresses intentional discrimination, including (i) decisions explicitly based on protected characteristics; and (ii) intentional discrimination via proxy variables (e.g literacy tests for voting eligibility). Disparate impact addresses facially neutral practices that might nevertheless have an “unjustified adverse impact on members of a protected class” [1]. One might hope that detecting unjustified impact were as simple as detecting unequal outcomes. However, absent intentional discrimination, unequal outcomes can emerge due to correlations between protected and unprotected characteristics. Complicating matters, unequal outcomes may not always signal unlawful discrimination [2].\n\nRecently, owing to the increased use of machine learning (ML) to assist in consequential decisions, the topic of quantifying and mitigating ML-based discrimination has attracted interest in both policy and ML. However, while the existing legal doctrine offers qualitative ideas, intervention in an MLbased system requires more concrete formalism. Inspired by the relevant legal concepts, technical papers have proposed several criteria to quantify discrimination. One criterion requires that the fraction given a positive decision be equal across different groups. Another criterion states that a",
      "context_after": "[Section: 2 Disparate Learning Processes]\n\nTo begin our formal description of the prior work, we’ll introduce some notation. A dataset consists of $n$ examples, or data points $\\{ \\mathbf { x } _ { i } \\in \\mathcal { X } , y _ { i } \\in \\mathcal { Y } \\}$ , each consisting of a feature vector $\\mathbf { x } _ { i }$ and a label $y _ { i }$ . A supervised learning algorithm $f : \\mathcal { X } ^ { n } \\times \\mathcal { Y } ^ { n }  ( \\mathcal { X }  [ 0 , 1 ] )$ is a mapping from datasets to models. The learning algorithm produces a model $\\hat { y } : \\mathcal { X } \\to \\mathcal { Y }$ , which given a feature vector $\\mathbf { x } _ { i }$ , predicts the corresponding output $y _ { i }$ . In this discussion we focus on binary classification $( \\mathcal { V } = \\{ 0 , 1 \\} )$ ).\n\nWe consider probabilistic classifiers which produce estimates $\\hat { p } ( { \\bf x } )$ of the conditional probability $\\mathbb { P } ( y = 1 \\mid \\mathbf { x } )$ of the label given a feature vector x. To make a prediction $\\hat { y } ( \\mathbf x ) \\in \\mathcal { V }$ given an estimated probability $\\hat { p } ( { \\bf x } )$ a threshold rule is used: $\\hat { y } _ { i } = 1$ iff $\\hat { p } _ { i } > t$ . The optimal choice of the threshold $t$ depends on the performance metric being optimized. In our theoretical analysis, we consider\n\noptimizing the immediate utility [10], of which classification accuracy (expected 0 − 1 loss) is a special case. We will define this metric more precisely in the next section.\n\nIn formal descriptions of discrimination-aware ML, a dataset possesses a protected feature $z _ { i } \\in { \\mathcal { Z } }$ , making each example a three-tuple $\\left( \\mathbf { x } _ { i } , y _ { i } , z _ { i } \\right)$ . The protected characteristic may be real-valued, like age, or categorical, like race or gender. The goal of many methods in discrimination-aware ML is not only to maximize accuracy, but also to ensure some form of impact parity. Following related work, we consider binary protected features that divide the set of examples into two groups $a$ and $b$ . Our analysis extends directly to settings with more than two groups.\n\nOf the various measures of impact disparity, the two that are the most relevant here are the Calders-Verwer gap and the p-% rule. At a given threshold $t$ , let $\\begin{array} { r } { q _ { z } = \\frac { 1 } { n _ { z } } \\sum _ { i : z _ { i } = z } \\mathbb { 1 } ( \\hat { p } _ { i } > t ) } \\end{array}$ , where $n _ { z } =$ $\\textstyle \\sum _ { i } ^ { n } \\mathbb { 1 } ( z _ { i } = z )$ . The Calders-Verwer (CV) gap, $q _ { a } - q _ { b }$ , is the difference between the proportions assigned to the positive class in the advantaged group $a$ and the disadvantaged group $b$ [4]. The p- $\\%$ rule is a related metric[5]. Classifiers satisfy the $\\mathbf { p } \\mathbf { - } \\%$ rule if $q _ { b } / q _ { a } \\ge p / 1 0 0$ .\n\nMany papers in discrimination-aware ML propose to optimize accuracy (or some other risk) subject to constraints on the resulting level of impact parity as assessed by some metric [3, 7, 11–14]. Use of DLPs presupposes that using the protected feature $z$ as a model input is impermissible in this effort. Discarding protected features, however, does not guarantee impact parity [15]. DLPs incorporate $z$ in the learning algorithm, but without making it an input to the classifier. Formally, a DLP is a mapping: $\\mathcal { X } ^ { n } \\times \\mathcal { Y } ^ { n } \\times \\mathcal { Z } ^ { n }  ( \\mathcal { X }  \\mathcal { Y } )$ . By definition, DLPs achieve treatment parity. However, satisfying treatment parity in this fashion may still violate disparate treatment.\n\nAlternative approaches. Researchers have proposed a number of other techniques for reconciling accuracy and impact parity. One approach consists of preprocessing the training data to reduce the dependence between the resulting model predictions and the sensitive attribute [6, 16–19]. These methods differ in terms of which variables they affect and the degree of independence achieved. [6] proposed flipping negative labels of training examples form the protected group. [20] proposed learning representations (cluster assignments) so that group membership cannot be inferred from cluster membership. [17] and [19] also construct representations designed to be marginally independent from $Z$ .\n\n[Section: 3 Theoretical Analysis]",
      "referring_paragraphs": [
        "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1).",
        "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women.",
        "Starting from the\n\nTable 1: Statistics of public datasets.",
        "Basic info about these datasets (including the prediction target and protected feature) is shown in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1711.07076_page0_fig1.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1711.07076",
      "figure_id": "1711.07076_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg",
      "image_filename": "1711.07076_page0_fig2.jpg",
      "caption": "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP (“treatment”), while upward triangles indicate individuals accepted only by the DLP. The remaining ${ \\sim } 4 { , } 0 0 0$ blue/yellow dots indicate people whose decisions are not altered. Many students benefiting from the DLP are males who ‘look like’ females based on other features, whereas females who ‘look like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.",
      "context_before": "[Section: 4.1 Synthetic data example: work experience and hair length in hiring]\n\nTo begin, we illustrate our arguments empirically with a simple synthetic data experiment. To construct the data, we sample $n _ { \\mathrm { a l l } } = 2 0 0 0$ total observations from the data-generating process described below. $70 \\%$ of the observations are used for training, and the remaining $30 \\%$ are reserved for model testing.\n\nThis data-generating process has the following key properties: (i) the historical hiring process was based solely on the number of years of work experience; (ii) because women on average have fewer years of work experience than men (5 years vs. 11), men have been hired at a much higher rate than women; and (iii) women have longer hair than men, a fact that was irrelevant to historical hiring practice.\n\nFigure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women. We apply the DLP proposed by Zafar et al. [5], using code available from the authors.2 While the DLP nearly equalizes hiring rates (satisfying a $10 5 \\mathrm { - } \\%$ rule), it does so through a problematic within-class discrimination mechanism. The DLP rule advantages individuals with longer hair over those with shorter hair and considerably longer work experience. We find that several women who would have been hired under historical practices, owing to their $^ { 1 2 + }$ years of work experience, would not be hired under the DLP due to their short hair (i.e., their male-like characteristics captured in x). Similarly, several men, who would not have been hired based on work experience alone, are advantaged by the DLP due to their longer hair (i.e., their ‘female-like’ characteristics in $\\mathbf { x }$ ). The DLP violates rational ordering, and harms some of the most qualified individuals in the protected group. Group parity is achieved at the cost of individual unfairness.\n\nGranted, we might not expect factors such as hair length to knowingly be used as inputs to a typical hiring algorithm. We construct this toy example to illustrate a more general point: since DLPs do not have direct access to the protected feature, they must infer from the other features which people are most likely to belong to each subgroup. Using the protected feature directly can yield more",
      "context_after": "[Section: 4.2 Case study: Gender bias in CS graduate admissions]\n\nFor our next example, we demonstrate a similar result but this time by analyzing real data with synthetic discrimination, to empirically demonstrate our arguments. We consider a sample of ${ \\sim } 9 { , } 0 0 0$ students considered for admission to the MS program of a large US university over an 11-year period. Half of the examples are withheld for testing. Available attributes include basic information, such as country of origin, interest areas, and gender, as well as quantitative fields such as GRE scores. Our data also includes a label in the form of an ‘above-the-bar’ decision provided by faculty reviewers. Admission rates for male and female applicants were observed to be within $1 \\%$ of each other. So, to demonstrate the effects of DLPs, we corrupt the data with synthetic discrimination. Of all women who were admitted, i.e., $z _ { i } = b , y _ { i } = 1$ , we flip $2 5 \\%$ of those labels to 0: giving noisy labels $\\bar { y } _ { i } = y _ { i } \\cdot \\eta$ , for $\\eta \\sim B e r n o u l l i ( . 2 5 )$ . This simulates historical bias in the training data.\n\nWe then train three logistic regressors: (1) To predict the (synthetically corrupted) labels $\\bar { y } _ { i }$ from the non-sensitive features $x _ { i }$ ; (2) The same model, applying the DLP of [5]; and (3) A model to predict the sensitive feature $z _ { i }$ from the non-sensitive features $x _ { i }$ . The data contains limited information that predicts gender, though such predictions can be made better than random $\\mathrm { \\ A U C { = } } 0 . 5 9$ ) due to different rates of gender imbalance across (e.g.) countries and interests.\n\nFigure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary. Furthermore, students predicted to be male $\\mathbf { \\dot { X } }$ -axis) tend to be flipped to the negative class (left half of plot) while students predicted to be female tend to be flipped to the positive class (right half of plot). This is shown in detail in Figure 2 (center and right). Of the 43 students whose decisions are flipped to ‘non-admit,’ 5 are female, each of whom has ‘male-like’ characteristics according to their other features as demonstrated in our synthetic hair-length example. Demonstrated here with real-world data, the DLP both disrupts the within-group ordering and violates the do no harm principle by disadvantaging some women who, but for the DLP, would have been admitted.\n\nComparison with Treatment Disparity. To demonstrate the better performance of per-group thresholding, we implement a simple decision scheme and compare its performance to the DLP.\n\nOur thresholding rule for maximizing accuracy subject to a $p { - } \\%$ rule works as follows: Recall that the $p { - } \\%$ rule requires that $q _ { b } / q _ { a } > p / 1 0 0$ , which can be written as $\\begin{array} { r } { \\frac { p } { 1 0 0 } q _ { a } - q _ { b } < 0 } \\end{array}$ . We denote the quantity $\\frac { p } { 1 0 0 } q _ { a } - q _ { b }$ as the $p$ -gap. To maximize accuracy subject to satisfying the $p { - } \\%$ rule, we construct a score that quantifies reduction in $p$ -gap per reduction in accuracy. Starting from the\n\nTable 1: Statistics of public datasets.   \n\n<table><tr><td>dataset</td><td>source</td><td>protected feature</td><td>prediction target</td><td>n</td></tr><tr><td>Income</td><td>UCI [22]</td><td>Gender (female)</td><td>income &gt; $50k</td><td>32,561</td></tr><tr><td>Marketing</td><td>UCI [23]</td><td>Status (married)</td><td>customer subscribes</td><td>45,211</td></tr><tr><td>Credit</td><td>UCI [24]</td><td>Gender (female)</td><td>credit card default</td><td>30,000</td></tr><tr><td>Employee Attr.</td><td>IBM [25]</td><td>Status (married)</td><td>employee attrition</td><td>1,470</td></tr><tr><td>Customer Attr.</td><td>IBM [25]</td><td>Status (married)</td><td>customer attrition</td><td>7,043</td></tr></table>\n\naccuracy-maximizing classifications $\\hat { y }$ (thresholding at .5), we then flip those predictions which close the gap fastest:\n\n1. Assign each example with $\\{ \\tilde { y } _ { i } = 0 , z _ { i } = b \\}$ or $\\{ \\tilde { y } _ { i } = 1 , z _ { i } = a \\}$ , a score $c _ { i }$ equal to the reduction in the p-gap divided by the reduction in accuracy:\n\n$a$ $\\hat { y } _ { i } = 1$ $\\begin{array} { r } { c _ { i } = \\frac { p } { 1 0 0 n _ { a } ( 2 \\hat { p } _ { i } - 1 ) } } \\end{array}$   \n(b) For each example in group b with initial yˆi = 0, ci = 1nb(1−2ˆpi) . $b$ $\\hat { y } _ { i } = 0$ $\\begin{array} { r } { c _ { i } = \\frac { 1 } { n _ { b } \\left( 1 - 2 \\hat { p } _ { i } \\right) } } \\end{array}$\n\n2. Flip examples in descending order according to this score until the desired CV-score is reached.\n\nThese scores do not change after each iteration, so the greedy policy leads to optimal flips (equivalently, optimal classification thresholds).\n\nThe unconstrained classifier achieves a $p \\%$ rule of $7 1 . 4 \\%$ . By applying this thresholding strategy, we were able to obtain the same accuracy as the method of [5], but with a higher $p \\%$ rule of $7 8 . 3 \\%$ compared to $7 7 . 6 \\%$ . Note that on this data, the method of [5] maxes out at a $p \\%$ rule of $7 7 . 6 \\%$ . That is, the method is limited in what $p \\text{‰}$ rules may be achieved. By contrast, the thresholding rule can achieve any desired parity level. Subject to a $< 1 \\%$ drop in accuracy relative to the DLP we can achieve a $p \\mathrm { - } \\%$ rule of $\\sim 1 0 0 \\%$ .\n\n[Section: 4.3 Examples on public datasets]",
      "referring_paragraphs": [
        "Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability, on unseen test data.",
        "Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of admission for the unconstrained classifier (y-axis), students whose decisions are ‘flipped’ (after applying the fairness constraint) tend to be those close to the decision boundary.",
        "the protected characteristic (Table 2), so no synthetic discrimination is applied."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1711.07076_page0_fig3.jpg",
        "1711.07076_page0_fig4.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig4.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1711.08536": [
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg",
      "image_filename": "1711.08536_page0_fig0.jpg",
      "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ",
      "context_before": "Creating large data sets from scratch can be costly. As such, it is common for practitioners to use freely available open source data sets such as ImageNet [4] and Open Images [3] to train vision models. This is particularly desirable when using ML for the developing world, where resources for creating new data sets may be limited. However, if these data sets are not representative of the locations of interest, predictive performance of models may suffer.\n\nIn this paper, we assess the geo-diversity of large data sets and the differences that models trained on them exhibit when classifying images from varying geographical locations. We find an observable amerocentric and eurocentric bias shown in both forms of assessement. This is the case despite these data sets’ creators efforts to encourage diversity. We present these findings not as a criticism but as a case study in the difficulties in creating a geographically balanced data set.\n\n[Section: 2 Background: ImageNet and Open Images]\n\nIn this work, we analyze two popular public data sets: ImageNet [4] and Open Images [3]. These two data sets are generally considered academic benchmarks and are not necessarily constructed to cover every possible use case. However, in the absence of a robust data source for a particular application, it is quite common to fall back to these standard data sets.\n\nThe first version of the ImageNet data set was released in 2009 by Deng et al. [2]. An updated 2012 release [4], used to train the model in this paper, consisted of approximately 1.2 million image thumbnails and URLs from 1000 categories. Each image in the data set is associated with a humanverified single label. The Open Images data set, released in 2016 by Krasin et al. [3], contains about 9 million URLs to Creative Commons licensed images. There are 6012 image-level human-verified labels, and each image can be associated with multiple labels.",
      "context_after": "[Section: 3 Analyzing Geo-Diversity]\n\nOur first goal was to assess the geo-diversity of the images in the open source data sets. It is naturally difficult to identify the geo-location of every image in previously released open source image data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.\n\nFor the purposes of this study, we take this country identification, accepting the possibility of noise in the coverage and accuracy of the country-level geo-location as unlikely to qualitatively impact the larger trends shown.\n\nGeo-Diversity of Open Images. Of the 9 million images in the Open Images data set, we were able to acquire country-level geo-location for roughly 2 million. This is a large (but potentially non-uniform) subset of the overall data. Geo-location data is shown in Figures 1 and 2. Overall, more than $32 \\%$ of the sample data was US-based and $60 \\%$ of the data was from the six most represented",
      "referring_paragraphs": [
        "Figure 1: Fraction of Open Images and ImageNet images from each country.",
        "We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1711.08536_page0_fig1.jpg",
        "1711.08536_page0_fig2.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg",
      "image_filename": "1711.08536_page0_fig3.jpg",
      "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.",
      "context_before": "[Section: 3 Analyzing Geo-Diversity]\n\nOur first goal was to assess the geo-diversity of the images in the open source data sets. It is naturally difficult to identify the geo-location of every image in previously released open source image data sets. However, proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information at the country level for a large number of images in each data set.\n\nFor the purposes of this study, we take this country identification, accepting the possibility of noise in the coverage and accuracy of the country-level geo-location as unlikely to qualitatively impact the larger trends shown.\n\nGeo-Diversity of Open Images. Of the 9 million images in the Open Images data set, we were able to acquire country-level geo-location for roughly 2 million. This is a large (but potentially non-uniform) subset of the overall data. Geo-location data is shown in Figures 1 and 2. Overall, more than $32 \\%$ of the sample data was US-based and $60 \\%$ of the data was from the six most represented",
      "context_after": "[Section: 4 Analyzing Classification Behavior Based on Geo-Location]\n\nWe examined how lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations. We collected image data for specific geographical regions using two separate methods.\n\nCrowdsourced Data. Our first method of collecting stress-test data was to ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels, specifically from a community that they identified with in an effort to avoid amerocentric or eurocentric bias.\n\nSpot checking the results of this collection showed that images for labels such as groom, bridegroom did appear to represent traditions commonly associated with India. In other cases the human raters found it difficult to find an image from a community that they belonged to. Some of these cases were for US-centric classes, (e.g., an “infielder” baseball player or “Captain America”).\n\nGeo-located web images. While the raters in Hyderabad gave us one source of location-specific image data, we needed another approach to find data from a wider range of countries. To this end, we first identified 15 countries to target and joined the per-country location proxy described above with inferred labels from a classifier similar to Google Cloud Vision API, across a large data store of images from the web. For analysis, we focused on labels related to “people”, such as bridegroom, police officer, and greengrocer.\n\nOne limitation of this work is that even our geographically diverse images were collected from the internet using tools that rely (at least partially) on image classifiers themselves. The human raters used web search to find images that depicted people from their communities. Similarly, when building a data set from underrepresented countries using geo-located web images to stress-test a classifier, an image classifier was used to filter for relevant images.",
      "referring_paragraphs": [
        "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets.",
        "Figure 3 shows some categories that showed noticeable differences in performance."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1711.08536_page0_fig4.jpg",
        "1711.08536_page0_fig5.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig5.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg",
      "image_filename": "1711.08536_page0_fig6.jpg",
      "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.",
      "context_before": "[Section: 4 Analyzing Classification Behavior Based on Geo-Location]\n\nWe examined how lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations. We collected image data for specific geographical regions using two separate methods.\n\nCrowdsourced Data. Our first method of collecting stress-test data was to ask crowdsourced raters located in Hyderabad to find and return URLs of images on the internet that matched particular labels, specifically from a community that they identified with in an effort to avoid amerocentric or eurocentric bias.\n\nSpot checking the results of this collection showed that images for labels such as groom, bridegroom did appear to represent traditions commonly associated with India. In other cases the human raters found it difficult to find an image from a community that they belonged to. Some of these cases were for US-centric classes, (e.g., an “infielder” baseball player or “Captain America”).\n\nGeo-located web images. While the raters in Hyderabad gave us one source of location-specific image data, we needed another approach to find data from a wider range of countries. To this end, we first identified 15 countries to target and joined the per-country location proxy described above with inferred labels from a classifier similar to Google Cloud Vision API, across a large data store of images from the web. For analysis, we focused on labels related to “people”, such as bridegroom, police officer, and greengrocer.\n\nOne limitation of this work is that even our geographically diverse images were collected from the internet using tools that rely (at least partially) on image classifiers themselves. The human raters used web search to find images that depicted people from their communities. Similarly, when building a data set from underrepresented countries using geo-located web images to stress-test a classifier, an image classifier was used to filter for relevant images.",
      "context_after": "[Section: 5 Discussion]\n\nIt is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world. This is not too surprising, as these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems.\n\nThis study highlights the importance of assessing the appropriateness of a given data set before using it to learn models for use in the developing world. Equally, this work emphasizes the importance of creating new data sets that prioritize broad geo-representation as first class goals, in order to aid ML in the developing world.",
      "referring_paragraphs": [
        "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1711.08536_page0_fig7.jpg",
        "1711.08536_page0_fig8.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig8.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1711.08536",
      "figure_id": "1711.08536_fig_10",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg",
      "image_filename": "1711.08536_page0_fig9.jpg",
      "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.",
      "context_before": "[Section: 5 Discussion]\n\nIt is clear that standard open source data sets such as ImageNet or Open Images may not have sufficient geo-diversity for broad representation across the developing world. This is not too surprising, as these data sets were designed for specific purposes, and it is only the practice of later adoption for other purposes that may introduce problems.\n\nThis study highlights the importance of assessing the appropriateness of a given data set before using it to learn models for use in the developing world. Equally, this work emphasizes the importance of creating new data sets that prioritize broad geo-representation as first class goals, in order to aid ML in the developing world.",
      "context_after": "[Section: References]\n\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems, 2015.   \n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.   \n[3] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit, S. Belongie, V. Gomes, A. Gupta, C. Sun, G. Chechik, D. Cai, Z. Feng, D. Narayanan, and K. Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017.   \n[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.   \n[5] D. Smilkov, N. Thorat, B. Kim, F. B. Viégas, and M. Wattenberg. Smoothgrad: removing noise by adding noise. CoRR, abs/1706.03825, 2017.   \n[6] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.",
      "referring_paragraphs": [
        "Figure 5 plots images of groom, bridegroom images from different countries by log likelihood.",
        "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    }
  ],
  "1801.04385": [
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg",
      "image_filename": "1801.04385_page0_fig0.jpg",
      "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ",
      "context_before": "e parameters $\\alpha$ and $\\beta$ are ed using Maximum likelihood, while α βtest of the null hypothesis $H _ { 0 } : \\beta = 0$ is performed using the Likelihood Ratio Test [6].\n\ne eleven variables in Stack Exchange data, result in 110 possible Simpson’s pairs. Among these, our method identies seven as instance of paradox. ese are listed in Table 1.\n\nOur approach reveals that the previously reported nding that acceptance probability decreases with answer position [9] is an instance of Simpson’s paradox and would not have been observed had the data not been disaggregated by session length. More interestingly, our approach also identies previously unknown instances of Simpson’s paradox. We explore these in greater detail below, illustrating how it can lead to deeper insights into online behavior.\n\n4.2.1 Answer Position & Session Length. We measure session length by the number of answers a user posts before taking an extended break. Session length was shown to be an important confounding variable in online activity. Analysis of the quality of comments posted on a social news platform Reddit showed that, once disaggregated by the length of session, the quality of comments declines over the course of a session, with each successive comment wrien by a user becoming shorter, less textually complex, receiving fewer responses and a lower score from others [19]. Similarly, each successive answer posted during a session by a user on Stack Exchange is shorter, less well documented with external links and code, and less likely to be accepted by the asker as the best answer [9].\n\nOur approach automatically identies this example as Simpson’s paradox, as illustrated in Fig. 1. e gure shows average acceptance probability for an answer as a function of its position (or\n\nindex) within a session. According to Fig. 1a, which reports aggregate acceptance probability, answers wrien later in a session are more likely to be accepted than earlier answers. However, once the same data is disaggregated by session length, the trend reverses (Fig. 1b): each successive answer within the same session is less likely to be accepted than the previous answer. For example, for sessions during which ve answers were wrien, the rst answer is more likely to be accepted than the second answer, which is more likely to be accepted than the third answer, etc., which is more likely to be accepted than the h answer. e lines in Fig. 1 represent ts to data using logistic regression.\n\nis example highlights the necessity to properly disaggregate data to identify the subgroups for analysis. Unless data is disaggregated, wrong conclusions may be drawn, in this case, for example, that user performance improves during a session.\n\n4.2.2 Number of Answers & Reputation. Experience plays an important role in the quality of answers wrien by users. Stack Exchange veterans, i.e., users who have been active on Stack Exchange for more than six months, post longer, beer documented answers, that are also more likely to be accepted as best answers by askers [9]. ere are several ways to measure experience on Stack Exchange. Reputation, according to Stack Exchange, gauges how much the community trusts a user to post good questions and provide useful answers. While reputation can be gained or lost with dierent actions, a more straightforward measure of experience is user tenure, which measures time since the user became active on Stack Exchange, or Percentile, normalized rank of a user’s tenure. Alternately, experience can be measured by the Number of Answers a user posted during his or her tenure before writing the current answer.\n\nOur method uncovers a novel Simpson’s paradox for user experience variables Reputation and Number of Answers. In the aggregate data, acceptance probability increases as a function of the Number of Answers (Fig. 2a). is is consistent with our expectations that the more experienced users—who have wrien more answers over their tenure on Stack Exchange—produce higher quality answers. However, when data is conditioned on Reputation, the trend reverses (Fig. 2b). In other words, focusing on groups of users with the same reputation, those who have wrien more answers over their tenure are less likely to have a new answer accepted than the less active answerers.\n\n[Section: 4.3 e Origins of Simpson’s paradox]\n\nTo understand why Simpson’s paradox occurs in Stack Exchange data, we illustrate the mathematical explanation of Section 3.2 with examples from our study. Consider the paradox for Answer Position–Session Length Simpson’s pair, illustrated in Fig. 1a. In the disaggregated data, trend lines of acceptance probability for sessions of dierent length are stacked (Fig. 1b): answers produced during longer sessions are more likely to be accepted than answers\n\nTable 2: Number of data points in each group   \n\n<table><tr><td>Session Length</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Data points</td><td>7.2M</td><td>2.6M</td><td>1.3M</td><td>0.7M</td><td>0.4M</td><td>0.3M</td><td>0.2M</td><td>0.1M</td></tr></table>",
      "context_after": "",
      "referring_paragraphs": [
        "Table 1: Examples of Simpson’s paradox in Stack Exchange data.",
        "Figure 1: Simpson’s paradox in Stack Exchange data."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Aggregated Data",
        "(b) Disaggregated Data",
        "1801.04385_page0_fig1.jpg",
        "1801.04385_page0_fig2.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg",
      "image_filename": "1801.04385_page0_fig3.jpg",
      "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "1b): answers produced during longer sessions are more likely to be accepted than answers\n\nTable 2: Number of data points in each group   \n\n<table><tr><td>Session Length</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>Data points</td><td>7.2M</td><td>2.6M</td><td>1.3M</td><td>0.7M</td><td>0.4M</td><td>0.3M</td><td>0.2M</td><td>0.1M</td></tr></table>",
        "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) Disaggregated Data"
      ],
      "quality_score": 0.65,
      "metadata": {}
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
      "image_filename": "1801.04385_page0_fig4.jpg",
      "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "context_before": "",
      "context_after": "Meanwhile, for all other values of $X _ { c }$ greater than $a$ , the probability mass at $X _ { p } ~ = ~ a + 1$ c is the same as that at $X _ { p } \\ = \\ a$ (as the num-Xp a Xp aber of data points is constant along sessions of same length) but normalized to account for the sessions of length $a$ , i.e.,\n\ne rate of change of these probability masses with respect to $X _ { p }$ is\n\ne probability mass function $\\mathrm { P r } ( X _ { c } = x _ { c } | X _ { p } = a )$ decreases for $X _ { c } = a$ c c pcorresponding to the smallest value of acceptance proba-Xc ability, while increasing for all other values $X _ { c } > a$ . Moreover, the crate of increase of this probability mass is greater than the rate at\n\nwhich the acceptance probability decreases, resulting in an upward trend when the data is aggregated.\n\nA similar eect plays out in the Number of Answers–Reputation Simpson’s pair. Figure 3a shows the heatmap of acceptance probability for dierent values of the Number of Answers wrien over a user’s tenure and user Reputation, while Fig. 3b shows the correlated joint distribution of the two variables. e gures illustrate the rst condition of Simpson’s paradox (Eq. (7)): as $X _ { p }$ changes, the distribution of the values of $X _ { c }$ Xpmust also change. is dependency can Xcbe clearly seen in Fig. 3b—as $X _ { p } =$ increases Xp Number o f Answersthen the distribution of =  shis to increasing values, Xcwhich produces the paradox.\n\nIn the real world this means that users, who have wrien more answers are not more likely to have a new answer they write accepted. In fact, among users with same Reputation, those who earned this reputation with fewer answers are more likely to have a new answer they write accepted as best answer. is suggests that such users are simply beer at answering questions, and that this can be detected early in their tenure on Stack Exchange (while they still have low reputation). Note, however, that an exception to the trend reversal occurs for users with very high reputation. In Stack Exchange, users can gain reputation by “Answer is marked accepted”, “Answer is voted up”, “estion is voted up”, etc. It seems that, high reputation users and low reputation users are dierent: for high reputation users, experience (number of wrien answers) is important, while for low reputation users the quality of answers, which may lead to votes, is more important. Analysis of this behavior is beyond the scope of this paper.",
      "referring_paragraphs": [
        "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Disaggregated data",
        "(b) Joint distribution of $X _ { c }$ and $X _ { p }$",
        "1801.04385_page0_fig5.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig5.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg",
      "image_filename": "1801.04385_page0_fig6.jpg",
      "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.",
      "context_before": "e probability mass function $\\mathrm { P r } ( X _ { c } = x _ { c } | X _ { p } = a )$ decreases for $X _ { c } = a$ c c pcorresponding to the smallest value of acceptance proba-Xc ability, while increasing for all other values $X _ { c } > a$ . Moreover, the crate of increase of this probability mass is greater than the rate at\n\nwhich the acceptance probability decreases, resulting in an upward trend when the data is aggregated.\n\nA similar eect plays out in the Number of Answers–Reputation Simpson’s pair. Figure 3a shows the heatmap of acceptance probability for dierent values of the Number of Answers wrien over a user’s tenure and user Reputation, while Fig. 3b shows the correlated joint distribution of the two variables. e gures illustrate the rst condition of Simpson’s paradox (Eq. (7)): as $X _ { p }$ changes, the distribution of the values of $X _ { c }$ Xpmust also change. is dependency can Xcbe clearly seen in Fig. 3b—as $X _ { p } =$ increases Xp Number o f Answersthen the distribution of =  shis to increasing values, Xcwhich produces the paradox.\n\nIn the real world this means that users, who have wrien more answers are not more likely to have a new answer they write accepted. In fact, among users with same Reputation, those who earned this reputation with fewer answers are more likely to have a new answer they write accepted as best answer. is suggests that such users are simply beer at answering questions, and that this can be detected early in their tenure on Stack Exchange (while they still have low reputation). Note, however, that an exception to the trend reversal occurs for users with very high reputation. In Stack Exchange, users can gain reputation by “Answer is marked accepted”, “Answer is voted up”, “estion is voted up”, etc. It seems that, high reputation users and low reputation users are dierent: for high reputation users, experience (number of wrien answers) is important, while for low reputation users the quality of answers, which may lead to votes, is more important. Analysis of this behavior is beyond the scope of this paper.\n\n[Section: 4.4 Discussion and Implications]\n\nPresence of a Simpson’s paradox in data can indicate interesting or surprising paerns [8], and for trends in social data, important behavioral dierences within a population. Since social data is oen generated by a mixture of subgroups, existence of Simpson’s",
      "context_after": "[Section: 5 CONCLUSION]\n\nWe presented a method for systematically uncovering instances of Simpson’s paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data. Our mathematical analysis suggests that Simspon’s paradox",
      "referring_paragraphs": [
        "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1801.04385",
      "figure_id": "1801.04385_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg",
      "image_filename": "1801.04385_page0_fig7.jpg",
      "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.",
      "context_before": "[Section: 5 CONCLUSION]\n\nWe presented a method for systematically uncovering instances of Simpson’s paradox in data. e method identies pairs of variables, such that a trend in an outcome as a function of one variable disappears or reverses itself when the same data is disaggregated by conditioning it on the second variable. e disaggregated data corresponds to subgroups within the population generating the data. Our mathematical analysis suggests that Simspon’s paradox",
      "context_after": "[Section: Acknowledgments]\n\nWe acknowledge funding support from the James S. McDonnell Foundation, Air Force Oce of Scientic Research (FA9550-17-1- 0327), and by the Army Research Oce (W911NF-15-1-0142).\n\n[Section: REFERENCES]",
      "referring_paragraphs": [
        "Figure 5: A pair which multivariate logistic regression cannot nd in the data."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) Disaggregated data",
        "(b) Joint distribution of $X _ { c }$ and $X _ { p }$",
        "1801.04385_page0_fig8.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig8.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1801.07593": [
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg",
      "image_filename": "1801.07593_page0_fig0.jpg",
      "caption": "Figure 1: The architecture of the adversarial network.",
      "context_before": "Machine learning leverages data to build models capable of assessing the labels and properties of novel data. Unfortunately, the available training data frequently contains biases with respect to things that we would rather not use for decision making. Machine learning builds models faithful to training data and can lead to perpetuating these undesirable biases. For example, systems designed to predict creditworthiness and systems designed to perform analogy completion have been demonstrated to be biased against racial minorities and women respectively. Ideally we would be able to build a model which captures exactly those generalizations from the data which are useful for performing some task which are not discriminatory in a way which the people building those models consider unfair.\n\nWork on training machine learning systems that output fair decisions has defined several useful measurements for fairness: Demographic Parity, Equality of Odds, and Equality of Opportunity. These can be imposed as constraints or\n\nincorporated into a loss function in order to mitigate disproportional outcomes in the system’s output predictions regarding a protected demographic, such as sex.\n\nIn this paper, we examine these fairness measures in the context of adversarial debiasing. We consider supervised deep learning tasks in which the task is to predict an output variable $Y$ given an input variable $X$ , while remaining unbiased with respect to some variable $Z$ . We refer to $Z$ as the protected variable. For these learning systems, the predictor ${ \\hat { Y } } = f ( X )$ can be constructed as (input, output, protected) tuples $( X , Y , Z )$ . The predictor $f ( X )$ is usually given access to the protected variable $Z$ , though this is not strictly necessary. This construction allows the determination of which types of bias are considered undesirable for a particular application to be chosen through the specification of the protected variable.\n\nWe speak to the concept of mitigating bias using the known term debiasing1, following definitions provided by Hardt et al. (2016) and refined by Beutel et al. (2017).\n\nDefinition 1. DEMOGRAPHIC PARITY. A predictor $\\hat { Y }$ satisfies demographic parity if $\\hat { Y }$ and $Z$ are independent.\n\nThis means that $P ( \\hat { Y } = \\hat { y } )$ is equal for all values of the protected variable $Z \\colon P ( \\hat { Y } = \\hat { y } ) = P ( \\hat { Y } = \\hat { y } | Z = z )$ .\n\nDefinition 2. EQUALITY OF ODDS. A predictor $\\hat { Y }$ satisfies equality of odds if $\\hat { Y }$ and $Z$ are conditionally independent given $Y$ .\n\nThis means that, for all possible values of the true label Y , $P ( \\hat { Y } = \\hat { y } )$ is the same for all values of the protected variable: $P ( \\hat { Y } = \\hat { y } | Y = y ) = P ( \\hat { Y } = \\hat { y } | Z = z , Y = y )$\n\nDefinition 3. EQUALITY OF OPPORTUNITY. If the output variable $Y$ is discrete, a predictor $\\hat { Y }$ satisfies equality of opportunity with respect to a class $y$ if $\\hat { Y }$ and $Z$ are independent conditioned on $Y = y$ .\n\nThis means that, for a particular value of the true label $Y$ , $P ( \\hat { Y } = \\hat { y } )$ is the same for all values of the protected variable: $P ( \\hat { Y } = \\hat { y } | Y = y ) = P ( \\hat { Y } = \\hat { y } | Z = z , Y = y )$\n\nWe present an adversarial technique for achieving whichever one of these definitions is desired.2 A predictor $f$ will be trained to model $Y$ as accurately as possible while satisfying one of the above equality constraints. Demographic parity will be achieved by introducing an adversary $g$ which will attempt to predict a value for $Z$ from $\\hat { Y }$ . The gradient of $g$ will then be incorporated into the weight update rule of $f$ so as to reduce the amount of information about $Z$ transmitted through $\\hat { Y }$ . Equality of odds will be achieved by also giving $g$ access to the true label $Y$ , thereby limiting any information about $Z$ which $\\hat { Y }$ contains beyond the information already contained in $Y$ .\n\nWe consider the case where the protected variable is a discrete feature present in the training set as well as the case in which the protected variable must be inferred from latent semantics (in particular, gender from word embeddings). In order to accomplish the latter we adapt a technique presented by Bolukbasi et al. (2016) to define a subspace capturing the semantics of the protected variable, and then train a model to perform a word analogies task accurately, while unbiased on this protected variable. A consequence of this technique is that the network learns “debiased” embeddings, embeddings that have the semantics of the protected variable removed. These embeddings are still able to perform the analogy task well, but are better at avoiding problematic examples such as those shown in Bolukbasi et al. (2016).\n\nResults on the UCI Adult Dataset demonstrate the technique we introduce allows us to train a model that achieves equality of odds to within $1 \\%$ on both protected groups.\n\nWe also compare with the related previous work of Beutel et al. (2017), and find we are able to better equalize the differences between the two groups, measured by both False Positive Rate and False Negative Rate (1 - True Positive Rate), although note that the previous work performs better overall for False Negative Rate.\n\nWe provide some discussion on caveats pertaining to this approach, difficulties in training these models that are shared by many adversarial approaches, as well as some discussion on difficulties that the fairness constraints introduce.\n\n[Section: 2 Related Work]\n\nThere has been significant work done in the area of debiasing various specific types of data or predictor.\n\nDebiasing word embeddings: Bolukbasi et al. (2016) devises a method to remove gender bias from word embeddings. The method relies on a lot of human input; namely, it needs a large “training set” of gender-specific words.\n\nSimple models: Lum and Johndrow (2016) demonstrate that removing the protected variable from the training data fails to yield a debiased model (since other variables can be highly correlated with the protected variable), and devise a method for learning fair predictive models in cases when the learning model is simple (e.g. linear regression). Hardt et al.",
      "context_after": "[Section: 3 Adversarial Debiasing]\n\nWe begin with a model, which we call the predictor, trained to accomplish the task of predicting $Y$ given $X$ . As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\nThe output layer of the predictor is then used as an input to another network called the adversary which attempts to predict $Z$ . This is part of the network corresponds to the discriminator in a typical GAN (Goodfellow et al. 2014). We will suppose the adversary has loss term $L _ { A } ( \\hat { z } , z )$ and weights $U$ . Depending on the definition of fairness being achieved, the adversary may have other inputs.\n\n• For DEMOGRAPHIC PARITY, the adversary gets the predicted label $\\hat { Y }$ . Intuitively, this allows the adversary to try to predict the protected variable using nothing but the predicted label. The goal of the predictor is to prevent the adversary from doing this.   \n• For EQUALITY OF ODDS, the adversary gets $\\hat { Y }$ and the true label $Y$ .   \n• For EQUALITY OF OPPORTUNITY on a given class $y$ , we can restrict the training set of the adversary to training examples where $Y = { \\mathfrak { y } }$ .3\n\nIn order for gradients to propagate correctly, $\\hat { Y }$ above refers to the output layer of the network, not to the discrete prediction; for example, for a classification problem, $\\hat { Y }$ could refer to the output of the softmax layer.\n\nWe update $U$ to minimize $L _ { A }$ at each training time step, according to the gradient $\\nabla _ { U } L _ { A }$ . We modify $W$ according",
      "referring_paragraphs": [
        "Figure 1: The architecture of the adversarial network.",
        "As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.",
        "(2016), the\n\nTable 1: Completions for he : she :: doctor : ?"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1801.07593",
      "figure_id": "1801.07593_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg",
      "image_filename": "1801.07593_page0_fig1.jpg",
      "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.",
      "context_before": "[Section: 3 Adversarial Debiasing]\n\nWe begin with a model, which we call the predictor, trained to accomplish the task of predicting $Y$ given $X$ . As in Figure 1, we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L _ { P } ( \\hat { y } , y )$ , using a gradient-based method such as stochastic gradient descent.\n\nThe output layer of the predictor is then used as an input to another network called the adversary which attempts to predict $Z$ . This is part of the network corresponds to the discriminator in a typical GAN (Goodfellow et al. 2014). We will suppose the adversary has loss term $L _ { A } ( \\hat { z } , z )$ and weights $U$ . Depending on the definition of fairness being achieved, the adversary may have other inputs.\n\n• For DEMOGRAPHIC PARITY, the adversary gets the predicted label $\\hat { Y }$ . Intuitively, this allows the adversary to try to predict the protected variable using nothing but the predicted label. The goal of the predictor is to prevent the adversary from doing this.   \n• For EQUALITY OF ODDS, the adversary gets $\\hat { Y }$ and the true label $Y$ .   \n• For EQUALITY OF OPPORTUNITY on a given class $y$ , we can restrict the training set of the adversary to training examples where $Y = { \\mathfrak { y } }$ .3\n\nIn order for gradients to propagate correctly, $\\hat { Y }$ above refers to the output layer of the network, not to the discrete prediction; for example, for a classification problem, $\\hat { Y }$ could refer to the output of the softmax layer.\n\nWe update $U$ to minimize $L _ { A }$ at each training time step, according to the gradient $\\nabla _ { U } L _ { A }$ . We modify $W$ according",
      "context_after": "Note that, in this proof, the adversary can be operating in a few different ways, as long as it is given $\\hat { Y }$ as one of its inputs; for example, for demographic parity, it could be given only $\\hat { Y }$ ; for equality of odds, it can be given both $\\hat { Y }$ and $Y$ .\n\nWe will show in the next propositions that the adversary gaining no advantage from information about $\\hat { Y }$ is exactly the condition needed to guarantee that desired definitions of equality are satisfied.\n\nProposition 2. Let the training data be comprised of triples $( X , { \\bar { Y } } , Z )$ drawn according to some distribution $D$ . Suppose:\n\n1. The protected variable $Z$ is discrete.   \n2. The adversary is trained for DEMOGRAPHIC PARITY; i.e. the adversary is given only the prediction $\\hat { y }$ .\n\n3. The adversary is strong enough that, at convergence, it has learned a randomized function A that minimizes the cross-entropy loss $\\mathbb { E } _ { ( x , y , z ) \\sim D } [ - \\log P ( A ( \\hat { y } ) = z ) ]$ ; i.e. the adversary in fact achieves the optimal accuracy with which you can predict $Z$ from $\\hat { Y }$   \n4. The predictor completely fools the adversary; in particular, the adversary achieves loss $H ( Z )$ , the entropy of $Z$ .\n\nThen the predictor satisfies DEMOGRAPHIC PARITY; i.e.,$\\hat { Y } \\perp Z$ .\n\nProof. Notice that if the adversary draws $A ( \\hat { y } )$ according to the distribution $Z | \\hat { Y } = \\hat { y }$ , then its loss is exactly the conditional entropy\n\nwhere the expectation is taken over $( x , y , z ) \\sim D$ . Now suppose for contradiction that $\\hat { Y }$ is dependent on $Z$ . Then $H ( Z | \\hat { Y } ) < H ( Z )$ , so the adversary can achieve loss less than $H ( Z )$ , contradicting assumption (4). □\n\nProposition 3. If assumptions (2)-(4) above are replaced with the analogous equality of odds assumptions; in particular, that the adversary is given $\\hat { y }$ and $y$ , and the adversary cannot achieve loss better than $H ( Z | Y )$ then the predictor will satisfy EQUALITY OF ODDS; i.e., $( { \\hat { Y } } \\bot Z ) | Y$\n\nProof. Analogous to the above. Notice that if the adversary draws $A ( \\hat { y } , y ) \\sim ( Z | \\hat { Y } = \\hat { y } , Y = y )$ , then its loss is exactly the conditional entropy\n\nwhere the expectation is again taken over $( x , y , z ) \\sim D$ . But if $\\hat { Y }$ is conditionally dependent on $Z$ given $Y$ , then $H ( Z | \\hat { Y } , Y ) < H ( Z | Y )$ , so the adversary can achieve loss less than $H ( Z | Y )$ . □\n\nNote that Propositions 2 and 3 work analogously in the case of continuous $Y$ and $Z$ , with the probability mass function $P$ replaced with the probability density function $p$ , and the discrete entropy $H$ replaced by the differential entropy $h ( X ) = \\mathbb { E } [ - \\log { \\bar { p } ( x ) } ]$ , since the relevant property $( h ( A ) = h ( A | B )$ iff $A \\perp B$ ) holds for differential entropy as well. They also work analogously when the adversary $A$ is restricted to a limited set of predictors.\n\nFor example, an adversary using least-squares regression trying to enforce equality of odds can be thought of as one that outputs $A ( \\hat { y } , y ) ~ \\sim ~ N ( \\mu ( \\hat { y } , y ) , \\sigma ^ { 2 } )$ where $\\mu ( \\hat { y } , y )$ is the output of the regressor, and $\\sigma ^ { 2 } ~ > ~ 0$ is a fixed constant. Note now that the differential entropy $h ( Z | \\hat { Y } , Y ) =$ $\\mathbb { E } [ - \\log p ( z | \\hat { y } , y ) ]$ is nothing more than the expected loglikelihood, and so the function $\\mu$ that minimizes this quantity is the optimal least-squares regressor. Thus, for example, if we restrict $\\mu$ to be a linear function of $( \\hat { y } , y )$ , and the other\n\nconditions of Proposition 3 hold, then an analogous argument to the above propositions shows that $\\hat { Y }$ has no linear relationship with $Z$ after conditioning on $Y$ .\n\nThese claims together illustrate that a sufficiently powerful adversary trained on a sufficiently large training set can indeed accurately enforce the demographic parity or equality of odds constraints on the predictor, if the adversary and predictor converge. Guaranteed convergence is harder to achieve, both in theory and practice. In the practical scenarios below we discuss methods to encourage the training algorithm to converge, as well as reasonable choices of the adversary model that are both powerful and easy to train.",
      "referring_paragraphs": [
        "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.\n\nto the expression:",
        "Table 2: Features in the UCI dataset per individual.",
        "Details on the features that the dataset provides are available in Table 2."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1801.07593_page0_fig2.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1802.08139": [
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg",
      "image_filename": "1802.08139_page0_fig0.jpg",
      "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .",
      "context_before": "We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a causal approach to disregard effects along unfair pathways that simplifies and generalizes previous literature. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information, and does not require an often intractable computation of the path-specific effect. We leverage recent developments in deep learning and approximate inference to achieve a solution that is widely applicable to complex, non-linear scenarios.\n\n[Section: 1. Introduction]\n\nMachine learning is increasingly being used to take decisions that can severely affect people’s lives, e.g. in policing, education, hiring (Hoffman et al., 2015), lending, and criminal risk assessment (Dieterich et al., 2016). However, most often the training data contains bias that exists in our society. This bias can be absorbed or even amplified by the systems, leading to decisions that are unfair with respect to sensitive attributes (e.g. race and gender).\n\nIn response to calls from governments and institutions, the research community has recently started to address the issue of fairness through a variety of perspectives and frameworks. The simplest solution to this challenge is to down-weight or discard the sensitive attribute (Zeng et al., 2016). This can adversely impact model accuracy, and most often does not result in a fair procedure as the sensitive attribute might be correlated with the other attributes. A more sophisticated approach is to pre-process the data or extract representations that do not contain information about the sensitive attribute (Zemel et al., 2013; Feldman et al., 2015; Edwards & Storkey, 2016; Louizos et al., 2016; Calmon et al., 2017).\n\nBoth types of approach assume that the influence of the sensitive attribute on the decision is entirely unfair.\n\nIn an attempt to formalize different notions of fairness, the community has introduced several statistical criteria for establishing whether a decision system is fair (Dwork et al., 2012; Feldman et al., 2015; Chouldechova, 2017; Corbett-Davies et al., 2017), and algorithms have been developed to attain a given fairness criterion by imposing constraints into the optimization, or by using an adversary (Zafar et al., 2017; Zhang et al., 2018). However, it is often unclear which criterion is most appropriate for a given decision problem. Even more problematic is the fact that criteria that intuitively seem to correspond to similar types of fairness cannot always be concurrently satisfied on a dataset (Kleinberg et al., 2016; Berk et al., 2017; Chouldechova, 2017). Finally, approaches based on statistical relations among observations are in danger of not discerning correlation from causation, and are unable to distinguish the different ways in which the sensitive attribute might influence the decision.\n\nIt was recently argued (Qureshi et al., 2016; Bonchi et al., 2017; Kilbertus et al., 2017; Kusner et al., 2017; Nabi & Shpitser, 2018; Russell et al., 2017; Zhang et al., 2017; Zhang & Wu, 2017; Zhang & Bareinboim, 2018) that using a causal framework (Pearl, 2000; Dawid, 2007; Pearl et al., 2016; Peters et al., 2017) would lead to a more intuitive, powerful, and less error-prone way of reasoning about fairness. The suggestion is to view unfairness as the presence of an unfair causal effect of the sensitive attribute on the decision, as already done in Pearl (2000) and Pearl et al. (2016), e.g., to analyse the case of Berkeley’s alleged sex bias in graduate admission.\n\nKusner et al. (2017) recently introduced a causal definition of fairness, called counterfactual fairness, which states that a decision is fair toward an individual if it coincides with the one that would have been taken in a counterfactual world in which the sensitive attribute were different, and suggested a general algorithm to achieve this notion. This definition considers the entire effect of the sensitive attribute on the decision as problematic. However, in many practical scenarios this is not the case. For example, in the Berkeley alleged sex bias case, female applicants were rejected more often than male applicants as they were more often applying to departments with lower admission rates. Such an effect of",
      "context_after": "[Section: 2. Background on Causality]\n\nCausal relationships among random variables can be visually expressed using Graphical Causal Models (GCMs), which are a special case of Graphical Models (see Chiappa (2014) for a quick introduction on GMs). GCMs comprise nodes representing the variables, and links describing statistical and causal relationships among them. In this work, we will restrict ourselves to directed acyclic graphs, i.e. graphs in which a node cannot be an ancestor of itself. In directed acyclic graphs, the joint distribution over all nodes is given by the product of the conditional distributions of each node $X _ { i }$ given its parents par $( X _ { i } )$ , $p ( X _ { i } | \\mathfrak { p a r } ( X _ { i } ) )$ .\n\nGCMs enable us to give a graphical definition of causes and causal effects: if a variable $Y$ is a child of another variable $A$ , then $A$ is a direct cause of $Y$ . If $Y$ is a descendant of $A$ , then $A$ is a potential cause of $Y$ .\n\nIt follows that the causal effect of $A$ on $Y$ can be seen as the information that $A$ sends to $Y$ through causal paths, i.e. directed paths, or as the conditional distribution of $Y$ given $A$ restricted to those paths. This implies that the causal effect of $A$ on $Y$ coincides with $p ( Y | A )$ only if there are no open noncausal, i.e. undirected, paths between $A$ and $Y$ . An example of an open undirected path from $A$ to $Y$ is given by $A \\left. C \\right. Y$ in Fig. 1(a): the variable $C$ is said to be a confounder for the effect of $A$ on $Y$ .\n\nIf confounders are present, then the causal effect can be retrieved by intervening on $A$ , which replaces the conditional distribution of $A$ with, in the case considered in this paper, a fixed value $a$ . For the model in Fig. 1(a), intervening on $A$ by setting it to the fixed value $a$ would correspond to replacing $p ( A | C )$ with a delta distribution $\\delta _ { A = a }$ , thereby removing the link from $C$ to $A$ and leaving the remaining conditional distributions $p ( { \\boldsymbol { Y } } | A , C )$ and $p ( C )$ unaltered. After intervention, the causal effect is given by the conditional distribution in the intervened graph, namely by $\\begin{array} { r } { p ^ { * } ( Y | A = a ) = \\sum _ { C } p ( Y | A = a , C ) p ( C ) } \\end{array}$ .\n\nWe define $Y _ { A = a }$ to be the random variable that results from such constrained conditioning, i.e. with distribution $p ( Y _ { A = a } ) = p ^ { * } ( Y | A = a )$ . $Y _ { A = a }$ is called a potential outcome variable and, when not ambiguous, we will refer to it with the shorthand $Y _ { a }$ .\n\n[Section: 3. Causal Effect along Different Pathways]",
      "referring_paragraphs": [
        "Figure 1.",
        "Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1802.08139_page0_fig1.jpg",
        "(c)",
        "1802.08139_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg",
      "image_filename": "1802.08139_page0_fig3.jpg",
      "caption": "Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).",
      "context_before": "[Section: 4. Path-Specific Counterfactual Fairness]\n\nTo gain insights into the problem of path-specific fairness, consider the following linear model\n\nThe variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms with variance $\\sigma _ { a } ^ { 2 } , \\sigma _ { c } ^ { 2 } , \\sigma _ { m } ^ { 2 } , \\sigma _ { l } ^ { 2 }$ and $\\sigma _ { y } ^ { 2 }$ . The GCM corresponding to this model is depicted in Fig. 2(c).",
      "context_after": "The mean of this distribution is $\\theta ^ { y } + \\theta _ { m } ^ { y } \\theta ^ { m } + \\theta _ { l } ^ { y } ( \\theta _ { l } +$ $\\theta _ { m } ^ { l } \\theta ^ { m } ) + \\theta _ { a } ^ { y } a + \\theta _ { m } ^ { y } \\theta _ { a } ^ { m } a + \\theta _ { l } ^ { y } ( \\theta _ { a } ^ { l } a ^ { \\prime } + \\theta _ { m } ^ { l } \\ddot { \\theta _ { a } ^ { m } } a )$ . The PSE, namely difference between this quantity and the mean of the effect of $A \\ = \\ a ^ { \\prime }$ , is therefore given by $\\mathrm { { P S E } = }$ $\\theta _ { a } ^ { y } ( a - a ^ { \\prime } ) + \\theta _ { m } ^ { y } \\theta _ { a } ^ { m } ( a - a ^ { \\prime } ) + \\theta _ { l } ^ { y } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m } ( a - a ^ { \\prime } )$ which, as $a ^ { \\prime } = 0$ and $a = 1$ , simplifies to\n\nNabi & Shpitser (2018) suggest learning the subset of parameters $\\theta \\ = \\ \\{ \\theta ^ { m } , \\theta _ { a } ^ { m } , \\theta _ { c } ^ { m } , \\theta ^ { l } , \\theta _ { a } ^ { l } , \\theta _ { c } ^ { l } , \\theta _ { m } ^ { l } , \\theta ^ { y } , \\theta _ { a } ^ { y } , \\theta _ { c } ^ { y } , \\theta _ { m } ^ { y } , \\theta _ { l } ^ { y } \\}$ whilst constraining the PSE to lie in a small range. They then form a prediction ${ \\hat { y } } ^ { n }$ of a new instance $\\{ a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } \\}$ as the mean of $\\begin{array} { r l } { p ( Y | a ^ { n } , c ^ { n } ) } & { { } = } \\end{array}$ $\\begin{array} { r } { \\int _ { M , L } p ( Y | a ^ { n } , c ^ { n } , M , L ) p ( L | a ^ { n } , c ^ { n } , M ) p ( M | a ^ { n } , c ^ { n } ) . } \\end{array}$ , i.e. by integrating out $M$ and $L$ . Therefore, to form ${ \\hat { y } } ^ { n }$ , the individual-specific information within $m ^ { n }$ and $l ^ { n }$ is disregarded. With $\\Theta ^ { m } = \\theta ^ { m } + \\theta _ { a } ^ { m } + \\theta _ { c } ^ { m } c ^ { n }$ and $a ^ { n } = 1$ , $\\hat { y } ^ { n } = \\theta ^ { y } + \\theta _ { a } ^ { y } + \\theta _ { c } ^ { y } c ^ { n } + \\theta _ { m } ^ { y } \\Theta ^ { m } + \\theta _ { l } ^ { y } \\big ( \\theta ^ { l } + \\theta _ { a } ^ { l } + \\theta _ { c } ^ { l } c ^ { n } + \\theta _ { m } ^ { l } \\Theta ^ { m } \\big )$ If $a ^ { n } = 0$ the terms in red are omitted.\n\nNabi & Shpitser (2018) justify averaging over $M$ and $L$ due to the need to account for the constraints that are potentially imposed on $\\theta _ { a } ^ { m }$ and $\\theta _ { m } ^ { l }$ , and suggest that this is an inherent limitation of path-specific fairness. It is true that, if a constraint is imposed on a parameter, the corresponding variable\n\nneeds to be integrated out to ensure that such a constraint is taken into account in the prediction. However, all the descendants of $A$ along unfair pathways must be integrated out, regardless of whether or not constraints are imposed on the corresponding parameters. Indeed the observations $m ^ { n }$ and $l ^ { n }$ contain the unfair part of the effect from $A$ , which needs to be disregarded. To better understand this point, notice that we could obtain $\\mathrm { P S E } = \\theta _ { a } ^ { y } + \\theta _ { a } ^ { m } ( \\theta _ { m } ^ { y } + \\theta _ { l } ^ { y } \\theta _ { m } ^ { l } ) = 0$ by a priori imposing the constraints $\\theta _ { a } ^ { y } = 0$ and $\\theta _ { a } ^ { m } = 0$ , which would not constrain $\\theta _ { m } ^ { l }$ . However, to form a prediction ${ \\hat { y } } ^ { n }$ , we would still need to integrate over $L$ , as the observation $l ^ { n }$ contains the problematic term $\\theta _ { l } ^ { y } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m }$ .\n\nIn this simple case, we could avoid having to integrate over $M$ and $L$ by a priori imposing the constraints $\\theta _ { a } ^ { y } = 0$ and $\\theta _ { m } ^ { y } = - \\theta _ { l } ^ { y } \\theta _ { m } ^ { l }$ , i.e. constraining the conditional distribution used to form the prediction ${ \\hat { y } } ^ { n }$ , $p ( Y | A , C , M , L )$ . This coincides with the constraint proposed in Kilbertus et al. (2017) to avoid proxy discrimination. However, this approach achieves removal of the problematic terms in $m ^ { n }$ and $l ^ { n }$ by canceling out the entire $m ^ { n }$ from the prediction. This is also suboptimal, as all information within $m ^ { n }$ is disregarded. Furthermore, it is not clear how to extend this approach to more complex scenarios.\n\nOur insight is to notice that we can achieve a fair prediction of a new instance $\\{ a ^ { n } = 1 , c ^ { n } , m ^ { n } , l ^ { n } \\}$ whilst retaining all fair information using $\\hat { y } _ { \\mathrm { f a i r } } ^ { n } = \\langle Y \\rangle _ { p ( Y | a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } ) } - \\mathrm { P S E }$\n\nAlternatively, the same solution can be achieved as follows. We first estimate the values of the noise terms $\\epsilon _ { m } ^ { n }$ and $\\epsilon _ { l } ^ { n }$ from $a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n }$ m and the model equations, i.e. $\\epsilon _ { m } ^ { n } = $ $m ^ { n } - \\theta ^ { m } - \\theta _ { a } ^ { m } - \\theta _ { c } ^ { m } c ^ { n }$ and $\\epsilon _ { l } ^ { n } = \\theta ^ { l } - \\mathbf { \\bar { \\theta } } _ { a } ^ { l } - \\theta _ { c } ^ { l } c ^ { n } - \\theta _ { m } ^ { l } m ^ { n }$ . We then obtain fair transformations of $m ^ { n }$ and $l ^ { n }$ and a fair prediction ${ \\hat { y } } _ { \\mathrm { f a i r } } ^ { n }$ by substituting $\\epsilon _ { m } ^ { n }$ and $\\epsilon _ { l } ^ { n }$ into the model equations with the problematic terms $\\theta _ { a } ^ { m }$ and $\\theta _ { a } ^ { l }$ removed,",
      "referring_paragraphs": [
        "The variables $A , C , M , L$ and $Y$ are observed, whilst $\\epsilon _ { a }$ , $\\epsilon _ { c }$ , $\\epsilon _ { m }$ and $\\epsilon _ { l }$ are unobserved independent zero-mean Gaussian terms with variance $\\sigma _ { a } ^ { 2 } , \\sigma _ { c } ^ { 2 } , \\sigma _ { m } ^ { 2 } , \\sigma _ { l } ^ { 2 }$ and $\\sigma _ { y } ^ { 2 }$ . The GCM corresponding to this model is depicted in Fig. 2(c).",
        "Figure 2.",
        "For addressing a more general data-generation process mismatch than the one considered above, we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m }$ and $H _ { l }$ are",
        "Table 2."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1802.08139_page0_fig4.jpg",
        "(c)",
        "1802.08139_page0_fig5.jpg"
      ],
      "quality_score": 0.9,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig5.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_7",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg",
      "image_filename": "1802.08139_page0_fig6.jpg",
      "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .",
      "context_before": "Nabi & Shpitser (2018) suggest learning the subset of parameters $\\theta \\ = \\ \\{ \\theta ^ { m } , \\theta _ { a } ^ { m } , \\theta _ { c } ^ { m } , \\theta ^ { l } , \\theta _ { a } ^ { l } , \\theta _ { c } ^ { l } , \\theta _ { m } ^ { l } , \\theta ^ { y } , \\theta _ { a } ^ { y } , \\theta _ { c } ^ { y } , \\theta _ { m } ^ { y } , \\theta _ { l } ^ { y } \\}$ whilst constraining the PSE to lie in a small range. They then form a prediction ${ \\hat { y } } ^ { n }$ of a new instance $\\{ a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } \\}$ as the mean of $\\begin{array} { r l } { p ( Y | a ^ { n } , c ^ { n } ) } & { { } = } \\end{array}$ $\\begin{array} { r } { \\int _ { M , L } p ( Y | a ^ { n } , c ^ { n } , M , L ) p ( L | a ^ { n } , c ^ { n } , M ) p ( M | a ^ { n } , c ^ { n } ) . } \\end{array}$ , i.e. by integrating out $M$ and $L$ . Therefore, to form ${ \\hat { y } } ^ { n }$ , the individual-specific information within $m ^ { n }$ and $l ^ { n }$ is disregarded. With $\\Theta ^ { m } = \\theta ^ { m } + \\theta _ { a } ^ { m } + \\theta _ { c } ^ { m } c ^ { n }$ and $a ^ { n } = 1$ , $\\hat { y } ^ { n } = \\theta ^ { y } + \\theta _ { a } ^ { y } + \\theta _ { c } ^ { y } c ^ { n } + \\theta _ { m } ^ { y } \\Theta ^ { m } + \\theta _ { l } ^ { y } \\big ( \\theta ^ { l } + \\theta _ { a } ^ { l } + \\theta _ { c } ^ { l } c ^ { n } + \\theta _ { m } ^ { l } \\Theta ^ { m } \\big )$ If $a ^ { n } = 0$ the terms in red are omitted.\n\nNabi & Shpitser (2018) justify averaging over $M$ and $L$ due to the need to account for the constraints that are potentially imposed on $\\theta _ { a } ^ { m }$ and $\\theta _ { m } ^ { l }$ , and suggest that this is an inherent limitation of path-specific fairness. It is true that, if a constraint is imposed on a parameter, the corresponding variable\n\nneeds to be integrated out to ensure that such a constraint is taken into account in the prediction. However, all the descendants of $A$ along unfair pathways must be integrated out, regardless of whether or not constraints are imposed on the corresponding parameters. Indeed the observations $m ^ { n }$ and $l ^ { n }$ contain the unfair part of the effect from $A$ , which needs to be disregarded. To better understand this point, notice that we could obtain $\\mathrm { P S E } = \\theta _ { a } ^ { y } + \\theta _ { a } ^ { m } ( \\theta _ { m } ^ { y } + \\theta _ { l } ^ { y } \\theta _ { m } ^ { l } ) = 0$ by a priori imposing the constraints $\\theta _ { a } ^ { y } = 0$ and $\\theta _ { a } ^ { m } = 0$ , which would not constrain $\\theta _ { m } ^ { l }$ . However, to form a prediction ${ \\hat { y } } ^ { n }$ , we would still need to integrate over $L$ , as the observation $l ^ { n }$ contains the problematic term $\\theta _ { l } ^ { y } \\theta _ { m } ^ { l } \\theta _ { a } ^ { m }$ .\n\nIn this simple case, we could avoid having to integrate over $M$ and $L$ by a priori imposing the constraints $\\theta _ { a } ^ { y } = 0$ and $\\theta _ { m } ^ { y } = - \\theta _ { l } ^ { y } \\theta _ { m } ^ { l }$ , i.e. constraining the conditional distribution used to form the prediction ${ \\hat { y } } ^ { n }$ , $p ( Y | A , C , M , L )$ . This coincides with the constraint proposed in Kilbertus et al. (2017) to avoid proxy discrimination. However, this approach achieves removal of the problematic terms in $m ^ { n }$ and $l ^ { n }$ by canceling out the entire $m ^ { n }$ from the prediction. This is also suboptimal, as all information within $m ^ { n }$ is disregarded. Furthermore, it is not clear how to extend this approach to more complex scenarios.\n\nOur insight is to notice that we can achieve a fair prediction of a new instance $\\{ a ^ { n } = 1 , c ^ { n } , m ^ { n } , l ^ { n } \\}$ whilst retaining all fair information using $\\hat { y } _ { \\mathrm { f a i r } } ^ { n } = \\langle Y \\rangle _ { p ( Y | a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } ) } - \\mathrm { P S E }$\n\nAlternatively, the same solution can be achieved as follows. We first estimate the values of the noise terms $\\epsilon _ { m } ^ { n }$ and $\\epsilon _ { l } ^ { n }$ from $a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n }$ m and the model equations, i.e. $\\epsilon _ { m } ^ { n } = $ $m ^ { n } - \\theta ^ { m } - \\theta _ { a } ^ { m } - \\theta _ { c } ^ { m } c ^ { n }$ and $\\epsilon _ { l } ^ { n } = \\theta ^ { l } - \\mathbf { \\bar { \\theta } } _ { a } ^ { l } - \\theta _ { c } ^ { l } c ^ { n } - \\theta _ { m } ^ { l } m ^ { n }$ . We then obtain fair transformations of $m ^ { n }$ and $l ^ { n }$ and a fair prediction ${ \\hat { y } } _ { \\mathrm { f a i r } } ^ { n }$ by substituting $\\epsilon _ { m } ^ { n }$ and $\\epsilon _ { l } ^ { n }$ into the model equations with the problematic terms $\\theta _ { a } ^ { m }$ and $\\theta _ { a } ^ { l }$ removed,\n\nIn other words, we compute a fair prediction by intervening on $A$ , setting it to the baseline value along the links that create unfairness $A  M$ and $A  Y$ . This is effectively an extension of the procedure for performing counterfactual reasoning in structural equation models (Pearl, 2000), where the counterfactual correction is only restricted to the problematic links $A  M$ and $A  Y$ .\n\nModel-Observations Mismatch. Whilst this approach provides us with an elegant and straightforward way to",
      "context_after": "where $N _ { a }$ indicates the number of observations for which $\\boldsymbol { a } ^ { n } = \\boldsymbol { a }$ . We encourage $\\tilde { p } ( H _ { m } | A = a )$ to have small dependence on $A$ during training through the maximum mean discrepancy (MMD) criterion (Gretton et al., 2012). We can then use, e.g., the mean of $p ( H _ { m } | a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } )$ , rather than $\\epsilon _ { m } ^ { n }$ , in Eq. (4). $( p ( H _ { m } | a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } )$ is Gaussian and can be computed analytically, see the Appendix).\n\nKusner et al. (2017), who also use a latent-variable approach, do not enforce small dependence. To demonstrate that this is necessary, we learned the parameters of the modified model with Gaussian distribution $p ( H _ { m } )$ , using an expectation maximization approach. $\\tilde { p } ( H _ { m } | A )$ is shown in Fig. 3(a). As we can see, the extra term $f ( A , C )$ is absorbed by the latent variable. In other words, even if $p ( H _ { m } | A ) = p ( H _ { m } )$ , the mismatch between the model and the observations implies $\\tilde { p } ( H _ { m } | A ) \\neq \\tilde { p } ( H _ { m } )$ .\n\nIn the next section, we explain how the approach described above can be implemented in a general algorithm that is\n\napplicable to complex, non-linear models.\n\n[Section: 4.1. Latent Inference-Projection Approach]\n\nFor addressing a more general data-generation process mismatch than the one considered above, we need to explicitly incorporate a latent variable for each descendant of the sensitive attribute that needs to be corrected. General equations for the GCM of Fig. 2(c) with extra latent variables $H _ { m }$ and $H _ { l }$ are",
      "referring_paragraphs": [
        "Figure 3.",
        "Consider the GCM in Fig. 3(a), corresponding to Eq. (3) with the addition of a Gaussian latent variable $H _ { m } \\sim \\mathcal { N } ( \\theta ^ { h } , \\sigma _ { h } ^ { 2 } )$ in the equation for $M$ . The joint distribution $p ( Z = \\{ Y , L , M , C , H _ { m } \\} | A )$ is Gaussian with exponent proportional to $- { \\textstyle \\frac { 1 } { 2 } } \\big ( Z ^ { \\prime } N Z - 2 n \\big )$ with"
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(a)",
        "(b)",
        "1802.08139_page0_fig7.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig7.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_9",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg",
      "image_filename": "1802.08139_page0_fig8.jpg",
      "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.",
      "context_before": "If $M$ is categorical, $p _ { \\theta } ( M | A , C , H _ { m } ) \\ = \\ f _ { \\theta } ( A , C , H _ { m } )$ , where $f _ { \\theta } ( A , C , H _ { m } )$ can be any function, e.g. a neural network. If $M$ is continuous, $p _ { \\theta } ( M | A , C , H _ { m } )$ is Gaussian with mean $f _ { \\theta } ( A , C , H _ { m } )$ .\n\nThe model likelihood $p _ { \\theta } ( A , C , M , L , Y )$ , and posterior distributions $p _ { \\theta } ( H _ { m } | A , C , M , L )$ and $p _ { \\theta } ( H _ { l } | A , C , M , L )$ required to form fair predictions, are generally intractable. We address this with a variational approach that computes Gaussian approximations $q _ { \\phi } ( H _ { m } | A , C , L , M )$ and $q _ { \\phi } ( H _ { l } | A , C , L , M )$ , of $p _ { \\theta } ( H _ { m } | A , C , M , L )$ and $p _ { \\theta } ( H _ { l } | A , C , M , L )$ respectively, parametrized by $\\phi$ , as discussed in detail below.\n\nAfter learning $\\theta$ and $\\phi$ , to form a fair prediction ${ \\hat { y } } _ { \\mathrm { f a i r } } ^ { n }$ of a new instance $\\{ a ^ { n } = a , c ^ { n } , m ^ { n } , l ^ { n } \\}$ , we proceed analogously to Eq. (4)first draw samples $h _ { m } ^ { n , i } \\sim q _ { \\phi } ( H _ { m } | a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } )$ : weand $h _ { l } ^ { n , i } \\sim q _ { \\phi } ( H _ { l } | a ^ { n } , c ^ { n } , m ^ { n } , l ^ { n } )$ , for $i = 1 , \\dots , I$ , and then form\n\nIf we group the observed and latent variables as $V =$ $\\{ A , C , M , L , Y \\}$ and $H \\ = \\ \\{ H _ { m } , H _ { l } \\}$ respectively, the variational approximation $q _ { \\phi } ( H | V )$ to the intractable posterior $p _ { \\theta } ( H | V )$ is obtained by finding the variational parameters $\\phi$ that minimize the Kullback-Leibler divergence $K L ( q _ { \\phi } ( H | V ) | | p _ { \\theta } ( H | V ) )$ . This is equivalent to maximizing a lower bound $\\mathcal { F } _ { \\theta , \\phi }$ on the log marginal likelihood $\\log p _ { \\theta } ( V ) \\geq \\mathcal { F } _ { \\theta , \\phi }$ with\n\nIn our case, rather than $q ( H | V )$ , we use $q ( H | V ^ { * } \\equiv V \\setminus Y )$ . Our approach is therefore to learn simultaneously the latent embedding and predictive distributions in Eq. (5).",
      "context_after": "The first part of the gradient of $\\mathcal { F } _ { \\theta , \\phi }$ with respect to $\\phi$ , $\\nabla _ { \\phi } \\mathcal { F } _ { \\theta , \\phi }$ , can be computed analytically, whilst the second part is approximated by\n\nThe variational parameters $\\phi$ are parametrized by a neural network taking as input $V ^ { * }$ . In order to ensure that $\\tilde { q } ( H | A )$ does not depend on $A$ , we follow a MMD penalization approach as in Louizos et al. (2016). This approach adds a MMD estimator term, approximated using random Fourier features, to the bound $\\mathcal { F } _ { \\theta , \\phi }$ , weighted by a factor $\\beta$ .\n\n[Section: 5. Experiments]",
      "referring_paragraphs": [
        "Figure 4."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1802.08139_page0_fig9.jpg",
        "1802.08139_page0_fig10.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig10.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_12",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg",
      "image_filename": "1802.08139_page0_fig11.jpg",
      "caption": "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 6. Histograms of $\\widetilde { q } ( H _ { s } | A )$ after 2,000 and 8,000 training steps for one dimension of the variable housing."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1802.08139_page0_fig12.jpg",
        "1802.08139_page0_fig13.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig13.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_15",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg",
      "image_filename": "1802.08139_page0_fig14.jpg",
      "caption": "Figure 5. Histograms of (one dimension of) $\\tilde { q } ( H _ { m } | A )$ after 5,000, 8,000, 15,000 and 20,000 training steps.",
      "context_before": "",
      "context_after": "[Section: 5.3. The UCI German Credit Dataset]\n\nThe German Credit dataset from the UCI repository contains 20 attributes of 1,000 individuals applying for loans. Each applicant is classified as a good or bad credit risk, i.e. as likely or not likely to repay the loan. We assume the GCM in Fig. 4(b), where $A$ corresponds to the protected attribute sex, $C$ to age, $S$ to the triple status of checking account, savings, and housing, and $R$ the duple credit amount and repayment duration. The attributes age, credit amount, and repayment duration are continuous, whilst checking account, savings, and housing are categorical. Besides the direct effect $A  Y$ , we would like to remove the effect of $A$ on $Y$ through $S$ . We only need to introduce a hidden variable $H _ { s }$ for $S$ , as $R$ does not need to be corrected.\n\nWe divided the dataset into training and test sets of sizes 700 and 300 respectively. We used $\\beta = 0$ for the first 2,000 training steps, and $\\beta = 1 0 0$ afterward. For the Monte-Carlo approximation in Eq. (5), we used $I = 1 0 0 0$ . Counterfactual correction was performed for both males and females.\n\nTable 2. In order columns represent: unfair test accuracy, fair test accuracy, and MMD values for $H _ { s }$ $( \\times ~ 1 0 0 )$ for the UCI German Credit dataset. Rows represent values after 2,000, 4,000, and 8,000 training steps.   \n\n<table><tr><td>74.67%</td><td>73.67%</td><td>1.12</td><td>2.82</td><td>5.47</td></tr><tr><td>76.33%</td><td>76.00%</td><td>1.23</td><td>2.38</td><td>2.27</td></tr><tr><td>76.00%</td><td>76.00%</td><td>1.27</td><td>2.20</td><td>1.79</td></tr></table>\n\nIn Table 2, we show the unfair and fair test accuracy and the MMD values for $H _ { s }$ after 2,000, 4,000, and 8,000 training steps (the results remain similar with a higher number of training steps). As we can see, unfair and fair accuracy, and MMD values are similar for all iterations. This indicates that, unlike the Adult dataset, model-observations mismatch is not problematic. This is confirmed by $\\widetilde { q } ( H _ { s } | A )$ ; we learn a structured distribution which does not differ significantly for females and males. In Fig. 6, we show $\\widetilde { q } ( H _ { s } | A )$ for one dimension of the variable housing, which shows the most significant difference between females and males. The remaining variables are shown in the Appendix.\n\n[Section: 6. Conclusions]",
      "referring_paragraphs": [
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1802.08139_page0_fig15.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig15.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_17",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
      "image_filename": "1802.08139_page0_fig16.jpg",
      "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.",
      "context_before": "We have introduced a latent inference-projection method to achieve path-specific counterfactual fairness which simplifies, generalizes and outperforms previous literature. A fair decision is achieved by correcting the variables that are descendants of the protected attribute along unfair pathways, rather than by imposing constraints on the model parameters. This enables us to retain fair information contained in the problematic descendants and to leave unaltered the underlying data-generation mechanism. In the future, we plan to investigate alternative techniques to MMD for enforcing independence between the latent space and the sensitive attribute.\n\n[Section: References]\n\nBerk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. Fairness in criminal justice risk assessments: The state state of the art. 2017.   \nBonchi, F., Hajian, S., Mishra, B., and Ramazzotti, D. Exposing the probabilistic causal structure of discrimination. International Journal of Data Science and Analytics, 3 (1):1–21, 2017.   \nCalmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K. N., and Varshney, K. R. Optimized pre-processing for discrimination prevention. In Advances in Neural Information Processing Systems 30, pp. 3995–4004, 2017.   \nChiappa, S. Explicit-duration Markov switching models. Foundations and Trends in Machine Learning, 7(6):803– 886, 2014.   \nChouldechova, A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2):153–163, 2017.   \nCorbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 797806, 2017.   \nDawid, P. Fundamentals of statistical causality. Technical report, University Colledge London, 2007.   \nDieterich, W., Mendoza, C., and Brennan, T. Compas risk scales: Demonstrating accuracy equity and predictive parity, 2016.   \nDwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pp. 214–226, 2012.   \nEdwards, H. and Storkey, A. Censoring representations with an adversary. In 4th International Conference on Learning Representations, 2016.   \nFeldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., and Venkatasubramanian, S. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 259–268, 2015.   \nGretton, A., Borgwardt, K. M., Rasch, M. J., Scholkopf, ¨ B., and Smola, A. A kernel two-sample test. Journal of Machine Learning Research, 13:723–773, 2012.   \nHoffman, M., Kahn, L. B., and Li, D. Discretion in hiring, 2015.\n\nKilbertus, N., Rojas-Carulla, M., Parascandolo, G., Hardt, M., Janzing, D., and Scholkopf, B. Avoiding discrimi-¨ nation through causal reasoning. In Advances in Neural Information Processing Systems 30, pp. 656–666, 2017.   \nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, 2015.   \nKingma, D. P. and Welling, M. Auto-encoding variational Bayes. In 2nd International Conference on Learning Representations, 2014.   \nKleinberg, J., Mullainathan, S., and Raghavan, M. Inherent trade-offs in the fair determination of risk scores. In 8th Innovations in Theoretical Computer Science Conference, pp. 43:1–43:23, 2016.   \nKusner, M. J., Loftus, J. R., Russell, C., and Silva, R. Counterfactual fairness. In Advances in Neural Information Processing Systems 30, pp. 4069–4079, 2017.   \nLichman, M. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.   \nLouizos, C., Swersky, K., Li, Y., Welling, M., and Zemel, R. The variational fair autoencoder. In 4th International Conference on Learning Representations, 2016.   \nNabi, R. and Shpitser, I. Fair inference on outcomes. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.   \nPearl, J. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.   \nPearl, J. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, pp. 411–420, 2001.   \nPearl, J. The causal mediation formula – a guide to the assessment of pathways and mechanisms. Prevention Science, 13:426–436, 2012.   \nPearl, J., Glymour, M., and Jewell, N. P. Causal Inference in Statistics: A Primer. Wiley, 2016.   \nPeters, J., Janzing, D., and Scholkopf, B. ¨ Elements of Causal Inference: Foundations and Learning Algorithms. MIT Press, 2017.   \nQureshi, B., Kamiran, F., Karim, A., and Ruggieri, S. Causal discrimination discovery through propensity score analysis. ArXiv e-prints, 2016.   \nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286, 2014.\n\nRussell, C., Kusner, M. J., Loftus, J., and Silva, R. When worlds collide: Integrating different counterfactual assumptions in fairness. In Advances in Neural Information Processing Systems 30, pp. 6417–6426, 2017.   \nShpitser, I. Counterfactual graphical models for longitudinal mediation analysis with unobserved confounding. Cognitive Science, 37(6):1011–1035, 2013.   \nZafar, B., Valera, I., Gomez-Rodriguez, M., and Gummadi, K. Fairness constraints: Mechanisms for fair classification. In 20th International Conference on Artificial Intelligence and Statistics, 2017.   \nZemel, R, Wu, Y., Swersky, K., Pitassi, T., and Dwork, C. Learning fair representations. In Proceedings of the 30th International Conference on Machine Learning, pp. 325–333, 2013.   \nZeng, J., Ustun, B., and Rudin, C. Interpretable classification models for recidivism prediction. Journal of the Royal Stat. Soc.: Series A (Statistics in Society), 2016.   \nZhang, B., Lemoine, H, and Mitchell, M. Mitigating unwanted biases with adversarial learning. In Proceedings of the 1st AAAI/ACM Conference on AI, Ethics, and Society, 2018.   \nZhang, J. and Bareinboim, E. Fairness in decision-making – the causal explanation formula. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018.   \nZhang, L. and Wu, X. Anti-discrimination learning: a causal modeling-based framework. International Journal of Data Science and Analytics, pp. 1–16, 2017.   \nZhang, L., Wu, Y., and Wu, X. A causal framework for discovering and removing direct and indirect discrimination. In IJCAI, pp. 3929–3935, 2017.",
      "context_after": "[Section: A. Identifiability of PSE]\n\nWe summarize the method described in Shpitser (2013) to graphically establish whether a PSE is identifiable.\n\nAcyclic Directed Mixed Graph (ADMG): An ADMG is a causal graph containing two kinds of links, directed links (either green or black depending on whether we are interested in the corresponding causal path), and red bidirected links, indicating the presence of an unobserved common cause. The ADMG corresponding to Fig. 7(a) is given by Fig. 7(b).\n\nDistrict: The set of nodes in an ADMG that are reachable from $A$ through bidirected paths is called the district of $A$ . For example, the district of $Y$ in Fig. 7(b) is $\\{ M , Y \\}$ .\n\nRecanting District: Let $\\mathcal { G }$ be an ADMG, and $\\pi$ a subset of causal paths which start in $A$ and end in $Y$ . Let $\\nu$ be the set of potential causes of $Y$ that differ from $A$ and that influence $Y$ through causal paths that do not intersect $A$ . Let $\\mathcal { G } _ { \\nu }$ be the subgraph of $\\mathcal { G }$ containing only the nodes in $\\nu$ . A district $D$ in $\\mathcal { G } _ { \\nu }$ is called the recanting district for the effect of $A$ on $Y$ along the paths in $\\pi$ if there exist nodes $X _ { i } , X _ { j } \\in D$ such that there is a causal path $A \\to X _ { i } \\to \\ldots \\to Y \\in \\pi$ and a causal path $A \\to X _ { j } \\to \\ldots \\to Y \\not \\in \\pi$ . If $\\mathcal { G } _ { \\nu }$ contains a recanting district for $\\pi$ , then the effect along $\\pi$ is non-identifiable.\n\nFor example, the set $\\nu$ in Fig. 7(b) is $\\{ M , W , Y \\}$ . The districts in $\\mathcal { G } _ { \\nu }$ are $\\{ M , Y \\}$ . This district is recanting for the effect along $A  Y$ , as $A \\to Y \\in \\pi$ , whilst $A \\to M \\to Y \\notin \\pi$ . (This district is not recanting for the effect along $A \\to W \\to Y .$ .)\n\n[Section: B. Latent-Variable Conditional Distribution]",
      "referring_paragraphs": [
        "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "1802.08139_page0_fig17.jpg",
        "1802.08139_page0_fig18.jpg",
        "1802.08139_page0_fig19.jpg",
        "1802.08139_page0_fig20.jpg",
        "1802.08139_page0_fig21.jpg",
        "1802.08139_page0_fig22.jpg",
        "1802.08139_page0_fig23.jpg",
        "1802.08139_page0_fig24.jpg",
        "1802.08139_page0_fig25.jpg",
        "1802.08139_page0_fig26.jpg",
        "1802.08139_page0_fig27.jpg",
        "(a)",
        "1802.08139_page0_fig28.jpg",
        "1802.08139_page0_fig29.jpg",
        "1802.08139_page0_fig30.jpg",
        "1802.08139_page0_fig31.jpg",
        "1802.08139_page0_fig32.jpg",
        "1802.08139_page0_fig33.jpg",
        "1802.08139_page0_fig34.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig24.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig25.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig26.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig27.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig28.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig29.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig30.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig31.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig32.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig33.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig34.jpg"
        ],
        "panel_count": 19
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_36",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg",
      "image_filename": "1802.08139_page0_fig35.jpg",
      "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.",
      "context_before": "",
      "context_after": "[Section: C. Experimental Details]\n\nFor all datasets, as the prior distribution $p$ for each latent variable we used a mixture of two-dimensional Gaussians with ten mixture components and diagonal covariances. As the variational posterior distribution $q$ we used a two-dimensional Gaussian with diagonal covariance, with means and log variances obtained as the outputs of a neural network with two linear layers of size 20 and tanh activation, followed by a linear layer. In the conditional distributions, $f _ { \\theta }$ was a neural network with one linear layer of size 100 with tanh activation, followed by a linear layer. The outputs were Gaussian means for continuous variables and logits for categorical variables. We used the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.01, mini-batch size 128, and default values $\\beta _ { 1 } = 0 . 9$ , $\\beta _ { 2 } = 0 . 9 9 9$ , and $\\epsilon = 1 e { - } 8$ .\n\n[Section: C.1. UCI Adult Dataset]",
      "referring_paragraphs": [
        "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.",
        "In Fig. 8 we show histograms for prior and posterior distributions in the latent space."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1802.08139_page0_fig36.jpg",
        "(b)",
        "1802.08139_page0_fig37.jpg",
        "1802.08139_page0_fig38.jpg",
        "1802.08139_page0_fig39.jpg",
        "1802.08139_page0_fig40.jpg",
        "1802.08139_page0_fig41.jpg",
        "1802.08139_page0_fig42.jpg",
        "1802.08139_page0_fig43.jpg",
        "1802.08139_page0_fig44.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig36.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig37.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig38.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig39.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig40.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig41.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig42.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig43.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig44.jpg"
        ],
        "panel_count": 10
      }
    },
    {
      "doc_id": "1802.08139",
      "figure_id": "1802.08139_fig_46",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg",
      "image_filename": "1802.08139_page0_fig45.jpg",
      "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).",
      "context_before": "",
      "context_after": "[Section: C.2. UCI German Credit Dataset]\n\nIn Fig. 9 we show histograms for posterior distributions in the latent space.",
      "referring_paragraphs": [
        "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).",
        "In Fig. 9 we show histograms for posterior distributions in the latent space."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1802.08139_page0_fig46.jpg",
        "1802.08139_page0_fig47.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig46.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig47.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1803.04383": [
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig0.jpg",
      "image_filename": "1803.04383_page0_fig0.jpg",
      "caption": "",
      "context_before": "We remark that many of our results also go through if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau )$ simply refers to an abstract change in well-being, not necessarily a change in the mean score. Furthermore, it is possible to modify the definition of $\\Delta \\mu _ { \\mathrm { j } } ( \\tau )$ such that it directly considers outcomes of those who are not selected.2 Lastly, we assume that the success of an individual is independent of their group given the score; that is, the score summarizes all relevant information about the success event, so there exists a function $\\rho : \\mathcal { X }  [ 0 , 1 ]$ such that individuals of score $x$ succeed with probability $\\rho ( x )$ .\n\nWe now introduce the specific domain of credit scores as a running example in the rest of the paper, after which we present two more examples showing the general applicability of our formulation to many domains.\n\nExample 2.1 (Credit scores). In the setting of loans, scores $x \\in [ C ]$ represent credit scores, and the bank serves as the institution. The bank chooses to grant or refuse loans to individuals according to a policy $_ \\tau$ . Both bank and personal utilities are given as functions of loan repayment, and\n\ntherefore depend on the success probabilities $\\rho ( x )$ , representing the probability that any individual with credit score $x$ can repay a loan within a fixed time frame. The expected utility to the bank is given by the expected return from a loan, which can be modeled as an affine function of $\\rho ( x )$ : $\\pmb { u } ( x ) = u _ { + } \\pmb { \\rho } ( x ) + u _ { - } ( 1 - \\pmb { \\rho } ( x ) )$ , where $u _ { + }$ denotes the profit when loans are repaid and $u _ { - }$ the loss when they are defaulted on. Individual outcomes of being granted a loan are based on whether or not an individual repays the loan, and a simple model for $\\Delta ( x )$ may also be affine in $\\rho ( x )$ : $\\Delta ( x ) = c _ { + } \\rho ( x ) + c _ { - } ( 1 - \\rho ( x ) )$ , modified accordingly at boundary states. The constant $c _ { + }$ denotes the gain in credit score if loans are repaid and $c _ { - }$ is the score penalty in case of default.\n\nExample 2.2 (Advertising). A second illustrative example is given by the case of advertising agencies making decisions about which groups to target. An individual with product interest score $x$ responds positively to an ad with probability $\\rho ( x )$ . The ad agency experiences utility $\\pmb { u } ( \\boldsymbol { x } )$ related to click-through rates, which increases with $\\rho ( x )$ . Individuals who see the ad but are uninterested may react negatively (becoming less interested in the product), and $\\Delta ( x )$ encodes the interest change. If the product is a positive good like education or employment opportunities, interest can correspond to well-being. Thus the advertising agency’s incentives to only show ads to individuals with extremely high interest may leave behind groups whose interest is lower on average. A related historical example occurred in advertisements for computers in the 1980s, where male consumers were targeted over female consumers, arguably contributing to the current gender gap in computing.\n\nExample 2.3 (College Admissions). The scenario of college admissions or scholarship allotments can also be considered within our framework. Colleges may select certain applicants for acceptance according to a score $x$ , which could be thought encode a “college preparedness” measure. The students who are admitted might “succeed” (this could be interpreted as graduating, graduating with honors, finding a job placement, etc.) with some probability $\\rho ( x )$ depending on their preparedness. The college might experience a utility $\\pmb { u } ( \\boldsymbol { x } )$ corresponding to alumni donations, or positive rating when a student succeeds; they might also show a drop in rating or a loss of invested scholarship money when a student is unsuccessful. The student’s success in college will affect their later success, which could be modeled generally by $\\Delta ( x )$ . In this scenario, it is challenging to ensure that a single summary statistic $x$ captures enough information about a student; it may be more appropriate to consider $x$ as a vector as well as more complex forms of $\\rho ( x )$ .\n\nWhile a variety of applications are modeled faithfully within our framework, there are limitations to the accuracy with which real-life phenomenon can be measured by strictly binary decisions and success probabilities. Such binary rules are necessary for the definition and execution of existing fairness criteria, (see Sec. 2.2) and as we will see, even modeling these facets of decision making as binary allows for complex and interesting behavior.\n\n[Section: 2.1 The Outcome Curve]\n\nWe now introduce important outcome regimes, stated in terms of the change in average group score. A policy $( \\tau _ { \\mathsf { A } } , \\tau _ { \\mathsf { B } } )$ is said to cause active harm to group j if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) < 0$ , stagnation if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) = 0$ , and improvement if $\\Delta \\mu _ { \\mathrm { j } } ( \\tau _ { \\mathrm { j } } ) > 0$ . Under our model, MaxUtil policies can be chosen in a standard fashion which applies the same threshold $\\tau ^ { \\mathrm { M a x U t i 1 } }$ for both groups, and is agnostic to the distributions $\\pi _ { \\mathsf { A } }$ and $\\pi _ { \\mathsf { B } }$ . Hence, if we define",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {}
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg",
      "image_filename": "1803.04383_page0_fig1.jpg",
      "caption": "Figure 1: The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.",
      "context_before": "",
      "context_after": "[Section: 2.2 Decision Rules and Fairness Criteria]\n\nWe will consider policies that maximize the institution’s total expected utility, potentially subject to a constraint: $\\tau \\in \\mathcal { C } \\in [ 0 , 1 ] ^ { 2 C }$ which enforces some notion of “fairness”. Formally, the institution selects $\\tau _ { * } \\in$ argmax $\\mathcal { U } ( \\tau )$ s.t. $\\tau \\in \\mathcal { C }$ . We consider the three following constraints:\n\nDefinition 2.2 (Fairness criteria). The maximum utility (MaxUtil) policy corresponds to the nullconstraint $\\begin{array} { r } { \\mathcal { C } = [ 0 , 1 ] ^ { 2 C } } \\end{array}$ , so that the institution is free to focus solely on utility. The demographic parity (DemParity) policy results in equal selection rates between both groups. Formally, the constraint is $\\begin{array} { r } { \\mathcal { C } = \\left\\{ ( \\tau _ { \\mathsf { A } } , \\tau _ { \\mathsf { B } } ) : \\sum _ { \\boldsymbol { x } \\in \\mathcal { X } } \\pi _ { \\mathsf { A } } ( \\boldsymbol { x } ) \\tau _ { \\mathsf { A } } = \\sum _ { \\boldsymbol { x } \\in \\mathcal { X } } \\pi _ { \\mathsf { B } } ( \\boldsymbol { x } ) \\tau _ { \\mathsf { B } } \\right\\} } \\end{array}$ . The equal opportunity (EqOpt) policy results in equal true positive rates (TPR) between both group, where TPR is defined as T $\\begin{array} { r } { \\mathrm { { { \\vec { \\rho } R _ { j } } } } ( \\tau ) : = \\frac { \\sum _ { x \\in \\mathcal { X } } \\pi _ { \\mathrm { j } } ( x ) \\rho ( x ) \\tau ( x ) } { \\sum _ { x \\in \\mathcal { X } } \\pi _ { \\mathrm { j } } ( x ) \\rho ( x ) } } \\end{array}$ Px∈X πj(x)ρ(x) . EqOpt ensures that the conditional probability of selection given that the individual will be successful is independent of the population, formally enforced by the constraint $\\mathcal { C } = \\{ ( \\pmb { \\tau } _ { \\mathsf { A } } , \\pmb { \\tau } _ { \\mathsf { B } } ) : \\mathrm { T P R } _ { \\mathsf { A } } ( \\pmb { \\tau } _ { \\mathsf { A } } ) = \\mathrm { T P R } _ { \\mathsf { B } } ( \\pmb { \\tau } _ { \\mathsf { B } } ) \\}$ .\n\nJust as the expected outcome $\\Delta \\mu$ can be expressed in terms of selection rate for threshold policies, so can the total utility $\\mathcal { U }$ . In the unconstrained cause, $\\boldsymbol { \\mathcal { U } }$ varies independently over the selection rates for group A and B; however, in the presence of fairness constraints the selection rate for one group determines the allowable selection rate for the other. The selection rates must be equal for DemParity, but for EqOpt we can define a transfer function, $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ , which for every loan rate $\\beta$ in group A gives the loan rate in group B that has the same true positive rate. Therefore, when considering threshold policies, decision rules amount to maximizing functions of single parameters. This idea is expressed in Figure 2, and underpins the results to follow.\n\n[Section: 3 Results]",
      "referring_paragraphs": [
        "We introduce the notion of an outcome curve (Figure 1) which succinctly describes the different regimes in which one criterion is preferable over the others.",
        "Figure 1: The above figure shows the outcome curve.",
        "Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure 1) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1803.04383_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg",
      "image_filename": "1803.04383_page0_fig3.jpg",
      "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.",
      "context_before": "In order to clearly characterize the outcome of applying fairness constraints, we make the following assumption.\n\nAssumption 1 (Institution utilities). The institution’s individual utility function is more stringent than the expected score changes, ${ \\pmb u } ( { \\boldsymbol x } ) > 0 \\implies { \\pmb \\Delta } ( { \\boldsymbol x } ) > 0$ . (For the linear form presented in Example 2.1, u+ $\\frac { u _ { - } } { u _ { + } } < \\frac { c _ { - } } { c _ { + } }$ c− is necessary and sufficient.)\n\nThis simplifying assumption quantifies the intuitive notion that institutions take a greater risk by accepting than the individual does by applying. For example, in the credit setting, a bank loses the amount loaned in the case of a default, but makes only interest in case of a payback. Using Assumption 1, we can restrict the position of MaxUtil on the outcome curve in the following sense.\n\nProposition 3.1 (MaxUtil does not cause active harm). Under Assumption 1, $0 \\leq \\Delta \\mu ^ { \\mathrm { M a x U t i 1 } } \\leq$ $\\Delta \\mu ^ { * }$ .\n\nWe direct the reader to Appendix C for the proof of the above proposition, and all subsequent results presented in this section. The results are corollaries to theorems presented in Section 6.\n\n[Section: 3.1 Prospects and Pitfalls of Fairness Criteria]\n\nWe begin by characterizing general settings under which fairness criteria act to improve outcomes over unconstrained MaxUtil strategies. For this result, we will assume that group A is disadvantaged",
      "context_after": "[Section: 3.2 Comparing EqOpt and DemParity]\n\nOur analysis of the acceptance rates of EqOpt and DemParity in Section 6 suggests that it is difficult to compare DemParity and EqOpt without knowing the full distributions $\\pi _ { \\mathsf { A } } , \\pi _ { \\mathsf { B } }$ , which is necessary to compute the transfer function $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemParity causes improvement while EqOpt causes harm. There cannot be one general rule as to which fairness criteria provides better outcomes in all settings. We now present simple sufficient conditions on the geometry of the distributions for which EqOpt is always better than DemParity in terms of $\\Delta \\pmb { \\mu } _ { \\mathsf { A } }$ .\n\nCorollary 3.5 (EqOpt may avoid active harm where DemParity fails). Fix a selection rate $\\beta$ . Suppose $\\pi _ { \\mathsf { A } } , \\pi _ { \\mathsf { B } }$ are identical up to a translation with $\\mu _ { \\mathsf { A } } ~ < ~ \\mu _ { \\mathsf { B } }$ , i.e. $\\pi _ { \\mathsf { A } } ( x ) \\ = \\ \\pi _ { \\mathsf { B } } ( x + ( \\pmb { \\mu } _ { \\mathsf { B } } - \\pmb { \\mu } _ { \\mathsf { A } } ) )$ . For simplicity, take $\\rho ( x )$ to be linear in $x$ . Suppose\n\nThen there exists an interval $[ g _ { 1 } , g _ { 2 } ] ~ \\subseteq ~ [ 0 , 1 ]$ , such that ∀gA > g1, $\\beta ^ { \\tt E q 0 p t } ~ < ~ \\beta$ while $\\forall g _ { \\mathsf { A } } < g _ { 2 }$ $\\beta ^ { \\mathrm { D e m P a r i t y } } ~ > ~ \\beta$ . In particular, when $\\beta ~ = ~ \\beta _ { 0 }$ , this implies DemParity causes active harm but EqOpt causes improvement for $g _ { \\mathsf { A } } \\in \\mathsf { \\Gamma } [ g _ { 1 } , g _ { 2 } ]$ , but for any gA such that DemParity causes improvement, EqOpt also causes improvement.\n\nTo interpret the conditions under which Corollary 3.5 holds, consider when we might have $\\beta _ { 0 } > \\sum _ { x > \\mu _ { \\tt A } } \\pi _ { \\tt A }$ . This is precisely when $\\begin{array} { r } { \\Delta \\mu _ { \\mathrm { A } } ( \\sum _ { x > \\mu _ { \\mathrm { A } } } \\pi _ { \\mathrm { A } } ) > 0 } \\end{array}$ , that is, $\\Delta \\mu _ { \\mathsf { A } } > 0$ for a policy that A Aselects every individual whose score is above the group A mean, which is reasonable in reality.\n\nIndeed, the converse would imply that group A has such low scores that even selecting all above average individuals in A would hurt the average score. In such a case, Corollary 3.5 suggests that EqOpt is better than DemParity at avoiding active harm, because it is more conservative. A natural question then is: can EqOpt cause relative harm by being too stingy?\n\nCorollary 3.6 (DemParity never loans less than MaxUtil, but EqOpt might). Recall the definition of the TPR functions TPRj, and suppose that the MaxUtil policy τ MaxUtil is such that",
      "referring_paragraphs": [
        "This idea is expressed in Figure 2, and underpins the results to follow.",
        "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group.",
        "The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1803.04383_page0_fig4.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg",
      "image_filename": "1803.04383_page0_fig5.jpg",
      "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).",
      "context_before": "For $f$ supported on $[ a , b ]$ , we say that $f$ is left- (resp. right-) differentiable if $\\partial _ { - } f ( x )$ exists for all $x \\in ( a , b ]$ (resp. $\\partial _ { + } f ( y )$ exists for all $y \\in [ a , b )$ ). We now state the fundamental derivative computation which underpins the results to follow:\n\nLemma 5.3. Let $e _ { x }$ denote the vector such that $e _ { x } ( x ) = 1$ , and $e _ { x } ( x ^ { \\prime } ) = 0$ for $x ^ { \\prime } \\neq x$ . Then $\\pi _ { \\mathrm { j } } \\circ r _ { \\pi _ { \\mathrm { j } } } ^ { - 1 } ( \\beta ) : [ 0 , 1 ]  [ 0 , 1 ] ^ { \\cup }$ is continuous, and has left and right derivatives\n\nThe above lemma is proved in Appendix A.3. Moreover, Lemma 5.3 implies that the outcome curve is concave under the assumption that $\\Delta ( x )$ is monotone:\n\nProposition 5.3. Let $\\pi$ be a distribution over $C$ states. Then $\\beta \\mapsto \\Delta \\pmb { \\mu } ( r _ { \\pi } ^ { - 1 } ( \\beta ) )$ is concave. In fact, if $w ( x )$ is any non-decreasing map from $\\mathcal { X }  \\mathbb { R }$ , $\\beta \\mapsto \\langle { \\pmb w } , r _ { \\pmb \\pi } ^ { - 1 } ( \\beta ) \\rangle$ is concave.\n\nProof. Recall that a univariate function $f$ is concave (and finite) on $[ a , b ]$ if and only (a) $f$ is left- and right-differentiable, (b) for all $x \\in ( a , b )$ , $\\partial _ { - } f ( x ) \\geq \\partial _ { + } f ( x )$ and (c) for any $x > y$ , $\\partial _ { - } f ( x ) \\leq \\partial _ { + } f ( y )$ .\n\nObserve that $\\Delta \\mu ( r _ { \\pi } ^ { - 1 } ( \\beta ) ) = \\langle \\Delta , \\pi \\circ r _ { \\pi } ^ { - 1 } ( \\beta ) \\rangle$ . By Lemma 5.3, $\\pi \\circ r _ { \\pi } ^ { - 1 } ( \\beta )$ has right and left derivatives $e _ { \\mathrm { Q } \\left( \\beta \\right) }$ and $e _ { \\mathrm { Q } ^ { + } \\left( \\beta \\right) }$ . Hence, we have that\n\nUsing the fact that $\\Delta ( x )$ is monotone, and that $\\mathrm { Q \\leq Q ^ { + } }$ , we see that $\\partial _ { + } \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) ) \\leq \\partial _ { - } \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) )$ , and that $\\partial \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) )$ and $\\partial _ { + } \\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( \\beta _ { \\mathsf { B } } ) )$ are non-increasing, from which it follows that $\\Delta \\mu ( f _ { \\pi } ^ { - 1 } ( { \\boldsymbol \\beta } _ { \\mathsf { B } } ) )$ is concave. The general concavity result holds by replacing $\\Delta ( x )$ with $w ( x )$ .",
      "context_after": "Utility Contour Plot   \nFigure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).\n\n[Section: 6 Proofs of Main Theorems]\n\nWe are now ready to present and prove theorems that characterize the selection rates under fairness constraints, namely DemParity and EqOpt. These characterizations are crucial for proving the results in Section 3. Our computations also generalize readily to other linear constraints, in a way that will become clear in Section 6.2.",
      "referring_paragraphs": [
        "Utility Contour Plot   \nFigure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves.",
        "Let us introduce the auxiliary variable $\\beta : = \\langle \\pi _ { \\mathsf { A } } , \\tau _ { \\mathsf { A } } \\rangle = \\langle \\pi _ { \\mathsf { B } } , \\tau _ { \\mathsf { B } } \\rangle$ corresponding to the selection rate which is held constant across groups, so that all feasible solutions lie on the green DP line in Figure 3. We can then express the following equivalent linear program:",
        "The magenta EO curve in Figure 3 illustrates that feasible solutions to this optimization problem lie on a curve parametrized by $t$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1803.04383_page0_fig6.jpg",
        "1803.04383_page0_fig7.jpg",
        "1803.04383_page0_fig8.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig8.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_10",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg",
      "image_filename": "1803.04383_page0_fig9.jpg",
      "caption": "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.",
      "context_before": "Repay Probability by Group",
      "context_after": "[Section: 7 Simulations]\n\nWe examine the outcomes induced by fairness constraints in the context of FICO scores for two race groups. FICO scores are a proprietary classifier widely used in the United States to predict credit worthiness. Our FICO data is based on a sample of 301,536 TransUnion TransRisk scores from 2003 [US Federal Reserve, 2007], preprocessed by Hardt et al. [2016]. These scores, corresponding to $x$ in our model, range from 300 to 850 and are meant to predict credit risk. Empirical data labeled by race allows us to estimate the distributions $\\pi _ { \\mathrm { j } }$ , where j represents race, which is restricted to two values: white non-Hispanic (labeled “white” in figures), and black. Using national demographic data, we set the population proportions to be 18% and $8 2 \\%$ .\n\nIndividuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empirical data (see Figure 4). Our outcome curve framework allows for this relaxation; however, this discrepancy can also be attributed to group-dependent mismeasurement of score, and adjusting the scores accordingly would allow for a single $\\rho ( x )$ . We use the success probabilities to define the affine utility and score change functions defined in Example 2.1. We model individual penalties as a score drop of $c _ { - } = - 1 5 0$ in the case of a default, and in increase of $c _ { + } = 7 5$ in the case of successful repayment.\n\nIn Figure 5, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities. In the case that the bank experiences a loss/profit ratio of $\\frac { u _ { - } } { u _ { + } } = - 1 0$ , no fairness criteria surpass the active harm rate $\\beta _ { 0 }$ ; however, in the case of $\\frac { u _ { - } } { u _ { + } } = - 4$ , DemParity overloans, in line with the statement in Corollary 3.3. u+\n\nThese results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group. To plot the MaxUtil utility curves, the group that is not on display has selection rate fixed at $\\beta ^ { \\mathrm { M a x U t i 1 } }$ . In this figure, the top panel corresponds to the average change in credit scores for each group under different loaning rates $\\beta$ ; the bottom panels shows the corresponding total utility $\\boldsymbol { u }$ (summed over both groups and weighted by group population sizes) for the bank.",
      "referring_paragraphs": [
        "Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.",
        "Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, $\\rho _ { \\mathrm { j } } ( x )$ , which we allow to vary by group to match the empirical data (see Figure 4)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1803.04383_page0_fig10.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig10.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_12",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg",
      "image_filename": "1803.04383_page0_fig11.jpg",
      "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In Figure 5, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities.",
        "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1803.04383_page0_fig12.jpg",
        "1803.04383_page0_fig13.jpg",
        "1803.04383_page0_fig14.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig14.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1803.04383",
      "figure_id": "1803.04383_fig_16",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg",
      "image_filename": "1803.04383_page0_fig15.jpg",
      "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.",
      "context_before": "",
      "context_after": "We use the following technical lemma in the proof of the above lemma.\n\nLemma C.3. If $\\pi _ { \\mathsf { A } } , \\pi _ { \\mathsf { B } }$ that are identical up to a translation with $\\mu _ { \\mathsf { A } } < \\mu _ { \\mathsf { B } }$ , then\n\nProof. For (32), observe that $\\mathrm { T P R } _ { \\mathsf { A } } = \\rho ( \\mu _ { \\mathsf { A } } ) < \\mathrm { T P R } _ { \\mathsf { B } } = \\rho ( \\mu _ { \\mathsf { B } } )$ . For any $\\beta$ , we can write $\\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) =$ $\\mu _ { \\mathsf { B } } + c$ and $\\mathrm { Q _ { A } } ( \\beta ) = \\mu _ { \\mathsf { A } } + c$ for some $c$ , since $\\pi _ { \\mathsf { A } } , \\pi _ { \\mathsf { B } }$ that are identical up to translation by\n\n$\\mu _ { \\mathsf { A } } - \\mu _ { \\mathsf { B } }$ . Thus, by computation, we can see that for ${ \\mathrm { Q } } ( \\beta ) < \\mu$ , $\\partial _ { + } G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) > 1$ and for $\\mathrm { Q } ( \\beta ) < \\mu$ , $\\partial _ { + } G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) < 1$ . Since $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ is monotonically increasing on $[ 0 , 1 ]$ , we must have $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) } ( \\beta ) > \\beta$ for every $\\beta \\in \\lbrack 0 , 1 ]$ .\n\nFor (33), we have $\\beta > \\sum _ { x > \\mu } \\pi _ { \\mathsf { A } }$ , we can again write $\\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) = \\pmb { \\mu } _ { \\mathsf { B } } - c$ and $\\mathrm { Q } _ { \\mathsf { A } } ( \\beta ) = \\pmb { \\mu } _ { \\mathsf { A } } - c$ , for some $c > 0$ . Then it is clear than we have µB < $\\frac { \\mu _ { \\mathsf { B } } } { \\mu _ { \\mathsf { A } } } < \\frac { \\mathrm { Q } _ { \\mathsf { B } } ( \\beta ) } { \\mathrm { Q } _ { \\mathsf { A } } ( \\beta ) }$ . □\n\n[Section: C.6 Proof of Corollary 3.6]",
      "referring_paragraphs": [
        "u+\n\nThese results are further examined in Figure 6, which displays the normalized outcome curves and the utility curves for both the white and the black group.",
        "Figure 6 highlights that the position of the utility optima in the lower panel determines the loan (selection) rates.",
        "selection rate   \nFigure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1803.04383_page0_fig16.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig16.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1804.06876": [
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig0.jpg",
      "image_filename": "1804.06876_page0_fig0.jpg",
      "caption": "",
      "context_before": "We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are avialable at http://winobias.org.\n\n[Section: 1 Introduction]\n\nCoreference resolution is a task aimed at identifying phrases (mentions) referring to the same entity. Various approaches, including rule-based (Raghunathan et al., 2010), feature-based (Durrett and Klein, 2013; Peng et al., 2015a), and neuralnetwork based (Clark and Manning, 2016; Lee et al., 2017) have been proposed. While significant advances have been made, systems carry the risk of relying on societal stereotypes present in training data that could significantly impact their performance for some demographic groups.\n\nIn this work, we test the hypothesis that coreference systems exhibit gender bias by creating a new challenge corpus, WinoBias.This dataset follows the winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015b), and contains references to people using a vocabulary of 40 occupations. It contains two types of challenge sentences that require linking gendered pro-",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "1804.06876_page0_fig1.jpg"
      ],
      "quality_score": 0.35,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1804.06876",
      "figure_id": "1804.06876_fig_3",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg",
      "image_filename": "1804.06876_page0_fig2.jpg",
      "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.",
      "context_before": "",
      "context_after": "[Section: 2 WinoBias]\n\nTo better identify gender bias in coreference resolution systems, we build a new dataset centered on people entities referred by their occupations from a vocabulary of 40 occupations gathered from the US Department of Labor, shown in Table 1.3 We use the associated occupation statistics to determine what constitutes gender stereotypical roles (e.g. $90 \\%$ of nurses are women in this survey). Entities referred by different occupations are paired and used to construct test case scenarios. Sentences are duplicated using male and female pronouns, and contain equal numbers of correct coreference decisions for all occupations. In total, the dataset contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging\n\nTable 1: Occupations statistics used in WinoBias dataset, organized by the percent of people in the occupation who are reported as female. When woman dominate profession, we call linking the noun phrase referring to the job with female and male pronoun as ‘pro-stereotypical‘, and ‘anti-stereotypical‘, respectively. Similarly, if the occupation is male dominated, linking the noun phrase with the male and female pronoun is called, ‘pro-stereotypical‘ and ‘anti-steretypical‘, respectively.   \n\n<table><tr><td>Occupation</td><td>%</td><td>Occupation</td><td>%</td></tr><tr><td>carpenter</td><td>2</td><td>editor</td><td>52</td></tr><tr><td>mechanician</td><td>4</td><td>designers</td><td>54</td></tr><tr><td>construction worker</td><td>4</td><td>accountant</td><td>61</td></tr><tr><td>laborer</td><td>4</td><td>auditor</td><td>61</td></tr><tr><td>driver</td><td>6</td><td>writer</td><td>63</td></tr><tr><td>sheriff</td><td>14</td><td>baker</td><td>65</td></tr><tr><td>mover</td><td>18</td><td>clerk</td><td>72</td></tr><tr><td>developer</td><td>20</td><td>cashier</td><td>73</td></tr><tr><td>farmer</td><td>22</td><td>counselors</td><td>73</td></tr><tr><td>guard</td><td>22</td><td>attendant</td><td>76</td></tr><tr><td>chief</td><td>27</td><td>teacher</td><td>78</td></tr><tr><td>janitor</td><td>34</td><td>sewer</td><td>80</td></tr><tr><td>lawyer</td><td>35</td><td>librarian</td><td>84</td></tr><tr><td>cook</td><td>38</td><td>assistant</td><td>85</td></tr><tr><td>physician</td><td>38</td><td>cleaner</td><td>89</td></tr><tr><td>ceo</td><td>39</td><td>housekeeper</td><td>89</td></tr><tr><td>analyst</td><td>41</td><td>nurse</td><td>90</td></tr><tr><td>manager</td><td>43</td><td>receptionist</td><td>90</td></tr><tr><td>supervisor</td><td>44</td><td>hairdressers</td><td>92</td></tr><tr><td>salesperson</td><td>48</td><td>secretary</td><td>95</td></tr></table>\n\nand designed to cover cases requiring semantics and syntax separately.4\n\nType 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun] [circumstances]. Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1). Such examples are challenging because they contain no syntactic cues.\n\nType 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]. These tests can be resolved using syntactic information and understanding of the pronoun (Figure 1; Type 2). We expect systems to do well on such cases because both semantic and syntactic cues help disambiguation.\n\nEvaluation To evaluate models, we split the data in two sections: one where correct coreference decisions require linking a gendered pronoun to an occupation stereotypically associated with the gender of the pronoun and one that requires linking to the anti-stereotypical occupation. We say that a model passes the WinoBias\n\ntest if for both Type 1 and Type 2 examples, prostereotyped and anti-stereotyped co-reference decisions are made with the same accuracy.\n\n[Section: 3 Gender Bias in Co-reference]",
      "referring_paragraphs": [
        "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset.",
        "Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure 1; Type 1)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    }
  ],
  "1804.09301": [
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg",
      "image_filename": "1804.09301_page0_fig0.jpg",
      "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
      "context_before": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.\n\n[Section: 1 Introduction]\n\nThere is a classic riddle: A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, “I can’t operate on this boy, he’s my son!” How can this be?\n\nThat a majority of people are reportedly unable to solve this riddle1 is taken as evidence of underlying implicit gender bias (Wapman and Belle, 2014): many first-time listeners have difficulty assigning both the role of “mother” and “surgeon” to the same entity.\n\nAs the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems.\n\nThere are many ways one could approach this question; here we focus on gender bias with respect to occupations, for which we have corresponding U.S. employment statistics. Our approach is to construct a challenge dataset in",
      "context_after": "[Section: 2 Coreference Systems]\n\nIn this work, we evaluate three publiclyavailable off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven\n\nstatistical systems, and neural systems.\n\nRule-based In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system (Lee et al., 2011). A deterministic system, the sieve consists of multiple rule-based models which are applied in succession, from highest-precision to lowest. Gender is among the set of mention attributes identified in the very first stage of the sieve, making this information available throughout the system.\n\nStatistical Statistical methods, often with millions of parameters, ultimately surpassed the performance of rule-based systems on shared task data (Durrett and Klein, 2013; Bjorkelund and¨ Kuhn, 2014). The system of Durrett and Klein (2013) replaced hand-written rules with simple feature templates. Combinations of these features implicitly capture linguistic phenomena useful for resolving antecedents, but they may also unintentionally capture bias in the data. For instance, for occupations which are not frequently found in the data, an occupation+pronoun feature can be highly informative, and the overly confident model can exhibit strong bias when applied to a new domain.\n\nNeural The move to deep neural models led to more powerful antecedent scoring functions, and the subsequent learned feature combinations resulted in new state-of-the-art performance (Wiseman et al., 2015; Clark and Manning, 2016b). Global inference over these models further improved performance (Wiseman et al., 2016; Clark and Manning, 2016a), but from the perspective of potential bias, the information available to the model is largely the same as in the statistical models. A notable exception is in the case of systems which make use of pre-trained word embeddings (Clark and Manning, 2016b), which have been shown to contain bias and have the potential to introduce bias into the system.\n\nNoun Gender and Number Many coreference resolution systems, including those described here, make use of a common resource released by Bergsma and Lin $( 2 0 0 6 ) ^ { 3 }$ (“B&L”): a large list of English nouns and noun phrases with gender and\n\nnumber counts over 85GB of web news. For example, according to the resource, $9 . 2 \\%$ of mentions of the noun “doctor” are female. The resource was compiled by bootstrapping coreference information from the dependency paths between pairs of pronouns. We employ this data in our analysis.\n\n[Section: 3 Winogender Schemas]",
      "referring_paragraphs": [
        "As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1).",
        "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.",
        "Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1.",
        "Table 1: Correlation values for Figures 3 and 4.",
        "Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig1.jpg",
      "image_filename": "1804.09301_page0_fig1.jpg",
      "caption": "Figure 2: A “Winogender” schema for the occupation paramedic. Correct answers in bold. In general, OC-CUPATION and PARTICIPANT may appear in either order in the sentence.",
      "context_before": "In this work, we evaluate three publiclyavailable off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven\n\nstatistical systems, and neural systems.\n\nRule-based In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system (Lee et al., 2011). A deterministic system, the sieve consists of multiple rule-based models which are applied in succession, from highest-precision to lowest. Gender is among the set of mention attributes identified in the very first stage of the sieve, making this information available throughout the system.\n\nStatistical Statistical methods, often with millions of parameters, ultimately surpassed the performance of rule-based systems on shared task data (Durrett and Klein, 2013; Bjorkelund and¨ Kuhn, 2014). The system of Durrett and Klein (2013) replaced hand-written rules with simple feature templates. Combinations of these features implicitly capture linguistic phenomena useful for resolving antecedents, but they may also unintentionally capture bias in the data. For instance, for occupations which are not frequently found in the data, an occupation+pronoun feature can be highly informative, and the overly confident model can exhibit strong bias when applied to a new domain.\n\nNeural The move to deep neural models led to more powerful antecedent scoring functions, and the subsequent learned feature combinations resulted in new state-of-the-art performance (Wiseman et al., 2015; Clark and Manning, 2016b). Global inference over these models further improved performance (Wiseman et al., 2016; Clark and Manning, 2016a), but from the perspective of potential bias, the information available to the model is largely the same as in the statistical models. A notable exception is in the case of systems which make use of pre-trained word embeddings (Clark and Manning, 2016b), which have been shown to contain bias and have the potential to introduce bias into the system.\n\nNoun Gender and Number Many coreference resolution systems, including those described here, make use of a common resource released by Bergsma and Lin $( 2 0 0 6 ) ^ { 3 }$ (“B&L”): a large list of English nouns and noun phrases with gender and\n\nnumber counts over 85GB of web news. For example, according to the resource, $9 . 2 \\%$ of mentions of the noun “doctor” are female. The resource was compiled by bootstrapping coreference information from the dependency paths between pairs of pronouns. We employ this data in our analysis.\n\n[Section: 3 Winogender Schemas]\n\nOur intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1. To this end, we create a specialized evaluation set consisting of 120 hand-written sentence templates, in the style of the Winograd Schemas (Levesque et al., 2011). Each sentence contains three referring expressions of interest:\n\n1. OCCUPATION , a person referred to by their occupation and a definite article, e.g., “the paramedic.”   \n2. PARTICIPANT , a secondary (human) participant, e.g., “the passenger.”   \n3. PRONOUN , a pronoun that is coreferent with either OCCUPATION or PARTICIPANT.\n\nWe use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresponding gender percentages available from the U.S. Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2). For each sentence template, there are three PRO-NOUN instantiations (female, male, or neutral), and two PARTICIPANT instantiations (a specific participant, e.g., “the passenger,” and a generic paricipant, “someone.”) With the templates fully instantiated, the evaluation set contains 720 sentences: 60 occupations $\\times 2$ sentence templates per occupation $\\times 2$ participants $\\times 3$ pronoun genders.\n\nValidation Like Winograd schemas, each sentence template is written with one intended correct answer (here, either OCCUPATION or PAR-",
      "context_after": "",
      "referring_paragraphs": [
        "Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is coreferent with OCCUPATION, and one in which it is coreferent with PARTICIPANT (see Figure 2).",
        "Figure 2: A “Winogender” schema for the occupation paramedic.",
        "We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these “gotchas.”8 (See Table 2.)\n\nBecause coreference systems need to make discrete choices about which mentions are coreferent, percentage-wise differences in real-world statistics may translate into absolute differences in system predictions."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg",
      "image_filename": "1804.09301_page0_fig2.jpg",
      "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
      "context_before": "",
      "context_after": "[Section: 4 Results and Discussion]\n\nWe evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).\n\nBy multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems. First we observe that these systems do not behave in a gender-neutral fashion. That is to say, we have designed test sentences where correct pronoun resolution is not a function of gender (as validated by human annotators), but system predictions do exhibit sensitivity to pronoun gender: $68 \\%$ of male-female minimal pair test sentences are resolved differently by the RULE system; $28 \\%$ for STAT; and $13 \\%$ for NEURAL.\n\nOverall, male pronouns are also more likely to be resolved as OCCUPATION than female or neutral pronouns across all systems: for RULE, $72 \\%$ male vs $29 \\%$ female and $1 \\%$ neutral; for STAT, $71 \\%$ male vs $63 \\%$ female and $50 \\%$ neutral; and for NEURAL, $87 \\%$ male vs $80 \\%$ female and $36 \\%$ neutral. Neutral pronouns are often resolved as neither OCCUPATION nor PARTICIPANT, possibly due to the number ambiguity of “they/their/them.”",
      "referring_paragraphs": [
        "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .",
        "This illustrates two related phenomena: first, that datadriven NLP pipelines are susceptible to sequential amplification of bias throughout a pipeline, and second, that although the gender statistics from B&L correlate with BLS employment statistics, they are systematically male-skewed (Figure 3)."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1804.09301",
      "figure_id": "1804.09301_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg",
      "image_filename": "1804.09301_page0_fig3.jpg",
      "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.",
      "context_before": "[Section: 4 Results and Discussion]\n\nWe evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL).\n\nBy multiple measures, the Winogender schemas reveal varying degrees of gender bias in all three systems. First we observe that these systems do not behave in a gender-neutral fashion. That is to say, we have designed test sentences where correct pronoun resolution is not a function of gender (as validated by human annotators), but system predictions do exhibit sensitivity to pronoun gender: $68 \\%$ of male-female minimal pair test sentences are resolved differently by the RULE system; $28 \\%$ for STAT; and $13 \\%$ for NEURAL.\n\nOverall, male pronouns are also more likely to be resolved as OCCUPATION than female or neutral pronouns across all systems: for RULE, $72 \\%$ male vs $29 \\%$ female and $1 \\%$ neutral; for STAT, $71 \\%$ male vs $63 \\%$ female and $50 \\%$ neutral; and for NEURAL, $87 \\%$ male vs $80 \\%$ female and $36 \\%$ neutral. Neutral pronouns are often resolved as neither OCCUPATION nor PARTICIPANT, possibly due to the number ambiguity of “they/their/them.”",
      "context_after": "[Section: 5 Related Work]\n\nHere we give a brief (and non-exhaustive) overview of prior work on gender bias in NLP systems and datasets. A number of papers explore (gender) bias in English word embeddings:\n\nhow they capture implicit human biases in modern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukbasi et al., 2016). Further work on debiasing models with adversarial learning is explored by Beutel et al. (2017) and Zhang et al. (2018).\n\nPrior work also analyzes social and gender stereotyping in existing NLP and vision datasets (van Miltenburg, 2016; Rudinger et al., 2017). Tatman (2017) investigates the impact of gender and dialect on deployed speech recognition systems, while Zhao et al. (2017) introduce a method to reduce amplification effects on models trained with gender-biased datasets. Koolen and van Cranenburgh (2017) examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies. Both Larson (2017) and Koolen and van Cranenburgh (2017) offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable. Hovy and Spruit (2016) introduce a helpful set of terminology for identifying and categorizing types of bias that manifest in AI systems, including overgeneralization, which we observe in our work here.\n\nFinally, we note independent but closely related work by Zhao et al. (2018), published concurrently with this paper. In their work, Zhao et al. (2018) also propose a Winograd schema-like test for gender bias in coreference resolution systems (called “WinoBias”). Though similar in appearance, these two efforts have notable differences in substance and emphasis. The contribution of this work is focused primarily on schema construction and validation, with extensive analysis of observed system bias, revealing its correlation with biases present in real-world and textual statistics; by contrast, Zhao et al. (2018) present methods of debiasing existing systems, showing that simple approaches such as augmenting training data with gender-swapped examples or directly editing noun phrase counts in the B&L resource are effective at reducing system bias, as measured by the schemas. Complementary differences exist between the two schema formulations: Winogender schemas (this work) include gender-neutral pronouns, are syntactically diverse, and are human-validated; Wino-Bias includes (and delineates) sentences resolvable from syntax alone; a Winogender schema has one occupational mention and one “other participant” mention; WinoBias has two occupational\n\nmentions. Due to these differences, we encourage future evaluations to make use of both datasets.\n\n[Section: 6 Conclusion and Future Work]",
      "referring_paragraphs": [
        "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1804.09301_page0_fig4.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1805.03094": [
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig0.jpg",
      "image_filename": "1805.03094_page0_fig0.jpg",
      "caption": "",
      "context_before": "First, we study answerer performance on Stack Exchange (SE). Launched in 2008 as a forum for asking computer programming questions, Stack Exchange has grown to encompass a variety of technical and non-technical topics. Any user can ask a question, which others may answer. Users can vote for answers they find helpful, but only the asker can accept one of the answers as the best answer to the question. We used anonymized data representing all answers to questions posted on Stack Exchange from August 2008 until September 2014.2 Approximately half of the 9.6M questions had an accepted answer, and we included in the study questions that received two or more answers.\n\nTo understand factors affecting user performance on SE, we study the relationship between the various features extracted from data and the outcome, here a binary attribute\n\nTable 1: Variables defining important disaggregations of Stack Exchange data, along with their pseudo- $\\bar { . } R ^ { 2 }$ scores.   \n\n<table><tr><td>R2Mc</td><td>Covariate Xj</td><td>Conditioning on Xc</td></tr><tr><td>0.03</td><td>Answer position</td><td>Number of answers</td></tr><tr><td>0.03</td><td>Session length</td><td>Number of answers</td></tr><tr><td>0.02</td><td>Number of answers</td><td>Reputation</td></tr><tr><td>0.02</td><td>Answer position</td><td>Reputation</td></tr><tr><td>0.02</td><td>Session length</td><td>Reputation</td></tr><tr><td>0.01</td><td>Readability</td><td>Lines of codes</td></tr><tr><td>&lt; 10-2</td><td>Answer position</td><td>Session length</td></tr><tr><td>&lt; 10-2</td><td>Time since prev ans</td><td>Answer position</td></tr></table>\n\ndenoting whether the answer written by a user is accepted by the asker as best answer to his or her question. To this end, for each answer written by a user, we create a list of features describing the answer and the user. Features include the numbers of words, hyperlinks, and lines of code the answer contains, and its Flesch readability score (Kincaid et al. 1975). Features describing answerers are their reputation, tenure on SE (in seconds and in terms of percentile rank) and the total number of answers written during their tenure. These features relate to user experience. We also use activity-related features, including time since previous answer written by the user, session length, giving the number of answers user writes during the session, and answer position within that session. We define a session as a period of activity without a break of 100 minutes of longer.\n\nOf the 110 potential disaggregations of SE data arising from all possible pairs of covariates, our method identified 8 as significant. Table 1 ranks these disaggregations along with their pseudo- $R ^ { 2 }$ scores. Note that user experience, either in terms of the reputation or the number of answers written by the user over his or her tenure, comes up as an important conditioning variable in several disaggregations. Features related to user activity, such as answer position within a session, session length, and time since previous answer, appear as important dimensions of performance. This suggests that answerer behavior over the course of a session changes significantly, and these changes are different across different sub-populations.\n\nFigure 1 visualizes the data, disaggregated on the number of answers. Each horizontal band in the heatmap in Fig. 1(a) is a different bin of the conditioning variable number of answers, and it corresponds to a distinct subgroup within the data. The first bin ranges in value from one to eleven answers, the second bin from 12 to over 50 answers, etc. Within each bin, the color shows the relationship between the outcome—the probability the answer is accepted—and answer’s position within a session. Dark blue corresponds to the lowest acceptance probability, and dark red to the highest. Within each bin, the color changes from lighter blue to darker blue (for the bottom-most bins), indicating a lower acceptance probability for answers written later in the session. For the top-most bins, the acceptance probability is overall higher, but also decreases, e.g., from pink to white to blue. Note that data is noisy, as manifested by color flipping, where there are few data points (Fig. 1(b)).\n\nThe trends corresponding to these empirical observations are captured in Fig. 1(c). Note that the decreasing trends are in contrast to the trend in aggregate data (Fig. 1(d)), which shows performance increasing with answer position within the session. This suggests that user experience, as captured by the number of answers, is an important factor differentiating the behavior of users.\n\nFigure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation. This disaggregation is slightly worse, resulting in a somewhat lower value pseudo- $R ^ { \\bar { 2 } }$ . While performance declines in the lower reputation subgroups as a function of answer position, the highest reputation users appear to write better answers in longer sessions. The acceptance probability for high reputation users is more than 0.50, potentially indicating that askers pay attention to very high reputation users and are more likely to accept their answers.\n\n[Section: Khan Academy]\n\nKhan Academy3 (KA) is an educational platform offering online tools to help students learn a variety of subjects. Student progress by watching short videos and complete exercises by solving problems. We studied an anonymized dataset, collected over two years, which contains information about attempts by KA adult students to solve problems. We partitioned student activity into sessions, also defined as a sequence of problems without a break of more than one hour between them. The vast majority of students completed only a single session.\n\nAs an outcome variable in this data, we take student performance on a problem, a binary variable equal to one when the student solved the problem correctly on the first try, and zero otherwise (either did not solve it correctly, or used hints). To study factors affecting performance, we extracted the features of problems and users. These included the overall solving time during user activity, total solve time and the number of attempts made to solve the problem, time since the previous problem (tspp), the number of sessions prior to the current one, all sessions user contributed to, the session length in terms of the number of problems solved, problem position within the session (session index of the problem), the timestamp of the attempt, including the month, day of week, type of weekday ( whether it is weekend or not) and hour the student attempted to solve the problem, the month the student joined KA, his or her tenure, the number of all attempts on all problems solved since joining, and how many of the problems were solved correctly on the first attempts. As a proxy of skill or some background knowledge the student brings, we use how many problems were correctly solved during the student’s five first attempts to solve problems. For example, the least prepared students answered few of the five problems they attempted to solve, but best students would have solved all five correctly.\n\nOur method identified 32 significant disaggregations of KA data, out of 342 potential disaggregations. Some of these are presented in Table 2. The table lists conditioning variables for selected covariates, sorted by their pseudo- $R ^ { \\breve { 2 } }$",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {}
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg",
      "image_filename": "1805.03094_page0_fig1.jpg",
      "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1 visualizes the data, disaggregated on the number of answers.",
        "Figure 1: Disaggregation of Stack Exchange data."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Disaggregated data",
        "(c) Subgroup trends",
        "1805.03094_page0_fig2.jpg",
        "(b) Number of samples",
        "(d) Aggregate trend",
        "1805.03094_page0_fig3.jpg",
        "1805.03094_page0_fig4.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig4.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg",
      "image_filename": "1805.03094_page0_fig5.jpg",
      "caption": "Figure 2: Disaggregation of Stack Exchange data similar to Fig. 1, but instead disaggreagted on user reputation. (a) The heat map shows acceptance probability as a function of its answer position within a session. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in (c) disaggregated data and (d) aggregate data.",
      "context_before": "",
      "context_after": "[Section: Duolingo]\n\nDuolingo (DL) is an online language learning platform, which allows users to learn dozens of different lan-\n\nguages. DL offers a gamified learning environment, where users progress through levels by practicing vocabulary and dictation skills. The DL halflife-regression (Settles and Meeder 2016) dataset (https://github.com/duolingo/halfliferegression) follows a subset of learners over a period of two weeks. Users are shown vocabulary words and asked to recall them correctly. Users may be shown between 7 and 20 words per lesson, and may have multiple lessons in a session. Sessions are defined in a similar way as before—a period of activity without a break longer than one hour.\n\nUsers in general perform quite well, correctly recalling a large number of words in a lesson. This makes it difficult to discern changes in performance. Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their pseudo- $\\bar { \\boldsymbol { R } } ^ { 2 }$ scores.   \n\n<table><tr><td>R2Mc</td><td>Covariate Xj</td><td>Conditioning on Xc</td></tr><tr><td>0.06</td><td>All attempts</td><td>All first attempts</td></tr><tr><td>0.03</td><td>All attempts</td><td>All problems</td></tr><tr><td>0.01</td><td>All attempts</td><td>Tenure</td></tr><tr><td>0.01</td><td>All attempts</td><td>Total solve time</td></tr><tr><td>0.04</td><td>Hour24</td><td>All first attempts</td></tr><tr><td>0.04</td><td>Session number</td><td>All first attempts</td></tr><tr><td>0.02</td><td>Session number</td><td>All problems</td></tr><tr><td>0.01</td><td>Session number</td><td>Tenure</td></tr><tr><td>0.01</td><td>Session number</td><td>All attempts</td></tr><tr><td>0.01</td><td>Session number</td><td>All sessions</td></tr><tr><td>0.01</td><td>Session number</td><td>Total solve time</td></tr><tr><td>0.0</td><td>Session number</td><td>Join month</td></tr><tr><td>0.03</td><td>Month</td><td>Five first attempts</td></tr><tr><td>0.01</td><td>Month</td><td>Session index</td></tr><tr><td>0.01</td><td>Month</td><td>Total solve time</td></tr><tr><td>0.01</td><td>Month</td><td>Timestamp</td></tr><tr><td>&lt; 10-2</td><td>Month</td><td>Week day</td></tr><tr><td>0.01</td><td>Problem position</td><td>Session length</td></tr></table>",
      "referring_paragraphs": [
        "Figure 2 shows an alternate disaggregation of SE data for the covariate answer position, here conditioned on user reputation.",
        "Some of these are presented in Table 2.",
        "Figure 2: Disaggregation of Stack Exchange data similar to Fig.",
        "Therefore, we define performance in a more stringent way, as a binary variable, which is equal to one if the user had perfect performance (i.e., correctly recalled all words in a lesson), and zero oth-\n\nTable 2: Variables defining important disaggregations of the Khan Academy data, along with their pseudo- $\\bar { \\boldsymbol { R } } ^ { 2 }$ scores."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Disaggregated data",
        "(c) Subgroup trends",
        "1805.03094_page0_fig6.jpg",
        "(b) Number of samples",
        "(d) Aggregate trend",
        "1805.03094_page0_fig7.jpg",
        "1805.03094_page0_fig8.jpg",
        "1805.03094_page0_fig9.jpg",
        "(a) Disaggregated data",
        "1805.03094_page0_fig10.jpg",
        "(b) Number of samples",
        "(c) Subgroup trends",
        "(d) Aggregate trend",
        "1805.03094_page0_fig11.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig11.jpg"
        ],
        "panel_count": 7
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_13",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig12.jpg",
      "image_filename": "1805.03094_page0_fig12.jpg",
      "caption": "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
      "context_before": "erwise. We used more than two dozen features to describe performance. These include the number of words seen and correctly answered during a lesson (lesson seen and lesson correct), the number of distinct words shown during a lesson, lesson index among all lessons for this user, time to next lesson, time since the previous lesson, lesson position within its session, session length in terms of the number of lessons and duration, etc. User-related features include the number of five first lessons correctly answers, number of all perfect lessons with perfect performance, total number of lessons,",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3 takes a closer look at the disaggregation corresponding to covariate hour24.",
        "Figure 3: A disaggregation of the Khan Academy data showing performance as a function of hour of day, conditioned on all first attempts. (a) The heat map shows average performance within a subgroup as a function of the hour of day. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
        "Table 3 reports disaggregations associated with select covariates, including lesson’s position within a session, lesson index in user’s history, the number of lessons the user completed, etc."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_14",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg",
      "image_filename": "1805.03094_page0_fig13.jpg",
      "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.",
      "context_before": "",
      "context_after": "along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance. After 20 new words are shown in a lesson, users can no longer answer all the words correctly. Also in-",
      "referring_paragraphs": [
        "Figure 4 shows the disaggregation corresponding to the",
        "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Disaggregated data",
        "(c) Subgroup trends",
        "1805.03094_page0_fig14.jpg",
        "(b) Number of samples",
        "(d) Aggregate trend",
        "1805.03094_page0_fig15.jpg",
        "1805.03094_page0_fig16.jpg",
        "1805.03094_page0_fig17.jpg",
        "(a) Disaggregated data",
        "1805.03094_page0_fig18.jpg",
        "(b) Number of samples",
        "(c) Subgroup trends",
        "(d) Aggregate trend",
        "1805.03094_page0_fig19.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig19.jpg"
        ],
        "panel_count": 7
      }
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_21",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg",
      "image_filename": "1805.03094_page0_fig20.jpg",
      "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
      "context_before": "along the diagonal show perfect lessons, where users answered all words they were shown correctly. However, as the lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance. After 20 new words are shown in a lesson, users can no longer answer all the words correctly. Also in-",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 5 examines the impact of experience on performance.",
        "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1805.03094",
      "figure_id": "1805.03094_fig_22",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
      "image_filename": "1805.03094_page0_fig21.jpg",
      "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.",
      "context_before": "",
      "context_after": "[Section: Discussion]\n\nThere are several commonalities emerging from the three data sets we studied. Across platforms, initial performance, captured by first five attempts in the KA data or first five lessons in the DL data, appeared as an important conditioning variable differentiating the subgroups. Those users who were initially high performers appear to be different from the low performers, especially when looking at how their performance changes over time. While initial performance could capture skill or background knowledge, further analysis is needed to link it to this characteristic.\n\nExperience also appeared as an important feature differentiating users. As a proxy of experience we used such features as the number of lessons in DL data, user tenure in KA data, and number of answers and reputation in SE data. However, whether this variable reflects the benefits of practice, or simply captures user motivation, is not clear.\n\nFeatures linked to user activity differentiated many important subgroups across all three data sets. Features such as time since previous lesson, session length and position within a session, total time solving a problem, were all significant conditioning variables. This suggests that performance on these platforms has a non-trivial dynamic component that merits deeper investigation. Indeed, a study of Stack Exchange observed short term deterioration in perfor-\n\nmance (Ferrara et al. 2017). Our study suggests that such an effect may be general. In addition, temporal features, such as month, type of weekday, timestamp, were found to be important covariates. This indicates that temporal effects can explain differences in performance: e.g., people who use learning web sites on weekends are different from those who use them during the week. Our analysis helps identify such subgroups and understand their behavior.\n\n[Section: Conclusion]",
      "referring_paragraphs": [
        "Another disaggregation of DL data is shown in Figure 6.",
        "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Disaggregated data",
        "1805.03094_page0_fig22.jpg",
        "(b) Number of samples",
        "(c) Subgroup trends",
        "(d) Aggregate trend",
        "1805.03094_page0_fig23.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig23.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1805.03677": [
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg",
      "image_filename": "1805.03677_page0_fig0.jpg",
      "caption": "Figure 1.​ Model Development Pipeline",
      "context_before": "Artificial intelligence (AI) systems built on incomplete or biased data will often exhibit problematic outcomes. Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label (the Label) is a diagnostic framework that1 lowers the barrier to standardized data analysis by providing a distilled yet comprehensive overview of dataset “ingredients” before AI model development. Building a Label that can be applied across domains and data types requires that the framework itself be flexible and adaptable; as such, the Label is comprised of diverse qualitative and quantitative modules generated through multiple statistical and probabilistic modelling backends, but displayed in a standardized format. To demonstrate and advance this concept, we generated and published an open source prototype with seven sample modules on the 2 ProPublica Dollars for Docs dataset. The benefits of the Label are manyfold. For data specialists, the Label will drive more robust data analysis practices, provide an efficient way to select the best dataset for their purposes, and increase the overall quality of AI models as a result of more robust training datasets and the ability to check for issues at the time of model development. For those building and publishing datasets, the Label creates an expectation of explanation, which will drive better data collection practices. We also explore the limitations of the Label, including the challenges of generalizing across diverse datasets, and the risk of using “ground truth” data as a comparison dataset. We discuss ways to move forward given the limitations identified. Lastly, we lay out future directions for the Dataset Nutrition Label project, including research and public policy agendas to further advance consideration of the concept.\n\n[Section: KEYWORDS]\n\nDataset, data, quality, bias, analysis, artificial intelligence, machine learning, model development, nutrition label, computer science, data science, governance, AI accountability, algorithms\n\nData driven decision making systems play an increasingly important and impactful role in our lives. These frameworks are built on increasingly sophisticated artificial intelligence (AI) systems and are tuned by a growing population of data specialists to infer a vast diversity of outcomes: the song that plays next3 on your playlist, the type of advertisement you are most likely to see, or whether you qualify for a mortgage and at what rate [1]. These systems deliver untold societal and economic benefits, but they can also pose harm. Researchers continue to uncover troubling consequences of these systems [2,3].\n\nData is a fundamental ingredient in AI, and the quality of a dataset used to build a model will directly influence the outcomes it produces. Like the fruit of a poisoned tree, an AI model trained on problematic or missing data will likely produce problematic outcomes [4, 5]. Examples of these problems include gender bias in language translations surfaced through natural language processing [4], and skin shade bias in facial recognition systems due to non-representative data [5]. Typically the model development pipeline (Figure 1​) begins with a question or goal. Within the realm of supervised learning, for example, a data specialist will curate a labeled dataset of previous answers in response to the guiding question. Such data is then used to train a model to respond in a way that accurately correlates with past occurrences. In this way, past answers are used to forecast the future. This is particularly problematic when outcomes of past events are contaminated with (often unintentional) bias.",
      "context_after": "",
      "referring_paragraphs": [
        "Typically the model development pipeline (Figure 1​) begins with a question or goal.",
        "Figure 1.​ Model Development Pipeline\n\nModels often come under scrutiny only after they are built, trained, and deployed.",
        "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​).",
        "Figure 1​) provide as-is dataset information."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg",
      "image_filename": "1805.03677_page0_fig1.jpg",
      "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data",
      "context_before": "",
      "context_after": "[Section: 1.1 LABELS IN CONTEXT]\n\nTo inform the development of our prototype and concept, we surveyed the literature for labeling efforts. Labels and warnings are utilized effectively in product safety [6], pharmaceuticals [7], energy [8], and material safety [9]. We largely draw from the fields of nutrition, online privacy, and algorithmic accountability as they are particularly salient for our purposes. The former is the canonical example and a long standing practice subject to significant study; the latter provides valuable insights in the application of a “nutrition label” in other domains, particularly in subjective contexts and where there is an absence of\n\nlegal mandates. Collectively, they elucidate the impacts of labels on audience engagement, education, and user decision making.\n\nIn 1990, Congress passed the Nutrition Labeling and Education Act (P.L. 101 - 535), which includes a requirement that certain foodstuffs display a standardized “Nutrition Facts” label [10]. By mandating the label, vital nutritional facts were communicated in the context of the “Daily Value” benchmark, and consumers could quickly assess nutrition information and more effectively abide by dietary recommendations at the moment of decision [10–12]. In the nearly three decades since its implementation, several studies have examined the efficacy of the now ubiquitous “Nutrition Facts” label; these studies include analyses of how consumers use the label [11,13], and the effect it has had on the market [14].\n\nThough some cast doubt on the benefits of the mandate in light of its costs [15], most research concludes that the “Nutrition Facts” label has positive impact [16,17]. Surveys demonstrate widespread consumer awareness of the label, and its influence in decision making around food, despite a relatively short time since the passage of the Nutrition Labeling and Education Act [18]. According to the International Food Information Council, more than $80 \\%$ of consumers reported they looked at the “Nutrition Facts” label when deciding what foods to purchase or consume, and only four percent reported never using the label [19]. Five years after the mandate, the Food Marketing Institute found that about one-third of consumers stopped buying a food because of what they read on the label [18]. With regard to the information contained on the label and consumer understanding, researchers found that “label format and inclusion of (external) reference value information appear to have (positive) effects on consumer perceptions and evaluations,” [20] but consumers indicated confusion about the “Daily Value” comparison, suggesting that more information about the source and reliability of ground truth information would be useful [19]. The literature focuses primarily on the impact to consumers rather than on industry operations. However, the significant impact of reported sales and marketing materials on consumers [14] provides a foundation for further inquiry into how this has affected the greater food industry.\n\nIn the field of privacy and privacy disclosures, the nutrition label serves as a useful point of reference and inspiration [21]. Researchers at Carnegie Mellon and Microsoft created the “Privacy Nutrition Label” to better surface essential privacy information to assist consumer decision making with regard to the collection, use, and sharing of personal information [22]. The “Privacy Nutrition Label” operates much like “Nutrition Facts” and sits atop existing disclosures. It improves the functionality of the Platform for Privacy Notices, a machine readable format developed by the World Wide Web Consortium, itself an effort to standardize and improve legibility of privacy policies [23]. User surveys that tested the “Privacy Nutrition Label” against alternative formats found that the label outperformed alternatives with “significant positive effects on the accuracy and speed of information finding and reader enjoyment with privacy policies,” as well as improved consumer understanding [22,23].\n\nRanking and scoring algorithms also pose challenges in terms of their complexity, opacity, and sensitivity to the influence of data. End users and even model developers face difficulty in interpreting an algorithm and its ranking outputs, and this difficulty is further compounded when the model and the data on which it is trained is proprietary or otherwise confidential, as is often the case. “Ranking Facts” is a web-based system that generates a “nutrition label” for scoring and ranking algorithms based on factors or “widgets” to communicate an algorithm’s methodology or output [24]. Here, the label serves more as an interpretability tool than as a summary of information as the “Nutrition Facts” and “Privacy Nutrition Label,” operate. The widgets work together, not modularly, to assess the algorithm on transparency, fairness, stability, and diversity. The demonstration scenarios for using real datasets from college\n\nrankings, criminal risk assessment, and financial services establish that the label can be applied to a diverse range of domains. This lends credence to the potential utility in other fields as well, including the rapidly evolving field of AI.\n\n[Section: 1.2 RELATED WORK]",
      "referring_paragraphs": [
        "We conducted an anonymous online survey (Figure 2), ​the results of which further lend credence to this problem.",
        "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data\n\nTo improve the accuracy and fairness of AI systems, it is imperative that data specialists are able to more quickly assess the viability and fitness of datasets, and more easily find and use better quality data to train their models.",
        "Table 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   \n\n<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>",
        "While the generation of some modules is fully automated, some require human input (Table 2​).",
        "Figure 2​) starts to offer a glimpse into the dataset distributions.",
        "Ordinal   \nNominal   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>id</td><td>number</td><td>500</td><td>488 including mi...</td><td>missing value (13)</td><td>multiple detected</td><td>2.60%</td></tr><tr><td>applicable_man...</td><td>number</td><td>500</td><td>4</td><td>100000000232 (...)</td><td>multiple detected</td><td>0%</td></tr><tr><td>date_of-payment</td><td>date</td><td>500</td><td>213 including mi...</td><td>missing value (27)</td><td>multiple detected</td><td>5.40%</td></tr><tr><td>general(transac...)</td><td>number</td><td>500</td><td>467 including mi...</td><td>missing value (34)</td><td>multiple detected</td><td>6.80%</td></tr><tr><td>program_year</td><td>number</td><td>500</td><td>2 including missi...</td><td>2014 (495)</td><td>missing value (5)</td><td>1.00%</td></tr></table>\n\n<table><tr><td>name</td><td>type</td><td>count</td><td>uniqueEntries</td><td>mostFrequent</td><td>leastFrequent</td><td>missing</td></tr><tr><td>product_name</td><td>string</td><td>500</td><td>16 including mis...</td><td>Xarelto (200)</td><td>Aciphex (1)</td><td>3.20%</td></tr><tr><td>original_product...</td><td>string</td><td>500</td><td>15</td><td>Xarelto (212)</td><td>Aciphex (1)</td><td>0%</td></tr><tr><td>product_ndc</td><td>number</td><td>500</td><td>21 including mis...</td><td>5045857810 (201)</td><td>multiple detected</td><td>5.00%</td></tr><tr><td>product_is_drug</td><td>boolean</td><td>500</td><td>2 including miss...</td><td>t (492)</td><td>missing value (8)</td><td>1.60%</td></tr><tr><td>payment_has_m...</td><td>boolean</td><td>500</td><td>3 including miss...</td><td>f (267)</td><td>missing value (29)</td><td>5.80%</td></tr><tr><td>teaching_hospit...</td><td>number</td><td>500</td><td>2 including miss...</td><td>0 (464)</td><td>missing value (36)</td><td>7.20%</td></tr><tr><td>physician_profile...</td><td>number</td><td>500</td><td>230 including mi...</td><td>missing value (32)</td><td>multiple detected</td><td>6.40%</td></tr><tr><td>recipient_state</td><td>string</td><td>500</td><td>40</td><td>CA (56)</td><td>multiple detected</td><td>0%</td></tr><tr><td>applicable_man...</td><td>string</td><td>500</td><td>5 including miss...</td><td>Janssen Pharm...</td><td>multiple detected</td><td>7.00%</td></tr><tr><td>teaching_hospit...</td><td>number</td><td>500</td><td>2 including miss...</td><td>0 (481)</td><td>missing value (19)</td><td>3.80%</td></tr><tr><td>product_slug</td><td>string</td><td>500</td><td>15 including mis...</td><td>drug-xarelto (196)</td><td>drug-aciphex (1)</td><td>8.20%</td></tr></table>\n\nContinuous   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>min</td><td>median</td><td>max</td><td>mean</td><td>standardD...</td><td>missing</td><td>zeros</td></tr><tr><td>total_amo...</td><td>number</td><td>500</td><td>0.14</td><td>14.00</td><td>5000</td><td>134.21</td><td>501.99</td><td>9.40%</td><td>0%</td></tr></table>\n\nDiscrete   \n\n<table><tr><td>name</td><td>type</td><td>count</td><td>min</td><td>median</td><td>max</td><td>mean</td><td>standardD...</td><td>missing</td><td>zeros</td></tr><tr><td>number_o...</td><td>number</td><td>500</td><td>1</td><td>1.00</td><td>1</td><td>1.00</td><td>0.00</td><td>4.80%</td><td>0%</td></tr></table>\n\nSupplement figure 2.​ Prototype Label demonstrating the Statistics module, splitting the variables into 4 groups: ordinal, nominal, continuous, and discrete."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1805.03677_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg",
      "image_filename": "1805.03677_page0_fig3.jpg",
      "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.",
      "context_before": "The Label is designed in an extensible fashion with multiple distinct components that we refer to as “modules” (Table 1​). The modules are stand-alone, allowing for greater flexibility as arrangements of different modules can be used for different types of datasets. This format also caters to a wide range of requirements and information available for a specific dataset. During label generation and subsequent updates, it also accommodates data specialists of different backgrounds and technical skill levels.\n\nModules (Table 1 & 2) range from the purely non-technical, such as the Metadata module, to the highly technical, such as the Probabilistic Computing module. Some modules require manual effort to generate, such as those that provide qualitative descriptions of the data (Metadata, Provenance, Variables), while others can ideally be the result of an automated process (Statistics, Pair Plots). Modules also vary in their subjectivity, especially where there exists a reliance on the Label author to identify which questions should be asked of the data and in what way (e.g. Probabilistic Computing). Many of the example modules are also interactive, highlighting a crucial benefit of a label living on a platform (such as a web page) that supports user interaction. This allows Label users to interrogate various dataset aspects with great flexibility and free of preconceived notions developed during Label generation. Lastly, some modules could be designed to act as proxies for their corresponding dataset as they do not expose the underlying data. This could be key when dealing with proprietary datasets, as much of this data will not or cannot be released to the public based on intellectual property or other constraints. Other modules expose information such as distribution metrics which, in theory, would allow adversaries to approximate the dataset contents. The choice of module(s) is thus based on the availability of information, level of willingness and effort volunteered to document the dataset, and privacy concerns.\n\nTable 1.​ Table illustrating 7 modules of the Dataset Nutrition Label, together with their description, role, and contents.   \n\n<table><tr><td>Module Name</td><td>Description</td><td>Contents</td></tr><tr><td>Metadata</td><td>Meta information. This module is the only required module. It represents the absolute minimum information to be presented</td><td>Filename, file format, URL, domain, keywords, type, dataset size, % of missing cells, license, release date, collection range, description</td></tr><tr><td>Provenance</td><td>Information regarding the origin and lineage of the dataset</td><td>Source and author contact information with version history</td></tr><tr><td>Variables</td><td>Descriptions of each variable (column) in the dataset</td><td>Textual descriptions</td></tr><tr><td>Statistics</td><td>Simple statistics for all variables, in addition to stratifications into ordinal, nominal, continuous, and discrete</td><td>Least/most frequent entries, min/max, median, mean,.etc</td></tr><tr><td>Pair Plots</td><td>Distributions and linear correlations between 2 chosen variables</td><td>Histograms and heatmaps</td></tr><tr><td>Probabilistic Model</td><td>Synthetic data generated using distribution hypotheses from which the data was drawn - leverages a probabilistic programming backend</td><td>Histograms and other statistical plots</td></tr><tr><td>Ground Truth Correlations</td><td>Linear correlations between a chosen variable in the dataset and variables from other datasets considered to be &quot;ground truth&quot;, such as Census Data</td><td>Heatmaps</td></tr></table>\n\nThe list of modules currently examined in this study, while not exhaustive, provides a solid representation of the kinds of flexibility supported by the Label framework. Other modules considered for future iterations or additional datasets include but are not limited to: a comments section for users to interact with authors of the Label for feedback or other purposes; an extension of the Provenance section that includes the versioning history and change logs of the dataset and associated Labels over time, similar to Git; a privacy-focused module that indicates any sensitive information and whether the data was collected with consent; and finally, a usage tracking module that documents data utilization and references using some form of identifier, similar to the Digital Object Identifier [30] and associated citation systems in scientific publishing.\n\nTable 2.​ Variability of attributes across prototype modules highlights the potential diversity of information included in a Label   \n\n<table><tr><td></td><td colspan=\"5\">Module Characteristic - Level Required</td></tr><tr><td>Module Name</td><td>Technical Expertise</td><td>Manual Effort</td><td>Subjectivity</td><td>Interactivity</td><td>Data Exposure</td></tr><tr><td>Metadata</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Provenance</td><td>Low</td><td>High</td><td>Low</td><td>Low</td><td>Low</td></tr><tr><td>Variables</td><td>Low</td><td>High</td><td>Medium</td><td>Low</td><td>Medium</td></tr><tr><td>Statistics</td><td>Medium</td><td>Low</td><td>Low</td><td>Low</td><td>Medium</td></tr><tr><td>Pair Plots</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td><td>High</td></tr><tr><td>Probabilistic Modeling</td><td>High</td><td>Medium</td><td>High</td><td>Low</td><td>High</td></tr><tr><td>Ground Truth Correlations</td><td>Medium</td><td>Medium</td><td>Low</td><td>Low</td><td>High</td></tr></table>\n\n[Section: 2.2 WEB-BASED APPLICATION]\n\nThe label is envisioned as a digital object that can be both generated and viewed by web-based applications. The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​). Given a specific dataset, the label maker application allows users to select the desired modules and generate them. While the generation of some modules is fully automated, some require human input (Table 2​). For instance, the Metadata module mainly requires explicit input, while the Pair Plots module can be generated automatically from the dataset. The Label generator pre-populates as many fields as possible and alerts users to those requiring action. The Label itself lives in a .json format, as one that is human readable and well supported. The Label can then be viewed within the label viewer application where formating is carried out to achieve the desired user interface and user interaction effects. In terms of visual appearance and design, format and typeface requirements of the “Nutrition Facts” label [31] is used. These guidelines, such as the all black font color on white contrasting background, are optimized for clarity and conciseness. Design changes are anticipated in further iterations, and should be informed by user testing.",
      "context_after": "[Section: 2.3 BACKENDS]\n\nSimple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends with the ultimate aim of providing the Label authors with a diverse set of options, fueled by the plethora of tools developed by research groups for automating the generation of summaries, insights, and understandings of datasets. The Label thus becomes a medium for the continuous deployment and testing of these tools. A somewhat recent and particularly powerful example of this is probabilistic computing, and specifically, BayesDB [32], an open source platform developed by researchers at MIT. With minimal modeling and programming effort, BayesDB enables inference of a model that captures the structure underlying the data and generates statistical summaries based on such structure.\n\n[Section: 3 RESULTS]",
      "referring_paragraphs": [
        "The label ecosystem comprises two main components: a label maker and a label viewer (Figure 3​).",
        "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_5",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg",
      "image_filename": "1805.03677_page0_fig4.jpg",
      "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.",
      "context_before": "Simple statistical analyses involving the generation of histograms, distribution information, and linear correlations are carried out directly in the browser, given tabular datasets of ${ < } 1 0 0 \\mathrm { K }$ rows. Server-side processing is thus reserved for more specialized and sophisticated analyses requiring additional computational power. Such processing could run multiple backends with the ultimate aim of providing the Label authors with a diverse set of options, fueled by the plethora of tools developed by research groups for automating the generation of summaries, insights, and understandings of datasets. The Label thus becomes a medium for the continuous deployment and testing of these tools. A somewhat recent and particularly powerful example of this is probabilistic computing, and specifically, BayesDB [32], an open source platform developed by researchers at MIT. With minimal modeling and programming effort, BayesDB enables inference of a model that captures the structure underlying the data and generates statistical summaries based on such structure.\n\n[Section: 3 RESULTS]\n\nTo test the concept generally and the modular framework specifically, we built a prototype with a dataset that included information about people and was maintained by an organization invested in better understanding the data. This combination of factors provides necessary information and access to build a wide variety of modules, including those that require full knowledge of the data and the ability to contact the organization that maintains the dataset. We were granted access to the “Dollars for Docs” database from ProPublica, an independent, nonprofit newsroom that produces investigative journalism in the public interest . The dataset, which contains payments to doctors and teaching hospitals from pharmaceutical and4 medical device companies over a two-year time period (August 2013 - December 2015), was originally released by the U.S. Centers for Medicare and Medicaid Services (CMS) and compiled by ProPublica into a single, comprehensive database.\n\nThe resulting prototype successfully demonstrates how disparate modules can be built on a specific dataset in order to highlight multiple, complementary facets of the data, ideally to be leveraged for further investigation by data specialists through the use of additional tools and strategies. The prototype Label includes seven modules (Table 1, 2​). The Metadata, Provenance, and Variables modules (Supp. Figure 1​) provide as-is dataset information. They mirror information submitted by the Label authors as well as provide a standard format for both the generation and consumption of such data. The Statistics module (Supp. Figure 2​) starts to offer a glimpse into the dataset distributions. For instance, the skewness of a 500 row dataset subset towards a particular drug \"Xarelto\" can be quickly identified as the most frequent entry under the variable \"product_name\", and “Aciphex” as the least frequent entry. The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another. A specialist building a model predicting marketing spend in each state, for example, may choose to compare “recipient_state” and “total_amount_of_payment_usdollars,” and will observe that some states (CA, NY) are more highly correlated with spend. In this case, the specialist would probably normalize for population as the next step beyond consulting the Label in order to identify anomalous spending trends.",
      "context_after": "",
      "referring_paragraphs": [
        "The Pair Plot module (Figure 4​) starts to introduce interactivity into the label where the viewer is able to choose the variable pair being compared to one another.",
        "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_6",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg",
      "image_filename": "1805.03677_page0_fig5.jpg",
      "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "While all modules thus far investigate the dataset itself, the Probabilistic Model module (Figure 5​) attempts to generate synthetic data by utilizing the aforementioned BayesDB backend.",
        "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {}
    },
    {
      "doc_id": "1805.03677",
      "figure_id": "1805.03677_fig_7",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg",
      "image_filename": "1805.03677_page0_fig6.jpg",
      "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.",
      "context_before": "",
      "context_after": "[Section: 4 DISCUSSION]\n\nThe Label offers many benefits. Overall, it prompts critical questions and interrogation in the preprocessing phase of model development. It also expedites decision making, which saves time in the overall model development phase without sacrificing the quality or thoroughness of the data interrogation itself, perhaps encouraging better practices at scale. These benefits apply across the spectrum of data specialists’ skill and experience, but are particularly useful for those new to the field or less attuned to concerns around bias and algorithmic accountability. First, the Label creates a pre-generated “floor” for basic data interrogation in the data selection phase. It also indicates key dataset attributes in a standardized format. This gives data specialists a distilled yet comprehensive overview of the “ingredients” of the dataset, which allows for a quick and effective comparison of multiple datasets before committing to one for further investigation. It also enables the data specialist to better understand and ascertain the fitness of a dataset by scanning missing values, summary statistics of the data, correlations\n\nor proxies, and other important factors. As a result, the data specialist may discard a problematic dataset or work to improve its viability prior to utilizing it.\n\nImproved dataset selection affords a secondary benefit: higher quality models. The Label provides data specialists improved means by which to interrogate the selected dataset during model development, previously a costly and onerous enterprise. The Ground Truth Correlation module, in particular, provides a helpful point of reference for the data specialist before model completion, and surfaces issues such as surprising variable correlations, missing data, anomalous data distributions, or other factors that could reinforce or perpetuate bias in the dataset. Addressing these factors in the model creation and training phase saves costs, time, and effort, and also could prevent bad outcomes early on, rather than addressing them after the fact.\n\nThe Label is built with scalability in mind, and with an eye towards standardization. The modular framework provides flexibility for dataset authors and publishers to identify the \"right\" kind and amount of information to include in a Label; over time, this could become a set of domain-specific best practices. The interactivity of the Label also permits flexibility, as insights about the dataset may arise over time. For example, the ground truth data used for comparison could evolve, rendering a previously unsuitable dataset suitable. Interactive Labels also give data specialists the ability to dive further into anomalous data, rather than simply accepting static information provided by the Label author. With some modules more subjective in nature, and with a range of domain expertise across data specialists, this is particularly important. For advanced data specialists, the flexible Label backend makes it easy to \"plug-in\" more complex engines. Such complex backends can provide different statistical tools; for example, the Probabilistic Computing module makes it possible to investigate low frequency variables by generating synthetic data. Synthetic data gives data specialists the ability to address incomplete data, and opens a potential path to privacy preserving data usage [34].\n\nLastly, the Label functioning as a proxy for the dataset itself is an intriguing, even if distant, possibility. Increased calls for accountability of AI systems demand investigation of datasets used in training models, but disclosing those datasets, even to a limited audience, may pose risks to privacy, security, and intellectual property that calls this approach into question. If the Label is able to convey the information essential for accountability in the dataset without disclosing the data itself, it would provide a valuable and much needed auditing tool [35] for AI systems while still preserving privacy, security, and proprietary information.\n\n[Section: 4.1 LIMITATIONS AND MITIGATIONS]",
      "referring_paragraphs": [
        "The Ground Truth Correlation module (Figure 6​) provides the data specialist initial evidence as to whether such relationships are likely, thus warranting further analysis.",
        "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    }
  ],
  "1805.05859": [
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg",
      "image_filename": "1805.05859_page0_fig0.jpg",
      "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .",
      "context_before": "[Section: 3.2 Interventions, Counterfactuals and Predictions]\n\nThe effect of a cause follows from an operational notion of intervention. This notion says that a perfect intervention on a variable $V _ { i }$ , at value $v$ , corresponds to overriding $f _ { i } ( \\cdot )$ with the equation $V _ { i } ~ = ~ v$ . Once this is done, the joint distribution of the remaining variables $V _ { \\backslash i } \\equiv V \\backslash \\{ V _ { i } \\}$ is given by the causal model. Following [29], we will denote this operation as $P ( V _ { \\setminus i } \\mid d o ( V _ { i } = v _ { i } ) )$ . This notion is immediately extendable to a set of simultaneous interventions on multiple variables.\n\nThe introduction of the $d o ( \\cdot )$ operator emphasises the difference between $P ( V _ { \\setminus i } \\mid d o ( V _ { i } = v _ { i } ) )$ and $P ( V _ { \\backslash i } \\mid V _ { i } = v _ { i } )$ . As an example, consider the following structural equations:\n\nThe corresponding graph is shown in Figure 1(a). Assuming that the background variables follow a standard Gaussian with diagonal covariance matrix, standard algebraic manipulations allows us to calculate that $P ( Y = y ~ \\vert ~ A = a )$ has a Gaussian density with a mean that depends on $\\lambda _ { a z } , \\lambda _ { y a }$ and $\\lambda _ { y z }$ . In contrast, $\\mathsf E [ Y \\mid d o ( A = a ) ] = \\lambda _ { y a } a$ , which can be obtained by first erasing (6) and replacing $A$ with $a$ on the right-hand side of (7) followed by marginalizing the",
      "context_after": "[Section: 3.3 Counterfactuals Require Untestable Assumptions]\n\nUnless structural equations depend on observed variables only, they cannot be tested for correctness (unless other untestable assumptions are imposed). We can illustrate this problem by noting that a conditional density function $P ( V _ { j } \\mid V _ { i } = v )$ can be written as an equation $V _ { j } = f _ { 1 } ( v , U ) \\equiv F _ { V _ { i } = v } ^ { - 1 } ( U ) =$ $F _ { V _ { i } = v } ^ { - 1 } ( g ^ { - 1 } ( g ( U ) ) ) \\equiv f _ { 2 } ( v , U ^ { \\prime } )$ , where $F _ { V _ { i } = V } ^ { - 1 } ( \\cdot )$ is the inverse cumulative distribution function corresponding to $P ( V _ { j } \\mid V _ { i } = v )$ , $U$ is an uniformly distributed random variable on $[ 0 , 1 ]$ , $g ( \\cdot )$ is some arbitrary invertible function on $[ 0 , 1 ]$ , and $U ^ { \\prime } \\equiv g ( U )$ . While this is not fundamental for effects of causes, which depend solely on predictive distributions that at least in theory can be estimated from RCTs, different structural equations with the same interventional distributions will imply different joint distributions over the counterfactuals.\n\nThe traditional approach for causal inference in statistics tries to avoid any estimand that cannot be expressed by the marginal distributions of the counterfactuals (i.e., all estimands in which marginals $P ( Y ( a ) = y _ { a } )$ and $P ( Y ( a ^ { \\prime } ) = y _ { a ^ { \\prime } } )$ would provide enough information, such as the average causal effect $\\mathsf E [ Y ( a ) -$ $Y ( a ^ { \\prime } ) ] = \\mathsf E [ Y \\mid d o ( A = a ) ] - \\mathsf E [ Y \\mid d o ( A = a ^ { \\prime } ) ] )$ . Models that follow this approach and specify solely the univariate marginals of a counterfactual joint distribution are sometimes called single-world models [32]. However, as we will see, crossworld models seem a natural fit to algorithmic fairness. In particular, they are required for non-trivial statements that concern fairness at an individual level as opposed to fairness measures averaged over groups of individuals.\n\n[Section: 4 Why Causality is Critical For Fairness]",
      "referring_paragraphs": [
        "The corresponding graph is shown in Figure 1(a).",
        "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1805.05859_page0_fig1.jpg",
        "1805.05859_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1805.05859",
      "figure_id": "1805.05859_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg",
      "image_filename": "1805.05859_page0_fig3.jpg",
      "caption": "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.",
      "context_before": "When the goal is to minimise the number of untestable assumptions, one direction is to avoid making any explicit assumptions about the structural equations of the causal model. In this line of work, assumptions about the directed edges in the causal graph and a parametric formulation for the observed distributions are allowed, as well as independence constraints among counterfactuals. However, no explicit structural equations are allowed.\n\nThe work by [28] presents such ideas by making a direct connection to the long tradition in graphical causal models of providing algorithms for identifying causal estimands. That is, without assuming any particular parametric contribution from latent variables, such approaches either provide a function of $P ( V )$ (recall that $V$ is the set of all observed variables) that is equal to a causal effect of interest, or report whether such a transformation is not at all possible (that is, the causal effect of interest is unidentifiable from $P ( V )$ and a causal graph)\n\n[29]. The main idea of [28] is to first identify which causal effects of protected attribute $A$ on outcome $Y$ should be (close to) zero (i.e., those we wish to make fair). For instance, similar to counterfactual fairness, we may require that $\\mathsf E [ Y ( a ) ] = \\mathsf E [ Y ( a ^ { \\prime } ) ]$ . In some models, we can express $\\mathsf E [ Y ( a ) ] - \\mathsf E [ Y ( a ^ { \\prime } ) ] = 0$ as a constraint on $P ( V )$ . The model can then be fit subject to such a constraint. Predictor $\\hat { Y }$ is what the constrained model predicts as $Y$ .\n\nDespite its desirable features, there are major prices to be paid by avoiding structural equations. For technical reasons, enforcing such constraints require throwing away information from any descendant of $A$ that is judged to be on an “unfair path” from $A$ to $Y$ . Further, in many cases, the causal effect of interest is not identifiable. Even if it is, the constrained fitting procedure can be both computationally challenging and not necessarily have a clear relationship to the actual loss function of interest in predicting $Y$ . This is because it first requires assuming a model for $Y$ and fitting a projection of the model to the space of constrained distributions. And while explicit claims about structural equations are avoided, the approach still relies on cross-world assumptions of independences among counterfactuals.\n\nA different take with a similar motivation was recently introduced by [44]. It focuses on decomposing counterfactual measures of causal effects across different paths to explain discrimination. For example, the association between $A$ and $Y$ can be decomposed by the causal effect of $A$ on $Y$ that is due to directed paths from $A$ to $Y$ and due to a common cause of $A$ and $Y$ . Although no particular way of building a fair predictor $\\hat { Y }$ is provided, [44] explicitly discuss how modifications to particular parts of the causal model can lead to changes in the counterfactual measures of discrimination, an important and complementary goal of the problem of fair predictions. This work also illustrates alternative interpretations of causal unfairness: in our work, we would not consider $\\hat { Y }$ to be unfair if the causal graph is $A \\left. X \\right. { \\hat { Y } }$ , as $A$ is not a cause of $\\hat { Y }$ , while [44] would label it as a type of “spurious” discrimination. Our take is that if $\\hat { Y }$ is deemed unfair, it must be so by considering $X$ as yet another protected attribute.\n\nRegarding the challenges posed by causal modelling, we advocate that assumptions about structural equations are still useful if interpreted as claims about how latent variables with a clear domain-specific meaning contribute to the pathways of interest. Even if imperfect, this direction is a way of increasing transparency about the motivation for labelling particular variables and paths as “unfair”. It still offers a chance of falsification and improvement when thenlatent variables are measured in subsequent studies. If there are substantive disagreements about the functional form of structural equations, multiple models can be integrated in the same predictive framework as introduced by [35].\n\n[Section: 5.3.3 Path-specific Variations]\n\nA common theme of the alternatives to counterfactual fairness [21, 28, 44] is the focus on path-specific effects, which was only briefly mentioned in our formulation. One example for understanding path-specificity and its relation to fairness\n\nis the previously discussed case study of gender bias in the admissions to the University of California at Berkeley in the 1970s: gender ( $A$ ) and admission ( $Y$ ) were found to be associated in the data, which lead to questions about fairness of the admission process. One explanation found was that this was due to the choice of department each individual was applying to ( $X$ ). By postulating the causal structure $A  X  Y$ , we could claim that, even though $A$ is a cause of $Y$ , the mechanism by which it changes $Y$ is “fair” in the sense that we assume free-will in the choice of department made by each applicant. This is of course a judgement call that leaves unexplained why there is an interaction between $A$ and other causes of $X$ , but one that many analysts would agree with. The problem gets more complicated if edge $A  Y$ is also present.\n\nThe approach by [28] can tap directly from existing methods for deriving path-specific effects as functions of $P ( V )$ (see [36] for a review). The method by [21] and the recent variation of counterfactual fairness by [7] consist of adding “corrections” to a causal model to deactivate particular causal contributions to $Y$ . This is done by defining a particular fairness difference we want to minimise, typically the expected difference of the outcomes $Y$ under different levels of $A$ . [7] suggest doing this without parameterising a family of $\\hat { Y }$ to be optimised. At its most basic formulation, if we define $P S E ( V )$ as the particular path-specific effect from $A$ to $Y$ for a particular set of observed variables $V$ , by taking expectations under $Y$ we have that a fair $\\hat { Y }$ can be simply defined as $\\mathsf { E } [ Y \\mid V \\backslash \\{ Y \\} ] - P S E ( V )$ . Like in [28], it is however not obvious which optimally is being achieved for the prediction of $Y$ as it is unclear how such a transformation would translate in terms of projecting $Y$ into a constrained space when using a particular loss function.\n\nOur own suggestion for path-specific counterfactual fairness builds directly on the original: just extract latent fair variables from observed variables that are known to be (path-specifically) fair and build a black-box predictor around them. For interpretation, it is easier to include $\\hat { Y }$ in the causal graph (removing $Y$ , which plays no role as an input to $\\hat { Y }$ ), adding edges from all other vertices into $\\hat { Y }$ . Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ . Assume that, similar to the Berkeley case, we forbid path $A $ $X _ { 1 }  { \\hat { Y } }$ as an unfair contribution to $\\hat { Y }$ , while allowing contributions via $X _ { 2 }$ (that is, paths $A  X _ { 2 }  { \\hat { Y } }$ and $A \\to X _ { 2 } \\to X _ { 1 } \\to { \\hat { Y } }$ . This generalises the Berkeley example, where $X _ { 2 }$ would correspond to department choice and $X _ { 1 }$ to, say, some source of funding that for some reason is also affected by the gender of the applicant). Moreover, we also want to exclude the direct contribution $A  { \\hat { Y } }$ . Assuming that a unit would be set to a factual baseline value $a$ for $A$ , the “unfair propagation” of a counterfactual value $a ^ { \\prime }$ of $A$ could be understood as passing it only through $A \\to X _ { 1 } \\to { \\hat { Y } }$ and $A  { \\hat { Y } }$ in Figure 2(b), leaving the inputs “through the other edges” at the baseline [30, 36]. The relevant counterfactual for $\\hat { Y }$ is the nested counterfactual $\\hat { Y } ( a ^ { \\prime } , X _ { 1 } ( a ^ { \\prime } , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) )$ . A direct extension of the definition of counterfactual fairness applies to this",
      "context_after": "To enforce the constraint in (9), the input to $\\hat { Y }$ can be made to depend only on the factuals which will not violate the above. For instance, $\\hat { Y } \\equiv X _ { 1 }$ will violate the above, as $\\dot { Y } ( a , X _ { 1 } ( a , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = X _ { 1 } ( a , X _ { 2 } ( a ) ) = x _ { 1 }$ while $\\hat { Y } ( a ^ { \\prime } , X _ { 1 } ( a , X _ { 2 } ( a ^ { \\prime } ) ) , X _ { 2 } ( a ^ { \\prime } ) ) = X _ { 1 } ( a ^ { \\prime } , X _ { 2 } ( a ) ) \\neq x _ { 1 }$ (in general). $\\dot { Y } \\equiv X _ { 2 }$ is acceptable, as $Y ( a , X _ { 1 } ( a , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = Y ( a ^ { \\prime } , X _ { 1 } ( a ^ { \\prime } , X _ { 2 } ( a ) ) , X _ { 2 } ( a ) ) = X _ { 2 } ( a ) =$ $x _ { 2 }$ .\n\nWe need a definition of $\\hat { Y }$ that is invariant as in (9) to any counterfactual world we choose. Assume that, in our example, $A \\in \\{ a _ { 1 } , \\ldots , a _ { K } \\}$ and the structural equation for $X _ { 1 }$ is $X _ { 1 } = f _ { X _ { 1 } } ( A , X _ { 2 } , U _ { X _ { 1 } } )$ for some background variable $U _ { X _ { 1 } }$ . We can simply set\n\nfor some arbitrary function $g ( \\cdot )$ . The only descendant of $A$ in this case is $X _ { 2 }$ , which we know we can use. Overall, this provides more predictive information than what would be allowed by using $U _ { X _ { 1 } }$ and $U _ { X _ { 2 } }$ only. One important point: we know that $f _ { X _ { 1 } } ( a , x _ { 2 } , U _ { X _ { 1 } } ) = x _ { 1 }$ when conditioning on $A = a$ and $X _ { 2 } = x _ { 2 }$ , which means that $\\ddot { Y }$ is a function of $X _ { 1 }$ . This seems to contradict (9). In fact it does not, because $g ( \\cdot )$ is defined without a priori knowledge of which values of $A$ will be factual for which units.\n\nFor instance, if $K = 2$ and $\\dot { Y } \\equiv \\alpha _ { 1 } f _ { X _ { 1 } } ( a _ { 1 } , X _ { 2 } , U _ { X _ { 1 } } ) + \\alpha _ { 2 } f _ { X _ { 1 } } ( a _ { 2 } , X _ { 2 } , U _ { X _ { 1 } } ) + \\nonumber$ $\\beta _ { 2 } X _ { 2 } + \\beta _ { 1 } U _ { X _ { 1 } }$ , then for a unit with $\\begin{array} { r } { A = a _ { 1 } , X _ { 2 } = x _ { 2 } , U _ { X _ { 1 } } = u _ { X _ { 1 } } , X _ { 1 } = x _ { 1 } = } \\end{array}$ $f _ { X _ { 1 } } ( a _ { 1 } , x _ { 2 } , u _ { X _ { 1 } } ) )$ we have\n\nFor a unit with $\\begin{array} { r } { \\stackrel { \\prime } { A } = a _ { 2 } , X _ { 2 } = x _ { 2 } , U _ { X _ { 1 } } = u _ { X _ { 1 } } , X _ { 1 } = x _ { 1 } ^ { \\prime } = f _ { X _ { 1 } } ( a _ { 2 } , x _ { 2 } , u _ { X _ { 1 } } ) ) . } \\end{array}$",
      "referring_paragraphs": [
        "Figure 2(a) shows an example with three variables $A , X _ { 1 } , X _ { 2 }$ and the predictor $\\hat { Y }$ .",
        "Figure 2: (a) A causal graph linking protected attribute $A$ to predictor $\\hat { Y }$ , where only a subset of edges will “carry” counterfactual values of $A$ in order to represent the constraints of path-specific counterfactual fairness. (b) This diagram, inspired by [30], is a representation of how counterfactuals are propagated only through some edges. For other edges, inputs are based on the baseline value $a$ of an individual.\n\npath-specific scenario: we require"
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1805.05859_page0_fig4.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1805.09458": [
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg",
      "image_filename": "1805.09458_page0_fig0.jpg",
      "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.",
      "context_before": "We have two modified VAE loss (Eq. 18) and modified VIB loss (Eq. 31). In both we have to learn an encoder and decoder pair $q ( z | x )$ and $p ( x | z , c )$ . We use feed forward networks to approximate these functions. For $q ( z | x )$ we use the Gaussian reparameterization trick, and for $p ( x | z , c )$ we simply concatenate $c$ onto $z$ as extra input features to be decoded. In the modified VIB we also have a predictor branch $p ( y | z )$ , which we also use a feed forward network to parametrize. Specific architectures (e.g. number of layers and nodes per layer for each branch) vary by domain.\n\nWe evaluate the performance on of our proposed invariance penalty on two datasets with a “fair classification” task. We also demonstrate “Fader Network”-like capabilities for manipulating specified factors in generative modeling on the MNIST dataset.\n\n[Section: 3.1 Fair Classification]\n\nFor each fair classification dataset/task we evaluated both prediction accuracy and adversarial error in predicting $c$ from the latent code. We compare against the Variational Fair Autoencoder (VFAE) [15], and the adversarial method proposed in Xie et al. [22]. Both datasets are from the UCI repository. The preprocessing for both datasets follow Zemel et al. 2013[23], which is also the source for the pre-processing in our baselines [15, 22].\n\nThe first dataset is the German dataset, containing 1000 samples of personal financial data. The objective is to predict whether a person has a good credit score, and the protected class is Age (which, as per [23], is binarized). The second dataset is the Adult dataset, containing 45,222 data points of US census data. The objective is to predict whether or not a person has over 50,000 dollars saved in the bank. The protected factor for the Adult dataset is Gender3.\n\nWherever possible we use architectural constraints from previous papers. All encoders and decoders are single layer, as specified by Louizos et al. [15] (including those in the baselines), and for both datasets we use 64 hidden units in our method as in Xie et al., while for VFAE we use their described architecture. We use a latent space of 30 dimensions for each case. We train using Adam using the same hyperparameter settings as in Xie et al., and a batch size of 128. Optimization and parameter tuning is done via a held-out validation set.\n\n<table><tr><td>German Dataset</td><td>Adv. Loss</td><td>Pred Acc.</td></tr><tr><td>Maj. Class</td><td>0.725</td><td>0.695</td></tr><tr><td>VFAE [15]</td><td>0.717</td><td>0.720</td></tr><tr><td>Xie et al. [22]</td><td>0.811</td><td>0.695</td></tr><tr><td>Proposed</td><td>0.698</td><td>0.710</td></tr></table>\n\n<table><tr><td>Adult Dataset</td><td>Adv. Loss</td><td>Pred Acc.</td></tr><tr><td>Maj. Class</td><td>0.675</td><td>0.752</td></tr><tr><td>VFAE [15]</td><td>0.882</td><td>0.842</td></tr><tr><td>Xie et al. [22]</td><td>0.888</td><td>0.831</td></tr><tr><td>Proposed</td><td>0.776</td><td>0.842</td></tr></table>",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets.",
        "For the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have",
        "For the larger Adult dataset shown on the bottom table of Figure 1, all three methods again have comparable predictive accuracy."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg",
      "image_filename": "1805.09458_page0_fig1.jpg",
      "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.",
      "context_before": "",
      "context_after": "[Section: 3.2 Unsupervised Learning]\n\nWe demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written. This allows us to manipulate the decoder at test time to produce different artificial digits based on the style of one digit. We use 2 hidden layers with 512 nodes for both the encoder and the decoder.\n\n[Section: 4 Results]",
      "referring_paragraphs": [
        "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1805.09458_page0_fig2.jpg",
        "1805.09458_page0_fig3.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig3.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1805.09458",
      "figure_id": "1805.09458_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg",
      "image_filename": "1805.09458_page0_fig4.jpg",
      "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.",
      "context_before": "We demonstrate a form of unsupervised image manipulation inspired by Fader Networks [14] on the MNIST dataset. We use the digit label as the covariate class $c$ , which pushes all non-class stylistic information into the latent space while attempting to remove information about the exact digit being written. This allows us to manipulate the decoder at test time to produce different artificial digits based on the style of one digit. We use 2 hidden layers with 512 nodes for both the encoder and the decoder.\n\n[Section: 4 Results]\n\nFor the German dataset shown on top table of Figure 1, the methods are roughly equivalent. All methods have comparable predictive accuracy, while the VFAE and the proposed method have",
      "context_after": "[Section: 5 Discussion]\n\nAs show analytically in Section 2.1.2, in the optimal case adversarial training can perform as well as our derived method; it is also intuitively simple and allows for more nuanced tuning. However, it introduces an extra layer of complexity (indeed, a second optimization problem) into our system. In this particular case of invariant representation, our results lead us to believe that adversarial training is unnecessary.\n\nThis does not mean that adversarial training for invariant representations is strictly worse in practice. There are certainly cases where training an adversary may be easier or less restrictive than other\n\nmethods, and due to its shared literature with Generative Adversarial Networks [10], there may be training heuristics or other techniques that can improve performance.\n\nOn the otherhand, we believe that our derivations here shed light on why these methods might fail. We believe specific failure modes of adversarial training can be attributed to Eq. 27, where the adversary fails to achieve the infimum. Bad approximations (i.e. weak or poorly trained adversaries) may provide bad gradient information to the system, leading to poor performance of the encoder against a post-hoc adversary.\n\nOur experimental results do not match those reported in Xie et al. While in general their method has comparable performance for predictive accuracy, we do not find that their adversarial error is low; instead, we find that the encoder/adversary pair becomes stuck in local minima. We also find that the adversary trained alongside the encoder performs badly against the encoder (i.e. the adversary cannot predict $c$ well), but a post-hoc trained adversary performs very well, easily predicting $c$ (as demonstrated by our experiments).\n\nIt may be that we have inadvertently built a stronger adversary. We have attempted to follow the author’s experimental design as closely as possible, using the same architecture and the same adversary (using the gradient-flip trick and 3-layer feed forward networks). With the details provided we could not replicate their reported adversarial error for their method, nor for the VFAE method. However, we are able to reproduce the adversarial error reported in Louizos et al., which uses logisic regression. In general for stronger adversaries the adversarial loss will increase, but the relative rankings should remain roughly the same.\n\n[Section: 6 Conclusion]",
      "referring_paragraphs": [
        "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1805.11202": [
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg",
      "image_filename": "1805.11202_page0_fig0.jpg",
      "caption": "Figure 1: Illustration of generative adversarial networks",
      "context_before": "In fairness-aware learning, the literature has studied notions of group fairness on dataset and classification [11, 14].\n\nDefinition 1 (Statistical Parity/Fairness in a Labeled Dataset). Given a labeled dataset $\\mathcal { D }$ , the property of statistical parity or fairness in the labeled dataset is defined as:\n\nThe discrimination in a labeled dataset w.r.t the protected attribute $s$ is evaluated by the risk difference: $d i s c ( \\mathcal { D } ) = P ( y = 1 | s =$ $1 ) - P ( y = 1 | s = 0 )$ .\n\nDefinition 2 (Statistical Parity/Fairness in a Classifier). Given a labeled dataset $\\mathcal { D }$ and a classifier $\\eta :  { \\mathcal { X } } \\to  { \\mathcal { Y } }$ , the property of ηstatistical parity or fairness in a classifier is defined as:\n\nWe can then derive the discrimination in a classifier in terms of risk difference as $d i s c ( \\eta ) = P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( \\eta ( \\mathbf { x } ) = 1 | s = 0 )$ .\n\ndisc η P η s P η sThe classification fairness on a dataset is achieved if both the disparate treatment and disparate impact are removed from the data. To remove the disparate treatment, the classifier cannot use the protected attribute to make decisions. As for the disparate impact, research in [9] proposed the concept of $\\epsilon$ -fairness to examine the potential disparate impact.\n\nDefinition 3 $\\epsilon$ -fairness [9]). A labeled dataset $\\mathcal { D } = ( \\boldsymbol { \\chi } , \\boldsymbol { y } , \\boldsymbol { s } )$ is said to be $\\epsilon$ ϵ-fair if for any classification algorithm $f : X \\to S$",
      "context_after": "indicates the average class-conditioned error of $f$ on distribu-BERtion $\\mathcal { D }$ over the pair $( \\chi , s )$ .\n\nThe $\\epsilon$ -fairness quantifies the fairness of data through the error ϵrate of predicting the protected attribute $s$ given the unprotected attributes $\\chi$ . If the error rate is low, it means $s$ is predictable by $\\chi$ . In the fair data generation scenario, for a classifier trained on the synthetic dataset and tested on the real dataset, the classification fairness is achieved if disparate impact in terms of the real protected attribute is removed from the synthetic dataset, i.e. $\\hat { X } \\perp \\perp s$ .\n\n[Section: 3.2 Generative Adversarial Network]\n\nGenerative adversarial nets (GAN) are generative models that consist of two components: a generator $G$ and a discriminator $D$ . Typically, both $G$ and $D$ Gare multilayer neural networks. $G ( \\mathbf { z } )$ Dgenerates fake samples from a prior distribution $P _ { \\mathbf { z } }$ on a noise variable z and learns a generative distribution $P _ { G }$ Pto match the real data distribution $P _ { \\mathrm { d a t a } }$ PG. The discriminative component $D$ is a binary classifier Pthat predicts whether an input is real data $\\mathbf { x }$ Dor fake data generated from $G ( \\mathbf { z } )$ . The objective function of $D$ is defined as:",
      "referring_paragraphs": [
        "Figure 1: Illustration of generative adversarial networks\n\nwith empirical probabilities estimated from $\\mathcal { D }$ , where  (balanced error rate) is defined as",
        "Figure 1 illustrates the structure of GAN.",
        "Table 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>\n\nshows the risk differences in the real and synthetic datasets."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg",
      "image_filename": "1805.11202_page0_fig1.jpg",
      "caption": "Figure 2: The Structure of FairGAN",
      "context_before": "where $D ( \\cdot )$ outputs the probability that $\\cdot$ is from the real data rather Dthan the generated fake data. In order to make the generative distribution $P _ { G }$ close to the real data distribution $P _ { \\mathrm { d a t a } }$ , $G$ is trained by PG P Gfooling the discriminator unable to distinguish the generated data from the real data. Thus, the objective function of $G$ is defined as:\n\nMinimization of Equation 2 ensures that the discriminator is fooled by $G ( \\mathbf { z } )$ and $D$ predicts high probability that $G ( \\mathbf { z } )$ is real data.\n\nG D GOverall, GAN is formalized as a minimax game min max $V ( G , D )$ G D with the value function:\n\nFigure 1 illustrates the structure of GAN. Theoretical analysis shows that GAN aims to minimize the Jensen-Shannon divergence (JSD) between $P _ { \\mathrm { d a t a } }$ and $P _ { G }$ [10]. Minimization of the JSD is achieved when $P _ { G } = P _ { \\mathrm { d a t a } }$ .\n\nPG PGAN for discrete data generation. The generator of a regular GAN cannot generate discrete samples because $G$ is trained by the loss from $D$ Gvia backpropagation [10]. In order to tackle this Dlimitation, medGAN incorporates an autoencoder model in a regular GAN model to generate high-dimensional discrete variables [5].",
      "context_after": "where $\\mathbf { x } ^ { \\prime } = D e c ( E n c ( \\mathbf { x } ) )$ . Because the hidden representation can Dec Encbe used to reconstruct the original input, it captures the salient information of the input.\n\nTo generate the dataset which contains discrete attributes, the generator $G _ { D e c }$ in medGAN consists of two components, the generator $G$ GDecand the decoder . The generator $G$ is trained to generate the salient representations. The decoder  from autoencoder Decseeks to construct the synthetic data from the salient representations $D e c ( G ( \\mathbf { z } ) )$ . Hence, the generator of medGAN $G _ { D e c } ( \\mathbf { z } )$ is Dedefined as:\n\nwhere z is a noise variable. The discriminator $D$ aims to distinguish whether the input is from real data or $D e c ( G ( \\mathbf { z } ) )$ . With playing the adversarial game with $D$ Dec G, the gradient backpropagates from $D$ to Dthe decoder  and further to the generator $G$ Din an end-to-end Decmanner. Hence, the generator $G _ { D e c }$ Gcan be viewed as a regular Decgenerator  with extra hidden layers that maps continuous salient representations to discrete samples.\n\n[Section: 4 FAIRGAN]",
      "referring_paragraphs": [
        "Figure 2: The Structure of FairGAN\n\nAutoencoder is a feedforward neural network used for unsupervised learning.",
        "Figure 2 shows the structure of FairGAN.",
        "In Table 2, we further evaluate the closeness between each synthetic dataset and the real dataset by calculating the Euclidean distance of joint and conditional probabilities $( P ( \\mathbf { x } , y ) , P ( \\mathbf { x } , y , s )$ , and $P ( \\mathbf { x } , y | s ) )$ P ,y P ,y, s."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1805.11202_page0_fig2.jpg",
        "(a) Toy Dataset",
        "1805.11202_page0_fig3.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig3.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
      "image_filename": "1805.11202_page0_fig4.jpg",
      "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.",
      "context_before": "",
      "context_after": "[Section: 5 EXPERIMENTS]\n\nWe evaluate the performance of FairGAN on fair data generation and fair classification.\n\n[Section: 5.1 Experimental Setup]",
      "referring_paragraphs": [
        "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset.",
        "We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called\n\nTable 3: Risk differences in classifiers and classification accuracies on various training and testing settings   \n\n<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Classifier</td><td>REAL2REAL</td><td colspan=\"4\">SYN2SYN</td><td colspan=\"4\">SYN2REAL</td></tr><tr><td></td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td rowspan=\"3\">Risk Difference</td><td>SVM (Linear)</td><td>0.1784</td><td>0.1341±0.0023</td><td>0.0018±0.0021</td><td>0.0073±0.0039</td><td>0.0371±0.0189</td><td>0.1712±0.0062</td><td>0.1580±0.0076</td><td>0.1579±0.0079</td><td>0.0461±0.0424</td></tr><tr><td>SVM (RBF)</td><td>0.1788</td><td>0.1292±0.0049</td><td>0.0018±0.0025</td><td>0.0074±0.0028</td><td>0.0354±0.0206</td><td>0.1623±0.0050</td><td>0.1602±0.0053</td><td>0.1603±0.0087</td><td>0.0526±0.0353</td></tr><tr><td>Decision Tree</td><td>0.1547</td><td>0.1396±0.0089</td><td>0.0015±0.0035</td><td>0.0115±0.0061</td><td>0.0535±0.0209</td><td>0.1640±0.0077</td><td>0.1506±0.0070</td><td>0.1588±0.0264</td><td>0.0754±0.0641</td></tr><tr><td rowspan=\"3\">Accuracy</td><td>SVM (Linear)</td><td>0.8469</td><td>0.8281±0.0103</td><td>0.8162±0.0133</td><td>0.8226±0.0126</td><td>0.8247±0.0115</td><td>0.8363±0.0108</td><td>0.8340±0.0091</td><td>0.8356±0.0018</td><td>0.8217±0.0093</td></tr><tr><td>SVM (RBF)</td><td>0.8433</td><td>0.8278±0.0099</td><td>0.8160±0.0100</td><td>0.8215±0.0130</td><td>0.8233±0.0103</td><td>0.8342±0.0036</td><td>0.8337±0.0060</td><td>0.8349±0.0012</td><td>0.8178±0.0128</td></tr><tr><td>Decision Tree</td><td>0.8240</td><td>0.8091±0.0059</td><td>0.7926±0.0083</td><td>0.8055±0.0102</td><td>0.8077±0.0144</td><td>0.8190±0.0051</td><td>0.8199±0.0041</td><td>0.8158±0.0069</td><td>0.8044±0.0140</td></tr></table>\n\nSYN2SYN; 3) the classifiers are trained on the synthetic datasets and tested on the real dataset, called SYN2REAL."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) NaïveFairGAN-I",
        "(c) NaïveFairGAN-II",
        "1805.11202_page0_fig5.jpg",
        "(d) FairGAN",
        "1805.11202_page0_fig6.jpg",
        "(a) Real Dataset",
        "1805.11202_page0_fig7.jpg",
        "(b) SYN1-GAN",
        "1805.11202_page0_fig8.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig8.jpg"
        ],
        "panel_count": 5
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_10",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg",
      "image_filename": "1805.11202_page0_fig9.jpg",
      "caption": "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs. $P ( \\mathbf { x } , y | s = 0 )$ ). Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the conditional probability given $s = 1$ P ,y s P ,y s. The y-axis represents the conditional probability given $s = 0$ . sThe diagonal line indicates the ideal fairness, where data have identical conditional probability distributions given .",
      "context_before": "",
      "context_after": "[Section: 5.2 Fair Data Generation]\n\nWe evaluate FairGAN on data generation from two perspectives, fairness and utility. Fairness is to check whether FairGAN can generate fair data, while the utility is to check whether FairGAN can learn the distribution of real data precisely.\n\nFairness. We adopt the risk difference in a labeled dataset $( d i s c ( \\mathscr { D } ) =$ $P ( y = 1 | s = 1 ) - P ( y = 1 | s = 0 ) ,$ disc as the metric to compare the per-P y s P y sformance of different GAN models on fair data generation. Table 1\n\nTable 1: Risk differences of real and synthetic datasets   \n\n<table><tr><td></td><td>Real Data</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td>\\( disk(D) \\)</td><td>0.1989</td><td>0.1798±0.0026</td><td>0.0025±0.0007</td><td>0.0062±0.0037</td><td>0.0411±0.0295</td></tr></table>\n\nshows the risk differences in the real and synthetic datasets. The risk difference in the Adult dataset is 0.1989, which indicates discrimination against female. The SYN-GAN, which is trained to be close to the real dataset, has the similar risk difference to the real dataset. On the contrary, SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN have lower risk differences than the real dataset. In particular, both SYN2-NFGANI and SYN3-NFGANII have extremely small risk differences. This is because the protected attribute of SYN2-NFGANI and SYN3-NFGANII is independently assigned, i.e., $\\hat { y } \\perp \\perp \\hat { s }$ . Hence, the synthetic datasets from SYN2-NFGANI and SYN3-NFGANII are free from disparate treatment. FairGAN prevents the disparate treatment by generating revised $\\hat { y }$ to make $\\hat { y } \\perp \\perp \\hat { s } .$ . The risk differy y sence of SYN4-FairGAN is 0.0411, which shows the effectiveness of FairGAN on fair data generation.\n\nIn Figure 4, we compare the dimension-wise conditional probability distributions between $P ( \\mathbf { x } , y | s = 1 )$ and $P ( \\mathbf { x } , y | s = 0 )$ . Each P ,y s P ,y sdot indicates one attribute. The diagonal line indicates the ideal fairness, where the conditional probability distributions of each attribute given $s = 1$ and $s = 0$ are identical. We can observe that the dimension-wise distributions of datasets with lower risk differences are closer to the diagonal line. For example, dimension-wise conditional probabilities of real dataset and SYN1-GAN are spread around the diagonal line (shown in Figures 4a and 4b), while conditional probabilities of SYN2-NFGANI and SYN3-NFGANII with the lowest risk difference are just on the diagonal line (shown in Figure 4c). SYN4-FairGAN also achieves reasonable risk difference, so the attribute dots are close to the diagonal line. Overall, the synthetic datasets from SYN2-NFGANI, SYN3-NFGANII, and SYN4-FairGAN can prevent the disparate treatment.\n\nWe further evaluate the $\\epsilon$ -fairness (disparate impact) by calcuϵlating the balanced error rates (BERs) in the real data and SYN4- FairGAN. Because the protected attribute in SYN2-NFGANI and SYN3-NFGANII are randomly assigned, the real  given xˆ is unsknown. The BERs in SYN2-NFGANI and SYN3-NFGANII cannot be calculated. The BER in the real dataset is 0.1538, which means a classifier can predict  given x with high accuracy. Hence, there is",
      "referring_paragraphs": [
        "Figure 4: Dimension-wise conditional probability distributions $P ( \\mathbf { x } , y | s = 1 )$ vs.",
        "In Figure 4, we compare the dimension-wise conditional probability distributions between $P ( \\mathbf { x } , y | s = 1 )$ and $P ( \\mathbf { x } , y | s = 0 )$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(c) SYN2-NFGANI",
        "(d) SYN3-NFGANII",
        "1805.11202_page0_fig10.jpg",
        "(e) SYN4-FairGAN",
        "1805.11202_page0_fig11.jpg",
        "(a) SYN1-GAN: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
        "1805.11202_page0_fig12.jpg",
        "(b) SYN1-GAN:$P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs.$P _ { G } ( \\mathbf { x } , y | s = 1 )$ )",
        "1805.11202_page0_fig13.jpg",
        "(c) SYN1-GAN: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$",
        "1805.11202_page0_fig14.jpg",
        "(d) SYN2-NFGANI: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
        "1805.11202_page0_fig15.jpg",
        "(e) SYN2-NFGANI: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 1 )$",
        "1805.11202_page0_fig16.jpg",
        "(f) SYN2-NFGANI: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$",
        "1805.11202_page0_fig17.jpg",
        "(g) SYN3-NFGANII: $\\bar { P } _ { \\mathbf { d a t a } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
        "1805.11202_page0_fig18.jpg",
        "(h) SYN3-NFGANII: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 1 )$ )",
        "1805.11202_page0_fig19.jpg",
        "(i) SYN3-NFGANII: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$",
        "1805.11202_page0_fig20.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig20.jpg"
        ],
        "panel_count": 12
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_22",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg",
      "image_filename": "1805.11202_page0_fig21.jpg",
      "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.",
      "context_before": "",
      "context_after": "[Section: 5.3 Fair Classification]\n\nIn this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness. We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called\n\nTable 3: Risk differences in classifiers and classification accuracies on various training and testing settings   \n\n<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Classifier</td><td>REAL2REAL</td><td colspan=\"4\">SYN2SYN</td><td colspan=\"4\">SYN2REAL</td></tr><tr><td></td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td rowspan=\"3\">Risk Difference</td><td>SVM (Linear)</td><td>0.1784</td><td>0.1341±0.0023</td><td>0.0018±0.0021</td><td>0.0073±0.0039</td><td>0.0371±0.0189</td><td>0.1712±0.0062</td><td>0.1580±0.0076</td><td>0.1579±0.0079</td><td>0.0461±0.0424</td></tr><tr><td>SVM (RBF)</td><td>0.1788</td><td>0.1292±0.0049</td><td>0.0018±0.0025</td><td>0.0074±0.0028</td><td>0.0354±0.0206</td><td>0.1623±0.0050</td><td>0.1602±0.0053</td><td>0.1603±0.0087</td><td>0.0526±0.0353</td></tr><tr><td>Decision Tree</td><td>0.1547</td><td>0.1396±0.0089</td><td>0.0015±0.0035</td><td>0.0115±0.0061</td><td>0.0535±0.0209</td><td>0.1640±0.0077</td><td>0.1506±0.0070</td><td>0.1588±0.0264</td><td>0.0754±0.0641</td></tr><tr><td rowspan=\"3\">Accuracy</td><td>SVM (Linear)</td><td>0.8469</td><td>0.8281±0.0103</td><td>0.8162±0.0133</td><td>0.8226±0.0126</td><td>0.8247±0.0115</td><td>0.8363±0.0108</td><td>0.8340±0.0091</td><td>0.8356±0.0018</td><td>0.8217±0.0093</td></tr><tr><td>SVM (RBF)</td><td>0.8433</td><td>0.8278±0.0099</td><td>0.8160±0.0100</td><td>0.8215±0.0130</td><td>0.8233±0.0103</td><td>0.8342±0.0036</td><td>0.8337±0.0060</td><td>0.8349±0.0012</td><td>0.8178±0.0128</td></tr><tr><td>Decision Tree</td><td>0.8240</td><td>0.8091±0.0059</td><td>0.7926±0.0083</td><td>0.8055±0.0102</td><td>0.8077±0.0144</td><td>0.8190±0.0051</td><td>0.8199±0.0041</td><td>0.8158±0.0069</td><td>0.8044±0.0140</td></tr></table>\n\nSYN2SYN; 3) the classifiers are trained on the synthetic datasets and tested on the real dataset, called SYN2REAL. The ratio of the training set to testing set in these three settings is 1:1. We emphasize that only SYN2REAL is meaningful in practice as the classifiers are trained from the generated data and are adopted for decision making on the real data.\n\nWe adopt the following classifiers to evaluate the fair classification: 1) SVM (linear) which is a linear support vector machine with $C = 1$ ; 2) SVM (RBF) which is a support vector machine with Cthe radial basis kernel function; 3) Decision Tree with maximum tree depth as 5; Note that we do not adopt the protected attribute and only use the unprotected attributes to train classifiers, which ensures there is no disparate treatment in classifiers.\n\nFairness. We adopt the risk difference in a classifier $( d i s c ( \\eta ) =$ $P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( \\eta ( \\mathbf { x } ) = 1 | s = 0 ) \\}$ disc η) to evaluate the performance P η s P η sof classifier on fair prediction. Table 3 shows the risk differences in classifiers on various training and testing settings. We can observe that when the classifiers are trained and tested on real datasets (i.e., REAL2REAL), the risk differences in classifiers are high. It indicates that if there is disparate impact in the training dataset, the classifiers also incur discrimination for prediction. Since SYN1- GAN is close to the real dataset, classifiers trained on SYN1-GAN also have discrimination in both SYN2SYN and SYN2REAL settings.\n\nAlthough SYN2-NFGANI and SYN3-NFGANII have similar distributions as the real dataset on unprotected attributes and decision, i.e., $P _ { G } ( \\mathbf { x } , y ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ , classifiers which are trained and tested PG ,y P ,yin SYN2SYN settings achieve low risk differences. This is because the values of the protected attribute in SYN2-NFGANI or SYN3- NFGANII are independently generated or shuffled. Since both xˆ and $\\hat { y }$ have no correlations with the generated ˆ in SYN2-NFGANI and y sSYN3-NFGANII, the statistical parity in classifiers can be achieved when trained and tested on synthetic datasets.\n\nHowever, when classifiers are trained on SYN2-NFGANI and SYN3-NFGANII and tested on the real dataset (i.e., SYN2REAL), the classifiers still have significant discrimination against the protected group. Because the unprotected attributes of SYN2-NFGANI and SYN3-NFGANII are close to the real dataset, the correlations between the generated $\\hat { \\mathbf { x } }$ and the real  are still preserved. The sdisparate impact in terms of the real  on SYN2-NFGANI and SYN3- sNFGANII is not removed. When classifiers are tested on the real dataset where the correlations between x and  are preserved, the sclassification results indicate discrimination. On the contrary, when the classifiers are trained on SYN4-FairGAN and tested on the real dataset, we can observe that the risk differences in classifiers are small. Since the FairGAN prevents the discrimination by generating\n\nxˆ that don’t have correlations with the real , i.e. free from the dissparate impact, the classifier trained on SYN4-FairGAN can achieve fair classification on the real dataset. It demonstrates the advantage of FairGAN over the NaïveFairGAN models on fair classification.\n\nClassification accuracy. Table 3 further shows the classification accuracies of different classifiers on various training and testing settings. We can observe that the accuracies of classifiers on the SYN2REAL setting are close to the results on the REAL2REAL setting. It indicates synthetic datasets generated by different GAN models are similar to the real dataset, showing the good data generation utility of GAN models. Meanwhile, accuracies of classifiers which are trained on SYN4-FairGAN and tested on real dataset are only slightly lower than those trained on SYN1-GAN, which means the FairGAN model can achieve a good balance between utility and fairness. The small utility loss is caused by modifying unprotected attributes to remove disparate impact in terms of the real .\n\n[Section: 5.4 Parameter Sensitivity]",
      "referring_paragraphs": [
        "Figure 5: Dimension-wise probability distributions."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(j) SYN4-FairGAN: $P _ { \\mathrm { { d a t a } } } ( \\mathbf { x } , y )$ vs. $P _ { G } ( \\mathbf { x } , y )$",
        "(k) SYN4-FairGAN: $P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y | s = 1 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 1 )$",
        "1805.11202_page0_fig22.jpg",
        "(l) SYN4-FairGAN: $P _ { \\bf d a t a } ( { \\bf x } , y | s = 0 )$ vs. $P _ { G } ( \\mathbf { x } , y | s = 0 )$ )",
        "1805.11202_page0_fig23.jpg"
      ],
      "quality_score": 0.6,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig23.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1805.11202",
      "figure_id": "1805.11202_fig_25",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg",
      "image_filename": "1805.11202_page0_fig24.jpg",
      "caption": "Figure 6: The sensitivity analysis of FairGAN with various",
      "context_before": "[Section: 5.3 Fair Classification]\n\nIn this subsection, we adopt the real and synthetic datasets to train several classifiers and check whether the classifiers can achieve fairness. We evaluate the classifiers with three settings: 1) the classifiers are trained and tested on the real dataset, called REAL2REAL; 2) the classifiers are trained and tested on the synthetic datasets, called\n\nTable 3: Risk differences in classifiers and classification accuracies on various training and testing settings   \n\n<table><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Classifier</td><td>REAL2REAL</td><td colspan=\"4\">SYN2SYN</td><td colspan=\"4\">SYN2REAL</td></tr><tr><td></td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td><td>SYN1-GAN</td><td>SYN2-NFGANI</td><td>SYN3-NFGANII</td><td>SYN4-FairGAN</td></tr><tr><td rowspan=\"3\">Risk Difference</td><td>SVM (Linear)</td><td>0.1784</td><td>0.1341±0.0023</td><td>0.0018±0.0021</td><td>0.0073±0.0039</td><td>0.0371±0.0189</td><td>0.1712±0.0062</td><td>0.1580±0.0076</td><td>0.1579±0.0079</td><td>0.0461±0.0424</td></tr><tr><td>SVM (RBF)</td><td>0.1788</td><td>0.1292±0.0049</td><td>0.0018±0.0025</td><td>0.0074±0.0028</td><td>0.0354±0.0206</td><td>0.1623±0.0050</td><td>0.1602±0.0053</td><td>0.1603±0.0087</td><td>0.0526±0.0353</td></tr><tr><td>Decision Tree</td><td>0.1547</td><td>0.1396±0.0089</td><td>0.0015±0.0035</td><td>0.0115±0.0061</td><td>0.0535±0.0209</td><td>0.1640±0.0077</td><td>0.1506±0.0070</td><td>0.1588±0.0264</td><td>0.0754±0.0641</td></tr><tr><td rowspan=\"3\">Accuracy</td><td>SVM (Linear)</td><td>0.8469</td><td>0.8281±0.0103</td><td>0.8162±0.0133</td><td>0.8226±0.0126</td><td>0.8247±0.0115</td><td>0.8363±0.0108</td><td>0.8340±0.0091</td><td>0.8356±0.0018</td><td>0.8217±0.0093</td></tr><tr><td>SVM (RBF)</td><td>0.8433</td><td>0.8278±0.0099</td><td>0.8160±0.0100</td><td>0.8215±0.0130</td><td>0.8233±0.0103</td><td>0.8342±0.0036</td><td>0.8337±0.0060</td><td>0.8349±0.0012</td><td>0.8178±0.0128</td></tr><tr><td>Decision Tree</td><td>0.8240</td><td>0.8091±0.0059</td><td>0.7926±0.0083</td><td>0.8055±0.0102</td><td>0.8077±0.0144</td><td>0.8190±0.0051</td><td>0.8199±0.0041</td><td>0.8158±0.0069</td><td>0.8044±0.0140</td></tr></table>\n\nSYN2SYN; 3) the classifiers are trained on the synthetic datasets and tested on the real dataset, called SYN2REAL. The ratio of the training set to testing set in these three settings is 1:1. We emphasize that only SYN2REAL is meaningful in practice as the classifiers are trained from the generated data and are adopted for decision making on the real data.\n\nWe adopt the following classifiers to evaluate the fair classification: 1) SVM (linear) which is a linear support vector machine with $C = 1$ ; 2) SVM (RBF) which is a support vector machine with Cthe radial basis kernel function; 3) Decision Tree with maximum tree depth as 5; Note that we do not adopt the protected attribute and only use the unprotected attributes to train classifiers, which ensures there is no disparate treatment in classifiers.\n\nFairness. We adopt the risk difference in a classifier $( d i s c ( \\eta ) =$ $P ( \\eta ( \\mathbf { x } ) = 1 | s = 1 ) - P ( \\eta ( \\mathbf { x } ) = 1 | s = 0 ) \\}$ disc η) to evaluate the performance P η s P η sof classifier on fair prediction. Table 3 shows the risk differences in classifiers on various training and testing settings. We can observe that when the classifiers are trained and tested on real datasets (i.e., REAL2REAL), the risk differences in classifiers are high. It indicates that if there is disparate impact in the training dataset, the classifiers also incur discrimination for prediction. Since SYN1- GAN is close to the real dataset, classifiers trained on SYN1-GAN also have discrimination in both SYN2SYN and SYN2REAL settings.\n\nAlthough SYN2-NFGANI and SYN3-NFGANII have similar distributions as the real dataset on unprotected attributes and decision, i.e., $P _ { G } ( \\mathbf { x } , y ) = P _ { \\mathrm { d a t a } } ( \\mathbf { x } , y )$ , classifiers which are trained and tested PG ,y P ,yin SYN2SYN settings achieve low risk differences. This is because the values of the protected attribute in SYN2-NFGANI or SYN3- NFGANII are independently generated or shuffled. Since both xˆ and $\\hat { y }$ have no correlations with the generated ˆ in SYN2-NFGANI and y sSYN3-NFGANII, the statistical parity in classifiers can be achieved when trained and tested on synthetic datasets.\n\nHowever, when classifiers are trained on SYN2-NFGANI and SYN3-NFGANII and tested on the real dataset (i.e., SYN2REAL), the classifiers still have significant discrimination against the protected group. Because the unprotected attributes of SYN2-NFGANI and SYN3-NFGANII are close to the real dataset, the correlations between the generated $\\hat { \\mathbf { x } }$ and the real  are still preserved. The sdisparate impact in terms of the real  on SYN2-NFGANI and SYN3- sNFGANII is not removed. When classifiers are tested on the real dataset where the correlations between x and  are preserved, the sclassification results indicate discrimination. On the contrary, when the classifiers are trained on SYN4-FairGAN and tested on the real dataset, we can observe that the risk differences in classifiers are small. Since the FairGAN prevents the discrimination by generating\n\nxˆ that don’t have correlations with the real , i.e. free from the dissparate impact, the classifier trained on SYN4-FairGAN can achieve fair classification on the real dataset. It demonstrates the advantage of FairGAN over the NaïveFairGAN models on fair classification.\n\nClassification accuracy. Table 3 further shows the classification accuracies of different classifiers on various training and testing settings. We can observe that the accuracies of classifiers on the SYN2REAL setting are close to the results on the REAL2REAL setting. It indicates synthetic datasets generated by different GAN models are similar to the real dataset, showing the good data generation utility of GAN models. Meanwhile, accuracies of classifiers which are trained on SYN4-FairGAN and tested on real dataset are only slightly lower than those trained on SYN1-GAN, which means the FairGAN model can achieve a good balance between utility and fairness. The small utility loss is caused by modifying unprotected attributes to remove disparate impact in terms of the real .\n\n[Section: 5.4 Parameter Sensitivity]",
      "context_after": "[Section: 6 CONCLUSIONS AND FUTURE WORK]\n\nIn this paper, we have developed FairGAN to generate fair data, which is free from disparate treatment and disparate impact in terms of the real protected attribute, while retaining high data utility. As a result, classifiers trained on the generated fair data are not subject to discrimination when making decision of the real data. FairGAN consists of one generator and two discriminators. In particular, the generator generates fake samples conditioned on the protected attribute. One discriminator is trained to identify whether samples are real or fake, while the other discriminator is trained to distinguish whether the generated samples are from the protected group or unprotected group. The generator can generate fair data with high utility by playing the adversarial games with these two discriminators. The experimental results showed the effectiveness of FairGAN. In future, we plan to improve FairGAN to make classifiers trained on synthetic datasets achieve equalized odds or equal opportunity on the real dataset besides statistical parity. We emphasize this is the first work to study the use GAN for generating fair data, which is different from pre-process approaches [9, 13, 14, 25] widely studied in fairness aware learning. In our future work, we will conduct comprehensive comparisons both theoretically and empirically in terms of fairness-utility tradeoff.\n\n[Section: ACKNOWLEDGMENTS]",
      "referring_paragraphs": [
        "Figure 6: The sensitivity analysis of FairGAN with various\n\nWe evaluate how the  in FairGAN affects the synthetic datasets λfor fair data generation and fair classification."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Utility and fairness in synthetic datasets from FairGAN with various .",
        "(b) Accuracy and fairness in a linear SVM which is trained on synthetic datasets from FairGAN with various and tested on real dataset.",
        "1805.11202_page0_fig25.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig25.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1808.08166": [
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg",
      "image_filename": "1808.08166_page0_fig0.jpg",
      "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]",
      "context_before": "Kearns et al. [2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal et al. [2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.\n\n[Section: 1 Introduction]\n\nThe most common definitions of fairness in machine learning are statistical in nature. They proceed by fixing a small number of “protected subgroups” (such as racial or gender groups), and then ask that some statistic of interest be approximately equalized across groups. Standard choices for these statistics include positive classification rates [Calders and Verwer, 2010], false positive or false negative rates [Hardt et al., 2016, Kleinberg et al., 2017, Chouldechova, 2017] and positive predictive value [Chouldechova, 2017, Kleinberg et al., 2017] — see Berk et al. [2018] for more examples. These definitions are pervasive in large part because they are easy to check, although there are interesting computational challenges in learning subject to these constraints in the worst case — see e.g. Woodworth et al. [2017].\n\nUnfortunately, these statistical definitions are not very meaningful to individuals: because they are constraints only over averages taken over large populations, they promise essentially nothing about how an individual person will be treated. Dwork et al. [2012] enumerate a “catalogue of evils” which show how definitions of this sort can fail to provide meaningful guarantees. Kearns et al. [2018] identify a particularly troubling failure of standard statistical definitions of fairness, which can arise naturally without malicious intent, called “fairness gerrymandering”. They illustrate the idea with the following toy example shown in Figure 1, described as follows.",
      "context_after": "[Section: 1.1 Further Related Work]\n\nWhile Kearns et al. [2018] propose and study rich sub-group fairness for false positive and negative constraints, H´ebert-Johnson et al. [2018] study the analogous notion for calibration constraints, which they call multi-calibration. Kim et al. [2018a] extend this style of analysis to accuracy constraints (asking that a classifier be equally accurate on a combinatorially large collection of subgroups). Kim et al. [2018b] also extend it to metric fairness constraints, converting the individual metric fairness constraint of Dwork et al. [2012] into a statistical constraint that asks that on average, individuals in (combinatorially many) subgroups should be treated differently only in proportion to the average difference between individuals in the subgroups, as measured with respect to some similarity metric.\n\n[Section: 2 Definitions]",
      "referring_paragraphs": [
        "They illustrate the idea with the following toy example shown in Figure 1, described as follows.",
        "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]\n\nSuppose individuals each have two sensitive attributes: race (say blue and green) and gender (say male and female).",
        "The heuristic finds a linear threshold function as follows:\n\nTable 1: Description of Data Sets.",
        "The properties of these datasets are summarized in Table 1, including the number of instances, the prediction being made, the overall number of features (which varies from 10 to 128), the number of protected features in the subgroup class (which varies from 3 to 18), the nature of the protected features, and the baseline (majority class) error rate."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "1808.08166_page0_fig1.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg",
      "image_filename": "1808.08166_page0_fig2.jpg",
      "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .",
      "context_before": "",
      "context_after": "[Section: 3.2 Empirical Convergence of SUBGROUP]\n\nWe begin with an examination of the convergence properties of the SUBGROUP algorithm on the four datasets. Kearns et al. [2018] had already reported preliminary convergence results for the Communities and Crime dataset, showing that their algorithm converges quickly, and that varying the input $\\gamma$ provides an appealing trade-off between error and fairness. In addition to replicating those findings for Communities and Crime, we also find that they are not an optimistic anomaly. For example, for the Law School dataset, in Figure 2 we plot both the error $\\varepsilon _ { t }$ (panel (a)) and the fairness violation $\\gamma _ { t }$ (panel (b)) as a function of the iteration $t$ , for values of the input $\\gamma$ ranging from 0 to 0.03. We see that the algorithm converges relatively quickly (on the order of thousands of iterations), and that increasing the input $\\gamma$ generally yields decreasing error and increasing fairness violation (typically saturating the input $\\gamma$ ), as suggested by the idealized theory.\n\nBut on other datasets the empirical convergence does not match the idealized theory as cleanly, presumably due to the use of imperfect Learner and Auditor heuristics. In panels (c) and (d) of Figure 2 we again\n\nplot $\\varepsilon _ { t }$ and $\\gamma _ { t }$ , but now for the Adult dataset. Even after approximately 180,000 iterations, the algorithm does not appear to have converged, with $\\varepsilon _ { t }$ still showing long-term oscillatory behavior, $\\gamma _ { t }$ exhibiting extremely noisy dynamics (especially at smaller input $\\gamma$ values), and there being no clear systematic, monotonic relationship between the input $\\gamma$ and error acheived. But despite this departure from the theory, it remains the case that varying $\\gamma$ still yields a diverse set of $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs, as we will see in the next section. In this sense, even in the absence of convergence the algorithm can be viewed as a valuable search tool for models trading off accuracy and fairness.\n\nOverall, we found rather similar convergent behavior on the Communities and Crime and Law School datasets, and less convergent behavior on the Adult and Student datasets.\n\n[Section: 3.3 Subgroup Pareto Frontiers and Comparison to Marginal Fairness]",
      "referring_paragraphs": [
        "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .",
        "For example, for the Law School dataset, in Figure 2 we plot both the error $\\varepsilon _ { t }$ (panel (a)) and the fairness violation $\\gamma _ { t }$ (panel (b)) as a function of the iteration $t$ , for values of the input $\\gamma$ ranging from 0 to 0.03.",
        "Regardless of convergence, for plots such as those in Figure 2, it is natural to take the $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ pairs across all $t$ and all input $\\gamma$ , and compute the undominated or Pareto frontier of these pairs."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b)",
        "(c)",
        "1808.08166_page0_fig3.jpg",
        "(d)",
        "1808.08166_page0_fig4.jpg",
        "(a)",
        "1808.08166_page0_fig5.jpg",
        "(b)",
        "1808.08166_page0_fig6.jpg",
        "(c)",
        "1808.08166_page0_fig7.jpg",
        "(d)",
        "1808.08166_page0_fig8.jpg",
        "(e)",
        "1808.08166_page0_fig9.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig9.jpg"
        ],
        "panel_count": 8
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_11",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
      "image_filename": "1808.08166_page0_fig10.jpg",
      "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.",
      "context_before": "",
      "context_after": "[Section: 3.4 Flattening the Discrimination Surface]\n\nRecall that in the various analyses and plots above, we rely on the Auditor of SUBGROUP to detect unfairness. This Auditor is in turn a heuristic, relying on an optimization procedure without any theoretical guarantees, which could potentially fail in practice. This means that while any detected unfairness is a lower bound on the true subgroup unfairness, it could be the case that the heuristic Auditor is simply failing to detect a larger disparity, and that the models learned by SUBGROUP look more fair than they really are.\n\nWe explore this possibility on the Communities & Crime dataset by implementing a brute force Auditor that runs alongside the SUBGROUP algorithm. To make brute force auditing computationally tractable, we designate only two attributes as protected; pctwhite and pctblack, the percentage of each community that consists of white and black people respectively. While the SUBGROUP algorithm uses the same heuristic Auditor it always does, at each round we also perform a brute force audit as follows. Subgroups $g _ { \\theta }$ are defined by a linear threshold function $\\theta$ over the 2 sensitive attributes, e.g. $( x _ { 1 } , x _ { 2 } ) \\in g _ { \\theta }$ iff $\\langle \\theta , ( x _ { 1 } , x _ { 2 } ) \\rangle \\geq 0$ . We discretize $\\theta \\in [ - 1 , 1 ] ^ { 2 }$ in increments of 0.1, and for the subgroup defined by each $\\theta$ in the discretization we compute the $\\gamma$ -unfairness. Hence at each round we can take the current classifier of the Learner, and plot for each group $g _ { \\theta }$ the point $( \\theta _ { 1 } , \\theta _ { 2 } , \\gamma )$ .\n\nNote that in addition to making brute force auditing tractable, restricting to two dimensions permits direct visualization of discrimination. In Figure 4, we show a sequence of “discrimination surfaces” for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$ . The $x \\mathrm { ~ - ~ } y$ axes are the coefficients of $\\theta$ corresponding to whitepct and blackpct respectively, and the $z$ -axis is the $\\gamma$ -unfairness of the corresponding subgroup. This is our first non-heuristic view of $\\gamma$ -unfairness, and also shows us the entire surface of $\\gamma$ -unfairness, rather than just the most violated subgroup. Note that perfect subgroup fairness would correspond to an entirely flat discrimination surface at $z = 0$ .\n\nWe observe first that the unconstrained classifier in $t \\ = \\ 1$ (panel (a)) shows a very systematic bias along the lines of our sensitive attributes. In particular groups with whitepct $> 0$ and blackpct $< 0$ , e.g. communities with large numbers of white residents and relatively fewer black residents have a much higher false positive rate for being classified as violent. Conversely, majority black communities are less likely to be incorrectly labeled as violent. The mean $\\gamma$ -unfairness (base rate - community rate) for whitepct $> 0$ blackpct $< 0$ communities is $- 0 . 0 2 4 2$ , whereas the mean for whitepct $< 0$ , blackpct > 0 groups is 0.0247. The maximum $\\gamma$ -unfairness in $t = 1$ is 0.028, and 61.25% of the 400 subgroups have $\\gamma$ -unfairness $> 0 . 0 2$ . Recall that this corresponds to e.g. a 20% disparity of the false positive rate from the base rate, for groups as large as $1 0 \\%$ of the population. We are thus far from perfect subgroup fairness.\n\nAs the algorithm proceeds, we see this discrimination flip by $t = 7$ (panel (b)), into a regime with a higher false positive rate for predominantly black communities, and then revert again by $t = 1 3$ . Over the early iterations these oscillations continue, growing less drastic as the $\\gamma$ -unfairness surface starts to flatten out noticeably by $t = 3 7$ (panel (g)). In panel (h) we plot $t = 1 3 0 1$ and see that the surface has almost completely flattened, with maximum $\\gamma$ -unfairness below .0028. So over the course of the first 1300 iterations of SUBGROUP we’ve reduced the $\\gamma$ -unfairness from over 0.02 in most of the subgroups, to less than 0.0028 in every subgroup. Recall again that this corresponds to false positive rate disparities of at most 2.8% in subgroups that represent 10% of the population — a reduction from false positive rate disparities of 20% many similarly sized subgroups. This represents an order of magnitude improvement that results from using the classifier learned by SUBGROUP.\n\n[Section: 3.5 Understanding the Dynamics]",
      "referring_paragraphs": [
        "In the left column of Figure 3, we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets, and also the pairs achieved by the MARGINAL algorithm.",
        "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(f)",
        "(g)",
        "1808.08166_page0_fig11.jpg",
        "(h)",
        "1808.08166_page0_fig12.jpg",
        "(a)",
        "1808.08166_page0_fig13.jpg",
        "(b)",
        "1808.08166_page0_fig14.jpg",
        "(c)",
        "1808.08166_page0_fig15.jpg",
        "(d)",
        "1808.08166_page0_fig16.jpg",
        "(e)",
        "1808.08166_page0_fig17.jpg"
      ],
      "quality_score": 0.7000000000000001,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig17.jpg"
        ],
        "panel_count": 8
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_19",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg",
      "image_filename": "1808.08166_page0_fig18.jpg",
      "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In Figure 4, we show a sequence of “discrimination surfaces” for the SUBGROUP algorithm over the 2 protected features, with input $\\gamma = 0$ .",
        "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12"
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(f)",
        "(g)",
        "1808.08166_page0_fig19.jpg",
        "(h)",
        "1808.08166_page0_fig20.jpg",
        "(a)",
        "1808.08166_page0_fig21.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig21.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1808.08166",
      "figure_id": "1808.08166_fig_23",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg",
      "image_filename": "1808.08166_page0_fig22.jpg",
      "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .",
      "context_before": "",
      "context_after": "[Section: 4 Conclusions]\n\nIn this work we have established the empirical efficacy of the notion of rich subgroup fairness and the algorithm of Kearns et al. [2018] on four fairness-sensitive datasets, and the necessity of explicitly enforcing subgroup (as opposed to only marginal) fairness. There are a number of interesting directions for further experimental work we plan to pursue, including:\n\n• Experiments with richer Learner model classes $\\mathcal { H }$ , while keeping the Auditor subgroup class $\\mathcal { G }$ relatively simple and fixed. One conjecture is that by making the hypothesis space richer, more appealing Pareto curves may be achieved. There is also some rationale for keeping $\\vec { \\mathcal { G } }$ simple, since we would like to have some intuitive interpretation of what the subgroups represent, while the same constraint may not hold for $\\mathcal { H }$ .   \n• Implementation and experimentation with the no-regret algorithm of Kearns et al. [2018], which may have superior convergence and other properties due to its stronger theoretical guarantees.   \n• Experiments on the generalization performance of subgroup fairness in the form of test-set Pareto curves. While as mentioned, standard VC theory can be applied to obtain worst-case bounds, one might expect even better empirical generalization.\n\n[Section: References]",
      "referring_paragraphs": [
        "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b)",
        "(c)",
        "1808.08166_page0_fig23.jpg",
        "(d)",
        "1808.08166_page0_fig24.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig24.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1809.01496": [
    {
      "doc_id": "1809.01496",
      "figure_id": "1809.01496_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg",
      "image_filename": "1809.01496_page0_fig0.jpg",
      "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.",
      "context_before": "where $\\Omega ^ { \\prime }$ is a set of predefined gender word pairs.\n\nWe use stochastic gradient descent to optimize Eq. (1). To reduce the computational complexity in training the wording embedding, we assume $v _ { g }$ is a fixed vector (i.e., we do not derive gradient w.r.t $v _ { g }$ in updating $w ^ { ( a ) } , \\forall w \\in \\Omega ^ { \\prime } )$ and estimate $v _ { g }$ only at the beginning of each epoch.\n\n[Section: 4 Experiments]\n\nIn this section, we conduct the following qualitative and quantitative studies: 1) We visualize the embedding space and show that GN-GloVe separates the protected gender attribute from other latent aspects; 2) We measure the ability of GN-GloVe to distinguish between gender-definition\n\nwords and gender-stereotype words on a newly annotated dataset; 3) We evaluate GN-GloVe on standard word embedding benchmark datasets and show that it performs well in estimating word proximity; 4) We demonstrate that GN-Glove reduces gender bias on a downstream application, coreference resolution.\n\nWe compare GN-GloVe with two embedding models, GloVe and Hard-GloVe. GloVe is a widely-used model (Pennington et al., 2014), and we apply the post-processing step introduced in (Bolukbasi et al., 2016) to reduce gender bias in GloVe and name it after Hard-GloVe. All the embeddings are trained on 2017 English Wikipedia dump with the default hyper-parameters decribed in (Pennington et al., 2014). When training GN-GloVe, we constrain the value of each dimension within $[ - 1 , 1 ]$ to avoid numerical difficulty. We set $\\lambda _ { d }$ and $\\lambda _ { e }$ both to be 0.8. In our preliminary study on development data, we observe that the model is not sensitive to these parameters. Unless other stated, we use $J _ { D } ^ { L 1 }$ in the GN-GloVe model.\n\nSeparate protected attribute First, we demonstrate that GN-GloVe preserves the gender association (either definitional or stereotypical associations) in $w ^ { ( g ) 4 }$ . To illustrate the distribution of gender information of different words, we plot Fig. 1a using $w ^ { ( g ) }$ for the $\\mathbf { X }$ -axis and a random value for the y-axis to spread out words in the plot. As shown in the figure, the gender-definition words, e.g. “waiter” and “waitress”, fall far away from each other in $w ^ { ( g ) }$ . In addition, words such as “housekeeper” and “doctor” are inclined to different genders and their $w ^ { ( g ) }$ preserves such information.\n\nNext, we demonstrate that GN-GloVe reduces gender stereotype using a list of profession titles from (Bolukbasi et al., 2016). All these profession titles are neutral to gender by definition. In Fig. 1b and Fig. 1c, we plot the cosine similarity between each word vector $w ^ { ( a ) }$ and the gender direction $v _ { g }$ (i.e., kwkkvg $\\frac { w ^ { T } v _ { g } } { \\| w \\| \\| v _ { g } \\| } )$ wT vg . Result shows that words, such as “doctor” and “nurse”, possess no gender association by definition, but their GloVe word vectors exhibit strong gender stereotype. In contrast, the gender projects of GN-GloVe word vectors $w ^ { ( a ) }$ are closer to zero. This demonstrates",
      "context_after": "[Section: 5 Conclusion and Discussion]\n\nIn this paper, we introduced an algorithm for training gender-neutral word embedding. Our method is general and can be applied in any language as long as a list of gender definitional words is provided as seed words (e.g., gender pronouns). Future directions include extending the proposed approach to model other properties of words such as sentiment and generalizing our analysis beyond binary gender.\n\n[Section: Acknowledgement]",
      "referring_paragraphs": [
        "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) $w ^ { ( g ) }$ dimension for all the professions",
        "(b) Gender-neutral profession words projected to gender direction in GloVe",
        "1809.01496_page0_fig1.jpg",
        "(c) Gender-neutral profession words projected to gender direction in GN-GloVe",
        "1809.01496_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1809.02208": [
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg",
      "image_filename": "1809.02208_page0_fig0.jpg",
      "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.",
      "context_before": "where $G$ is the universal gravitational constant. This is a trained model because the gravitational constant $G$ is determined by statistical inference over the results of a series of experiments that contain stochastic experimental error. It is also a deterministic (non-probabilistic) model because it states an exact functional relationship. I believe that Chomsky has no objection to this kind of statistical model. Rather, he seems to reserve his criticism for statistical models like Shannon’s that have quadrillions of parameters, not just one or two.\n\nChomsky and Norvig’s debate [30] is a microcosm of the two leading standpoints about the future of science in the face of increasingly sophisticated statistical models. Are we, as Chomsky seems to argue, jeopardizing science by relying on statistical tools to perform predictions instead of perfecting traditional science models, or are these tools, as Norvig argues, components of the scientific standard since its conception? Currently there are no satisfactory resolutions to this conundrum, but perhaps statistical models pose an even greater and more urgent threat to our society.\n\nOn a 2014 article, Londa Schiebinger suggested that scientific research fails to take gender issues into account, arguing that the phenomenon of male defaults on new technologies such as Google Translate provides a window into this asymmetry [35]. Since then, recent worrisome results in machine learning have somewhat supported Schiebinger’s view. Not only Google photos’ statistical image labeling algorithm has been found to classify darkskinned people as gorillas [15] and purportedly intelligent programs have been suggested to be negatively biased against black prisoners when predicting criminal behavior [1] but the machine learning revolution has also indirectly revived heated debates about the controversial field of physiognomy, with proposals of AI systems capable of identifying the sexual orientation of an individual through its facial characteristics [38]. Similar concerns are growing at an unprecedented rate in the media, with reports of Apple’s Iphone X face unlock feature failing to differentiate between two different Asian people [32] and automatic soap dispensers which reportedly do not recognize black hands [28]. Machine bias, the phenomenon by which trained statistical models unbeknownst to their creators grow to reflect controversial societal asymmetries, is growing into a pressing concern for the modern times, invites us to ask ourselves whether there are limits to our dependence on these techniques – and more importantly, whether some of these limits have already been traversed. In the wave of algorithmic bias, some have argued for the creation of some kind of agency in the likes of the Food and Drug Administration, with the sole purpose of regulating algorithmic discrimination [23].\n\nWith this in mind, we propose a quantitative analysis of the phenomenon of gender bias in machine translation. We illustrate how this can be done by simply exploiting Google Translate to map sentences from a gender neutral language into English. As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and $\\it C E O$ are translated with male ones.\n\n[Section: 2. Motivation]\n\nAs of 2018, Google Translate is one of the largest publicly available machine translation tools in existence, amounting 200 million users daily[36]. Initially relying on United Nations and European Parliament transcripts to gather data, since 2014 Google Translate has inputed content from its users through the Translate Community initiative[22]. Recently however there has been a growing concern about gender asymmetries in the translation mechanism, with some heralding it as “sexist” [31]. This concern has to at least some extent a scientific backup: A recent study has shown that word embeddings are particularly prone to yielding gender stereotypes[5]. Fortunately, the researchers propose a relatively simple debiasing",
      "context_after": "[Section: 3. Assumptions and Preliminaries]\n\nIn this paper we assume that a statistical translation tool should reflect at most the inequality existent in society – it is only logical that a translation tool will poll from examples that society produced and, as such, will inevitably retain some of that bias. It has been argued that one’s language affects one’s knowledge and cognition about the world [21], and this leads to the discussion that languages that distinguish between female and male genders grammatically may enforce a bias in the person’s perception of the world, with some studies\n\ncorroborating this, as shown in [6], as well some relating this with sexism [37] and gender inequalities [34].\n\nWith this in mind, one can argue that a move towards gender neutrality in language and communication should be striven as a means to promote improved gender equality. Thus, in languages where gender neutrality can be achieved – such as English – it would be a valid aim to create translation tools that keep the gender-neutrality of texts translated into such a language, instead of defaulting to male or female variants.\n\nWe will thus assume throughout this paper that although the distribution of translated gender pronouns may deviate from 50:50, it should not deviate to the extent of misrepresenting the demographics of job positions. That is to say we shall assume that Google Translate incorporates a negative gender bias if the frequency of male defaults overestimates the (possibly unequal) distribution of male employees per female employee in a given occupation.\n\n[Section: 4. Materials and Methods]",
      "referring_paragraphs": [
        "As Figure 1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and $\\it C E O$ are translated with male ones.",
        "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation.",
        "As Figure 1 clearly shows, the same template yields a male pronoun when “nurse” is replaced by “engineer”."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg",
      "image_filename": "1809.02208_page0_fig1.jpg",
      "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
      "context_before": "While it is possible to construct gender neutral sentences in two of the languages omitted in our experiments (namely Korean and Nepali), we have chosen to omit them for the following reasons:\n\n1. We faced technical difficulties to form templates and automatically translate sentences with the right-to-left, top-to-bottom nature of the script and, as such, we have decided not to include it in our experiments.   \n2. Due to Nepali having a rather complex grammar, with possible male/female gender demarcations on the phrases and due to none of the authors being fluent or able to reach someone fluent in the language, we were not confident enough in our ability to produce the required templates. Bengali was almost discarded under the same rationale, but we have decided to keep it because of our sentence template for Bengali has a simple grammatical structure which does not require any kind of inflection.   \n3. One can construct gender neutral phrases in Korean by omitting the gender pronoun; in fact, this is the default procedure. However, the expressiveness of this omission depends on the context of the sentence being clear, which is not possible in the way we frame phrases.\n\n[Section: 5. Distribution of translated gender pronouns per occupation category]\n\nA sensible way to group translation data is to coalesce occupations in the same category and collect statistics among languages about how prominent male defaults are in each field. What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics [29]. Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation. For instance, STEM (Science, Technology, Engineering and Mathematics) fields are grouped into a single category, which helps us compare the large asymmetry between gender pro-\n\nnouns in these fields (72% of male defaults) to that of more evenly distributed fields such as healthcare (50%).\n\nTable 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table   \n\n<table><tr><td>Category</td><td>Female (%)</td><td>Male (%)</td><td>Neutral (%)</td></tr><tr><td>Office and administrative support</td><td>11.015</td><td>58.812</td><td>16.954</td></tr><tr><td>Architecture and engineering</td><td>2.299</td><td>72.701</td><td>10.92</td></tr><tr><td>Farming, fishing, and forestry</td><td>12.179</td><td>62.179</td><td>14.744</td></tr><tr><td>Management</td><td>11.232</td><td>66.667</td><td>12.681</td></tr><tr><td>Community and social service</td><td>20.238</td><td>62.5</td><td>10.119</td></tr><tr><td>Healthcare support</td><td>25.0</td><td>43.75</td><td>17.188</td></tr><tr><td>Sales and related</td><td>8.929</td><td>62.202</td><td>16.964</td></tr><tr><td>Installation, maintenance, and repair</td><td>5.22</td><td>58.333</td><td>17.125</td></tr><tr><td>Transportation and material moving</td><td>8.81</td><td>62.976</td><td>17.5</td></tr><tr><td>Legal</td><td>11.905</td><td>72.619</td><td>10.714</td></tr><tr><td>Business and financial operations</td><td>7.065</td><td>67.935</td><td>15.58</td></tr><tr><td>Life, physical, and social science</td><td>5.882</td><td>73.284</td><td>10.049</td></tr><tr><td>Arts, design, entertainment, sports, and media</td><td>10.36</td><td>67.342</td><td>11.486</td></tr><tr><td>Education, training, and library</td><td>23.485</td><td>53.03</td><td>9.091</td></tr><tr><td>Building and grounds cleaning and maintenance</td><td>12.5</td><td>68.333</td><td>11.667</td></tr><tr><td>Personal care and service</td><td>18.939</td><td>49.747</td><td>18.434</td></tr><tr><td>Healthcare practitioners and technical</td><td>22.674</td><td>51.744</td><td>15.116</td></tr><tr><td>Production</td><td>14.331</td><td>51.199</td><td>18.245</td></tr><tr><td>Computer and mathematical</td><td>4.167</td><td>66.146</td><td>14.062</td></tr><tr><td>Construction and extraction</td><td>8.578</td><td>61.887</td><td>17.525</td></tr><tr><td>Protective service</td><td>8.631</td><td>65.179</td><td>12.5</td></tr><tr><td>Food preparation and serving related</td><td>21.078</td><td>58.333</td><td>17.647</td></tr><tr><td>Total</td><td>11.76</td><td>58.93</td><td>15.939</td></tr></table>\n\n1. Note that rows do not in general add up to 100%, as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.\n\nTable 7: Percentage of female, male and neutral gender pronouns obtained for each of the merged occupation category, averaged over all occupations in said category and tested languages detailed in Table   \n\n<table><tr><td>Category</td><td>Female (%)</td><td>Male (%)</td><td>Neutral (%)</td></tr><tr><td>Service</td><td>10.5</td><td>59.548</td><td>16.476</td></tr><tr><td>STEM</td><td>4.219</td><td>71.624</td><td>11.181</td></tr><tr><td>Farming / Fishing / Forestry</td><td>12.179</td><td>62.179</td><td>14.744</td></tr><tr><td>Corporate</td><td>9.167</td><td>66.042</td><td>14.861</td></tr><tr><td>Healthcare</td><td>23.305</td><td>49.576</td><td>15.537</td></tr><tr><td>Legal</td><td>11.905</td><td>72.619</td><td>10.714</td></tr><tr><td>Arts / Entertainment</td><td>10.36</td><td>67.342</td><td>11.486</td></tr><tr><td>Education</td><td>23.485</td><td>53.03</td><td>9.091</td></tr><tr><td>Production</td><td>14.331</td><td>51.199</td><td>18.245</td></tr><tr><td>Construction / Extraction</td><td>8.578</td><td>61.887</td><td>17.525</td></tr><tr><td>Total</td><td>11.76</td><td>58.93</td><td>15.939</td></tr></table>\n\n1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.\n\nPlotting histograms for the number of gender pronouns per occupation category sheds further light on how female, male and gender-neutral pronouns are differently distributed. The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution. Furthermore we can see both on Figures 2 and 3 how STEM fields (labeled in beige exhibit predominantly male defaults – amounting predominantly near $X = 0$ in the female histogram although much to the right in the male histogram.\n\nThese values contrast with BLS’ report of gender participation, which will be discussed in more detail in Section 8.",
      "context_after": "",
      "referring_paragraphs": [
        "We then separated the curated jobs into broader categories (Artistic, Corporate, Theatre, etc.) as shown in Table 2.",
        "<table><tr><td>Category</td><td>Group</td><td># Occupations</td><td>Female Partici-pation</td></tr><tr><td>Education, training, and library</td><td>Education</td><td>22</td><td>73.0%</td></tr><tr><td>Business and financial operations</td><td>Corporate</td><td>46</td><td>54.0%</td></tr><tr><td>Office and administra-tive support</td><td>Service</td><td>87</td><td>72.2%</td></tr><tr><td>Healthcare support</td><td>Healthcare</td><td>16</td><td>87.1%</td></tr><tr><td>Management</td><td>Corporate</td><td>46</td><td>39.8%</td></tr><tr><td>Installation, maintenance, and repair</td><td>Service</td><td>91</td><td>4.0%</td></tr><tr><td>Healthcare practitioners and technical</td><td>Healthcare</td><td>43</td><td>75.0%</td></tr><tr><td>Community and social service</td><td>Service</td><td>14</td><td>66.1%</td></tr><tr><td>Sales and related</td><td>Corporate</td><td>28</td><td>49.1%</td></tr><tr><td>Production</td><td>Production</td><td>264</td><td>28.9%</td></tr><tr><td>Architecture and engineering</td><td>STEM</td><td>29</td><td>16.2%</td></tr><tr><td>Life, physical, and so-cial science</td><td>STEM</td><td>34</td><td>47.4%</td></tr><tr><td>Transportation and material moving</td><td>Service</td><td>70</td><td>17.3%</td></tr><tr><td>Arts, design, entertainment, sports, and media</td><td>Arts / Entertainment</td><td>37</td><td>46.9%</td></tr><tr><td>Legal</td><td>Legal</td><td>7</td><td>52.8%</td></tr><tr><td>Protective Service</td><td>Service</td><td>28</td><td>22.3%</td></tr><tr><td>Food preparation and serving related</td><td>Service</td><td>17</td><td>53.8%</td></tr><tr><td>Farming, fishing, and forestry</td><td>Farming / Fishing / Forestry</td><td>13</td><td>23.4%</td></tr><tr><td>Computer and mathematical</td><td>STEM</td><td>16</td><td>25.5%</td></tr><tr><td>Personal care and service</td><td>Service</td><td>33</td><td>76.1%</td></tr><tr><td>Construction and extraction</td><td>Construction / Extraction</td><td>68</td><td>3.0%</td></tr><tr><td>Building and grounds cleaning and maintenance</td><td>Service</td><td>10</td><td>40.7%</td></tr><tr><td>Total</td><td>-</td><td>1019</td><td>41.3%</td></tr></table>\n\nTable 2: Selected occupations obtained from the U.S.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table",
        "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table 2, but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM – in which male defaults are second to most prominent.",
        "Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg",
      "image_filename": "1809.02208_page0_fig2.jpg",
      "caption": "Figure 3: In contrast to Figure   ",
      "context_before": "",
      "context_after": "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
      "referring_paragraphs": [
        "The same basic template can be ported to all other gender neutral languages, as depicted in Table 3.",
        "Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
        "The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg",
      "image_filename": "1809.02208_page0_fig3.jpg",
      "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .",
      "context_before": "7) extends to higher values.   \nFigure 3: In contrast to Figure   \n2 male pronouns are seemingly skew normally distributed, with a peak at $X = 6$ . One can see how STEM fields concentrate mainly to the right ( $X \\geq 6$ ).",
      "context_after": "",
      "referring_paragraphs": [
        "Finally, Table 4 shows thirty examples of randomly selected occupations from our dataset.",
        "<table><tr><td>Language</td><td>Occupation sentence template</td><td>Adjective sentence template</td></tr><tr><td>Malay</td><td>dia adalah &lt;occupation&gt;</td><td>dia &lt;adjective&gt;</td></tr><tr><td>Estonian</td><td>ta on &lt;occupation&gt;</td><td>ta on &lt;adjective&gt;</td></tr><tr><td>Finnish</td><td>hãn on &lt;occupation&gt;</td><td>hãn on &lt;adjective&gt;</td></tr><tr><td>Hungarian</td><td>ő egy &lt;occupation&gt;</td><td>ő &lt;adjective&gt;</td></tr><tr><td>Armenian</td><td>na &lt;occupation&gt; e</td><td>na &lt;adjective&gt; e</td></tr><tr><td>Bengali</td><td>E ekajana &lt;occupation&gt;\nYini ekajana &lt;occupation&gt;\nÖ ekajana &lt;occupation&gt;\nUni ekajana &lt;occupation&gt;\nSe ekajana &lt;occupation&gt;\nTini ekajana &lt;occupation&gt;</td><td>E &lt;adjective&gt;\nYini &lt;adjective&gt;\nÖ &lt;adjective&gt;\nUni &lt;adjective&gt;\nSe &lt;adjective&gt;\nTini &lt;adjective&gt;</td></tr><tr><td>Japanese</td><td>あの人は &lt;occupation&gt;aidu</td><td>あの人は &lt;adjective&gt;aidu</td></tr><tr><td>Turkish</td><td>o bir &lt;occupation&gt;</td><td>o &lt;adjective&gt;</td></tr><tr><td>Yoruba</td><td>o je &lt;occupation&gt;</td><td>o je &lt;adjective&gt;</td></tr><tr><td>Basque</td><td>&lt;occupation&gt;bat da</td><td>&lt;adjective&gt;da</td></tr><tr><td>Swahili</td><td>yeye ni &lt;occupation&gt;</td><td>yeye ni &lt;adjective&gt;</td></tr><tr><td>Chinese</td><td>ta shi &lt;occupation&gt;</td><td>ta hen &lt;adjective&gt;</td></tr></table>\n\nTable 4: A randomly selected example subset of thirty occupations obtained from our dataset with a total of 1019 different occupations.",
        "The histogram in Figure 2 suggests that the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4 –, while the same data for male pronouns (shown in Figure 3) suggests a skew normal distribution.",
        "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg",
      "image_filename": "1809.02208_page0_fig4.jpg",
      "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "These words are presented in Table 5.",
        "<table><tr><td>Insurance sales agent</td><td>Editor</td><td>Rancher</td></tr><tr><td>Ticket taker</td><td>Pile-driver operator</td><td>Tool maker</td></tr><tr><td>Jeweler</td><td>Judicial law clerk</td><td>Auditing clerk</td></tr><tr><td>Physician</td><td>Embalmer</td><td>Door-to-door salesperson</td></tr><tr><td>Packer</td><td>Bookkeeping clerk</td><td>Community health worker</td></tr><tr><td>Sales worker</td><td>Floor finisher</td><td>Social science technician</td></tr><tr><td>Probation officer</td><td>Paper goods machine setter</td><td>Heating installer</td></tr><tr><td>Animal breeder</td><td>Instructor</td><td>Teacher assistant</td></tr><tr><td>Statistical assistant</td><td>Shipping clerk</td><td>Trapper</td></tr><tr><td>Pharmacy aide</td><td>Sewing machine operator</td><td>Service unit operator</td></tr></table>\n\nTable 5: Curated list of 21 adjectives obtained from the top one thousand most frequent words in this category in the Corpus of Contemporary American English (COCA)   \n\n<table><tr><td>Happy</td><td>Sad</td><td>Right</td></tr><tr><td>Wrong</td><td>Afraid</td><td>Brave</td></tr><tr><td>Smart</td><td>Dumb</td><td>Proud</td></tr><tr><td>Strong</td><td>Polite</td><td>Cruel</td></tr><tr><td>Desirable</td><td>Loving</td><td>Sympathetic</td></tr><tr><td>Modest</td><td>Successful</td><td>Guilty</td></tr><tr><td>Innocent</td><td>Mature</td><td>Shy</td></tr></table>\n\nhttps://corpus.byu.edu/coca/.",
        "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).",
        "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg",
      "image_filename": "1809.02208_page0_fig5.jpg",
      "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_7",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg",
      "image_filename": "1809.02208_page0_fig6.jpg",
      "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 6 summarizes these data, and Table 7 summarizes it even further by coalescing occupation categories into broader groups to ease interpretation.",
        "The bar plots in Figure 7 help us visualize how much of the distribution of each occupation category is composed of female, male and gender-neutral pronouns.",
        "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_8",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg",
      "image_filename": "1809.02208_page0_fig7.jpg",
      "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
        "Because of this, Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue (see Table 8 for the three examples cited above."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_9",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig8.jpg",
      "image_filename": "1809.02208_page0_fig8.jpg",
      "caption": "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 9: Heatmap for the translation probability into male pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
        "<table><tr><td></td><td>Mal.</td><td>Est.</td><td>Fin.</td><td>Hun.</td><td>Arm.</td><td>Ben.</td><td>Jap.</td><td>Tur.</td><td>Yor.</td><td>Bas.</td><td>Swa.</td><td>Chi.</td><td>Total</td></tr><tr><td>Service</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>STEM</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.984</td><td>&lt;α</td><td>.07</td><td>&lt;α</td></tr><tr><td>Farming</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">.135</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">.068</td><td rowspan=\"3\">1.0</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td><td rowspan=\"3\">&lt;α</td></tr><tr><td>Fishing</td></tr><tr><td>Forestry</td></tr><tr><td>Corporate</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Healthcare</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.39</td><td>1.0</td><td>&lt;α</td><td>.088</td><td>&lt;α</td></tr><tr><td>Legal</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.145</td><td>&lt;α</td><td>&lt;α</td><td>.771</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Arts Entertainment</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.07</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Education</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.093</td><td>&lt;α</td><td>&lt;α</td><td>.5</td><td>&lt;α</td><td>.068</td><td>&lt;α</td></tr><tr><td>Production</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>.412</td><td>1.0</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr><tr><td>Construction</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">.92</td><td rowspan=\"2\">1.0</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td><td rowspan=\"2\">&lt;α</td></tr><tr><td>Extraction</td></tr><tr><td>Total</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td><td>1.0</td><td>&lt;α</td><td>&lt;α</td><td>&lt;α</td></tr></table>\n\nTable 9: Computed p-values relative to the null hypothesis that the number of translated male pronouns is not significantly greater than that of gender neutral pronouns, organized for each language and each occupation category."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_10",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg",
      "image_filename": "1809.02208_page0_fig9.jpg",
      "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.",
      "context_before": "",
      "context_after": "[Section: 6. Distribution of translated gender pronouns per language]\n\nWe have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias. Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good example of this difficulty, although the quality of Bengali, Yoruba, Chinese and Turkish translations are also compromised.\n\nTable 11: Percentage of female, male and neutral gender pronouns obtained for each language, averaged over all occupations detailed in Table   \n\n<table><tr><td>Language</td><td>Female (%)</td><td>Male (%)</td><td>Neutral (%)</td></tr><tr><td>Malay</td><td>3.827</td><td>88.420</td><td>0.000</td></tr><tr><td>Estonian</td><td>17.370</td><td>72.228</td><td>0.491</td></tr><tr><td>Finnish</td><td>34.446</td><td>56.624</td><td>0.000</td></tr><tr><td>Hungarian</td><td>34.347</td><td>58.292</td><td>0.000</td></tr><tr><td>Armenian</td><td>10.010</td><td>82.041</td><td>0.687</td></tr><tr><td>Bengali</td><td>16.765</td><td>37.782</td><td>25.63</td></tr><tr><td>Japanese</td><td>0.000</td><td>66.928</td><td>24.436</td></tr><tr><td>Turkish</td><td>2.748</td><td>62.807</td><td>18.744</td></tr><tr><td>Yoruba</td><td>1.178</td><td>48.184</td><td>38.371</td></tr><tr><td>Basque</td><td>0.393</td><td>5.496</td><td>58.587</td></tr><tr><td>Swahili</td><td>14.033</td><td>76.644</td><td>0.000</td></tr><tr><td>Chinese</td><td>5.986</td><td>51.717</td><td>24.338</td></tr></table>\n\n1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.",
      "referring_paragraphs": [
        "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_11",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg",
      "image_filename": "1809.02208_page0_fig10.jpg",
      "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.",
      "context_before": "[Section: 6. Distribution of translated gender pronouns per language]\n\nWe have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table 11. This can help us visualize the effect of different cultures in the genesis – or lack thereof – of gender bias. Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good example of this difficulty, although the quality of Bengali, Yoruba, Chinese and Turkish translations are also compromised.\n\nTable 11: Percentage of female, male and neutral gender pronouns obtained for each language, averaged over all occupations detailed in Table   \n\n<table><tr><td>Language</td><td>Female (%)</td><td>Male (%)</td><td>Neutral (%)</td></tr><tr><td>Malay</td><td>3.827</td><td>88.420</td><td>0.000</td></tr><tr><td>Estonian</td><td>17.370</td><td>72.228</td><td>0.491</td></tr><tr><td>Finnish</td><td>34.446</td><td>56.624</td><td>0.000</td></tr><tr><td>Hungarian</td><td>34.347</td><td>58.292</td><td>0.000</td></tr><tr><td>Armenian</td><td>10.010</td><td>82.041</td><td>0.687</td></tr><tr><td>Bengali</td><td>16.765</td><td>37.782</td><td>25.63</td></tr><tr><td>Japanese</td><td>0.000</td><td>66.928</td><td>24.436</td></tr><tr><td>Turkish</td><td>2.748</td><td>62.807</td><td>18.744</td></tr><tr><td>Yoruba</td><td>1.178</td><td>48.184</td><td>38.371</td></tr><tr><td>Basque</td><td>0.393</td><td>5.496</td><td>58.587</td></tr><tr><td>Swahili</td><td>14.033</td><td>76.644</td><td>0.000</td></tr><tr><td>Chinese</td><td>5.986</td><td>51.717</td><td>24.338</td></tr></table>\n\n1. Note that rows do not in general add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun.",
      "context_after": "[Section: 7. Distribution of translated gender pronouns for varied adjectives]\n\nWe queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus [https://corpus.byu.edu/coca/], but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being. The list of adjectives extracted from the corpus is available on the Github repository: https://github.com/marceloprates/Gender-Bias.\n\nApart from occupations, which we have exhaustively examined by collecting labor data from the U.S. Bureau of Labor Statistics, we have also selected a small subset of adjectives from the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, in an attempt to provide preliminary evidence that the phenomenon of gender bias may extend beyond the professional context examined in this paper. Because a large number of\n\nadjectives are not applicable to human subjects, we manually curated a reasonable subset of such words. The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3.\n\nOnce again the data points towards male defaults, but some variation can be observed throughout different adjectives. Sentences containing the words Shy, Attractive, Happy, Kind and Ashamed are predominantly female translated (Attractive is translated as female and gender-neutral in equal parts), while Arrogant, Cruel and Guilty are disproportionately translated with male pronouns (Guilty is in fact never translated with female or neutral pronouns).\n\nTable 12: Number of female, male and neutral pronominal genders in the translated sentences for each selected adjective.   \n\n<table><tr><td>Adjective</td><td>Female (%)</td><td>Male (%)</td><td>Neutral (%)</td></tr><tr><td>Happy</td><td>36.364</td><td>27.273</td><td>18.182</td></tr><tr><td>Sad</td><td>18.182</td><td>36.364</td><td>18.182</td></tr><tr><td>Right</td><td>0.000</td><td>63.636</td><td>27.273</td></tr><tr><td>Wrong</td><td>0.000</td><td>54.545</td><td>36.364</td></tr><tr><td>Afraid</td><td>9.091</td><td>54.545</td><td>0.000</td></tr><tr><td>Brave</td><td>9.091</td><td>63.636</td><td>18.182</td></tr><tr><td>Smart</td><td>18.182</td><td>45.455</td><td>18.182</td></tr><tr><td>Dumb</td><td>18.182</td><td>36.364</td><td>18.182</td></tr><tr><td>Proud</td><td>9.091</td><td>72.727</td><td>9.091</td></tr><tr><td>Strong</td><td>9.091</td><td>54.545</td><td>18.182</td></tr><tr><td>Polite</td><td>18.182</td><td>45.455</td><td>18.182</td></tr><tr><td>Cruel</td><td>9.091</td><td>63.636</td><td>18.182</td></tr><tr><td>Desirable</td><td>9.091</td><td>36.364</td><td>45.455</td></tr><tr><td>Loving</td><td>18.182</td><td>45.455</td><td>27.273</td></tr><tr><td>Sympathetic</td><td>18.182</td><td>45.455</td><td>18.182</td></tr><tr><td>Modest</td><td>18.182</td><td>45.455</td><td>27.273</td></tr><tr><td>Successful</td><td>9.091</td><td>54.545</td><td>27.273</td></tr><tr><td>Guilty</td><td>0.000</td><td>72.727</td><td>0.000</td></tr><tr><td>Innocent</td><td>9.091</td><td>54.545</td><td>9.091</td></tr><tr><td>Mature</td><td>36.364</td><td>36.364</td><td>9.091</td></tr><tr><td>Shy</td><td>36.364</td><td>27.273</td><td>27.273</td></tr><tr><td>Total</td><td>30.3</td><td>98.1</td><td>41.7</td></tr></table>",
      "referring_paragraphs": [
        "Nevertheless, the barplots in Figure 11 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages.",
        "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively.",
        "61–79 (2003)   \n[7] Bureau of Labor Statistics: ”Table 11: Employed persons by detailed occupation, sex, race, and Hispanic or Latino ethnicity, 2017”."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_12",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg",
      "image_filename": "1809.02208_page0_fig11.jpg",
      "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.",
      "context_before": "[Section: 7. Distribution of translated gender pronouns for varied adjectives]\n\nWe queried the 1000 most frequently used adjectives in English, as classified in the COCA corpus [https://corpus.byu.edu/coca/], but since not all of them were readily applicable to the sentence template we used, we filtered the N adjectives that would fit the templates and made sense for describing a human being. The list of adjectives extracted from the corpus is available on the Github repository: https://github.com/marceloprates/Gender-Bias.\n\nApart from occupations, which we have exhaustively examined by collecting labor data from the U.S. Bureau of Labor Statistics, we have also selected a small subset of adjectives from the Corpus of Contemporary American English (COCA) https://corpus.byu.edu/coca/, in an attempt to provide preliminary evidence that the phenomenon of gender bias may extend beyond the professional context examined in this paper. Because a large number of\n\nadjectives are not applicable to human subjects, we manually curated a reasonable subset of such words. The template used for adjectives is similar to that used for occupations, and is provided again for reference in Table 3.\n\nOnce again the data points towards male defaults, but some variation can be observed throughout different adjectives. Sentences containing the words Shy, Attractive, Happy, Kind and Ashamed are predominantly female translated (Attractive is translated as female and gender-neutral in equal parts), while Arrogant, Cruel and Guilty are disproportionately translated with male pronouns (Guilty is in fact never translated with female or neutral pronouns).\n\nTable 12: Number of female, male and neutral pronominal genders in the translated sentences for each selected adjective.   \n\n<table><tr><td>Adjective</td><td>Female (%)</td><td>Male (%)</td><td>Neutral (%)</td></tr><tr><td>Happy</td><td>36.364</td><td>27.273</td><td>18.182</td></tr><tr><td>Sad</td><td>18.182</td><td>36.364</td><td>18.182</td></tr><tr><td>Right</td><td>0.000</td><td>63.636</td><td>27.273</td></tr><tr><td>Wrong</td><td>0.000</td><td>54.545</td><td>36.364</td></tr><tr><td>Afraid</td><td>9.091</td><td>54.545</td><td>0.000</td></tr><tr><td>Brave</td><td>9.091</td><td>63.636</td><td>18.182</td></tr><tr><td>Smart</td><td>18.182</td><td>45.455</td><td>18.182</td></tr><tr><td>Dumb</td><td>18.182</td><td>36.364</td><td>18.182</td></tr><tr><td>Proud</td><td>9.091</td><td>72.727</td><td>9.091</td></tr><tr><td>Strong</td><td>9.091</td><td>54.545</td><td>18.182</td></tr><tr><td>Polite</td><td>18.182</td><td>45.455</td><td>18.182</td></tr><tr><td>Cruel</td><td>9.091</td><td>63.636</td><td>18.182</td></tr><tr><td>Desirable</td><td>9.091</td><td>36.364</td><td>45.455</td></tr><tr><td>Loving</td><td>18.182</td><td>45.455</td><td>27.273</td></tr><tr><td>Sympathetic</td><td>18.182</td><td>45.455</td><td>18.182</td></tr><tr><td>Modest</td><td>18.182</td><td>45.455</td><td>27.273</td></tr><tr><td>Successful</td><td>9.091</td><td>54.545</td><td>27.273</td></tr><tr><td>Guilty</td><td>0.000</td><td>72.727</td><td>0.000</td></tr><tr><td>Innocent</td><td>9.091</td><td>54.545</td><td>9.091</td></tr><tr><td>Mature</td><td>36.364</td><td>36.364</td><td>9.091</td></tr><tr><td>Shy</td><td>36.364</td><td>27.273</td><td>27.273</td></tr><tr><td>Total</td><td>30.3</td><td>98.1</td><td>41.7</td></tr></table>",
      "context_after": "[Section: 8. Comparison with women participation data across job positions]\n\nA sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those that have less than 25% of women participation[40], according to the U.S. Department of Labor Women’s Bureau). In this context, the argument in favor of a critical revision of statistic translation algorithms weakens considerably, and possibly shifts the blame away from these tools.\n\nThe U.S. Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13. The data shows us that Google Translate outputs fail to follow the real-world distribution of female workers across a comprehensive set of job positions. The\n\ndistribution of translated female pronouns is consistently inversely distributed, with female pronouns accumulating in the first 12-quantile. By contrast, BLS data shows that female participation peaks in the fourth 12-quantile and remains significant throughout the next ones.",
      "referring_paragraphs": [
        "Table 12: Number of female, male and neutral pronominal genders in the translated sentences for each selected adjective.",
        "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_13",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg",
      "image_filename": "1809.02208_page0_fig12.jpg",
      "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.",
      "context_before": "[Section: 8. Comparison with women participation data across job positions]\n\nA sensible objection to the conclusions we draw from our study is that the perceived gender bias in Google Translate results stems from the fact that possibly female participation in some job positions is itself low. We must account for the possibility that the statistics of gender pronouns in Google Translate outputs merely reflects the demographics of maledominated fields (male-dominated fields can be considered those that have less than 25% of women participation[40], according to the U.S. Department of Labor Women’s Bureau). In this context, the argument in favor of a critical revision of statistic translation algorithms weakens considerably, and possibly shifts the blame away from these tools.\n\nThe U.S. Bureau of Labor Statistics data summarized in Table 2 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13. The data shows us that Google Translate outputs fail to follow the real-world distribution of female workers across a comprehensive set of job positions. The\n\ndistribution of translated female pronouns is consistently inversely distributed, with female pronouns accumulating in the first 12-quantile. By contrast, BLS data shows that female participation peaks in the fourth 12-quantile and remains significant throughout the next ones.",
      "context_after": "[Section: 9. Discussion]\n\nAt the time of the writing up this paper, Google Translate offered only one official translation for each input word, along with a list of synonyms. In this context, all experiments reported here offer an analysis of a “screenshot” of that tool as of August 2018, the moment they were carried out. A preprint version of this paper was posted the in well-known Cornell University-based arXiv.org open repository on September 6, 2018. The manuscript soon enjoyed a significant amount of media coverage, featuring on The Register [10], Datanews [3], t3n [33], among others, and more recently on Slator [12] and Jornal do Comercio [24]. On December 6, 2018 the company’s policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation (Figure 14). According to the company, this decision is part of a broader goal of promoting fairness and reducing biases in machine learning. They also acknowledged the technical reasons behind gender bias in their model, stating that:\n\nGoogle Translate learns from hundreds of millions of already-translated examples from the web. Historically, it has provided only one translation for a query, even if the translation could have either a feminine or masculine form. So when the model produced one translation, it inadvertently replicated gender biases that already existed. For example: it would skew masculine for words like “strong” or “doctor,” and feminine for other words, like “nurse” or “beautiful.”\n\nTheir statement is very similar to the conclusions drawn on this paper, as is their motivation for redesigning the tool. As authors, we are incredibly happy to see our vision and beliefs align with those of Google in such a short timespan from the initial publishing of our work, although the company’s statement does not cite any study or report in particular and thus we cannot know for sure whether this paper had an effect on their decision or not. Regardless of whether their decision was monocratic, guided by public opinion or based on published research, we understand it as an important first step on an ongoing fight against algorithmic bias, and we praise the Google Translate team for their efforts.\n\nGoogle Translate’s new feminine and masculine forms for translated sentences exemplifies how, as this paper also suggests, machine learning translation tools can be debiased, dropping the need for resorting to a balanced training set. However, it should be noted that important as it is, GT’s new feature is still a first step. It does not address all of the shortcomings described in this paper, and the limited language coverage means that many users will still experience gender biased translation results. Furthermore, the system does not yet have support for non-binary results, which may exclude part of their user base.\n\nIn addition, one should note that further evidence is mounting about the kind of bias examined in this paper: it is becoming clear that this is a statistical phenomenon independent from any proprietary tool. In this context, the research carried out in [5] presents a very convincing argument for the sensitivity of word embeddings to gender bias in the training",
      "referring_paragraphs": [
        "We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted sideby-side in Figure 13.",
        "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1809.02208",
      "figure_id": "1809.02208_fig_14",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg",
      "image_filename": "1809.02208_page0_fig13.jpg",
      "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.",
      "context_before": "[Section: 9. Discussion]\n\nAt the time of the writing up this paper, Google Translate offered only one official translation for each input word, along with a list of synonyms. In this context, all experiments reported here offer an analysis of a “screenshot” of that tool as of August 2018, the moment they were carried out. A preprint version of this paper was posted the in well-known Cornell University-based arXiv.org open repository on September 6, 2018. The manuscript soon enjoyed a significant amount of media coverage, featuring on The Register [10], Datanews [3], t3n [33], among others, and more recently on Slator [12] and Jornal do Comercio [24]. On December 6, 2018 the company’s policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation (Figure 14). According to the company, this decision is part of a broader goal of promoting fairness and reducing biases in machine learning. They also acknowledged the technical reasons behind gender bias in their model, stating that:\n\nGoogle Translate learns from hundreds of millions of already-translated examples from the web. Historically, it has provided only one translation for a query, even if the translation could have either a feminine or masculine form. So when the model produced one translation, it inadvertently replicated gender biases that already existed. For example: it would skew masculine for words like “strong” or “doctor,” and feminine for other words, like “nurse” or “beautiful.”\n\nTheir statement is very similar to the conclusions drawn on this paper, as is their motivation for redesigning the tool. As authors, we are incredibly happy to see our vision and beliefs align with those of Google in such a short timespan from the initial publishing of our work, although the company’s statement does not cite any study or report in particular and thus we cannot know for sure whether this paper had an effect on their decision or not. Regardless of whether their decision was monocratic, guided by public opinion or based on published research, we understand it as an important first step on an ongoing fight against algorithmic bias, and we praise the Google Translate team for their efforts.\n\nGoogle Translate’s new feminine and masculine forms for translated sentences exemplifies how, as this paper also suggests, machine learning translation tools can be debiased, dropping the need for resorting to a balanced training set. However, it should be noted that important as it is, GT’s new feature is still a first step. It does not address all of the shortcomings described in this paper, and the limited language coverage means that many users will still experience gender biased translation results. Furthermore, the system does not yet have support for non-binary results, which may exclude part of their user base.\n\nIn addition, one should note that further evidence is mounting about the kind of bias examined in this paper: it is becoming clear that this is a statistical phenomenon independent from any proprietary tool. In this context, the research carried out in [5] presents a very convincing argument for the sensitivity of word embeddings to gender bias in the training",
      "context_after": "[Section: 10. Conclusions]\n\nIn this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults. Although implicit, these biases possibly stem from the real world data which is used to train them, and in this context possibly provide a window into the way our society talks (and writes) about women in the workplace. In this paper, we suggest that and test the hypothesis that statistical translation tools can be probed to yield insights about stereotypical gender roles in our society – or at least in their training data. By translating professional-related sentences such as “He/She is an engineer” from gender neutral languages such as Hungarian\n\nand Chinese into English, we were able to collect statistics about the asymmetry between female and male pronominal genders in the translation outputs. Our results show that male defaults are not only prominent, but exaggerated in fields suggested to be troubled with gender stereotypes, such as STEM (Science, Technology, Engineering and Mathematics) occupations. And because Google Translate typically uses English as a lingua franca to translate between other languages (e.g. Chinese English Portuguese) [16, 4], our findings possibly extend to translations between gender neutral languages and non-gender neutral languages (apart from English) in general, although we have not tested this hypothesis.\n\nOur results seem to suggest that this phenomenon extends beyond the scope of the workplace, with the proportion of female pronouns varying significantly according to adjectives used to describe a person. Adjectives such as Shy and Desirable are translated with a larger proportion of female pronouns, while Guilty and Cruel are almost exclusively translated with male ones. Different languages also seemingly have a significant impact in machine gender bias, with Hungarian exhibiting a better equilibrium between male and female pronouns than, for instance, Chinese. Some languages such as Yoruba and Basque were found to translate sentences with gender neutral pronouns very often, although this is the exception rather than the rule and Basque also exhibits a high frequency of phrases for which we could not automatically extract a gender pronoun.\n\nIn order to strengthen our results, we ran pronominal gender translation statistics against the U.S. Bureau of Labor Statistics data on the frequency of women participation for each job position. Although Google Translate exhibits male defaults, this phenomenon may merely reflect the unequal distribution of male and female workers in some job positions. To test this hypothesis, we compared the distribution of female workers with the frequency of female translations, finding no correlation between said variables. Our data shows that Google Translate outputs fail to reflect the real-world distribution of female workers, under-estimating the expected frequency. That is to say that even if we do not expect a 50:50 distribution of translated gender pronouns, Google Translate exhibits male defaults in a greater frequency that job occupation data alone would suggest. The prominence of male defaults in Google Translate is therefore to the best of our knowledge yet lacking a clear justification.\n\nWe think this work sheds new light on a pressing ethical difficulty arising from modern statistical machine translation, and hope that it will lead to discussions about the role of AI engineers on minimizing potential harmful effects of the current concerns about machine bias. We are optimistic that unbiased results can be obtained with relatively little effort and marginal cost to the performance of current methods, to which current debiasing algorithms in the scientific literature are a testament.\n\n[Section: 11. Acknowledgments]",
      "referring_paragraphs": [
        "On December 6, 2018 the company’s policy changed, and a statement was released detailing their efforts to reduce gender bias on Google Translate, which included a new feature presenting the user with a feminine as well as a masculine official translation (Figure 14).",
        "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1809.02208_page0_fig14.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig14.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1809.02244": [
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg",
      "image_filename": "1809.02244_page0_fig0.jpg",
      "caption": "Figure 1. (a) A simple causal DAG, with a single treatment $A$ , a single outcome $Y$ , a vector $X$ of baseline variables, and a single mediator $M$ . (b) A causal DAG corresponding to our (simplified) child welfare example with baseline factors $X$ , sensitive feature $S$ , action $A$ , vector of mediators (including e.g. socioeconomic variables, histories of drug treatment) $M$ , an indicator $Y_{1}$ of whether a child is separated from their parents, and an indicator of child hospitalization $Y_{2}$ . (d) A multistage decision problem, which corresponds to a complete DAG over vertices $X, S, M, A_{1}, Y_{1}, \\dots, A_{K}, Y_{K}$ .",
      "context_before": "As an example, $Y(a = f_A(X))$ in Fig. 1(a) is defined as $Y(a = f_A(X), M(a = f_A(X), X), X)$ , and its distribution is identified as $\\sum_{x,m} p(Y|a = f_A(x), M = m, X = x)p(M|a = f_A(x), X = x)p(X = x)$ .\n\nGiven an identified response to a fixed set of policies $f_{A}$ , we consider search for the optimal policy set $f_{A}^{*}$ , defined to be one that maximizes $\\mathbb{E}[Y(f_A)]$ . Since $Y(f_{A})$ is a counterfactual quantity, validating the found set of policies is difficult given only retrospective data, with statistical bias due to model misspecification being a particular worry. This stands in contrast with online policy learning problems in reinforcement learning, where new data under any policy may be generated and validation is therefore automatic. Partly in response to this issue, a set of orthogonal methods for policy learning have been developed that model different parts of the observed data likelihood function. Q-learning, value search, and g-estimation are common methods used in dynamic treatment regimes literature for learning optimal policies (Chakraborty & Moodie, 2013). We defer detailed descriptions to later in the paper and the supplement.\n\n[Section: 3. From fair prediction to fair policies]\n\nNabi & Shpitser (2018) argue that fair inference for prediction requires imposing hard constraints on the prediction problem, in the form of restricting certain path-specific effects. We adapt this approach to optimal sequential decision-making. A feature of this approach is that the relevant restrictions are user-specified and context-specific; thus we will generally require input from policymakers, legal experts, bioethicists, or the general public in applied settings. Which pathways may be considered impermissible depends on the domain and the semantics of the variables involved. We do not defend this perspective on fairness here for lack of space; please see Nabi & Shpitser (2018) for more details.\n\nWe summarize the proposal from Nabi & Shpitser (2018) with a brief example, inspired by the aforementioned child welfare case. Consider a simple causal model for this scenario, shown in Fig. 1(b). Hotline operators receive thousands of calls per year, and must decide on an action $A$ for each call, e.g., whether or not to send a caseworker. These decisions are made on the basis of a (high-dimensional) vectors of covariates $X$ and $M$ , as well as possibly sensitive features $S$ , such as race. $M$ consists of mediators of the effect of $S$ on $A$ . $Y_{1}$ corresponds to an indicator for whether the child is separated from their family by child protective services, and $Y_{2}$ corresponds to child hospitalization (presumably attributed to domestic abuse or neglect). The observed joint distribution generated by this causal model",
      "context_after": "[Section: 3.1. Inference in a nearby \"fair world\"]\n\nWe now describe the specifics of the proposal. We assume the data is generated according to some (known) causal model, with observed data distribution $p(\\cdot)$ , and that we can characterize the fair world by a fair distribution $p^*(\\cdot)$ where some set of pre-specified PSEs are constrained to be\n\nzero, or within a tolerance range. Without loss of generality we can assume the utility variable $Y$ is some deterministic function of $Y_{1}$ and $Y_{2}$ (i.e., $Y \\equiv u(Y_{1},Y_{2})$ ) and thus use $Y$ in place of $Y_{1}$ and $Y_{2}$ in what follows. Then $Z = (Y,X,S,M,A)$ in our child welfare example. For the purposes of illustration, assume the following two PSEs are impermissible: $\\mathrm{PSE}^{sa}$ , corresponding to the direct effect of $S$ on $A$ and defined as $\\mathbb{E}[A(s,M(s'))] - \\mathbb{E}[A(s')]$ , and $\\mathrm{PSE}^{sy}$ , corresponding to the effect of $S$ on $Y$ along the edge $S \\rightarrow Y$ , and the path $S \\rightarrow A \\rightarrow Y$ and defined as $\\mathbb{E}[Y(s,A(s',M(s')))] - \\mathbb{E}[Y(s')]$ .\n\nIf the PSEs are identified under the considered causal model, they can be written as functions of the observed distribution. For example, the unfair PSE of the sensitive feature $S$ on outcome $Y$ in our child welfare example may be written as a functional $\\mathrm{PSE}^{sy} = g_1(Z) \\equiv g_1(p(Y,X,S,M,A))$ . Similarly the unfair PSE of $S$ on $A$ is $\\mathrm{PSE}^{sa} = g_2(Z) \\equiv g_2(p(Y,X,S,M,A))$ . Generally, given a set of identified PSEs $g_j(Z) \\forall j \\in \\{1,\\dots,J\\}$ and corresponding tolerated lower/upper bounds $\\epsilon_j^-, \\epsilon_j^+$ , the fair distribution $p^*(Z)$ is defined:\n\nsubject to $\\epsilon_j^- \\leq g_j(Z) \\leq \\epsilon_j^+$ , $\\forall j \\in \\{1, \\dots, J\\}$ , (1)\n\nwhere $D_{KL}$ is the KL-divergence and $J$ is the number of constraints. In finite sample settings, Nabi & Shpitser (2018) propose solving the following constrained maximum likelihood problem:",
      "referring_paragraphs": [
        "Figure 1.",
        "Table 1."
      ],
      "figure_type": "example",
      "sub_figures": [
        "(a)",
        "(b)",
        "1809.02244_page0_fig1.jpg",
        "(c)",
        "1809.02244_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig3.jpg",
      "image_filename": "1809.02244_page0_fig3.jpg",
      "caption": "Figure 2. Group-level incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "context_before": "We generated synthetic data for a two-stage decision problem according to the causal model shown in Fig. 1(c) $(K = 2)$ , where all variables are binary except for the continuous response utility $Y \\equiv Y_{2}$ . Details on the specific models used are reported in the Supplement. We generated a dataset of size 5,000, with 100 bootstrap replications, where the sensitive variable $S$ is randomly assigned and where $S$ is chosen to be an informative covariate in estimating $Y$ .\n\nWe use estimators in Theorem 1 to compute $\\mathrm{PSE}^{sy}$ , $\\mathrm{PSE}^{sa_1}$ , and $\\mathrm{PSE}^{sa_2}$ which entail using $M$ and $S$ models. In this setting, the $\\mathrm{PSE}^{sy}$ is 1.918 (on the mean scale) and is restricted to lie between $-0.1$ and 0.1. The $\\mathrm{PSE}^{sa_1}$ is 0.718, and $\\mathrm{PSE}^{sa_2}$ is 0.921 (on the odds ratio scale) and both are restricted to lie between 0.95 and 1.05. We only constrain $M$ and $S$ models to approximate $p^*$ and fit these two models by maximizing the constrained likelihood using the R package nloptr. The parameters in all other models were estimated by maximizing the likelihood.\n\nOptimal fair polices along with optimal (unfair) policies were estimated using the two techniques described in Section 4 (where we used the \"averaging\" approach in both cases). We evaluated the performance of both techniques by comparing the population-level response under fair policies versus unfair policies. One would expect the unfair policies to lead to higher expected outcomes compared to fair policies since satisfying fairness constraints requires sacrificing some policy effectiveness. The expected outcomes under unfair polices obtained from Q-learning and value search were $7.219 \\pm 0.005$ and $7.622 \\pm 0.265$ , respectively. The values dropped to $6.104 \\pm 0.006$ and $6.272 \\pm 0.133$ under fair polices, as expected. In addition, both fair and unfair optimal polices had higher expected outcomes than the observed population-level outcome, using both methods. In our simulations, the population outcome under observed policies was $4.82 \\pm 0.007$ . Some additional results are reported in the Supplement.\n\n[Section: Application to the COMPAS dataset]\n\nCOMPAS is a criminal justice risk assessment tool created by the company Northpointe that has been used across the US to determine whether to release or detain a defendant before their trial. Each pretrial defendant receives several COMPAS scores based on factors including but not limited to demographics, criminal history, family history, and social status. Among these scores, we are primarily interested in the \"risk of recidivism.\" We use the data made available by Propublica and described in Angwin et al. (2016). The COMPAS risk score for each defendant ranges from 1 to 10, with 10 being the highest risk. In addition to this score $(A)$ , the data also includes records on defendants age $(X_{1} \\in X)$ , gender $(X_{2} \\in X)$ , race $(S)$ , prior convictions $(M)$ , and whether or not recidivism occurred in a span of two years $(R)$ . We limited our attention to the cohort consisting of African-Americans and Caucasians, and to individuals who either had not been arrested for a new offense or who had recidivated within two years. Our sample size is 5278. All variables were binarized including the COMPAS score, which we treat as an indicator of a binary decision to incarcerate versus release (pretrial) \"high risk\" individuals, i.e., we assume those with score $\\geq 7$ were incarcerated. In this data, $28.9\\%$ of individuals had scores $\\geq 7$ .\n\nSince the data does not include any variable that corresponds to utility, and there is no uncontroversial definition of what function one should optimize, we define a heuristic utility function from the data as follows. We assume there is some (social, economic, and human) cost, i.e., negative utility, associated with incarceration (deciding $A = 1$ ), and that there is some cost to releasing individuals who go on to reoffend (i.e., for whom $A = 0$ and $R = 1$ ). Also, there is positive utility associated with releasing individuals who do not go on to recidivate (i.e., for whom $A = 0$ and $R = 0$ ). A crucial feature of any realistic utility function is how to balance these relative costs, e.g., how much (if any) \"worse\" it is to release an individual who goes on to reoffend than to incarcerate them. To model these considerations we define utility $Y \\equiv (1 - A) \\times \\{-\\theta R + (1 - R)\\} - A$ . The utility function is thus parameterized by $\\theta$ , which quantifies how much \"worse\" is the case where individuals are released and reoffend as compared with the other two possibilities which are treated symmetrically. We emphasize that this utility function is a heuristic we use to illustrate our optimal policy learning method, and that a realistic utility function would be much more complicated (possibly depending also on factors not recorded in the available data).\n\nWe apply our proposed Q-learning procedure to optimize $\\mathbb{E}[Y]$ , assuming $K = 1$ and exogenous $S$ . The fair policy constrains the $S \\to A$ and $S \\to Y$ pathways. We describe details of our implementation as well as additional results in the Supplement. The proportion of individuals incarcerated",
      "context_after": "[Section: 6. Conclusion]\n\nWe have extended a formalization of algorithmic fairness from Nabi & Shpitser (2018) to the setting of learning optimal policies under fairness constraints. We show how to constrain a set of statistical models and learn a policy such that subsequent decision making given new observations from the \"unfair world\" induces high-quality outcomes while satisfying the specified fairness constraints in the induced joint distribution. In this sense, our approach can be said to \"break the cycle of injustice\" in decision-making. We investigated the performance of our proposals on synthetic and real data, where in the latter case we have supplemented the data with a heuristic utility function. In future work, we hope to develop and implement more sophisticated constrained optimization methods, to use information as efficiently as possible while satisfying the desired theoretical guarantee, and to explore nonparametric techniques for complex settings where the likelihood is not known.\n\n[Section: Acknowledgments]",
      "referring_paragraphs": [
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig4.jpg",
      "image_filename": "1809.02244_page0_fig4.jpg",
      "caption": "Figure 3. Overall incarceration rates for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "context_before": "[Section: Appendix C: Details and additional results on the COMPAS data experiment]\n\nThe regression models we used in the COMPAS data analysis were specified as follows:\n\nFor estimating the PSEs which we constrain, we used the same IPW estimators described in the main paper and reproduced in the theorem below. We constrained the PSEs to lie between $-0.05$ and 0.05 and 0.95 and 1.05, respectively.",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1809.02244",
      "figure_id": "1809.02244_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg",
      "image_filename": "1809.02244_page0_fig5.jpg",
      "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .",
      "context_before": "",
      "context_after": "[Section: Appendix D: Proofs]\n\nTheorem 1 Assume $S$ is binary. Under the causal model above, the following are consistent estimators of $PSE^{sy}$ and $PSE^{sak_k}$ , assuming all models are correctly specified:\n\nProof: The latent projection (Verma and Pearl, 1990) of any $K$ stage DAG onto $X, S, M, A, Y$ suffices to identify and estimate the two path-specific effects in question, and this latent projection is the complete DAG with topological ordering $X, S, M, A, Y$ . The consistency of the estimators above then follows directly from derivations in (Tchetgen Tchetgen and Shpitser, 2014). As an example, we have the following derivation for the first term of $g^{sy}(Z)$ :",
      "referring_paragraphs": [
        "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    }
  ],
  "1809.04737": [
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg",
      "image_filename": "1809.04737_page0_fig0.jpg",
      "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .",
      "context_before": "The fairness-aware classification aims to find a classifier that minimizes the empirical loss while satisfying certain fairness constraints. Several fairness notions or definitions are proposed in the literature, such as demographic parity (Pedreshi, Ruggieri, and Turini 2008), mistreatment parity (Zafar et al. 2017a), calibration (Pleiss et al. 2017), etc., and research shows that different fairness notions are generally incomparable with each other and cannot be satisfied simultaneously (Kleinberg, Mullainathan, and Raghavan 2016). Our framework is not limited to a specific fairness notion. In this paper, we present our framework based on the demographic parity. In the appendix, we show how the framework can be easily generalized to other fairness notions.\n\nDemographic parity is the most widely-used fairness notion in the fairness-aware learning field. It requires the decision made by the classifier is independent to certain sensitive attribute, such as sex or race. We denote the sensitive attribute by $S$ , assuming that it is associated with two values: sensitive group $s ^ { - }$ and non-sensitive group $s ^ { + }$ . Usually, demographic parity is quantified with regard to risk difference, i.e., the difference of the positive predictions between the sensitive group and non-sensitive group. For example, in hiring, risk difference can be given by the probability difference of being classified as hired between male applicants and female applicants. Using the same language as that in the previous subsection, the risk difference produced by a classifier $f$ is expressed as\n\nAs a quantitative metric, we say that classifier $f$ is considered as fair if $| \\mathbb { R } \\mathbb { D } ( f ) | \\le \\tau$ , where $\\tau$ is the user-defined threshold. For instance, the 1975 British legislation for sex discrimination sets $\\tau = 0 . 0 5$ .\n\nBy directly incorporating the risk difference into the optimization problem, we obtain\n\nsubject to $\\mathbb { R D } ( f ) \\leq \\tau , \\quad - \\mathbb { R D } ( f ) \\leq \\tau .$\n\nObviously, the above optimization problem is non-convex. Similar to the loss function, we adopt surrogate functions to convert the risk difference to convex constraints. By using predictive function $h$ and the indicator function, we can rewrite the risk difference as",
      "context_after": "For simplicity, we may want to denote $P ( S = s ^ { + } | \\mathbf { x } )$ by $\\eta ( \\mathbf { x } )$ and $P ( \\bar { S } = s ^ { + } )$ by $p$ . Similarly, the indicator function in above equation can be replaced with the surrogate function. The issue here is, two constraints $\\mathbb { R D } ( f ) \\leq \\tau$ and $- \\mathbb { R } \\mathbb { D } ( f ) \\leq \\tau$ are opposite to each other. Thus, replacing all indicator functions with a single surrogate function will result in a convex-concave problem, where only heuristic solutions for finding local optima are known to exist. Therefore, we adopt two surrogate functions, a convex one $\\kappa ( \\cdot )$ and a concave one $\\delta ( \\cdot )$ , each of which replaces the indicator function for one constraint. As a result, the formulated constrained optimization problem is convex and can be efficiently solved. We call the risk difference represented by $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ as the $\\kappa , \\delta$ -risk difference, denoted by $\\mathbb { R D } _ { \\kappa } ( h )$ and $\\mathbb { R D } _ { \\delta } ( h )$ . Almost all commonly-used surrogate functions can be adopted for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ , by performing some shift or flip. Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.\n\nTable 1: An example of admitting students.   \n\n<table><tr><td rowspan=\"2\">Sex</td><td colspan=\"2\">GPA</td></tr><tr><td>High</td><td>Low</td></tr><tr><td>Male</td><td>51</td><td>49</td></tr><tr><td>Female</td><td>48</td><td>52</td></tr></table>\n\nTo sum up, we obtain the following convex optimization formulation for learning fair classifiers.\n\nProblem Formulation 1. The goal of the fairness-aware classification is to find a classifier $f$ which minimizes the empirical loss $\\mathbb { L } ( f )$ while satisfying fairness constraint $| \\mathbb { R } \\mathbb { D } ( f ) | \\leq \\tau$ . It can be approached by solving the following constrained optimization problem\n\nsubject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq c _ { 1 } , \\quad - \\mathbb { R } \\mathbb { D } _ { \\delta } ( h ) \\leq c _ { 2 } ,$\n\nwhere $f = s i g n ( h )$ , $\\kappa ( \\cdot )$ is a convex surrogate function, $\\delta ( \\cdot )$ is a concave surrogate function, $\\eta ( \\mathbf { x } ) ~ = ~ P ( S ~ = ~ s ^ { + } | \\mathbf { x } ) ,$ ,\n\n$p = P ( S = s ^ { + } )$ , $c _ { 1 } , c _ { 2 }$ are the thresholds of the $\\kappa , \\delta$ -risk difference, and\n\nNext, we will present two important results within the framework, namely a constraint-free criterion on the training data that ensures fairness for any classifier learned from it, as well as the lower and upper bounds of the risk difference using the $\\kappa , \\delta$ -risk difference.",
      "referring_paragraphs": [
        "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .\n\nIt follows that",
        "Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ are shown in Figure 1.",
        "The statistics of the dataset is shown in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg",
      "image_filename": "1809.04737_page0_fig1.jpg",
      "caption": "Figure 2: Two classifiers and their predictions.",
      "context_before": "We similarly define $H ^ { + } ( \\eta )$ the maximal conditional risk difference, $H _ { \\delta } ^ { + } ( \\eta )$ the maximal conditional $\\delta$ -risk difference, $\\mathbb { R } \\mathbb { D } _ { \\delta } ^ { + }$ the maximal $\\delta$ -risk difference, as well as $H _ { \\delta } ^ { \\circ } ( \\eta )$ the minimal conditional $\\delta$ -risk difference within interval $\\alpha$ s.t. $\\alpha ( \\eta _ { S } - p ) \\ge 0$ .\n\nNow, we are able to present our results, which are given in Theorem 3 and Corollary 4. The proof is skipped here and can be found in the appendix.\n\nTheorem 3. If $\\kappa ( \\cdot )$ is convex and differentiable at zero with $\\kappa ^ { \\prime } ( 0 ) > 0 , \\delta ( \\cdot )$ is concave and differentiable at zero with $\\delta ^ { \\prime } ( 0 ) > 0$ , then for any predictive function $h$ , we have\n\nCorollary 4. For any predictive function $h _ { ; }$ , let classifier $f = s i g n ( h )$ , if $\\kappa ( \\cdot )$ is convex and differentiable at zero with $\\kappa ^ { \\prime } { ( 0 ) } > 0$ , δ(·) is concave and differentiable at zero with $\\delta ^ { \\prime } ( 0 ) > 0$ , and $H _ { \\kappa } ^ { - } ( \\eta ) \\ < \\ H _ { \\kappa } ^ { \\circ } ( \\eta )$ and $H _ { \\delta } ^ { + } ( \\eta ) ~ > ~ H _ { \\delta } ^ { \\circ } ( \\eta )$ hold for all $\\eta \\in [ 0 , 1 ] , \\eta \\neq p$ , then risk difference $\\mathbb { R D } ( f )$ is bounded by following inequalities1:\n\nBased on the upper and lower bounds of $\\mathbb { R D } ( f )$ , we modify Problem Formulation 1 to obtain Problem Formulation 2 with refined fairness constraints which guarantee the real fairness requirement.",
      "context_after": "subject to $\\mathbb { R } \\mathbb { D } _ { \\kappa } ( h ) \\leq \\psi _ { \\kappa } \\big ( c _ { 1 } - \\mathbb { R } \\mathbb { D } ^ { - } \\big ) + \\mathbb { R } \\mathbb { D } _ { \\kappa } ^ { - } ,$\n\nNote that the RHS of above two inequalities are constants for a given dataset. Therefore, the constrained optimization problem is still convex. As stated, we can adopt almost any type of surrogate function for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ . Some commonlyused surrogate functions are listed in Tables 2. Their corresponding $\\psi _ { \\kappa } ( \\cdot )$ and $\\psi _ { \\delta } ( \\cdot )$ are derived and shown in the last column where the derivation details are skipped.\n\n[Section: Experiments]",
      "referring_paragraphs": [
        "Figure 2: Two classifiers and their predictions.\n\nProblem Formulation 2. A fair classifier $f = s i g n ( h )$ that achieves fairness constraint $- c _ { 2 } \\leq \\mathbb { R D } ( f ) \\leq c _ { 1 }$ can be obtained by solving the following constrained optimization",
        "For our\n\nTable 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa } ( \\mu )$ and $\\psi _ { \\delta } ( \\mu )$   \n\n<table><tr><td>Name of κ-δ</td><td>κ(α) for α ∈ R</td><td>δ(α) for α ∈ R</td><td>ψκ(μ) or ψδ(μ) for μ ∈ (0,1/p]</td></tr><tr><td>Hinge</td><td>max{α+1,0}</td><td>min{α,1}</td><td>μ</td></tr><tr><td>Square</td><td>(α+1)2</td><td>1-(1-α)2</td><td>μ2</td></tr><tr><td>Exponential</td><td>exp(α)</td><td>1-exp(-α)</td><td>(√(1-p)μ+1-√1-pμ)2</td></tr></table>"
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) A classifier that meets the κ, δ-risk difference constraint makes unfair predictions.",
        "(b) A classifier that doesn’t meet the $\\kappa , \\delta$ -risk difference constraint makes fair predictions.",
        "1809.04737_page0_fig2.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1809.04737",
      "figure_id": "1809.04737_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg",
      "image_filename": "1809.04737_page0_fig3.jpg",
      "caption": "Figure 3: Comparison of fair classifiers.",
      "context_before": "To demonstrate the sufficiency criterion of learning fair classifiers, we build the maximal/minimal risk difference classifiers $f _ { \\mathrm { m i n } }$ , $f _ { \\mathrm { m a x } }$ for both Adult and Dutch datasets, and measure the risk differences they produce, i.e., $\\mathbb { R D } ^ { - } , \\mathbb { R D } ^ { + }$ . The results are shown in the first two rows in Table 3. As can be seen, in both datasets we have large maximal and minimal risk differences. In order to evaluate a situation with small a risk difference, we also create a variant of Adult, referred to as Adult*, where all attributes are binarized and the sensitive attribute sex is shuffled to incur a small risk difference. Then, we build a number of classifiers including Linear Regression (LR), Support Vector Machine (SVM) with linear kernel, Decision Tree (DT), and Naive Bayes (NB), using the three datasets as the training data with with 5-fold crossvalidation. After that, their risk differences are quantified on the testing data, as shown in the last four rows in Table 3. We can see that all values are within $\\mathbb { R D } ^ { - }$ , $\\mathbb { R } \\mathbb { D } ^ { + }$ which are consistent with our criterion.\n\n[Section: Learning Fair Classifiers]\n\nWe build our fair classifiers on both Adult and Dutch datasets by solving the optimization problem defined in Problem Formulation 2. For surrogate functions, we use the logistic function for $\\phi ( \\cdot )$ , and the hinge function for $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ . We also compare our methods with Zafar-1 and Zafar-2. The results are shown in Figure 3, which depict the relationship between the obtained risk difference and empirical loss. For our\n\nTable 2: Some common surrogate functions for κ-δ and the corresponding $\\psi _ { \\kappa } ( \\mu )$ and $\\psi _ { \\delta } ( \\mu )$   \n\n<table><tr><td>Name of κ-δ</td><td>κ(α) for α ∈ R</td><td>δ(α) for α ∈ R</td><td>ψκ(μ) or ψδ(μ) for μ ∈ (0,1/p]</td></tr><tr><td>Hinge</td><td>max{α+1,0}</td><td>min{α,1}</td><td>μ</td></tr><tr><td>Square</td><td>(α+1)2</td><td>1-(1-α)2</td><td>μ2</td></tr><tr><td>Exponential</td><td>exp(α)</td><td>1-exp(-α)</td><td>(√(1-p)μ+1-√1-pμ)2</td></tr></table>",
      "context_after": "[Section: Related Work]\n\nMany methods have been proposed for constructing fairnessaware classifiers, which can be broadly classified into\n\npre/post-processing and in-processing methods. The pre/postprocessing methods propose to modify the training data and/or tweak the predictions to obtain fair predictions. Data mining techniques have been proposed to remove bias from a dataset since 2008 (Pedreshi, Ruggieri, and Turini 2008). After that, a number of techniques have been proposed either based on correlations between the sensitive attribute and the decision (Dwork et al. 2012; Feldman et al. 2015; Wu and Wu 2016; Zliobaite, Kamiran, and Calders 2011) or the causal relationship among all attributes (Kilbertus et al. 2017; Zhang and Bareinboim 2018; Zhang and Wu 2017; Zhang, Wu, and Wu 2017b). In (Hardt et al. 2016), the authors proposed to tweak the output of the classifier after the classifier makes predictions. As suggested by a recent work (Zhang, Wu, and Wu 2018), both the pre-processing and post-processing phases are necessary in achieving fair predictions. Another category of methods are in-processing methods which adjust the learning process of the classifier (Kamishima, Akaho, and Sakuma 2011; Agarwal et al. 2017; Menon and Williamson 2018). In recent years, a number of methods are proposed to incorporate fairness as constraints in the optimization, e.g., (Kamishima, Akaho, and Sakuma 2011; Goh et al. 2016; Krishna and Williamson 2018; Zafar et al. 2017a; Zafar et al. 2017b; Woodworth et al. 2017; Olfat and Aswani 2018). As discussed in the paper, there lacks a theoretical framework for guiding the formulation of the constrained optimization problem. This paper proposes a general framework for fairness-aware classification.\n\n[Section: Conclusions]",
      "referring_paragraphs": [
        "The results are shown in the first two rows in Table 3.",
        "The results are shown in Figure 3, which depict the relationship between the obtained risk difference and empirical loss.",
        "Figure 3: Comparison of fair classifiers."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Adult",
        "(b) Dutch",
        "1809.04737_page0_fig4.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1809.10083": [
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg",
      "image_filename": "1809.10083_page0_fig0.jpg",
      "caption": "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design",
      "context_before": "In this section, we describe a generalized framework for unsupervised induction of invariance to nuisance factors by disentangling information required for predicting $y$ from other unrelated information contained in $x$ through the incorporation of data reconstruction as a competing task for the primary prediction task and a disentanglement term in the training objective. This is achieved by learning a split representation of data as $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , such that information essential for the prediction task is pulled into $e _ { 1 }$ while all other information about $x$ migrates to $e _ { 2 }$ . We present an adversarial instantiation of this framework, which we call Unsupervised Adversarial Invariance.\n\n[Section: 3.1 Unsupervised Invariance Induction]\n\nData samples $( x )$ can be abstractly represented as a set of underlying factors of variation $F = \\{ f _ { i } \\}$ . This can be as simple as a collection of numbers denoting the position of a point in space or as complicated as information pertaining to various facial attributes that combine non-trivially to form the image of someone’s face. Understanding and modeling the interactions between factors of variation of data is an open problem. However, supervised learning of the mapping of $x$ to target $( y )$ involves a relatively simpler (yet challenging) problem of finding those factors of variation $( F _ { y } )$ that contain all the information required for predicting $y$ and discarding all the others $( \\overline { { F } } _ { y } )$ . Thus, $F _ { y }$ and $\\overline { { F } } _ { y }$ form a partition of $F$ , where we are more interested in the former than the latter. Since $y$ is independent of $\\overline { { F } } _ { y }$ , i.e., $\\boldsymbol { y } \\perp \\overline { { \\boldsymbol { F } } } _ { y }$ , we get $p ( y | x ) = p ( y | F _ { y } )$ . Estimating $p ( y | x )$ as $q ( y | F _ { y } )$ from data is beneficial because the nuisance factors, which comprise $\\overline { { F } } _ { y }$ , are never presented to the estimator, thus avoiding inaccurate learning of associations between nuisance factors and $y$ . This forms the basis for “feature selection”, a research area that has been well-studied.\n\nWe incorporate the idea of splitting $F$ into $F _ { y }$ and $\\overline { { F } } _ { y }$ in our framework in a more relaxed sense as learning a disentangled latent representation of $x$ in the form of $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , where $e _ { 1 }$ aims to capture all the information in $F _ { y }$ and $e _ { 2 }$ that in $\\overline { { F } } _ { y }$ . Once trained, the model can be used to infer $e _ { 1 }$ from $x$ followed by $y$ from $e _ { 1 }$ . More formally, our general framework for unsupervised invariance induction comprises four core modules: (1) an encoder $E n c$ that embeds $x$ into $\\boldsymbol { e } = \\left[ e _ { 1 } e _ { 2 } \\right]$ , (2) a predictor P red that infers $y$ from $e _ { 1 }$ , (3) a noisy-transformer $\\psi$ that converts $e _ { 1 }$ into its noisy version $\\tilde { e } _ { 1 }$ , and",
      "context_after": "The predictor and the decoder are designed to enter into a competition, where P red tries to pull information relevant to $y$ into $e _ { 1 }$ while $D e c$ tries to extract all the information about $x$ into $e _ { 2 }$ . This is made possible by $\\psi$ , which makes $\\tilde { e } _ { 1 }$ an unreliable source of information for reconstructing $x$ . Moreover, a version of this framework without $\\psi$ can converge to a degenerate solution where $e _ { 1 }$ contains all the information about $x$ and $e _ { 2 }$ contains nothing (noise), because absence of $\\psi$ allows $e _ { 1 }$ to be readily available to Dec. The competitive pulling of information into $e _ { 1 }$ and $e _ { 2 }$ induces information separation such that $e _ { 1 }$ tends to contain more information relevant for predicting $y$ and $e _ { 2 }$ more information irrelevant to the prediction task. However, this competition is not sufficient to completely partition information of $x$ into $e _ { 1 }$ and $e _ { 2 }$ . Without the disentanglement term $( L _ { d i s } )$ in the objective, $e _ { 1 }$ and $e _ { 2 }$ can contain redundant information such that $e _ { 2 }$ has information relevant to $y$ and, more importantly, $e _ { 1 }$ contains nuisance factors. The disentanglement term in the training objective encourages the desired clean partition. Thus, essential factors required for predicting $y$ concentrate into $e _ { 1 }$ and all other factors migrate to $e _ { 2 }$ .\n\n[Section: 3.2 Adversarial Model Design and Optimization]\n\nWhile there are numerous ways to implement the proposed unsupervised invariance induction framework, we adopt an adversarial model design, introducing a novel approach to disentanglement in the process. Enc, P red and Dec are modeled as neural networks. $\\psi$ can be modeled as a parametric noisy-channel, where the parameters of $\\psi$ can also be learned during training. However, we model $\\psi$ as dropout [18] (multiplicative Bernoulli noise) because it provides an easy and straightforward method for noisy-transformation of $e _ { 1 }$ into $\\tilde { e } _ { 1 }$ without complicating the training process.\n\nWe augment these core modules with two adversarial disentanglers $D i s _ { 1 }$ and $D i s _ { 2 }$ . While $D i s _ { 1 }$ aims to predict $e _ { 2 }$ from $e _ { 1 }$ , $D i s _ { 2 }$ aims to do the inverse. Hence, their objectives are in direct opposition to the desired disentanglement, forming the basis for adversarial minimax optimization. Thus, Enc, P red and Dec can be thought of as a composite model $( M _ { 1 } )$ that is pit against another composite model $( M _ { 2 } )$ containing $D i s _ { 1 }$ and $D i s _ { 2 }$ . Figure 1b shows our complete model design with $M _ { 1 }$ represented by the color blue and $M _ { 2 }$ with orange. The model is trained end-to-end through backpropagation by playing the minimax game described in Equation 2.",
      "referring_paragraphs": [
        "Figure 1: (a) Unsupervised Invariance Induction Framework and (b) Adversarial Model Design\n\n(4) a decoder Dec that reconstructs $x$ from $\\tilde { e } _ { 1 }$ and $e _ { 2 }$ . Additionally, the training objective contains a loss-term that enforces disentanglement between $E n c ( x ) _ { 1 } = e _ { 1 }$ and $E n c ( x ) _ { 2 } \\stackrel { - } { = } e _ { 2 }$ . Figure 1a shows our generalized framework. The training objective for this system can be written as Equation 1.",
        "We optimize the proposed adversarial model using a scheduled update scheme where we freeze the weights of a composite\n\nTable 1: Results on Extended Yale-B dataset   \n\n<table><tr><td>Metric</td><td>NN + MMD [13]</td><td>VFAE [14]</td><td>CAI [19]</td><td>Ours</td></tr><tr><td>Accuracy of predicting y from e1 (Ay)</td><td>0.82</td><td>0.85</td><td>0.89</td><td>0.95</td></tr><tr><td>Accuracy of predicting z from e1 (Az)</td><td>-</td><td>0.57</td><td>0.57</td><td>0.24</td></tr></table>\n\nplayer model ( $M _ { 1 }$ or $M _ { 2 }$ ) when we update the weights of the other.",
        "Table 1 summarizes the results."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1809.10083_page0_fig1.jpg",
        "(a)",
        "1809.10083_page0_fig2.jpg",
        "(b)",
        "1809.10083_page0_fig3.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig3.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg",
      "image_filename": "1809.10083_page0_fig4.jpg",
      "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
        "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   \n\n<table><tr><td>Metric</td><td>CAI</td><td>Ours</td></tr><tr><td>Ay</td><td>0.68</td><td>0.74</td></tr><tr><td>Az</td><td>0.69</td><td>0.34</td></tr></table>",
        "Table 2 summarizes the results, showing that our model outperforms CAI on both $A _ { y }$ and $A _ { z }$ ."
      ],
      "figure_type": "example",
      "sub_figures": [
        "(c)",
        "(d)",
        "1809.10083_page0_fig5.jpg",
        "1809.10083_page0_fig6.jpg",
        "1809.10083_page0_fig7.jpg",
        "1809.10083_page0_fig8.jpg",
        "1809.10083_page0_fig9.jpg",
        "1809.10083_page0_fig10.jpg",
        "1809.10083_page0_fig11.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig11.jpg"
        ],
        "panel_count": 8
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_13",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg",
      "image_filename": "1809.10083_page0_fig12.jpg",
      "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs.",
        "A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic\n\nTable 3: Results on MNIST-ROT.",
        "Table 3 summarizes the results, showing that our unsupervised adversarial model not only performs better than the baseline ablation versions but also outperforms CAI, which uses supervised information about the rotation angle."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1809.10083_page0_fig13.jpg",
        "(b)",
        "1809.10083_page0_fig14.jpg"
      ],
      "quality_score": 0.6,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig14.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_16",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg",
      "image_filename": "1809.10083_page0_fig15.jpg",
      "caption": "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).",
        "Figure 2 shows t-SNE [15] visualization of raw data and embeddings $e _ { 1 }$ and $e _ { 2 }$ for our model.",
        "Table 2: Results on Chairs. High $A _ { y }$ and low $A _ { z }$ are desired.   \n\n<table><tr><td>Metric</td><td>CAI</td><td>Ours</td></tr><tr><td>Ay</td><td>0.68</td><td>0.74</td></tr><tr><td>Az</td><td>0.69</td><td>0.34</td></tr></table>",
        "Table 2 summarizes the results, showing that our model outperforms CAI on both $A _ { y }$ and $A _ { z }$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1809.10083_page0_fig16.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig16.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_18",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg",
      "image_filename": "1809.10083_page0_fig17.jpg",
      "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   \n(a)",
        "<table><tr><td>Metric</td><td>Angle</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td rowspan=\"3\">Ay</td><td>Θ</td><td>0.958</td><td>0.977</td><td>0.974</td><td>0.972</td></tr><tr><td>±55°</td><td>0.826</td><td>0.856</td><td>0.826</td><td>0.829</td></tr><tr><td>±65°</td><td>0.662</td><td>0.696</td><td>0.674</td><td>0.682</td></tr><tr><td>Az</td><td>-</td><td>0.384</td><td>0.338</td><td>0.586</td><td>0.409</td></tr></table>\n\nTable 4: MNIST-DIL – Accuracy of predicting y $( A _ { y } )$ .",
        "Figure 4 shows t-SNE visualization of raw MNIST-ROT images and $e _ { 1 }$ learned by our model."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_19",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg",
      "image_filename": "1809.10083_page0_fig18.jpg",
      "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .",
      "context_before": "",
      "context_after": "[Section: 5.2 Effective use of synthetic data augmentation for learning invariance]\n\nData is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic\n\nTable 3: Results on MNIST-ROT. $\\Theta \\ : = \\ :$ $\\{ 0 , \\pm 2 2 . 5 ^ { \\circ } , \\pm 4 5 ^ { \\circ } \\}$ was used for training. High $A _ { y }$ and low $A _ { z }$ are desired.   \n\n<table><tr><td>Metric</td><td>Angle</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td rowspan=\"3\">Ay</td><td>Θ</td><td>0.958</td><td>0.977</td><td>0.974</td><td>0.972</td></tr><tr><td>±55°</td><td>0.826</td><td>0.856</td><td>0.826</td><td>0.829</td></tr><tr><td>±65°</td><td>0.662</td><td>0.696</td><td>0.674</td><td>0.682</td></tr><tr><td>Az</td><td>-</td><td>0.384</td><td>0.338</td><td>0.586</td><td>0.409</td></tr></table>\n\nTable 4: MNIST-DIL – Accuracy of predicting y $( A _ { y } )$ . $k = - 2$ represents erosion with kernel-size of 2.   \n\n<table><tr><td>k</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td>-2</td><td>0.816</td><td>0.880</td><td>0.872</td><td>0.870</td></tr><tr><td>2</td><td>0.933</td><td>0.958</td><td>0.942</td><td>0.940</td></tr><tr><td>3</td><td>0.795</td><td>0.874</td><td>0.847</td><td>0.853</td></tr><tr><td>4</td><td>0.519</td><td>0.606</td><td>0.534</td><td>0.550</td></tr></table>",
      "referring_paragraphs": [
        "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d).",
        "Figure 5 shows the results.",
        "Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods VFAE and Domain Adversarial Neural Network (DANN) [6]."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b)",
        "(c)",
        "1809.10083_page0_fig19.jpg",
        "1809.10083_page0_fig20.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig20.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1809.10083",
      "figure_id": "1809.10083_fig_22",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg",
      "image_filename": "1809.10083_page0_fig21.jpg",
      "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .",
      "context_before": "[Section: 5.2 Effective use of synthetic data augmentation for learning invariance]\n\nData is often not available for all possible variations of nuisance factors. A popular approach to learn models robust to such expected yet unobserved or infrequently seen (during training) variations is data augmentation through synthetic generation using methods ranging from simple operations [10] like rotation and translation to Generative Adversarial Networks [1, 8] for synthesis of more realistic\n\nTable 3: Results on MNIST-ROT. $\\Theta \\ : = \\ :$ $\\{ 0 , \\pm 2 2 . 5 ^ { \\circ } , \\pm 4 5 ^ { \\circ } \\}$ was used for training. High $A _ { y }$ and low $A _ { z }$ are desired.   \n\n<table><tr><td>Metric</td><td>Angle</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td rowspan=\"3\">Ay</td><td>Θ</td><td>0.958</td><td>0.977</td><td>0.974</td><td>0.972</td></tr><tr><td>±55°</td><td>0.826</td><td>0.856</td><td>0.826</td><td>0.829</td></tr><tr><td>±65°</td><td>0.662</td><td>0.696</td><td>0.674</td><td>0.682</td></tr><tr><td>Az</td><td>-</td><td>0.384</td><td>0.338</td><td>0.586</td><td>0.409</td></tr></table>\n\nTable 4: MNIST-DIL – Accuracy of predicting y $( A _ { y } )$ . $k = - 2$ represents erosion with kernel-size of 2.   \n\n<table><tr><td>k</td><td>CAI</td><td>Ours</td><td>B0</td><td>B1</td></tr><tr><td>-2</td><td>0.816</td><td>0.880</td><td>0.872</td><td>0.870</td></tr><tr><td>2</td><td>0.933</td><td>0.958</td><td>0.942</td><td>0.940</td></tr><tr><td>3</td><td>0.795</td><td>0.874</td><td>0.847</td><td>0.853</td></tr><tr><td>4</td><td>0.519</td><td>0.606</td><td>0.534</td><td>0.550</td></tr></table>",
      "context_after": "[Section: 5.3 Domain Adaptation]\n\nDomain adaptation has been treated as an invariance induction task in recent literature [6, 14] where the goal is to make the prediction task invariant to the “domain” information. We evaluate the performance of our model at domain adaptation on the Amazon Reviews dataset [4] using the same preprocessing as [14]. The dataset contains text reviews on products in four domains – “books”, “dvd”, “electronics”, and “kitchen”. Each review is represented as a feature vector of unigram and bigram counts. The target $y$ is the sentiment of the review – either positive or negative. We use the same experimental setup as [6, 14] where the model is trained on one domain and tested on another, thus creating 12 source-target combinations. We design the architectures of the encoder and the decoder in our model to be similar to those of VFAE, as presented in [14]. Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods VFAE and Domain Adversarial Neural Network (DANN) [6]. The results of the prior works are quoted directly from [14]. The results show that our model outperforms both VFAE and DANN at nine out of the twelve tasks. Thus, our model can also be used effectively for domain adaptation.\n\n[Section: 6 Conclusion And Future Work]",
      "referring_paragraphs": [
        "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1810.01943": [
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg",
      "image_filename": "1810.01943_page0_fig0.jpg",
      "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.",
      "context_before": "Several open source libraries have been developed in recent years to provide various levels of functionality in learning fair AI models. Many of these deal only with bias detection, and provide no techniques for mitigating such bias. Fairness Measures (Zehlike et al., 2017), for example, provides several fairness metrics, including difference of means, disparate impact, and odds ratio. A set of datasets is also provided, though some datasets are not in the public domain and need explicit permission from the owners to access/use the data. Similarly, FairML (Adebayo, 2016) provides an auditing tool for predictive models by quantifying the relative effects of various inputs on a models predictions. This, in turn, can be used to assess the models fairness. FairTest (Tramer et al. ` , 2017), on the other hand, approaches the task of detecting biases in a dataset by checking for associations between predicted labels and protected attributes. The methodology also provides a way to identify regions of the input space where an algorithm might incur unusually high errors. This toolkit also includes a rich catalog of datasets. Aequitas (Stevens et al., 2018) is another auditing toolkit for data scientists as well as policy makers; it has a Python library as well as an associated web site where data can be uploaded for bias analysis. It offers several fairness metrics, including demographic or statistical parity and disparate impact, along with a ”fairness tree” to help users identify the correct metric to use for their particular situation. Aequitas’s license does not allow\n\ncommercial use. Finally, Themis (Galhotra et al., 2017) is an open source bias toolbox that automatically generates test suites to measure discrimination in decisions made by a predictive system.\n\nA handful of toolkits address both bias detection as well as bias mitigation. Themis-ML (Bantilan, 2018) is one such repository that provides a few fairness metrics, such as mean difference, as well as some bias mitigation algorithms, such as relabeling (Kamiran & Calders, 2012), additive counterfactually fair estimator (Kusner et al., 2017), and reject option classification (Kamiran et al., 2012). The repository contains a subset of the methods described in the paper. Fairness Comparison (Friedler et al., 2018) is one of the more extensive libraries. It includes several bias detection metrics as well as bias mitigation methods, including disparate impact remover (Feldman et al., 2015), prejudice remover (Kamishima et al., 2012), and two-Naive Bayes (Calders & Verwer, 2010). Written primarily as a test-bed to allow different bias metrics and algorithms to be compared in a consistent way, it also allows the addition of additional algorithms and datasets.\n\nOur work on AIF360 aims to unify these efforts and bring together in one open source toolkit a comprehensive set of bias metrics, bias mitigation algorithms, bias metric explanations, and industrial usability. Another contribution of this work is a rigorous architectural design focused on extensibility, usability, explainability, and ease of benchmarking that goes beyond the existing work. We outline these design aspects in more details in the following sections.\n\n[Section: 4 OVERARCHING PARADIGM AND ARCHITECTURE]\n\nAIF360 is designed as an end-to-end workflow with two goals: (a) ease of use and (b) extensibility. Users should be able to go from raw data to a fair model as easily as possible, while comprehending the intermediate results. Researchers should be able to contribute new functionality with minimal effort.\n\nFigure 1 shows our generic pipeline for bias mitigation. Every output in this process (rectangles in the figure) is a new dataset that shares, at least, the same protected attributes as other datasets in the pipeline. Every transition is a transformation that may modify the features or labels or both between its input and output. Trapezoids represent learned models that can be used to make predictions on test data. There are also various stages in the pipeline where we can assess if bias is present using fairness metrics (not pictured), and obtain relevant explanations for the same (not pictured). These will each be discussed below.\n\nTo ensure ease of use, we created simple abstractions for datasets, metrics, explainers, and algorithms. Metric\n\nclasses compute fairness and accuracy metrics using one or two datasets, explainer classes provide explanations for the metrics, and algorithm classes implement bias mitigation algorithms. Each of these abstractions are discussed in detail in the subsequent sections. The term “dataset” refers to the dataset object created using our abstraction, as opposed to a CSV data file or a Pandas DataFrame.\n\nThe base classes for the abstractions are general enough to be useful, but specific enough to prevent errors. For example, there is no functional difference between predictions and ground-truth data or training and testing data. Each dataset object contains both features and labels, so it can be both an output of one transformation and an input to another. More specialized subclasses benefit from inheritance while also providing some basic error-checking, such as determining what metrics are available for certain types of datasets. Finally, we are able to generate high-quality, informative documentation automatically by using Sphinx1 to parse the docstring blocks in the code.\n\nThere are three main paths to the goal of making fair predictions (bottom right) — these are labelled in bold: fair pre-processing, fair in-processing, and fair post-processing. Each corresponds to a category of bias mitigation algorithms we have implemented in AIF360. Functionally, however, all three classes of algorithms act on an input dataset and produce an output dataset. This paradigm and the terminology we use for method names are familiar to the machine learning/data science community and similar to those used in other libraries such as scikit-learn.2 Block arrows marked “learn” in Figure 1 correspond to the fit method for a particular algorithm or class of algorithms. Sequences of arrows marked “apply” correspond to transform or predict methods. Predictions, by convention, result in an output that differs from the input by labels and not features or protected attributes. Transformations result in an output that may differ in any of those attributes.\n\nAlthough pre-, in-, and post-processing algorithms are all treated the same in our design, there are important considerations that the user must make in choosing which to use. For example, post-processing algorithms are easy to apply to existing classifiers without retraining. By making the distinction clear, which many libraries listed in Section 3 do not do, we hope to make the process transparent and easy to understand.\n\nAs for extensibility, while we cannot generalize from only one user, we were very encouraged about how easy it is to use the toolkit when within days of the toolkit being made available, a researcher from the AI fairness field submit-",
      "context_after": "[Section: 5 DATASET CLASS]\n\nThe Dataset class and its subclasses are a key abstraction that handle all forms of data. Training data is used to learn classifiers. Testing data is used to make predictions and compare metrics. Besides these standard aspects of a machine learning pipeline, fairness applications also require associating protected attributes with each instance or record in the data. To maintain a common format, independent of what algorithm or metric is being applied, we chose to structure the Dataset class so that all of these relevant attributes — features, labels, protected attributes, and their respective identifiers (names describing each) — are present and accessible from each instance of the class. Subclasses add further attributes that differentiate the dataset and dictate with which algorithms and metrics it is able to\n\ninteract.\n\nStructured data is the primary form of dataset studied in the fairness literature and is represented by the StructuredDataset class. Further distinction is made for a BinaryLabelDataset — a structured dataset that has a single label per instance that can only take one of two values: favorable or unfavorable. Unstructured data, which is receiving more attention from the fairness community, can be accommodated in our architecture by constructing a parallel class to the StructuredDataset class, without affecting existing classes or algorithms that are not applicable to unstructured data.\n\nThe classes provide a common structure for the rest of the pipeline to use. However, since raw data comes in many forms, it is not possible to automatically load an arbitrary raw dataset without input from the user about the data format. The toolkit includes a StandardDataset class that standardizes the process of loading a dataset from CSV format and populating the necessary class attributes. Raw data must often be “cleaned” before being used and categorical features must be encoded as numerical entities. Furthermore, with the same raw data, different experiments are often run using subsets of features or protected attributes. The StandardDataset class handles these common tasks by providing a simple interface for the\n\nuser to specify the columns of a Pandas DataFrame that correspond to features, labels, protected attributes, and optionally instance weights; the values of protected attributes that correspond to privileged status; the values of labels that correspond to favorable status; the features that need to be converted from categorical to numerical; and the subset of features to keep for the subsequent analysis. It also allows for arbitrary, user-defined data pre-processing, such as deriving new features from existing ones or filtering invalid instances.\n\nWe extend the StandardDataset class with examples of commonly used datasets that can be used to load datasets in different manners without modifying code by simply passing different arguments to the constructor. This is in contrast with other tools that make it more difficult to configure the loading procedure at runtime. We currently provide an interface to seven popular datasets: Adult Census Income (Kohavi, 1996), German Credit (Dheeru & Karra Taniskidou, 2017), ProPublica Recidivism (COM-PAS) (Angwin et al., 2016), Bank Marketing (Moro et al., 2014), and three versions of Medical Expenditure Panel Surveys (AHRQ, 2015; 2016).\n\nBesides serving as a structure that holds data to be used by bias mitigation algorithms or metrics, the Dataset class provides many important utility functions and capabilities. Using Python’s $= =$ operator, we are able to compare equality of two datasets and even compare a subset of fields using a custom context manager temporarily ignore3 The split method allows for easy partitioning into training, testing, and possibly validation sets. We are also able to easily convert to Pandas DataFrame format for visualization, debugging, and compatibility with externally implemented code. Finally, we track basic metadata associated with each dataset instance. Primary among these is a simple form of provenance-tracking: after each modification by an algorithm, a new object is created and a pointer to the previous state is kept in the metadata along with details of the algorithm applied. This way, we maintain trust and transparency in the pipeline.\n\n[Section: 6 METRICS CLASS]",
      "referring_paragraphs": [
        "Figure 1 shows our generic pipeline for bias mitigation.",
        "Figure 1.",
        "and its subclass BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset.",
        "This is illustrated in Figure 1.",
        "Table 1 provides the statistics and code coverage information as reported by the tool py.test --cov and Jupyter notebook coverage using py.test --nbval .",
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg",
      "image_filename": "1810.01943_page0_fig1.jpg",
      "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.",
      "context_before": "The Metric class and its subclasses compute various individual and group fairness metrics to check for bias in datasets and models.4 The DatasetMetric class\n\n3A sample snippet of this functionality is as follows:\n\nwith transf.temporarily ignore(‘labels’): return transf $= =$ pred\n\nThis returns True if the two datasets transf and pred differ only in labels.\n\n4In the interest of space, we omit mathematical notation and definitions here. They may be found elsewhere, including the documentation for AIF360.\n\nand its subclass BinaryLabelDatasetMetric examine a single dataset as input (StructuredDataset and BinaryLabelDataset, respectively) and are typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset. The metrics therein are the group fairness measures of disparate (DI) and statistical parity difference (SPD) — the ratio and difference, respectively, of the base rate conditioned on the protected attribute — and the individual fairness measure consistency defined by Zemel et al. (2013).\n\nIn contrast, the SampleDistortionMetric and ClassificationMetric classes examine two datasets as input. For classification metrics, the first input is the original or transformed dataset containing true labels, and the second input is the predicted dataset or fair predicted dataset, respectively, containing predicted labels. This metric class implements accuracy and fairness metrics on models. The sample distortion class contains distance computations between the same individual point in the original and transformed datasets for different distances. Such metrics are used, e.g. by Calmon et al. (2017), to quantify individual fairness.\n\nA large collection of group fairness and accuracy metrics in the classification metric class are functions of the confusion matrix of the true labels and predicted labels, e.g. false negative rate difference and false discovery rate ratio. Two metrics important for later reference are average odds difference (the mean of the false positive rate difference and the true positive rate difference), and equal opportunity difference (the true positive rate difference). Classification metrics also include disparate impact and statistical parity difference, but based on predicted labels. Since the main computation of confusion matrices is common for a large set of metrics, we utilize memoization and caching of computations for performance on large-scale datasets. This class also contains metrics based on the generalized entropy index, which is able to quantify individual and group fairness with a single number (Speicher et al., 2018). Additional details on the metrics used in the evaluations (Section 10) are given in Appendix C.\n\nThere is a need for a large number and variety of fairness metrics in the toolkit because there is no one best metric relevant for all contexts. It must be chosen carefully, based on subject matter expertise and worldview (Friedler et al., 2016). The comprehensiveness of the toolkit allows a user to not be hamstrung in making the most appropriate choice.\n\n[Section: 7 EXPLAINER CLASS]\n\nThe Explainer class is intended to be associated with the Metric class and provide further insights about computed fairness metrics. Different subclasses of varying",
      "context_after": "[Section: 7.1 Reporting]\n\nTextExplainer, a subclass of Explainer, returns a plain text string with a metric value. For example, the explanation for the accuracy metric is simply the text string “Classification accuracy on hcounti instances: haccuracyi”, where hcounti represents the number of records, and haccuracyi the accuracy. This can be invoked for both the privileged and unprivileged instances by passing arguments.\n\nJSONExplainer extends TextExplainer and produces three output attributes in JSON format: (a) metaattributes about the metric such as its name, a natural language description of its definition and its ideal value in a bias-free world, (b) statistical attributes that include the raw and derived numbers, and (c) the plain text explanation passed unchanged from the superclass TextExplainer. Outputs from this class are consumed by the Web application described in Section 11.\n\n[Section: 7.2 Fine-grained localization]",
      "referring_paragraphs": [
        "Figure 2.",
        "Figure 2 illustrates protected attribute bias localization on the German Credit dataset, with age as the protected attribute.",
        "Table 2."
      ],
      "figure_type": "example",
      "sub_figures": [
        "(a)",
        "(b)",
        "1810.01943_page0_fig2.jpg"
      ],
      "quality_score": 0.9,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg",
      "image_filename": "1810.01943_page0_fig3.jpg",
      "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.",
      "context_before": "TextExplainer, a subclass of Explainer, returns a plain text string with a metric value. For example, the explanation for the accuracy metric is simply the text string “Classification accuracy on hcounti instances: haccuracyi”, where hcounti represents the number of records, and haccuracyi the accuracy. This can be invoked for both the privileged and unprivileged instances by passing arguments.\n\nJSONExplainer extends TextExplainer and produces three output attributes in JSON format: (a) metaattributes about the metric such as its name, a natural language description of its definition and its ideal value in a bias-free world, (b) statistical attributes that include the raw and derived numbers, and (c) the plain text explanation passed unchanged from the superclass TextExplainer. Outputs from this class are consumed by the Web application described in Section 11.\n\n[Section: 7.2 Fine-grained localization]\n\nA more insightful explanation for fairness metrics is the localization of the source of bias at a fine granularity in the protected attribute and feature spaces. In the protected attribute space, the approach finds the values in which the given fairness metric is diminished (unprivileged group) or",
      "context_after": "[Section: 8 ALGORITHMS CLASS]\n\nBias mitigation algorithms attempt to improve the fairness metrics by modifying the training data, the learning algorithm, or the predictions. These algorithm categories are known as pre-processing, in-processing, and post-processing, respectively (d’Alessandro et al., 2017).\n\n[Section: 8.1 Bias mitigation approaches]",
      "referring_paragraphs": [
        "Figure 3."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) SPD - Re-weighing",
        "1810.01943_page0_fig4.jpg",
        "(b) SPD - Optimized pre-proc.",
        "1810.01943_page0_fig5.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig5.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg",
      "image_filename": "1810.01943_page0_fig6.jpg",
      "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For fair pre-processing algorithms, since the original dataset itself gets transformed (see Figure 1), we compute fairness metrics before and after this transformation and present results in Figure 4.",
        "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   \n(a) Statistical parity difference"
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(c) DI - Re-weighing",
        "(d) DI - Optimized pre-proc.",
        "1810.01943_page0_fig7.jpg",
        "1810.01943_page0_fig8.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig8.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_10",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
      "image_filename": "1810.01943_page0_fig9.jpg",
      "caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "An example result for Adult Census Income dataset with\n\nrace as protected attribute is shown in Figure 5.",
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) Disparate impact",
        "(c) Average odds difference",
        "1810.01943_page0_fig10.jpg",
        "(d) Equal opportunity difference",
        "1810.01943_page0_fig11.jpg",
        "1810.01943_page0_fig12.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig12.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_14",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg",
      "image_filename": "1810.01943_page0_fig13.jpg",
      "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.",
      "context_before": "",
      "context_after": "After adversarial debiasing mitigation   \nFigure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.\n\nconvey toolkit richness while still being approachable. In the final design, a short textual introduction to the content of the site, along with direct links to the API documentation and code repository, is followed by a number of direct links to various levels of advice and examples. Further links to the individual datasets, the bias checkers, and the mitigation algorithms are also provided. In all this, we ensured the site was suitably responsive across all major desktop and mobile platforms.\n\n[Section: 11.2 Design of the back-end service]\n\nThe demo web application not only provides a gentle introduction to the capabilities of the toolkit, but also serves as an example of deploying the toolkit to the cloud and converting it into a web service. We used Python’s Flask framework for building the service and exposed a REST API that generates a bias report based on the following input parameters from a user: the dataset name, the protected attributes, the privileged and unprivileged groups, the chosen fairness metrics, and the chosen mitigation algorithm, if any. With these inputs, the back-end then runs a series of steps to 1) split the dataset into training, development, and validation sets; 2) train a logistic regression classifier on the training set; 3) run the bias-checking metrics on the classifier against the test dataset; 4) if a mitigation algorithm is chosen, run the mitigation algorithm with the appropriate pipeline (pre-processing, in-processing, or post-processing). The end result is then cached so that if the exact same inputs are provided, the result can be directly retrieved from cache and no additional computation is needed.\n\nThe reason to truly use the toolkit code in serving the Web application rather than having a pre-computed lookup table of results is twofold: we want to make the app a real representation of the underlying capabilities (in fact, creating the Web app helped us debug a few items in the code), and we also avoid any issues of synchronizing updates to the metrics, explainers, and algorithms with the results shown: synchronization is automatic. Currently, the service is limited to three built-in datasets, but it can be expanded to support the user’s own data upload. The service is also limited to building logistic regression classifiers, but again this can be expanded. Such expansions can be more easily implemented if this fairness service is integrated into a full AI suite that provides various classifier options and data storage solutions.",
      "referring_paragraphs": [
        "Figure 6 shows the before and after mitigation graphs from the interactive Web experience.",
        "After adversarial debiasing mitigation   \nFigure 6."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.7000000000000001,
      "metadata": {}
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_15",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg",
      "image_filename": "1810.01943_page0_fig14.jpg",
      "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.",
      "context_before": "Adebayo, J. A. ”FairML : Toolbox for diagnosing bias in predictive modeling”. Master’s thesis, Massachusetts Institute of Technology, 2016. URL https://github. com/adebayoj/fairml.   \nAHRQ. Medical Expenditure Panel Survey data: 2015 Full Year Consolidated Data File, 2015. URL https://meps.ahrq.gov/mepsweb/data_ stats/download_data_files_detail.jsp? cboPufNumber $=$ HC-181.   \nAHRQ. Medical Expenditure Panel Survey data: 2016 Full Year Consolidated Data File, 2016. URL https://meps.ahrq.gov/mepsweb/data_ stats/download_data_files_detail.jsp? cboPufNumber $=$ HC-192.   \nAngwin, J., Larson, J., Mattu, S., and Kirchner, L. Machine bias: Theres software used across the country to predict future criminals. And its biased against blacks. ProPublica, 2016. URL https://github.com/ propublica/compas-analysis.\n\nBantilan, N. Themis-ml: A fairness-aware machine learning interface for end-to-end discrimination discovery and mitigation. Journal of Technology in Human Services, 36(1):15–30, 2018. URL https://github.com/ cosmicBboy/themis-ml.   \nCalders, T. and Verwer, S. Three naive Bayes approaches for discrimination-free classification. Data Mining journal; special issue with selected papers from ECML/PKDD, 2010.   \nCalmon, F. P., Wei, D., Vinzamuri, B., Natesan Ramamurthy, K., and Varshney, K. R. Optimized preprocessing for discrimination prevention. In Advances in Neural Information Processing Systems 30, pp. 3992– 4001, Long Beach, USA, December 2017.   \nd’Alessandro, B., O’Neil, C., and LaGatta, T. Conscientious classification: A data scientist’s guide to discrimination-aware classification. Big Data, 5(2):120– 134, June 2017.   \nDheeru, D. and Karra Taniskidou, E. UCI machine learning repository, 2017. URL https: //archive.ics.uci.edu/ml/datasets/ Statlog+%28German+Credit+Data%29.   \nDieterich, W., Mendoza, C., and Brennan, T. COM-PAS risk scales: Demonstrating accuracy equity and predictive parity, 2016. URL https://assets. documentcloud.org/documents/2998391/ ProPublica-Commentary-Final-070616. pdf.   \nFeldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., and Venkatasubramanian, S. Certifying and removing disparate impact. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 259–268, Sydney, Australia, August 2015.   \nFriedler, S. A., Scheidegger, C., and Venkatasubramanian, S. On the (im)possibility of fairness. arXiv:1609.07236, September 2016.   \nFriedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., and Roth, D. A comparative study of fairness-enhancing interventions in machine learning. http://arxiv.org/abs/1802.04422, Feb. 2018. URL https://github.com/ algofairness/fairness-comparison.   \nGalhotra, S., Brun, Y., and Meliou, A. Fairness Testing: Testing software for discrimination. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2017, pp. 498–510, 2017. doi: http://doi.acm.org/10.1145/\n\n3106237.3106277. URL https://github.com/ LASER-UMASS/Themis.   \nHardt, M., Price, E., and Srebro, N. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems 29, pp. 3315–3323, Barcelona, Spain, December 2016.   \nHu, L. and Chen, Y. Welfare and distributional impacts of fair classification. In Proc. Fairness Accountability Transparency Machine Learning Workshop, July 2018.   \nKamiran, F. and Calders, T. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1–33, 2012. doi: https://doi. org/10.1007/s10115-011-0463-8.   \nKamiran, F., Karim, A., and Zhang, X. Decision theory for discrimination-aware classification. In IEEE International Conference on Data Mining, pp. 924–929, 2012. doi: https://doi.org/10.1109/ICDM.2012.45.   \nKamishima, T., Akaho, S., Asoh, H., and Sakuma, J. Fairness-aware classifier with prejudice remover regularizer. Machine Learning and Knowledge Discovery in Databases, pp. 35–50, 2012.   \nKleinberg, J., Mullainathan, S., and Raghavan, M. Inherent trade-offs in the fair determination of risk scores. Innovations in Theoretical Computer Science. ACM, 2017.   \nKohavi, R. Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996. URL https://archive. ics.uci.edu/ml/datasets/adult.   \nKusner, M. J., Loftus, J. R., Russell, C., and Silva, R. Counterfactual fairness. In NIPS, 2017.   \nLarson, J. and Angwin, J. Technical response to Northpointe, 2016. URL http: //www.propublica.org/article/ technical-response-to-northpointe.   \nLarson, J., Mattu, S., Kirchner, L., and Angwin, J. How we analyzed the COMPAS recidivism algorithm, 2016. URL http: //www.propublica.org/article/ how-we-analyzed-the-compas-recidivism-algorithm.   \nMoro, S., Cortez, P., and Rita, P. A datadriven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22–31, 2014. URL https://archive.ics.uci.edu/ ml/datasets/Bank+Marketing.\n\nNarayanan, A. Translation tutorial: 21 fairness definitions and their politics. In Conference on Fairness, Accountability, and Transparency, February 2018.   \nPierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Ramachandran, V., Phillips, C., and Goel, S. A largescale analysis of racial disparities in police stops across the united states. arXiv preprint arXiv:1706.05678, 2017.   \nPleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K. Q. On fairness and calibration. In Advances in Neural Information Processing Systems, pp. 5680–5689, 2017.   \nSpeicher, T., Heidari, H., Grgic-Hlaca, N., Gummadi, K. P., Singla, A., Weller, A., and Zafar, M. B. A unified approach to quantifying algorithmic unfairness: Measuring individual & group unfairness via inequality indices. In Proc. ACM SIGKDD Int. Conf. Knowl. Disc. Data Min., pp. 2239–2248, London, UK, August 2018.   \nStevens, A., Anisfeld, A., Kuester, B., London, J., Saleiro, P., and Ghani, R. Aequitas: Bias and fairness audit, 2018. URL https://github.com/dssg/ aequitas. Center for Data Science and Public Policy, The University of Chicago.   \nTramer, F., Atlidakis, V., Geambasu, R., Hsu, D., Hubaux, ` J.-P., Humbert, M., Juels, A., and Lin, H. FairTest: Discovering unwarranted associations in data-driven applications. In IEEE European Symposium on Security and Privacy, pp. 401–416, 2017. doi: https://doi.org/ 10.1109/EuroSP.2017.29. URL https://github. com/columbia/fairtest.   \nUstun, B., Spangher, A., and Liu, Y. Actionable recourse in linear classification. arXiv:1809.06514, September 2018.   \nWachter, S., Mittelstadt, B., and Russell, C. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & Technology, 31(2):841–887, Spring 2018.   \nZehlike, M., Castillo, C., Bonchi, F., Hajian, S., and Megahed, M. Fairness Measures: Datasets and software for detecting algorithmic discrimination, 2017. URL http://fairness-measures.org/.   \nZemel, R., Wu, Y. L., Swersky, K., Pitassi, T., and Dwork, C. Learning fair representations. In Proceedings of the International Conference on Machine Learning, pp. 325–333, Atlanta, USA, June 2013.   \nZhang, B. H., Lemoine, B., and Mitchell, M. Mitigating unwanted biases with adversarial learning. In Proc. AAAI/ACM Conf. Artif. Intell., Ethics, Society, New Orleans, USA, February 2018.\n\n[Section: Appendices]\n\n[Section: A UML CLASS DIAGRAM]",
      "context_after": "[Section: B CODE SNIPPETS]\n\nThis example provides Python code snippets for some common tasks that the user might perform using our toolbox. The example involves the user loading a dataset, splitting it into training and testing partitions, understanding the outcome disparity between two demographic groups, and transforming the dataset to mitigate this disparity. A more detailed version of this example is available in url.redacted.\n\n[Section: B.1 Dataset operations]",
      "referring_paragraphs": [
        "Figure 7."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_16",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg",
      "image_filename": "1810.01943_page0_fig15.jpg",
      "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ",
      "context_before": "This is the difference in true positive rates between unprivileged and privileged groups. This is a method in the ClassificationMetric class and hence needs to be computed using the input and output datasets to a classifier. A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.\n\n[Section: D EVALUATION ON DIFFERENT DATA SETS]\n\nWe present additional results with bias mitigation obtained for various datasets and protected attributes. These correspond to the setting described in Section 10.",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 8."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Statistical parity difference",
        "(b) Disparate impact",
        "(c) Average odds difference",
        "(d) Equal opportunity difference",
        "1810.01943_page0_fig16.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig16.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_18",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg",
      "image_filename": "1810.01943_page0_fig17.jpg",
      "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 10."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Statistical parity difference",
        "(b) Disparate impact",
        "(c) Average odds difference",
        "(d) Equal opportunity difference",
        "1810.01943_page0_fig18.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig18.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_20",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg",
      "image_filename": "1810.01943_page0_fig19.jpg",
      "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.",
      "context_before": "",
      "context_after": "[Section: 4.Compare original vs. mitigated results]\n\nDataset: Adult census income\n\nMitigation: Optimized Pre-processing algorithm applied\n\nProtected Attribute: Race\n\nPrivileged Group: White, Unprivileged Group: Non-white\n\nAccuracy after mitigation changed from $8 2 \\%$ to $7 4 \\%$\n\nBias against unprivileged group was reduced to acceptable levels*for1 of 2 previously biased metrics\n\n(1 of 5 metrics stillindicate bias for unprivileged group)",
      "referring_paragraphs": [
        "Figure 12."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Statistical parity difference",
        "(b) Disparate impact",
        "(c) Average odds difference",
        "(d) Equal opportunity difference",
        "1810.01943_page0_fig20.jpg",
        "1810.01943_page0_fig21.jpg",
        "1810.01943_page0_fig22.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig22.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1810.01943",
      "figure_id": "1810.01943_fig_24",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig23.jpg",
      "image_filename": "1810.01943_page0_fig23.jpg",
      "caption": "Figure 13. A screen shot from the web interactive experience, showing the results of mitigation applied to one of the available datasets.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 13. A screen shot from the web interactive experience, showing the results of mitigation applied to one of the available datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1810.01943_page0_fig24.jpg",
        "1810.01943_page0_fig25.jpg"
      ],
      "quality_score": 0.44999999999999996,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig24.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig25.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1810.03611": [
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg",
      "image_filename": "1810.03611_page0_fig0.jpg",
      "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.",
      "context_before": "Choice of corpus and hyperparameters. We use two corpora in our experiments, each with a different set of GloVe hyperparameters. This first setup consists of a corpus constructed from a Simple English Wikipedia dump (2017- 11-03) (Wikimedia, 2018) using 75-dimensional word vectors. These dimensions are small by the standards of a\n\ntypical word embedding, but sufficient to start capturing syntactic and semantic meaning. Performance on the TOP-1 analogies test shipped with the GloVe code base was around $3 5 \\%$ , lower than state-of-the-art performance but still clearly capturing significant meaning.\n\nOur second setup is more representative of the academic and commercial contexts in which our technique could be applied. The corpus is constructed from 20 years of New York Times (NYT) articles (Sandhaus, 2008), using 200- dimensional vectors. The TOP-1 analogy performance is approximately $54 \\%$ . The details of these two configurations are tabulated in the supplemental material.\n\nChoice of experimental bias metric. Throughout our experiments, we consider the effect size of two different WEAT biases as presented by Caliskan et al. (2017). Recall that these metrics have been shown to correlate with known human biases as measured by the Implicit Association Test. In WEAT1, the target word sets are science and arts terms, while the attribute word sets are male and female terms. In WEAT2, the target word sets are musical instruments and weapons, while the attribute word sets are pleasant and unpleasant terms. A full list of the words in these sets can be found in the supplemental material. They are summarized in Table 1. These sets were chosen so as to include one societal bias that would be widely viewed as problematic, and another which would be widely viewed as benign.\n\n[Section: 5.2. Testing the Accuracy of our Method]\n\nExperimental Methodology. To test the accuracy of our methodology, ideally we would simply remove a single document from a word embedding’s corpus, train a new embedding, and compare the change in bias with our differential bias approximation. However, the cosine similarities between small sets of word vectors in two word embeddings trained on the same corpus can differ considerably simply because of the stochastic nature of the optimization (Antoniak & Mimno, 2018). As a result, the WEAT biases vary between training runs. The effect of removing a single document, which is near zero for a typical document, is hidden in this variation. Fixing the random seed is not a practical approach. Many popular word embedding implementations also require limiting training to a single thread to fully eliminate randomness. This would make experimentation prohibitively slow.\n\nIn order to obtain measurable changes, we instead remove sets of documents, resulting in larger corpus perturbations. Accuracy is assessed by comparing our method’s predictions to the actual change in bias measured when each document set is removed from the corpus and a new embedding is trained on this perturbed corpus. Furthermore, we make all predictions and assessments using several embeddings, each\n\nTable 1. WEAT Target and Attribute Sets   \n\n<table><tr><td></td><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td rowspan=\"2\">Target Sets</td><td>S</td><td>science</td><td>instruments</td></tr><tr><td>T</td><td>arts</td><td>weapons</td></tr><tr><td rowspan=\"2\">Attribute Sets</td><td>A</td><td>male</td><td>pleasant</td></tr><tr><td>B</td><td>female</td><td>unpleasant</td></tr></table>\n\nTable 2. Baseline WEAT Effect Sizes   \n\n<table><tr><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td>Wiki</td><td>0.957 (± 0.150)</td><td>0.108 (± 0.213)</td></tr><tr><td>NYT</td><td>1.14, (± 0.124)</td><td>1.32, (± 0.056)</td></tr></table>\n\ntrained with the same hyperparameters, but differing in their random seeds.\n\nWe construct three types of perturbation sets: increase, random, and decrease. The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1. The random perturbation sets are simply documents chosen from the corpus uniformly at random. For a more detailed description, please refer to the supplemental material. Most of the code used in the experimentation has been made available online2.\n\nExperimental Results. Here we present a subset of our experimental results, principally from NYT WEAT1 (science vs. arts). Complete sets of results from the four configurations $( \\{ \\mathrm { N Y T } , \\mathrm { W i k i } \\} \\times \\{ \\mathrm { W E A T 1 } , \\mathrm { W E A T 2 } \\} )$ ) can be found in the supplemental materials.\n\nThe baseline WEAT effect sizes ( $\\pm 1$ std. dev.) are shown in Table 2. It is worth noting that the WEAT2 (weapons vs. instruments) bias was not significant in our Wiki setup. However, our analysis does not require that the bias under consideration fall within any particular range of values.\n\nA histogram of the differential bias of removal for each document in our NYT setup (WEAT1) can be seen in Figure 1. Notice the log scale on the vertical axis, and how the vast majority of documents are predicted to have a very small impact on the differential bias.\n\nWe assess the accuracy of our approximations by measuring how they correlate with the ground truth change in bias (as measured by retraining the embedding after removing a subset of the training corpus). Recall these ground truth changes are obtained using several retraining runs with different random seeds. We find extremely strong correlations $( r ^ { 2 } \\geq 0 . 9 8 5 )$ in every configuration, for example Figure 2.\n\nWe further compare our approximations to the ground truth",
      "context_after": "",
      "referring_paragraphs": [
        "They are summarized in Table 1.",
        "The targeted (increase, decrease) perturbation sets are constructed from the documents whose removals were predicted (by our method) to cause the greatest differential bias, e.g., the documents located in the tails of the histograms in Figure 1.",
        "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg",
      "image_filename": "1810.03611_page0_fig1.jpg",
      "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.",
      "context_before": "",
      "context_after": "[Section: 5.3. Comparison to a PPMI Baseline]\n\nWe have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
      "referring_paragraphs": [
        "9 8 5 )$ in every configuration, for example Figure 2.",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg",
      "image_filename": "1810.03611_page0_fig2.jpg",
      "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)",
      "context_before": "[Section: 5.3. Comparison to a PPMI Baseline]\n\nWe have shown that our method can be used to identify bias-influencing documents and accurately approximate the impact of their removal, but how does it compare to a more",
      "context_after": "[Section: 5.4. Impact on Word2Vec and Other Bias Metrics]\n\nThe documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings. Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\n\nWe start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
      "referring_paragraphs": [
        "in Figure 3.",
        "Figure 3.",
        "0 7 \\%$ of articles can reverse the WEAT effect size in the New York Times, as is shown in Figure 3, decrease-1000.",
        "Table 3 presents a summary of the corpora and embedding hyperparameters used throughout our experimentation.",
        "We start by training 10 word embeddings using the parameters in Table 3 above, but using different random seeds."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg",
      "image_filename": "1810.03611_page0_fig3.jpg",
      "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)",
      "context_before": "[Section: 5.4. Impact on Word2Vec and Other Bias Metrics]\n\nThe documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings. Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\n\nWe start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
      "context_after": "[Section: 5.5. Qualitative Analysis]\n\nWe’ve demonstrated that removing the most influential documents identified by our methodology significantly impacts the WEAT, a metric that has been shown to correlate with known human biases. But can the semantic content of these documents be intuitively understood to affect bias?\n\nWe comment here on the 50 most bias influencing doc-",
      "referring_paragraphs": [
        "The documents identified as influential by our method clearly have a strong impact on the WEAT effect size in GloVe embeddings. Here we explore how those same documents impact the bias in word2vec embeddings, as well as other bias metrics.\n\nWe start by training five word2vec emebeddings with comparable hyperparameters3 for each perturbation set, and measure how their removals affect the bias. Figure 4 shows",
        "Figure 4.",
        "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.\n\nTable 4. Correlation of Approximated and Validated Mean Biases   \n\n<table><tr><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td>Wiki</td><td>r2: 0.986</td><td>r2: 0.993</td></tr><tr><td>NYT</td><td>r2: 0.995</td><td>r2: 0.997</td></tr></table>",
        "ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg",
      "image_filename": "1810.03611_page0_fig4.jpg",
      "caption": "Figure 5. The effect of removing the 10k most bias increasing and bias decreasing documents as identified by our method on the projection of the target words onto the gender axis vs. unperturbed corpus (base); error bars show one std dev; corpus word frequency noted in parentheses. (NYT - WEAT1)",
      "context_before": "[Section: 5.5. Qualitative Analysis]\n\nWe’ve demonstrated that removing the most influential documents identified by our methodology significantly impacts the WEAT, a metric that has been shown to correlate with known human biases. But can the semantic content of these documents be intuitively understood to affect bias?\n\nWe comment here on the 50 most bias influencing doc-",
      "context_after": "[Section: 6. Conclusion]\n\nIn this work, we introduce the problem of tracing the origins of bias in word embeddings, and we develop and experimentally validate a methodology to solve it. We conceptualize the problem as measuring the resulting change in bias when we remove a training document (or small subset of the training corpus), and interpret this as the amount of bias contributed by the document to the overall embedding bias. Computing this naively for each training document would be infeasible. We develop an efficient approximation of this differential bias using influence functions and apply it to the GloVe word embedding algorithm. We experimentally validate our approach and find that it very accurately approximates the true change in bias that results from manually removing training documents and retraining. It performs well on tests using Simple Wikipedia and New York Times corpora and two WEAT bias metrics.\n\nOur work represents a new approach to understanding how machine learning algorithms learn biases from training data. Our methodology could be applied to assess how the bias of a set of texts has evolved over time. For example, using publicly available datasets of newspaper articles or books, one could measure how cultural biases as measured by WEAT or other metrics have evolved over time. More broadly, our efficient method for tracing how perturbations in training data affect changes in the bias of the output is a general idea, and could be applied in many other contexts.\n\n[Section: References]",
      "referring_paragraphs": [
        "In Figure 5 we show the baseline projections and compare them to the projections after having removed the 10k most bias increasing and bias decreasing documents.",
        "Figure 5.",
        "Table 5."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1810.03611_page0_fig5.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig5.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_7",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg",
      "image_filename": "1810.03611_page0_fig6.jpg",
      "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.",
      "context_before": "",
      "context_after": "Validated Effect Size",
      "referring_paragraphs": [
        "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.\n\nTable 4. Correlation of Approximated and Validated Mean Biases   \n\n<table><tr><td></td><td>WEAT1</td><td>WEAT2</td></tr><tr><td>Wiki</td><td>r2: 0.986</td><td>r2: 0.993</td></tr><tr><td>NYT</td><td>r2: 0.995</td><td>r2: 0.997</td></tr></table>"
      ],
      "figure_type": "other",
      "sub_figures": [
        "1810.03611_page0_fig7.jpg",
        "1810.03611_page0_fig8.jpg",
        "1810.03611_page0_fig9.jpg",
        "1810.03611_page0_fig10.jpg",
        "1810.03611_page0_fig11.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig11.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_13",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg",
      "image_filename": "1810.03611_page0_fig12.jpg",
      "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.",
      "context_before": "Validated Effect Size",
      "context_after": "Validated Effect Size   \nFigure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.\n\nTable 5. A comparison of the effect of removing the most impactful documents as identified by a PPMI baseline technique versus when identified by our method (Wiki setup, mean of WEAT1 in 10 retrained GloVe embeddings).   \n\n<table><tr><td colspan=\"2\">Document Set</td><td rowspan=\"2\">ΔB when \nbaseline</td><td rowspan=\"2\">Identified by \nour method</td></tr><tr><td>objective</td><td>num. docs.</td></tr><tr><td>correct</td><td>300</td><td>-67%</td><td>-187%</td></tr><tr><td>correct</td><td>100</td><td>-50%</td><td>-147%</td></tr><tr><td>correct</td><td>30</td><td>-23%</td><td>-57%</td></tr><tr><td>correct</td><td>10</td><td>-4%</td><td>-40%</td></tr><tr><td>aggravate</td><td>10</td><td>-0.5%</td><td>32%</td></tr><tr><td>aggravate</td><td>30</td><td>20%</td><td>53%</td></tr><tr><td>aggravate</td><td>100</td><td>15%</td><td>79%</td></tr><tr><td>aggravate</td><td>300</td><td>47%</td><td>84%</td></tr></table>",
      "referring_paragraphs": [
        "Validated Effect Size   \nFigure 7."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1810.03611_page0_fig13.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig13.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_15",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg",
      "image_filename": "1810.03611_page0_fig14.jpg",
      "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines"
      ],
      "figure_type": "other",
      "sub_figures": [
        "1810.03611_page0_fig15.jpg",
        "1810.03611_page0_fig16.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig16.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1810.03611",
      "figure_id": "1810.03611_fig_18",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg",
      "image_filename": "1810.03611_page0_fig17.jpg",
      "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).",
      "context_before": "",
      "context_after": "[Section: E. Influential Documents - NYT WEAT 1]\n\nThe below documents were identified to be the 50 most WEAT1 bias influencing documents in our NYT setup. We list the article titles. Publication dates range from January 1, 1987 to June 19, 2007. Most can be found through https://www.nytimes.com/search. A subscription may be required for access.\n\n[Section: ∆docB Bias Decreasing]",
      "referring_paragraphs": [
        "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1810.03611_page0_fig18.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig18.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1810.03993": [
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg",
      "image_filename": "1810.03993_page0_fig0.jpg",
      "caption": "",
      "context_before": "[Section: Ethical Considerations]\n\n• Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.\n\n[Section: Caveats and Recommendations Caveats and Recommendations]",
      "context_after": "Quantitative Analyses",
      "referring_paragraphs": [],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {}
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg",
      "image_filename": "1810.03993_page0_fig1.jpg",
      "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.",
      "context_before": "Quantitative Analyses",
      "context_after": "[Section: ]\n\n• Does not capture race or skin type, which has been reported as a source of disproportionate errors [5].   \n• Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.   \n• An ideal evaluation dataset would additionally include annotations for Fitzpatrick skin type, camera details, and environment (lighting/humidity) details.\n\n[Section: Model Card - Toxicity in Text]",
      "referring_paragraphs": [
        "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).",
        "For example, slicing the evaluation across groups functions to highlight errors that may fall disproportionately on some groups of people, and accords with many recent notions of mathematical fairness (discussed further in the example model card in Figure 2).",
        "Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2.",
        "Figure 2 shows our prototype.",
        "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1810.03993_page0_fig2.jpg",
        "1810.03993_page0_fig3.jpg",
        "1810.03993_page0_fig4.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig4.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1810.03993",
      "figure_id": "1810.03993_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg",
      "image_filename": "1810.03993_page0_fig5.jpg",
      "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.",
      "context_before": "Quantitative Analyses",
      "context_after": "[Section: 7 ACKNOWLEDGEMENTS]\n\nThank you to Joy Buolamwini, Shalini Ananda and Shira Mitchell for invaluable conversations and insight.\n\n[Section: REFERENCES]",
      "referring_paragraphs": [
        "We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [36] (Figure 2), and a public toxicity detection model [32] (Figure 3).",
        "Our second example provides a model card for Perspective API’s TOXICITY classifier built to detect ‘toxicity’ in text [32], and is presented in Figure 3.",
        "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1810.03993_page0_fig6.jpg",
        "1810.03993_page0_fig7.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig7.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1810.08683": [
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_1",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg",
      "image_filename": "1810.08683_page0_fig0.jpg",
      "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).",
      "context_before": "We employed the Adult dataset from the UCI repository7 and the Correctional Offender Management Profilingfor Alternative Sanctions (COMPAS) dataset8.\n\nThe Adult dataset contains 14 features concerning demographic characteristics of 45222 instances (32561 for training and 12661 for testing), 2 features, Gender (G) and Race (R), can be considered sensitive. The task is to predict if a person has an income per year that is more (or less) than 50 000$. Some statistics of the adult ,dataset with reference to the sensitive features are reported in Table 1.\n\nThe COMPAS dataset is constructed by the commercial algorithm COMPAS, which is used by judges and parole officers for scoring\n\nTable 1: Adult dataset: statistics with reference to the sensitive features.   \n\n<table><tr><td>Sens.</td><td>Group</td><td>%</td></tr><tr><td rowspan=\"2\">G</td><td>Male (M)</td><td>66.9</td></tr><tr><td>Female(F)</td><td>33.2</td></tr><tr><td rowspan=\"5\">R</td><td>White (W)</td><td>85.5</td></tr><tr><td>Black (B)</td><td>9.6</td></tr><tr><td>Asian-Pac-Islander (API)</td><td>3.1</td></tr><tr><td>Amer-Indian-Eskimo (AIE)</td><td>1.0</td></tr><tr><td>Other (O)</td><td>0.8</td></tr><tr><td rowspan=\"10\">G+R</td><td>W&amp;M</td><td>58.8</td></tr><tr><td>W&amp;F</td><td>26.7</td></tr><tr><td>B&amp;M</td><td>4.9</td></tr><tr><td>B&amp;F</td><td>4.7</td></tr><tr><td>API&amp;M</td><td>2.1</td></tr><tr><td>API&amp;F</td><td>1.1</td></tr><tr><td>AIE&amp;M</td><td>0.6</td></tr><tr><td>AIE&amp;F</td><td>0.4</td></tr><tr><td>O&amp;M</td><td>0.5</td></tr><tr><td>O&amp;F</td><td>0.3</td></tr></table>\n\ncriminal defendants likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants based on a 2-years follow up study. This dataset contains variables used by the COMPAS algorithm in scoring defendants, along with their outcomes within two years of the decision, for over 10000 criminal defendants in Broward County, Florida. In the original data, 3 subsets are provided. We concentrate on the one that includes only violent recividism. Table 2, analogously to Table 1, reports the statistics with reference to the sensitive features.\n\nIn all the experiments, we compare STL, ITL, and MTL in different settings. Specifically we test each method in the following cases: when the models use the sensitive feature $\\scriptstyle ( S = 1 )$ ) or not $( \\mathsf { S } \\mathrm { = } 0 )$ , when the fairness constraint is active $\\left( \\mathrm { F } { = } 1 \\right)$ ) or not $( \\mathrm { F } { = } 0 )$ , when we consider the group specific models $\\mathrm { ( D } { = } 1 \\mathrm { ) }$ ) or the shared model between groups $\\mathrm { ( D = } 0 )$ ), and when we use the true sensitive feature $( \\mathrm { P } { = } 1 )$ or the predicted one $( { \\mathrm { P } } { = } 0 )$ . Note that when $\\mathrm { D } { = } 0$ we can only compare STL with MTL, since only these two models produce a shared model between the groups, and furthermore, when $\\mathrm { D } { = } 1$ we can only compare ITL with MTL, since these produce group specific models.\n\nWe collect statistics concerning the classification average accuracy per group in percentage (ACC) on the test set, difference of equal opportunities on both the positive and negative class (denoted as $\\mathrm { D E O ^ { + } }$ and $\\mathrm { D E O ^ { - } }$ , respectively), and the difference of equalized odds (DEOd) of the selected model - see Section 2 for a definition of these quantities.\n\nWe selected the best hyperparameters9 by the two steps 10-fold cross validation (CV) procedure described in [16]. In the first step, the value of the hyperparameters with highest accuracy is identified. In the second step, we shortlist all the hyperparameters with\n\n<table><tr><td>Sens.</td><td>Group</td><td>%</td></tr><tr><td rowspan=\"2\">G</td><td>Female (F)</td><td>19.34</td></tr><tr><td>Male (M)</td><td>80.66</td></tr><tr><td rowspan=\"6\">R</td><td>African-American (AA)</td><td>51.23</td></tr><tr><td>Asian (A)</td><td>0.44</td></tr><tr><td>Caucasian (C)</td><td>34.02</td></tr><tr><td>Hispanic (H)</td><td>8.83</td></tr><tr><td>Native American (NA)</td><td>0.25</td></tr><tr><td>Other (O)</td><td>5.23</td></tr><tr><td rowspan=\"12\">G+R</td><td>Female African-American</td><td>9.04</td></tr><tr><td>Female Asian</td><td>0.03</td></tr><tr><td>Female Caucasian</td><td>7.86</td></tr><tr><td>Female Hispanic</td><td>1.48</td></tr><tr><td>Female Native American</td><td>0.06</td></tr><tr><td>Female Other</td><td>0.93</td></tr><tr><td>Male African-American</td><td>42.20</td></tr><tr><td>Male Asian</td><td>0.45</td></tr><tr><td>Male Caucasian</td><td>26.16</td></tr><tr><td>Male Hispanic</td><td>7.40</td></tr><tr><td>Male Native American</td><td>0.19</td></tr><tr><td>Male Other</td><td>4.30</td></tr></table>\n\nTable 2: COMPAS dataset: statistics with reference to the sensitive features.   \nTable 3: Adult Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.   \n\n<table><tr><td>R</td><td>W</td><td>B</td><td>API</td><td>AIE</td><td>O</td></tr><tr><td>G</td><td>M</td><td>F</td><td></td><td></td><td></td></tr><tr><td>M</td><td>58.2</td><td>3.8</td><td>78.5</td><td>1.7</td><td>0.5</td></tr><tr><td>B</td><td>4.6</td><td>7.8</td><td>0.1</td><td>0.0</td><td>0.0</td></tr><tr><td>API</td><td>0.5</td><td>0.0</td><td>0.8</td><td>0.0</td><td>0.0</td></tr><tr><td>AIE</td><td>1.5</td><td>0.1</td><td>0.0</td><td>2.6</td><td>0.0</td></tr><tr><td>O</td><td>0.4</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.7</td></tr></table>\n\naccuracy close to the best one (in our case, above $9 7 \\%$ of the best accuracy). Finally, from this list, we select the hyperparameters with the lowest fairness measure. This validation procedure, ensures that fairness cannot be achieved by a mere modification of hyperparameter selection procedure.\n\n[Section: 5.2 Results]\n\nThe results for all possible combinations described above, are reported in Table 5. In Figures 2, 3, and 4, we present a visualization of Table 5 for the Adult dataset (results are analogous for the COMPAS one). Where both the error (i.e., 1-ACC), and the EOd are normalized to be between 0 and 1, column-wise. The closer a point is to the origin, the better the result.\n\nTable 4: COMPAS Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.   \n\n<table><tr><td>R</td><td>AA</td><td>A</td><td>C</td><td>H</td><td>NA</td><td>O</td></tr><tr><td>AA</td><td>44.8</td><td>0.0</td><td>3.4</td><td>0.6</td><td>0.0</td><td>0.3</td></tr><tr><td>A</td><td>0.1</td><td>0.3</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>C</td><td>4.4</td><td>0.0</td><td>29.6</td><td>0.4</td><td>0.0</td><td>0.2</td></tr><tr><td>H</td><td>1.2</td><td>0.0</td><td>0.6</td><td>7.7</td><td>0.0</td><td>0.1</td></tr><tr><td>NA</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td></tr><tr><td>O</td><td>0.7</td><td>0.0</td><td>0.4</td><td>0.1</td><td>0.0</td><td>4.6</td></tr></table>",
      "context_after": "",
      "referring_paragraphs": [
        "Table 2, analogously to Table 1, reports the statistics with reference to the sensitive features.",
        "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_2",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig1.jpg",
      "image_filename": "1810.08683_page0_fig1.jpg",
      "caption": "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 3: Adult Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
        "Figure 3: Adult dataset: complete results set for Race (text close to the symbols in plot are P, D, F, and S).",
        "Table 3 and Table 4 report the confusion matrices computed on the test set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_3",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig2.jpg",
      "image_filename": "1810.08683_page0_fig2.jpg",
      "caption": "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 4: COMPAS Dataset: confusion matrices in percentage (true class in columns and predicted classes in rows) obtained by predicting Gender and Race from the other nonsensitive features using Random Forests.",
        "Figure 4: Adult dataset: complete results set for Gender+Race (text close to the symbols in plot are P, D, F, and S).",
        "Table 3 and Table 4 report the confusion matrices computed on the test set."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1810.08683",
      "figure_id": "1810.08683_fig_4",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
      "image_filename": "1810.08683_page0_fig3.jpg",
      "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .",
      "context_before": "",
      "context_after": "[Section: 6 DISCUSSION]\n\nWe have presented two novel, but related, ideas in this work. Firstly, to resolve the tension between accuracy gains obtained by using a sensitive feature as part of the model, and the potential inapplicability of such an approach, we have suggested to first predict the sensitive feature based on the non-sensitive features, and then use the predicted value in the functional form of a model, allowing to treat people belonging to different groups, but having similar non-sensitive features, equally. Furthermore, we have demonstrated how the predicted sensitive feature can then used in a fairness constrained MTL framework. We confirmed the validity of the above approach empirically, giving us substantial improvements in both accuracy and fairness, compared to STL and ITL. We believe this to be a fruitful area of possible future research. Of course, a non-linear extension of the above framework would be interesting to study, although we did not notice any substantial improvements on the Adult and COMPAS datasets considered in this work. Moreover, it would be interesting to see if the above framework can be extended to include other fairness definitions, apart from the EOp and EOd\n\nTable 7: Results when group specific models are trained with ITL and MTL with or without the sensitive feature as predictor and with or without the fairness constraint.   \n\n<table><tr><td rowspan=\"3\" colspan=\"2\"></td><td colspan=\"31\">Adult Dataset</td><td></td></tr><tr><td>-01</td><td>-1</td><td>-1</td><td>-1</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"2\">MTL</td><td colspan=\"2\">STLITL</td><td colspan=\"4\">MTL</td><td></td><td></td></tr><tr><td>P</td><td>D</td><td>F</td><td>S</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp+</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOp-</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOd</td><td>ACC</td><td>DEOd</td><td>ACC</td><td></td></tr><tr><td rowspan=\"3\">G</td><td>0</td><td>1</td><td>0</td><td>0</td><td></td><td>74.5</td><td>0.18</td><td>90.0</td><td>0.14</td><td></td><td>74.7</td><td>0.15</td><td>91.0</td><td>0.13</td><td></td><td>74.6</td><td>0.17</td><td>90.2</td><td>0.14</td><td></td><td>70.7</td><td>0.19</td><td>84.5</td><td>0.15</td><td>70.9</td><td>0.17</td><td>84.4</td><td>0.14</td><td>70.8</td><td>0.16</td><td>83.6</td><td>0.13</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td></td><td>69.7</td><td>0.08</td><td>88.3</td><td>0.04</td><td></td><td>69.9</td><td>0.07</td><td>89.2</td><td>0.04</td><td></td><td>69.8</td><td>0.08</td><td>88.5</td><td>0.04</td><td></td><td>66.1</td><td>0.08</td><td>83.0</td><td>0.04</td><td>66.3</td><td>0.08</td><td>82.8</td><td>0.04</td><td>66.2</td><td>0.07</td><td>82.1</td><td>0.04</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td></td><td>69.7</td><td>0.08</td><td>88.1</td><td>0.03</td><td></td><td>69.9</td><td>0.07</td><td>89.1</td><td>0.03</td><td></td><td>69.8</td><td>0.08</td><td>88.3</td><td>0.03</td><td></td><td>66.1</td><td>0.09</td><td>82.9</td><td>0.07</td><td>66.3</td><td>0.08</td><td>82.8</td><td>0.06</td><td>66.2</td><td>0.08</td><td>82.1</td><td>0.06</td><td></td></tr><tr><td rowspan=\"3\">R</td><td>0</td><td>1</td><td>0</td><td>0</td><td></td><td>67.4</td><td>0.13</td><td>91.8</td><td>0.10</td><td></td><td>67.6</td><td>0.11</td><td>92.8</td><td>0.08</td><td></td><td>67.5</td><td>0.13</td><td>92.0</td><td>0.10</td><td></td><td>67.3</td><td>0.12</td><td>91.7</td><td>0.08</td><td>67.5</td><td>0.11</td><td>92.7</td><td>0.07</td><td>67.4</td><td>0.12</td><td>92.0</td><td>0.08</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td></td><td>62.5</td><td>0.05</td><td>90.0</td><td>0.03</td><td></td><td>62.7</td><td>0.05</td><td>90.9</td><td>0.03</td><td></td><td>62.6</td><td>0.05</td><td>90.2</td><td>0.03</td><td></td><td>62.4</td><td>0.07</td><td>90.1</td><td>0.02</td><td>62.6</td><td>0.06</td><td>91.0</td><td>0.02</td><td>62.5</td><td>0.07</td><td>90.3</td><td>0.02</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td></td><td>62.6</td><td>0.06</td><td>90.4</td><td>0.03</td><td></td><td>62.7</td><td>0.05</td><td>91.3</td><td>0.03</td><td></td><td>62.6</td><td>0.06</td><td>90.6</td><td>0.03</td><td></td><td>62.4</td><td>0.07</td><td>90.0</td><td>0.03</td><td>62.5</td><td>0.07</td><td>91.0</td><td>0.03</td><td>62.4</td><td>0.07</td><td>90.2</td><td>0.03</td><td></td></tr><tr><td rowspan=\"3\">G+r</td><td>0</td><td>1</td><td>0</td><td>0</td><td></td><td>64.0</td><td>0.23</td><td>91.5</td><td>0.15</td><td></td><td>64.2</td><td>0.20</td><td>92.2</td><td>0.15</td><td></td><td>64.1</td><td>0.22</td><td>91.8</td><td>0.15</td><td></td><td>64.2</td><td>0.24</td><td>91.4</td><td>0.16</td><td>64.3</td><td>0.21</td><td>92.2</td><td>0.16</td><td>64.3</td><td>0.22</td><td>91.7</td><td>0.16</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td></td><td>59.3</td><td>0.14</td><td>89.8</td><td>0.05</td><td></td><td>59.4</td><td>0.12</td><td>90.6</td><td>0.05</td><td></td><td>59.3</td><td>0.13</td><td>90.1</td><td>0.05</td><td></td><td>59.2</td><td>0.13</td><td>90.1</td><td>0.05</td><td>59.4</td><td>0.11</td><td>90.8</td><td>0.05</td><td>59.3</td><td>0.12</td><td>90.4</td><td>0.05</td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td></td><td>59.2</td><td>0.13</td><td>90.0</td><td>0.05</td><td></td><td>59.4</td><td>0.11</td><td>90.8</td><td>0.05</td><td></td><td>59.3</td><td>0.12</td><td>90.3</td><td>0.05</td><td></td><td>59.4</td><td>0.13</td><td>89.9</td><td>0.05</td><td>59.5</td><td>0.11</td><td>90.7</td><td>0.05</td><td>59.5</td><td>0.12</td><td>90.3</td><td>0.05</td><td></td></tr></table>\n\nTable 8: Adult dataset: ACC of STL, ITL, and MTL when $P { = } 0$ , $F { = } 0$ , and $S { = } 0$ .   \n\n<table><tr><td rowspan=\"2\">Sens.</td><td rowspan=\"2\">Group</td><td colspan=\"2\">D=0</td><td colspan=\"2\">D=1</td></tr><tr><td>STL</td><td>MTL</td><td>ITL</td><td>MTL</td></tr><tr><td rowspan=\"2\">G</td><td>M</td><td>85.4</td><td>88.5</td><td>78.8</td><td>92.8</td></tr><tr><td>F</td><td>81.2</td><td>85.9</td><td>74.2</td><td>91.0</td></tr><tr><td rowspan=\"5\">R</td><td>W</td><td>86.7</td><td>89.8</td><td>89.7</td><td>93.2</td></tr><tr><td>B</td><td>83.5</td><td>88.9</td><td>83.5</td><td>92.8</td></tr><tr><td>API</td><td>82.3</td><td>87.9</td><td>65.2</td><td>92.1</td></tr><tr><td>AIE</td><td>82.1</td><td>87.6</td><td>48.5</td><td>92.0</td></tr><tr><td>O</td><td>81.2</td><td>86.9</td><td>47.5</td><td>92.1</td></tr><tr><td rowspan=\"10\">G+R</td><td>W&amp;M</td><td>87.8</td><td>92.8</td><td>85.8</td><td>94.7</td></tr><tr><td>W&amp;F</td><td>85.6</td><td>89.5</td><td>84.7</td><td>93.2</td></tr><tr><td>B&amp;M</td><td>84.4</td><td>89.9</td><td>66.3</td><td>93.2</td></tr><tr><td>B&amp;F</td><td>82.4</td><td>88.1</td><td>64.6</td><td>92.1</td></tr><tr><td>API&amp;M</td><td>83.6</td><td>89.2</td><td>67.3</td><td>93.0</td></tr><tr><td>API&amp;F</td><td>81.8</td><td>88.0</td><td>63.5</td><td>92.8</td></tr><tr><td>AIE&amp;M</td><td>83.0</td><td>88.8</td><td>50.2</td><td>92.7</td></tr><tr><td>AIE&amp;F</td><td>81.9</td><td>87.3</td><td>45.1</td><td>92.5</td></tr><tr><td>O&amp;M</td><td>81.7</td><td>88.3</td><td>50.7</td><td>93.1</td></tr><tr><td>O&amp;F</td><td>81.1</td><td>86.6</td><td>43.3</td><td>92.1</td></tr></table>\n\nthat we have tested. Finally, it would be valuable to provide theoretical conditions on the data distribution for which our approach provably works.\n\n[Section: ACKNOWLEDGMENTS]",
      "referring_paragraphs": [
        "The results for all possible combinations described above, are reported in Table 5.",
        "This hypothesis is also supported by the results of Figure 5, in which we check how the accuracy and fairness, as measured with the EOd, varies by varying .",
        "The fairness constraints do\n\n<table><tr><td rowspan=\"3\" colspan=\"2\"></td><td colspan=\"101\">Adult Dataset</td></tr><tr><td>-1</td><td>01</td><td>-1-</td><td>-1</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td colspan=\"3\">STLITL</td><td colspan=\"3\">MTL</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>P</td><td>D</td><td>F</td><td>S</td><td colspan=\"3\">ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"2\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp+</td><td>ACC</td><td colspan=\"3\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td>ACC</td><td colspan=\"2\">DEOp-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"16\">G</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.2</td><td colspan=\"3\">0.11</td><td>83.4</td><td>0.13</td><td>80.4</td><td colspan=\"3\">0.09</td><td>84.3</td><td colspan=\"3\">0.12</td><td>80.3</td><td colspan=\"2\">0.10</td><td>83.6</td><td>0.13</td><td>76.1</td><td colspan=\"3\">0.15</td><td>78.1</td><td colspan=\"3\">0.12</td><td>76.3</td><td colspan=\"3\">0.14</td><td>78.0</td><td colspan=\"3\">0.11</td><td>76.2</td><td colspan=\"3\">0.13</td><td>77.3</td><td colspan=\"2\">0.10</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.3</td><td colspan=\"3\">0.14</td><td>83.9</td><td>0.13</td><td>83.5</td><td colspan=\"3\">0.12</td><td>84.8</td><td colspan=\"3\">0.12</td><td>83.4</td><td colspan=\"2\">0.13</td><td>84.1</td><td>0.13</td><td>79.3</td><td colspan=\"3\">0.15</td><td>79.2</td><td colspan=\"3\">0.13</td><td>79.5</td><td colspan=\"3\">0.14</td><td>79.1</td><td colspan=\"3\">0.12</td><td>79.4</td><td colspan=\"3\">0.13</td><td>78.4</td><td colspan=\"2\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.7</td><td colspan=\"3\">0.03</td><td>81.8</td><td>0.06</td><td>75.8</td><td colspan=\"3\">0.02</td><td>82.7</td><td colspan=\"3\">0.05</td><td>75.7</td><td colspan=\"2\">0.03</td><td>82.0</td><td>0.06</td><td>71.5</td><td colspan=\"3\">0.03</td><td>76.5</td><td colspan=\"3\">0.03</td><td>71.7</td><td colspan=\"3\">0.03</td><td>76.4</td><td colspan=\"3\">0.03</td><td>71.6</td><td colspan=\"3\">0.03</td><td>75.7</td><td colspan=\"2\">0.03</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>78.6</td><td colspan=\"3\">0.06</td><td>82.4</td><td>0.04</td><td>78.8</td><td colspan=\"3\">0.05</td><td>83.3</td><td colspan=\"3\">0.04</td><td>78.7</td><td colspan=\"2\">0.05</td><td>82.6</td><td>0.04</td><td>74.4</td><td colspan=\"3\">0.05</td><td>77.4</td><td colspan=\"3\">0.05</td><td>74.6</td><td colspan=\"3\">0.05</td><td>77.3</td><td colspan=\"3\">0.04</td><td>74.5</td><td colspan=\"3\">0.05</td><td>76.6</td><td colspan=\"2\">0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>0</td><td>74.5</td><td colspan=\"3\">0.18</td><td>90.0</td><td>0.14</td><td>74.7</td><td colspan=\"3\">0.15</td><td>91.0</td><td colspan=\"3\">0.13</td><td>74.6</td><td colspan=\"2\">0.17</td><td>90.2</td><td>0.14</td><td>70.7</td><td colspan=\"3\">0.19</td><td>84.5</td><td colspan=\"3\">0.15</td><td>70.9</td><td colspan=\"3\">0.17</td><td>84.4</td><td colspan=\"3\">0.14</td><td>70.8</td><td colspan=\"3\">0.16</td><td>83.6</td><td colspan=\"2\">0.13</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>74.6</td><td colspan=\"3\">0.17</td><td>89.7</td><td>0.14</td><td>74.7</td><td colspan=\"3\">0.15</td><td>90.7</td><td colspan=\"3\">0.13</td><td>74.7</td><td colspan=\"2\">0.16</td><td>90.0</td><td>0.14</td><td>70.9</td><td colspan=\"3\">0.19</td><td>84.5</td><td colspan=\"3\">0.14</td><td>71.1</td><td colspan=\"3\">0.18</td><td>84.4</td><td colspan=\"3\">0.13</td><td>71.0</td><td colspan=\"3\">0.16</td><td>83.6</td><td colspan=\"2\">0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>69.7</td><td colspan=\"3\">0.08</td><td>88.3</td><td>0.04</td><td>69.9</td><td colspan=\"3\">0.07</td><td>89.2</td><td colspan=\"3\">0.04</td><td>69.8</td><td colspan=\"2\">0.08</td><td>88.5</td><td>0.04</td><td>66.1</td><td colspan=\"3\">0.08</td><td>83.0</td><td colspan=\"3\">0.04</td><td>66.3</td><td colspan=\"3\">0.08</td><td>82.8</td><td colspan=\"3\">0.04</td><td>66.2</td><td colspan=\"3\">0.07</td><td>82.1</td><td colspan=\"2\">0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>69.7</td><td colspan=\"3\">0.08</td><td>88.1</td><td>0.03</td><td>69.9</td><td colspan=\"3\">0.07</td><td>89.1</td><td colspan=\"3\">0.03</td><td>69.8</td><td colspan=\"2\">0.08</td><td>88.3</td><td>0.03</td><td>66.1</td><td colspan=\"3\">0.09</td><td>82.9</td><td colspan=\"3\">0.07</td><td>66.3</td><td colspan=\"3\">0.08</td><td>82.8</td><td colspan=\"3\">0.06</td><td>66.2</td><td colspan=\"3\">0.08</td><td>82.1</td><td colspan=\"2\">0.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>78.4</td><td colspan=\"3\">0.09</td><td>82.3</td><td>0.09</td><td>78.6</td><td colspan=\"3\">0.07</td><td>83.2</td><td colspan=\"3\">0.09</td><td>78.5</td><td colspan=\"2\">0.08</td><td>82.5</td><td>0.09</td><td>74.6</td><td colspan=\"3\">0.12</td><td>77.3</td><td colspan=\"3\">0.10</td><td>74.8</td><td colspan=\"3\">0.11</td><td>77.2</td><td colspan=\"3\">0.09</td><td>74.7</td><td colspan=\"3\">0.10</td><td>76.5</td><td colspan=\"2\">0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>81.7</td><td colspan=\"3\">0.13</td><td>83.1</td><td>0.08</td><td>81.9</td><td colspan=\"3\">0.11</td><td>84.0</td><td colspan=\"3\">0.07</td><td>81.8</td><td colspan=\"2\">0.12</td><td>83.3</td><td>0.08</td><td>77.6</td><td colspan=\"3\">0.13</td><td>78.1</td><td colspan=\"3\">0.09</td><td>77.8</td><td colspan=\"3\">0.12</td><td>78.0</td><td colspan=\"3\">0.09</td><td>77.7</td><td colspan=\"3\">0.11</td><td>77.3</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>73.7</td><td colspan=\"3\">0.02</td><td>80.7</td><td>0.01</td><td>73.9</td><td colspan=\"3\">0.02</td><td>81.6</td><td colspan=\"3\">0.01</td><td>73.8</td><td colspan=\"2\">0.02</td><td>80.9</td><td>0.01</td><td>70.1</td><td colspan=\"3\">0.03</td><td>75.9</td><td colspan=\"3\">0.01</td><td>70.3</td><td colspan=\"3\">0.03</td><td>75.8</td><td colspan=\"3\">0.01</td><td>70.2</td><td colspan=\"3\">0.03</td><td>75.1</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>76.8</td><td colspan=\"3\">0.03</td><td>81.5</td><td>0.01</td><td>77.0</td><td colspan=\"3\">0.03</td><td>82.4</td><td colspan=\"3\">0.01</td><td>76.9</td><td colspan=\"2\">0.03</td><td>81.7</td><td>0.01</td><td>73.1</td><td colspan=\"3\">0.05</td><td>76.7</td><td colspan=\"3\">0.01</td><td>73.3</td><td colspan=\"3\">0.05</td><td>76.6</td><td colspan=\"3\">0.01</td><td>73.2</td><td colspan=\"3\">0.04</td><td>75.9</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>73.0</td><td colspan=\"3\">0.14</td><td>89.1</td><td>0.09</td><td>73.2</td><td colspan=\"3\">0.12</td><td>90.1</td><td colspan=\"3\">0.08</td><td>73.1</td><td colspan=\"2\">0.13</td><td>89.3</td><td>0.09</td><td>69.3</td><td colspan=\"3\">0.17</td><td>83.7</td><td colspan=\"3\">0.09</td><td>69.5</td><td colspan=\"3\">0.15</td><td>83.6</td><td colspan=\"3\">0.08</td><td>69.4</td><td colspan=\"3\">0.14</td><td>82.8</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>0</td><td>72.8</td><td colspan=\"3\">0.15</td><td>88.9</td><td>0.10</td><td>73.0</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.09</td><td>72.9</td><td colspan=\"2\">0.14</td><td>89.1</td><td>0.10</td><td>69.3</td><td colspan=\"3\">0.15</td><td>83.7</td><td colspan=\"3\">0.10</td><td>69.5</td><td colspan=\"3\">0.14</td><td>83.6</td><td colspan=\"3\">0.09</td><td>69.4</td><td colspan=\"3\">0.13</td><td>82.9</td><td colspan=\"2\">0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>68.0</td><td colspan=\"3\">0.06</td><td>87.4</td><td>0.01</td><td>68.2</td><td colspan=\"3\">0.05</td><td>88.3</td><td colspan=\"3\">0.01</td><td>68.1</td><td colspan=\"2\">0.05</td><td>87.6</td><td>0.01</td><td>64.7</td><td colspan=\"3\">0.06</td><td>82.3</td><td colspan=\"3\">0.01</td><td>64.9</td><td colspan=\"3\">0.05</td><td>82.1</td><td colspan=\"3\">0.01</td><td>64.8</td><td colspan=\"3\">0.05</td><td>81.4</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>68.0</td><td colspan=\"3\">0.06</td><td>87.4</td><td>0.01</td><td>68.1</td><td colspan=\"3\">0.05</td><td>88.3</td><td colspan=\"3\">0.01</td><td>68.1</td><td colspan=\"2\">0.06</td><td>87.6</td><td>0.01</td><td>64.6</td><td colspan=\"3\">0.06</td><td>82.1</td><td colspan=\"3\">0.01</td><td>64.8</td><td colspan=\"3\">0.06</td><td>82.0</td><td colspan=\"3\">0.01</td><td>64.7</td><td colspan=\"3\">0.05</td><td>81.3</td><td colspan=\"2\">0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"14\">R</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.3</td><td colspan=\"3\">0.08</td><td>84.2</td><td>0.07</td><td>80.5</td><td colspan=\"3\">0.07</td><td>85.1</td><td colspan=\"3\">0.06</td><td>80.4</td><td colspan=\"2\">0.08</td><td>84.4</td><td>0.07</td><td>80.2</td><td colspan=\"3\">0.09</td><td>84.2</td><td colspan=\"3\">0.08</td><td>80.4</td><td colspan=\"3\">0.08</td><td>85.1</td><td colspan=\"3\">0.07</td><td>80.3</td><td colspan=\"3\">0.09</td><td>84.4</td><td colspan=\"2\">0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.2</td><td colspan=\"3\">0.09</td><td>85.3</td><td>0.09</td><td>83.4</td><td colspan=\"3\">0.08</td><td>86.2</td><td colspan=\"3\">0.08</td><td>83.3</td><td colspan=\"2\">0.09</td><td>85.5</td><td>0.09</td><td>83.2</td><td colspan=\"3\">0.10</td><td>84.9</td><td colspan=\"3\">0.08</td><td>83.4</td><td colspan=\"3\">0.09</td><td>85.8</td><td colspan=\"3\">0.07</td><td>83.3</td><td colspan=\"3\">0.10</td><td>85.1</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.3</td><td colspan=\"3\">0.02</td><td>82.6</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.02</td><td>83.5</td><td colspan=\"3\">0.01</td><td>75.4</td><td colspan=\"2\">0.02</td><td>82.8</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.04</td><td>82.4</td><td colspan=\"3\">0.03</td><td>75.7</td><td colspan=\"3\">0.04</td><td>83.3</td><td colspan=\"3\">0.03</td><td>75.6</td><td colspan=\"3\">0.04</td><td>82.6</td><td>0.03</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>78.4</td><td colspan=\"3\">0.03</td><td>83.4</td><td>0.03</td><td>78.6</td><td colspan=\"3\">0.03</td><td>84.3</td><td colspan=\"3\">0.02</td><td>78.5</td><td colspan=\"2\">0.03</td><td>83.6</td><td>0.03</td><td>78.5</td><td colspan=\"3\">0.05</td><td>83.5</td><td colspan=\"3\">0.02</td><td>78.7</td><td colspan=\"3\">0.04</td><td>84.4</td><td colspan=\"3\">0.02</td><td>78.6</td><td colspan=\"3\">0.05</td><td>83.7</td><td>0.02</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>0</td><td>67.4</td><td colspan=\"3\">0.13</td><td>91.8</td><td>0.10</td><td>67.6</td><td colspan=\"3\">0.11</td><td>92.8</td><td colspan=\"3\">0.08</td><td>67.5</td><td colspan=\"2\">0.13</td><td>92.0</td><td>0.10</td><td>67.3</td><td colspan=\"3\">0.12</td><td>91.7</td><td colspan=\"3\">0.08</td><td>67.5</td><td colspan=\"3\">0.11</td><td>92.7</td><td colspan=\"3\">0.07</td><td>67.4</td><td colspan=\"3\">0.12</td><td>92.0</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>67.2</td><td colspan=\"3\">0.13</td><td>91.8</td><td>0.08</td><td>67.4</td><td colspan=\"3\">0.12</td><td>92.8</td><td colspan=\"3\">0.07</td><td>67.3</td><td colspan=\"2\">0.13</td><td>92.1</td><td>0.08</td><td>67.4</td><td colspan=\"3\">0.13</td><td>91.8</td><td colspan=\"3\">0.09</td><td>67.5</td><td colspan=\"3\">0.11</td><td>92.8</td><td colspan=\"3\">0.08</td><td>67.4</td><td colspan=\"3\">0.13</td><td>92.0</td><td>0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>62.5</td><td colspan=\"3\">0.05</td><td>90.0</td><td>0.03</td><td>62.7</td><td colspan=\"3\">0.05</td><td>90.9</td><td colspan=\"3\">0.03</td><td>62.6</td><td colspan=\"2\">0.05</td><td>90.2</td><td>0.03</td><td>62.4</td><td colspan=\"3\">0.07</td><td>90.1</td><td colspan=\"3\">0.02</td><td>62.6</td><td colspan=\"3\">0.06</td><td>91.0</td><td colspan=\"3\">0.02</td><td>62.5</td><td>0.07</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>62.6</td><td colspan=\"3\">0.06</td><td>90.4</td><td>0.03</td><td>62.7</td><td colspan=\"3\">0.05</td><td>91.3</td><td colspan=\"3\">0.03</td><td>62.6</td><td colspan=\"2\">0.06</td><td>90.6</td><td>0.03</td><td>62.4</td><td colspan=\"3\">0.07</td><td>90.0</td><td colspan=\"3\">0.03</td><td>62.5</td><td colspan=\"3\">0.07</td><td>91.0</td><td colspan=\"3\">0.03</td><td>62.4</td><td>0.07</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>78.5</td><td colspan=\"3\">0.07</td><td>83.2</td><td>0.04</td><td>78.7</td><td colspan=\"3\">0.06</td><td>84.1</td><td colspan=\"3\">0.04</td><td>78.6</td><td colspan=\"2\">0.07</td><td>83.4</td><td>0.04</td><td>78.4</td><td colspan=\"3\">0.08</td><td>83.3</td><td colspan=\"3\">0.06</td><td>78.6</td><td colspan=\"3\">0.07</td><td>84.2</td><td colspan=\"3\">0.05</td><td>78.5</td><td>0.08</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>67.1</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td>77.3</td><td colspan=\"3\">0.01</td><td>83.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"2\">0.01</td><td>82.7</td><td>0.01</td><td>77.0</td><td colspan=\"3\">0.02</td><td>82.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"3\">0.01</td><td>83.2</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>77.1</td><td colspan=\"3\">0.01</td><td>82.5</td><td>0.01</td><td>77.3</td><td colspan=\"3\">0.01</td><td>83.4</td><td colspan=\"3\">0.01</td><td>77.2</td><td colspan=\"2\">0.01</td><td>82.7</td><td>0.01</td><td>75.5</td><td colspan=\"3\">0.12</td><td>90.8</td><td colspan=\"3\">0.05</td><td>65.7</td><td colspan=\"3\">0.11</td><td>91.8</td><td colspan=\"3\">0.05</td><td>65.6</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>65.8</td><td colspan=\"3\">0.12</td><td>90.8</td><td>0.06</td><td>66.0</td><td colspan=\"3\">0.11</td><td>91.8</td><td colspan=\"3\">0.05</td><td>65.9</td><td colspan=\"2\">0.12</td><td>91.0</td><td>0.06</td><td>65.7</td><td colspan=\"3\">0.12</td><td>90.8</td><td colspan=\"3\">0.07</td><td>65.8</td><td colspan=\"3\">0.11</td><td>91.7</td><td colspan=\"3\">0.07</td><td>65.7</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>61.2</td><td colspan=\"3\">0.06</td><td>89.3</td><td>0.01</td><td>61.3</td><td colspan=\"3\">0.05</td><td>90.3</td><td colspan=\"3\">0.01</td><td>61.2</td><td colspan=\"2\">0.06</td><td>89.5</td><td>0.01</td><td>60.8</td><td colspan=\"3\">0.05</td><td>89.2</td><td colspan=\"3\">0.01</td><td>61.0</td><td colspan=\"3\">0.05</td><td>60.9</td><td colspan=\"3\">0.01</td><td>60.9</td><td>0.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>60.8</td><td colspan=\"3\">0.06</td><td>89.2</td><td>0.01</td><td>61.0</td><td colspan=\"3\">0.05</td><td>90.2</td><td colspan=\"3\">0.01</td><td>60.9</td><td colspan=\"2\">0.06</td><td>89.4</td><td>0.01</td><td>60.9</td><td colspan=\"3\">0.04</td><td>89.0</td><td colspan=\"3\">0.01</td><td>61.1</td><td colspan=\"3\">0.04</td><td>89.9</td><td colspan=\"3\">0.01</td><td>61.0</td><td>0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"14\">G-R</td><td>0</td><td>0</td><td>0</td><td>0</td><td>80.2</td><td colspan=\"3\">0.16</td><td>84.6</td><td>0.14</td><td>80.4</td><td colspan=\"3\">0.14</td><td>85.3</td><td colspan=\"3\">0.14</td><td>80.3</td><td colspan=\"2\">0.15</td><td>84.9</td><td>0.14</td><td>80.2</td><td colspan=\"3\">0.16</td><td>84.8</td><td colspan=\"3\">0.14</td><td>80.4</td><td colspan=\"3\">0.14</td><td>85.5</td><td colspan=\"3\">0.14</td><td>80.3</td><td>0.15</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>83.1</td><td colspan=\"3\">0.18</td><td>85.7</td><td>0.16</td><td>83.4</td><td colspan=\"3\">0.16</td><td>86.4</td><td colspan=\"3\">0.16</td><td>83.3</td><td colspan=\"2\">0.17</td><td>86.0</td><td>0.16</td><td>83.3</td><td colspan=\"3\">0.18</td><td>85.5</td><td colspan=\"3\">0.16</td><td>83.5</td><td colspan=\"3\">0.15</td><td>86.2</td><td colspan=\"3\">0.16</td><td>83.4</td><td>0.16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>75.2</td><td colspan=\"3\">0.05</td><td>83.2</td><td>0.04</td><td>75.3</td><td colspan=\"3\">0.04</td><td>83.9</td><td colspan=\"3\">0.04</td><td>75.3</td><td colspan=\"2\">0.05</td><td>83.5</td><td>0.04</td><td>75.3</td><td colspan=\"3\">0.05</td><td>83.1</td><td colspan=\"3\">0.05</td><td>75.5</td><td colspan=\"3\">0.04</td><td>83.8</td><td colspan=\"3\">0.05</td><td>75.4</td><td>0.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>78.5</td><td colspan=\"3\">0.05</td><td>83.9</td><td>0.05</td><td>78.7</td><td colspan=\"3\">0.04</td><td>84.6</td><td colspan=\"3\">0.05</td><td>78.6</td><td colspan=\"2\">0.05</td><td>84.2</td><td>0.05</td><td>78.6</td><td colspan=\"3\">0.06</td><td>84.1</td><td colspan=\"3\">0.04</td><td>78.8</td><td colspan=\"3\">0.05</td><td>84.7</td><td colspan=\"3\">0.04</td><td>78.7</td><td>0.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>0</td><td>0</td><td>64.0</td><td colspan=\"3\">0.23</td><td>91.5</td><td>0.15</td><td>64.2</td><td colspan=\"3\">0.20</td><td>92.2</td><td colspan=\"3\">0.15</td><td>64.1</td><td colspan=\"2\">0.22</td><td>91.8</td><td>0.15</td><td>64.2</td><td colspan=\"3\">0.24</td><td>91.4</td><td colspan=\"3\">0.16</td><td>64.3</td><td colspan=\"3\">0.21</td><td>92.2</td><td colspan=\"3\">0.16</td><td>64.3</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>63.9</td><td colspan=\"3\">0.24</td><td>91.7</td><td>0.16</td><td>64.0</td><td colspan=\"3\">0.21</td><td>92.4</td><td colspan=\"3\">0.16</td><td>64.0</td><td colspan=\"2\">0.23</td><td>92.0</td><td>0.16</td><td>64.1</td><td colspan=\"3\">0.23</td><td>91.5</td><td colspan=\"3\">0.15</td><td>64.3</td><td colspan=\"3\">0.20</td><td>92.2</td><td colspan=\"3\">0.15</td><td>64.2</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>59.3</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.05</td><td>59.4</td><td colspan=\"3\">0.12</td><td>90.6</td><td colspan=\"3\">0.12</td><td>59.3</td><td colspan=\"2\">0.13</td><td>90.1</td><td>0.05</td><td>59.2</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.05</td><td>59.5</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.05</td><td>59.5</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>59.2</td><td colspan=\"3\">0.13</td><td>90.0</td><td>0.15</td><td>59.4</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.15</td><td>59.3</td><td colspan=\"2\">0.12</td><td>90.3</td><td>0.05</td><td>59.2</td><td colspan=\"3\">0.13</td><td>83.8</td><td colspan=\"3\">0.09</td><td>78.6</td><td colspan=\"3\">0.12</td><td>84.5</td><td>0.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>58.5</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.11</td><td>58.7</td><td colspan=\"3\">0.11</td><td>84.6</td><td colspan=\"3\">0.12</td><td>78.6</td><td colspan=\"2\">0.12</td><td>84.2</td><td>0.12</td><td>78.4</td><td colspan=\"3\">0.13</td><td>82.5</td><td colspan=\"3\">0.11</td><td>78.9</td><td colspan=\"3\">0.12</td><td>83.4</td><td>0.12</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.9</td><td colspan=\"3\">0.14</td><td>82.3</td><td>0.01</td><td>73.9</td><td colspan=\"3\">0.01</td><td>83.0</td><td colspan=\"3\">0.11</td><td>73.8</td><td colspan=\"2\">0.11</td><td>82.6</td><td>0.01</td><td>73.6</td><td colspan=\"3\">0.02</td><td>82.5</td><td colspan=\"3\">0.01</td><td>73.8</td><td colspan=\"3\">0.02</td><td>83.1</td><td>0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>56.8</td><td colspan=\"3\">0.04</td><td>83.1</td><td>0.01</td><td>76.9</td><td colspan=\"3\">0.04</td><td>83.8</td><td colspan=\"3\">0.01</td><td>76.8</td><td colspan=\"2\">0.04</td><td>83.4</td><td>0.01</td><td>76.8</td><td colspan=\"3\">0.04</td><td>83.2</td><td colspan=\"3\">0.01</td><td>77.0</td><td colspan=\"3\">0.04</td><td>76.8</td><td>0.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>62.5</td><td colspan=\"3\">0.13</td><td>89.8</td><td>0.11</td><td>59.4</td><td colspan=\"3\">0.11</td><td>90.8</td><td colspan=\"3\">0.12</td><td>59.3</td><td colspan=\"2\">0.12</td><td>90.3</td><td>0.12</td><td>59.2</td><td colspan=\"3\">0.13</td><td>89.9</td><td colspan=\"3\">0.11</td><td>59.5</td><td colspan=\"3\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.9</td><td colspan=\"3\">0.14</td><td>89.8</td><td>0.11</td><td>57.9</td><td colspan=\"3\">0.18</td><td>89.9</td><td colspan=\"3\">0.12</td><td>57.8</td><td colspan=\"2\">0.19</td><td>89.9</td><td>0.12</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td colspan=\"3\">0.11</td><td>57.8</td><td colspan=\"3\">0.19</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td>0.12</td><td>57.9</td><td colspan=\"3\">0.19</td><td>89.8</td><td colspan=\"3\">0.11</td><td>57.8</td><td colspan=\"2\">0.19</td><td>89.9</td><td>0.12</td><td>57.7</td><td colspan=\"3\">0.10</td><td>89.9</td><td colspan=\"3\">0.11</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 5: Complete results set.",
        "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ ."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1810.08683_page0_fig4.jpg",
        "1810.08683_page0_fig5.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig5.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1811.00103": [
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg",
      "image_filename": "1811.00103_page0_fig0.jpg",
      "caption": "",
      "context_before": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations. We show on several real-world data sets, PCA has higher reconstruction error on population $A$ than on $B$ (for example, women versus men or lower- versus higher-educated individuals). This can happen even when the data set has a similar number of samples from $A$ and $B$ . This motivates our study of dimensionality reduction techniques which maintain similar fidelity for $A$ and $B$ . We define the notion of Fair PCA and give a polynomial-time algorithm for finding a low dimensional representation of the data which is nearly-optimal with respect to this measure. Finally, we show on real-world data sets that our algorithm can be used to efficiently generate a fair low dimensional representation of the data.\n\n[Section: 1 Introduction]\n\nIn recent years, the ML community has witnessed an onslaught of charges that real-world machine learning algorithms have produced “biased” outcomes. The examples come from diverse and impactful domains. Google Photos labeled African Americans as gorillas [Twitter, 2015; Simonite, 2018] and returned queries for CEOs with images overwhelmingly male and white [Kay et al., 2015], searches for African American names caused the display of arrest record advertisements with higher frequency than searches for white names [Sweeney, 2013], facial recognition has wildly different accuracy for white men than dark-skinned women [Buolamwini and Gebru, 2018], and recidivism prediction software has labeled low-risk African Americans as high-risk at higher rates than low-risk white people [Angwin et al., 2018].\n\nThe community’s work to explain these observations has roughly fallen into either “biased data” or “biased algorithm” bins. In some cases, the training data might under-represent (or over-represent) some group, or have noisier labels for one population than another, or use an imperfect proxy for the prediction label (e.g., using arrest records in lieu of whether a crime was committed). Separately, issues of imbalance and bias might occur due to an algorithm’s behavior, such as focusing on accuracy across the entire distribution rather than guaranteeing similar false positive rates across populations, or by improperly accounting for confirmation bias and feedback loops in data collection. If an algorithm fails to distribute loans or bail to a deserving population, the algorithm won’t receive additional data showing those people would have paid back the loan, but it will continue to receive more data about the populations it (correctly) believed should receive loans or bail.\n\nMany of the proposed solutions to “biased data” problems amount to re-weighting the training set or adding noise to some of the labels; for “biased algorithms”, most work has focused on maximizing accuracy subject to a constraint forbidding (or penalizing) an unfair model. Both of these concerns",
      "context_after": "Average reconstruction error (RE) of PCA on LFW",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {}
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg",
      "image_filename": "1811.00103_page0_fig1.jpg",
      "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).",
      "context_before": "Average reconstruction error (RE) of PCA on LFW",
      "context_after": "Average reconstruction error (RE) of PCA on LFW (resampled)   \nFigure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).\n\nand approaches have significant merit, but form an incomplete picture of the ML pipeline and where unfairness might be introduced therein. Our work takes another step in fleshing out this picture by analyzing when dimensionality reduction might inadvertently introduce bias. We focus on principal component analysis (henceforth PCA), perhaps the most fundamental dimensionality reduction technique in the sciences [Pearson, 1901; Hotelling, 1933; Jolliffe, 1986]. We show several real-world data sets for which PCA incurs much higher average reconstruction error for one population than another, even when the populations are of similar sizes. Figure 1 shows that PCA on labeled faces in the wild data set (LFW) has higher reconstruction error for women than men even if male and female faces are sampled with equal weight.\n\nThis work underlines the importance of considering fairness and bias at every stage of data science, not only in gathering and documenting a data set [Gebru et al., 2018] and in training a model, but also in any interim data processing steps. Many scientific disciplines have adopted PCA as a default preprocessing step, both to avoid the curse of dimensionality and also to do exploratory/explanatory data analysis (projecting the data into a number of dimensions that humans can more easily visualize). The study of human biology, disease, and the development of health interventions all face both aforementioned difficulties, as do numerous economic and financial analysis. In such high-stakes settings, where statistical tools will help in making decisions that affect a diverse set of people, we must take particular care to ensure that we share the benefits of data science with a diverse community.\n\nWe also emphasize this work has implications for representational rather than just allocative harms, a distinction drawn by Crawford [2017] between how people are represented and what goods or opportunities they receive. Showing primates in search results for African Americans is repugnant primarily due to its representing and reaffirming a racist painting of African Americans, not because it directly reduces any one person’s access to a resource. If the default template for a data set begins with running PCA, and PCA does a better job representing men than women, or white people over minorities, the new representation of the data set itself may rightly be considered an unacceptable sketch of the world it aims to describe.\n\nOur work proposes a different linear dimensionality reduction which aims to represent two populations $A$ and $B$ with similar fidelity—which we formalize in terms of reconstruction error. Given an $n$ -dimensional data set and its $d$ -dimensional approximation, the reconstruction error of the data with respect to its low-dimensional approximation is the sum of squares of distances between the original data points and their approximated points in the $d$ -dimensional subspace. To eliminate the effect of size of a population, we focus on average reconstruction error over a population. One possible objective for our goal would find a $d$ -dimensional approximation of the data which minimizes the maximum reconstruction error over the two populations. However, this objective doesn’t avoid grappling with the fact that population $A$ may perfectly embed into $d$ dimensions, whereas $B$ might require many more dimensions to have low reconstruction error. In such cases, this objective would not necessarily favor a solution with average reconstruction error of $\\epsilon$ for $A$ and $y \\gg \\epsilon$ for $B$ over one with $y$ error for $A$ and $y$ error for $B$ . This holds even if $B$ requires $y$ reconstruction error to be embedded into $d$ dimensions and thus the first solution is nearly optimal for both populations in $d$ dimensions.\n\nThis motivates our focus on finding a projection which minimizes the maximum additional or marginal reconstruction error for each population above the optimal $n$ into $d$ projection for that population alone. This quantity captures how much a population’s reconstruction error increases by including another population in the dimensionality reduction optimization. Despite this computational problem appearing more difficult than solving “vanilla” PCA, we introduce a polynomial-time algorithm which finds an $n$ into $( d + 1 )$ -dimensional embedding with objective value better than any $d$ -dimensional embedding. Furthermore, we show that optimal solutions have equal additional average error for populations $A$ and $B$ .\n\nSummary of our results We show PCA can overemphasize the reconstruction error for one population over another (equally sized) population, and we should therefore think carefully about dimensionality reduction in domains where we care about fair treatment of different populations. We propose a new dimensionality reduction problem which focuses on representing $A$ and $B$ with similar additional error over projecting $A$ or $B$ individually. We give a polynomial-time algorithm which finds near-optimal solutions to this problem. Our algorithm relies on solving a semidefinite program (SDP), which can be prohibitively slow for practical applications. We note that it is possible to (approximately) solve an SDP with a much faster multiplicative-weights style algorithm, whose running time in practice is equivalent to solving standard PCA at most 10-15 times. The details of the algorithm are given in the full version of this work. We then evaluate the empirical performance of this algorithm on several human-centric data sets.\n\n[Section: 2 Related work]\n\nThis work contributes to the area of fairness for machine learning models, algorithms, and data representations. One interpretation of our work is that we suggest using Fair PCA, rather than PCA, when creating a lower-dimensional representation of a data set for further analysis. Both pieces of work which are most relevant to our work take the posture of explicitly trying to reduce the correlation between a sensitive attribute (such as race or gender) and the new representation of the data. The first piece is a broad line of work [Zemel et al., 2013; Beutel et al., 2017; Calmon et al., 2017; Madras et al., 2018; Zhang et al., 2018] that aims to design representations which will be conditionally independent of the protected attribute, while retaining as much information as possible (and particularly task-relevant information for some fixed classification task). The second piece is the work by Olfat and Aswani [2018], who also look to design PCA-like maps which reduce the projected data’s dependence on a sensitive attribute. Our work has a qualitatively different goal: we aim not to hide a sensitive attribute, but instead to maintain as much information about each population after projecting the data. In other words, we look for representation with similar richness for population $A$ as $B$ , rather than making $A$ and $B$ indistinguishable.\n\nOther work has developed techniques to obfuscate a sensitive attribute directly [Pedreshi et al., 2008; Kamiran et al., 2010; Calders and Verwer, 2010; Kamiran and Calders, 2011; Luong et al., 2011; Kamiran et al., 2012; Kamishima et al., 2012; Hajian and Domingo-Ferrer, 2013; Feldman et al., 2015; Zafar et al., 2015; Fish et al., 2016; Adler et al., 2016]. This line of work diverges from ours in two ways. First, these works focus on representations which obfuscate the sensitive attribute rather than a representation with high fidelity regardless of the sensitive attribute. Second, most of these works do not give formal guarantees on how much an objective will degrade after their transformations. Our work directly minimizes the amount by which each group’s marginal reconstruction error increases.\n\nMuch of the other work on fairness for learning algorithms focuses on fairness in classification or scoring [Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2016; Chouldechova, 2017], or online learning settings [Joseph et al., 2016; Kannan et al., 2017; Ensign et al., 2017b,a]. These works focus on either statistical parity of the decision rule, or equality of false positives or negatives, or an algorithm with a fair decision rule. All of these notions are driven by a single learning task rather than a generic transformation of a data set, while our work focuses on a ubiquitous, task-agnostic preprocessing step.",
      "referring_paragraphs": [
        "Average reconstruction error (RE) of PCA on LFW (resampled)   \nFigure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender.",
        "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women on the LFW data set."
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(a) The best one dimensional PCA projection for group $A$ is vector $( 1 , 0 )$ and for group $B$ it is vector $( 0 , 1 )$ .",
        "1811.00103_page0_fig2.jpg",
        "(b) Group $B$ has a perfect one-dimensional projection. For group $A$ , any one-dimensional projection is equally bad.",
        "1811.00103_page0_fig3.jpg",
        "1811.00103_page0_fig4.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig4.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_6",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg",
      "image_filename": "1811.00103_page0_fig5.jpg",
      "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.",
      "context_before": "where $U _ { A _ { i } }$ are matrices with rows corresponding to rows of $U$ for groups $A _ { i }$ .\n\nTheorem 5.2. There is a polynomial-time algorithm to find a projection such that it is of dimension at most $d + k - 1$ and achieves the optimal fairness objective value for dimension $d$ .\n\nIn contrast to the case of two groups, when there are more than two groups in the data, it is possible that all optimal solutions to fair PCA will not assign the same loss to all groups. However, with $k - 1$ extra dimensions, we can ensure that the loss of each group remains at most the optimal fairness objective in $d$ dimension. The result of Theorem 5.2 follows by extending algorithm in Theorem 5.1 by adding linear constraints to SDP and LP for each extra group. An extreme solution $( { \\bar { \\lambda } } , z ^ { * } )$ of the resulting LP contains at most $k$ of $\\lambda _ { i }$ ’s that are strictly in between 0 and 1. Therefore, the final projection matrix $P ^ { * }$ has rank at most $d + k - 1$ .\n\nRuntime We now analyze the runtime of Algorithm 1, which consists of solving SDP (4) and finding an extreme solution to an LP (5)-(9). The SDP and LP can be solved up to additive error of $\\epsilon >$ 0 in the objective value in ${ \\cal O } ( n ^ { 6 . 5 } \\log ( 1 / \\epsilon ) )$ [Ben-Tal and Nemirovski, 2001] and ${ \\cal O } ( n ^ { 3 . 5 } \\log ( 1 / \\epsilon ) )$ [Schrijver, 1998] time, respectively. The running time of SDP dominates the algorithm both in theory and practice, and is too slow for practical uses for moderate size of $n$ .\n\nWe propose another algorithm of solving SDP using the multiplicative weight (MW) update method. In theory, our MW takes $O ( \\textstyle { \\frac { 1 } { \\epsilon ^ { 2 } } } )$ iterations of solving standard PCA, giving a total of $\\bar { O ( \\frac { n ^ { 3 } } { \\epsilon ^ { 2 } } ) }$ runtime, which may or may not be faster than ${ \\cal O } ( n ^ { 6 . 5 } \\log ( 1 / \\epsilon ) )$ depending on $n , \\epsilon$ . In practice, however, we observe that after appropriately tuning one parameter in MW, the MW algorithm achieves accuracy $\\epsilon < 1 0 ^ { - 5 }$ within tens of iterations, and therefore is used to obtain experimental results in this paper. Our MW can handle data of dimension up to a thousand with running time in less than a minute. The details of implementation and analysis of MW method are in Appendix A.\n\n[Section: 6 Experiments]\n\nWe use two common human-centric data sets for our experiments. The first one is labeled faces in the wild (LFW) [Huang et al., 2007], the second is the Default Credit data set [Yeh and Lien, 2009]. We preprocess all data to have its mean at the origin. For the LFW data, we normalized each pixel value by $\\scriptstyle { \\frac { 1 } { 2 5 5 } }$ . The gender information for LFW was taken from Afifi and Abdelhamed [2017], who manually verified the correctness of these labels. For the credit data, since different attributes are measurements of incomparable units, we normalized the variance of each attribute to be equal to 1. The code of all experiments is publicly available at https://github.com/samirasamadi/Fair-PCA.\n\nResults We focus on projections into relatively few dimensions, as those are used ubiquitously in early phases of data exploration. As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women on the LFW data set. This gap is at the scale of up to $10 \\%$ of the total reconstruction error when we project to 20 dimensions. This still holds when we subsample male and female faces with equal probability from the data set, and so men and women have equal magnitude in the objective function of PCA (Figure 1 right).\n\nFigure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data. As we expect,",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data.",
        "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1811.00103_page0_fig6.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig6.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1811.00103",
      "figure_id": "1811.00103_fig_8",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg",
      "image_filename": "1811.00103_page0_fig7.jpg",
      "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.",
      "context_before": "",
      "context_after": "[Section: Proof of Lemma 4.8:]\n\nWe prove that the value of function $g _ { A }$ at its local minima is equal to its value at its global minimum, which we know is the subspace spanned by a top $d$ eigenvectors of $A ^ { T } A$ . More precisely, we prove the following: Let $\\{ v _ { 1 } , \\ldots , v _ { n } \\}$ be an orthonormal basis of eigenvectors of $A ^ { T } A$ with corresponding eigenvalues $\\lambda _ { 1 } \\geq \\lambda _ { 2 } \\geq . . . \\geq \\lambda _ { n }$ where ties are broken arbitrarily. Let $V ^ { * }$ be the subspace spanned by $\\{ v _ { 1 } , \\ldots , v _ { d } \\}$ and let $U$ be some $d$ -dimensional subspace s.t. $g _ { A } ( U ) > g _ { A } ( V ^ { * } )$ . There is a continuous path from $U$ to $V ^ { * }$ s.t. the value of $g _ { A }$ is monotonically decreasing for every $d$ -dimensional subspace on the path.\n\nBefore starting the proof, we will make a couple of notes which would be used throughout the proof. First note that $g _ { A } ( V )$ is well-defined i.e., the value of $g _ { A } ( V )$ is only a function of the subspace $V$ . More precisely, ${ \\dot { g _ { A } } } ( V )$ is invariant with respect to different choices of orthonormal basis of the subspace $V$ . Second, given Lemma 4.7, $g _ { A } ( \\dot { V } ) = \\| A \\| _ { F } ^ { 2 } - \\textstyle \\sum _ { i } \\| A v _ { i } \\| ^ { 2 }$ . Therefore, proving that $g _ { A } ( V )$ is decreasing is equivalent to proving that $\\textstyle \\sum _ { i } \\| A v _ { i } \\| ^ { 2 }$ is increasing as a function of any choice of orthonormal basis of the subspaces on the path.\n\n$g _ { A } ( U ) > g _ { A } ( V ^ { * } )$ therefore $U \\neq V ^ { * }$ . Let $k$ be the smallest index such that $v _ { k } \\notin U$ . Extend $\\{ v _ { 1 } , \\dotsc , v _ { k - 1 } \\}$ to an orthonormal basis of $U$ : $\\{ v _ { 1 } , \\ldots , v _ { k - 1 } , v _ { k } ^ { \\prime } , \\ldots , v _ { d } ^ { \\prime } \\}$ . Let $q \\geq k$ be the smallest index such that $\\| A v _ { q } \\| ^ { 2 } > \\| A v _ { q } ^ { \\prime } \\| ^ { 2 }$ . Such an index $q$ must exist given that $g _ { A } ( U ) > g _ { A } ( V ^ { * } )$ . Without loss of generality we can assume that $q = 1$ (this will be clear throughout the proof). Therefore, we assume that $v _ { 1 }$ , the top eigenvector of $A ^ { T } A$ , is not in $U$ and that it strictly maximizes the function $\\| A u \\| ^ { 2 }$ over the space of unit vectors $u$ . Specifically, for any unit vector $u \\in U$ , $\\| A u \\| ^ { 2 } < \\| A \\ddot { v } _ { 1 } \\| ^ { 2 } = \\lambda _ { 1 }$ .\n\nLet $v _ { 1 } = \\sqrt { 1 - a ^ { 2 } } z _ { 1 } + a z _ { 2 }$ where $z _ { 1 } \\in U$ and $z _ { 2 } \\perp U$ , $\\| z _ { 1 } \\| = \\| z _ { 2 } \\| = 1$ i.e., the projection of $v _ { 1 }$ to $U$ is $\\sqrt { 1 - a ^ { 2 } } z _ { 1 }$ . We distinguish two cases:\n\nCase $z _ { 1 } = 0$ . $v _ { 1 } \\perp U$ . Let $w = \\sqrt { 1 - \\epsilon ^ { 2 } } u _ { 1 } + \\epsilon v _ { 1 }$ . $\\lVert \\boldsymbol { w } \\rVert = 1$ . Note that $\\{ w , u _ { 2 } , \\ldots , u _ { d } \\}$ is an orthonormal set of vectors. We set $U _ { \\epsilon } = s p a n \\{ w , u _ { 2 } , \\ldots , u _ { d } \\}$ . We show that $g _ { A } ( U _ { \\epsilon } ) < g _ { A } ( U )$ . Using the formulation of $g$ from Lemma 4.7, we need to show that $\\| A w \\| ^ { 2 } + \\| A u _ { 2 } \\| ^ { 2 } + . . . + \\| A \\bar { u } _ { d } \\| ^ { 2 } >$ $\\| A \\bar { u _ { 1 } } \\| ^ { 2 } + \\| A u _ { 2 } \\| ^ { 2 } + . . . + \\| A u _ { d } \\| ^ { 2 }$ or equivalently that $\\| A w \\| ^ { 2 } > \\| A u _ { 1 } \\| ^ { 2 }$ .\n\nwhere $u _ { 1 } ^ { T } A ^ { T } A v _ { 1 } = u _ { 1 } ^ { T } ( \\lambda _ { 1 } v _ { 1 } ) = \\lambda _ { 1 } u _ { 1 } ^ { T } v _ { 1 } = 0$ since $v _ { 1 }$ is an eigenvector of $A ^ { T } A$ and $v _ { 1 } \\perp u _ { 1 }$ . This, and considering the fact that $\\| A u _ { 1 } \\| ^ { 2 } < \\lambda _ { 1 }$",
      "referring_paragraphs": [
        "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1811.00103_page0_fig8.jpg",
        "1811.00103_page0_fig9.jpg",
        "1811.00103_page0_fig10.jpg",
        "1811.00103_page0_fig11.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig11.jpg"
        ],
        "panel_count": 5
      }
    }
  ],
  "1811.03654": [
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg",
      "image_filename": "1811.03654_page0_fig0.jpg",
      "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .",
      "context_before": "In this study, our motivation is to investigate how information on an individual task-specific feature (i.e., the candidates’ loan repayment rate) influences perceptions of fairness. We present participants with a scenario in which two individuals have each applied for a loan. The participants know no personal information about the two individuals except their loan repayment rates. We choose three allocation rules, described in the following paragraphs, that allow us to formulate qualitative judgments regarding the three fairness definitions.\n\n[Section: Procedure]\n\nWe recruited 200 participants from Amazon Mechanical Turk (MTurk) on March 18-19, 2018. The majority of them identified themselves as white $(82 \\% )$ , $8 \\%$ as black, $6 \\%$ as Asian or Asian-American, $2 \\%$ as Hispanic, and the rest with multiple races. The average age was 39.43 $( \\mathrm { S D } = 1 2 . 4 7 )$ ). Most $( 9 1 \\% )$ had attended some college, while almost all other participants had a high school degree or GED. (All demographic information was self-reported.) All participants were U.S. residents, and each were paid $\\$ 0.20$ for participating.\n\nWe presented participants with the scenario presented in Figure 4 in the appendix.\n\nThis experiment employed a between-subjects design with four conditions. We varied the individual candidates’ similarity (dissimilarity) in ability to pay back their loan (i.e., their loan repayment rate), as an operationalization of task-specific similarity (dissimilarity) relevant to the three fairness definitions. Participants were randomly shown one of four loan repayment rates: $55 \\%$ and $50 \\%$ (Treatment 1), $70 \\%$ and $40 \\%$ (Treatment 2), $90 \\%$ and $10 \\%$ (Treatment 3), and $100 \\%$ and $20 \\%$ (Treatment 4). One treatment had a very small difference between the loan repayment rates of the two candidates (Treatment 1). The next treatment had a larger difference between the loan repayment rates (Treatment 2), with the next two treatments (Treatments 3 and 4) having a much larger difference in their loan repayment rates. Each participant was only shown one Treatment.\n\nWe held all other information about the two candidates constant. We then presented participants with three possible decisions for how to allocate the money between the two individuals. The order of the three decisions was counterbalanced.\n\nEach decision was designed to help us to untangle the three fairness definitions.\n\n“All A” Decision. Give all the money to the candidate with the higher payback rate. This decision is allowed in all treatments under meritocratic fairness as defined Joseph et al. (2016), where a worse applicant is never favored over a\n\nbetter one. It would also be allowed under the definition formulated by Dwork et al. (2012), in the more extreme treatments, and even in every treatment in the case that the similarity metric was very discerning. This decision would not be allowed in any treatment under the calibrated fairness definition (Liu et al. 2017).\n\n“Equal” Decision. Split the money 50/50 between the candidates, giving $\\$ 23,400$ to each. This decision is allowed in all treatments under Dwork et al. (2012) – treating similar people similarly. Moreover, under their definition, when two individuals are deemed to be similar to each other, then this is the textitonly allowable decision (in Treatment 1, for example). This decision is also allowed in all the treatments under the meritocratic definition (Joseph et al. 2016), as the candidate with the higher loan repayment rate is given at least as much as the other candidate, and, hence, is weakly favored. The decision, however, would not be allowed in any treatment under calibrated fairness (Liu et al. 2017), since the candidates are not being treated in proportion of their quality (loan repayment rate).\n\n“Ratio” Decision. Split the money between two candidates in proportion of their loan repayment rates. This decision is allowed in all treatments under calibrated fairness, where resources are divided in proportion to the true quality of the candidates. Moreover, this is the only decision allowed under this definition. This decision could also align with the definition proposed by Dwork et al. (2012), but only for suitably defined similarity metrics that allow the distance between decisions implied by the ratio allocation. Finally, this decision would be allowed under meritocratic fairness (Joseph et al. 2016) for the same reasons as the “Equal” decision. Namely, the candidate with the higher loan repayment rate is weakly favored to the other candidate.\n\nIt is important to note that we are testing human perceptions regarding the outcomes that different fairness definition allow, not the definitions themselves. However, if a certain definition allows multiple decisions, then we would expect these decisions to receive similar support. Where the perception of the fairness of outcomes is inconsistent with the allowable decisions for a rule, this is worthwhile to understand.\n\nIf it is true that participants most prefer the treating similar people similarly definition, one would expect that they would prefer the “Equal” decision to the other two decisions for a wider range of similarity metrics and treatments. If it is true that participants most prefer the meritocratic definition, one would expect no significant difference in support for the three different decisions. If it is true that participants most prefer the calibrated fairness definition, one would expect that the “Ratio” decision is perceived as more fair than the other two decisions.\n\nWe formulated the following set of hypotheses:\n\nHypothesis 1A. Across all treatments, participants perceive the “Ratio” decision as more fair than the “Equal” decision.\n\nHypothesis 1B. Across all treatments, participants perceive the “Ratio” decision as more fair than the “All A” decision. Furthermore, we made the following predictions:\n\nHypothesis 2. Participants perceive the “Equal” decision as more fair than the “All A” decision in Treatment 1. That\n\nis, participants may view the candidates in Treatment 1 as “similar enough” to be treated similarly.\n\nHypothesis 3. Participants perceive the “All A” decision as more fair than the “Equal” decision in Treatments 3 and 4.",
      "context_after": "[Section: Results]\n\nFirst, we tested hypotheses H1A and H1B, which conjecture that participants will consider the “Ratio” decision as the most fair. We found evidence in support of H1A in all treatments: participants consistently rated dividing the $\\$ 50,000$ between the two individuals in proportion of their loan repayment rates (the “Ratio” decision) as more fair than splitting the $\\$ 50,000$ equally (the “Equal” decision) (see Figure 1). We found partial support for H1B: participants rated the “Ratio” decision as more fair than the “All A” decision in Treatments 1 and 2 (see Figure 1).\n\nSecond, we found that participants in Treatment 1 rated the “Equal” decision as more fair than the “All A” definition (see Figure 1), supporting H2. We see that when the difference in the loan repayment rates of the individuals was small $( 5 \\% )$ , participants perceived the decision to divide the money equally between the individuals as more fair than giving all the money to the individual with the higher loan repayment rate.\n\nThird, we found that participants rated the “All A” decision as more fair than the “Equal” decision in Treatment 3, but not in Treatment 4 (see Figure 1).\n\n[Section: Discussion]",
      "referring_paragraphs": [
        "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .",
        "We found evidence in support of H1A in all treatments: participants consistently rated dividing the $\\$ 50,000$ between the two individuals in proportion of their loan repayment rates (the “Ratio” decision) as more fair than splitting the $\\$ 50,000$ equally (the “Equal” decision) (see Figure 1)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg",
      "image_filename": "1811.03654_page0_fig1.jpg",
      "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
      "context_before": "In this study, our motivation is to investigate how the addition of sensitive information to information on an individual task-specific feature (i.e., the candidates’ loan repayment rate) influences perceptions of fairness.\n\nWe employed the same experimental paradigm as in Study 1, presenting participants with the scenario of two individuals applying for a loan, and three possible ways of allocating the loan money. Importantly, in Study 2, in addition to providing information on the individuals’ loan repayment rates, we also provided information on the individuals’ race. We investigate how information on the candidates’ loan repayment rates and the candidates’ race influence people’s fairness judgments of the three allocation decisions.\n\n[Section: Procedure]\n\nWe recruited a separate sample of 1800 participants from Amazon Mechanical Turk (MTurk) on April 20-21, 2018, none of whom had taken part in Study 1. Most of them identified as white $( 7 4 \\% )$ , $9 \\%$ as black, $7 \\%$ as Asian or Asian-American, $5 \\%$ as Hispanic, and the rest with multiple races. The average age was 36.97 $\\mathrm { S D } = 1 2 . 5 4 ,$ ). Most $( 8 9 \\% )$ had attended some college, while almost all other participants had a high school degree or GED. All participants were U.S. residents, and each was paid $\\$ 0.20$ for participating. (All demographic information was self-reported.)\n\nWe presented participants with the same scenario as in Study 1, but this time also providing the candidates’ race and gender. We held the gender of the candidates constant (both were male), and randomized race (black or white). Thus, either the white candidate had the higher loan repayment rate, or the black candidate had the higher loan repayment rate.\n\nThe question presented to the participants in Study 2 can be found in Figure Figure 5 in the appendix.\n\nWe presented the same question and choices for loan allocations, and tested the same hypotheses, as in Study 1.",
      "context_after": "[Section: Results]\n\nWe found that participants viewed the “Ratio” decision as more fair than the “Equal” decision in Treatments 2, 3, and 4, regardless of race, in support of H1A. We also found that participants viewed the “Ratio” decision as more fair than the “All A” decision in all treatments, regardless of race, thus supporting H1B. (See Figures 2 and 3.) Thus, participants in Study 2 consistently gave most support to the decision to divide the $\\$ 50,000$ between the two individuals in proportion to their loan repayment rates.\n\nFurthermore, we found that participants viewed the “Equal” decision as more fair than the “All A” decision in Treatment 1, regardless of race, in support of H2 (see Figures 2 and 3). Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2).\n\nWhen the difference between the two candidates’ repay-\n\nment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3). By contrast, when the candidate with the higher loan repayment rate was white, participants did not rate the two decisions differently (see Figure 2).",
      "referring_paragraphs": [
        "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
        "Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg",
      "image_filename": "1811.03654_page0_fig2.jpg",
      "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .",
      "context_before": "[Section: Results]\n\nWe found that participants viewed the “Ratio” decision as more fair than the “Equal” decision in Treatments 2, 3, and 4, regardless of race, in support of H1A. We also found that participants viewed the “Ratio” decision as more fair than the “All A” decision in all treatments, regardless of race, thus supporting H1B. (See Figures 2 and 3.) Thus, participants in Study 2 consistently gave most support to the decision to divide the $\\$ 50,000$ between the two individuals in proportion to their loan repayment rates.\n\nFurthermore, we found that participants viewed the “Equal” decision as more fair than the “All A” decision in Treatment 1, regardless of race, in support of H2 (see Figures 2 and 3). Participants also rated the “Equal” decision as more fair than the “All A” decision in Treatment 2, but only when the candidate with the higher repayment rate was white (see Figure 2).\n\nWhen the difference between the two candidates’ repay-\n\nment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3). By contrast, when the candidate with the higher loan repayment rate was white, participants did not rate the two decisions differently (see Figure 2).",
      "context_after": "[Section: Discussion]\n\nIn Study 2, we tested whether participants’ perceptions of these three fairness definitions could be influenced by additional information regarding the candidates’ race.\n\nOur results show that participants perceived the “Ratio” decision to be more fair than the other two, hence supporting the results from Study 1 and the related discussion. These results are not dependent on the race attribute. Furthermore, regardless of race, when the difference between the loan repayment rates was small (Treatment 1), participants preferred the “Equal” decision to the “All A” decision. This supports the corresponding results from Study 1, Treatment 1, which indicate that one should account for similarity of individuals when designing fair rules.\n\nHowever, we also found evidence that race does affect participants’ perception of fairness. When the difference in\n\nloan repayment rates was larger (Treatments 3 and 4), participants rated the “All A” decision as more fair than the “Equal” decision, but only when the candidate with the higher repayment rate was black. These results suggest a boundary condition of H3: people may support giving all the loan money to the candidate with the higher payback rate, compared to splitting the money equally, when the candidate with the higher payback rate is a member of a group that is historically disadvantaged.\n\nEach definition, from meritocratic to similarity to calibrated fairness is successively stronger in our context, ruling out additional decisions. In this light, it is interesting that the ratio decision is most preferred, providing support for the calibrated fairness definition, even though this definition is the strongest of the three in the present context. When historically disadvantaged individuals have a higher repayment rate, participants are more supportive of more decisive allocations in favor of the stronger, and historically disadvantaged, individual.\n\n[Section: Conclusion]",
      "referring_paragraphs": [
        "When the difference between the two candidates’ repay-\n\nment rates was larger (Treatments 3 and 4), participants viewed the “All A” decision as more fair than the “Equal” decision but only when the candidate with the higher repayment rate was black (see Figure 3).",
        "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_4",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg",
      "image_filename": "1811.03654_page0_fig3.jpg",
      "caption": "Figure 6: Age distribution of the participants in Study 1.",
      "context_before": "To be eligible to take our surveys, the Amazon Mechanical Turk workers had to be located in the United States of America. We stipulated this restriction via TurkPrime, which is a platform for performing crowdsourced research when using Amazon Mechanical Turk.\n\nAmazon Mechanical Turk workers (’MTurker’) could only participate in on one of the two studies, and not both.\n\nThe first section contains all questions the workers were asked in the studies. The second section contains the demographics data for the respondents of both studies. The third section details how many participants in each study were shown the four treatments.\n\n[Section: Questions from the studies]\n\nThe question asked in Study 1 is presented in Figure 4. The question asked in Study 2 is presented in Figure 5.\n\nThere are two candidates - Person A and Person B,they are identical in every way, except that Person A has a loan repayment rate of 100%,while Person B has a loan repayment rate of20%.Both of them have applied for a $5o,ooo loan to start a business,and the loan officer only has $50.000.\n\nTo what extent do you think the following decisions are fair? For each decision, please indicate how fair you think the decision is by dragging the slider bar to a point on the line,where 1 means \"not fair at all,and 9 means \"completely fair\"\n\nNot fair at all Completely fair 1 8 9 The loan officer has decided to give all the money (S5o,ooo) to Person A\n\nThe loan offcer has decided to split the money 50/50 between the two candidates,giving $25,000 to PersonAand$25,000 toPersonB.\n\nThe loan oficer has decided to give Person A $41,666, which is proportional to that person's payback rateof10o%,and give PersonBS,33,which isproportionaltothat person'spayback rateof 20%.\n\nFigure 4: Question presented to the participants in Study 1.\n\nThere are two candidates - Person Aand Person B,they are identical in every way,except their race and loan repayment rates .Both of them have applied fora $5o,oo loan to start a business,and the loan offcer only has $50,000.\n\n<table><tr><td></td><td>Person A</td><td>Person B</td></tr><tr><td>Gender</td><td>Male</td><td>Male</td></tr><tr><td>Race</td><td>White</td><td>Black</td></tr><tr><td>Individual loan repayment rate</td><td>70%</td><td>40%</td></tr><tr><td>Amount requested</td><td>$50,000</td><td>$50,000</td></tr></table>\n\nTo what extent do you think the following decisions are fair? For each decision, please indicate how fair you think the decision is by dragging the slider bar to a point on the line,where 1 means \"not fair at all\",and 9 means \"completely fair\".\n\nNot fair at all Completely fair 1 2 3 4 5 6 7 8 9 The loan officer has decided to split the money 50/50 between the two candidates,giving $25,000 toPerson Aand$25,000toPersonB.\n\nThe loan officer has decided to give Person A $31,818,which is proportional to that person's payback rate of 70%,and give Person B $18,181, which is proportional to that person's payback rateof $4 0 \\%$\n\nThe loan officer has decided to give all the money ($5o,ooo) to Person A\n\nFigure 5: Question presented to the participants in Study 2.\n\nDemographics questions Every Amazon Mechanical Turk worker in both the surveys was asked that study’s question. After they answered that, they were asked the following demographics questions that were voluntary to answer. Since they were voluntary to answer, not all respondents answered them, although most $( > 9 0 \\% )$ answered some or all of them.\n\n1. What state do you live in?   \n2. Do you identify as:\n\n$\\bigcirc$ Male   \n$\\bigcirc$ Female   \n$\\bigcirc$ Other (please specify):\n\n3. What is the highest level of school you have completed or the highest degree you have received?\n\n$\\bigcirc$ Less than high school degree   \n$\\bigcirc$ High school degree or equivalent   \n$\\bigcirc$ Some college but no degree   \n$\\bigcirc$ Associate degree   \n$\\bigcirc$ Bachelor degree   \n$\\bigcirc$ Graduate degree\n\n4. Do you identify as:\n\nSpanish, Hispanic, or Latino   \nWhite   \nBlack or African-American   \nAmerican-Indian or Alaskan Native   \nAsian   \nAsian-American\n\nNative Hawaiian or other Pacific Islander   \n2 Other (please specify):\n\n5. In what type of community do you live:\n\n2 City or urban community   \nSuburban community   \n2 Rural community   \nOther (please specify):\n\n6. What is your age?\n\n7. Which political party do you identify with?\n\nDemocratic Party   \n2 Republican Party   \n2 Green Party   \nLibertarian Party   \n2 Independent   \nOther (please specify):",
      "context_after": "Study 1: Demographic information of the participants   \nFigure 6: Age distribution of the participants in Study 1.",
      "referring_paragraphs": [
        "Study 1: Demographic information of the participants   \nFigure 6: Age distribution of the participants in Study 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_5",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig4.jpg",
      "image_filename": "1811.03654_page0_fig4.jpg",
      "caption": "Figure 7: Education of the participants in Study 1.",
      "context_before": "Study 1: Demographic information of the participants   \nFigure 6: Age distribution of the participants in Study 1.",
      "context_after": "[Section: Education]",
      "referring_paragraphs": [
        "Figure 7: Education of the participants in Study 1."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_6",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg",
      "image_filename": "1811.03654_page0_fig5.jpg",
      "caption": "Figure 8: Gender breakdown of the participants in Study 1.",
      "context_before": "[Section: Education]",
      "context_after": "Gender   \nFigure 8: Gender breakdown of the participants in Study 1.",
      "referring_paragraphs": [
        "Gender   \nFigure 8: Gender breakdown of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_7",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig6.jpg",
      "image_filename": "1811.03654_page0_fig6.jpg",
      "caption": "Figure 9: Political Affiliation of the participants in Study 1.",
      "context_before": "Gender   \nFigure 8: Gender breakdown of the participants in Study 1.",
      "context_after": "[Section: Political Affiliation]",
      "referring_paragraphs": [
        "Figure 9: Political Affiliation of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_8",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg",
      "image_filename": "1811.03654_page0_fig7.jpg",
      "caption": "Figure 11: Breakup by state of the participants in Study 1.",
      "context_before": "[Section: Political Affiliation]",
      "context_after": "State breakup   \nFigure 11: Breakup by state of the participants in Study 1.",
      "referring_paragraphs": [
        "State breakup   \nFigure 11: Breakup by state of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_9",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig8.jpg",
      "image_filename": "1811.03654_page0_fig8.jpg",
      "caption": "Figure 10: Race of the participants in Study 1.",
      "context_before": "State breakup   \nFigure 11: Breakup by state of the participants in Study 1.",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 10: Race of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_10",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg",
      "image_filename": "1811.03654_page0_fig9.jpg",
      "caption": "Figure 12: Residential breakdown of the participants in Study 1.",
      "context_before": "",
      "context_after": "Residential Community   \nFigure 12: Residential breakdown of the participants in Study 1.",
      "referring_paragraphs": [
        "Residential Community   \nFigure 12: Residential breakdown of the participants in Study 1."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1811.03654_page0_fig10.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig10.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_12",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg",
      "image_filename": "1811.03654_page0_fig11.jpg",
      "caption": "Figure 13: Age distribution of the participants in Study 2.   ",
      "context_before": "Study 2: Demographic information of the participants   \nAge",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 13: Age distribution of the participants in Study 2.   \nEducation   \nFigure 14: Education of the participants in Study 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_13",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig12.jpg",
      "image_filename": "1811.03654_page0_fig12.jpg",
      "caption": "Figure 15: Gender breakdown of the participants in Study 2.",
      "context_before": "",
      "context_after": "Gender   \nFigure 15: Gender breakdown of the participants in Study 2.",
      "referring_paragraphs": [
        "Gender   \nFigure 15: Gender breakdown of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_14",
      "figure_number": 16,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg",
      "image_filename": "1811.03654_page0_fig13.jpg",
      "caption": "Figure 16: Political Affiliation of the participants in Study 2.",
      "context_before": "Gender   \nFigure 15: Gender breakdown of the participants in Study 2.",
      "context_after": "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.",
      "referring_paragraphs": [
        "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1811.03654",
      "figure_id": "1811.03654_fig_15",
      "figure_number": 17,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg",
      "image_filename": "1811.03654_page0_fig14.jpg",
      "caption": "Figure 17: Race of the participants in Study 2.   ",
      "context_before": "Political Affiliation   \nFigure 16: Political Affiliation of the participants in Study 2.",
      "context_after": "[Section: Race]",
      "referring_paragraphs": [
        "Figure 17: Race of the participants in Study 2.   \nResidential Community   \nFigure 18: Residential breakdown of the participants in Study 2."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1811.03654_page0_fig15.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig15.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1811.10104": [
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg",
      "image_filename": "1811.10104_page0_fig0.jpg",
      "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.",
      "context_before": "[Section: 2 HISTORY OF FAIRNESS IN TESTING]\n\n[Section: 2.1 1960s: Bias and Unfair Discrimination]\n\nConcerned with the fairness of tests for black and white students, T. Anne Cleary defined a quantitative measure of test bias for the",
      "context_after": "[Section: 2.2 1970s: Fairness]\n\nAs the 1960s turned to the 1970s, work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness. Following Thorndike [62], “The discussion of ‘fairness’ in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...” Thorndike’s sentiment was shared by other academics of the time, who, in examining the earlier work of Cleary, objected that it failed to take into account the differing false positive and false negative rates that occur when subgroups have different base rates (i.e.,  is not independent of ) [24, 62].\n\nA YWith the goal of moving beyond simplified models, Thorndike [62] proposed one of the first quantitative criteria for measuring test fairness. With this shift, Thorndike advocated for considering the contextual use of a test:\n\nA judgment on test-fairness must rest on the inferences that are made from the test rather than on a comparison of mean scores in the two populations. One must then focus attention on fair use of the test scores, rather than on the scores themselves.\n\nContrary to Cleary, Thorndike argued that sharing a common regression line is not important, as one can achieve fair selection goals by using different regression lines and different selection thresholds for the two groups.\n\nAs an alternative to Cleary, Thorndike proposed that the ratio of predicted positives to ground truth positives be equal for each group. Using confusion matrix terminology, this is equivalent to requiring that the ratio $( T P + F P ) / ( T P + F N )$ be equal for each T P FP T P F Nsubgroup. According to Thorndike, the situation in Figure 1a is fair for test cutoff $x ^ { * }$ . Figure 1b is unfair using any single threshold, but xfair if threshold $x _ { 1 } ^ { * }$ is used for group $\\pi _ { 1 }$ and threshold $x _ { 2 } ^ { * }$ is used for group $\\pi _ { 2 }$ .\n\nπSimilar to modern day ML fairness, e.g., Friedler et al. in 2016 [28], Thorndike also pointed out the tension between individual notions of fairness and group notions of fairness: “the two definitions of fairness—one based on predicted criterion score for individuals and the other on the distribution of criterion scores in the two groups— will always be in conflict.” The conflict was also raised by others in the period, including Sawyer et al. [57], in a foreshadowing of the compas debate of 2016:\n\nA conflict arises because the success maximization procedures based on individual parity do not produce equal opportunity (equal selection for equal success) based on group parity and the opportunity procedures do not produce success maximization (equal treatment for equal prediction) based on individual parity.\n\nAlmost as an aside, Thorndike mentions the existence of another regression line ignored by Cleary: the line that estimates the value of the test score $R$ given the target variable . This idea hints at the notion of equal opportunity for those with a given value of , an idea which soon was picked up by Darlington [19] and Cole [14].\n\nAt a glance, Cleary’s and Thorndike’s definitions are difficult to compare directly because of the different ways in which they’re defined. Darlington [19] helped to shed light on the relationship between Cleary and Thorndike’s conceptions of fairness by expressing them in a common formalism. He defines four fairness criteria in terms of the correlation $\\rho _ { A R }$ between the demographic variable ρand the test score. Following Darlington,\n\n(1) Cleary’s criterion can be restated in terms of correlations of the “culture variable” with test scores. If Cleary’s criterion holds for every subgroup, then $\\rho _ { A R } = \\rho _ { A Y } / \\rho _ { R Y }$ [63]. 2   \nρ ρ ρ(2) Similarly, Thorndike’s criterion is equivalent to requiring that $\\rho _ { A R } = \\rho _ { A Y }$ .   \nρ ρ(3) The criterion $\\rho _ { A R } = \\rho _ { A Y } \\times \\rho _ { R Y }$ is motivated by thinking about $R$ ρ ρ ρas a dependent variable affected by independent Rvariables $A$ and . If $A$ has no direct effect on $R$ once  is A Y A R Ytaken into account then we have a zero partial correlation, i.e. $\\rho _ { A R . Y } = 0 .$ 3 .   \nρ .(4) An alternative “starkly simple” criterion of $\\rho _ { A R } = 0$ (rec ognizable as modern day demographic parity [23]) is intro duced but not dwelt on.\n\nDarlington’s mapping of Cleary’s and Thorndike’s criteria lets him prove that they’re incompatible except in the special cases",
      "referring_paragraphs": [
        "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria.",
        "Cleary worked for Educational\n\nTable 1: Categories of Fairness Criteria   \n\n<table><tr><td>Category</td><td>Description</td></tr><tr><td>INDIVIDUAL</td><td>Fairness criterion defined purely in terms of individuals</td></tr><tr><td>NON-COMPARATIVE</td><td>Fairness criterion for each subgroup does not reference other subgroups</td></tr><tr><td>SUBGROUP PARITY</td><td>Fairness criterion defined in terms of parity of some value across subgroups</td></tr><tr><td>CORRELATION</td><td>Fairness criterion defined in terms of the correlation of the demographic variable with the model output</td></tr></table>\n\nTesting Services, and one can imagine a test being designed allowing for a range of use cases, since it may not be knowable in advance either i) the precise populations on which it will be deployed, nor ii) the number of students which an institution deploying the test is able to offer places to."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Labels on regression lines indicate which subgroup they fit.",
        "(b) The regression line labeled $\\pi _ { c }$ fits both subgroups separately (and hence also their union).",
        "1811.10104_page0_fig1.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg",
      "image_filename": "1811.10104_page0_fig2.jpg",
      "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)",
      "context_before": "[Section: 2.2 1970s: Fairness]\n\nAs the 1960s turned to the 1970s, work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness. Following Thorndike [62], “The discussion of ‘fairness’ in what has gone before is clearly oversimplified. In particular, it has been based upon the premise that the available criterion score is a perfectly relevant, reliable and unbiased measure...” Thorndike’s sentiment was shared by other academics of the time, who, in examining the earlier work of Cleary, objected that it failed to take into account the differing false positive and false negative rates that occur when subgroups have different base rates (i.e.,  is not independent of ) [24, 62].\n\nA YWith the goal of moving beyond simplified models, Thorndike [62] proposed one of the first quantitative criteria for measuring test fairness. With this shift, Thorndike advocated for considering the contextual use of a test:\n\nA judgment on test-fairness must rest on the inferences that are made from the test rather than on a comparison of mean scores in the two populations. One must then focus attention on fair use of the test scores, rather than on the scores themselves.\n\nContrary to Cleary, Thorndike argued that sharing a common regression line is not important, as one can achieve fair selection goals by using different regression lines and different selection thresholds for the two groups.\n\nAs an alternative to Cleary, Thorndike proposed that the ratio of predicted positives to ground truth positives be equal for each group. Using confusion matrix terminology, this is equivalent to requiring that the ratio $( T P + F P ) / ( T P + F N )$ be equal for each T P FP T P F Nsubgroup. According to Thorndike, the situation in Figure 1a is fair for test cutoff $x ^ { * }$ . Figure 1b is unfair using any single threshold, but xfair if threshold $x _ { 1 } ^ { * }$ is used for group $\\pi _ { 1 }$ and threshold $x _ { 2 } ^ { * }$ is used for group $\\pi _ { 2 }$ .\n\nπSimilar to modern day ML fairness, e.g., Friedler et al. in 2016 [28], Thorndike also pointed out the tension between individual notions of fairness and group notions of fairness: “the two definitions of fairness—one based on predicted criterion score for individuals and the other on the distribution of criterion scores in the two groups— will always be in conflict.” The conflict was also raised by others in the period, including Sawyer et al. [57], in a foreshadowing of the compas debate of 2016:\n\nA conflict arises because the success maximization procedures based on individual parity do not produce equal opportunity (equal selection for equal success) based on group parity and the opportunity procedures do not produce success maximization (equal treatment for equal prediction) based on individual parity.\n\nAlmost as an aside, Thorndike mentions the existence of another regression line ignored by Cleary: the line that estimates the value of the test score $R$ given the target variable . This idea hints at the notion of equal opportunity for those with a given value of , an idea which soon was picked up by Darlington [19] and Cole [14].\n\nAt a glance, Cleary’s and Thorndike’s definitions are difficult to compare directly because of the different ways in which they’re defined. Darlington [19] helped to shed light on the relationship between Cleary and Thorndike’s conceptions of fairness by expressing them in a common formalism. He defines four fairness criteria in terms of the correlation $\\rho _ { A R }$ between the demographic variable ρand the test score. Following Darlington,\n\n(1) Cleary’s criterion can be restated in terms of correlations of the “culture variable” with test scores. If Cleary’s criterion holds for every subgroup, then $\\rho _ { A R } = \\rho _ { A Y } / \\rho _ { R Y }$ [63]. 2   \nρ ρ ρ(2) Similarly, Thorndike’s criterion is equivalent to requiring that $\\rho _ { A R } = \\rho _ { A Y }$ .   \nρ ρ(3) The criterion $\\rho _ { A R } = \\rho _ { A Y } \\times \\rho _ { R Y }$ is motivated by thinking about $R$ ρ ρ ρas a dependent variable affected by independent Rvariables $A$ and . If $A$ has no direct effect on $R$ once  is A Y A R Ytaken into account then we have a zero partial correlation, i.e. $\\rho _ { A R . Y } = 0 .$ 3 .   \nρ .(4) An alternative “starkly simple” criterion of $\\rho _ { A R } = 0$ (rec ognizable as modern day demographic parity [23]) is intro duced but not dwelt on.\n\nDarlington’s mapping of Cleary’s and Thorndike’s criteria lets him prove that they’re incompatible except in the special cases",
      "context_after": "[Section: 2.3 Mid-1970s: The Fairness Tide Turns]\n\nImmediately after the the journal issue of 1976, research into quantitative definitions of test fairness seems to have come to a halt. Considering why this happened may be a valuable lesson to learn from for modern day fairness research. The same Cole who in 1973 proposed equality of TPR, wrote in 2001 that [15]:\n\nIn short, research over the last 30 or so years has not supplied any analyses to unequivocally indicate fairness or unfairness, nor has it produced clear procedures to avoid unfairness. To make matters worse, the views of fairness of the measurement profession and the views of the general public are often at odds.\n\nForeshadowing this outcome, statements from researchers in the 1970s indicate an increasing concern with how fairness criteria obscure “the fundamental problem, which is to find some rational basis for providing compensatory treatment for the disadvantaged” [48]. Following Peterson and Novick, the concepts of\n\nculture-fairness and group parity are not viable in practice, leading to models that can sanction the discrimination they seek to rectify [52]. They argue that fairness should be reconceptualized as a problem in maximizing expected utility [51], recognizing “high social utility in equalizing opportunity and reducing disadvantage” [48].\n\nA related thread of work highlights that different fairness criteria encode different value systems [34], and that quantitative techniques alone cannot answer the question of which to use. In 1971, Darlington [19] urges that the concept of “cultural fairness” be replaced by “cultural optimality”, which takes into account a policy-level question concerning the optimum balance between accuracy and cultural factors. In 1974, Thorndike points out that “one’s value system is deeply involved in one’s judgment as to what is ‘fair use’ of a selection device” [48]), and similarly, in 1976, Linn [44] draws attention to the fact that “Values are implicit in the models. To adequately address issues of values they need to be dealt with explicitly.” Hunter and Schmidt [34] begin to address this issue by bringing ethical theory to the discussion, relating fairness to theories of individualism and proportional representation. Current work may learn from this point in history by explicitly connecting fairness criteria to different cultural and social values.\n\n[Section: 2.4 1970s on: Differential Item Functioning]",
      "referring_paragraphs": [
        "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4).",
        "With the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial\n\nTable 2: Early technical definitions of fairness in educational and employment testing.",
        "This appendix provides some details of fairness definitions included in Table 2 that were not introduced in the text of Section 2."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1811.10104",
      "figure_id": "1811.10104_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg",
      "image_filename": "1811.10104_page0_fig3.jpg",
      "caption": "Figure 3: Original graph from [22] illustrating DIF.",
      "context_before": "DIF became very influential in the education field, and to this day DIF is in the toolbox of test designers. Items displaying a DIF are ideally examined further to identify the cause of bias, and possibly removed from the test [50].\n\n[Section: 2.5 1980s and beyond]\n\nWith the start of the 1980s came renewed public debate about the existence of racial differences in general intelligence, and the implications for fair testing, following the publication of the controversial\n\nTable 2: Early technical definitions of fairness in educational and employment testing. Variables:  is the test score;  is the R Ytarget variable;  is the demographic variable. The Proposition column indicates whether fairness is considered a property of Athe way in which a test is used, or of the test itself. † indicates that the criterion is discussed in the appendix.   \n\n<table><tr><td>Source</td><td>Criterion</td><td>Category</td><td>Proposition</td></tr><tr><td>Guion (1966)</td><td>“people with equal probabilities of success on the job have equal probabilities of being hired for the job”</td><td>INDIVIDUAL</td><td>Is the use of the test fair?</td></tr><tr><td>Cleary (1966)</td><td>“a subgroup does not have consistent errors”</td><td>NON-COMPARATIVE</td><td>Is the test fair to subgroup a?</td></tr><tr><td>Einhorn and Bass (1971)†</td><td>Prob(Y &gt; y * |R = ra*, A = a) is constant for all subgroups a</td><td>SUBGROUP PARITY</td><td>Is the use of the test fair with respect to A?</td></tr><tr><td>Thorndike (1971)</td><td>Prob(R &gt;= ra * |A = a)/Prob(Y &gt;= y * |A = a) is constant for all subgroups a</td><td>SUBGROUP PARITY</td><td>Is the use of the test fair with respect to A?</td></tr><tr><td>Darlington (1971) (1)</td><td>ρAX = ρAY/ρRY (equivalent to ρAY.R = 0)</td><td></td><td></td></tr><tr><td>Darlington (1971) (2)</td><td>ρAR = ρAY</td><td></td><td></td></tr><tr><td>Darlington (1971) (3)</td><td>ρAR = ρAY × ρRY (equivalent to ρAR.Y = 0)</td><td>CORRELATION</td><td>Is the test fair with respect to A?</td></tr><tr><td>Darlington (1971) (4)</td><td>ρAR = 0</td><td></td><td></td></tr><tr><td>Darlington (1971) culturally optimum†</td><td>ρR(Y-kA), is maximized where k is the subjective value placed on subgroup attribute A = 1</td><td>CORRELATION</td><td>Does the test produce the optimal outcome w.r.t. A?</td></tr><tr><td>Cole (1973)</td><td>Prob(R &gt;= ra * |Y &gt;= y*, A = a) is constant for all subgroups a</td><td>SUBGROUP PARITY</td><td>Is the use of the test fair with respect to A?</td></tr><tr><td>Linn (1973)</td><td>Prob(Y &gt;= y * |R &gt;= ra*, A = a) is constant for all subgroups a</td><td>SUBGROUP PARITY</td><td>Is the use of the test fair with respect to A?</td></tr><tr><td>Jones (1973) mean fair†</td><td>E(Ŷ|a) = E(Y|a)</td><td>NON-COMPARATIVE</td><td>Is the test fair to subgroup a?</td></tr><tr><td>Jones (1973) general standard†</td><td>a subgroup a has equal representation in the top-n candidates ranked by model score as it has in the top-n candidates ranked by Y, for all n</td><td>NON-COMPARATIVE</td><td>Is the test fair to subgroup a?</td></tr><tr><td>Jones (1973) at position n†</td><td>a subgroup a has equal representation in the top-n candidates ranked by model score as it has in the top-n candidates ranked by Y</td><td>NON-COMPARATIVE</td><td>Is the use of the test fair to subgroup a?</td></tr><tr><td>Peterson &amp; Novick (1976) conditional probability and its converse</td><td>Prob(R &gt;= ra * |Y &gt;= y*, A = a) is constant for all subgroups a, and Prob(R &lt; ra * |Y &lt; y*, A = a) is constant for all subgroups a</td><td>SUBGROUP PARITY</td><td>Is the use of the test fair with respect to A?</td></tr><tr><td>Peterson &amp; Novick (1976) equal probability and its converse</td><td>Prob(Y &gt;= y * |R &gt;= ra*, A = a) is constant for all subgroups a, and Prob(Y &lt; y * |R &lt; ra*, A = a) is constant for all subgroups a</td><td>SUBGROUP PARITY</td><td>Is the use of the test fair with respect to A?</td></tr></table>\n\nBias in Mental Testing [36]. Political opponents of group-based considerations in educational and employment practices framed them in terms of “preferential treatment” for minorities and “reverse discrimination” against whites. Despite, or perhaps because of, much public debate, neither Congress nor the courts gave unambiguous answers to the question of how to balance social justice considerations with the historical and legal importance placed on the individual in the United States [18].\n\nInto the 1980s, courts were asked to rule on many cases involving (un)fairness in educational testing. To give just one example, Zwick and Dorans [71] described the case of Debra P. v. Turlington 1984, in which a lawsuit was filed on behalf of “present and future twelfth grade students who had failed or would fail” a high school\n\ngraduation test. The initial ruling found that the test perpetuated past discrimination and was in violation of the Civil Rights Act. More examples of court rulings on fairness are given by [53, 71].\n\nBy the early 1980s, ideas about fairness were having a widespread influence on U.S. employment practices. In 1981, with no public debate, the United States Employment Services implemented scoreadjustment strategy that was sometimes called “race-norming” [54]. Each individual is assigned a percentile ranking within their own ethnic group, rather than to the test-taking population. By the mid-1980s, race-norming was “a highly controversial issue sparking heated debate.” The debate was settled through legislation, with the 1991 Civil Rights Act banning the practice of race-norming [65].",
      "context_after": "[Section: 3 CONNECTIONS TO ML FAIRNESS]\n\n[Section: 3.1 Equivalent Notions]\n\nMany of the fairness criteria we have overviewed are identical to modern-day fairness definitions. Here is a brief summary of these connections:\n\n• Peterson and Novick’s “conditional probability and its converse” is equivalent to what in ML fairness is variously called sufficiency [4], equalized odds [32], or conditional procedure accuracy [5], sometimes expressed as the conditional independence $A \\perp D | Y$ .   \nA D Y• Similarly, their “equal probability and its converse” is equivalent to what is called sufficiency [4] or conditional use accuracy equality [5], $A \\perp Y | D$ .   \nA Y D• Cole’s 1973 fairness definition is identical to equality of opportunity [32], $A \\perp D | Y = 1$ .   \nA D Y• Linn’s 1973 definition is equivalent to predictive parity [9], $A \\perp Y | D = 1$ .   \n• Darlington’s criterion (1) is equivalent to sufficiency in the special case where ,  and  have a multivariate Gaussian A R Ydistribution. This is because for this special case the partial correlation $\\rho _ { A Y . X } = 0$ is equivalent to $A \\perp Y | R$ [3]. In genρ . A Y Reral though, we cannot assume even a one way implication, since $A \\perp Y | R$ does not imply $\\rho _ { A Y . X } = 0$ (see [63] for a A Y Rcounterexample).   \n• Similarly, Darlington’s criteria (2) and (3) are equivalent to independence and separation only in the special cases of multivariate Gaussian distributions.   \n• Darlington’s definition (4) is a relaxation of what is called independence [4] or demographic parity in ML fairness, i.e. $A \\perp R$ ; it is equivalent when $A$ and $R$ have a bivariate Gauss-A Rian distribution.   \n• Guion’s definition “people with equal probabilities of success on the job have equal probabilities of being hired for the job” is a special case of Dwork’s [23] individual fairness with the presupposition that “probability of success on the job” is a construct that can be meaningfully reasoned about.\n\nThe fairness literature in both the fields of ML and in testing have also been motivated by causal considerations [32, 41]. Darlington [19] motivate his definition (3) on the basis of a causal relationship between  and $R$ (since an ability being measured affects the\n\nperformance on the test). However [34] have pointed out that in testing scenarios we typically only have a proxy for ability, such as later GPA 4 years later, and it is wrong to draw a causal connection from GPA to college entrance exam.\n\nHardt et al. [32] describe the challenge in building causal models, by considering two distinct models and their consequences and concluding that “no test based only on the target labels, the protected attribute and the score would give different indications for the optimal score $R ^ { * }$ in the two scenarios.” This is remarkably Rreminiscent of Anastasi [1], writing in 1961 about test fairness:\n\nNo test can eliminate causality. Nor can a test score, however derived, reveal the origin of the behavior it reflects. If certain environmental factors influence behavior, they will also influence those samples of behavior covered by tests. When we use tests to compare different groups, the only question the tests can answer directly is: “How do these groups differ under existing cultural conditions?”\n\nBoth the testing fairness and ML fairness literatures have also paid great attention to impossibility results, such as the distinction between group fairness and individual fairness, and the impossibility of obtaining more than one of separation, sufficiency and independence except under special conditions [4, 9, 19, 40, 52, 62].\n\nIn addition, we see some striking parallels in the framing of fairness in terms of ethical theories, including explicit advocacy for utilitarian approaches.\n\n• Petersen and Novick’s utility-based approaches relate to Corbett-Davies et al.’s framing of the cost of fairness [17].   \n• Hunter and Schmidt’s analysis of the value systems underlying fairness criteria is similar in spirit to Friedler et al.’s relation of fairness criteria and different worldviews [28].",
      "referring_paragraphs": [
        "Figure 3: Original graph from [22] illustrating DIF.",
        "Table 3 summarizes these connections, linking the historical criteria introduced in Section 2 to modern day categories."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    }
  ],
  "1901.10436": [
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg",
      "image_filename": "1901.10436_page0_fig0.jpg",
      "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).",
      "context_before": "Given the above issues, we were motivated to create the $D i F$ data set to obtain a scientific and computationally practical basis for ensuring fairness and accuracy in face recognition. At one extreme the challenge of diversity could be solved by building a data set comprised from the face of every person in the world. However, this would not be practical or even possible, let alone the significant privacy concerns. For one, our facial appearances are constantly changing due to ageing, among other factors. At best this would give a solution for a point in time. Rather, a solution needs to come from obtaining or generating a representative sample of faces with sufficient coverage and balance. That, however, is also not a simple task. There are many challenging questions: what does coverage mean computationally? How should balance be measured? Are age, gender and skin color sufficient? What about other highly personal attributes that are part of our identity, such as race, ethnicity, culture, geography, or visible forms of self-expression that are reflected in our faces in a myriad of ways? We realized very quickly that until these questions were answered we could not construct a complete and balanced data set of face images.\n\nWe formulated a new approach that would help answer these questions. We designed the $D i F$ data set to provide a scientific foundation for research into facial diversity. We reviewed the scientific literature on face analysis and studied prior work in fields as diverse as psychology, sociology, dermatology, cosmetology and facial surgery. We concluded that no single facial feature or combination of commonly used classifications – such as age, gender and skin color – would suffice. Therefore, we formulated a novel multi-modal approach that incorporates a diversity of face analysis methods. Based on study of the large body of prior work, we chose to implement a solid starter-set of ten facial coding schemes. The criteria for selecting these coding schemes included several important considerations: (1) strong scientific basis as evidenced by highly cited prior work, (2) extracting the coding scheme was computationally feasible, (3) the coding scheme produced continuous valued dimensions that could feed subsequent analysis, as opposed to generating only categorical values or labels, and (4) the coding scheme would allow for human interpretation to help with our understanding.\n\nWe chose YFCC-100M [1] to be the source for the sample of images. There were a number of important reasons for this. Ideally, we would be able to automatically obtain any large sample of images from any source meeting any characteristics of diversity we desire. However, practical considerations prevent this, including the fact that various copyright laws and privacy regulations must be respected. YFCC-100M is one of the largest image collections, consisting of more than 100 millions photos. It was populated by users of the Flickr photo service. There is a large diversity in these photos overall, where people and faces appear in an enormous number of ways. Also, importantly, a large portion of the photos have Creative Commons license. The downside of using YFCC-100M is that there is skew in the Flickr user community that contributed the photos. We cannot rely on the set of users or their photos to be inherently diverse. A consequence of this is that the set of images used in the $D i F$ is not completely balanced on its own. However, it still provides the desired basis for studying methods for characterizing facial diversity.\n\n[Section: 3.1 Data Selection]\n\nWhile the YFCC-100M photo data set is large, not all images could be considered. Naturally, we excluded photos that did not contain a face. We also excluded black and white and grayscale photos and those with significant blur. Although face recognition needs to be robust for non-color photos, we deferred incorporating these images in the initial $D i F$ data set in order to focus on intrinsic facial variation rather than image variation due to color processing.",
      "context_after": "[Section: 3.2 Pre-processing Pipeline]\n\nThe YFCC-100M data set gives a set of URLs that point to the Flickr web page for each of the photos. The first step we took was to check whether the URL was still active. If so, we then checked the license. We proceeded with the download only if the license type was Creative Commons. Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object detector trained for faces based on Faster-RCNN [61]. For each detected face, we then extracted both pose and 68 face key-points using the open source DLIB toolkit [62]. If there was any failure in the image processing steps, we excluded the face from further consideration. We also removed faces of size less than $5 0 \\times 5 0$ pixels or with inter-ocular distance of less than 30 pixels. We removed faces with substantial non-frontal pose. The overall process is shown in Figure 1.\n\nFinally, we generated two instances of each face. One is a rectified instance whereby the center points of each eye are fixed to a specific location in the overall image. The second crops an expanded region surrounding each face to give 50% additional spatial context. This overall process filtered the 100 million YFCC-100M photos down to approximately one million mostly frontal faces with adequate size. The surviving face images were the ones used for the $D i F$ data set. Note that the overall process of sampling YFCC-100M used only factors described above including color, size, quality and pose. We did not bias the sampling towards intrinsic facial characteristics or by using metadata associated with each photo, such as a geo-tag, date, labels or Flickr user name. In this sense, the $D i F$ data distribution is expected to closely follow the overall distribution of the YFCC-100M photos. In future efforts to grow the $D i F$ data set, we may relax some of the constraints based on size, pose and quality, or we may bias the sampling based on other properties. However, one million publicly available face images provides a good start. Given this compiled set of faces, we next process each one by extracting the ten facial coding schemes.\n\n[Section: 4 Facial Coding Scheme Implementation]",
      "referring_paragraphs": [
        "The facial coding schemes, summarized in Table 1, are among the strongest identified in the scientific literature and build a solid foundation to our collective knowledge.",
        "Table 1: Summary of the ten facial coding schemes used in the $D i F$ data set and their references.",
        "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61].",
        "The overall process is shown in Figure 1."
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(a) face detection (size)",
        "(b) face keypoints (pose and iod)",
        "1901.10436_page0_fig1.jpg",
        "(c) face rectification",
        "1901.10436_page0_fig2.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_4",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig3.jpg",
      "image_filename": "1901.10436_page0_fig3.jpg",
      "caption": "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3.",
      "context_before": "The YFCC-100M data set gives a set of URLs that point to the Flickr web page for each of the photos. The first step we took was to check whether the URL was still active. If so, we then checked the license. We proceeded with the download only if the license type was Creative Commons. Once we retrieved the photo, we processed it using face detection to find all depicted faces. For the face detection step, we used a Convolutional Neural Network (CNN) object detector trained for faces based on Faster-RCNN [61]. For each detected face, we then extracted both pose and 68 face key-points using the open source DLIB toolkit [62]. If there was any failure in the image processing steps, we excluded the face from further consideration. We also removed faces of size less than $5 0 \\times 5 0$ pixels or with inter-ocular distance of less than 30 pixels. We removed faces with substantial non-frontal pose. The overall process is shown in Figure 1.\n\nFinally, we generated two instances of each face. One is a rectified instance whereby the center points of each eye are fixed to a specific location in the overall image. The second crops an expanded region surrounding each face to give 50% additional spatial context. This overall process filtered the 100 million YFCC-100M photos down to approximately one million mostly frontal faces with adequate size. The surviving face images were the ones used for the $D i F$ data set. Note that the overall process of sampling YFCC-100M used only factors described above including color, size, quality and pose. We did not bias the sampling towards intrinsic facial characteristics or by using metadata associated with each photo, such as a geo-tag, date, labels or Flickr user name. In this sense, the $D i F$ data distribution is expected to closely follow the overall distribution of the YFCC-100M photos. In future efforts to grow the $D i F$ data set, we may relax some of the constraints based on size, pose and quality, or we may bias the sampling based on other properties. However, one million publicly available face images provides a good start. Given this compiled set of faces, we next process each one by extracting the ten facial coding schemes.\n\n[Section: 4 Facial Coding Scheme Implementation]\n\nIn this Section, we describe the implementation of the ten facial coding schemes and the process of extracting them from the $D i F$ face images. The advantage of using ten coding schemes is that it gives a diversity of methods and allows us to compare statistical measures for facial diversity. As described above, the ten schemes have been selected based on their strong scientific basis, computational feasibility, numerical representation and interpretability. Overall the chosen ten coding schemes capture multiple modalities of facial features, which includes craniofacial distances, areas and ratios, facial symmetry and contrast, skin color, age and gender predictions, subjective annotations, and pose and resolution. Three of the $D i F$ facial coding schemes are based on craniofacial features. As prior work has pointed out, skin color alone is not a strong predictor of race, and other features such as facial proportions are important [6, 63–65]. Face morphology is also relevant for attributes such as age and gender [4]. We incorporated multiple facial coding schemes aimed at capturing facial morphology using craniofacial features [2–4]. The basis of craniofacial science is the measurement of the face in terms of distances, sizes and ratios between specific points such as the tip of the nose, corner of the eyes, lips, chin, and so on. Many of these measures can be reliably estimated from photos of frontal faces using 47 landmark points of the head and face [2]. To provide the basis for the three craniofacial feature coding schemes used in $D i F$ , we built on the subset of 19 facial landmarks listed in Table 5. For brevity we adopt the abbreviations from [2] when referring to these facial landmark points instead of using the full anatomical terms.\n\nTable 5: Anatomical terms and corresponding abbreviations (as in [2]) for the set of facial landmarks employed to compute the craniofacial measurements for facial coding schemes 1–3.   \n\n<table><tr><td>Anatomical term</td><td>Abbreviation</td><td>Anatomical term</td><td>Abbreviation</td></tr><tr><td>tragion</td><td>tn</td><td>subalare</td><td>sbal</td></tr><tr><td>orbitale</td><td>or</td><td>subnasale</td><td>sn</td></tr><tr><td>palpebrale superius</td><td>ps</td><td>crista philtre</td><td>ceph</td></tr><tr><td>palpebrale inferius</td><td>pi</td><td>labiale superius</td><td>ls</td></tr><tr><td>endocanthion</td><td>en</td><td>stornion</td><td>sto</td></tr><tr><td>exocanthion</td><td>ex</td><td>labiale inferius</td><td>li</td></tr><tr><td>nasion</td><td>n</td><td>chelion</td><td>ch</td></tr><tr><td>pronasale</td><td>c&#x27;</td><td>gonion</td><td>go</td></tr><tr><td>zygion</td><td>zy</td><td>gnathion</td><td>gn</td></tr><tr><td>alare</td><td>al</td><td></td><td></td></tr></table>\n\nIn order to extract the 19 facial landmark points, we leveraged standard DLIB facial key-point extraction tools that provide a set 68 key-points for each face. As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2]. These 19 landmarks were used for extracting the craniofacial features. Note that for illustrative purposes, the example face used in Figure 2 was adopted from [66] and was generated synthetically using a progressive Generative Adversarial Network (GAN) model. The face does not correspond to a known individual person. However, the image is subject to license terms as per [66]. In order to incorporate a diversity of approaches, we implemented three facial coding schemes for craniofacial features. The first, coding scheme 1, provides a set of craniofacial distance measures from [2]. The second, coding scheme 2, provides an expanded set of craniofacial areas from [3]. The third, coding scheme 3, provides a set of craniofacial ratios from [4].",
      "context_after": "[Section: 4.1 Coding Scheme 1: Craniofacial Distances]\n\nThe first coding scheme for craniofacial distances has been adopted from [2]. It comprises eight measures which characterize all the vertical distances between elements in a face: the top of the forehead, the eyes, the nose, the mouth and the chin. In referring to the implementation of the coding scheme, we use the abbreviations from Table 5. We note that two required points, tn and $s t o$ , were not part of the set of 68 DLIB key-points. As such, we had to derive them in the following manner: tn was computed as the topmost point vertically above $n$ in the rectified facial image, and sto was computed from the vertical average of $\\mathit { l s }$ and $l i$ . The eight dimensions of craniofacial distances are summarized in Table 6.\n\n[Section: 4.2 Coding Scheme 2: Craniofacial Areas]",
      "referring_paragraphs": [
        "Table 2: Distribution of age groups for seven prominent face image data sets.",
        "As shown in Figure 2, we mapped the 68 DLIB key-points to the 19 facial landmarks [2].",
        "Figure 2: We used the 68 key-points extracted using DLIB from each face (small dots) to localize 19 facial landmarks (large dots, labeled), out of the 47 introduced in [2]. Those 19 landmarks were employed as the basis for extraction of the craniofacial measures for coding schemes 1–3."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg",
      "image_filename": "1901.10436_page0_fig4.jpg",
      "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.",
      "context_before": "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.\n\n[Section: 4.4 Coding Scheme 4: Facial Symmetry]\n\nFacial symmetry has been found in psychology and anthropology studies to be correlated with subjective and objective traits including expression variation [67] and attractiveness [5]. We adopted facial symmetry for coding scheme 4, given its intrinsic nature. To represent the symmetry of each face we computed two measures, following the work of Liu et al. [67]. We processed each face as shown in Figure 3. We used three of the DLIB key-points detected in the face image to spatially normalize and rectify it to the following locations: the inner canthus of each eye (C1 and C2) to reference locations $C 1 = ( 4 0 , 4 8 )$ , $C 2 = ( 8 8 , 4 8 )$ and the philtrum C3 was mapped to $C 3 = ( 6 4 , 8 4 )$ . Next, the face mid-line (point $b$ in Figure 3(a)) was computed as the line passing through the mid-point of the line segment connecting $C 1 - C 2$ (point $a$ in Figure 3(a)) and the philtrum C3.\n\nTable 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].   \n\n<table><tr><td>Craniofacial ratio</td><td>Measure</td></tr><tr><td>Facial index</td><td>(n-gn)/(zy-zy)</td></tr><tr><td>Mandibular index</td><td>(sto-gn)/(go-go)</td></tr><tr><td>Intercanthal index</td><td>(en-en)/(ex-ex)</td></tr><tr><td>Orbital width index (left and right)</td><td>(ex-en)/(en-en)</td></tr><tr><td>Eye fissure index (left and right)</td><td>(p-s-pi)/(ex-en)</td></tr><tr><td>Nasal index</td><td>(al-al)/(n-sn)</td></tr><tr><td>Vermilion height index</td><td>(ls-sto)/(sto-li)</td></tr><tr><td>Mouth-face width index</td><td>(ch-ch)/(zy-zy)</td></tr></table>",
      "context_after": "[Section: 4.5 Coding Scheme 5: Facial Regions Contrast]\n\nPrior studies have shown that facial contrast is a cross-cultural cue for perceiving facial attributes such as age. An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast faces. The study also found that artificially increasing the aspects of facial contrast that decrease with age across diverse races makes faces look younger, independent of the ethnic origin of the face or cultural origin of the observers [6]. On one hand, the age that you are is one dimension that needs to be addressed in terms of fairness and accuracy of face recognition. However, the age that you look, considering possible artificial changes, should not change requirements for fairness and accuracy.",
      "referring_paragraphs": [
        "Table 3: Distribution of gender and skin color/type for seven prominent face image data sets.",
        "We processed each face as shown in Figure 3.",
        "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ )."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1901.10436_page0_fig5.jpg",
        "(c)",
        "1901.10436_page0_fig6.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig6.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_8",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg",
      "image_filename": "1901.10436_page0_fig7.jpg",
      "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.",
      "context_before": "[Section: 4.5 Coding Scheme 5: Facial Regions Contrast]\n\nPrior studies have shown that facial contrast is a cross-cultural cue for perceiving facial attributes such as age. An analysis of full face color photographs of Chinese, Latin American and black South African women aged 20–80 in [6] found similar changes in facial contrast with ageing across races and were comparable to changes with Caucasian faces. This study found that high-contrast faces were judged to be younger than low-contrast faces. The study also found that artificially increasing the aspects of facial contrast that decrease with age across diverse races makes faces look younger, independent of the ethnic origin of the face or cultural origin of the observers [6]. On one hand, the age that you are is one dimension that needs to be addressed in terms of fairness and accuracy of face recognition. However, the age that you look, considering possible artificial changes, should not change requirements for fairness and accuracy.",
      "context_after": "[Section: 4.6 Coding Scheme 6: Skin Color]\n\nSkin occupies a large fraction of the face. As such, characteristics of the skin influence the appearance and perception of faces. Prior work has studied different methods of characterizing skin based on skin color [7, 69, 70], skin type [7, 38] and skin reflectance [71]. Early studies used Fitzpatrick skin type (FST) to classify sun-reactive skin types [38], which was also adopted recently in [36]. However, to-date, there is no universal measure for skin color, even within the dermatology field. In a study of 556 participants in South Africa, self-identified as either black, Indian/Asian, white, or mixed, Wilkes et al. found a high correlation between the Melanin Index (MI), which is frequently used to assign FST, with Individual Typology Angle (ITA) [72]. Since a dermatology expert is typically needed to assign the FST, the high correlation of MI and ITA indicates that ITA may be a practical method for measuring skin color given the simplicity of computing ITA. In order to explore this further, we designed coding scheme 6 to use ITA for representing skin color [7]. ITA has a strong advantage over Fitzpatrick in that it can be computed directly from an image. As in [7], we implemented ITA in the CIE-Lab space. For obvious practical reasons, we could not obtain measurements through a device directly applied on the skin of each individual, but instead converted the $R G B$ image to CIE-Lab space using standard image processing. The $L$ axis quantifies luminance, whereas $a$ quantifies absence or presence of redness, and $b$ quantifies yellowness. Figure 5 depicts the image processing steps for extracting the coding scheme 6 for skin color.\n\nIt is important to note that that ITA is a point measurement. Hence, every pixel corresponding to skin can have an ITA measurement. In order to generate a feature measure for the whole face, we extract ITA for pixels within a masked face region as shown in Figure 5(g). This masked region is determined in the following steps:\n\n1. Extract the skin mask in the face (pixels corresponding to skin) using a deep neural network, as described in [73].   \n2. Extract regions corresponding to the chin, two cheeks and forehead using the extracted 68 DLIB key-points",
      "referring_paragraphs": [
        "Table 4 summarizes many of the prominent face image data sets used for evaluating face recognition technology.",
        "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5.",
        "Table 4 outlined many of the currently used face data sets."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "1901.10436_page0_fig8.jpg",
        "(b)",
        "1901.10436_page0_fig9.jpg",
        "(c)",
        "1901.10436_page0_fig10.jpg",
        "1901.10436_page0_fig11.jpg",
        "(e)",
        "1901.10436_page0_fig12.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig12.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_14",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg",
      "image_filename": "1901.10436_page0_fig13.jpg",
      "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.",
      "context_before": "",
      "context_after": "[Section: 4.7 Coding Scheme 7: Age Prediction]\n\nAge is an attribute we all possess and our faces are predictors of our age, whether it is our actual age or manipulated age appearance [6]. As discussed in Section 4.5, particular facial features such as facial contrast are correlated with age. As an alternative to designing specific feature representations for predicting age, for coding scheme 7, we adopt a Convolutional Neural Network (CNN) that is trained from face images to predict age. We adopt the DEX model [8, 74] that is among the highest performing on some of the known face image data sets. The model is based on a pre-trained VGG16-face neural network for face identity that was subsequently fine-tuned on the IMDB-wiki data set [8] to predict age (years in the range 0-100). Since the DEX model was trained within a narrow context, it is not likely to be fair. However, our initial use here is to get some continuous measure of age in order to study diversity. Ultimately, it will require an iterative process of understanding diversity to make more balanced data sets and create more fair models. In order to predict age using DEX, each face was pre-processed as in [74]. First, the bounding box\n\nwas expanded by 40% both horizontally and vertically, then resized to 256x256 pixels. Inferencing was then performed on the 224x224 square cropped at the center of the image. Since softmax loss was used during the fine-tuning process, age prediction is output from the softmax layer, which is computed from $\\begin{array} { r } { E ( P ) = \\sum _ { i = 0 } ^ { 1 0 0 } p _ { i } y _ { i } } \\end{array}$ , where $p _ { i } \\in P$ are the softmax output probabilities for each of the 101 discrete years $y _ { i } \\in Y$ corresponding to each class $i$ , with $Y = \\{ 0 , . . . , 1 0 0 \\}$ .\n\n[Section: 4.8 Coding Scheme 8: Gender Prediction]",
      "referring_paragraphs": [
        "To provide the basis for the three craniofacial feature coding schemes used in $D i F$ , we built on the subset of 19 facial landmarks listed in Table 5.",
        "In referring to the implementation of the coding scheme, we use the abbreviations from Table 5.",
        "Figure 5 depicts the image processing steps for extracting the coding scheme 6 for skin color.",
        "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA)."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(f)",
        "1901.10436_page0_fig14.jpg",
        "(h)",
        "1901.10436_page0_fig15.jpg",
        "(a)",
        "1901.10436_page0_fig16.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig16.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_18",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg",
      "image_filename": "1901.10436_page0_fig17.jpg",
      "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.",
      "context_before": "",
      "context_after": "[Section: 5.1 Coding Scheme 1: Craniofacial Distances]\n\nFigure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1. The highest Simpson $D$ value is 5.888 and the lowest is 5.832. The highest and lowest Shannon $H$ values are 1.782 and 1.777. Based on the Shannon $H$ values, this feature dimension would typically map to 6 classes. Evenness is generally balanced with highest Simpson $E$ and Shannon $E$ of 0.981 and 0.995, respectively.\n\n<table><tr><td>Coding Scheme</td><td>Measurement</td><td>Simp. D</td><td>Simp. E</td><td>Shan. H</td><td>Shan. E</td><td>Mean</td><td>Var</td></tr><tr><td rowspan=\"8\">Craniofacial Distance</td><td>n - sto</td><td>5.875</td><td>0.979</td><td>1.781</td><td>0.994</td><td>33.20</td><td>3.93</td></tr><tr><td>ps - pi</td><td>5.886</td><td>0.981</td><td>1.782</td><td>0.995</td><td>3.26</td><td>0.98</td></tr><tr><td>or - sci</td><td>5.867</td><td>0.978</td><td>1.780</td><td>0.994</td><td>12.6</td><td>2.22</td></tr><tr><td>sn - cc</td><td>5.832</td><td>0.972</td><td>1.777</td><td>0.992</td><td>6.50</td><td>1.69</td></tr><tr><td>sn - sto</td><td>5.870</td><td>0.978</td><td>1.780</td><td>0.994</td><td>9.95</td><td>2.12</td></tr><tr><td>sto - li</td><td>5.862</td><td>0.977</td><td>1.780</td><td>0.994</td><td>5.86</td><td>1.97</td></tr><tr><td>cph - cph</td><td>5.872</td><td>0.979</td><td>1.781</td><td>0.994</td><td>6.99</td><td>1.35</td></tr><tr><td>sbal - lss</td><td>5.832</td><td>0.972</td><td>1.777</td><td>0.992</td><td>7.07</td><td>1.63</td></tr><tr><td rowspan=\"12\">Craniofacial Area</td><td>tr - n</td><td>5.887</td><td>0.981</td><td>1.782</td><td>0.995</td><td>33.83</td><td>5.25</td></tr><tr><td>tr - gn</td><td>5.879</td><td>0.980</td><td>1.781</td><td>0.994</td><td>89.27</td><td>9.44</td></tr><tr><td>n - gn</td><td>5.877</td><td>0.979</td><td>1.781</td><td>0.994</td><td>55.44</td><td>7.48</td></tr><tr><td>sn - gn</td><td>5.866</td><td>0.978</td><td>1.780</td><td>0.993</td><td>32.19</td><td>5.82</td></tr><tr><td>zy - zy</td><td>5.868</td><td>0.978</td><td>1.780</td><td>0.994</td><td>63.67</td><td>4.42</td></tr><tr><td>go - go</td><td>5.888</td><td>0.981</td><td>1.782</td><td>0.995</td><td>43.47</td><td>3.89</td></tr><tr><td>en - en</td><td>5.875</td><td>0.979</td><td>1.781</td><td>0.994</td><td>17.49</td><td>1.06</td></tr><tr><td>en - ex</td><td>5.880</td><td>0.980</td><td>1.781</td><td>0.994</td><td>11.56</td><td>0.56</td></tr><tr><td>ex - ex</td><td>5.882</td><td>0.980</td><td>1.782</td><td>0.994</td><td>40.62</td><td>1.00</td></tr><tr><td>n - sn</td><td>5.873</td><td>0.979</td><td>1.781</td><td>0.994</td><td>23.24</td><td>3.02</td></tr><tr><td>al - al</td><td>5.876</td><td>0.979</td><td>1.781</td><td>0.994</td><td>13.62</td><td>1.65</td></tr><tr><td>ch - ch</td><td>5.858</td><td>0.976</td><td>1.780</td><td>0.993</td><td>27.09</td><td>4.04</td></tr><tr><td rowspan=\"8\">Craniofacial Ratio</td><td>(n - gn)/(zy - zy)</td><td>5.878</td><td>0.980</td><td>1.781</td><td>0.994</td><td>0.87</td><td>0.11</td></tr><tr><td>(sto - gn)/(go - go)</td><td>5.878</td><td>0.980</td><td>1.781</td><td>0.994</td><td>0.51</td><td>0.08</td></tr><tr><td>(en - en)/(ex - ex)</td><td>5.885</td><td>0.981</td><td>1.782</td><td>0.995</td><td>0.43</td><td>0.02</td></tr><tr><td>(ex - en)/(en - en)</td><td>5.870</td><td>0.978</td><td>1.781</td><td>0.994</td><td>0.66</td><td>0.06</td></tr><tr><td>(ps - pi)/(ex - en)</td><td>5.878</td><td>0.980</td><td>1.781</td><td>0.994</td><td>0.28</td><td>0.08</td></tr><tr><td>(al - al)/(n - sn)</td><td>5.902</td><td>0.984</td><td>1.783</td><td>0.995</td><td>0.59</td><td>0.08</td></tr><tr><td>(ls - sto)/(sto - li)</td><td>5.884</td><td>0.981</td><td>1.782</td><td>0.994</td><td>0.67</td><td>0.34</td></tr><tr><td>(ch - ch)/(zy - zy)</td><td>5.872</td><td>0.979</td><td>1.781</td><td>0.994</td><td>0.42</td><td>0.06</td></tr><tr><td rowspan=\"2\">Facial Symmetry</td><td>Density difference</td><td>5.510</td><td>0.918</td><td>1.748</td><td>0.975</td><td>0.12</td><td>0.07</td></tr><tr><td>Edge or. similarity</td><td>5.002</td><td>0.834</td><td>1.692</td><td>0.944</td><td>0.01</td><td>0.01</td></tr><tr><td rowspan=\"9\">Facial Contrast</td><td>Lips L contrast</td><td>5.857</td><td>0.976</td><td>1.779</td><td>0.993</td><td>-0.07</td><td>0.09</td></tr><tr><td>Lips a contrast</td><td>5.780</td><td>0.963</td><td>1.772</td><td>0.989</td><td>0.02</td><td>0.02</td></tr><tr><td>Lips b contrast</td><td>5.868</td><td>0.978</td><td>1.780</td><td>0.994</td><td>-0.01</td><td>0.01</td></tr><tr><td>Eyes L contrast</td><td>5.724</td><td>0.954</td><td>1.766</td><td>0.986</td><td>-0.18</td><td>0.14</td></tr><tr><td>Eyes a contrast</td><td>5.872</td><td>0.979</td><td>1.781</td><td>0.994</td><td>-0.02</td><td>0.01</td></tr><tr><td>Eyes b contrast</td><td>5.862</td><td>0.977</td><td>1.780</td><td>0.993</td><td>-0.03</td><td>0.02</td></tr><tr><td>Eb L contrast</td><td>5.722</td><td>0.954</td><td>1.766</td><td>0.986</td><td>-0.11</td><td>0.11</td></tr><tr><td>Eb a contrast</td><td>5.759</td><td>0.960</td><td>1.769</td><td>0.987</td><td>-0.01</td><td>0.01</td></tr><tr><td>Eb b contrast</td><td>5.666</td><td>0.944</td><td>1.760</td><td>0.982</td><td>-0.01</td><td>0.01</td></tr><tr><td>Skin Color</td><td>ITA</td><td>5.283</td><td>0.755</td><td>1.773</td><td>0.911</td><td>13.99</td><td>45.13</td></tr><tr><td>Age</td><td>Age prediction</td><td>4.368</td><td>0.624</td><td>1.601</td><td>0.823</td><td>26.28</td><td>14.65</td></tr><tr><td>Gender</td><td>Gender prediction</td><td>3.441</td><td>0.573</td><td>1.488</td><td>0.831</td><td>0.27</td><td>0.32</td></tr><tr><td rowspan=\"2\">Subjective Annotation</td><td>Gender labeling</td><td>2.000</td><td>1.000</td><td>0.693</td><td>1.000</td><td>0.49</td><td>0.50</td></tr><tr><td>Age labeling</td><td>4.400</td><td>0.629</td><td>1.675</td><td>0.861</td><td>30.44</td><td>16.99</td></tr><tr><td rowspan=\"3\">Pose &amp; Resolution</td><td>Pose</td><td>1.224</td><td>0.408</td><td>0.390</td><td>0.355</td><td>0.11</td><td>0.84</td></tr><tr><td>IOD</td><td>4.989</td><td>0.832</td><td>1.691</td><td>0.944</td><td>43.60</td><td>22.14</td></tr><tr><td>Face Region Size</td><td>2.816</td><td>0.469</td><td>1.197</td><td>0.668</td><td>93.29</td><td>42.95</td></tr></table>\n\nTable 12: Summary of facial coding scheme analysis for the $D i F$ data set using Simpson $D$ (diversity), Simpson $E$ (evenness), Shannon $H$ (diversity), Shannon $E$ (evenness), mean and variance.",
      "referring_paragraphs": [
        "The eight dimensions of craniofacial distances are summarized in Table 6.",
        "Table 6: Coding scheme 1 is made up eight craniofacial measures corresponding to different vertical distances in the face [2].",
        "Figure 6 illustrates these measures on two example distributions.",
        "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution."
      ],
      "figure_type": "table_img",
      "sub_figures": [
        "(b)",
        "(c)",
        "1901.10436_page0_fig18.jpg",
        "(d)",
        "1901.10436_page0_fig19.jpg",
        "1901.10436_page0_fig20.jpg",
        "1901.10436_page0_fig21.jpg",
        "1901.10436_page0_fig22.jpg",
        "1901.10436_page0_fig23.jpg",
        "1901.10436_page0_fig24.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig24.jpg"
        ],
        "panel_count": 8
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_26",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg",
      "image_filename": "1901.10436_page0_fig25.jpg",
      "caption": "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set.",
      "context_before": "",
      "context_after": "[Section: 5.2 Coding Scheme 2: Craniofacial Areas]\n\nFigure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2. The highest Simpson $D$ value is 5.888 and the smallest is 5.858. The highest Shannon $H$ value is 1.782 and the lowest is 1.780. Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes. Evenness ranges between 0.981 and 0.976.\n\n[Section: 5.3 Coding Scheme 3: Craniofacial Ratios]",
      "referring_paragraphs": [
        "Table 7 summarizes the twelve dimensions of the craniofacial area features.",
        "Figure 7 summarizes the feature distribution for the 8 craniofacial distances in coding scheme 1.",
        "Figure 7: Feature distribution of craniofacial distances (coding scheme 1) for the $D i F$ data set."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1901.10436_page0_fig26.jpg",
        "1901.10436_page0_fig27.jpg",
        "1901.10436_page0_fig28.jpg",
        "1901.10436_page0_fig29.jpg",
        "1901.10436_page0_fig30.jpg",
        "1901.10436_page0_fig31.jpg",
        "1901.10436_page0_fig32.jpg",
        "1901.10436_page0_fig33.jpg",
        "1901.10436_page0_fig34.jpg",
        "1901.10436_page0_fig35.jpg",
        "1901.10436_page0_fig36.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig25.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig26.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig27.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig28.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig29.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig30.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig31.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig32.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig33.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig34.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig35.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig36.jpg"
        ],
        "panel_count": 12
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_38",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg",
      "image_filename": "1901.10436_page0_fig37.jpg",
      "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.",
      "context_before": "",
      "context_after": "[Section: 5.6 Coding Scheme 6: Skin Color]\n\nFigure 12 summarizes the feature distribution for skin color in coding scheme 6. The Simpson $D$ value is 5.283 and Shannon $H$ value is 1.773 which translates to about 5.88 classes, which shows a good match with the number of bins we used. The evenness is weaker than a uniform distribution.\n\n[Section: 5.7 Coding Scheme 7: Age Prediction]",
      "referring_paragraphs": [
        "The third coding scheme comprises measures corresponding to different ratios of the face. These features were used to estimate age progression from faces in the age groups of 0 to 18 in [4]. Similar to the above features, the craniofacial ratios used the mapped DLIB key-points as facial landmarks. Table 8 summarizes the eight dimensions of the craniofacial ratio features.",
        "Table 8: Coding scheme 3 is made up of eight craniofacial measures that correspond to different ratios of the face [3].",
        "Figure 8 summarizes the feature distribution for the 12 craniofacial areas in coding scheme 2. The highest Simpson $D$ value is 5.888 and the smallest is 5.858. The highest Shannon $H$ value is 1.782 and the lowest is 1.780. Compared to coding scheme 1, these values are in the similar range, mapping to 6 classes. Evenness ranges between 0.981 and 0.976.",
        "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1901.10436_page0_fig38.jpg",
        "1901.10436_page0_fig39.jpg",
        "1901.10436_page0_fig40.jpg",
        "1901.10436_page0_fig41.jpg",
        "1901.10436_page0_fig42.jpg",
        "1901.10436_page0_fig43.jpg",
        "1901.10436_page0_fig44.jpg",
        "1901.10436_page0_fig45.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig38.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig39.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig40.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig41.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig42.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig43.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig44.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig45.jpg"
        ],
        "panel_count": 9
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_47",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg",
      "image_filename": "1901.10436_page0_fig46.jpg",
      "caption": "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Finally, we compute two facial symmetry measures based on density difference $D D ( x , y )$ and edge orientation similarity $E O S ( x , y )$ as follows: for each pixel $( x , y )$ in the left 128x64 part ( $I$ and $I _ { e }$ ) and the corresponding 128x64 right part ( $I ^ { \\prime }$ and $I _ { e } ^ { \\prime }$ ) are computed as summarized in Table 9, where $\\phi ( I _ { e } ( x , y ) , I _ { e } ^ { \\prime } ( x , y ) )$ is the angle between the two edge orientations of images $I _ { e }$ and $I _ { e } ^ { \\prime }$ at pixel $( x , y )$ .",
        "Figure 9 summarizes the feature distribution for the 8 craniofacial ratios in coding scheme 3. The largest Simpson $D$ value is 5.902 and smallest is 5.870. Similarly, the largest Shannon $H$ value is 1.783 and smallest is 1.781. This would map to approximately to 6 classes. While Simpson $E$ has a range between 0.978 to 0.984, Shannon $E$ ranges between 0.994 to 0.995. The evenness of coding scheme 3 is similar to coding scheme 2.",
        "Figure 9: Feature distribution of craniofacial ratios (coding scheme 3) for the $D i F$ data set.   \n(a)"
      ],
      "figure_type": "other",
      "sub_figures": [
        "1901.10436_page0_fig47.jpg",
        "1901.10436_page0_fig48.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig46.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig47.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig48.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_50",
      "figure_number": 10,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg",
      "image_filename": "1901.10436_page0_fig49.jpg",
      "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.",
      "context_before": "",
      "context_after": "[Section: 5.9 Coding Scheme 9: Subjective Annotation]\n\nFigure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9. The Simpson $D$ for gender distribution is 2.0 and Shannon $H$ is 0.693, indicating the equivalent classes to be near 2, which is understandable. The evenness is very high, indicating a nearly flat distribution. The Simpson $D$ is 4.368 and Shannon $H$ is 1.675, resulting in a equivalent class index of approximately 5.3. However, the evenness scores are low at 0.629, indicating unevenness, as is visible in the distribution of the annotated age scores.\n\n[Section: 5.10 Coding Scheme 10: Pose and Resolution]",
      "referring_paragraphs": [
        "The computation is summarized in Table 10, where $I _ { k } ( x , y )$ is the pixel intensity at $( x , y )$ for CIE-Lab channel $k$ and $p t _ { o u t e r } , p t _ { i n n e r }$ correspond to the outer and inner regions around each facial part $p t$ .",
        "Figure 10 summarizes the feature distribution for facial symmetry in coding scheme 4. The diversity value is in a middle range compared to the previous coding schemes. For example, the highest Simpson $D$ is 5.510 and the largest Shannon $H$ is 1.748. The evenness values are lower as well with highest Simpson $E$ value being 0.918 and highest Shannon $E$ value being 0.975. The Shannon $H$ value of 1.692 translates to about 5.4 classes.",
        "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1901.10436_page0_fig50.jpg",
        "1901.10436_page0_fig51.jpg",
        "1901.10436_page0_fig52.jpg",
        "1901.10436_page0_fig53.jpg",
        "1901.10436_page0_fig54.jpg",
        "1901.10436_page0_fig55.jpg",
        "1901.10436_page0_fig56.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig50.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig51.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig52.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig53.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig54.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig55.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig56.jpg"
        ],
        "panel_count": 8
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_58",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg",
      "image_filename": "1901.10436_page0_fig57.jpg",
      "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Average the values to give a single ITA score for each face\n\nTable 11 gives the formula for computing the ITA values for each pixel in the masked face region.",
        "Figure 11 summarizes the feature distribution for facial contrast in coding scheme 5. The highest Simpson $D$ value is 5.872 and highest Shannon $H$ value is 1.781, which is equivalent to 5.9 classes. The evenness factor Shannon $E$ is very close to 0.979 indicating that the measures are close to even.",
        "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   \nFigure 12: Feature distribution of skin color using Individual Typology Angle (ITA) (coding scheme 6) for the $D i F$ data set."
      ],
      "figure_type": "other",
      "sub_figures": [
        "1901.10436_page0_fig58.jpg",
        "1901.10436_page0_fig59.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig58.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig59.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_61",
      "figure_number": 13,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg",
      "image_filename": "1901.10436_page0_fig60.jpg",
      "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 13(a) summarizes the feature distribution for age prediction in coding scheme 7, where we bin the age values into seven groups: [0-3],[4-12],[13-19],[20-30],[31-45],[46-60],[61-]. The Simpson $D$ and Shannon $H$ values are 4.368 and 1.601. Because of the data distribution not being even, we can see a lower $E$ value around 0.624. The Shannon $H$ value of 1.601 maps to 5 classes.",
        "Figure 13 also summarizes the feature distribution for gender prediction in coding scheme 8.",
        "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   \n(a)"
      ],
      "figure_type": "other",
      "sub_figures": [
        "(a)",
        "(b)",
        "1901.10436_page0_fig61.jpg",
        "1901.10436_page0_fig62.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig61.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig62.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_64",
      "figure_number": 14,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig63.jpg",
      "image_filename": "1901.10436_page0_fig63.jpg",
      "caption": "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.",
      "context_before": "",
      "context_after": "[Section: 5.11 Discussion]\n\nSome observations come from this statistical analysis of the ten coding schemes on the $D i F$ face image data. One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one. What this shows is that there is higher variability in these measures, and they are capturing information that age and gender alone do not. Interestingly, facial regions contrast, which was designed to capture information about age, has a higher diversity score and better evenness than that for either neural network prediction of age or subjective human annotation of age. Again, it implies that this continuous valued feature of facial contrast is capturing information that goes beyond simple age prediction or labeling. The only feature dimension with lower diversity is understandably pose, which was a controlled variable in selecting images for the $D i F$ data set, since only mostly frontal faces were incorporated. In future work, we will use these methods to assess diversity of other face data sets, which will provide a basis for comparison and further insight.",
      "referring_paragraphs": [
        "Figure 14 summarizes the feature distribution for the subjective annotations of age and gender for coding scheme 9.",
        "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.\n\nare shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "1901.10436",
      "figure_id": "1901.10436_fig_65",
      "figure_number": 15,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg",
      "image_filename": "1901.10436_page0_fig64.jpg",
      "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).",
      "context_before": "[Section: 5.11 Discussion]\n\nSome observations come from this statistical analysis of the ten coding schemes on the $D i F$ face image data. One is that the many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes. Generally, they are higher than measures used for age and gender, whether using a predictive model or subjective annotation. Similarly, their evenness scores are also closer to one. What this shows is that there is higher variability in these measures, and they are capturing information that age and gender alone do not. Interestingly, facial regions contrast, which was designed to capture information about age, has a higher diversity score and better evenness than that for either neural network prediction of age or subjective human annotation of age. Again, it implies that this continuous valued feature of facial contrast is capturing information that goes beyond simple age prediction or labeling. The only feature dimension with lower diversity is understandably pose, which was a controlled variable in selecting images for the $D i F$ data set, since only mostly frontal faces were incorporated. In future work, we will use these methods to assess diversity of other face data sets, which will provide a basis for comparison and further insight.",
      "context_after": "[Section: 6 Summary and Future Work]\n\nWe described the new Diversity in Faces $( D i F )$ data set, which has been developed to help advance the study of fairness and accuracy in face recognition technology. $D i F$ provides a data set of annotations of publicly available face images sampled from the YFCC-100M data set of 100 million images. The annotations are defined from facial coding schemes that provide quantitative measures related to intrinsic characteristics of faces including craniofacial features, facial symmetry, facial contrast, skin color, age, gender, subjective annotations and pose and resolution. We described the process for generating the $D i F$ data set as well as the implementation and extraction of the ten facial coding schemes. We also provided a statistical analysis of the facial coding scheme measures on the one million $D i F$ images using measures of diversity, evenness and variance. For one, this kind of analysis has provided insight into how the 47 total feature dimensions within the ten facial coding schemes provide measures of data set diversity for the one million images. While it may not yet be possible to conclude that the goal is to drive all of these feature dimensions to be maximally diverse and even, we believe the approach outlined in this work provides a needed methodology for advancing the study of diversity for face recognition.\n\nThere are multiple next directions for this work. Table 4 outlined many of the currently used face data sets. We plan to perform the equivalent statistical analysis on some of these other data sets using the ten coding schemes. This will provide an important basis for comparing data sets in terms of diversity. Using the statistical measures outlined in this paper, including diversity, evenness and variance, we will begin to answer questions of whether one data set is better than another, or where a data set falls short in terms of coverage and balance. We also strongly encourage others to build on this work. We selected a solid starting point by using over one million publicly available face images and by implementing ten facial coding schemes. We hope that others will find ways to grow the data set to include more faces. As more insight comes from the type of analysis outlined in this paper, we see that an iterative process can more proactive sampling to fill in gaps. For example, as technologies like Generative Adversarial Networks (GANs) continue to improve [66, 78], it may be possible to generate faces of any variety to synthesize training data as needed. We expect that $D i F$ will be valuable to steer these generative methods towards gaps and blind spots in the training data, for example, to assist methods that automatically generate faces corresponding to a specific geographical area [79]. We also hope that others will see ways to improve on the initial ten coding schemes and add new ones. Pulling together our collective efforts is the best way to make progress on this important topic. We hope that the $D i F$ data set provides a useful foundation for creating more fair and accurate face recognition systems in practice.\n\n[Section: References]",
      "referring_paragraphs": [
        "Figure 15 summarizes the feature distribution for pose and resolution for coding scheme 10. Pose uses three dimensions from the output of DLIB face detection and the distribution is shown in 15 (a). When computing mean and variance for pose in Table 12, we used the following values: Frontal Tilted Left -1, Frontal 0, and Frontal Tilted Right 1. The IOD and box size distribution",
        "Figure 14: Feature distributions of subjective annotations (coding scheme 9) for (a) age and (b) gender for the $D i F$ data set.\n\nare shown in Figure 15 (b)-(c). The distances have been binned to six classes. The three class pose distribution has a Shannon $H$ value of 0.39. The Shannon $H$ value for IOD is 1.69 (mapping to equivalent of 5.4 classes) while for the box size it is 1.197, translating to 3.3 classes.",
        "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD)."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1901.10436_page0_fig65.jpg",
        "(c)",
        "1901.10436_page0_fig66.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig65.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig66.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1902.03519": [
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig0.jpg",
      "image_filename": "1902.03519_page0_fig0.jpg",
      "caption": "",
      "context_before": "Definition 2.1 (Fairlet Decomposition). Suppose that $P \\subseteq Y$ is a collection of points such that each is either colored red or blue. Moreover, suppose that balance $( P ) \\geq { \\frac { b } { r } }$ for some integers $1 \\leq b \\leq r$ such that $\\operatorname* { g c d } ( r , b ) = 1$ . A clustering ${ \\mathcal { X } } = \\{ D _ { 1 } , \\cdots , D _ { m } \\}$ of $P$ is an $( r , b )$ -fairlet decomposition if (a) each point $| D _ { i } | \\leq b + r$ , and (c) for each $D _ { i } \\in \\mathcal { X }$ $p \\in P$ , balance belongs to exactly one cluster $\\begin{array} { r } { ( D _ { i } ) \\geq \\frac { b } { r } } \\end{array}$ . $D _ { i } \\in { \\mathcal { X } }$ , (b) for each $D _ { i } \\in \\mathcal { V }$ ,\n\nProbabilistic metric embedding. A probabilistic metric $( X , { \\overline { { d } } } )$ is defined as a set of $\\ell$ metrics $( X , d _ { 1 } ) , \\cdot \\cdot \\cdot , ( X , d _ { \\ell } )$ along with a probability distribution of support size $\\ell$ denoted by $\\alpha _ { 1 } , \\cdots , \\alpha _ { \\ell }$\n\nsuch that $\\begin{array} { r } { \\overline { { d } } ( p , q ) = \\sum _ { i = 1 } ^ { \\ell } \\alpha _ { i } \\cdot d _ { i } ( p , q ) } \\end{array}$ . For any finite metric $M = ( Y , d )$ and probabilistic metric $( X , { \\overline { { d } } } )$ , an embedding $f : Y \\to X$ has distortion $c _ { f }$ , if:\n\n• for all $p , q \\in Y$ and $i \\leq k$ , $d _ { i } ( f ( p ) , f ( q ) ) \\geq d ( p , q )$ ,   \n• $\\overline { { d } } ( f ( p ) , f ( q ) ) \\leq c _ { f } \\cdot d ( p , q ) .$ .\n\nDefinition 2.2 ( $\\gamma$ -HST). $A$ tree $T$ rooted at vertex $r$ is $a$ hierarchically well-separated tree ( $\\gamma$ - HST) if all edges of $T$ have non-negative weights and the following two conditions hold:\n\n1. The (weighted) distances from any node to all its children are the same.   \n2. For each node $v \\in V \\setminus \\{ r \\}$ , the distance of v to its children is at most $1 / \\gamma$ times the distance of v to its parent.\n\nWe build on the following result due to Bartal (1996), which gives a probabilistic embedding from $\\mathbb { R } ^ { d }$ to a $\\gamma$ -HST. Our algorithm explicitly computes this embedding which we describe in more detail in the next section. For a proof of its properties refer to (Indyk, 2001) and the references therein. In this paper, we assume that the given pointset has poly(n) aspect ratio (i.e. the ratio between the maximum and minimum distance is poly(n)).\n\nTheorem 2.3 (Bartal (1996)). Any finite metric space $M$ on points in $\\mathbb { R } ^ { d }$ can be embedded into probabilistic metric over $\\gamma$ -HST metrics with $O ( \\gamma \\cdot d \\cdot \\log _ { \\gamma } n )$ distortion in $O ( d \\cdot n \\cdot \\log _ { \\gamma } n )$ time.\n\n[Section: 3 High-level Description of Our Algorithm]\n\nOur algorithm for $( r , b )$ -fair $k$ -median problem in Euclidean space follows the high-level approach of Chierichetti et al. (2017): it first computes an approximately optimal $( r , b )$ -fairlet decomposition for the input point set $P$ (see Algorithm 1). Then, in the second phase, it clusters the $( r , b )$ -fairlets produced in the first phase into $k$ clusters (see Algorithm 2). Our main contribution is designing a scalable algorithm for the first phase of this approach, namely $( r , b )$ -fairlet decomposition.\n\nPreprocessing phase: embedding to $\\gamma$ -HST. An important step in our algorithm is to embed the input pointset $P$ into a $\\gamma$ -HST (see Section 2 for more details on HST metrics). To this end, we exploit the following standard construction of $\\gamma$ -HST using randomly shifted grids.\n\nSuppose that all points in $P$ lie in $\\{ - \\Delta , \\cdots , \\Delta \\} ^ { d }$ . We generate a random tree $T$ (which is a -HST embedding of $P$ ) recursively. We translate the $d$ -dimensional hypercube $H = [ - 2 \\Delta , 2 \\Delta ] ^ { d }$ $\\gamma$ via a uniformly random shift vector $\\sigma \\in \\{ - \\Delta , \\cdot \\cdot \\cdot , \\Delta \\} ^ { d }$ . It is straightforward to verify that all points in $P$ are enclosed in $H + \\sigma$ . We then split each dimension of $H$ into equal pieces to create $\\gamma$ a grid with $\\gamma ^ { d }$ cells. Then we proceed recursively with each non-empty cell to create a hierarchy of nested $d$ -dimensional grids with $O ( \\log _ { \\gamma } { \\frac { \\Delta } { \\varepsilon } } )$ levels (each cell in the final level of the recursion either contains exactly one point of $P$ or has side length $\\varepsilon$ ). Next, we construct a tree $T$ corresponding to the described hierarchy nested $d$ -dimensional grids as follows. Consider a cell $C$ in the $i$ -th level (level 0 denote the initial hypercube $H$ ) of the hierarchy. Let $T _ { C } ^ { 1 } , \\cdots , T _ { C } ^ { \\ell }$ denote the trees constructed recursively for each non-empty cells of $C$ . Denote the root of each tree $T _ { C } ^ { j }$ by $u _ { C } ^ { j }$ . Then we connect $u _ { C }$ (corresponding to cell $C$ ) to each of $u _ { j } ^ { C }$ with an edge of length proportional to the diameter of $C$ (i.e., $( \\sqrt { d } \\cdot \\Delta ) / \\gamma ^ { i } \\rangle$ ).\n\nNote that the final tree generated by the above construction is a $\\gamma$ -HST: on each path from the root to a leaf, the length of consecutive edges decrease exponentially (by a factor of $\\gamma$ ) and the distance from any node to all of its children are the same. Moreover, we assume that $\\Delta / \\varepsilon = n ^ { O ( 1 ) }$ .",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [
        "(a) The original space partitioned into hypercubes.",
        "(b) A 2-HST embedding of the input points.",
        "1902.03519_page0_fig1.jpg",
        "(c) Stage 1: we must connect 3 blue points from the left node through the root.",
        "1902.03519_page0_fig2.jpg"
      ],
      "quality_score": 0.35,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg",
      "image_filename": "1902.03519_page0_fig3.jpg",
      "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.",
      "context_before": "",
      "context_after": "[Section: 4 Fairlet Decomposition: a Top-down Approach on $\\gamma$ -HST]\n\nIn this section, we provide a complete description of the first phase in our $( r , b )$ -fair $k$ -median algorithm (described in Section 3), namely our scalable $( r , b )$ -fairlet decomposition algorithm.\n\nThe first step in our algorithm is to embed the input point set into a $\\gamma$ -HST (for a value of $\\gamma$ to be determined later in this section). Once we build a $\\gamma$ -HST embedding $T$ of the input points $P \\subseteq \\mathbb { R } ^ { d }$ , the question is how to partition the points into $( r , b )$ -fairlets. We assume that each node $v \\in { \\mathcal { I } }$ is augmented with extra information $N _ { r }$ and $N _ { b }$ respectively denoting the number of red and blue points in the subtree $T ( v )$ rooted at $v$ .\n\nTo compute the total cost of a fairlet decomposition, it is important to specify the clustering cost model (e.g., $k$ -median, $k$ -center). Here, we define costmedian to denote the cost of a fairlet (or cluster) with respect to the cost function of $k$ -median clustering: for any subset of points $S \\subset \\mathbb { R } ^ { d }$ , $\\begin{array} { r } { \\mathrm { c o s t } _ { \\mathrm { m e d i a n } } ( S ) : = \\operatorname* { m i n } _ { p \\in S } \\sum _ { q \\in S } d ( p , q ) } \\end{array}$ where $d ( p , q )$ denotes the distance of $p , q$ in $T$ .\n\nIn this section, we design a fast fairlet decomposition algorithm with respect to costmedian.\n\nTheorem 4.1. There exists an $\\widetilde O ( n )$ time algorithm that given an $O ( r ^ { 5 } + b ^ { 5 } )$ -HST embedding $T$ of the point set $P$ and balance parameters $( r , b )$ , computes an $O ( r ^ { 3 } + b ^ { 3 } )$ -approximate $( r , b )$ -fairlet decomposition of $P$ with respect to $\\mathrm { c o s t } _ { \\mathrm { m e d i a n } } \\ o n \\ T$ $T$ .\n\nThus, by Theorem 4.1 and the bound on the expected distortion of embeddings into HST metrics (Theorem 2.3), we can prove Theorem 3.3.\n\nProof of Theorem 3.3. We first embed the points into an $O ( r ^ { 5 } + b ^ { 5 } )$ -HST $T$ and then perform the algorithm guaranteed in Theorem 4.1. By Theorem 2.3, the expected distortion of our embedding to $T$ is $O ( d \\cdot \\gamma \\cdot \\log _ { \\gamma } n )$ and by Theorem 4.1, there exists an algorithm that computes an $O ( r ^ { 3 } + b ^ { 3 } )$ - approximate fairlet-decomposition of $P$ with respect to distances in $T$ . Hence, the overall algorithm achieves $O ( d \\cdot ( r ^ { 8 } + b ^ { 8 } ) \\cdot \\log n )$ -approximation.\n\nSince the embedding $T$ can be constructed in time $O ( d \\cdot n \\cdot \\log n )$ and the fairlet-decomposition algorithm of Theorem 4.1 runs in near-linear time, the overall algorithm also runs in ${ \\widetilde { \\cal O } } ( n )$ . \u0003\n\nBefore describing the algorithm promised in Theorem 4.1, we define a modified cost function $\\mathrm { c o s t } _ { \\mathrm { m e d } }$ which is a simplified variant of $\\mathrm { c o s t } _ { \\mathrm { m e d i a n } }$ and is particularly useful for computing the cost over trees. Consider a $\\gamma$ -HST embedding of $P$ denoted by $T$ and assume that $\\mathcal { X }$ is a fairlet decomposition of $P$ . Moreover, $h ( D )$ denotes the height of $| { \\mathsf { c a } } ( D ) $ in $T$ . For each fairlet $D$ , $\\mathrm { c o s t } _ { \\mathrm { m e d } } ( D )$ is defined as\n\nIn particular, $\\mathrm { c o s t } _ { \\mathrm { m e d } } ( D )$ relaxes the task of finding “the best center in fairlets” and at the same time, its value is within a small factor of $\\mathrm { c o s t } _ { \\mathrm { m e d i a n } } ( D )$ .\n\nClaim 4.2. Suppose that $T$ is a $\\gamma$ -HST embedding of the set of points $P$ . For any $( r , b )$ -fairlet $S$ of $P$ , costmedian(S) ≤ costmed(S) ≤ (r + b) · costmedian(S) where the distance function is defined w.r.t. $T$ .\n\nIn the rest of this section we prove the following result which together with Claim 4.2 imply Theorem 4.1.\n\nLemma 4.3. There exists a near-linear time algorithm that given an $O ( r ^ { 5 } + b ^ { 5 } )$ -HST embedding $T$ of the point set $P$ and balance parameters $( r , b )$ , computes an $O ( r ^ { 2 } + b ^ { 2 } )$ -approximate $( r , b )$ -fairlet decomposition of $P$ with respect to $\\mathrm { c o s t } _ { \\mathrm { m e d } }$ on $T$ .\n\nLemma 4.4. For any tree $T$ with the root vertex $\\boldsymbol { v }$ , the number of heavy points with respect to $\\boldsymbol { v }$ in the $( r , b )$ -fairlet decomposition constructed by MinHeavyPoints $( v , r , b )$ is at most $O ( r ^ { 2 } + b ^ { 2 } )$ times the minimum number of heavy points in any valid $( r , b )$ -fairlet decomposition of $T$ .",
      "referring_paragraphs": [
        "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ .",
        "Although the number of children of $T ( v )$ can be as large as $\\gamma ^ { d }$ , for each node $v$ in $T$ , MinHeavyPoints performs\n\nTable 1: The table compares the performance of our fairlet-decomposition algorithm and the algorithm of (Chierichetti et al., 2017).",
        "Comparing the cost of the solution returned by our fairlet decomposition algorithm with the result of (Chierichetti et al., 2017) (as in Table 1) shows that we achieve empirical improvements on all instances."
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(d) Stage 2: we can connect 1 red point from the middle node through the root.",
        "(e) Stage 3: we add the unsaturated fairlet in the right node to the root and make it balanced.",
        "1902.03519_page0_fig4.jpg",
        "(f) The final fairlet clustering.",
        "1902.03519_page0_fig5.jpg",
        "1902.03519_page0_fig6.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig6.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1902.03519",
      "figure_id": "1902.03519_fig_8",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg",
      "image_filename": "1902.03519_page0_fig7.jpg",
      "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.",
      "context_before": "",
      "context_after": "[Section: Acknowledgment]\n\nThe authors would like to thank Ravi Kumar for many helpful discussions. This project was supported by funds from the MIT-IBM Watson AI Lab, NSF, and Simons Foundation.\n\n[Section: References]",
      "referring_paragraphs": [
        "At the same time, the empirical runtime of our algorithm scales almost linearly in the number of points, making it applicable to massive data sets (see Figure 2).",
        "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II."
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "1902.03519_page0_fig8.jpg",
        "1902.03519_page0_fig9.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig9.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1902.07823": [
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig0.jpg",
      "image_filename": "1902.07823_page0_fig0.jpg",
      "caption": "",
      "context_before": "then the prediction stability $\\mathcal { A }$ is upper bounded by $\\operatorname* { P r } _ { S , A } \\left[ | A _ { S } ( X ) | \\le \\tau \\beta _ { N } \\right]$ .\n\nProof. For any $S , S ^ { i } \\in \\mathcal { D } ^ { N }$ and $\\boldsymbol { s } = ( x , z , y )$ , we have\n\nHence, if $| \\mathcal { A } _ { S } ( \\cdot ) | > \\tau \\beta _ { N }$ , then we have\n\nBy Definition 2.2, this implies the lemma.",
      "context_after": "[Section: 2.3 The stable and fair optimization problem]\n\nOur goal is to design fair classification algorithms that have a uniform stability guarantee. We focus on extending fair classification algorithms that are formulated as constrained empirical risk minimization problem over the collection $\\mathcal { F }$ of classifiers that is a reproducing kernel Hilbert space (RKHS), e.g., [77, 78, 34]; see the following program.\n\nHere, $\\Omega _ { S } ( \\cdot ) : \\mathcal { F }  \\mathbb { R } ^ { a }$ is a convex function given explicitly for a specific fairness requirement on the dataset $S$ . When $S$ is clear by the content, we may use $\\Omega ( \\cdot )$ for simplicity. For instance, if we consider the statistical rate $\\gamma ( f )$ (Eq. (8)) as the fairness metric, then the fairness requirement can be $0 . 8 - \\gamma ( f ) \\leq 0$ . However, $0 . 8 - \\gamma ( f )$ is non-convex with respect to $f$ . To address this problem, in the literature, one usually defines a convex function $\\Omega ( f )$ to estimate $0 . 8 - \\gamma ( f )$ , e.g., $\\Omega ( f )$ is formulated as a covariance-type function which is the\n\naverage signed distance from the feature vectors to the decision boundary in [78], and is formulated as the weighted sum of the logs of the empirical estimate of favorable bias in [34]. In what follows, a fair classification algorithm is an algorithm that solves Program (ConFair).\n\nNote that an empirical risk minimizer of Program (ConFair) might heavily depend on and even overfit the training set. Hence, replacing a sample from the training set might cause a significant change in the learnt fair classifier – the uniform stability guarantee might be large. To address this problem, a useful high-level idea is to introduce a regularization term to the objective function, which can penalize the “complexity” of the learned classifier. Intuitively, this can make the change in the learnt classifier smaller when a sample from the training set is replaced. This idea comes from [10] who considered stability for unconstrained empirical risk minimization.\n\nMotivated by the above intuition, we consider the following constrained optimization problem which is an extension of Program (ConFair) by introducing a stability-focused regularization term $\\lambda \\| f \\| _ { k } ^ { 2 }$ . Here, $\\lambda > 0$ is a regularization parameter and $\\| f \\| _ { k } ^ { 2 }$ is the norm of $f$ in RKHS $\\mathcal { F }$ where $k$ is the kernel function (defined later in Definition 2.5). We consider such a regularization term since it satisfies a nice property that relates $| f ( x ) |$ and $\\| f \\| _ { k }$ for any $x \\in \\mathcal { X }$ (Claim 2.6). This property is useful for proving making the intuition above concrete and providing a uniform stability guarantee.",
      "referring_paragraphs": [],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.4,
      "metadata": {}
    },
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg",
      "image_filename": "1902.07823_page0_fig1.jpg",
      "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.",
      "context_before": "[Section: 5 Empirical results]\n\n[Section: 5.1 Empirical setting]\n\nAlgorithms and baselines. We select three fair classification algorithms designed to ensure statistical parity that can be formulated in the convex optimization framework of Program (ConFair). We choose ZVRG [77] since it is reported to achieve the better fairness than, and comparable accuracy to, other algorithms [32]. We also select KAAS [43] and GYF [34] as representatives of algorithms that are formulated as Program (RegFair). Specifically, [34] showed that the performance of GYF is comparable to ZVRG over the Adult dataset. We extend them by introducing a stability-focused regularization term.4\n\n• ZVRG [78]. Zafar et al. re-express fairness constraints (which can be nonconvex) via a convex relaxation. This allows them to maximize accuracy subject to fairness constraints.5 We denote the extended, stability included, algorithm by ZVRG-St.   \n• KAAS [43]. Kamishima et al. introduce a fairness-focused regularization term and apply it to a logistic regression classifier. Their approach requires numerical input and a binary sensitive attribute. Let the extended algorithm be KAAS-St.   \n• GYF [34]. Goel et al. introduce negative weighted sum of logs as fairness-focused regularization term and apply it to a logistic regression classifier. Let the extended algorithm be GYF-St.\n\nDataset. Our simulations are over an income dataset Adult [23], that records the demographics of 45222 individuals, along with a binary label indicating whether the income of an individual is greater than 50k USD or not. We use the pre-processed dataset as in [32]. We take race and sex to be the sensitive attributes, that are binary in the dataset.\n\nStability metrics. The following stability metric that measures the prediction difference between classifiers learnt from two random training sets. Given an integer $N$ , a testing set $T$ and algorithm $\\mathcal { A }$ , we define stabT (A) as",
      "context_after": "Adult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.",
      "referring_paragraphs": [
        "Table 1: The performance (mean and standard deviation in parenthesis), of KAAS-St and ZVRG-St with respect to accuracy and the fairness metrics $\\gamma$ on the Adult dataset with race/sex attribute.",
        "Adult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.",
        "Our framework can be easily extended to other fairness metrics; see a summary in Table 1 of [14].",
        "Table 1 summarizes the accuracy\n\nand fairness metric under different regularization parameters $\\lambda$ ."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1902.07823",
      "figure_id": "1902.07823_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg",
      "image_filename": "1902.07823_page0_fig2.jpg",
      "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.",
      "context_before": "Adult dataset,race attribute   \nFigure 1: stab vs. $\\lambda$ for race attribute.",
      "context_after": "Adult dataset, sex attribute   \nFigure 2: stab vs. $\\lambda$ for sex attribute.\n\n$\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ indicates the expected number of different predictions of $A _ { S }$ and $\\mathbf { \\boldsymbol { A } } _ { S ^ { \\prime } }$ over the testing set $T$ . Note that this metric is considered in [32], but is slightly different from prediction stability since $S$ and $S ^ { \\prime }$ may differ by more than one training sample. We investigate $\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ instead of prediction stability so that we can distinguish the performances of prediction difference under different regularization parameters. Since $\\Im$ is unknown, we generate $n$ training sets $S _ { 1 } , \\ldots , S _ { n }$ and use the following metric to estimate $\\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ :\n\nNote that we have $\\mathbb { E } _ { S _ { 1 } , . . . , S _ { n } } \\left[ \\mathrm { s t a b } _ { T , n } ( \\mathcal { A } ) \\right] = \\mathrm { s t a b } _ { T } ( \\mathcal { A } )$ .\n\nFairness metric. Let $D$ denote the empirical distribution over the testing set. Given a classifier $f$ , we consider a fairness metric for statistical rate, which has been applied in [57, 4]. Suppose the sensitive attribute is binary, i.e., $Z \\in \\{ 0 , 1 \\}$ .\n\nOur framework can be easily extended to other fairness metrics; see a summary in Table 1 of [14].\n\nImplementation details. We first generate a common testing set (20%). Then we perform 50 repetitions, in which we uniformly sample a training set $( 7 5 \\% )$ from the remaining data. For all three algorithms, we set the regularization parameter $\\lambda$ to be 0, 0.01, 0.02, 0.03, 0.04, 0.05 and compute the resulting stability metric stab, average accuracy and average fairness. Note that $\\lambda = 0$ is equivalent to the case without stability-focused regularization term.",
      "referring_paragraphs": [
        "Adult dataset, sex attribute   \nFigure 2: stab vs."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1903.08136": [
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg",
      "image_filename": "1903.08136_page0_fig0.jpg",
      "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.",
      "context_before": "Community detection is an important task in social network analysis, allowing us to identify and understand the communities within the social structures. However, many community detection approaches either fail to assign low degree (or lowly-connected) users to communities, or assign them to trivially small communities that prevent them from being included in analysis. In this work, we investigate how excluding these users can bias analysis results. We then introduce an approach that is more inclusive for lowly-connected users by incorporating them into larger groups. Experiments show that our approach outperforms the existing state-of-the-art in terms of F1 and Jaccard similarity scores while reducing the bias towards low-degree users.\n\n[Section: 1 INTRODUCTION]\n\nCommunity detection is a fundamental task in social network analysis [19], which identifies sub-groups within social networks. These groups can represent a variety of things including karate club membership, political leanings, and deeply-held beliefs. Traditionally, these groups are identified by searching for densely-connected groups of nodes in the graph [14]. More recently, attributed approaches go beyond merely the links to cluster nodes based upon their attributes and their network connections [18]. Existing community detection approaches suffer from a major flaw: the inability to assign lowly-connected users into communities. Despite the fact that lowly-connected users are not well-blended into the social network, the information they provide can be crucial for better understanding the motivations and beliefs of the community, especially considering there could be a long-tail of lowly-connected users. Failure to incorporate the lowly-connected users may result in biased results. For instance, studying groups within a social network, such as a Twitter retweet network, can be biased towards users whose tweets get a large number of retweets. This will lead to a biased analysis of the data in which not all users’ voices are heard. Instead, those high-degree users with non-genuine retweets will end up having more voice in the study. Consider the toy example shown in Figure 1. Each node represents a user and the shapes represent difference of opinion in the network; the shade and different texture of each shape represent their variability of expressing opinions. Traditional community detection methods that solely focus on the highly-connected nodes would only capture the opinions of users within the dark-grey area; however, many diverse opinions are lost due to the fact that the users in the light grey area are excluded because of their low degree in the network.\n\nFred Morstatter\n\nUSC Information Sciences Institute\n\nfredmors@isi.edu\n\nAram Galstyan\n\nUSC Information Sciences Institute\n\ngalstyan@isi.edu\n\nThe goal of this paper is twofold. First, we demonstrate empirically the existence of biases in existing community detection approaches using real-world datasets. Through the analysis, we show that current state-of-the-art community detection methods suffer from ignoring low-degree users that have few links either by failing to incorporate them, or by putting them into small groups which are then ignored in the study. These low-degree users have value to be included in the communities, as they offer a more diverse, nuanced representation of the communities. Second, to overcome this issue, we introduce a new community detection method, Communities with Lowly-connected Attributed Nodes (CLAN), that would mitigate the existence of this bias towards low-degree nodes.",
      "context_after": "[Section: 2 BIASES IN COMMUNITY DETECTION METHODS]\n\nAnalysis on community detection tends to focus on the largest communities. Methods that tend to exclude low-degree nodes are at greater risk of losing information in their detected significant communities. We demonstrate the existence of this bias by showing what is omitted by existing community detection approaches. We use two state-of-the-art approaches, CESNA [21], and the Louvain method [3]. Louvain uses only the network while assigning communities, while CESNA uses both the network and user attributes. We apply these methods to two separate datasets, one based on Gamergate and one based on the 2016 U.S. Presidential Election. Both of these datasets contain two major contingents, and ground truth information (both datasets, and the methodology for obtaining ground truth, will be introduced in detail later). Table 1 shows the information that is omitted by excluding the lowly-connected users. These statistics are taken by comparing the ground truth labels with the labels assigned by the community detection algorithm. For example, in one of the methods, CESNA, the “#refugeeswelcome” hashtag and the user who tweeted this hashtag remained unlabeled by the user being put in a small insignificant community from the election dataset, discussed in the later sections. By not including this user tweeting this hashtag, we lose this piece of information when we analyze the communities. In other words, degree is not correlated with relevancy to the topic and therefore merely degree and connections in social networks should not be indicative of community membership. Other examples of such are demonstrated in Table 1, such as the “#Christians4Hillary,” “#gamersagainstgamergate,” and many other hashtags of such type in two of the datasets that we will be using in this paper. The goal of this work is to introduce a method that would mitigate this bias by the inclusion of low-degree users into their correct belonging significant communities.\n\n[Section: 3 RELATED WORK]",
      "referring_paragraphs": [
        "Consider the toy example shown in Figure 1.",
        "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods.",
        "Table 1 shows the information that is omitted by excluding the lowly-connected users."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg",
      "image_filename": "1903.08136_page0_fig1.jpg",
      "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.",
      "context_before": "The populations used in this study are drawn from social media with a particular focus on datasets with node attributes, such as text, and a social network structure. Moreover, we want datasets where the underlying communities come from different backgrounds. To satisfy this, we utilized two different datasets: Gamergate and U.S. Presidential Election. Both datasets have ground truth community labels. Our process for obtaining these labels will be discussed in detail.\n\n[Section: 4.1 Gamergate]\n\nThe Gamergate dataset consists of tweets posted in 2014 between months of August through October. The tweets surround the Gamergate controversy [16]. It contains 21,441 users who collectively produced 104,914 tweets. These users fall into one of the two groups surrounding the controversy. One group consists of Gamergate supporters who are tweeting about ethics in journalism and believe that regardless of the relationship between journalists and game developers, journalists should give honest reviews to game developers. The other group, Gamergate opposers, argues that Gamergate supporters attack female game developers and also feminist critics, and that they are not concerned with ethics in journalism, but are using the opportunity to attack women in the gaming industry.\n\nIn this study, we conducted an Amazon Mechanical Turk experiment discussed later in the paper to obtain ground truth labels for each of the users in this controversy. The retweet network of this dataset is shown in Figure 3.",
      "context_after": "[Section: 4.2 U.S. Presidential Election]\n\nThis dataset contains 10,074 users who discuss the U.S. presidential election in 2016. This dataset consists of two major groups which indicate the political party of each user. This dataset comes from [1] in which we only utilized the seed users from the whole dataset which brought our dataset size down from more than million users to 10,074 users since we required pure ground truth labels that were obtained away from the network structure and label propagation. The network structure of this dataset is shown in Figure 2.\n\n[Section: 5 OBTAINING GROUND TRUTH LABELS FOR GAMERGATE]",
      "referring_paragraphs": [
        "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.",
        "The network structure of this dataset is shown in Figure 2.",
        "Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels.",
        "Table 2: The quantitative results obtained from calculating the F1 and Jaccard similarity scores with regards to the ground truth labels for each of the methods.",
        "Table 2 contains results for the F1 and Jaccard similarity scores obtained from comparisons done between the ground truth labels and labels obtained by applying each of the methods on the two datasets on hand."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg",
      "image_filename": "1903.08136_page0_fig2.jpg",
      "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.",
      "context_before": "In order to collect labels for each user in the Gamergate dataset, all the tweets associated to a user were mapped to the particular user so that the dataset was on a user level. Out of 21,441 total users, we excluded users who had only single tweets and duplicated users with same tweets. We then asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left based on their tweets into one of the following groups:\n\n(1) Gamergate Supporter: Users fighting for ethics in journalism and criticizing some women game-developers and journalists for their unethical relationships.   \n(2) Gamergate Opposer: Users advocating for women’s rights and protecting attacks from Gamergate supporters to female game-developers and feminist advocates.   \n(3) Unaffiliated: Users who belong to neither of the groups.\n\nTurkers were given complete description of the controversy and a detailed explanation of the labeling procedure. In order to make sure that the turkers were following the standards, some sanity check questions were put under each page for us to be able to identify bot turkers. These sanity check questions were trivial, made-up users with tweets that were easy to be categorized into one of the three groups, Gamergate Opposer, Gamergate Supporter, and Unaffiliated. After identifying bot turkers and excluding their labels from the total labels, we took the maximum agreement between 8,128 users that were labeled by at least three turkers. This resulted in a new dataset discussed in the next section which served as our ground truth.\n\n[Section: 5.2 Dataset]\n\nThe Gamergate dataset had 21,441 users initially. In this study, we consider only users who posted at least two original tweets. The resulting dataset contains 8,128 users which were then labeled by the turkers. After analyzing the Turker agreement between 8,128 labeled users, we considered only users where turkers agreed two out of three times on a single label. After this filtering, we have 7,320 labeled users in total. These 7,320 users served as our ground truth labels.",
      "context_after": "[Section: 5.3 Results]\n\nAfter obtaining the ground truth labels and having three ground truth groups of users, Gamergate supporters, Gamergate opposers, and unaffiliated, the communities obtained using network attributes were then compared with the groups obtained by the labels from the Mechanical Turk experiment. Surprisingly these results had a very low agreement which will be discussed in detail in \"Community Detection Results\" section. Figure 2 confirms this fact by showing the disagreement between the left hand side picture, which is colored based on the network structure, and the picture on the right colored by the ground truth labels. There is a significant amount of disagreement between these two results. The purple nodes represent Gamergate opposers and the green nodes represent Gamergate supporters. Using network structure and attributes would put almost all of Gamergate supporters in the green portion of the network and Gamergate opposers in the purple section of the network completely separated; however, the ground truth labels tend to have mixed users into each of the sections. The ground truth results are expected as many opposers may retweet Gamergate supporters; therefore, using network attributes merely on the retweet network might not be a good idea for separating these users, and other attributes and characteristics of users can be used for a more accurate community detection task. These results illustrate the fact that network does not explain everything and additional information is required in order to obtain accurate communities of users. This motivated us to come up with a new method that would not only address the bias from creating small communities and exclusion of low-degree nodes but would also have higher agreement with the ground truth communities by using other node attributes in addition to the network structure which will be discussed in the next section.\n\n[Section: 6 PROPOSED METHOD]",
      "referring_paragraphs": [
        "The retweet network of this dataset is shown in Figure 3.",
        "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.",
        "Presidential Election Dataset</td></tr><tr><td>Method</td><td>F1 Score</td><td>Jaccard</td><td>F1 Score</td><td>Jaccard</td></tr><tr><td>CESNA</td><td>0.343</td><td>0.211</td><td>0.253</td><td>0.149</td></tr><tr><td>Modularity</td><td>0.434</td><td>0.282</td><td>0.753</td><td>0.604</td></tr><tr><td>CLAN</td><td>0.478</td><td>0.318</td><td>0.787</td><td>0.649</td></tr></table>\n\nTable 3: Percentage of unlabeled users in each of the methods.",
        "In Table 3, we reported the percentage of the users who were left unlabeled or put in insignificant communities in each of the methods from the two datasets.",
        "As expected, CESNA would have many red nodes as this method tends to have a very low recall value and as shown in Table 3, this method has a high tendency to exclude many users by not assigning them a label."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1903.08136_page0_fig3.jpg",
        "1903.08136_page0_fig4.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig4.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg",
      "image_filename": "1903.08136_page0_fig5.jpg",
      "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.",
      "context_before": "",
      "context_after": "CESNA   \nFigure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.\n\n<table><tr><td>Gamerge Tweet</td><td>CESNA Label</td><td>Modularity Label</td><td>CLAN Label</td><td>Ground Truth Label</td></tr><tr><td>&quot;#gamerge is like a particularly lame version of pakistani twitter political flame wars like ridiculously lame&quot;</td><td>N/A</td><td>N/A</td><td>Anti Gamerge</td><td>Anti Gamerge</td></tr><tr><td>&quot;#gamerge is brutally put in its place by a journalist&quot;</td><td>N/A</td><td>N/A</td><td>Anti Gamerge</td><td>Anti Gamerge</td></tr><tr><td>&quot;#gamerge #done i suspect the #ottawashooting has more in common with #montrealmassacre #columbine #gamerge than it does with #islam&quot;</td><td>N/A</td><td>N/A</td><td>Anti Gamerge</td><td>Anti Gamerge</td></tr><tr><td>&quot;#gamerge i support #gamerge and #notyour-shield i stand against harassment threats and doxing no matter who or why&quot;</td><td>N/A</td><td>N/A</td><td>Gamerge Sup-porter</td><td>Gamerge Supporter</td></tr><tr><td>&quot;#gamerge #extralife2014 only two more days until our stream starts are you hyped we sure are current score to beat 100. I just supported #gamerge extra life charity&quot;</td><td>N/A</td><td>Gamerge Sup-porter</td><td>Gamerge Sup-porter</td><td>Gamerge Supporter</td></tr></table>\n\nTable 4: Sample Gamergate users with their corresponding tweets and labels assigned to each user by the three methods. “N/A” means that the method failed to classify the user.\n\nshowing some visualized results and real examples drawn from our datasets to further prove our results from the previous subsection. We will start our qualitative results by showing a visualization of the retweet network in the Gamergate dataset in the three methods discussed in this paper. Each node in these graphs represent a user and the nodes are color coded based on their agreement with the ground truth labels. The green nodes represent agreement between the label that was assigned to that particular user obtained from the method used and the ground truth label, and the red node represent disagreement between the two labels associated to that node. Therefore, more green nodes in a graph represent the degree of agreement of that method with the ground truth label and generally its superiority in terms of agreement with the ground truth compared to the other methods. The results of these visualizations are shown in Figure 4. In addition to disagreement, the red nodes may also represent the fact that a method has low recall value and that many users were assigned no labels while the ground truth has assigned it a label. This of course is a sort of disagreement between the labels, so the nodes are colored as red. As expected, CESNA would have many red nodes as this method tends to have a very low recall value and as shown in Table 3, this method has a high tendency to exclude many users by not assigning them a label. Therefore, it suffers from low agreement with the ground truth and as expected highly covered by red nodes.\n\nThis also confirms the existence of bias towards these red users who suffered from CESNA’s low recall issue. The result associated to this method is shown on the far right side of Figure 4. As we move to the next method in the middle of Figure 4, we see less red nodes. This is because using modularity value has a higher recall and generally more agreement with the ground truth, but one can still spot many red users in this method. Moving on to the last graph on the far left side of Figure 4, we can see the graph associated to CLAN. Due to CLAN’s use of node attributes in addition to the network attributes, it is able to obtain higher agreement with the ground truth and therefore less red nodes compared to the previous method. CLAN was able to address many red nodes in the top portion as well as the bottom portion of the graph compared to the modularity method. In addition to the agreement graphs shown in Figure 4, we provided two sets of tables, Table 4 and 5, in which some examples from each of the datasets are provided where it shows how each of the methods labeled a particular user with sets of tweets they tweeted. The ground truth labels are also listed for comparison purposes. Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset. These examples also highlight the fact that the baseline approaches are suffering from the type of bias that roots from exclusion of users by not assigning\n\n<table><tr><td>2016 U.S. Presidential Election Tweet</td><td>CESNA Label</td><td>Modularity Label</td><td>CLAN Label</td><td>Ground Truth Label</td></tr><tr><td>“Trump wants to ban Muslim immigrants like my parents. I wrote a piece for telling him to go fuck himself.”</td><td>N/A</td><td>N/A</td><td>Democrat</td><td>Democrat</td></tr><tr><td>“Frauds at NY Times coverup Hillary’s serious crimes...but try to make Trump into a criminal for LEGAL tax reduction strategy. WOW.”</td><td>N/A</td><td>Republican</td><td>Republican</td><td>Republican</td></tr><tr><td>“Fox News Host Makes Hillary Supporter Admit That Clinton Foundation Is A Huge Scam.”</td><td>Democrat</td><td>Republican</td><td>Republican</td><td>Republican</td></tr></table>\n\nTable 5: Sample 2016 U.S. Presidential Election users with their corresponding tweets and labels assigned by the three methods.",
      "referring_paragraphs": [
        "CESNA   \nFigure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1903.08136_page0_fig6.jpg",
        "1903.08136_page0_fig7.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig7.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_9",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg",
      "image_filename": "1903.08136_page0_fig8.jpg",
      "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.",
      "context_before": "",
      "context_after": "[Section: 8 CLAN’S RESILIENCE TO SKEWED DATA]\n\nOne potential issue with community detection is that it could completely ignore entire communities if membership in that community is correlated with degree. For example, a community of introverts is likely to be overlooked because they do not form many links. This presents a challenge to our approach as well, because Step 1 consists of identifying the major communities in the dataset. In order to test our method against this potential drawback, we conducted an experiment in which methods discussed in this paper would be tested in a synthetic environment in which the community distribution is skewed according to degree. This experiment serves two goals. First, it would compare the methods in terms of their performance on a different set of data. Secondly, it would show our method’s behavior along with other methods while the distribution of each dataset is changed in terms of the node degree in the network which correlates with the popularity of a user in the social network.\n\n[Section: 8.1 Experimental Setup]",
      "referring_paragraphs": [
        "Table 4 contains the results from the Gamergate dataset, while Table 5 contains the results for the 2016 presidential election dataset.",
        "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.",
        "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network representation. Figure 6 contains the same graphs and networks for the U.S. Presidential Election dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1903.08136_page0_fig9.jpg",
        "1903.08136_page0_fig10.jpg",
        "1903.08136_page0_fig11.jpg",
        "1903.08136_page0_fig12.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig12.jpg"
        ],
        "panel_count": 5
      }
    },
    {
      "doc_id": "1903.08136",
      "figure_id": "1903.08136_fig_14",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg",
      "image_filename": "1903.08136_page0_fig13.jpg",
      "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.",
      "context_before": "",
      "context_after": "Degree   \nFigure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.\n\n[Section: 9 CONCLUSIONS AND FUTURE WORK]\n\nIn this paper, we introduce a new community detection method that mitigates the bias in existing detection methods that fail to properly account for sparsely-connected nodes in social networks. CLAN minimizes such biases by including the lowly-connected nodes into their true communities. Our empirical results demonstrate that inclusion of those users enables CLAN to achieve overall superior performance in terms of F1-score and Jaccard similarity. We reported these results by providing evidence through our qualitative and quantitative experiments. Through qualitative analysis, we are able to show that these lowly-connected users, in aggregate, offer information that can be of use for analysis of social network data.\n\nFinally, we show that our method is capable of outperforming other methods not only in real datasets but also in different types of synthetic environments with different population distributions, namely distributions where the users community is correlated with their connectivity. The results reported the performance of methods with regards to F1 and Jaccard similarity scores.\n\nWe see two promising directions for future work. First, to extend this method to introduce a new hierarchical community detection method capable of detecting subgroups within larger communities. Second, we would like to consider edge attributes simultaneously with node attributes to enable this method to consider different types of connections.",
      "referring_paragraphs": [
        "In Figure 5, the original distribution of the Gamergate dataset is shown on the top right corner of the figure with its corresponding network colored with the modularity value, and one of the synthetic distributions with a particular slope is shown in the bottom left with its corresponding network representation. Figure 6 contains the same graphs and networks for the U.S. Presidential Election dataset.",
        "Degree   \nFigure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1903.08136_page0_fig14.jpg",
        "1903.08136_page0_fig15.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig15.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1903.10561": [
    {
      "doc_id": "1903.10561",
      "figure_id": "1903.10561_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg",
      "image_filename": "1903.10561_page0_fig0.jpg",
      "caption": "Figure 1: Significance of results for all models and tests.",
      "context_before": "CBoW: As a simple baseline, we encode sentences as an average of the word embeddings. We\n\nuse 300-dimensional GloVe vectors trained on the Common Crawl (Pennington et al., 2014).\n\nInferSent: A 4096-dimensional BiLSTM trained on both MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) with max pooling over the hidden states of the sequence (Conneau et al., 2017).\n\nGenSen: A 2048-dimensional BiLSTM jointly trained on MultiNLI, SNLI, next sentence prediction, translation, and constituency parsing, concatenated to a similar BiLSTM trained without parsing; denoted $^ { \\mathrm { \\tiny { < < } } } \\mathrm { + } \\mathrm { S T N + F r + D e \\thinspace + N L I + L \\thinspace + S T P }$ $+ \\mathrm { P a r } ^ { \\prime \\prime }$ in Subramanian et al. (2018). We take the 4096-dimensional last hidden state of the sequence as the overall sentence encoding (Subramanian et al., 2018). In the full set of results we also evaluate the component models individually (the BiLSTM jointly trained on MultiNLI, SNLI, next sentence prediction, translation, and constituency parsing, and separately the BiLSTM jointly trained on MultiNLI, SNLI, next sentence prediction, and translation).\n\nUniversal Sentence Encoder (USE): A variant of the deep averaging network (Iyyer et al., 2015), which passes an average of unigram and bigram embeddings in the sentence to a feedforward neural network to produce a 512-dimensional sentence encoding. The model is trained on SNLI, Wikipedia, web news, and other online sources (Cer et al., 2018).\n\nELMo: A pair of two-layer LSTM language models: one processes the text in order and the other in reverse. For each word in the sentence, the corresponding hidden state of the two language models are concatenated. The sentence encoding is then a sequence of vectors, one per word. To accommodate ELMo to the association tests, we use mean-pooling over the sequence followed by summation over the aggregated layer outputs; the resulting vector is 1024-dimensional. Summing layer outputs produces a constant multiple of mean pooling, a special case of the weighted-mean layer combination proposed in the original work (Peters et al., 2018). In the full set of results we also evaluate max-pooling over the sequence and then summing layer outputs, as well as max-pooling over the sequence and then concatenating layer outputs.\n\nGPT: A unidirectional Transformer (Vaswani et al., 2017) language model trained on Toronto\n\nBook Corpus (Zhu et al., 2015). We use the 768- dimensional top hidden state corresponding to the last word in the sequence as the overall sentence representation, as per the original work (Radford et al., 2018).\n\nBERT: A bidirectional Transformer trained on filling in missing words in a sentence and next sentence prediction. Each sentence is prepended with a special [CLS] token, and we use the top-most hidden state corresponding to [CLS] as a vector representation of the whole sequence, as per the original work (Devlin et al., 2018). We report results using the 1024-dimensional “large” cased version. In the full set of results we also evaluate the “base” cased, “large” uncased, and “base” uncased versions.\n\n[Section: D Results]\n\nA full set of results is provided in the included tab-separated value (TSV) file, results.tsv, of the supplementary data. This file has nine columns; the first row is a header containing the names of the columns, as described in Table 5.\n\nThe Holm-Bonferroni multiple testing correction applied in the paper is computed over all rows in this file (except the header), as follows. Let $n$ be the number of rows. Sort the rows by $p { \\cdot }$ - value in increasing order. Let $P _ { ( r ) }$ be the $p$ -value at rank $r$ in the sorted list and let $H _ { ( r ) }$ be the corresponding (null) hypothesis, such that $r = 1$ for the first (smallest) $p$ -value and $ { \\boldsymbol { r } } \\equiv n$ for the last (largest) $p$ -value. Given a significance level $\\alpha$ (in our case $\\alpha ~ = ~ 0 . 0 1$ ), find the smallest rank $k$ such that $P _ { ( k ) } > \\alpha / ( 1 + n - k )$ , reject $H _ { ( 1 ) } , \\ldots , H _ { ( k - 1 ) }$ at significance level $\\alpha$ and do not reject $H _ { ( k ) } , \\ldots , H _ { ( n ) }$ (Holm, 1979).\n\nWe also provide a visualization of our results: Figure 1 depicts the significant results in our matrix of models and bias tests.\n\nTable 5: Names and descriptions of columns in results.tsv.   \n\n<table><tr><td>model</td><td>Name of the model</td></tr><tr><td>options</td><td>Options passed to the model (model variation)</td></tr><tr><td>test</td><td>Name of the bias test, corresponding to a bias test JSON file</td></tr><tr><td>p_value</td><td>The p-value (before multiple testing correction)</td></tr><tr><td>effect_size</td><td>The effect size</td></tr><tr><td>num_targ1</td><td>Number of words/sentences in the 1sttarget concept set</td></tr><tr><td>num_targ2</td><td>Number of words/sentences in the 2ndtarget concept set</td></tr><tr><td>num_attr1</td><td>Number of words/sentences in the 1stattribute set</td></tr><tr><td>num_attr2</td><td>Number of words/sentences in the 2ndattribute set</td></tr></table>",
      "context_after": "",
      "referring_paragraphs": [
        "Table 1: Subsets of target concepts and attributes from Caliskan Test 3.",
        "These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2.",
        "To measure sentence encoders’ reproduction of the angry black woman stereotype, we create a test whose target concepts are black-identifying and white-identifying female given names from Sweeney (2013, Table 1) and whose attributes are adjectives used in the discussion of the stereotype in Collins (2004, pp.",
        "(2017, Table 1) row $N$ ; *: significant at 0.01, **: significant at 0.01 after multiple testing correction.",
        "We also provide a visualization of our results: Figure 1 depicts the significant results in our matrix of models and bias tests.",
        "Figure 1: Significance of results for all models and tests."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1903.10598": [
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg",
      "image_filename": "1903.10598_page0_fig0.jpg",
      "caption": "",
      "context_before": "For $\\nu \\in \\mathcal V$ , let $\\mathcal { L } ^ { \\mathrm { r } } ( \\nu )$ (resp. $\\mathcal { L } ^ { 1 } ( \\nu ) )$ denote all the leaf nodes that lie to the right (resp. left) of node $\\nu$ . Denote with $\\mathbf { x } _ { i , j }$ the value attained by the $j$ th feature of the ith data point and for $j \\in \\mathcal { F } _ { \\mathrm { c } }$ , let $\\chi _ { j }$ collect the possible levels attainable by feature $j$ . Consider the following MIP\n\nminimize $\\ell _ { \\mathrm { c / r } } ( \\mathcal T , \\hat { y } ) + \\lambda \\ell _ { \\mathrm { c / r } } ^ { \\mathrm { d } } ( \\mathcal T , \\hat { y } )$ (7a)\n\nsubject to $p \\in \\mathcal { P } , \\hat { y } \\in \\hat { \\mathcal { V } } ( z )$ (7b)\n\n(7c)\n\n$\\forall \\nu , i$ (7d)\n\n$\\forall \\nu , i$ (7e)\n\n$\\forall \\nu , i$ (7f)\n\n(7g)\n\n(7h)\n\n(7i)\n\n(7j)\n\n(7k)\n\n(7l)\n\n(7m)\n\nzil, wqiν , wciν , with variables $s _ { \\nu j k } \\in \\{ 0 , 1 \\}$ $p$ and $\\hat { y } ; \\ q _ { \\nu } , \\quad g _ { i \\nu } ^ { + } , \\quad g _ { i \\nu } ^ { - } \\in \\mathbb { R }$ i for all $i \\in \\mathcal N$ $i \\in \\mathcal { N } , l \\in \\mathcal { L } , \\nu \\in \\mathcal { V }$ ; and $j \\in \\mathcal { F }$ $j \\in \\mathcal { F } , k \\in \\mathcal { X } _ { j } , l \\in \\mathcal { L }$ $k \\in \\mathcal { X } _ { j }$ $l \\in \\mathcal L$ .\n\nAn interpretation of the variables other than z, p, and $\\hat { y }$ (which we introduced in Section 3.2) is as follows. The variables $q _ { \\nu }$ , $g _ { i \\nu } ^ { + }$ , $g _ { i \\nu } ^ { - }$ , and $w _ { i \\nu } ^ { \\mathrm { q } }$ are used to bound $z _ { i l }$ based on the branching decisions at each node $\\nu$ , whenever branching is performed on a quantitative feature at that node. The variable $q _ { \\nu }$ corresponds to the cut-off value at node $\\nu$ . The variables $g _ { i \\nu } ^ { + }$ and $g _ { i \\nu } ^ { - }$ represent the positive and negative parts of $\\begin{array} { r } { q _ { \\nu } - \\sum _ { j \\in \\mathcal { F } _ { \\mathrm { q } } } p _ { \\nu j } \\mathbf { x } _ { i , j } } \\end{array}$ , respectively. Whenever branching occurs on a quantitative (i.e., continuous or discrete and ordered) feature, the variable $w _ { i \\nu } ^ { \\mathrm { q } }$ will equal 1 if and only if $\\begin{array} { r } { q _ { \\nu } \\geq \\sum _ { j \\in \\mathcal { F } _ { \\mathrm { q } } } p _ { \\nu j } \\mathbf { x } _ { i , j } } \\end{array}$ , in which case the ith data point must go left in the branch. The variables $w _ { i \\nu } ^ { \\mathrm { c } }$ and $s _ { \\nu j k }$ are used to bound $z _ { i l }$ based on the branching decisions at each node $\\nu$ , whenever branching is performed on a categorical feature at that node. Whenever we branch on categorical feature $j \\in \\mathcal { F } _ { \\mathrm { c } }$ at node $\\nu$ , the variable $s _ { \\nu j k }$ equals 1 if and only if the points such that $\\mathbf { x } _ { i , j } = k$ must go left in the branch. If we do not branch on feature $j$ , then $s _ { \\nu j k }$ will equal zero. Finally, the variable $w _ { i \\nu } ^ { \\mathrm { c } }$ equals 1 if and only if we branch on a categorical feature at node $\\nu$ and data point $i$ must go left at the node.\n\nAn interpretation of the constraints is as follows. Constraints (7b) impose the adequate structure for the decision tree, see Examples 3.1-3.3. Constraints (7c)-(7h) are used to bound $z _ { i l }$ based on the branching decisions at each node $\\nu$ , whenever branching is performed on a quantitative attribute at that node. Constraints (7c)-(7f) are used to define $w _ { i \\nu } ^ { \\mathrm { q } }$ to equal 1 if and only if $\\begin{array} { r } { q _ { \\nu } \\geq \\sum _ { j \\in \\mathcal { F } _ { \\mathrm { q } } } p _ { \\nu j } \\mathbf { x } _ { i , j } } \\end{array}$ . Constraint (7g) stipulates that if we branch on a quantitative attribute at node $\\nu$ and the ith record goes left at the node (i.e., $w _ { i \\nu } ^ { \\mathrm { q } } = 1 \\mathrm { \\dot { \\Omega } }$ ), then that record cannot reach any leaf node that lies to the right of the node. Constraint (7h) is symmetric to $( 7 \\mathrm { g } )$ for the case when the data point goes right at the node. Constraints (7i)- (7l) are used to bound $z _ { i l }$ based on the branching decisions at each node $\\nu$ , whenever branching is performed on a categorical attribute at that node. Constraint (7i) stipulates that if we do not branch on attribute $j$ at node $\\nu$ , then $s _ { \\nu j k } = 0$ Constraint (7j) is used to define $w _ { i \\nu } ^ { \\mathrm { c } }$ such that it is equal to 1 if and only if we branch on a particular attribute $j$ , the value attained by that attribute in the ith record is $k$ and data points with attribute value $k$ are assigned to the left branch of the node. Constraints (7k) and (7l) mirror constraints (7g) and (7h), for the case of categorical attributes.\n\nWith the loss function taken as the misclassification rate or the mean absolute error and the discrimination loss function taken as one of the indices from Section 2, Problem (7) is a MIP involving a convex piecewise linear objective and linear constraints. It can be linearized using standard techniques and be written equivalently as an MILP. The number of decision variables (resp. constraints) in the problem is $\\mathcal { O } ( | \\mathcal { V } | | \\mathcal { F } | \\operatorname* { m a x } _ { j } | \\mathcal { X } _ { j } | + \\bar { | } \\mathcal { N } | | \\mathcal { V } | )$ (resp. $\\mathcal { O } ( | \\mathcal { V } | ^ { 2 } | \\dot { \\mathcal { N } } | +$ $| \\mathcal { V } | | \\mathcal { F } | \\operatorname* { m a x } _ { j } | \\mathcal { X } _ { j } | )$ , i.e., polynomial in the size of the dataset.\n\nRemark 3.1. Our approach of penalizing unfairness using a regularizer can be applied to existing MIP models for learning optimal trees such as the ones in (Verwer and Zhang 2017; Bertsimas and Dunn 2017). Contrary to these papers which require one-hot encoding of categorical features, our approach yields more interpretable and flexible trees.\n\nCustomizing Interpretability. An appealing feature of our\n\nframework is that it can cater for interpretability requirements. First, we can limit the value of $K$ . Second, we can augment our formulation through the addition of linear interpretability constraints. For example, we can conveniently limit the number of times that a particular feature is employed in a test by imposing an upper bound on $\\sum _ { \\nu \\in \\mathcal { V } } p _ { v j }$ . We can also easily limit the number of features employed in branching rules.\n\nRemark 3.2. Preference elicitation techniques can be used to make a suitable choice for $\\lambda$ and to learn the relative priorities of decision-makers in terms of the three conflicting objectives of predictive power, fairness, and interpretability.\n\n[Section: 4 Numerical Results]\n\nClassification. We evaluate our approach on 3 datasets: (A) The Default dataset of Taiwanese credit card users (Dheeru and Karra Taniskidou 2017; Yeh and Lien 2009) with $| \\mathcal { N } | = 3 0 , 0 0 0$ and $d = 2 3$ features, where we predict whether individuals will default and the protected attribute is gender; (B) The Adult dataset (Dheeru and Karra Taniskidou 2017; Kohavi 1996) with $| \\mathcal { N } | ~ = ~ 4 5 , 0 0 0$ , $d \\ = \\ 1 3$ , where we predict if an individual earns more than $\\$ 50\\mathbf { k }$ per year and the protected attribute is race; (C) The COMPAS dataset (Angwin et al. 2016; Corbett-Davies et al. 2017) with $| \\mathcal { N } | = 1 0 , \\bar { 5 } 0 0$ data points and $d = 1 6$ , where we predict if a convicted individual will commit a violent crime and the protected attribute is race. These datasets are standard in the literature on fair ML, so useful for benchmarking. We compare our approach (MIP-DT) to 3 other families: i) The MIP approach to classification where $\\lambda = 0$ (CART); ii) the discrimination-aware decision tree approach (DADT) of (Kamiran, Calders, and Pechenizkiy 2010) with information gain w.r.t. the protected attribute $\\mathbb { I } \\mathtt { G C } + \\mathbb { I } \\mathtt { G S } )$ and with relabeling algorithm (IGC+IGS Relab); iii) The fair logistic regression methods of (Berk et al. 2017) (log, log-ind, and $\\mathtt { l o g \\mathrm { - } g r p }$ for regular logistic regression, logistic regression with individual fairness, and group fairness penalty functions, respectively). Finally, we also discuss the performance of an Approximate variant of our approach (MIP-DT-A) in which we assume that individuals that have similar outcomes are similar and replace the distance between features in (4) by the distance between outcomes, as is always done in the literature (Berk et al. 2017). As we will see, this approximation results in loss in performance. In all approaches, we conduct a pre-processing step in which we eliminate the protected features from the learning phase. We do not compare to uninterpretable fairness in-processing approaches since we could not find any such approach.\n\nRegression. We evaluate our approach on the Crime dataset (Dheeru and Karra Taniskidou 2017; Redmond and Baveja 2002) with $\\vert \\mathcal { N } \\vert = 1 9 9 3$ and $d = 1 2 8$ . We add a binary column called “race” which is labeled 1 iff the majority of a community is black and 0 otherwise and we predict violent crime rates using race as the protected attribute. We use the “repeatedcv” method in R to select the 11 most important features. We compare our approach (MIP-DT and $\\mathtt { M I P - D T - A }$ where A stands for Approximate distance function) to 2 other families: i) The MIP regression tree approach where $\\lambda = 0$ (CART); ii) The linear regression methods in (Berk et al.\n\n2017) (marked as reg, LR-ind, and LR-grp for regular linear regression, linear regression with individual fairness, and group fairness penalty functions).\n\nFairness and Accuracy. In all our experiments, we use $\\mathsf { D T D l } _ { \\mathrm { c / r } }$ as the discrimination index. First, we investigate the fairness/accuracy trade-off of all methods by evaluating the performance of the most accurate models with low discrimination. We do $k$ -fold cross validation where for classification (regression) $k$ is 5(4). For each (fold, approach) pair, we select the optimal $\\lambda$ (call it $\\lambda ^ { \\star }$ ) in the objective (6) as follows: for each $\\lambda$ in $\\{ 0 , 0 . 1 , 0 . 2 , \\ldots \\}$ , we compute the tree on the fold using the given approach and determine the associated discrimination level on the fold; we stop when the discrimination level is $< 0 . 0 1 \\%$ and return $\\lambda$ as $\\lambda ^ { \\star }$ ; we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with $\\lambda ^ { \\star }$ on the test set and add this as a point in the corresponding graph in Figure 1. For classification (regression), each fold is 1000 to 5000 (400) samples. Figures 1(a)-(c) (resp. (d)) show the fairness-accuracy results for classification (resp. regression) datasets. On average, our approach yields results with discrimination closer to zero but also higher accuracy. Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3. From Figure 3(a), it can be seen that our approach is more accurate than the fair log approach and has slightly higher accuracy compared to DADT. These improved results come at computational cost: the average solver times for our approach in the 3 classification datasets are1 18421.43s, 15944.94s and 18161.64s, respectively. The log (resp. $\\mathbb { I } \\mathsf { G C } + \\mathbb { I } \\mathsf { G B } ,$ ) takes 18.43s, 16.04s, and 7.59s (65.68s, 23.39s, 4.78s). Figure 3(b) shows the MAE for each approach for zero discrimination. MIP-DT has far lower error than LR-ind/grp. The average solve time for MIP-DT (resp. LR-ind/grp) was 36007 (0.38/0.33) secs.\n\nFairness and Interpretability. Figures 2(a)-(b) show how the MIP objective and the accuracy and fairness values change in dependence of tree depth (a proxy for interpretability) on a fold from the Adult dataset. Such graphs can help non-technical decision-makers understand the trade-offs between fairness, accuracy, and interpretability. Figure 2(d) shows that the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART on the same dataset: this is in line with our metric – in this experiment, DTDI was $0 . 3 2 \\%$ $( 0 . 7 \\% )$ for MIP (CART).\n\nSolution Times Discussion. As seen, our approaches exhibit better performance but higher training computational cost. We emphasize that training decision-support systems for socially sensitive tasks is usually not time sensitive. At the same time, predicting the outcome of a new (unseen) sample with our approach, which is time-sensitive, is extremely fast (in the order of milliseconds). In addition, as seen in Figure 2(c), a near optimal solution is typically found very rapidly (these are results from a fold from the Adult dataset).",
      "context_after": "Family·MIP △DADT■CART•logistic",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "1903.10598_page0_fig1.jpg",
        "1903.10598_page0_fig2.jpg"
      ],
      "quality_score": 0.5,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg",
      "image_filename": "1903.10598_page0_fig3.jpg",
      "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
      "context_before": "",
      "context_after": "-AIGCS-RLblog log-grp   \n·MIP △CART ■Regression MIP-DTCARTregLR-indLR-grp   \nFigure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
      "referring_paragraphs": [
        "0 1 \\%$ and return $\\lambda$ as $\\lambda ^ { \\star }$ ; we then evaluate accuracy (misclassification rate/MAE) and discrimination of the classification/regression tree associated with $\\lambda ^ { \\star }$ on the test set and add this as a point in the corresponding graph in Figure 1.",
        "-AIGCS-RLblog log-grp   \n·MIP △CART ■Regression MIP-DTCARTregLR-indLR-grp   \nFigure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "1903.10598_page0_fig4.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_6",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg",
      "image_filename": "1903.10598_page0_fig5.jpg",
      "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2(d) shows that the likelihood for individuals (that only differ in their protected characteristics, being otherwise similar) to be treated in the same way is twice as high in MIP than in CART on the same dataset: this is in line with our metric – in this experiment, DTDI was $0 .",
        "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b)",
        "(c)",
        "1903.10598_page0_fig6.jpg",
        "1903.10598_page0_fig7.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig7.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1903.10598",
      "figure_id": "1903.10598_fig_9",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg",
      "image_filename": "1903.10598_page0_fig8.jpg",
      "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.",
      "context_before": "",
      "context_after": "[Section: Acknowledgments]\n\nThe authors gratefully acknowledge support from Schmidt Futures and from the James H. Zumberge Faculty Research and Innovation Fund at the University of Southern California. They thank the 6 anonymous referees whose reviews helped substantially improve the quality of the paper.\n\n[Section: References]",
      "referring_paragraphs": [
        "Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.",
        "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1903.10598_page0_fig9.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig9.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1904.03035": [
    {
      "doc_id": "1904.03035",
      "figure_id": "1904.03035_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg",
      "image_filename": "1904.03035_page0_fig0.jpg",
      "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.",
      "context_before": "Dealing with discriminatory bias in training data is a major issue concerning the mainstream implementation of machine learning. Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes, or distort the truth. Automated systems that depend on these models can take problematic actions based on biased profiling of individuals. The National Institute for Standards and Technology (NIST) evaluated several facial recognition algorithms and found that they are systematically biased based on gender (Ngan and Grother, 2015). Algorithms performed worse on faces labeled as female than those labeled as male.\n\nModels automating resume screening have also proved to have a heavy gender bias favoring male candidates (Lambrecht and Tucker, 2018). Such data and algorithmic biases have become a growing concern. Evaluation and mitigation of biases in data and models that use the data has been a growing field of research in recent years.\n\nOne natural language understanding task vulnerable to gender bias is language modeling. The task of language modeling has a number of practical applications, such as word prediction used in onscreen keyboards. If possible, we would like to identify the bias in the data used to train these models and reduce its effect on model behavior.\n\nTowards this pursuit, we aim to evaluate the effect of gender bias on word-level language models that are trained on a text corpus. Our contributions in this work include: (i) an analysis of the gender bias exhibited by publicly available datasets used in building state-of-the-art language models; (ii) an analysis of the effect of this bias on recurrent neural networks (RNNs) based word-level language models; (iii) a method for reducing bias learned in these models; and (iv) an analysis of the results of our method.\n\n[Section: 2 Related Work]\n\nA number of methods have been proposed for evaluating and addressing biases that exist in datasets and the models that use them. Recasens et al. (2013) studies the neutral point of view (NPOV) edit tags in the Wikipedia edit histories to understand linguistic realization of bias. According to their study, bias can be broadly categorized into two classes: framing and epistemological. While the framing bias is more explicit, the epistemological bias is implicit and subtle. Framing bias occurs when subjective or one-sided words are used. For example, in the",
      "context_after": "[Section: 3 Methods]\n\nWe first examine the bias existing in the datasets through qualitative and quantitative analysis of trained embeddings and cooccurrence patterns. We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs. As shown in Figure 1, we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender. We debias the input and the output embeddings individually as well as simultaneously. Finally, we assess the efficacy of the proposed method in reducing bias.\n\nWe observe that when both input and output embeddings are debiased together, the perplexity of the model shoots up by a much larger number than the input or the output embeddings debiased individually. We report our results when only input embeddings are debiased. This method, however, does not limit the model to capture other forms of bias being learned in other model parameters or output embeddings.\n\nThe code implementing our methods can be found in our GitHub repository.\n\n[Section: 3.1 Datasets and Text Preprocessing]",
      "referring_paragraphs": [
        "Figure 1: Word level language model is a three layer LSTM model.",
        "As shown in Figure 1, we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender.",
        "Table 1: Experimental results for Penn Treebank and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.83</td><td>1.00</td><td></td><td>3.81</td><td>4.65</td><td></td><td></td></tr><tr><td>0.0</td><td>0.74</td><td>0.91</td><td>0.40</td><td>2.23</td><td>2.90</td><td>0.38</td><td>62.56</td></tr><tr><td>0.001</td><td>0.69</td><td>0.88</td><td>0.34</td><td>2.43</td><td>2.98</td><td>0.35</td><td>62.69</td></tr><tr><td>0.01</td><td>0.63</td><td>0.81</td><td>0.31</td><td>2.56</td><td>3.40</td><td>0.36</td><td>62.83</td></tr><tr><td>0.1</td><td>0.64</td><td>0.82</td><td>0.33</td><td>2.30</td><td>3.09</td><td>0.24</td><td>62.48</td></tr><tr><td>0.5</td><td>0.70</td><td>0.91</td><td>0.39</td><td>2.91</td><td>3.76</td><td>0.38</td><td>62.5</td></tr><tr><td>0.8</td><td>0.76</td><td>0.96</td><td>0.45</td><td>3.43</td><td>4.06</td><td>0.26</td><td>63.36</td></tr><tr><td>1.0</td><td>0.84</td><td>0.94</td><td>0.38</td><td>2.42</td><td>3.02</td><td>-0.30</td><td>62.63</td></tr></table>\n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.80</td><td>1.00</td><td></td><td>3.70</td><td>4.60</td><td></td><td></td></tr><tr><td>0.0</td><td>0.70</td><td>0.84</td><td>0.29</td><td>3.48</td><td>4.29</td><td>0.15</td><td>67.67</td></tr><tr><td>0.001</td><td>0.69</td><td>0.84</td><td>0.27</td><td>2.32</td><td>3.12</td><td>0.16</td><td>67.84</td></tr><tr><td>0.01</td><td>0.61</td><td>0.79</td><td>0.20</td><td>1.88</td><td>2.69</td><td>0.14</td><td>67.78</td></tr><tr><td>0.1</td><td>0.65</td><td>0.82</td><td>0.24</td><td>2.26</td><td>3.11</td><td>0.06</td><td>67.89</td></tr><tr><td>0.5</td><td>0.70</td><td>0.88</td><td>0.31</td><td>2.25</td><td>3.17</td><td>0.20</td><td>69.07</td></tr><tr><td>0.8</td><td>0.65</td><td>0.84</td><td>0.28</td><td>2.07</td><td>2.98</td><td>0.18</td><td>69.36</td></tr><tr><td>1.0</td><td>0.74</td><td>0.92</td><td>0.27</td><td>2.32</td><td>3.21</td><td>-0.08</td><td>69.56</td></tr></table>\n\nTable 2: Experimental results for WikiText-2 and generated text for different $\\lambda$ values   \nTable 3: Experimental results for CNN/Daily Mail and generated text for different $\\lambda$ values   \n\n<table><tr><td rowspan=\"2\">λ</td><td colspan=\"3\">Fixed Context</td><td colspan=\"3\">Infinite Context</td><td rowspan=\"2\">Ppl.</td></tr><tr><td>μ</td><td>σ</td><td>β</td><td>μ</td><td>σ</td><td>β</td></tr><tr><td>train</td><td>0.72</td><td>0.94</td><td></td><td>0.77</td><td>1.05</td><td></td><td></td></tr><tr><td>0.0</td><td>0.51</td><td>0.68</td><td>0.22</td><td>0.43</td><td>0.59</td><td>0.29</td><td>118.01</td></tr><tr><td>0.1</td><td>0.38</td><td>0.52</td><td>0.19</td><td>0.85</td><td>1.38</td><td>0.22</td><td>116.49</td></tr><tr><td>0.5</td><td>0.34</td><td>0.48</td><td>0.14</td><td>0.79</td><td>1.31</td><td>0.20</td><td>116.19</td></tr><tr><td>0.8</td><td>0.40</td><td>0.56</td><td>0.19</td><td>0.96</td><td>1.57</td><td>0.23</td><td>121.00</td></tr><tr><td>1.0</td><td>0.62</td><td>0.83</td><td>0.21</td><td>1.71</td><td>2.65</td><td>0.31</td><td>120.55</td></tr></table>"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    }
  ],
  "1904.03310": [
    {
      "doc_id": "1904.03310",
      "figure_id": "1904.03310_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg",
      "image_filename": "1904.03310_page0_fig0.jpg",
      "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.",
      "context_before": "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo. We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a). The analysis shows that the Billion Word corpus contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female.\n\n[Section: 3.2 Geometry of Gender]\n\nNext, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016). The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender).",
      "context_after": "[Section: 3.3 Unequal Treatment of Gender]\n\nTo test how ELMo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence. We collect sentences containing gendered words (e.g., he-she, father-mother) and occupation words (e.g., doctor)1 from the OntoNotes 5.0 corpus (Weischedel et al., 2012), where we treat occupation words as a mention to an entity, and the gender of that entity is taken to the gender of a co-referring gendered word, if one exists. For example, in the sentence “the engineer went back to her home,” we take engineer to be a female mention. Then we split all such instances into training and test, with 539 and 62 instances, respectively and augment these sentences by swapping all the gendered words with words of the opposite gender such that the numbers of male\n\nand female entities are balanced.\n\nWe first test if ELMo embedding vectors carry gender information. We train an SVM classifier with an RBF kernel2 to predict the gender of a mention (i.e., an occupation word) based on its ELMo embedding. On development data, this classifier achieves $9 5 . 1 \\%$ and $8 0 . 6 \\%$ accuracy on sentences where the true gender was male and female respectively. For both male and female contexts, the accuracy is much larger than $50 \\%$ , demonstrating that ELMo does propagate gender information to other words. However, male information is more than $14 \\%$ more accurately represented in ELMo than female information, showing that ELMo propagates the information unequally for male and female entities.\n\n[Section: 4 Bias in Coreference Resolution]",
      "referring_paragraphs": [
        "We show that (1) training data for ELMo contains sig-\n\nTable 1: Training corpus for ELMo.",
        "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo.",
        "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016).",
        "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences."
      ],
      "figure_type": "example",
      "sub_figures": [
        "1904.03310_page0_fig1.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1905.03674": [
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig0.jpg",
      "image_filename": "1905.03674_page0_fig0.jpg",
      "caption": "Figure 1: Proportionality Example",
      "context_before": "The data points in machine learning are often real human beings. There is legitimate concern that traditional machine learning algorithms that are blind to this fact may inadvertently exacerbate problems of bias and injustice in society [25]. Motivated by concerns ranging from the granting of bail in the legal system to the quality of recommender systems, researchers have devoted considerable effort to developing fair algorithms for the canonical supervised learning tasks of classification and regression [13, 28, 20, 27, 34, 11, 30, 35, 26, 18, 21].\n\nWe extend this work to a canonical problem in unsupervised learning: centroid clustering. In centroid clustering, we want to partition data into $k$ clusters by choosing $k$ “centers” and then matching points to one of the centers. This is a classic context for clustering work [19, 33, 8, 2], and is perhaps best known as the setting for the celebrated $k$ -means heuristic (independently discovered many times, see [22] for a brief history). We provide a novel group based notion of fairness as proportionality, inspired by recent related work on the fair allocation of public resources [3, 10, 15, 17]. We suppose that data points represent the individuals to whom we wish to be fair, and that these agents prefer to be clustered accurately (that is, they prefer their cluster center to be representative of their features). A solution is fair if it respects the entitlements of groups of agents, where we assume that a subset of agents is entitled to choose a center for themselves if they constitute a sufficiently large fraction of the population with respect to the total number of clusters (e.g., $1 / k$ of the population, if we are clustering into $k$ groups). The guarantee must hold for all subsets of sufficient size, and therefore does not hinge on any particular a priori knowledge about which points should be protected. This is in line with other recent observations that information about which individuals should be protected may not be available in practice [21].\n\nConsider a motivating example where proportional clustering might be preferable to more standard clusterings that try to minimize the $k$ -means or $k$ -median objective. Suppose there are 3 spherical clusters in the data: A, B, and C, and we are computing a 3-clustering. A, B, and C each contain one third of the total data. The radius of A is very large compared to the radii of B and C, and A is very far away from B and C compared to the radius of A. The radii of B and C are very small, and B and C are close relative to the radius of A. More simply, A is a large sphere very far away from two small spheres B and C, which are close together.\n\nSimply placing centers at the middle of A, B, and C is proportional. However, the global k-means or k-median minimizer places 1 center for B and C to share, and uses the remaining 2 centers to cover A. Such a solution is arbitrarily not-proportional as the radii of B and C become arbitrarily small. Essentially, the global optimum forces B and C to share a center in order to pay for the high variance in A.\n\nTo interpret this example, suppose we are clustering home locations to decide where to build public parks. B and C are dense urban centers, and A is a suburb. Minimizing total distance seems reasonable, but the global optimum builds 2 parks for A, and only 1 that B and C must share. Alternatively, A, B, and C might represent clusters of patients in a medical study. Both solutions distinguish A from B and C, but the global optimum obscures the secondary difference between B and C. In both instances, B or C could represent a protected group (e.g., home location may be racially divided, and race or sex could cause differences in medical data), in which case proportionality provides a guarantee even if we do not have access to this information.\n\n[Section: 1.1 Preliminaries and Definition of Proportionality]\n\nWe have a set $\\mathcal { N }$ of $| { \\mathcal { N } } | = n$ individuals or data points, and a set $\\mathcal { M }$ of $| { \\mathcal { M } } | = m$ feasible cluster centers. We will sometimes consider the important special case where $\\mathcal { M } = \\mathcal { N }$ (i.e., where one is only given a single set of points as input), but most of our results are for the general case where we make no assumption about ${ \\mathcal { M } } \\cap { \\mathcal { N } }$ . For all $i , j \\in \\mathcal { N } \\cup \\mathcal { M }$ , we have a distance $d ( i , j )$ satisfying the triangle inequality. Our task is centroid clustering as treated in the classic $k$ -median, $k$ -means, and $k$ -center problems. We wish to open a set $X \\subseteq { \\mathcal { M } }$ of $| X | = k$ centers (assume $| { \\mathcal { M } } | \\geq k$ ), and then match all points in $\\mathcal { N }$ to their closest center in $X$ . For a particular solution $X$ and agent $i \\in \\mathcal N$ , let $D _ { i } ( X ) = \\operatorname* { m i n } _ { x \\in X } d ( i , x )$ . In general, a good clustering solution $X$ will have small values of $D _ { i } ( X )$ , although the aforementioned objectives differ slightly in how they measure this. In particular, the $k$ -median objective is $\\textstyle \\sum _ { i \\in { \\mathcal { N } } } D _ { i } ( X )$ , the $k$ -means objective is $\\textstyle \\sum _ { i \\in N } { ( D _ { i } ( X ) ) ^ { 2 } }$ , and the $k$ -center objective is $\\mathrm { m a x } _ { i \\in N } D _ { i } ( X )$ .\n\nTo define proportional clustering, we assume that individuals prefer to be closer to their center in terms of distance (i.e., ensuring that the center is more representative of the point). Any subset of at least $r \\lceil \\frac { n } { k } \\rceil$ individuals is entitled to choose $r$ centers. We call a solution proportional if there does not exist any such sufficiently large set of individuals who, using the number of centers to which they are entitled, could produce a clustering among themselves that is to their mutual benefit in the sense of Pareto dominance. More formally, a blocking coalition is a set $S \\subseteq { \\mathcal { N } }$ of at least $r \\lceil \\frac { n } { k } \\rceil$ points and a set $Y \\subseteq { \\mathcal { M } }$ of at most $r$ centers such that $D _ { i } ( Y ) < D _ { i } ( X )$ for all $i \\in S$ . It is easy to see that because $D _ { i } ( X ) = \\operatorname* { m i n } _ { x \\in X } d ( i , x )$ , this is functionally equivalent to Definition 1; a larger blocking coalition necessarily implies a blocking coalition with a single center.\n\nDefinition 1. Let $X \\subseteq { \\mathcal { M } }$ with $| X | = k$ . $S \\subseteq N$ is a blocking coalition against $X$ if $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and $\\exists y \\in { \\mathcal { M } }$ such that $\\forall i \\in S$ , $d ( i , y ) < D _ { i } ( X )$ . $X \\subseteq { \\mathcal { N } }$ is proportional if there is no blocking coalition against $X$ .\n\nEquivalently, $X$ is proportional if $\\forall S \\subseteq { \\mathcal { N } }$ with $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and for all $y \\in \\mathcal M$ , there exists $i \\in S$ with $d ( i , y ) \\geq D _ { i } ( X )$ . It is important to note that this quantification is over all subsets of sufficient size. Hence, in attempting to satisfy the guarantee for a particular subset $S$ , one cannot simply consider a single $i \\in S$ and ignore all of the other points, as $S \\backslash \\{ i \\}$ may itself be a subset to which the guarantee applies.\n\nIt is instructive to briefly consider an example. In Figure 1, $\\mathcal N = \\mathcal M$ , $k = 2$ , and there are 12 individuals, represented by the embedded points. Suppose we want to minimize the $k$ -center objective in the pursuit of fairness: we would then choose the red points. However, this is not a proportional solution because the middle six points constitute half of the points, and would all prefer to be matched to the central blue point. Furthermore, choosing the blue point (and any other center) is a proportional solution, because for any arbitrary group of six points and new proposed center, at least one of the six points will be closer to the blue point than the proposed center.",
      "context_after": "[Section: 1.2 Results and Outline]\n\nIn Section 2 we show that proportional solutions may not always exist. In fact, one cannot get better than a 2-proportional solution in the worst case. In contrast, we give a greedy algorithm (Algorithm 1) and prove√ Theorem 1: The algorithm yields a $( 1 + { \\sqrt { 2 } } )$ -proportional solution in the worst case.\n\nIn Section 3, we treat proportionality as a constraint and seek to optimize the $k$ -median objective subject to that constraint. We show how to write approximate proportionality as $m$ linear constraints. Incorporating this into the standard linear programming relaxation of the $k$ -median problem, we show how to use the rounding from [8] to find an $O ( 1 )$ -proportional solution that is an $O ( 1 )$ -approximation to the $k$ -median objective of the optimal proportional solution.\n\nIn Section 4, we show that proportionality is approximately preserved if we take a random sample of the data points of size $\\ddot { O } ( k ^ { 3 } )$ , where the $\\ddot { O }$ hides low order terms. This immediately implies that for constant $k$ , we can check if a given clustering is proportional as well as compute approximately proportional solutions in near linear time, comparable to the time taken to run the classic $k$ -means heuristic.\n\nIn Section 5, we provide a local search heuristic that efficiently searches for a proportional clustering. Our heuristic is able to consistently find nearly proportional solutions in practice. We test our heuristic and Algorithm 1 empirically against the celebrated $k$ -means heuristic in order to understand the tradeoff between proportionality and the $k$ -means objective. We find that the tradeoff is highly data dependent: Though these objectives are compatible on some datasets, there exist others on which these objectives are in conflict.\n\n[Section: 1.3 Related Work]",
      "referring_paragraphs": [
        "In Figure 1, $\\mathcal N = \\mathcal M$ , $k = 2$ , and there are 12 individuals, represented by the embedded points.",
        "Figure 1: Proportionality Example\n\nProportionality has many advantages as a notion of fairness in clustering, beyond the intuitive appeal of groups being entitled to a proportional share of centers."
      ],
      "figure_type": "example",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg",
      "image_filename": "1905.03674_page0_fig1.jpg",
      "caption": "Figure 2: Diagram for Proof of Theorem 1",
      "context_before": "[Section: Algorithm 1 Greedy Capture]\n\n1: $\\delta  0$ ; $X  \\emptyset$ ; $N \\gets { \\mathcal { N } }$   \n2: while $N \\neq \\emptyset$ do   \n3: Smoothly increase $\\delta$   \n4: while $\\exists x \\in X$ s.t. $| B ( x , \\delta ) \\cap N | \\geq 1$ do   \n5: $N \\gets N \\backslash B ( x , \\delta )$   \n6: while $\\exists x \\in ( { \\mathcal { M } } \\backslash X )$ s.t. $\\begin{array} { r } { | B ( x , \\delta ) \\cap N | \\ge \\lceil \\frac { n } { k } \\rceil } \\end{array}$ do   \n7: $X  X \\cup \\{ x \\}$   \n8: $N \\gets N \\backslash B ( x , \\delta )$   \n9: return X\n\nAlgorithm 1 runs in $\\tilde { O } ( m n )$ time.1 In essence, the algorithm grows balls continuously around the centers, and when the ball around a center has “captured” $\\lceil \\frac { n } { k } \\rceil$ points, we greedily open that center and disregard all of the captured points. Open centers continue to greedily capture points as their balls continue to expand. Though [24, 29] similarly expand balls about points to compute approximately optimal solutions to the $k$ -median problem, there is a crucial difference: They grow balls around data points rather than centers.\n\nTheorem 1. Algorithm 1 yields a $( 1 + { \\sqrt { 2 } } )$ -proportional clustering, and there exists an instance for which this bound is tight.\n\nProof. Let $X$ be the solution computed by Algorithm 1. First note that $X$ uses at most $k$ centers, since it only opens a center when $\\lceil \\frac { n } { k } \\rceil$ unmatched points are absorbed by the ball around that center, and this can happen at most $k$ times. Now, suppose for a contradiction that $X$ is not a $( 1 + { \\sqrt { 2 } } )$ -proportional clustering. Then there exists $S \\subseteq { \\mathcal { N } }$ with $\\left| S \\right| \\geq \\left\\lceil { \\frac { n } { k } } \\right\\rceil$ and $y \\in \\mathcal { M }$ such that\n\nLet be the distance of the farthest agent from in $S$ , that is, $r _ { y } : = \\operatorname* { m a x } _ { i \\in S } d ( i , y )$ , and call this agent $i ^ { * }$ . $r _ { y }$ $y$ There are two cases. In the first case, $B ( x , r _ { y } ) \\cap S = \\emptyset$ for all $x \\in X$ . This immediately yields a contradiction, because it implies that Algorithm 1 would have opened $y$ . In particular, note that $S \\subseteq B ( y , r _ { y } )$ , so if $S \\cap B ( x , r _ { y } ) = \\emptyset$ for all $x \\in X$ , then $B ( y , r _ { y } )$ would have had at least $\\lceil \\frac { n } { k } \\rceil$ unmatched points.",
      "context_after": "which violates equation 1.\n\nIt is not hard to show that there exists an instance for which Algorithm 1 yields exactly this bound. Consider the following instance with $\\mathcal { N } = \\{ a _ { 1 } , a _ { 2 } , \\ldots , a _ { 6 } \\}$ , $\\mathcal { M } = \\{ x _ { 1 } , x _ { 2 } , x _ { 3 } , x _ { 4 } \\}$ and $k = 3$ . Distances are specified in the following table, where $\\epsilon > 0$ is some small constant.\n\n<table><tr><td></td><td>x1</td><td>x2</td><td>x3</td><td>x4</td></tr><tr><td>a1</td><td>1</td><td>1+√2</td><td>∞</td><td>∞</td></tr><tr><td>a2</td><td>√2-1</td><td>1-ε</td><td>∞</td><td>∞</td></tr><tr><td>a3</td><td>1+√2</td><td>1-ε</td><td>∞</td><td>∞</td></tr><tr><td>a4</td><td>∞</td><td>∞</td><td>1</td><td>1+√2</td></tr><tr><td>a5</td><td>∞</td><td>∞</td><td>√2-1</td><td>1-ε</td></tr><tr><td>a6</td><td>∞</td><td>∞</td><td>1+√2</td><td>1-ε</td></tr></table>\n\nThe distances satisfy the triangle inequality. Note that Algorithm 1 will open $x _ { 2 }$ and $x _ { 4 }$ . The coalition $\\{ a _ { 1 } , a _ { 2 } \\}$ can each reduce their distance by a multiplicative factor approaching $1 + { \\sqrt { 2 } }$ as $\\epsilon  0$ by deviating to x1. $x _ { 1 }$ □\n\n[Section: 2.2 Local Capture Heuristic]\n\nWe observe that while our Greedy Capture algorithm (Algorithm 1) always produces an approximately proportional solution, it may not produce an exactly proportional solution in practice, even on instances where such solutions exist (see Figure 4a and Figure 4b). We therefore introduce a Local Capture heuristic for searching for more proportional clusterings. Algorithm 2 takes a target value of $\\rho$ as a parameter, and proceeds by iteratively finding a center that violates $\\rho$ -fairness and swapping it for the center in the current solution that is least demanded.\n\nAlgorithm 2 Local Capture Heuristic   \ninput $\\rho$ 1: Initialize $X$ as a random subset of $k$ centers from $\\mathcal{M}$ 2: repeat   \n3: for $y\\in \\mathcal{M}$ do   \n4: $S_{y}\\gets \\{i\\in N:\\rho \\cdot d_{iy} <   D_{i}(X)\\}$ 5: if $|S_y|\\geq \\lceil \\frac{n}{k}\\rceil$ then   \n6: $x^{*}\\gets \\mathrm{argmin}_{x\\in X}|\\{i\\in N:d_{ix} = D_i(X)\\} |$ 7: $X\\gets (X\\backslash \\{x^{*}\\})\\cup \\{y\\}$ 8: until no changes occur   \n9: return X\n\nEvery iteration of Algorithm 2 (the entire inner for loop) runs in $\\tilde { O } ( m n ^ { 2 } )$ time. There is no guarantee of convergence (for a given input $\\rho$ , there may not even exist a $\\rho$ -proportional solution), but if Algorithm 2 terminates, then it returns a $\\rho$ -proportional solution. In our experiments (see Section 5), we search for the minimum $\\rho$ for which the algorithm terminates in a small number of iterations via binary search over possible input of . In [2], the authors also evaluate a local search swapping procedure for the $k$ -median problem, but $\\rho$ their swap condition is based on the relative $k$ -median objective of two solutions, whereas our swap condition is based on violations to proportionality.",
      "referring_paragraphs": [
        "Figure 2: Diagram for Proof of Theorem 1\n\nIn the second case, $\\exists x \\in X$ and $\\exists i \\in N$ such that $i \\in B ( x , r _ { y } ) \\cap S$ . This case is drawn below in Figure 2. By the triangle inequality, $d ( x , y ) \\leq d ( i , x ) + d ( i , y )$ . Therefore, $d ( i ^ { * } , x ) \\leq r _ { y } + d ( i , x ) + d ( i , y )$ . Also, $d ( i , x ) \\leq r _ { y }$ , since $i \\in B ( x , r _ { y } )$ . Consider the minimum multiplicative improvement of $i$ and $i ^ { * }$ :"
      ],
      "figure_type": "diagram",
      "sub_figures": [
        "(a) Iris",
        "1905.03674_page0_fig2.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig2.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg",
      "image_filename": "1905.03674_page0_fig3.jpg",
      "caption": "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 4: Minimum $\\rho$ such that the solution is $\\rho$ -proportional   \n(a) Iris"
      ],
      "figure_type": "other",
      "sub_figures": [
        "(b) Diabetes",
        "(c) KDD, geometric scale",
        "1905.03674_page0_fig4.jpg",
        "1905.03674_page0_fig5.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig5.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1905.03674",
      "figure_id": "1905.03674_fig_7",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg",
      "image_filename": "1905.03674_page0_fig6.jpg",
      "caption": "Figure 5: $k$ -means objective",
      "context_before": "",
      "context_after": "[Section: 5.2 Proportionality and Low $k$ -means Objective]\n\nNote that if one is allowed to use $2 k$ centers when $k$ is given as input, one can trivially achieve the proportionality of Local Capture and the $k$ -means objective of the $k$ -means $^ { + + }$ algorithm by taking the union of the two solutions. Thinking in this way leads to a different way of quantifying the tradeoff between proportionality and the $k$ -means objective: Given an approximately proportional solution, how many extra centers are necessary to get comparable $k$ -means objective as the $k$ -means $^ { + + }$ algorithm? For a given data set, the answer is a value between 0 and $k$ , where larger numbers indicate more incompatibility, and lower numbers indicate less incompatibility.\n\nTo answer this question, we compute the union of centers found by Local Capture and the $k$ -means $^ { + + }$ algorithm. We then greedily remove centers as long as doing so does not increase the minimum $\\rho$ such that the solution is $\\rho$ -proportional (defined on $k$ , not $2 k$ ) by more than a multiplicative factor of $\\alpha$ , and does not increase the $k$ -means objective by more than a multiplicative factor $\\beta$ .\n\nOn the KDD dataset, we set $\\alpha = 1 . 2$ and $\\beta = 1 . 5$ , so the proportionality of the result is within 1.2 of Local Capture in Figure 4c, and the $k$ -means objective is within 1.5 of $k$ -means $^ { + + }$ in Figure 5c. We observe that this heuristic uses at most 3 extra centers for any $k \\leq 1 0$ . So while there is real tension between proportionality and the $k$ -means objective, this tension is still not maximal. In the worst case, one might need to add $k$ centers to a proportional solution to compete with the $k$ -means objective of the $k$ -means++ algorithm, but in practice we find that we need at most 3 for $k \\leq 1 0$ .\n\n[Section: 6 Conclusion and Open Directions]",
      "referring_paragraphs": [
        "Figure 5: $k$ -means objective"
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) Diabetes",
        "(c) KDD, geometric scale",
        "1905.03674_page0_fig7.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig7.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1905.10674": [
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg",
      "image_filename": "1905.10674_page0_fig0.jpg",
      "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.",
      "context_before": "Learning high-quality node embeddings is an important building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is compositional—meaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.\n\n[Section: 1. Introduction]\n\nLearning low-dimensional embeddings of the nodes in a graph is a fundamental technique underlying state-of-the-art approaches to link prediction and recommender systems (Hamilton et al., 2017b). However, in many applications— especially those involving social graphs—it is desirable to exercise control over the information contained within learned node embeddings. For instance, we may want to ensure that recommendations are fair or balanced with respect to certain attributes (e.g., that they do not depend on a user’s race or gender) or we may want to ensure privacy by not exposing certain attributes through learned node representations. In this work we investigate the feasibility of enforcing such invariance constraints on (social) graph embeddings.\n\n1McGill University 2Mila 3Facebook AI Research. Correspondence to: Avishek Joey Bose <joey.bose@mail.mcgill.ca>.\n\nProceedings of the ${ 3 6 } ^ { t h }$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).",
      "context_after": "[Section: 2. Related Work]\n\nWe now briefly highlight core related work on (social) graph embeddings and algorithmic fairness, which our research builds upon.\n\n[Section: 2.1. Graph Embedding]",
      "referring_paragraphs": [
        "Figure 1.",
        "We investigated the impact of enforcing invariance on graph embeddings using three datasets: Freebase $1 5 \\mathrm { k } { - } 2 3 7 ^ { 4 }$ , MovieLens- $1 \\mathbf { M } ^ { 5 }$ , and an edge-prediction dataset derived from Reddit.6 The dataset statistics are given in Table 1.",
        "To select the “sensitive” subreddit communities,\n\nTable 1."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_2",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg",
      "image_filename": "1905.10674_page0_fig1.jpg",
      "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).",
      "context_before": "where $a _ { r , 1 } , a _ { r , 1 } \\in \\mathcal { R }$ and $\\mathbf { P } _ { 1 } , \\mathbf { P } _ { 2 } \\in \\mathcal { R } ^ { d \\times d }$ are trainable parameters. In this case, the loss function is simply the negative of the log-likelihood score.\n\n[Section: REDDIT]\n\nThe final dataset we consider is based on the social media website Reddit—a popular, discussion-based website where users can post and comment on content in different topical communities, called “subreddits”. For this dataset, we consider a traditional edge prediction task, where the goal is to predict interactions between users and subreddit communities.\n\nTo construct the edge prediction task, we examined all comments from the month of November in 2017, and we placed an edge between a user and a community if this user commented on that community at least once within this time period. We then took the 10-core of this graph to remove low-degree nodes, which resulted in a graph with approximately 366K users, 18K communities, and 7M edges. Given this graph, the main task is to train an edge-prediction model on $9 0 \\%$ of the user-subreddit edges and then predict missing edges in a held-out test set of the remaining edges.\n\nReddit is a pseudonymous website with no public user attributes. Thus, to define sensitive attributes, we treat certain subreddit nodes as sensitive nodes, and the sensitive attributes for users are whether or not they have an edge connecting to these sensitive nodes. In other words, the fairness objective in this setting is to force the model to be invariant to whether or not a user commented on a particular community. To select the “sensitive” subreddit communities,\n\nTable 1. Statistics for the three datasets, including the total number of nodes $( | \\nu | )$ and number of nodes with sensitive attributes $| \\mathcal { T } ^ { * } |$ , the number of sensitive attributes and their types and the total number of edges in the graph.   \n\n<table><tr><td>DATASET</td><td>|V|</td><td>|T*|</td><td>#SENSITIVE ATTRIBUTES</td><td>EDGES</td><td>BINARY ATTRIBUTES?</td><td>MULTIClass ATTRIBUTES?</td></tr><tr><td>FB15K-237</td><td>14,940</td><td>14,940</td><td>3</td><td>168,618</td><td>✓</td><td>×</td></tr><tr><td>MOVIELENS1M</td><td>9,940</td><td>6,040</td><td>3</td><td>1,000,209</td><td>✓</td><td>✓</td></tr><tr><td>REDDIT COMMENTS</td><td>385,735</td><td>366,797</td><td>10</td><td>7,255,096</td><td>✓</td><td>×</td></tr></table>",
      "context_after": "[Section: 5.2. Results]\n\nWe now address the core experimental questions (Q1-Q3).\n\n[Section: Q1: THE INVARIANCE-ACCURACY TRADEOFF]",
      "referring_paragraphs": [
        "Figure 2.",
        "In other words, on these two datasets the sensitive attributes were nearly impossible to predict from the filtered embeddings, while the accuracy on the main edge prediction task was roughly $10 \\%$ worse than\n\nTable 2.",
        "Table 2 and Figure 2 summarize these results for the MovieLens data, where we can see that the accuracy of classifying the sensitive attributes is on-par with a majorityvote classifier (Table 2) while the RMSE degrades from 0.865 to 1.01 with the compositional adversary.",
        "In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_3",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg",
      "image_filename": "1905.10674_page0_fig2.jpg",
      "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
      "context_before": "We now address the core experimental questions (Q1-Q3).\n\n[Section: Q1: THE INVARIANCE-ACCURACY TRADEOFF]\n\nIn order to quantify the extent to which the learned embeddings are invariant to the sensitive attributes (e.g., after adversarial training), we freeze the trained compositional encoder C-ENC and train an new MLP classifier to predict each sensitive attribute from the filtered embeddings (i.e., we train one new classifier per sensitive attribute). We also evaluate the performance of these filtered embeddings on the original prediction tasks. In the best case, a newly trained MLP classifier should have random accuracy when attempting to predict the sensitive attributes from the filtered embeddings, but these embeddings should still provide strong",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.",
        "ADVERSARY</td><td>MAJORITY\nCLASSIFIER</td><td>RANDOM\nCLASSIFIER</td></tr><tr><td>GENDER</td><td>0.712</td><td>0.532</td><td>0.541</td><td>0.551</td><td>0.511</td><td>0.5</td><td>0.5</td></tr><tr><td>AGE</td><td>0.412</td><td>0.341</td><td>0.333</td><td>0.321</td><td>0.313</td><td>0.367</td><td>0.141</td></tr><tr><td>OCCUPATION</td><td>0.146</td><td>0.141</td><td>0.108</td><td>0.131</td><td>0.121</td><td>0.126</td><td>0.05</td></tr></table>\n\nTable 3.",
        "That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3)."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_4",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg",
      "image_filename": "1905.10674_page0_fig3.jpg",
      "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 4.",
        "That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).",
        "As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations.",
        "The main model however\n\nTable 4."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_5",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg",
      "image_filename": "1905.10674_page0_fig4.jpg",
      "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.5499999999999999,
      "metadata": {}
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_6",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg",
      "image_filename": "1905.10674_page0_fig5.jpg",
      "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .",
      "context_before": "",
      "context_after": "[Section: Q2: THE IMPACT OF COMPOSITIONALITY]\n\nIn all our experiments, we observed that our compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute. In fact, on the MovieLens-1M data (and the synthetic Freebase15k-237 data), the compostionally trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes (Table 2). In other words, training a model to jointly remove information about the sensitive attributes using the compositional encoder (Equation 6) removed more information about the sensitive attributes than training separate adversarially regularized embedding models for each sensitive attribute. This result is not entirely surprising, as it essentially indicates that the different sensitive attributes (age, gender, and occupation) are correlated in this dataset. Nonetheless, it is a positive result indicating that the extra flexibility afforded by the compositional approach does not necessarily lead to a decrease in performance. That\n\nsaid, on the Reddit data we observed the opposite trend and found that the compositional approach performed worse in terms of its ability to remove information about the sensitive attributes (Figure 4) as well as a small drop on the performance of the main edge prediction task (Figure 3).\n\n[Section: Q3: INVARIANCE ON UNSEEN COMBINATIONS]",
      "referring_paragraphs": [
        "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .\n\nracy on the original edge prediction task. This result is not entirely surprising, since for this dataset the “sensitive” attributes were synthetically constructed from entity type annotations, which are presumably very relevant to the main edge/relation prediction task. However, it is an interesting point of reference that demonstrates the potential limitations of removing sensitive information from learned graph embeddings."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.7,
      "metadata": {}
    },
    {
      "doc_id": "1905.10674",
      "figure_id": "1905.10674_fig_7",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg",
      "image_filename": "1905.10674_page0_fig6.jpg",
      "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.",
      "context_before": "One of the key benefits of the compositional encoder is that it can flexibly generate embeddings that are invariant to any subset of $S \\subseteq \\{ 1 , . . . , K \\}$ of the sensitive attributes in a domain. In other words, at inference time, it is possible to generate $2 ^ { K }$ distinct embeddings for an individual node, depending on the exact set of invariance constraints. However, given this combinatorially large output space, a natural question is whether this approach performs well when generalizing to unseen combinations of sensitive attributes.\n\nWe tested this phenomenon on the Reddit dataset, since it has the largest number of sensitive attributes (10, compared to 3 sensitive attributes for the other two datasets). During training we held out $1 0 \\%$ of the combinations of sensitive attributes, and we then evaluated the model’s ability to enforce invariance on this held-out set. As we can see in Figure 4, the performance drop for the held-out combinations is very small (0.025), indicating that our compositional approach is capable of effectively generalizing to unseen combinations. The Appendix contains further results demonstrating how this trends scales gracefully when we increase the number of sensitive attributes from 10 to 50.\n\n[Section: QUANTIFYING BIAS]\n\nIn all of the above results, we used the ability to classify the sensitive attributes as a proxy for bias being contained within the embeddings. While this is a standard approach, e.g., see Edwards & Storkey (2015), and an intuitive method for evaluating representational invariance—a natural question is whether the adversarial regularization also decreases bias in the edge prediction tasks. Ideally, after filtering the embeddings, we would have that the edge predictions themselves are not biased according to the sensitive attributes.\n\nTo quantify this issue, we computed a “prediction bias” score for the MovieLens1M dataset: For each movie, we computed the absolute difference between the average rating predicted for each possible value of a sensitive attribute and we then averaged these scores over all movies. Thus, for example, the bias score for gender corresponds to the average absolute difference in predicted ratings for male vs. female users, across all movies. From the perspective of fairness our adversary imposes a soft demographic parity constraint on the main task. A reduction in prediction bias across the different subgroups represents an empirical measure of achieving demographic parity. Figure 7 highlights these results, which show that adversarial regularization does in-",
      "context_after": "[Section: 6. Discussion and Conclusion]\n\nOur work sheds light on how fairness can be enforced in graph representation learning—a setting that is highly relevant to large-scale social recommendation and networking platforms. We found that using our proposed compositional adversary allows us to flexibly accomodate unseen combinations of fairness constraints without explicitly training on them. This highlights how fairness could be deployed in a real-word, user-driven setting, where it is necessary to optionally enforce a large number of possible invariance constraints over learned graph representations.\n\nIn terms of limitations and directions for future work, one important limitation is that we only consider one type of adversarial loss to enforce fairness. While this adversarial loss is theoretically motivated and known to perform well, there are other recent variations in the literature (e.g., Madras et al. (2018))—as well as related non-adversarial regularizers (e.g., Zemel et al. (2013)). Also, while we considered imposing fairness over sets of attributes, we did not explicitly model subgroup-level fairness (Kearns et al., 2017). Extending and testing our framework with these alternatives is a natural direction for future work.\n\nThere are also important questions about how our framework translates to real-world production systems. For instance, in this work we enforced fairness with respect to randomly sampled sets of attributes, but in real-world environments, these sets of attributes would come from user preferences, which may themselves be biased; e.g., it might be more common for female users to request fairness than male users potentially leading to new kinds of demographic inequalities. Understanding how these preference biases could impact our framework is an important direction for future inquiry.\n\n[Section: Acknowledgements]",
      "referring_paragraphs": [
        "Figure 7 highlights these results, which show that adversarial regularization does in-",
        "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.\n\ndeed drastically reduce prediction bias. Interestingly, using a compositional adversary works better than a single adversary for a specific sensitive attribute which we hypothesize is due to correlation between sensitive attributes."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    }
  ],
  "1905.12843": [
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig0.jpg",
      "image_filename": "1905.12843_page0_fig0.jpg",
      "caption": "",
      "context_before": "2. $\\widehat { Q } =$ null and problem (4) is infeasible.\n\n[Section: 6. Experiments]\n\nWe evaluate our method on the tasks of least-squares regression and logistic regression under statistical parity. We use the following three datasets:\n\nAdult: The adult income dataset (Lichman, 2013) has 48,842 examples. The task is to predict the probability that an individual makes more than $\\$ 50\\mathbf { k }$ per year via logistic loss minimization, with gender as the protected attribute.\n\nLaw school: Law School Admissions Councils National Longitudinal Bar Passage Study (Wightman, 1998) has 20,649 examples. The task is to predict a students GPA (normalized to [0, 1]) via square loss minimization, with race as the protected attribute (white versus non-white).\n\nCommunities & crime: The dataset contains socio-economic, law enforcement, and crime data about communities in the US (Redmond & Baveja, 2002) with 1,994 examples. The task is to predict the number of violent crimes per 100,000 population (normalized to [0, 1]) via square loss minimization, with race as the protected attribute (whether the majority population of the community is white).\n\nFor the two larger datasets (adult and law school), we also created smaller (subsampled) versions by picking random 2,000 points. Thus we ended up with a total of five datasets, and split each into $50 \\%$ for training and $50 \\%$ for testing.\n\nWe ran Algorithm 1 on each training set over a range of constraint slack values $\\hat { \\varepsilon }$ , with a fixed discretization grid of size 40: $\\mathcal { Z } = \\{ 1 / 4 0 , 2 / 4 0 , \\dots , 1 \\}$ . Among the solutions for different $\\hat { \\varepsilon }$ , we selected the ones on the Pareto front based on their training losses and SP disparity $\\operatorname* { m a x } _ { a , z } \\left\\{ \\hat { \\gamma } _ { a , z } \\right\\}$ . We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.\n\nWe ran our algorithm with the three types of reductions",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "1905.12843_page0_fig1.jpg",
        "1905.12843_page0_fig2.jpg"
      ],
      "quality_score": 0.35,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig2.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_4",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
      "image_filename": "1905.12843_page0_fig3.jpg",
      "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.",
      "context_before": "",
      "context_after": "[Section: Acknowledgements]\n\nZSW is supported in part by a Google Faculty Research Award, a J.P. Morgan Faculty Award, and a Facebook Research Award. Part of this work was completed while ZSW was at Microsoft Research-New York City.\n\n[Section: References]",
      "referring_paragraphs": [
        "We then evaluated the selected predictors on the test set, and show the resulting Pareto front in Figure 1.",
        "Figure 1.",
        "Table 1.",
        "The details are listed in Table 1."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1905.12843_page0_fig4.jpg",
        "1905.12843_page0_fig5.jpg",
        "1905.12843_page0_fig6.jpg",
        "1905.12843_page0_fig7.jpg",
        "1905.12843_page0_fig8.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig8.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_10",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg",
      "image_filename": "1905.12843_page0_fig9.jpg",
      "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "In Figure 2 we include the training performances of our algorithm and the baseline methods, including the SEO method and the unconstrained regressors.",
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1905.12843_page0_fig10.jpg",
        "1905.12843_page0_fig11.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig11.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1905.12843",
      "figure_id": "1905.12843_fig_13",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg",
      "image_filename": "1905.12843_page0_fig12.jpg",
      "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 3."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1905.12843_page0_fig13.jpg",
        "1905.12843_page0_fig14.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig14.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1906.02589": [
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg",
      "image_filename": "1906.02589_page0_fig0.jpg",
      "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).",
      "context_before": "We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also flexibly fair, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder— which does not require the sensitive attributes for inference—enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.\n\n[Section: 1. Introduction]\n\nMachine learning systems are capable of exhibiting discriminatory behaviors against certain demographic groups in high-stakes domains such as law, finance, and medicine (Kirchner et al., 2016; Aleo & Svirsky, 2008; Kim et al., 2015). These outcomes are potentially unethical or illegal (Barocas & Selbst, 2016; Hellman, 2018), and behoove researchers to investigate more equitable and robust models. One promising approach is fair representation learning: the design of neural networks using learning objectives that satisfy certain fairness or parity constraints in their outputs (Zemel et al., 2013; Louizos et al., 2016; Edwards & Storkey, 2016; Madras et al., 2018). This is attractive because neural network representations often generalize to tasks that are unspecified at train time, which implies that a properly specified fair network can act as a group parity bottleneck that reduces discrimination in unknown downstream tasks.\n\n1University of Toronto 2Vector Institute 3University of Tubingen ¨ 4Google Research. Correspondence to: Elliot Creager <creager@cs.toronto.edu>.\n\nProceedings of the ${ 3 6 } ^ { t h }$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\n\nCurrent approaches to fair representation learning are flexible with respect to downstream tasks but inflexible with respect to sensitive attributes. While a single learned representation can adapt to the prediction of different task labels $y$ , the single sensitive attribute $a$ for all tasks must be specified at train time. Mis-specified or overly constraining train-time sensitive attributes could negatively affect performance on downstream prediction tasks. Can we instead learn a flexibly fair representation that can be adapted, at test time, to be fair to a variety of protected groups and their intersections? Such a representation should satisfy two criteria. Firstly, the structure of the latent code should facilitate simple adaptation, allowing a practitioner to easily adapt the representation to a variety of fair classification settings, where each task may have a different task label $y$ and sensitive attributes $a$ . Secondly, the adaptations should be compositional: the representations can be made fair with respect to conjunctions of sensitive attributes, to guard against subgroup discrimination (e.g., a classifier that is fair to women but not Black women over the age of 60). This type of subgroup discrimination has been observed in commercial machine learning systems (Buolamwini & Gebru, 2018).\n\nIn this work, we investigate how to learn flexibly fair representations that can be easily adapted at test time to achieve fairness with respect to sets of sensitive groups or subgroups. We draw inspiration from the disentangled representation literature, where the goal is for each dimension of the representation (also called the “latent code”) to correspond to no more than one semantic factor of variation in the data (for example, independent visual features like object shape and position) (Higgins et al., 2017; Locatello et al., 2019). Our method uses multiple sensitive attribute labels at train time to induce a disentangled structure in the learned representation, which allows us to easily eliminate their influence at test time. Importantly, at test time our method does not require access to the sensitive attributes, which can be difficult to collect in practice due to legal restrictions (Elliot et al., 2008; DCCA, 1983). The trained representation permits simple and composable modifications at test time that eliminate the influence of sensitive attributes, enabling a wide variety of downstream tasks.\n\nWe first provide proof-of-concept by generating a variant of the synthetic DSprites dataset with correlated ground truth",
      "context_after": "[Section: 2. Background]\n\nGroup Fair Classification In fair classification, we consider labeled examples $x , a , y \\sim p _ { \\mathrm { d a t a } }$ where $y \\in \\mathcal { V }$ are labels we wish to predict, $a \\in { \\mathcal { A } }$ are sensitive attributes, and $x \\in \\mathcal { X }$ are non-sensitive attributes. The goal is to learn a classifier ${ \\hat { y } } = g ( x , a )$ (or ${ \\hat { y } } = g ( x ) )$ which is predictive of $y$ and achieves certain group fairness criteria w.r.t. a. These criteria are typically written as independence properties of the various random variables involved. In this paper we focus on demographic parity, which is satisfied when the predictions are independent of the sensitive attributes: ${ \\hat { y } } \\perp a$ . It is often impossible or undesirable to satisfy demographic parity exactly (i.e. achieve complete independence). In this case, a useful metric is demographic parity distance:\n\nwhere $\\bar { y }$ here is a binary prediction derived from model output $\\hat { y }$ . When $\\Delta _ { D P } = 0$ , demographic parity is achieved; in general, lower $\\Delta _ { D P }$ implies less unfairness.\n\nOur work differs from the fair classification setup as follows: We consider several sensitive attributes at once, and seek fair outcomes with respect to each; individually as well as jointly\n\n(cf. subgroup fair classification, Kearns et al. (2018); Hebert-Johnson et al. (2018)); also, we focus on representation learning rather than classification, with the aim of enabling a range of fair classification tasks downstream.\n\nFair Representation Learning In order to flexibly deal with many label and sensitive attribute sets, we employ representation learning to compute a compact but predicatively useful encoding of the dataset that can be flexibly adapted to different fair classification tasks. As an example, if we learn the function $f$ that achieves independence in the representations $z \\perp a$ with $z = f ( x , a )$ or $z = f ( x )$ , then any predictor derived from this representation will also achieve the desired demographic parity, ${ \\hat { y } } \\perp a$ with $\\hat { y } = g ( z )$ .\n\nThe fairness literature typically considers binary labels and sensitive attributes: $\\mathcal { A } = \\mathcal { y } = \\{ 0 , 1 \\}$ . In this case, approaches like regularization (Zemel et al., 2013) and adversarial regularization (Edwards & Storkey, 2016; Madras et al., 2018) are straightforward to implement. We want to address the case where $a$ is a vector with many dimensions. Group fairness must be achieved for each of the dimensions in $a$ (age, race, gender, etc.) and their combinations.\n\nVAE The vanilla Variational Autoencoder (VAE) (Kingma & Welling, 2013) is typically implemented with an isotropic Gaussian prior $p ( z ) = \\mathcal { N } ( 0 , I )$ . The objective to be maximized is the Evidence Lower Bound (a.k.a., the ELBO),",
      "referring_paragraphs": [
        "Figure 1.",
        "We call our model FFVAE for Flexibly Fair VAE (see Figure 1 for a schematic representation)."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) FFVAE learns the encoder distribution $q ( z , b | x )$ and decoder distributions $p ( x | z , b )$ , $p ( a | b )$ from inputs $x$ and multiple sensitive attributes $a$ . The disentanglement prior structures the latent space by encouraging low $\\mathrm { M I } ( b _ { i } , \\bar { a } _ { j } ) \\forall i \\neq j$ and low $\\mathrm { M I } ( b , z )$ where $\\mathrm { M I } ( \\cdot )$ denotes mutual information.",
        "(b) The FFVAE latent code $[ z , b ]$ can be modified by discarding or noising out sensitive dimensions $\\{ b _ { j } \\}$ , which yields a latent code $[ z , b ^ { \\prime } ]$ independent of groups and subgroups derived from sensitive attributes $\\{ a _ { j } \\}$ . A held out label $_ y$ can then be predicted with subgroup demographic parity.",
        "1906.02589_page0_fig1.jpg",
        "(a) a = Scale",
        "1906.02589_page0_fig2.jpg",
        "(b) a = Shape",
        "1906.02589_page0_fig3.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig3.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg",
      "image_filename": "1906.02589_page0_fig4.jpg",
      "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ",
      "context_before": "",
      "context_after": "[Section: 5.3. Communities & Crime]\n\nDataset Communities & Crime5 is a tabular UCI dataset containing neighborhood-level population statistics. 120 such statistics are recorded for each of the 1, 994 neighborhoods. Several attributes encode demographic information that may be protected. We chose three as sensitive: racePct-Black ( $\\%$ neighborhood population which is Black), black-PerCap (avg per capita income of Black residents), and pct-NotSpeakEnglWell ( $\\%$ neighborhood population that does not speak English well). We follow the same train/eval procedure as with DSpritesUnfair - we train FFVAE with the sensitive attributes and evaluate using naive MLPs to predict a held-out label (violent crimes per capita) on held-out data.\n\nFair Classification This dataset presents a more difficult disentanglement problem than DSpritesUnfair. The three sensitive attributes we chose in Communities and Crime were somewhat correlated with each other, a natural artefact of using real (rather than simulated) data. We note that in general, the disentanglement literature does not provide much guidance in terms of disentangling correlated attributes. Despite this obstacle, FFVAE performed reasonably well in the fair classification audit (Fig. 4). It achieved",
      "referring_paragraphs": [
        "Figure 2."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(c) a = Shape ∧ Scale",
        "(d) a = Shape ∨ Scale",
        "1906.02589_page0_fig5.jpg",
        "1906.02589_page0_fig6.jpg",
        "(a) $a = \\mathbf { R }$",
        "1906.02589_page0_fig7.jpg",
        "(b) $a = \\mathbf { B }$",
        "1906.02589_page0_fig8.jpg",
        "(c) a = P",
        "1906.02589_page0_fig9.jpg",
        "1906.02589_page0_fig10.jpg",
        "1906.02589_page0_fig11.jpg",
        "1906.02589_page0_fig12.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig12.jpg"
        ],
        "panel_count": 9
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_14",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg",
      "image_filename": "1906.02589_page0_fig13.jpg",
      "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.",
      "context_before": "",
      "context_after": "[Section: 5.4. Celebrity Faces]\n\nDataset The CelebA6 dataset contains over 200, 000 images of celebrity faces. Each image is associated with 40 human-labeled binary attributes (OvalFace, HeavyMakeup, etc.). We chose three attributes, Chubby, Eyeglasses, and Male as sensitive attributes7, and report fair classification results on 3 groups and 12 two-attribute-conjunction subgroups only (for brevity we omit three-attribute conjunctions). To our knowledge this is the first exploration of fair representation learning algorithms on the Celeb-A dataset. As in the previous sections we train the encoders on the train set, then evaluate performance of MLP classifiers trained on the encoded test set.",
      "referring_paragraphs": [
        "Figure 4."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(d) $a = \\mathbf { R }$ ∨ B",
        "(e) $a = \\mathbf { R }$ ∨ P",
        "1906.02589_page0_fig14.jpg",
        "(f) a = B ∨ P",
        "1906.02589_page0_fig15.jpg",
        "(a) a = C",
        "1906.02589_page0_fig16.jpg",
        "(b) a = E",
        "1906.02589_page0_fig17.jpg",
        "(c) $a = \\mathbf { M }$",
        "1906.02589_page0_fig18.jpg",
        "1906.02589_page0_fig19.jpg",
        "1906.02589_page0_fig20.jpg",
        "1906.02589_page0_fig21.jpg",
        "(d) a = C ∧ E",
        "1906.02589_page0_fig22.jpg",
        "(e) a = C ∧¬ E",
        "1906.02589_page0_fig23.jpg",
        "(f) a = ¬ C ∧ E",
        "1906.02589_page0_fig24.jpg",
        "(g) a = ¬ C ∧¬ E",
        "1906.02589_page0_fig25.jpg",
        "(h) a = C ∧ M",
        "1906.02589_page0_fig26.jpg",
        "(i) $a = \\mathbf { C } \\wedge \\lnot \\mathbf { M }$",
        "1906.02589_page0_fig27.jpg"
      ],
      "quality_score": 0.6,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig24.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig25.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig26.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig27.jpg"
        ],
        "panel_count": 15
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_29",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
      "image_filename": "1906.02589_page0_fig28.jpg",
      "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.",
      "context_before": "",
      "context_after": "[Section: 6. Discussion]\n\nIn this paper we discussed how disentangled representation learning aligns with the goals of subgroup fair machine learning, and presented a method for learning a structured latent code using multiple sensitive attributes. The proposed model, FFVAE, provides flexibly fair representations, which can be modified simply and compositionally at test time to yield a fair representation with respect to multiple sensitive attributes and their conjunctions, even when test-time sensitive attribute labels are unavailable. Empirically we found that FFVAE disentangled sensitive sources of variation in synthetic image data, even in the challenging scenario where attributes and labels correlate. Our method compared favorably with baseline disentanglement algorithms on downstream fair classifications by achieving better parity for a given accuracy budget across several group and subgroup definitions. FFVAE also performed well on the Communities & Crime and Celeb-A dataset, although none of the models performed robustly across all possible subgroups in the real-data setting. This result reflects the difficulty of subgroup fair representation learning and motivates further work on this topic.\n\nThere are two main directions of interest for future work. First is the question of fairness metrics: a wide range of fairness metrics beyond demographic parity have been proposed (Hardt et al., 2016; Pleiss et al., 2017). Understanding how to learn flexibly fair representations with respect to other metrics is an important step in extending our approach. Secondly, robustness to distributional shift presents an important challenge in the context of both disentanglement and fairness. In disentanglement, we aim to learn independent factors of variation. Most empirical work on evaluating disentanglement has used synthetic data with uniformly distributed factors of variation, but this setting is unrealistic. Meanwhile, in fairness, we hope to learn from potentially biased data distributions, which may suffer from both undersampling and systemic historical discrimination. We might wish to imagine hypothetical “unbiased” data or compute robustly fair representations, but must do so given the data at hand. While learning fair or disentangled representations from real data remains a challenge in practice, we hope that this investigation serves as a first step towards understanding and leveraging the relationship between the two areas.\n\n[Section: References]",
      "referring_paragraphs": [
        "Figure 5."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(j) a = ¬ C ∧ M",
        "(k) a = ¬ C ∧¬ M",
        "1906.02589_page0_fig29.jpg",
        "(l) a = ¬ E ∧ M",
        "1906.02589_page0_fig30.jpg"
      ],
      "quality_score": 0.6,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig29.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig30.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_32",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg",
      "image_filename": "1906.02589_page0_fig31.jpg",
      "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
      "context_before": "[Section: E. Mutual Information Gap]\n\nEvaluation Criteria Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG) (Chen et al., 2018) of FFVAE across various hyperparameter settings. With $J$ latent variables $z _ { j }$ and $K$ factors of variation $v _ { k }$ , MIG is defined as\n\nwhere $j _ { k } = \\underset { j } { \\mathrm { a r g m a x } } M I ( z _ { j } ; v _ { k } ) , M I ( \\cdot ; \\cdot )$ $M I ( \\because \\cdot )$ denotes mutual information, and $K$ is the number of factors of variation. Note that we can only compute this metric in the synthetic setting where the ground truth factors of variation are known. MIG measures the difference between the latent variables which have the highest and second-highest $M I$ with each factor of variation, rewarding models which allocate one latent variable to each factor of variation. We test our disentanglement by training our models on a biased version of DSprites, and testing on a balanced version (similar to the “skewed” data in Chen et al. (2018)). This allows us to separate out two sources of correlation — the correlation existing across the data, and the correlation in the model’s learned representation.\n\nResults In Fig. 6a, we show that MIG increases with $\\alpha$ , providing more evidence that the supervised structure of the FFVAE can create disentanglement. This improvement holds across values of $\\gamma$ , except for some training instability",
      "context_after": "",
      "referring_paragraphs": [
        "Evaluation Criteria Here we analyze the encoder mutual information in the synthetic setting of the DSpritesUnfair dataset, where we know the ground truth factors of variation. In Fig. 6, we calculate the Mutual Information Gap (MIG) (Chen et al., 2018) of FFVAE across various hyperparameter settings. With $J$ latent variables $z _ { j }$ and $K$ factors of variation $v _ { k }$ , MIG is defined as",
        "Figure 6.",
        "We can interpret Figure 6 as evidence that our learned representations are (at least partially) invariant to interventions on a."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) Color is $\\gamma$ , brighter colours $\\longrightarrow$ higher values",
        "(b) Colour is $\\alpha$ , brighter colors −→ higher values",
        "1906.02589_page0_fig32.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig32.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1906.02589",
      "figure_id": "1906.02589_fig_34",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg",
      "image_filename": "1906.02589_page0_fig33.jpg",
      "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 7."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) Colour is α",
        "(b) Colour is γ",
        "1906.02589_page0_fig34.jpg"
      ],
      "quality_score": 0.5499999999999999,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig34.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1907.06430": [
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig0.jpg",
      "image_filename": "1907.06430_page0_fig0.jpg",
      "caption": "",
      "context_before": "Machine learning is increasingly used in a wide range of decision-making scenarios that have serious implications for individuals and society, including financial lending [10,36], hiring [8,28], online advertising [27,41], pretrial and immigration detention [5,43], child maltreatment screening [14,47], health care [19,32], and social services [1,23]. Whilst this has the potential to overcome undesirable aspects of human decision-making, there is concern that biases in the training data and model inaccuracies can lead to decisions that treat historically discriminated groups unfavourably. The research community has therefore started to investigate how to ensure that learned models do not take decisions that are unfair with respect to sensitive attributes (e.g. race or gender).\n\nThis effort has led to the emergence of a rich set of fairness definitions [13,16,21,24,38] providing researchers and practitioners with criteria to evaluate existing systems or to design new ones. Many such definitions have been found to be mathematically incompatible [7,13,15,16,30], and this has been viewed as representing an unavoidable trade-off establishing fundamental limits on fair machine learning, or as an indication that certain definitions do not map on to social or legal understandings of fairness [17].\n\nMost fairness definitions express properties of the model output with respect to sensitive information, without considering the relations among the relevant variables underlying the data-generation mechanism. As different relations would require a model to satisfy different properties in order to be fair, this could lead\n\nto erroneously classify as fair/unfair models exhibiting undesirable/legitimate biases.\n\nIn this manuscript, we use the causal Bayesian network framework to draw attention to this point, by visually describing unfairness in a dataset as the presence of an unfair causal path in the data-generation mechanism. We then use this viewpoint to raise concern on the fairness debate surrounding the COMPAS pretrial risk assessment tool. Finally, we show that causal Bayesian networks offer a powerful tool for representing, reasoning about, and dealing with complex unfairness scenarios.\n\n[Section: 2 A Graphical View of (Un)fairness]\n\nConsider a dataset $\\varDelta = \\{ a ^ { n } , x ^ { n } , y ^ { n } \\} _ { n = 1 } ^ { N }$ , corresponding to $N$ individuals, where $a ^ { n }$ indicates a sensitive attribute, and $x ^ { n }$ a set of observations that can be used (possibly together with $a ^ { n }$ ) to form a prediction $\\hat { y } ^ { n }$ of outcome $y ^ { n }$ . We assume a binary setting $a ^ { n } , y ^ { n } , \\hat { y } ^ { n } \\in \\{ 0 , 1 \\}$ (unless otherwise specified), and indicate with $A , { \\mathcal { X } }$ , $Y$ , and $\\hat { Y }$ the (set of) random variables1 corresponding to $a ^ { n } , x ^ { n } , y ^ { n }$ , and ${ \\hat { y } } ^ { n }$ respectively.\n\nIn this section we show at a high-level that a correct use of fairness definitions concerned with statistical properties of $\\hat { Y }$ with respect to $A$ requires an understanding of the patterns of unfairness underlying $\\varDelta$ , and therefore of the relationships among $A$ , $\\mathcal { X }$ and $Y$ . More specifically we show that:\n\n(i) Using the framework of causal Bayesian networks (CBNs), unfairness in $\\varDelta$ can be viewed as the presence of an unfair causal path from $A$ to $\\mathcal { X }$ or $Y$ .   \n(ii) In order to determine which properties $\\hat { Y }$ should possess to be fair, it is necessary to question and understand unfairness in $\\varDelta$ .",
      "context_after": "Assume a dataset $\\varDelta = \\{ a ^ { n } , x ^ { n } = \\{ q ^ { n } , d ^ { n } \\} , y ^ { n } \\} _ { n = 1 } ^ { N }$ corresponding to a college admission scenario in which applicants are admitted based on qualifications $Q$ , choice of department $D$ , and gender $A$ ; and in which female applicants apply more often to certain departments. This scenario can be represented by\n\nthe CBN on the left (see Appendix A for an overview of BNs, and Sect. 3 for a detailed treatment of CBNs). The causal path $A  Y$ represents direct influence of gender $A$ on admission $Y$ , capturing the fact that two individuals with the same qualifications and applying to the same department can be treated differently depending on their gender. The indirect causal path $A  D  Y$ represents influence of $A$ on $Y$ through $D$ , capturing the fact that female applicants more often apply to certain departments. Whilst the direct path $A  Y$ is certainly an unfair one, the paths $A  D$ and $D  Y$ , and therefore $A  D  Y$ , could either be considered as fair or as unfair. For example, rejecting women more often due to department choice could be considered fair with respect to college\n\nresponsibility. However, this could be considered unfair with respect to societal responsibility if the departmental differences were a result of systemic historical or cultural factors (e.g. if female applicants apply to specific departments at lower rates because of overt or covert societal discouragement). Finally, if the college were to lower the admission rates for departments chosen more often by women, then the path $D  Y$ would be unfair.\n\nDeciding whether a path is fair or unfair2 requires careful ethical and sociological considerations and/or might not be possible from a dataset alone. Nevertheless, this example illustrates that we can view unfairness in a dataset as the presence of an unfair causal path from the sensitive attribute $A$ to $\\mathcal { X }$ or $Y$ .\n\nDifferent (un)fair path labeling requires $\\hat { Y }$ to have different characteristics in order to be fair. In the case in which the causal paths from $A$ to $Y$ are all unfair (e.g. if $A  D  Y$ is considered unfair), a $\\hat { Y }$ that is statistically independent of $A$ (denoted with $\\hat { Y } \\perp \\perp A ^ { \\prime }$ ) would not contain any of the unfair influence of $A$ on $Y$ . In such a case, $\\hat { Y }$ is said to satisfy demographic parity.\n\nDemographic Parity (DP). $\\hat { Y }$ satisfies demographic parity if $\\hat { Y } \\perp \\perp A$ , i.e. $p ( { \\hat { Y } } =$ $1 | A = 0 ) = p ( \\hat { Y } = 1 | A = 1 )$ ), where e.g. $p ( \\hat { Y } = 1 | A = 0 )$ can be estimated as\n\nwith $\\mathbb { 1 } _ { \\hat { y } ^ { n } = 1 , a ^ { n } = 0 } = 1$ if $\\hat { y } ^ { n } = 1$ and $\\boldsymbol { a } ^ { n } = 0$ (and zero otherwise), and where $N _ { 0 }$ indicates the number of individuals with $a ^ { \\prime \\iota } = 0$ . Notice that many classifiers, rather than a binary prediction $\\hat { y } ^ { n } \\in \\{ 0 , 1 \\}$ , output a degree of belief that the individual belongs to class 1, $r ^ { \\pi }$ , also called score. For example, in the case of logistic regression $r ^ { n }$ corresponds to the probability of class 1, i.e. $r ^ { n } = p ( Y =$ $1 | a ^ { n } , x ^ { n } )$ . To obtain the prediction $\\hat { y } ^ { n } \\in \\{ 0 , 1 \\}$ from $r ^ { \\pi }$ , it is common to use a threshold $\\theta$ , i.e. $\\hat { y } ^ { \\prime \\prime } = \\mathbb { 1 } _ { r ^ { n } > \\theta }$ . In this case, we can rewrite the estimate for $p ( \\hat { Y } = 1 | A = 0 )$ ) as\n\nNotice that $R \\perp \\perp A$ implies $\\hat { Y } \\perp \\perp A$ for all values of $\\theta$ .\n\nIn the case in which the causal paths from $A$ to $Y$ are all fair (e.g. if $A  Y$ is absent and $A  D  Y$ is considered fair), a $\\hat { Y }$ such that ${ \\hat { Y } } \\perp \\perp A | Y$ or $Y \\perp A | \\hat { Y }$ would be allowed to contain such a fair influence, but the (dis)agreement between $Y$ and $\\hat { Y }$ would not be allowed to depend on $A$ . In these cases, $\\hat { Y }$ is said to satisfy equal false positive/false negative rates and calibration respectively.\n\nEqual False Positive and Negative Rates (EFPRs/EFNRs). $\\hat { Y }$ satisfies EFPRs and EFNRs if ${ \\hat { Y } } \\perp \\perp A | Y$ , i.e. (EFPRs) $p ( \\hat { Y } = 1 | Y = 0 , A = 0 ) = p ( \\hat { Y } =$",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.4,
      "metadata": {}
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg",
      "image_filename": "1907.06430_page0_fig1.jpg",
      "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .",
      "context_before": "Calibration. $\\hat { Y }$ satisfies calibration if $Y ~ \\perp ~ A | \\hat { Y }$ . In the case of score output $R$ , this condition is often instead called predictive parity at threshold $\\theta$ , $p ( Y = 1 | R > \\theta , A = 0 ) = p ( Y = 1 | R > \\theta , A = 1 )$ , and calibration defined as requiring $Y \\perp \\perp A | R$ .\n\nIn the case in which at least one causal path from $A$ to $Y$ is unfair (e.g. if $A $ $Y$ is present), EFPRs/EFNRs and calibration are inappropriate criteria, as they would not require the unfair influence of $A$ on $Y$ to be absent from $\\hat { Y }$ (e.g. a perfect model ( ${ \\hat { Y } } = Y$ ) would automatically satisfy EFPRs/EFNRs and calibration, but would contain the unfair influence). This observation is particularly relevant to the recent debate surrounding the correctional offender management profiling for alternative sanctions (COMPAS) pretrial risk assessment tool. We revisit this debate in the next section.\n\n[Section: 2.1 The COMPAS Debate]\n\nOver the past few years, numerous state and local governments around the United States have sought to reform their pretrial court systems with the aim of reducing unprecedented levels of incarceration, and specifically the population of low-income defendants and racial minorities in America’s prisons and jails [2,25,31]. As part of this effort, quantitative tools for determining a person’s likelihood for reoffending or failure to appear, called risk assessment instruments (RAIs), were introduced to replace previous systems driven largely by opaque discretionary decisions and money bail [6,26]. However, the expansion of pretrial RAIs has unearthed new concerns of racial discrimination which would nullify the purported benefits of these systems and adversely impact defendants’ civil liberties.\n\nAn intense ongoing debate, in which the research community has also been heavily involved, was triggered by an exposé from investigative journalists at ProPublica [5] on the COMPAS pretrial RAI developed by Equivant (formerly Northpointe) and deployed in Broward County in Florida. The COMPAS general recidivism risk scale (GRRS) and violent recidivism risk scale (VRRS), the focus of ProPublica’s investigation, sought to leverage machine learning techniques to improve the predictive accuracy of recidivism compared to older RAIs such as the level of service inventory-revised [3] which were primarily based on theories and techniques from a sub-field of psychology known as the psychology of criminal conduct [4,9]3.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {}
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg",
      "image_filename": "1907.06430_page0_fig2.jpg",
      "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.",
      "context_before": "",
      "context_after": "[Section: 3 Causal Bayesian Networks]\n\nA Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies. Each node $X _ { i }$ in the graph is\n\nassociated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$ . The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditional distributions, i.e. $\\begin{array} { r } { p ( X _ { 1 } , \\ldots , X _ { I } ) = \\prod _ { i = 1 } ^ { I } p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) ) } \\end{array}$ (see Appendix A for more details on Bayesian networks).\n\nWhen equipped with causal semantic, namely when representing the datageneration mechanism, Bayesian networks can be used to visually express causal relationships. More specifically, CBNs enable us to give a graphical definition of causes and causal effects: if there exists a directed path from $A$ to $Y$ , then $A$ is a potential cause of $Y$ . Directed paths are also called causal paths.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.5,
      "metadata": {}
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_4",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg",
      "image_filename": "1907.06430_page0_fig3.jpg",
      "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .",
      "context_before": "[Section: 3 Causal Bayesian Networks]\n\nA Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies. Each node $X _ { i }$ in the graph is\n\nassociated with the conditional distribution $p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) )$ , where $\\operatorname { p a } ( X _ { i } )$ is the set of parents of $X _ { i }$ . The joint distribution of all nodes, $p ( X _ { 1 } , \\ldots , X _ { I } )$ , is given by the product of all conditional distributions, i.e. $\\begin{array} { r } { p ( X _ { 1 } , \\ldots , X _ { I } ) = \\prod _ { i = 1 } ^ { I } p ( X _ { i } | \\mathrm { p a } ( X _ { i } ) ) } \\end{array}$ (see Appendix A for more details on Bayesian networks).\n\nWhen equipped with causal semantic, namely when representing the datageneration mechanism, Bayesian networks can be used to visually express causal relationships. More specifically, CBNs enable us to give a graphical definition of causes and causal effects: if there exists a directed path from $A$ to $Y$ , then $A$ is a potential cause of $Y$ . Directed paths are also called causal paths.",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [
        "(a)",
        "(b)",
        "1907.06430_page0_fig4.jpg"
      ],
      "quality_score": 0.5,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig4.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_6",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg",
      "image_filename": "1907.06430_page0_fig5.jpg",
      "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .",
      "context_before": "",
      "context_after": "i.e. we retrieve the back-door adjustment formula.\n\nIn the remainder of the section we show that, by performing different interventions on $A$ along different causal paths, it is possible to isolate the contribution of the causal effect of $A$ on $Y$ along a group of paths.\n\n[Section: Direct and Indirect Effect]\n\nConsider the CBN of Fig. 4(b), containing the direct path $A  Y$ and one indirect causal path through the variable $M$ . Let $Y _ { a } ( M _ { \\bar { a } } )$ be the random variable\n\nwith distribution equal to the conditional distribution of $Y$ given $A$ restricted to causal paths, with $A = a$ along $A  Y$ and $A = { \\bar { a } }$ along $A  M  Y$ . The average direct effect (ADE) of $A = a$ with respect to $A = \\bar { a }$ , defined as",
      "referring_paragraphs": [
        "Consider the CBN of Fig. 4(b), containing the direct path $A  Y$ and one indirect causal path through the variable $M$ . Let $Y _ { a } ( M _ { \\bar { a } } )$ be the random variable\n\nwith distribution equal to the conditional distribution of $Y$ given $A$ restricted to causal paths, with $A = a$ along $A  Y$ and $A = { \\bar { a } }$ along $A  M  Y$ . The average direct effect (ADE) of $A = a$ with respect to $A = \\bar { a }$ , defined as"
      ],
      "figure_type": "other",
      "sub_figures": [
        "(a)",
        "1907.06430_page0_fig6.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig6.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg",
      "image_filename": "1907.06430_page0_fig7.jpg",
      "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
      "context_before": "[Section: Path-Specific Effect]\n\nTo estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention $A = a$ along the group of interest and $A = { \\bar { a } }$ along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of $A$ on $Y$ along the direct path $A  Y$ and the paths passing through $M$ , $A \\to M \\to , \\dots , \\to Y$ , namely along the red links. The path-specific effect (PSE) of $A = a$ with respect to $A = \\bar { a }$ for this group of paths is defined as\n\nwhere $p ( Y _ { a } ( M _ { a } , L _ { \\bar { a } } ( M _ { a } ) ) )$ is given by",
      "context_after": "To understand this, consider the hiring scenario described by the CBN on the left, where $A$ represents religious belief and $E$ educational background of the applicant, which influences religious participation ( $E  A$ ). Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\n\nonly based on $E$\n\n[Section: 4.2 Opening Closed Unfair Paths from $\\pmb { A }$ to $\\mathbf { Y }$]\n\nIn Sect. 2, we have seen that, in order to reason about fairness of $\\hat { Y }$ , it is necessary to question and understand unfairness in $\\varDelta$ . In this section, we warn that another crucial element needs to be considered in the fairness discussion around $\\hat { Y }$ , namely\n\n(i) The variables used to form $\\hat { Y }$ could project into $\\hat { Y }$ unfair patterns in $\\mathcal { X }$ that do not concern $Y$ .\n\nThis could happen, for example, if a closed unfair path from $A$ to $Y$ is opened when conditioning on the variables used to form $\\hat { Y }$ .",
      "referring_paragraphs": [
        "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).",
        "The same conclusion could have been obtained by looking at the graph annotated with path coefficients (Fig. 5 (bottom)). The PSE is obtained by summing over the three causal paths of interest ( $A  Y$ , $A  M  Y$ , and $A \\to M \\to L \\to Y$ ) the product of all coefficients in each path.\n\nNotice that AIE $\\bar { a } a$ , given by"
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1907.06430_page0_fig8.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig8.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_10",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg",
      "image_filename": "1907.06430_page0_fig9.jpg",
      "caption": "Fig. 6. CBN underlying a music degree scenario.",
      "context_before": "To understand this, consider the hiring scenario described by the CBN on the left, where $A$ represents religious belief and $E$ educational background of the applicant, which influences religious participation ( $E  A$ ). Whilst Y ✚⊥⊥A due to the open back-door path from $A$ to $Y$ , the hiring decision $Y$ is\n\nonly based on $E$\n\n[Section: 4.2 Opening Closed Unfair Paths from $\\pmb { A }$ to $\\mathbf { Y }$]\n\nIn Sect. 2, we have seen that, in order to reason about fairness of $\\hat { Y }$ , it is necessary to question and understand unfairness in $\\varDelta$ . In this section, we warn that another crucial element needs to be considered in the fairness discussion around $\\hat { Y }$ , namely\n\n(i) The variables used to form $\\hat { Y }$ could project into $\\hat { Y }$ unfair patterns in $\\mathcal { X }$ that do not concern $Y$ .\n\nThis could happen, for example, if a closed unfair path from $A$ to $Y$ is opened when conditioning on the variables used to form $\\hat { Y }$ .",
      "context_after": "i.e. $\\hat { Y } = \\gamma M$ . Therefore, this predictor would be fair. From the CBN we can see that the explicit use of $A$ can be of help in retrieving $M$ . Indeed, since $M { \\mathrel { \\mathop { A C A } } } | X$ , using $A$ in addition to $X$ can give information about $M$ . In general (e.g. in a non-linear setting) it is not guaranteed that using $A$ would ensure $\\hat { Y } \\perp \\perp A$ . Nevertheless, this example shows how explicit use of the sensitive attribute in a model can ensure fairness rather than leading to unfairness.\n\nThis observation is relevant to one of the simplest fairness definitions, motivated by legal requirements, called fairness through unawareness, which states that $\\hat { Y }$ is fair as long as it does not make explicit use of the sensitive attribute $A$ Whilst this fairness criterion is often indicated as problematic because some of the variables used to form $\\hat { Y }$ could be a proxy for $A$ (such as neighborhood for race), the example above shows a more subtle issue with it.\n\n[Section: 4.3 Path-Specific Population-level Unfairness]\n\nIn this section, we show that the path-specific effect introduced in Sect. 3 can be used to quantify unfairness in $\\varDelta$ in complex scenarios.\n\nConsider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A  D$ , and therefore $A  D  Y$ , is considered unfair, unfairness overall population can be quantified with $\\langle Y \\rangle _ { p ( Y | a ) } - \\langle Y \\rangle _ { p ( Y | \\bar { a } ) }$\n\n(coinciding with $\\mathrm { A T E } _ { \\bar { a } a } = \\langle Y _ { a } \\rangle _ { p ( Y _ { a } ) } - \\langle Y _ { \\bar { a } } \\rangle _ { p ( Y _ { \\bar { a } } ) } )$ where, for example, $A = a$ and $A = \\bar { a }$ indicate female and male applicants respectively.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.65,
      "metadata": {}
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_11",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
      "image_filename": "1907.06430_page0_fig10.jpg",
      "caption": "Fig. 7. CBN underlying a college admission scenario.",
      "context_before": "i.e. $\\hat { Y } = \\gamma M$ . Therefore, this predictor would be fair. From the CBN we can see that the explicit use of $A$ can be of help in retrieving $M$ . Indeed, since $M { \\mathrel { \\mathop { A C A } } } | X$ , using $A$ in addition to $X$ can give information about $M$ . In general (e.g. in a non-linear setting) it is not guaranteed that using $A$ would ensure $\\hat { Y } \\perp \\perp A$ . Nevertheless, this example shows how explicit use of the sensitive attribute in a model can ensure fairness rather than leading to unfairness.\n\nThis observation is relevant to one of the simplest fairness definitions, motivated by legal requirements, called fairness through unawareness, which states that $\\hat { Y }$ is fair as long as it does not make explicit use of the sensitive attribute $A$ Whilst this fairness criterion is often indicated as problematic because some of the variables used to form $\\hat { Y }$ could be a proxy for $A$ (such as neighborhood for race), the example above shows a more subtle issue with it.\n\n[Section: 4.3 Path-Specific Population-level Unfairness]\n\nIn this section, we show that the path-specific effect introduced in Sect. 3 can be used to quantify unfairness in $\\varDelta$ in complex scenarios.\n\nConsider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path $A  D$ , and therefore $A  D  Y$ , is considered unfair, unfairness overall population can be quantified with $\\langle Y \\rangle _ { p ( Y | a ) } - \\langle Y \\rangle _ { p ( Y | \\bar { a } ) }$\n\n(coinciding with $\\mathrm { A T E } _ { \\bar { a } a } = \\langle Y _ { a } \\rangle _ { p ( Y _ { a } ) } - \\langle Y _ { \\bar { a } } \\rangle _ { p ( Y _ { \\bar { a } } ) } )$ where, for example, $A = a$ and $A = \\bar { a }$ indicate female and male applicants respectively.",
      "context_after": "The relationships between $A , Q , D , Y$ and $Y _ { \\bar { a } } ( D _ { a } )$ in this model can be inferred from the twin Bayesian network [39] on the left resulting from the intervention $A \\ = \\ a$ along $A \\  \\ D$ and $A \\ = \\ \\bar { a }$ along $A  Y$ : in addition to $A , Q , D$ and $Y$ , the network contains the variables $Q ^ { * }$ , $D _ { a }$ and $Y _ { \\bar { a } } ( D _ { a } )$ corresponding to the counterfactual world in which $A = \\bar { a }$ along $A  Y$ , with $Q ^ { * } = \\theta ^ { q } + \\epsilon _ { q } , D _ { a } = \\theta ^ { d } + \\theta _ { a } ^ { d } a + \\epsilon _ { d }$ , and $Y _ { \\bar { a } } ( D _ { a } ) = \\theta ^ { y } + \\theta _ { a } ^ { y } \\bar { a } + \\theta _ { q } ^ { y } Q ^ { * } + \\theta _ { d } ^ { y } D _ { a } + \\epsilon _ { y }$ . The two groups of variables are connected through $\\epsilon _ { d } , \\epsilon _ { q } , \\epsilon _ { y }$ , indicating that the factual and counterfactual worlds\n\nshare the same unobserved randomness. From this network, we can deduce that $Y _ { \\bar { a } } ( D _ { a } ) \\perp \\perp \\{ A , Q , D , Y \\} | \\epsilon = \\{ \\epsilon _ { q } , \\epsilon _ { d } , \\epsilon _ { y } \\} ^ { 6 }$ , and therefore that we can express\n\nAs $p ( \\epsilon _ { q } ^ { n } | a , q ^ { n } , d ^ { n } , y ^ { n } ) \\ = \\ \\delta _ { \\epsilon _ { q } ^ { n } = q ^ { n } - \\theta ^ { q } }$ , p(\u000fnd |a, qn, dn, yn) = δ\u000fn=dn−θd−θda, and $p ( \\epsilon _ { y } ^ { n } | a , q ^ { n } , d ^ { n } , y ^ { n } ) = \\delta _ { \\epsilon _ { y } ^ { n } = y ^ { n } - \\theta ^ { y } - \\theta _ { a } ^ { y } a - \\theta _ { q } ^ { y } q ^ { n } - \\theta _ { d } ^ { y } d ^ { n } }$ , we obtain\n\nIndeed, by expressing $Y _ { \\bar { a } } ( D _ { a } )$ as a function of $\\epsilon _ { q } ^ { n } , \\epsilon _ { d } ^ { n }$ and $\\epsilon _ { y } ^ { n }$ , we obtain",
      "referring_paragraphs": [
        "Fig. 7. CBN underlying a college admission scenario.\n\nIn the more complex case in which the path $A $ $D  Y$ is considered fair, unfairness can instead be quantified with the path-specific effect along the direct path $A  Y$ , PSE $^ { a a }$ , given by"
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1907.06430_page0_fig11.jpg"
      ],
      "quality_score": 0.75,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig11.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "1907.06430",
      "figure_id": "1907.06430_fig_13",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg",
      "image_filename": "1907.06430_page0_fig12.jpg",
      "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.",
      "context_before": "The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Stepleton, Tom Everitt, and Shira Mitchell for useful discussions.\n\n[Section: Appendix A Bayesian Networks]\n\nA graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.\n\nA path from node $X _ { i }$ to node $X _ { j }$ is a sequence of linked nodes starting at $X _ { i }$ and ending at $X _ { j }$ . A directed path is a path whose links are directed and pointing from preceding towards following nodes in the sequence.",
      "context_after": "[Section: Appendix B EFPRs/EFNRs and Calibration]\n\nAssume that EFPRs/EFNRs are satisfied, i.e. $p ( \\hat { Y } = 1 | A = 0 , Y = 1 ) = p ( \\hat { Y } =$ $1 | A = 1 , Y = 1 ) \\equiv p _ { \\hat { Y } _ { 1 } | Y _ { 1 } }$ and $p ( \\dot { Y } = 1 | A = 0 , Y = 0 ) = p ( \\dot { Y } = 1 | A = 1 , Y = 0 ) \\equiv$ $\\psi _ { \\hat { Y } _ { 1 } | Y _ { 0 } }$ . From\n\nwe see that, to also satisfy $p ( Y = 1 | A = 0 , \\hat { Y } = 1 ) = p ( Y = 1 | A = 1 , \\hat { Y } = 1 ) .$ , we need $\\bigl ( \\underline { { p } } _ { \\hat { X } _ { 1 } | \\mathcal { H } _ { 1 } } \\underline { { p } } _ { \\hat { Y } _ { 1 } ^ { \\prime } | \\mathcal { A } _ { 1 } } + \\underline { { p } } _ { \\hat { Y } _ { 1 } | \\mathcal { T } _ { 0 } } \\bigl ( 1 - \\underline { { p } } _ { X _ { 1 } | \\mathcal { A } _ { 1 } } \\bigr ) \\bigr ) p _ { Y _ { 1 } | \\mathcal { A } _ { 0 } } = \\bigl ( \\underline { { p } } _ { \\hat { X } _ { 1 } | \\mathcal { H } _ { 1 } } \\underline { { p } } _ { \\hat { Y } _ { 1 } ^ { \\prime } | \\mathcal { A } _ { 0 } } + \\underline { { p } } _ { \\hat { Y } _ { 1 } | \\mathcal { T } _ { 0 } } \\bigl ( 1 - \\underline { { p } } _ { \\hat { X } _ { 1 } | \\mathcal { H } _ { 1 } } \\bigr ) \\bigr )$ $\\underbrace { p _ { Y _ { 1 } | A _ { 0 } } ) } _ { \\substack { p _ { Y _ { 1 } | A _ { 1 } } } }$ , i.e. $p _ { Y _ { 1 } | A _ { 0 } } = p _ { Y _ { 1 } | A _ { 1 } }$ .",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [
        "(a)",
        "(b)",
        "1907.06430_page0_fig13.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig13.jpg"
        ],
        "panel_count": 2
      }
    }
  ],
  "1907.09013": [
    {
      "doc_id": "1907.09013",
      "figure_id": "1907.09013_fig_1",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg",
      "image_filename": "1907.09013_page0_fig0.jpg",
      "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ",
      "context_before": "It is easy to dismiss claims that algorithmic decision-making produces unintentional discrimination by standing behind the supposed “objectivity” of data driven decision-making8. As of writing this article, the research community for discrimination-aware data mining is relatively small, and the set of published, and most importantly, commercially validated research is likewise limited. Prior efforts have been made to cover ways in which discrimination can emerge in a classification system1, 16. In this section we add to and synthesize the prior efforts, and we organize the discrimination drivers into a taxonomy.\n\nOur goal is twofold: first, to educate and convince data scientists that, even with the best of intentions, some form of discrimination may arise in their systems, and second, to connect these causes to stages within the model development lifecycle so that the practicing data scientist can both anticipate and proactively manage these potential problems.\n\n[Section: 3.1 Model Building 101]\n\nFigure 1 shows a commonly accepted process diagram for model building and deployment. This is known as the Cross Industry Standard Process for Data Mining (CRISP-DM)17, and has been a standard for well over a decade.\n\nThis diagram shows how data science tasks (particularly deployed supervised learning systems) are commonly approached as an iterative process, where the feedback from prior iterations informs models and decisions in subsequent steps. While the heart of a classification system is the training step, it is generally well understood by data scientists that training the actual algorithm comprises the minority of a data scientist’s time. The majority of time spent is generally focused on making somewhat subjective decisions, such as what events to predict, where and how to sample data, how to clean said data, how to evaluate the model and how to create a decision policy from the algorithm’s output.\n\nOur taxonomy will show that discrimination can creep in at any one of these stages, so persistent vigilance and awareness is advised throughout the process. When we discuss solutions later, we will also make recommendations on where in this process a data scientist can employ discrimination-aware unit tests to appropriately audit the need and effectiveness of their chosen discrimination-aware techniques.\n\nFigure 1: Process diagram for the CRISP-DM model. This model generalizes the commonly used practice for building and deploying a machine learning model.",
      "context_after": "[Section: 3.2 A Taxonomy of Causes]\n\nA classifier alone does not discriminate, but unintended discrimination can find its way into a classification system in various ways, and at all parts of the data mining process.\n\nWe focus on building and on using the classifier. For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification. When we consider how the classifier is used, we include in our taxonomy “procedural failures” that could lead to discrimination.\n\nFigure 2 shows a high level view of the taxonomy, along with indicators showing in what parts of CRISP-DM we might expect to encounter each cause. After providing more detail on each entry in the taxonomy, we will follow with a survey of remedies for the practicing data scientist to consider when faced with these issues.\n\nFigure 2: Commonly cited causes of classifier discrimination.   \n\n<table><tr><td>Component</td><td>Root Cause</td><td>Level 1 Description</td><td>Level 2 Description</td></tr><tr><td rowspan=\"11\">Classifier (Model Development)</td><td rowspan=\"7\">Misspecification</td><td rowspan=\"3\">Target Variable</td><td>Heterogeneous Target Variable (A,B,C,D)</td></tr><tr><td>Proxy Target Variable Learning (A,B,C,D)</td></tr><tr><td>Target Variable Subjectivity (A)</td></tr><tr><td rowspan=\"2\">Model/Feature</td><td>Inclusion of Protected Attributes without Control Variables (C,D)</td></tr><tr><td>Inclusion of Protected Attribute Proxies (Redlining) (C,D)</td></tr><tr><td rowspan=\"2\">Loss Function</td><td>Omitted Discrimination Penalties (D)</td></tr><tr><td>Failure to Specify Assymmetric Error Costs (D,E)</td></tr><tr><td rowspan=\"4\">Data Issue</td><td colspan=\"2\">Discrimination In Data (A,B,C,D)</td></tr><tr><td rowspan=\"2\">Sample Bias</td><td>Under-representation of Protected Class (B,C,D)</td></tr><tr><td>Over-representation of Protected Class (B,C,D)</td></tr><tr><td colspan=\"2\">Low Support (B,C,D)</td></tr><tr><td rowspan=\"4\">Classification System (Process Failures)</td><td rowspan=\"2\">Failure to Audit</td><td colspan=\"2\">Data Preparation (B,C)</td></tr><tr><td colspan=\"2\">Classifier: Pre-Deployment(D,E,F)</td></tr><tr><td colspan=\"3\">Poor Feedback Loop (E,F)</td></tr><tr><td colspan=\"3\">Non-Deference to Human Experts (A,E,F)</td></tr></table>\n\nCaption{Letters detail where in the CRISP-DM process a data scientist is likely to encounter each cause, corresponding to: $\\mathbf { A } =$ Business Understanding, $\\boldsymbol { \\mathrm { B } } =$ Data Understanding, $\\mathrm { C } =$ Data Preparation, $\\mathrm { D } =$ Modeling, $\\mathrm { E } =$ Evaluation, $\\mathrm { F } =$ Deployment.}\n\n[Section: 3.2.1 A Classifier’s Source of Discrimination]",
      "referring_paragraphs": [
        "Figure 2 shows a high level view of the taxonomy, along with indicators showing in what parts of CRISP-DM we might expect to encounter each cause."
      ],
      "figure_type": "diagram",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "1907.09013",
      "figure_id": "1907.09013_fig_2",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg",
      "image_filename": "1907.09013_page0_fig1.jpg",
      "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.",
      "context_before": "A classifier does not itself discriminate. It takes a human process that includes that classifier to discriminate. Therefore a data scientist needs to remain vigilant throughout the entire evaluation and deployment process. The greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior. This is an error that starts with the data scientist but should extend upwards to the executive team. As the developer and steward of the algorithm, the data scientist should meet with all constituents to understand what are the (potentially competing) objectives. Teams should discuss and agree upon a set of legal and ethical standards for each type of problem. With a concise set of standards for what discrimination means to the application, and what are the allowable boundaries, the data scientist can select an appropriate discrimination metric and begin a series of discrimination unit tests on decisions made by the system.\n\nAnother process failure involves designing appropriate feedback loops. If a system’s predictions can produce individual harm, it should be the responsibility of internal managers to monitor and verify the validity of the system’s output. Additionally, when designing the classification system’s feedback loop, the data scientist must account for the distortion that any model-based intervention may have on the data generating or sampling distribution.\n\nAn example of this arises in predictive policing, where non-violent crimes are more likely to occur (or really, be observed and registered) where police officers patrol. As most systems are trained on production data, once a model goes live, the distribution of the data the production system collects will shift towards the decisions made by the system. Such distortions can create extremely negative vicious cycles, where the interventions induce statistical deviations between protected and non-protected classes which then get learned and perpetuated by the next iteration of the algorithm.\n\nFinally, while data scientists should approach their work with scientific rigor in mind, we encourage an active and healthy level of skepticism towards all models and all results. Part of\n\nthis skepticism should include deference to human experts where applicable. One seldom finds a classifier with perfect or even near-perfect predictions. There will always be hard-to-classify points near a decision boundary, or irreducible error in the target variable, and in such cases, the type of systems we have been discussing here can do possibly irreparable harm to an individual or specific groups.\n\nPart of being a responsible data scientist is knowing how and when to involve human experts in the decision making process. Active Learning is a branch of machine learning that focuses on cost efficient ways to engage experts in a way to improve future iterations of a model24, and this body of work can provide tools for data scientists to not only address discrimination, but to mitigate classification errors that have a high human cost.\n\nAdditionally, classification systems are limited by the data at their disposal. A parole board making a judgment solely based on predictions derived from a classifier with a static set of features (such as criminal record and responses from a survey) may miss an opportunity to incorporate recent experiences with the convict, or any information outside of the observational scope of the classifier. In general, we should never expect any classifier to be feature complete. Keeping humans in the loop improves objectivity and nuanced flexibility.\n\nWhile we do not claim that the above set of issues and rules is exhaustive, we feel it is a comprehensive starting point for the practicing data scientist to learn to become aware of how unintended discrimination may occur. Awareness is a necessary but not sufficient condition for preventing discrimination in classification systems.\n\n[Section: 4. Auditing and Removing Unintended Discrimination]\n\nIn this section we propose a discrimination-aware auditing process that mirrors the standard process for developing classifiers. It has been amended to incorporate discrimination unit tests and highlights where state-of-the-art remedies may fit. We also present an overview of prior discrimination-aware data mining work as a reference to readers, with suggestions for how a data scientist might think about incorporating different methods into their common workflows. By developing discrimination-aware systems, data scientists can incorporate ethical and legal constraints into their models, and thereby result in the intended outcomes that ethics and law hope to achieve.",
      "context_after": "Figure 3\n\nCaption {We update standard a classification process diagram to reflect key testing and decision points that can inject discrimination awareness into the workflow. Dotted arrows and figures represent optional workflows, representing pre-processing, in-processing, and post-processing techniques, as suggested by the prior art}.\n\nOur process in Figure 3 begins with the raw data to be used for model development. We will assume here that the scientist has already carefully formulated the business problem and has consulted with the appropriate managers and legal team to understand the applicable antidiscrimination laws and/or ethical standards governing the application. This prerequisite work should define what data is available as a target variable and as predictive features, and what the appropriate discrimination metric should be.\n\nGiven this set up, we propose several “discrimination aware” unit tests that may guide the future development of the classifier. These tests fit into the process where exploratory analysis usually sits.\n\nThe first test is to use the chosen discrimination metric to determine if the underlying data discriminates against a protected attribute with respect to the target variable Y. These tests should\n\ninclude any separately measureable event that may be rolled up into the target variable (see above discussion on heterogeneous target variables for motivation). Next, the data scientist should test and document any features that have medium to high correlation with the protected attribute. Finally, for each level of each protected attribute, and possibly conjunctions of multiple attributes, the support $( P ( S ^ { i } ) )$ , which is to say the proportion in the overall population, should be computed to learn about which protected group errors may be more susceptible to statistical estimation errors.\n\nWhen any of the above tests score above or below a certain threshold (which should be set by the applicable legal and ethical standards), the data scientist can remove the discrimination via methods that fit into multiple stages of the process. For more comprehensive coverage of specific methods, we direct readers to prior surveys of the field8,16. We present in this section several high level methods following the organization nomenclature of Hajian et al from their tutorial in the 2016 ACMSigKDD conference.\n\nThe discrimination-aware methods are grouped into pre-processing, in-processing, and postprocessing techniques. These fit into our diagram in Figure 3 as techniques that can be executed in the data pre-processing, training, or post-scoring evaluation steps (respectively). Figure 3 shows these in diagrams with dotted lines, indicating that the data scientist may choose one or a combination of these techniques.\n\nPre-processing techniques attack the problem by removing the underlying discrimination from the data prior to any modeling. Kamiran and Calders present three promising pre-processing methods that are competitive to other non-pre-processing techniques25. These methods involve one of strategically relabeling examples near the classification margin, reweighting or resampling examples, and they particularly endorse the relabeling or resampling techniques. These are extended in future work to account for cases of conditional, or explainable discrimination. Feldman et al present another method which removes dependencies between $X$ and S but still allows $X$ to be predictive of Y. They acknowledge the tradeoff between fairness and accuracy and design the method so that the tradeoff can be tunable and tailored to a specific application. There is no direct comparison between these two methods, though we see more implementation barriers in the Feldman approach26. Kamiran et al present clear instruction on their methods with minimal mathematical notation, making the learning curve for adoption potentially shorter for data scientists looking for a quick prototype. Additionally, Feldman et al admit limitations on the applicability to non-numeric data (i.e., categorical), whereas the methods by Kamiran et al appear to be operable on all data types.\n\nIn-processing techniques can be thought of as modifications of traditional learning algorithms to address discrimination during the model training phase. This is referred to as ‘Discrim Aware Training’ in the process shown in Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.\n\nPost-processing is the final class of methods and can be performed post-training. It relies upon access to a holdout set that was not involved in the model’s training phase. We highlight a recent contribution to the prior art and focus on some highly desirable properties of this class of approaches. Hardt et al propose a post-processing technique that works very well in our present environment of black box classification systems. The elegance of this approach is that it relies solely on access to the prediction (A), protected attribute (S) and label (Y) in the data, and can be executed with only aggregated data. Details between the mapping of $X$ to $A$ are not needed, making this approach very amenable to situations in which third parties (either internally or externally) are engaged to independently de-discriminate the classification system. Despite these promising properties, there is no empirical evaluation between this approach and others, and the exact implementation details are somewhat obscured by mathematical detail.\n\nWhen and how to choose both a discrimination measure and a discrimination reducing method are critical questions that we acknowledge are perhaps too nuanced and complicated to offer a single blanket approach. The research literature provides inconsistent guidance on which methods to choose. They generally evaluate each method on two dimensions: accuracy vs. discrimination. Moreover, empirical comparisons are difficult to make because different researchers use different measures of discrimination depending on the study.\n\nAdditionally, while maximizing accuracy given a legal threshold of discrimination is an appropriate objective for research, many data scientists and firms do not operate with such simplistic goals. Hand laments that typical research comparing classifier performance fails to\n\nconsider the complete set of constraints or objectives that firms consider when evaluating options28.\n\nAs practicing data scientists ourselves, we can testify that firms tend to prioritize ease of implementation, run time scalability, and low risk of failure as high as model performance. Data science managers often advocate for using straightforward modeling techniques and focus on execution speed29. This is in spite of the fact that, as can be seen in the empirical studies we cite11,16,26, differences between the highest performing state of the art and previously established methods are often within a few percentage points of each other.\n\nWith all methods performing within a narrow competitive range, we would suggest that data scientists should give preference to those that can be easily adapted to existing discrimination measurement processes or that can still leverage commonly used open source training libraries (such as pre & post-processing).\n\nDiscrimination awareness starts with the awareness level of the data scientist herself, and learning the taxonomy of causes as laid out in this paper is real step closer to maintaining that awareness in all aspects of the data mining process. From here, the data scientist should seek out and possibly provoke or even lead the discussion with legal, ethical and management experts to translate the governing standards into an appropriate quantitative and qualitative definition of discrimination. From there it is a matter of execution, and we have offered a process to guide the data scientist, as well as a brief survey of methods as a starting point to consider what is right for the firm and the application.\n\n[Section: 5. Case Studies]\n\nIn lieu of pure empirical results (which would normally occupy this section of an article), we offer a few case studies to highlight both the highly dependent nature between problem specifics and optimal design, as well as how the general auditing framework we presented above can be applied to different problems . While there are numerous predictive modeling applications to10 choose from for case studies, we chose those that tend to have high social and human costs while being fairly opaque.",
      "referring_paragraphs": [
        "Figure 3\n\nCaption {We update standard a classification process diagram to reflect key testing and decision points that can inject discrimination awareness into the workflow."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    }
  ],
  "1907.12059": [
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig0.jpg",
      "image_filename": "1907.12059_page0_fig0.jpg",
      "caption": "",
      "context_before": "Proof. In the one-dimensional case of $p _ { S _ { 1 } }$ and $p _ { S _ { 2 } }$ , the total transportation cost $\\mathcal { W } _ { 1 } ( p _ { S _ { 1 } } , p _ { S _ { 2 } } )$ can be written as\n\nwhere $P _ { S _ { 1 } }$ and $P _ { S _ { 2 } }$ are the cumulative distribution functions of $S _ { 1 }$ and $S _ { 2 }$ respectively. This prove that (i) equals (ii).\n\nThe expected class prediction changes due to applying the transportation map $T$ is given by\n\nThis prove that (i) equals (iii).",
      "context_after": "Remark 1. Notice that $\\begin{array} { r } { ( i i ) = \\mathbb { E } _ { \\tau \\sim U ( \\Omega ) } | \\mathbb { P } ( S _ { 1 } > \\tau ) - } \\end{array}$ $\\mathbb { P } ( S _ { 2 } > \\tau ) | = 0$ if and only if $p _ { S _ { 1 } } = p _ { S _ { 2 } }$ . Indeed, by Proposition 1 and the property of the $\\mathcal { W } _ { 1 }$ metric, $( i i ) = 0$ $\\iff \\mathcal { W } _ { 1 } ( p _ { S _ { 1 } } , p _ { S _ { 2 } } ) = 0 \\iff p _ { S _ { 1 } } = p _ { S _ { 2 } } .$\n\nTo reach SDP, we need to achieve $p _ { S _ { a } } = p ^ { * } \\forall a \\in { \\mathcal { A } }$ , where $p ^ { * } \\in \\mathcal { P } ( \\Omega )$ , the space of pdfs on $\\Omega$ . We would like to choose transportation maps $T$ and a target distribution $p ^ { * }$ such that the transportation process from $p _ { S _ { a } }$ to $p ^ { * }$ incurs minimal total expected class prediction changes. Assume that the groups are all disjoint, so that the per-group transportation maps $T$ are independent from each other. Let $\\mathbb { T } ( p ^ { * } )$ be the set of transportation maps with elements $T$ such that, restricted to group $^ { a }$ , $T$ is a transportation map from $p _ { S _ { a } }$ to $p ^ { * }$ (i.e. $\\mathbb { T } ( p ^ { * } ) = \\{ T \\in \\mathcal { T } ( p _ { S } , p ^ { * } ) \\mid$ $T ( S ) { \\big \\vert } _ { A = a } = T _ { a } \\in { \\mathcal { T } } _ { a } = { \\mathcal { T } } ( p _ { S _ { a } } , p ^ { * } ) \\}$ where $\\boldsymbol { \\mathcal { T } } ( \\boldsymbol { p } _ { \\boldsymbol { S } } , \\boldsymbol { p } ^ { * } )$ denotes the space of transportation maps from $p _ { S }$ to $p ^ { * }$ ). We would like to obtain\n\nTherefore we are interested in\n\nwhich coincides with the Wasserstein-1 barycenter with normalized subgroup size as weight to every group distribution $p _ { S _ { a } }$ (Agueh and Carlier, 2011).\n\nIn summary, we have demonstrated that the optimal postprocessing procedure that minimizes total expected model prediction changes is to use the Wasserstein-1 optimal transport map $T ^ { * }$ to transport all group distributions $p _ { S _ { a } }$ to their weighted barycenter distribution $p _ { \\bar { S } }$ .\n\nOptimal Trade-Offs. We have shown that postprocessing the beliefs of a model through optimal transportation achieves SDP (and therefore $\\mathrm { S P D D } = 0$ ) whilst minimizing expected prediction changes. We now examine the case in which, after transportation, SDP is not attained, i.e. SPDD is positive. By triangle inequality",
      "referring_paragraphs": [],
      "figure_type": "diagram",
      "sub_figures": [
        "1907.12059_page0_fig1.jpg",
        "1907.12059_page0_fig2.jpg",
        "1907.12059_page0_fig3.jpg",
        "1907.12059_page0_fig4.jpg",
        "1907.12059_page0_fig5.jpg",
        "1907.12059_page0_fig6.jpg"
      ],
      "quality_score": 0.4,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig6.jpg"
        ],
        "panel_count": 7
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_8",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg",
      "image_filename": "1907.12059_page0_fig7.jpg",
      "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ",
      "context_before": "",
      "context_after": "[Section: 6 CONCLUSIONS]\n\nWe introduced an approach to ensure that the output of a classification system does not depend on sensitive information using the Wasserstein-1 distance. We demon-\n\nstrated that using the Wasserstein-1 barycenter enables us to reach independence with minimal modifications of the model decisions. We introduced two methods with different desirable properties, a Wasserstein-1 constrained method that does not necessarily require access to sensitive information at deployment time, and an alternative fast and practical approximation method that requires knowledge of sensitive information at test time. We showed that these methods outperform previous approaches in the literature.\n\n[Section: Acknowledgements]",
      "referring_paragraphs": [
        "(2016)) using a linear model as the underlying predictor and equal positive prediction rate between each group $\\mathcal { D } _ { a }$ and $\\mathcal { D }$\n\nTable 1: Adult Dataset – German Credit Dataset   \n\n<table><tr><td rowspan=\"2\"></td><td colspan=\"5\">Adult</td><td colspan=\"5\">German</td></tr><tr><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td><td>Err-.5</td><td>Err-Exp</td><td>DD-.5</td><td>SDD</td><td>SPDD</td></tr><tr><td>Unconstrained</td><td>.142</td><td>.198</td><td>.413</td><td>.426</td><td>.806</td><td>.248</td><td>.319</td><td>.124</td><td>.102</td><td>.103</td></tr><tr><td>Hardt&#x27;s Post-Process</td><td>.165</td><td>.289</td><td>.327</td><td>.551</td><td>1.058</td><td>.248</td><td>.333</td><td>.056</td><td>.045</td><td>.045</td></tr><tr><td>Constrained Opt.</td><td>.205</td><td>.198</td><td>.065</td><td>.087</td><td>.166</td><td>.318</td><td>.320</td><td>.173</td><td>.149</td><td>.149</td></tr><tr><td>Adv.",
        "Figure 1 shows overlaying model belief histograms for four demographic groups and their barycenter in the Adult dataset.",
        "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1907.12059_page0_fig8.jpg",
        "1907.12059_page0_fig9.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig9.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1907.12059",
      "figure_id": "1907.12059_fig_11",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg",
      "image_filename": "1907.12059_page0_fig10.jpg",
      "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).",
      "context_before": "Recall that we assume $\\forall a \\in { \\mathcal { A } }$ ,\n\nBy concentration of measure of Bernoulli random variables, with probability $\\geq 1 - \\frac { \\delta } { 2 }$ the following inequality holds simultaneously for all $\\mathbf { \\pmb { a } } \\in \\mathcal { A }$ :\n\nConsequently the desired result holds:",
      "context_after": "[Section: C Inverse CDFs]\n\nLemma 6. Given two differentiable and invertible cumulative distribution functions $f , g$ over the probability space $\\Omega = [ 0 , 1 ]$ , thus $f , g : [ 0 , 1 ] \\to [ 0 , 1 ]$ , we have\n\nIntuitively, we see that the left and right side of Eq. (12) correspond to two ways of computing the same shaded area in Figure 3. Here is a complete proof.\n\nProof. Invertible CDFs $f , g$ are strictly increasing functions due to being bijective and non-decreasing. Furthermore, we have $f ( 0 ) = 0 , f ( 1 ) = 1$ by definition of CDFs and $\\Omega = [ 0 , 1 ]$ , since $P ( X \\leq 0 ) = 0 , P ( X \\leq 1 ) = 1$ where $X$ is the corresponding random variable. The same holds for the function $g$ . Given an interval $( x _ { 1 } , x _ { 2 } ) \\subset [ 0 , 1 ]$ , let $y _ { 1 } = f ( x _ { 1 } ) , y _ { 2 } = f ( x _ { 2 } )$ . Since $f$ is differentiable, we have",
      "referring_paragraphs": [
        "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).\n\nIf $p _ { \\bar { S } }$ equals the weighted barycenter of the population level distributions $\\{ p _ { S _ { a } } \\}$ , then",
        "(12) correspond to two ways of computing the same shaded area in Figure 3."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(a) Left side of Eq. (12)",
        "(b) Right side of Eq. (12)",
        "1907.12059_page0_fig11.jpg",
        "1907.12059_page0_fig12.jpg"
      ],
      "quality_score": 0.95,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig12.jpg"
        ],
        "panel_count": 3
      }
    }
  ],
  "1909.05088": [
    {
      "doc_id": "1909.05088",
      "figure_id": "1909.05088_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg",
      "image_filename": "1909.05088_page0_fig0.jpg",
      "caption": "Figure 1: Percentage of female and male speakers per age group",
      "context_before": "One of the main obstacles for more personalized MT systems is finding large enough annotated parallel datasets with speaker information. Rabinovich et al. (2017) published an annotated parallel dataset for EN–FR and EN–DE. However, for many other language pairs no sufficiently large annotated datasets are available.\n\nTo address the aforementioned problem, we published online a collection of parallel corpora licensed under the Creative Commons Attribution 4.0 International License for 20 language pairs (Vanmassenhove and Hardmeier, 2018).2\n\nWe followed the approach described by Rabinovich et al. (2017) and tagged parallel sentences from Europarl (Koehn, 2005) with speaker information (name, gender, age, date of birth, euroID and date of the session) by retrieving speaker information provided by tags in the Europarl source files. The Europarl source files contain information about the speaker on the paragraph level and the filenames contain the data of the session. By retrieving the names of the speakers together with meta-information on the members of the European Parliament (MEPs) released by Rabinovich et al. (2017) (which includes among others name, country, date of birth and gender predictions per MEP), we were able to retrieve demographic annotations (gender, age, etc.). An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1.\n\n[Section: 3.1 Analysis of the EN–FR Annotated Dataset]\n\nWe first analysed the distribution of male and female sentence in our data. In the 10 different\n\nTable 1: Overview of annotated parallel sentences per language pair   \n\n<table><tr><td>Languages</td><td># sents</td><td>Languages</td><td># sents</td></tr><tr><td>EN-BG</td><td>306,380</td><td>EN-IT</td><td>1,297,635</td></tr><tr><td>EN-CS</td><td>491,848</td><td>EN-LT</td><td>481,570</td></tr><tr><td>EN-DA</td><td>1,421,197</td><td>EN-LV</td><td>487,287</td></tr><tr><td>EN-DE</td><td>1,296,843</td><td>EN-NL</td><td>1,419,359</td></tr><tr><td>EN-EL</td><td>921,540</td><td>EN-PL</td><td>478,008</td></tr><tr><td>EN-ES</td><td>1,419,507</td><td>EN-PT</td><td>1,426,043</td></tr><tr><td>EN-ET</td><td>494,645</td><td>EN-RO</td><td>303,396</td></tr><tr><td>EN-FI</td><td>1,393,572</td><td>EN-SK</td><td>488,351</td></tr><tr><td>EN-FR</td><td>1,440,620</td><td>EN-SL</td><td>479,313</td></tr><tr><td>EN-HU</td><td>251,833</td><td>EN-SV</td><td>1,349,472</td></tr></table>\n\ndatasets we experimented with, the percentage of sentences uttered by female speakers is very similar, ranging between $32 \\%$ and $33 \\%$ . This similarity can be explained by the fact that Europarl is largely a multilingual corpus with a big overlap between the different language pairs.\n\nWe conducted a more focused analysis on one of the subcorpora (EN–FR) with respect to the percentage of sentences uttered by males/females for various age groups to obtain a better grasp of what kind of data we are using for training. As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 . 7 1 \\% )$ , more male data is available in all age groups. Furthermore, when looking at the entire dataset, $6 7 . 3 9 \\%$ of the sentences are produced by male speakers. Moreover, almost half of the total number of sentences are uttered by the 50–60 age group $( 4 3 . 7 6 \\% )$ .",
      "context_after": "[Section: 4 Experimental Setup]\n\n[Section: 4.1 Datasets]\n\nWe carried out a set of experiments on 10 language pairs (the ones for which we compiled more than 500k annotated Europarl parallel sentences): EN–DE, EN–FR, EN–ES, EN–EL, EN–PT, EN– FI, EN–IT, EN–SV, EN–NL and EN–DA. We augmented every sentence with a tag on the English source side, identifying the gender of the speaker, as illustrated in (1). This approach for encoding sentence-specific information for NMT has been successfully exploited to tackle other types of issues, multilingual NMT systems (e.g., Zero Shot Translation (Johnson et al., 2017)), domain adaptation (Sennrich et al., 2016), etc.",
      "referring_paragraphs": [
        "An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1.",
        "As can be seen from Figure 1, with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences $( 0 .",
        "Figure 1: Percentage of female and male speakers per age group\n\nThe analysis shows that indeed, there is a gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    }
  ],
  "1910.10872": [
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg",
      "image_filename": "1910.10872_page0_fig0.jpg",
      "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
      "context_before": "[Section: Introduction]\n\nMachine learning and AI systems are becoming omnipresent in everyday lives. Recently, attention has been directed to problems concerning fairness and algorithmic bias. Some progress has been made on the analysis of gender stereotyping in different natural language processing (NLP) components, such as word embedding (Bolukbasi et al. 2016; Zhao et al. 2018b), co-reference resolution (Zhao et al. 2018a), machine translation (Font and Costa-jussa 2019) ` and sentence encoders (May et al. 2019). In this work we study bias in named entity recognition (NER) systems and show how they can propagate gender bias by analyzing the 139-year history of U.S. male and female names from census data.\n\nOur experiments show that widely used named entity recognition systems are susceptible to gender bias. We find that relatively more female names were tagged as non-PERSON than male names even though the names were used in a context where they should have been marked as PERSON. An example is “Charlotte,” ranked as the top 8th most popular female U.S. baby name in 2018. “Charlotte” is almost always tagged wrongfully as a location by the stateof-the-art NER systems despite being used in a context when it is clear that the entity should be a person. Figure 1 has more examples with names that are either not recognized as an entity or wrongfully tagged. We show that there are many\n\n[Section: Named Entity Recognition:]",
      "context_after": "[Section: Models and Experiments]\n\nTo measure the existence of bias in NER systems, we evaluated five named entity models used in research and industry. We used Flair (Akbik, Blythe, and Vollgraf 2018; Akbik et al. 2019), CoreNLP version 3.9 (Manning et al. 2014; Finkel, Grenager, and Manning 2005), and Spacy version 2.1 with small, medium, and large models.2 We test these models against 139 years of U.S. census data3 from years 1880 to 2018. Our benchmark evaluates these models based upon how well they recognize these names as a PERSON entity.\n\n[Section: Benchmark]",
      "referring_paragraphs": [
        "Figure 1 has more examples with names that are either not recognized as an entity or wrongfully tagged.",
        "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.",
        "Our benchmark dataset consists of nine templates listed in Table 1 which are templated sentences that start with the existing names in the census data followed by a sentence that represents a human-like activity."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) Error Type-1 Weightedfemale male",
        "1910.10872_page0_fig1.jpg",
        "(b) Error Type-2 Weighted",
        "1910.10872_page0_fig2.jpg",
        "(c) Error Type-3 Weightedfemale male",
        "1910.10872_page0_fig3.jpg",
        "1910.10872_page0_fig4.jpg",
        "1910.10872_page0_fig5.jpg",
        "1910.10872_page0_fig6.jpg",
        "1910.10872_page0_fig7.jpg",
        "1910.10872_page0_fig8.jpg",
        "1910.10872_page0_fig9.jpg",
        "1910.10872_page0_fig10.jpg",
        "1910.10872_page0_fig11.jpg",
        "1910.10872_page0_fig12.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig7.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig11.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig12.jpg"
        ],
        "panel_count": 13
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_14",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg",
      "image_filename": "1910.10872_page0_fig13.jpg",
      "caption": "Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.",
      "context_before": "",
      "context_after": "0 0Figure 2: Results from different models that spanned the 139-year history of baby names from the census data on different error 1875 1895 1915 1935 1955 1975 1995 2015types for the weighted cases using template #4. Female names have higher error rates for all the cases. The y axis shows the 0.09 Flair 0.14 calculated error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year 0.070.08 female malein which the baby name was given.\n\n0.03Different types of errors allow for fine-grained analysis into 0.02the existence of different biases. Our results indicate that all models are mostly more biased toward female names vs. 1875 1895 1915 1935 1955 1975 1995 2015male names, as shown in Figures 2 and 3 over the 139-year history. The fact that all the weighted cases are biased toward female names shows that more frequent and popular female names are susceptible to bias and error in named entity recognition systems—which is a more serious type of error to consider. For space considerations, we only report the results for one of the templates (Template #4) since the results were following similar trend for all the other templates wherein the models were mostly more biased toward female\n\n0.10.04names. We have included results from other templates in 0.080.02our supplementary material to demonstrate that other tem-0.060 1875 1895 1915 1935 1955 1975 1995 2015plates also follow a similar pattern. That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend. This result shows how context helps some models over others by bringing down the error rates when sentences are added to the names (templates #2 through #9). Other models perform better on template #1, showing that context, in fact confuses the model. We observe that in contextual-based models such as Flair, context indeed helps the model make the right de-",
      "referring_paragraphs": [
        "We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.",
        "<table><tr><td>Female Name</td><td>Frequency</td><td>Error Type</td></tr><tr><td>Charlotte</td><td>12,940</td><td>Tagged as LOC</td></tr><tr><td>Sofia</td><td>7,621</td><td>Tagged as LOC</td></tr><tr><td>Victoria</td><td>7,089</td><td>Tagged as LOC</td></tr><tr><td>Madison</td><td>7,036</td><td>Tagged as LOC</td></tr><tr><td>Aurora</td><td>4,785</td><td>Tagged as LOC</td></tr></table>\n\nTable 2: Top 5 mistagged examples from the Flair model on Template #4 of female and male names from our benchmark."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "1910.10872_page0_fig14.jpg",
        "1910.10872_page0_fig15.jpg",
        "(a) Error Type-1 Unweighted",
        "1910.10872_page0_fig16.jpg",
        "(b) Error Type-2 Unweightedfemale male",
        "1910.10872_page0_fig17.jpg",
        "(c) Error Type-3 Unweightedfemale male",
        "1910.10872_page0_fig18.jpg",
        "1910.10872_page0_fig19.jpg",
        "1910.10872_page0_fig20.jpg",
        "1910.10872_page0_fig21.jpg",
        "1910.10872_page0_fig22.jpg",
        "1910.10872_page0_fig23.jpg",
        "1910.10872_page0_fig24.jpg",
        "1910.10872_page0_fig25.jpg",
        "1910.10872_page0_fig26.jpg",
        "1910.10872_page0_fig27.jpg",
        "1910.10872_page0_fig28.jpg",
        "1910.10872_page0_fig29.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig18.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig21.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig24.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig25.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig26.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig27.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig28.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig29.jpg"
        ],
        "panel_count": 17
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_31",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg",
      "image_filename": "1910.10872_page0_fig30.jpg",
      "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.",
      "context_before": "",
      "context_after": "CoreNLP   \n0.040 0Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.\n\n0.03cisions by it having less error rates for templates #2 through 0.010.02#9 compared to template #1. Other models do not necessar-0ily follow this pattern. As an example, we provide the types of names and errors that can happen in these models. We list the top six most frequent male and female names which were tagged erroneously by the Flair model in year 2018 from our 0.72 Small Spacybenchmark evaluated on template #4 in Table 2.\n\n[Section: 6668Model Version Evaluation and Comparison]\n\n0.64Updates to models often lead to superior performance; however, this may come with a detriment to fairness and bias. In 0.58this section we want to see how much the version updates in\n\nmodels will be robust toward fairness constraints and errors defined in the previous section. Thus, we will first define this source of bias. Then we will show the results for the models from the previous section that have various versions.\n\nDefinition (Version Bias). This is a type of bias that arises from updates in the systems.\n\nIn order to report the results of our analysis, we used four of our models mentioned in the previous section that have version updates—namely small, medium, and large models from Spacy (versions 2.0 and 2.1) and versions 3.8 and 3.9 from CoreNLP. We then repeated the experiments from the previous section to report the results for Error Type-1,",
      "referring_paragraphs": [
        "As examples, some of the changes from version 3.8 to version 3.9 of the CoreNLP model are shown in Table 3.",
        "2017), machine translation (Font and Costa-jussa 2019), language models (Bordia and Bowman `\n\nTable 3: Some examples on how tagging changed during version update of the CoreNLP model."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1910.10872_page0_fig31.jpg",
        "1910.10872_page0_fig32.jpg",
        "1910.10872_page0_fig33.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig31.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig32.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig33.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_35",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg",
      "image_filename": "1910.10872_page0_fig34.jpg",
      "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.",
      "context_before": "",
      "context_after": "[Section: Bias in Data]\n\nSince data plays an important role in the outcome of the model and can directly affect the fairness constraints if it contains any biases, we decided to analyze some of the datasets that are widely used in the training of NER models to determine whether they show any biases toward a specific group that could result in the biased behavior observed in those results discussed in previous sections. We used the train, test, and development sets from two widely known CoNLL- $2 0 0 3 ^ { 4 }$ (Sang and De Meulder 2003) and",
      "referring_paragraphs": [
        "That being said, in Figure 4 we showed how all the models perform on all the tem-0plates, for Error Type-1 Weighted (the super-set error) case, 1875 1895 1915 1935 1955 1975 1995 2015which on its own is showing an interesting trend.",
        "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years.",
        "Our results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that.",
        "We also analyzed some datasets widely used in current state-of-the-art models and showed the existence of bias in these datasets as well which\n\nTable 4: Percentage of female and male names from the census data appearing in CoNLL 2003 and OntoNotes datasets with their corresponding counts."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1910.10872_page0_fig35.jpg",
        "1910.10872_page0_fig36.jpg",
        "(a) Error Type-2 Weighted",
        "1910.10872_page0_fig37.jpg",
        "1910.10872_page0_fig38.jpg",
        "(b) Error Type-3 WeightedVersion 3.9",
        "1910.10872_page0_fig39.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig35.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig36.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig37.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig38.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig39.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_41",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg",
      "image_filename": "1910.10872_page0_fig40.jpg",
      "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy",
      "context_before": "",
      "context_after": "0.05Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy\n\n0.3 Ve0.06OntoNotes- $. 5 ^ { 5 }$ 3.9 0.080.1(Weischedel et al. 2012) datasets which were 0.250.04 0.06used in the training and testing of Flair, Spacy, and many 0.20.02 0.02other models. The split of the OntoNotes-5 dataset into 0.150 0train, development, and test sets was performed according to 0.1          (Pradhan et al. 2013). We reported the percentages of male 0.05vs. female names from the census data that appeared in train, 0test, and development sets in each of the datasets and com-Male CoreNLPpared this to the percentages of male vs. female names in 0.18 Version 3.8Version 3.9reality from the census data to see how much these datasets 0.16are reflective of the reality or if they pertain to any bias to-0.12ward a specific gender group.\n\nOur results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that. Unlike the census data, which is represen-0tative of real-world statistics, wherein female names have more versatility— $62 \\%$ unique names vs. $38 \\%$ unique male names—datasets used in training the NER models contain $42 \\%$ female names vs. $58 \\%$ male names from the census data. Not only do the datasets not contain more versatile female names to reflect the reality, but instead have even less variety which can itself bias the models by not covering enough female names. Similar patterns are observable in test and development sets of datasets used in the NER systems.\n\n[Section: Related Work]\n\nWe have seen large amounts of attention and work regarding fairness in machine learning and natural language processing models and methods. Recent papers observed a type of stereotyping bias in word embedding methods and tried to mitigate this type of bias by proposing a method that respects the embeddings for gender-specific words but de-",
      "referring_paragraphs": [
        "Similarly, for CoreNLP more entities tried to be tagged—which resulted in a slight improvement in Error Type-3, as shown in Figure 5."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(c) Error Type-1 WeightedVersion 3.8Version 3.9",
        "1910.10872_page0_fig41.jpg",
        "1910.10872_page0_fig42.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig41.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig42.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_44",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg",
      "image_filename": "1910.10872_page0_fig43.jpg",
      "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.",
      "context_before": "0.05Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy\n\n0.3 Ve0.06OntoNotes- $. 5 ^ { 5 }$ 3.9 0.080.1(Weischedel et al. 2012) datasets which were 0.250.04 0.06used in the training and testing of Flair, Spacy, and many 0.20.02 0.02other models. The split of the OntoNotes-5 dataset into 0.150 0train, development, and test sets was performed according to 0.1          (Pradhan et al. 2013). We reported the percentages of male 0.05vs. female names from the census data that appeared in train, 0test, and development sets in each of the datasets and com-Male CoreNLPpared this to the percentages of male vs. female names in 0.18 Version 3.8Version 3.9reality from the census data to see how much these datasets 0.16are reflective of the reality or if they pertain to any bias to-0.12ward a specific gender group.\n\nOur results shown in Table 4 indicate that the datasets 0.06used do not reflect the real world, but rather exactly the op-0.020.04posite of that. Unlike the census data, which is represen-0tative of real-world statistics, wherein female names have more versatility— $62 \\%$ unique names vs. $38 \\%$ unique male names—datasets used in training the NER models contain $42 \\%$ female names vs. $58 \\%$ male names from the census data. Not only do the datasets not contain more versatile female names to reflect the reality, but instead have even less variety which can itself bias the models by not covering enough female names. Similar patterns are observable in test and development sets of datasets used in the NER systems.\n\n[Section: Related Work]\n\nWe have seen large amounts of attention and work regarding fairness in machine learning and natural language processing models and methods. Recent papers observed a type of stereotyping bias in word embedding methods and tried to mitigate this type of bias by proposing a method that respects the embeddings for gender-specific words but de-",
      "context_after": "0.1Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.\n\nbiases embeddings for gender-neutral words (Bolukbasi et al. 2016), or by generating a gender-neutral version of Glove (called GN-Glove) that aims to preserve gender information in some directions of word vectors, while setting other dimensions free from gender influence (Zhao et al. 2018b) or other data augmentation techniques (Brunet et al. 2019; Zhao et al. 2019). Other work tried to show and address bias in co-reference resolution (Zhao et al. 2018a), semantic role labeling (Zhao et al. 2017), machine translation (Font and Costa-jussa 2019), language models (Bordia and Bowman `\n\nTable 3: Some examples on how tagging changed during version update of the CoreNLP model. Note how the original problem of tagging PERSON entities correctly has not been addressed.   \n\n<table><tr><td>Female Name</td><td>CoreNLP version 3.8</td><td>CoreNLP version 3.9</td></tr><tr><td>Isabel</td><td>Not Tagged</td><td>Tagged as MISC</td></tr><tr><td>Angelina</td><td>Not Tagged</td><td>Tagged as MISC</td></tr><tr><td>June</td><td>Not Tagged</td><td>Tagged as DATE</td></tr><tr><td>Charlotte</td><td>Tagged as LOCATION</td><td>Tagged as CITY</td></tr><tr><td>Victoria</td><td>Tagged as LOCATION</td><td>Tagged as CITY</td></tr><tr><td>Sydney</td><td>Tagged as LOCATION</td><td>Tagged as CITY</td></tr><tr><td colspan=\"3\"></td></tr><tr><td>Male Name</td><td>CoreNLP version 3.8</td><td>CoreNLP version 3.9</td></tr><tr><td>Christian</td><td>Not Tagged</td><td>Tagged as RELIGION</td></tr><tr><td>Logan</td><td>Tagged as LOCATION</td><td>Tagged as CITY</td></tr><tr><td>Roman</td><td>Not Tagged</td><td>Tagged as MISC</td></tr><tr><td>Santiago</td><td>Tagged as LOCATION</td><td>Tagged as CITY</td></tr><tr><td>Jordan</td><td>Tagged as LOCATION</td><td>Tagged as CITY</td></tr><tr><td>Messiah</td><td>Not Tagged</td><td>Tagged as TITLE</td></tr></table>\n\n2019), and sentence embedding (May et al. 2019).\n\nAddressing fairness and bias, not only in NLP but also in general machine learning, has lately gained much attention. In Mehrabi et al. (2019b), the authors created a taxonomy on fairness and bias that discusses how researchers have addressed fairness related issues in different fields. From representation learning (Moyer et al. 2018) to graph embedding (Bose and Hamilton 2019) to community detection (Mehrabi et al. 2019a) and clustering (Backurs et al. 2019), researchers have studied biases in these areas and tried to address them by pointing out the observed problems and proposing new directions and ideas. In Buolamwini and Gebru (2018) authors show and analyze the existing gender bias in facial recognition systems, such as those used by IBM, Microsoft, and ${ \\mathrm { F a c e } } + +$ , and created a benchmark for better evaluation of bias in facial recognition systems. This is considered a significant contribution as it opens many future research questions and related papers. Paying attention to different AI applications and pointing out their issues in terms of fairness is an important issue that needs serious attention for significant future improvements to these systems.\n\n[Section: Conclusion and Future Work]\n\nIn this work we not only performed a historical analysis of named entity recognition systems and showed the existence of bias, but we also introduced a benchmark that can help future models evaluate the extent of gender bias in their systems. We then performed a cross version analysis of models and showed that model updates can sometimes amplify the existing bias in previous versions. We also analyzed some datasets widely used in current state-of-the-art models and showed the existence of bias in these datasets as well which\n\nTable 4: Percentage of female and male names from the census data appearing in CoNLL 2003 and OntoNotes datasets with their corresponding counts. Both datasets fail to reflect the variety of female names.   \n\n<table><tr><td></td><td>Dataset</td><td>Female Count</td><td>Male Count</td><td>Female Pct</td><td>Male Pct</td></tr><tr><td></td><td>Census</td><td>67,698</td><td>41,475</td><td>62%</td><td>38%</td></tr><tr><td rowspan=\"2\">Train</td><td>CoNLL 2003</td><td>1,810</td><td>2,506</td><td>42%</td><td>58%</td></tr><tr><td>OntoNotes5</td><td>2,758</td><td>3,832</td><td>42%</td><td>58%</td></tr><tr><td rowspan=\"2\">Dev</td><td>CoNLL 2003</td><td>962</td><td>1,311</td><td>42%</td><td>58%</td></tr><tr><td>OntoNotes5</td><td>1,159</td><td>1,524</td><td>43%</td><td>57%</td></tr><tr><td rowspan=\"2\">Test</td><td>CoNLL 2003</td><td>879</td><td>1,228</td><td>42%</td><td>58%</td></tr><tr><td>OntoNotes5</td><td>828</td><td>1,068</td><td>44%</td><td>56%</td></tr></table>\n\ncan directly affect the biased performance of models. Named entity recognition systems are extensively used in different downstream tasks and having biased NER systems can have implications beyond just the NER task. We believe that using our benchmark for evaluation of future named entity recognition systems can help mitigate the gender bias issue in these applications.\n\nThis work identifies an important problem with the current state-of-the-art in named entity recognition. Nevertheless, this measure is a glimpse into the many possible biases that NER may contain, and there are some key limitations that we plan to address in future work. First, the nine templates used to test the models are not necessarily representative of real-world text. There is a limitless supply of sentences that could be fed to the model. Moving forward, we seek to generate a sentence corpora that is based on realworld text. Second, our approach is based upon names taken from United States census data. This work can be extended to different languages to demonstrate the biases they pertain.\n\nThrough our analysis, we provide some suggestions for avoiding gender bias in NER systems, as listed below:\n\n1. Using benchmarks designed for evaluation of bias in future named entity recognition systems can help mitigate the gender bias issue in these applications.   \n2. Using contextual-based models as shown in our results can help reduce the error rates.   \n3. We encourage introduction of new models to overcome the observed gender bias in these systems. As future work, these benchmarks can be used as loss functions to train the NER system.   \n4. A collection of new datasets reflecting reality, such as following name distributions observed in the real-world, would be helpful to the community to build models that avoid these biases. As a step towards this, we will provide our code and data upon acceptance.",
      "referring_paragraphs": [
        "For in-TMPL6 TMPstance, as shown in Figure 6 for the Spacy medium model, the newer version is more erroneous when it comes to the Error Type-1 Weighted case which is a superset of all the error types discussed in this paper."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "1910.10872_page0_fig44.jpg",
        "(a) Error Type-1 Weightedfemale male",
        "1910.10872_page0_fig45.jpg",
        "1910.10872_page0_fig46.jpg",
        "1910.10872_page0_fig47.jpg",
        "1910.10872_page0_fig48.jpg",
        "1910.10872_page0_fig49.jpg",
        "(b) Error Type-2 Weighted",
        "1910.10872_page0_fig50.jpg",
        "1910.10872_page0_fig51.jpg",
        "1910.10872_page0_fig52.jpg",
        "1910.10872_page0_fig53.jpg",
        "1910.10872_page0_fig54.jpg",
        "(c) Error Type-3 Weightedfemale male",
        "1910.10872_page0_fig55.jpg",
        "1910.10872_page0_fig56.jpg",
        "1910.10872_page0_fig57.jpg",
        "1910.10872_page0_fig58.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig44.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig45.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig46.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig47.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig48.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig49.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig50.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig51.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig52.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig53.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig54.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig55.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig56.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig57.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig58.jpg"
        ],
        "panel_count": 16
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_60",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg",
      "image_filename": "1910.10872_page0_fig59.jpg",
      "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.",
      "context_before": "",
      "context_after": "CoreNLP   \n0 0Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.",
      "referring_paragraphs": [],
      "figure_type": "architecture",
      "sub_figures": [
        "1910.10872_page0_fig60.jpg",
        "(a) Error Type-1 Unweighted",
        "1910.10872_page0_fig61.jpg",
        "1910.10872_page0_fig62.jpg",
        "1910.10872_page0_fig63.jpg",
        "1910.10872_page0_fig64.jpg",
        "1910.10872_page0_fig65.jpg",
        "1910.10872_page0_fig66.jpg",
        "1910.10872_page0_fig67.jpg",
        "1910.10872_page0_fig68.jpg",
        "1910.10872_page0_fig69.jpg",
        "1910.10872_page0_fig70.jpg",
        "1910.10872_page0_fig71.jpg",
        "1910.10872_page0_fig72.jpg",
        "1910.10872_page0_fig73.jpg",
        "1910.10872_page0_fig74.jpg",
        "1910.10872_page0_fig75.jpg",
        "1910.10872_page0_fig76.jpg",
        "1910.10872_page0_fig77.jpg"
      ],
      "quality_score": 0.6,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig60.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig61.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig62.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig63.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig64.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig65.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig66.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig67.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig68.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig69.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig70.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig71.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig72.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig73.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig74.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig75.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig76.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig77.jpg"
        ],
        "panel_count": 19
      }
    },
    {
      "doc_id": "1910.10872",
      "figure_id": "1910.10872_fig_79",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg",
      "image_filename": "1910.10872_page0_fig78.jpg",
      "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
      "context_before": "",
      "context_after": "0.10 1875 1895 1915 1935 1955 1975 1995 2015 0 1875 1895 1915 1935 1955 1975 1995 2015Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.6,
      "metadata": {}
    }
  ],
  "2005.07293": [
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
      "image_filename": "2005.07293_page0_fig0.jpg",
      "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
      "context_before": "Machine learning systems have been shown to propagate the societal errors of the past. In light of this, a wealth of research focuses on designing solutions that are “fair.” Even with this abundance of work, there is no singular definition of fairness, mainly because fairness is subjective and context dependent. We propose a new fairness definition, motivated by the principle of equity, that considers existing biases in the data and attempts to make equitable decisions that account for these previous historical biases. We formalize our definition of fairness, and motivate it with its appropriate contexts. Next, we operationalize it for equitable classification. We perform multiple automatic and human evaluations to show the effectiveness of our definition and demonstrate its utility for aspects of fairness, such as the feedback loop.\n\n[Section: 1 Introduction]\n\nWith the omnipresent use of machine learning in different decision and policy making environments, fairness has gained significant importance. This became the case when researchers noticed that an AI system used to measure recidivism risk in bail decisions was biased against certain racial groups [Angwin et al., 2016]. As a reaction to the disclosure of this issue and various others, the AI community has made efforts to mitigate biased and unfair outcomes in decision making processes. Many researchers have proposed definitions of algorithmic fairness, while others have tried to use these definitions in different down-stream tasks in an effort to overcome unfair outcomes. Despite the abundance of fairness definitions, the majority of them are not complete [Gajane and Pechenizkiy, 2018]. Moreover,",
      "context_after": "[Section: 2 Defining Equity]\n\nEquity is the distribution of resources among groups to overcome obstacles and to raise their opportunities for access [Schement, 2001]. Thus, historically disadvantaged groups are compensated, and others get their fair share to reach their goals. To operationalize equity, we learn the existing biases from the historical data and compensate for them in the predictions generated by the model. This\n\ncan be viewed similarly to affirmative action in which present decisions (algorithmic or otherwise) are made to compensate for biases of the past but in such a way that groups reach an ultimate equality. The goal is to equalize all the groups in the long run and give each group their fair share.We modeled this phenomenon in our classification objective function which will be described in detail in this section.\n\nOur objective function consists of two terms:\n\n• The Fairness Objective: In which the goal is to enforce equity amongst groups.   \n• The Classification Objective: In which we enforce the classification objective to achieve predictive accuracy.\n\nFinally, we combine these two objectives and control the importance of each using a regularization parameter.\n\n[Section: 2.1 Operationalizing Equity]",
      "referring_paragraphs": [
        "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.",
        "that in Figure 1.",
        "We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values.",
        "We also show the significance of these results in Table 1.",
        "Through MannWhitney U significance\n\n<table><tr><td colspan=\"2\"></td><td colspan=\"2\">COMPAS Dataset</td><td colspan=\"2\">Adult Dataset</td></tr><tr><td colspan=\"2\"></td><td colspan=\"2\">p-value</td><td colspan=\"2\">p-value</td></tr><tr><td>Beta</td><td></td><td>Parity</td><td>Classifier</td><td>Parity</td><td>Classifier</td></tr><tr><td>0.1</td><td rowspan=\"9\">Equity</td><td>0.0003</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.2</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.3</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.4</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.5</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.6</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.7</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.8</td><td>0.0001</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr><tr><td>0.9</td><td>9.1e-05</td><td>3.2e-05</td><td>9.1e-05</td><td>3.2e-05</td></tr></table>\n\nTable 1: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets.",
        "• Scenario 1 (Equality vs Equity): We asked workers to rate pictures of equity and equality in Figure 1 and chose their preferred picture."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "2005.07293_page0_fig1.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg",
      "image_filename": "2005.07293_page0_fig2.jpg",
      "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.",
      "context_before": "",
      "context_after": "In other words, we measure how effective a method was in reducing disparities among demographics compared to a classifier with no fairness constraint. Note how this measure is similar to the statistical parity except considering both the history and future predictive outcome.\n\nFor robustness purposes, each of our experiments were performed on 10 random train, validation, and test splits for each of the datasets and the significance of our hypotheses were reported accordingly. The reported results are averages of 10 experiments performed on 10 different\n\nrandom splits. Due to the existing variance in the splits, we found the MannWhitney U tests more suitable and reliable in performing experiments. Thus, we reported the significance of the averaged results in terms of $p$ -value instead of standard deviation. However, the standard deviations and detailed averaged results are all listed in the appendixes section.\n\nHypothesis 1 The Equity classification objective will achieve the highest gain in fairness, at a slight cost to accuracy. We expect this fairness gain and accuracy degrade to be more noticeable for higher $\\beta$ values which controls the importance of fairness gain over classification accuracy.\n\n[Section: 3.2 COMPAS Dataset]\n\nThe COMPAS dataset contains information about defendants from Broward County. The labels in our prediction classification task were weather or not a criminal will re-offend within two years. The sensitive attribute in our experiments was gender. Among features in this dataset we used features as listed in Table 2. We split the dataset into 10 different random 80-10-10 splits for train, test, and validation sets. The averaged accuracy and fairness gain results obtained from applying different losses in our\n\nclassification task over 10 experiments on different splits with different $\\beta$ values on the COMPAS dataset is shown in Figure 2.\n\nFrom results shown in Figure 2, we can observe that classifier trained on our Equity loss is able to achieve higher fairness gain for all $\\beta$ values. We also show the significance of these results in terms of one vs all (Equity vs Parity and Classifier) MannWhitney U test in Table 1 for all the $\\beta$ values. Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid $\\beta$ values in this dataset. However, the degrade gets significant for higher $\\beta$ values as expected due to the control of $\\beta$ over accuracy-fairness trade-off. Although we reported the results for all the $\\beta$ values from 0.1 to extreme of 0.9, we recommend a $\\beta$ value around 0.3-0.5 which balances the fairness gain and test accuracy.",
      "referring_paragraphs": [
        "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values.",
        "The averaged accuracy and fairness gain results obtained from applying different losses in our\n\nclassification task over 10 experiments on different splits with different $\\beta$ values on the COMPAS dataset is shown in Figure 2.",
        "The averaged test accuracy and fairness gain results over 10 different splits for each $\\beta$ value obtained from applying different losses in our classification task on the Adult dataset is shown in Figure 2.",
        "Figure 2, demonstrates the behavior of different losses over different $\\beta$ values in terms of test accuracy and fairness gain for the COMPAS and Adult datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2005.07293_page0_fig3.jpg",
        "2005.07293_page0_fig4.jpg",
        "(a) β = 0.1",
        "2005.07293_page0_fig5.jpg",
        "(b) β = 0.5",
        "2005.07293_page0_fig6.jpg",
        "(c) β = 0.9",
        "2005.07293_page0_fig7.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig3.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig7.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_9",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg",
      "image_filename": "2005.07293_page0_fig8.jpg",
      "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.",
      "context_before": "",
      "context_after": "[Section: 5 Public Perception of Equity]\n\nIn order to understand the public’s perception of equity (via our proposed definition) and its comparison to equality in different real life scenarios, we conducted surveys on Amazon Mechanical Turk in the vein of [Saxena et al., 2019].\n\n[Section: 5.1 Experimental Setup]",
      "referring_paragraphs": [
        "Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy, the results in Table 3 show the insignificance of this degrade for low to mid $\\beta$ values in this dataset.",
        "Although from the results in Figure 2, one can observe a degrade in performance in terms of test accuracy and that results in Table 3 show the significance of this degrade, this degrade is still considered to be a reasonable price for fairness considering the gain in fairness.",
        "Table 3: One vs all (Equity loss vs Parity and Classifier losses) MannWhitney U test for COMPAS and Adult datasets.",
        "The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.5.",
        "Figure 3 reports $| p ( Y | A = { \\mathrm { f e m a l e } } ) - p ( Y | A = { \\mathrm { m a l e } } )$ |, averaged across 10 runs, as a measure for disparity for both predicted class labels $Y ~ = ~ 0$ and $Y ~ = ~ 1$ in each of the datasets for each of the fairness notions for each $\\beta$ value.",
        "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets.",
        "The obtained $p$ -values show the significance of our reported results in Figure 3 for $\\beta$ value of 0.1."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2005.07293_page0_fig9.jpg",
        "2005.07293_page0_fig10.jpg",
        "2005.07293_page0_fig11.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig11.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "2005.07293",
      "figure_id": "2005.07293_fig_13",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg",
      "image_filename": "2005.07293_page0_fig12.jpg",
      "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.",
      "context_before": "",
      "context_after": "PictureB\n\nPicture A (Picture on the left):\n\n0-Co pletely Unfai\n\n1-So hat Unfa\n\n2-Neither Fair Nor Unfair\n\n3-So what Fair\n\n4-Completely Fair\n\nPicture B (Picture on the right):\n\n0-Co letely Unfai\n\n1-So what Unfair\n\n2-Neither Fair Nor Unfair\n\n3-So what Fair\n\n4-C letely Fai\n\nWhich picture would you prefer the most?\n\nI would prefer Pictur\n\n(Picture on the left).\n\nwould prefer Picture B (Picture on the right).\n\nPlease tell us why you chose your preferred picture. (Must be filled in order for you to get paid)\n\n2. Scenario 2: ---- 3 students with identical qualifications apply for a student loan to cover their $10k (each) tuition for a semester. The school can only offer $21k in total loans. The circumstance of each student is as follows: Student A: Student A has previously received a $5k scholarship and applies for a loan to cover the rest of his/her $5k tuition. Student B: Student B has previously received a $4k scholarship and applies for a loan to cover the rest of his/her $6k tuition. Student C: Student C has received no scholarships and applies for the loan to cover her/his $10k tuition.\n\ndegree\n\nSolution 1: The school should allocate a $7k loan each to Student A, B, and C\n\n0-Completely Unfair\n\n1-Somewhat Unfair\n\n2-Neither Fair Nor Unfair\n\n3-Somewhat Fair\n\n4-Completely Fair\n\nSolution 2: The school should allocate a $5k loan to Student A, a $6k loan to Student B, and a $10k loan to Student C.\n\n0-Co letely Unfai\n\n2-Neither Fair Nor Unfair\n\n3-Som ewhat Fair\n\n4-Co pletely Fair\n\nWhich rio?\n\nould prefer solut\n\nI would olution 2. tion 2.\n\nPlease tell us why you chose\n\n3. Scenario 3: ---- The government is awarding 100 subsidized houses to families every year. In previous years 80 of these houses went to Caucasian families while only 20 went to people of color, although the applicant pool consisting equally of both groups. What would be the fair solution for government to take for this year's plan? Please rate the following solutions according to their fairness degree.\n\nSelution †- The gpvernment sheuld award 50 to peonle gf color and 50 tn tbe Caucasians\n\n0-Completely Unfair\n\n1-Somewhat Unfair\n\n2-Neither Fair N\n\n3-Somewhat Fair\n\n4-Completely Fair\n\nSolution 2: 80 houses should go to\n\n0-Completely Unfair\n\n1-Somewhat Unfair\n\n2-Neither Fair N\n\n3-Somewhat Fair\n\n4-Completely Fair\n\nWhich solution would you prefer the this scenario?\n\nI would prefer solut\n\nI would prefer solution 2.\n\nPlease tell us why you chose your preferred picture. (Must be filled in order for you to get paid)",
      "referring_paragraphs": [
        "Table 4: Performance of Mann-Whitney U test for showing the effectiveness of Equity in reducing bias in the feedback loop compared to Parity and Classifier losses over different iterations for COMPAS and Adult datasets.",
        "Table 4 shows the significance of these results for COM-PAS and Adult datasets for $\\beta$ value of 0.5 for different iterations supporting our hypothesis.2 This is consistent with our earlier finding that $\\beta = 0 .",
        "The statistics of ratings for each of the 4 scenarios is shown in Figure 4.",
        "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios."
      ],
      "figure_type": "other",
      "sub_figures": [
        "2005.07293_page0_fig13.jpg",
        "2005.07293_page0_fig14.jpg",
        "2005.07293_page0_fig15.jpg",
        "2005.07293_page0_fig16.jpg",
        "2005.07293_page0_fig17.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig15.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig17.jpg"
        ],
        "panel_count": 6
      }
    }
  ],
  "2103.11320": [
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_1",
      "figure_number": null,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig0.jpg",
      "image_filename": "2103.11320_page0_fig0.jpg",
      "caption": "",
      "context_before": "Selection of Target Groups Let $G$ be the graph of the CSKB that consists of commonsense assertions in the form of triples $( s , r , o )$ representing “subjectrelation-object”. Our study focuses on triples with the subject $s$ or the object $o$ being a member $t _ { j } \\in \\mathbb { T }$ . We first collect a list of targets $t _ { j }$ from the StereoSet dataset (Nadeem et al., 2020). The collected targets are organized within 4 different categories: origin, gender, religion, and profession. We renamed the “race” category from (Nadeem et al., 2020) to “origin” to be more precise as words such as British\n\nmay not necessarily represent a race but more of the origin or nationality of a person. Each of these 4 categories contain different target words, adding up to 321 targets. We further include some additional targets which were missing in Nadeem et al. (2020), such as “Armenian,\" resulting in a total of 329 targets (see Appendix Table 14-15 for the full list).\n\nCollection of CSKB Triples We collect all the triples from ConceptNet $5 . 7 ^ { 4 }$ (Speer et al., 2017) which contain the target words in each category, resulting in more than 100k triples. For GenericsKB, we use the GenericsKB-Best set (Bhakthavatsalam et al., 2020), which contains filtered, high-quality sentences and extract those that have one of our target words as their annotated topic of the sentence, resulting in around 30k statements (sentences).\n\nConverting Triples to Statement Sentences We convert every triple to a sentence by mapping the relation $r$ in the triple to its natural language form $\\tilde { r }$ , and concatenate s, r˜, and $o$ to be a statement $s _ { i }$ in S. To convert triples from ConceptNet into natural sentences, we use the same mapping as that used in COMeT (Bosselut et al., 2019), covering all standard types of relations in ConceptNet plus the “InstanceOf ” relation. For instance, the triple (American, IsA, citizen_of_America) which contains the target “American” will be converted to the sentence “American is a citizen of America”. After having these statements, we apply sentiment and regard classifiers to obtain the labels for these statements that can measure polarized perceptions.\n\nQuantifying Harms During the classification process using sentiment and regard classifiers, we mask all the demographic information from the sentences to avoid biases in sentiment and regard classifiers that may affect our analysis. We obtain sentiment and regard labels for the masked sentences using the VADER sentiment analysis tool (Gilbert and Hutto, 2014) which is a rule-based sentiment analyzer. For regard, we use the fine-tuned BERT model from (Sheng et al., 2019). After obtaining the labels, we use Eq. (1) and (2) to measure overgeneralization and Eq. (4) for disparity in overgeneralization.\n\n[Section: 3.2 Analysis of Representational Harms]\n\nResults on Overgeneralization We quantify overgeneralization using Eq. (1) and (2) in Section 2. The overall average percentage of overgeneralized",
      "context_after": "",
      "referring_paragraphs": [],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.35,
      "metadata": {}
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_2",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg",
      "image_filename": "2103.11320_page0_fig1.jpg",
      "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For example, ConceptNet originated from the Open\n\nTable 1: Biased cases in ConceptNet and GenericsKB.",
        "We consider overgeneralization that directly examines whether targets such as “lawyer” or “lady” are perceived positively or negatively in the statements (examples in Table 1), covering categories including stereotyping, denigration, and favoritism.",
        "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
        "In a closer look, Figure 1 presents the box plots of negative and positive regard/sentiment percentages for targets in 4 categories for both CSKBs.",
        "To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2103.11320_page0_fig2.jpg",
        "2103.11320_page0_fig3.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig3.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_5",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg",
      "image_filename": "2103.11320_page0_fig4.jpg",
      "caption": "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al. (2020) labeled by the regard measure. Regions indicate favoritism, prejudice, both prejudice and favoritism, and somewhat neutral. Higher negative regard percentages indicate prejudice-leaning and higher positive regard percentages indicate favoritism-leaning. We also compare ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) on the “Religion” category and find similar polarized perceptions of certain groups, despite a much larger percentage range for GenericsKB.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 2: Agreement of sentiment and regard labels with human annotators in terms of accuracy.",
        "<table><tr><td>CSKB</td><td>Method</td><td>Favoritism R/P/F1</td><td>Prejudice R/P/F1</td></tr><tr><td rowspan=\"3\">GenericsKB</td><td>Regard</td><td>0.551/0.579/0.565</td><td>0.809/0.333/0.472</td></tr><tr><td>Sentiment</td><td>0.441/0.622/0.516</td><td>0.432/0.541/0.480</td></tr><tr><td>Keyword</td><td>0.268/0.643/0.379</td><td>0.276/0.539/0.365</td></tr><tr><td rowspan=\"3\">ConceptNet</td><td>Regard</td><td>0.436/0.383/0.408</td><td>0.698/0.342/0.459</td></tr><tr><td>Sentiment</td><td>0.378/0.528/0.440</td><td>0.264/0.531/0.353</td></tr><tr><td>Keyword</td><td>0.201/0.556/0.295</td><td>0.105/0.470/0.172</td></tr></table>\n\nin Table 2, we found reasonable agreement in terms of accuracy for sentiment and regard with human labels.",
        "Figure 2: Examples of targets from the “Profession” and “Religion” categories from Nadeem et al.",
        "Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions."
      ],
      "figure_type": "example",
      "sub_figures": [
        "2103.11320_page0_fig5.jpg",
        "2103.11320_page0_fig6.jpg",
        "2103.11320_page0_fig7.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig5.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig6.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig7.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_9",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg",
      "image_filename": "2103.11320_page0_fig8.jpg",
      "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.",
      "context_before": "",
      "context_after": "Saffron terror is related to hindu.  Teacher causes the desire to study.Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.",
      "referring_paragraphs": [
        "As shown\n\nTable 3: Comparison of sentiment, regard, and baseline keyword-based approach in terms of favoritism and prejudice recall/precision/F1 scores.",
        "Teacher causes the desire to study.Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias.",
        "Severity of Overgeneralization Figure 3 further demonstrates how severe the problem of overgen-\n\neralization can be, along with some concrete examples.",
        "We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others."
      ],
      "figure_type": "example",
      "sub_figures": [
        "2103.11320_page0_fig9.jpg",
        "2103.11320_page0_fig10.jpg",
        "2103.11320_page0_fig11.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig9.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig11.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_13",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg",
      "image_filename": "2103.11320_page0_fig12.jpg",
      "caption": "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB. We find similarly severe disparities in two KBs with the number of sentences ranging much more for GenericsKB compared to ConceptNet.",
      "context_before": "",
      "context_after": "[Section: Overgeneralization Disparity]\n\nWe further analyze the disparities amongst targets in terms of overgeneralization (favoritism and prejudice perceptions measured by sentiment and regard) using Eq. (4), shown in Table 4. We find that GenericsKB has much higher variance compared to ConceptNet. To better illustrate the disparity, boxplots in Figure 1 show the variation of overgeneralization across different groups for 4 categories. These plots illustrate the dispersion of negative sentiment/regard percentages which represent\n\nTable 4: Disparity results quantified by variance across all targets on two CSKBs as shown in Equations 3 (statement #) and 4.   \n\n<table><tr><td>CSKB</td><td>Statement #</td><td>Pos Sent</td><td>Neg Sent</td><td>Pos Reg</td><td>Neg Reg</td></tr><tr><td>ConceptNet</td><td>141,538</td><td>20.4</td><td>4.03</td><td>3.74</td><td>8.74</td></tr><tr><td>GenericsKB</td><td>238,277</td><td>172</td><td>79</td><td>131</td><td>238</td></tr></table>\n\nprejudices against targets as well as positive sentiment/regard percentages for favoritism toward targets. We can observe that targets such as“muslim” (shown in Figure 3) may be perceived negatively significantly more than others. The same trend also holds for positive sentiment and regard scores. Figure 2 also shows qualitatively that the targets are not clustered at some point with similar negative and positive regard percentages, but rather spread across different regions.\n\n[Section: 4 Analysis on Downstream Applications]",
      "referring_paragraphs": [
        "Figure 4: Box plots demonstrating the representation disparity in terms of number of triples/sentences for “Gender” and “Profession” categories from Concept-Net and GenericsKB.",
        "(4), shown in Table 4."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2103.11320_page0_fig13.jpg",
        "(c) ConceptNet Profession",
        "(d) GenericsKB Profession",
        "2103.11320_page0_fig14.jpg",
        "2103.11320_page0_fig15.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig12.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig14.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig15.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_17",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg",
      "image_filename": "2103.11320_page0_fig16.jpg",
      "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.",
      "context_before": "",
      "context_after": "[Section: 4.3 Bias Mitigation on CSKB Completion]\n\nTo mitigate the observed representational harms in ConceptNet and their effects on downstream tasks, we propose a pre-processing data filtering technique that reduces the effect of existing representational harms in ConceptNet. We apply our mitigation technique on COMeT as a case study.\n\nTable 7: Mitigation results of the filtering technique (COMeT-Filtered) compared to standard COMeT. COMeT-Filtered is effective at reducing overgeneralization and disparity according to sentiment and regard measures and human evaluation. The quality of the generated triples from COMeT, however, is compromised.   \n\n<table><tr><td></td><td>NSM↑</td><td>NSV↓</td><td>NRM↑</td><td>NRV↓</td><td>HNM↑</td><td>Quality↑</td></tr><tr><td>COMeT</td><td>62.6</td><td>33.4</td><td>78.2</td><td>63.2</td><td>55.8</td><td>55.8</td></tr><tr><td>COMeT-Filtered</td><td>63.2</td><td>32.3</td><td>78.6</td><td>56.7</td><td>60.5</td><td>49.9</td></tr></table>\n\nMitigation Approach Our pre-processing technique relies on data filtering. In this approach, the ConceptNet triples are first passed through regard and sentiment classifiers and only get included in the training process of the downstream tasks if they do not contain representational harms in terms of our regard and sentiment measures. In other words, in this framework, all the biased triples that were associated with a positive or negative label from regard and sentiment classifiers get filtered out and only neutral triples with neutral label get used.\n\nResults on Overgeneralization To measure effectiveness of mitigation over overgeneralization, we consider increasing the overall mean of neutral triples which is indicative of reducing the overall favoritism and prejudice according to sentiment and regard measures. We report the effects on overgenaralization on sentiment as Neutral Sentiment Mean (NSM) and regard measure as Neutral Regard Mean (NRM). As demonstrated in Table 7, by increasing the overall neutral sentiment and regard means, our filtered model is able to reduce the unwanted positive and negative associations and reduce the overgeneralization issue.\n\nResults on Disparity in Overgeneralization To measure effectiveness of mitigation over disparity in overgeneralization, we consider reducing the existing variance amongst different targets. We report the disparity in overgeneralization on sentiment as Neutral Sentiment Variance (NSV) and on regard as Neutral Regard Variance (NRV). Shown in Table 7, our filtered technique reduces the variance and dis-\n\nparities amongst targets over the standard COMeT model in terms of regard and sentiment measures.\n\nHuman Evaluation of Mitigation Results In addition to reporting regard and sentiment scores, we perform human evaluation on 3,000 generated triples from standard COMeT and COMeT-Filtered models to evaluate both the quality of the generated triples and the bias aspect of it from the human perspective on Amazon Mechanical Turk. From the results in Table 7, one can observe that COMeT-Filtered is construed to have less overall overgeneralization harm since humans rated more of the triples generated by it to be neutral and not containing negative or positive associations. This is shown as Human Neutral Mean (HNM) in Table 7. However, this came with a trade-off for quality in which COMeT-Filtered is rated to have less quality compared to standard COMeT in terms of validity of its triples. We encourage future work to improve for higher quality. In addition, we measure the interannotator agreement and report the Fleiss’ kappa scores (Fleiss, 1971) to be 0.4788 and 0.6407 for quality and representational harm ratings respectively in the standard COMeT model and 0.4983 and 0.6498 for that of COMeT-Filtered.\n\n[Section: 5 Related Work]",
      "referring_paragraphs": [
        "For instance, the results from COMeT shown in Figure 5 demonstrate the fact that variances exist in both regard and sentiment measures which is an indication of disparity in overgeneralization.",
        "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "2103.11320_page0_fig17.jpg",
        "2103.11320_page0_fig18.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig17.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig18.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_20",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg",
      "image_filename": "2103.11320_page0_fig19.jpg",
      "caption": "Figure 6: Examples from ConceptNet.   ",
      "context_before": "Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The problem with bias: Allocative versus representational harms in machine learning. In 9th Annual Conference of the Special Interest Group for Computing, Information and Society.   \nSumithra Bhakthavatsalam, Chloe Anastasiades, and Peter Clark. 2020. Genericskb: A knowledge base of generic statements. arXiv preprint arXiv:2005.00660.   \nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454– 5476, Online. Association for Computational Linguistics.   \nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems, pages 4349–4357.   \nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for knowledge graph construction. In Association for Computational Linguistics (ACL).   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems,\n\nvolume 33, pages 1877–1901. Curran Associates, Inc.   \nTuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, and Nanyun Peng. 2020. $\\scriptstyle \\mathrm { \\mathrm { R } } \\ \\mathrm { \\hat { 3 } }$ : Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7976–7986, Online. Association for Computational Linguistics.   \nTing-Yun Chang, Yang Liu, Karthik Gopalakrishnan, Behnam Hedayatnia, Pei Zhou, and Dilek Hakkani-Tur. 2020. Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 74–79, Online. Association for Computational Linguistics.   \nKate Crawford. 2017. The trouble with bias. In Conference on Neural Information Processing Systems, invited speaker.   \nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 862–872.   \nEthan Fast, Binbin Chen, and Michael S. Bernstein. 2016. Empath: Understanding topic signals in largescale text. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, page 4647–4657, New York, NY, USA. Association for Computing Machinery.   \nJoseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.   \nJoel Escudé Font and Marta R Costa-jussà. 2019. Equalizing gender bias in neural machine translation with word embeddings techniques. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 147–154.   \nCHE Gilbert and Erric Hutto. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth International Conference on Weblogs and Social Media (ICWSM-14). Available at (20/04/16) http://comp. social. gatech. edu/papers/icwsm14. vader. hutto. pdf, volume 81, page 82.   \nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pretraining model for commonsense story generation. Transactions of the Association for Computational Linguistics, 8:93–108.\n\nTao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, and Vivek Srikumar. 2020. UnQovering stereotyping biases via underspecified questions. In Findings of EMNLP.   \nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel. 2016. Commonsense knowledge base completion. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1445–1455.   \nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2829–2839, Hong Kong, China. Association for Computational Linguistics.   \nGardner Ed Lindzey and Elliot Ed Aronson. 1968. The handbook of social psychology.   \nNinareh Mehrabi, Thamme Gowda, Fred Morstatter, Nanyun Peng, and Aram Galstyan. 2020. Man is to person as woman is to location: Measuring gender bias in named entity recognition. In Proceedings of the 31st ACM Conference on Hypertext and Social Media, pages 231–232.   \nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM Comput. Surv., 54(6).   \nGeorge A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41.   \nMoin Nadeem, Anna Bethke, and Siva Reddy. 2020. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.   \nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.   \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.   \nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3027–3035.   \nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2019. The woman worked as a\n\nbabysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3398–3403.   \nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2020. Towards Controllable Biases in Language Generation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239–3254, Online. Association for Computational Linguistics.   \nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4615–4629.   \nPush Singh, Thomas Lin, Erik T Mueller, Grace Lim, Travell Perkins, and Wan Li Zhu. 2002. Open mind common sense: Knowledge acquisition from the general public. In OTM Confederated International Conferences\" On the Move to Meaningful Internet Systems\", pages 1223–1237. Springer.   \nRobyn Speer. 2017. Conceptnet numberbatch 17.04: better, less-stereotyped word vectors. ConceptNet blog. April, 24.   \nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: an open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages 4444–4451.   \nShane Storks, Qiaozi Gao, and Joyce Y Chai. 2019. Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches. arXiv preprint arXiv:1904.01172, pages 1–60.   \nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019. Mitigating gender bias in natural language processing: Literature review. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1630–1640.   \nChris Sweeney and Maryam Najafian. 2019. A transparent framework for evaluating unintended demographic bias in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1662–1667, Florence, Italy. Association for Computational Linguistics.   \nYla R. Tausczik and James W. Pennebaker. 2010. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of Language and Social Psychology, 29(1):24–54.   \nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018a. Gender bias in coreference resolution: Evaluation and debiasing\n\nmethods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20.   \nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-Wei Chang. 2018b. Learning gender-neutral word embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847–4853.   \nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018. Commonsense knowledge aware conversation generation with graph attention. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 4623–4629. International Joint Conferences on Artificial Intelligence Organization.   \nPei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu, and Dilek Hakkani-Tur. 2021. Commonsensefocused dialogues for response generation: An empirical study. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 121–132, Singapore and Online. Association for Computational Linguistics.   \nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang, Muhao Chen, Ryan Cotterell, and Kai-Wei Chang. 2019. Examining gender bias in languages with grammatical gender. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5276–5284.\n\n[Section: A Qualitative Examples]\n\nWe include details in the appendix section both in terms of providing more qualitative analysis and also some detailed experimental results that we could not include in the main text due to the space limitation. For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet. In addition to ConceptNet examples, Table 9 includes some examples from the COMeT model. Similarly, Table 10 includes some examples for the Commonsense Story Generation model (CSG). Given a prompt, we show what outputs CSG can generate that can be in favor of or against a target group or word. Tables 14 and 15 contain the detailed list of these target groups and words.",
      "context_after": "[Section: B Mitigation Framework]\n\nIn addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.\n\n[Section: C Human Evaluation]",
      "referring_paragraphs": [
        "Some examples are listed in Table 6.",
        "Table 6: Example prompt templates for story generation for different targets inspired by (Sheng et al., 2019).",
        "Figure 6: Examples from ConceptNet.   \nFigure 7: Data filtering bias mitigation framework."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "2103.11320_page0_fig20.jpg",
        "2103.11320_page0_fig21.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig20.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig21.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_23",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg",
      "image_filename": "2103.11320_page0_fig22.jpg",
      "caption": "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "Table 8: Percentages represent how much regard and sentiment labels ran on COMeT and COMeT-Filtered triples agree with labels coming from humans.",
        "Figure 8: Examples of targets and the regions they fall under within each category considering the regard measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions."
      ],
      "figure_type": "example",
      "sub_figures": [
        "2103.11320_page0_fig23.jpg",
        "2103.11320_page0_fig24.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig22.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig23.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig24.jpg"
        ],
        "panel_count": 3
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_26",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg",
      "image_filename": "2103.11320_page0_fig25.jpg",
      "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ",
      "context_before": "",
      "context_after": "",
      "referring_paragraphs": [
        "For instance, in Table 9 we include more of qualitative results and demonstrate some destructive triples existing in ConceptNet.",
        "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure."
      ],
      "figure_type": "example",
      "sub_figures": [
        "2103.11320_page0_fig26.jpg",
        "2103.11320_page0_fig27.jpg",
        "(a) ConceptNet Origin",
        "2103.11320_page0_fig28.jpg"
      ],
      "quality_score": 0.65,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig26.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig27.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig28.jpg"
        ],
        "panel_count": 4
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_30",
      "figure_number": 11,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg",
      "image_filename": "2103.11320_page0_fig29.jpg",
      "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.",
      "context_before": "",
      "context_after": "british-negative sentiment",
      "referring_paragraphs": [
        "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.",
        "Table 11: Detailed mitigation results for filtering technique compared to vanilla COMeT for each category.",
        "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "(b) ConceptNet Religion",
        "(c) GenericsKB Origin",
        "2103.11320_page0_fig30.jpg",
        "(d) GenericsKB Religion",
        "2103.11320_page0_fig31.jpg",
        "2103.11320_page0_fig32.jpg",
        "2103.11320_page0_fig33.jpg",
        "2103.11320_page0_fig34.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig30.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig31.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig32.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig33.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig34.jpg"
        ],
        "panel_count": 6
      }
    },
    {
      "doc_id": "2103.11320",
      "figure_id": "2103.11320_fig_36",
      "figure_number": 12,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig35.jpg",
      "image_filename": "2103.11320_page0_fig35.jpg",
      "caption": "Figure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.",
      "context_before": "female-negative sentiment",
      "context_after": "female-negative regard   \nFigure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets.\n\nTable 14: Targets from the profession category in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.   \n\n<table><tr><td></td><td></td><td colspan=\"3\">Profession</td></tr><tr><td>barber</td><td>coach</td><td>businessperson</td><td>football player</td><td>construction worker</td></tr><tr><td>manager</td><td>CEO</td><td>accountant</td><td>commander</td><td>firefighter</td></tr><tr><td>mover</td><td>software developer</td><td>guard</td><td>baker</td><td>doctor</td></tr><tr><td>athlete</td><td>artist</td><td>dancer</td><td>mathematician</td><td>janitor</td></tr><tr><td>carpenter</td><td>mechanic</td><td>actor</td><td>handyman</td><td>musician</td></tr><tr><td>detective</td><td>politician</td><td>entrepreneur</td><td>model</td><td>opera singer</td></tr><tr><td>chief</td><td>lawyer</td><td>farmer</td><td>writer</td><td>librarian</td></tr><tr><td>army</td><td>real estate developer</td><td>broker</td><td>scientist</td><td>butcher</td></tr><tr><td>electrician</td><td>prosecutor</td><td>banker</td><td>cook</td><td>hairdresser</td></tr><tr><td>prisoner</td><td>plumber</td><td>attorney</td><td>boxer</td><td>chess player</td></tr><tr><td>priest</td><td>swimmer</td><td>tennis player</td><td>supervisor</td><td>attendant</td></tr><tr><td>housekeeper</td><td>maid</td><td>producer</td><td>researcher</td><td>midwife</td></tr><tr><td>judge</td><td>lampire</td><td>bartender</td><td>economist</td><td>physicist</td></tr><tr><td>psychologist</td><td>theologian</td><td>salesperson</td><td>physician</td><td>sheriff</td></tr><tr><td>cashier</td><td>assistant</td><td>receptionist</td><td>editor</td><td>engineer</td></tr><tr><td>comedian</td><td>painter</td><td>civil servant</td><td>diplomat</td><td>guitarist</td></tr><tr><td>linguist</td><td>poet</td><td>laborer</td><td>teacher</td><td>delivery man</td></tr><tr><td>realtor</td><td>pilot</td><td>professor</td><td>chemist</td><td>historian</td></tr><tr><td>pensioner</td><td>performing artist</td><td>singer</td><td>secretary</td><td>auditor</td></tr><tr><td>counselor</td><td>designer</td><td>soldier</td><td>journalist</td><td>dentist</td></tr><tr><td>analyst</td><td>nurse</td><td>tailor</td><td>waiter</td><td>author</td></tr><tr><td>architect</td><td>academic</td><td>director</td><td>illustrator</td><td>clerk</td></tr><tr><td>policeman</td><td>chef</td><td>photographer</td><td>drawing</td><td>cleaner</td></tr><tr><td>pharmacist</td><td>pianist</td><td>composer</td><td>handball player</td><td>sociologist</td></tr></table>\n\nTable 15: Targets from origin, gender, and religion categories in our dataset (most of them borrowed from Nadeem et al 2020). We considered triples that these words appeared in them.   \n\n<table><tr><td></td><td colspan=\"4\">Origin</td></tr><tr><td>African American</td><td>Armenian</td><td>Persian</td><td>American</td><td>Filipino</td></tr><tr><td>English</td><td>Dutch</td><td>Israeli</td><td>Nigerian</td><td>Ethiopia</td></tr><tr><td>Europe</td><td>European</td><td>Russian</td><td>Ukraine</td><td>Sudan</td></tr><tr><td>Afghanistan</td><td>Iraq</td><td>Yemen</td><td>Ukrainian</td><td>Russia</td></tr><tr><td>Italy</td><td>Somali</td><td>Iran</td><td>Afghan</td><td>Indian</td></tr><tr><td>Italian</td><td>Australian</td><td>Spanish</td><td>Guatemala</td><td>Hispanic</td></tr><tr><td>Venezuela</td><td>Sudanese</td><td>Oman</td><td>Finnish</td><td>Swedish</td></tr><tr><td>Venezuelan</td><td>Puerto Rican</td><td>Ghanaian</td><td>Moroccan</td><td>Somalia</td></tr><tr><td>Saudi Arabian</td><td>Syria</td><td>Chinese</td><td>Pakistani</td><td>China</td></tr><tr><td>India</td><td>Irish</td><td>Britain</td><td>France</td><td>Greece</td></tr><tr><td>Scotland</td><td>Mexican</td><td>Paraguayan</td><td>Brazil</td><td>African</td></tr><tr><td>Eritrean</td><td>Sierra Leonean</td><td>Africa</td><td>Jordan</td><td>Indonesia</td></tr><tr><td>Vietnam</td><td>Pakistan</td><td>German</td><td>Romania</td><td>Brazilian</td></tr><tr><td>Ecuadorian</td><td>Mexico</td><td>Puerto Rico</td><td>Kenyan</td><td>Liberian</td></tr><tr><td>Cameroonian</td><td>African Americans</td><td>Kenya</td><td>Liberia</td><td>Sierra Leon</td></tr><tr><td>Qatari</td><td>Syrian</td><td>Arab</td><td>Saudi Arabia</td><td>Lebanon</td></tr><tr><td>Indonesian</td><td>French</td><td>Norwegian</td><td>South Africa</td><td>Jordanian</td></tr><tr><td>Korea</td><td>Singapore</td><td>Romanian</td><td>Crimean</td><td>Native American</td></tr><tr><td>Germany</td><td>Ireland</td><td>Ecuador</td><td>Morocco</td><td>Omani</td></tr><tr><td>Iranian</td><td>Iraqi</td><td>Qatar</td><td>Turkey</td><td>Vietnamese</td></tr><tr><td>Nepali</td><td>Laos</td><td>Bangladesh</td><td>British</td><td>Polish</td></tr><tr><td>Greek</td><td>Scottish</td><td>Bolivian</td><td>Guatemala</td><td>Ghana</td></tr><tr><td>Cameroon</td><td>Japanese</td><td>Taiwanese</td><td>Bengali</td><td>Nepal</td></tr><tr><td>Albanian</td><td>Albania</td><td>Columbian</td><td>Peruvian</td><td>Argentinian</td></tr><tr><td>Spain</td><td>Paraguay</td><td>Ethiopian</td><td>Egyptian</td><td>Persian people</td></tr><tr><td>Sweden</td><td>Crimea</td><td>Portuguese</td><td>Argentina</td><td>Chile</td></tr><tr><td>Cape Verdean</td><td>Turkish</td><td>Yemeni</td><td>Taiwan</td><td>Austrian</td></tr><tr><td>White people</td><td>Finland</td><td>Australia</td><td>South African</td><td>Erteria</td></tr><tr><td>Egypt</td><td>Korean</td><td>Dutch people</td><td>Peru</td><td>Poland</td></tr><tr><td>Chilean</td><td>Columbia</td><td>Bolivia</td><td>Laotian</td><td>Lebanese</td></tr><tr><td>Japan</td><td>Norway</td><td>Cape Verde</td><td>Portugal</td><td>Austria</td></tr><tr><td>Singaporean</td><td>Netherlands</td><td></td><td></td><td></td></tr><tr><td></td><td colspan=\"4\">Gender</td></tr><tr><td>she</td><td>he</td><td>hers</td><td>him</td><td>her</td></tr><tr><td>herself</td><td>himself</td><td>his</td><td>woman</td><td>man</td></tr><tr><td>female</td><td>male</td><td>lady</td><td>gentleman</td><td>ladies</td></tr><tr><td>gentlemen</td><td>girl</td><td>boy</td><td>sir</td><td>ma am</td></tr><tr><td>mother</td><td>father</td><td>stepmother</td><td>stepfather</td><td>daughter</td></tr><tr><td>son</td><td>sister</td><td>brother</td><td>grandmother</td><td>grandfather</td></tr><tr><td>mommy</td><td>daddy</td><td>wife</td><td>husband</td><td>bride</td></tr><tr><td>groom</td><td>girlfriend</td><td>boyfriend</td><td>schoolgirl</td><td>schoolboy</td></tr><tr><td></td><td colspan=\"4\">Religion</td></tr><tr><td>Sharia</td><td>Jihad</td><td>Christian</td><td>Muslim</td><td>Islam</td></tr><tr><td>Hindu</td><td>Mohammed</td><td>church</td><td>Quran</td><td>Bible</td></tr><tr><td>Brahmin</td><td>Holy Trinity</td><td></td><td></td><td></td></tr></table>",
      "referring_paragraphs": [
        "In addition, we provide a visual for our mitigation framework in Figure 7 and detailed results of COMeT vs COMet_Filtered comparisons over different categories. Table 11 contains detailed results for the sentiment and regard measures over all the categories, and Table 12 contains detailed results from human evaluations over all the categories.",
        "<table><tr><td>Measure</td><td>Model</td><td>Origin</td><td>Religion</td><td>Gender</td><td>Profession</td></tr><tr><td rowspan=\"2\">Neutral Sentiment Mean ↑</td><td>COMeT</td><td>64.527</td><td>58.578</td><td>59.169</td><td>61.610</td></tr><tr><td>COMeT-Filtered</td><td>65.257</td><td>59.485</td><td>59.272</td><td>62.105</td></tr><tr><td rowspan=\"2\">Neutral Sentiment Variance ↓</td><td>COMeT</td><td>18.875</td><td>69.043</td><td>15.432</td><td>44.415</td></tr><tr><td>COMeT-Filtered</td><td>17.660</td><td>104.284</td><td>15.190</td><td>37.222</td></tr><tr><td rowspan=\"2\">Neutral Regard Mean ↑</td><td>COMeT</td><td>79.630</td><td>68.775</td><td>76.074</td><td>78.946</td></tr><tr><td>COMeT-Filtered</td><td>80.009</td><td>71.618</td><td>76.471</td><td>79.120</td></tr><tr><td rowspan=\"2\">Neutral Regard Variance ↓</td><td>COMeT</td><td>36.848</td><td>108.086</td><td>19.319</td><td>72.088</td></tr><tr><td>COMeT-Filtered</td><td>33.532</td><td>97.282</td><td>18.162</td><td>67.261</td></tr></table>\n\nTable 12: Detailed human annotator results for each category.",
        "female-negative regard   \nFigure 12: Wordcloud of phrases that appear in triples with negative regard and sentiment labels for “british” and “female” targets."
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 1.0,
      "metadata": {}
    }
  ],
  "2109.03952": [
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_1",
      "figure_number": 1,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
      "image_filename": "2109.03952_page0_fig0.jpg",
      "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.",
      "context_before": "[Section: 2.1 General Model: Incorporating Attention over Inputs in Classifiers]\n\nWe consider each feature value as an individual entity (like the words are considered in text-classification) and learn a fixed-size embedding $\\{ e _ { k } \\} _ { k = 1 } ^ { m }$ , $e _ { k } \\in \\mathbb { R } ^ { d ^ { e } }$ for each feature, $\\{ f _ { k } \\} _ { k = 1 } ^ { m }$ . These vectors are passed to the attention layer. The Computation of attention weights and the final representation for a sample is described in Eq. 1. $E = [ e _ { 1 } \\ldots e _ { m } ]$ , $E \\in$ $\\mathbb { R } ^ { d ^ { e } \\times m }$ is the concatenation of all the embeddings, $w \\in \\mathbb { R } ^ { d ^ { e } }$ is a learnable parameter, $r \\in \\mathbb { R } ^ { d ^ { e } }$ denotes the overall sample representation, and $\\alpha \\in \\mathbb { R } ^ { m }$ denotes the attention weights.\n\nThe resulting representation, $r$ , is passed to the feed-forward layers for classification. In this work, we have used two feedforward layers (See Fig. 1 for overall architecture).",
      "context_after": "[Section: 2.2 Fairness Attribution with Attention Weights]\n\nThe aforementioned classification model with the attention mechanism combines input feature embeddings by taking a weighted combination. By manipulating the weights, we can intuitively capture the effects of specific features on the output. To this end, we observe the effect of each attribute on the fairness of outcomes by zeroing out or reducing its attention weights and recording the change. Other works have used similar ideas to understand the effect of attention weights on accuracy and evaluate interpretability of the attention weights by comparing the difference in outcomes in terms of measures such as Jensen-Shannon Divergence (Serrano and Smith 2019) but not for fairness. We are interested in the effect of features on fairness measures. Thus, we measure the difference in fairness of the outcomes based on the desired fairness measure. A large change in fairness measure and a small change in performance of the model would indicate that this feature is mostly responsible for unfairness, and it can be dropped without causing large impacts on performance. The overall framework is shown in Fig. 1. First, the outcomes are recorded with the original attention weights intact (Fig. 1a). Next, attention weights corresponding to a particular feature are zeroed out, and the difference in performance and fairness measures is recorded (Fig. 1b). Based on the observed differences, one may conclude how incorporating this feature contributes to fairness/unfairness.\n\nTo measure the effect of the $k ^ { t h }$ feature on different fairness measures, we consider the difference in the fairness of outcomes of the original model and model with $k ^ { t h }$ feature’s effect removed. For example, for statistical parity difference, we will consider $\\mathrm { S P D } ( \\hat { \\mathbf { y } } _ { o } , \\mathbf { a } ) - \\mathrm { S P D } ( \\hat { \\mathbf { y } } _ { z } ^ { k } , \\mathbf { a } )$ . A negative value will indicate that the $\\dot { k } ^ { t h }$ feature helps mitigate unfairness,\n\nAlgorithm 1: Bias Mitigation with Attention   \nInput: decay rate $d_r$ $(0\\leq d_r < 1)$ n test samples indexed by variable i.   \nOutput: final predictions, unfair features.   \nCalculate the attention weights $\\alpha_{ki}$ for $k^{\\mathrm{th}}$ feature in sample i using the attention layer as in Eq. 1.   \nunfair_feature_set = {}   \nfor each feature (index) k do if $SPD(\\hat{y}_o,\\mathbf{a}) - SPD(\\hat{y}_z^k,\\mathbf{a})\\geq 0$ then unfair_feature_set $\\equiv$ unfair_feature_set $\\cup \\{k\\}$ end   \nend   \nfor each feature (index) k do if $k$ in unfair_feature_set then Set $\\alpha_{ki}\\gets (d_r\\times \\alpha_{ki})$ for all n samples end   \nend   \nUse new attention weights to obtain the final predictions $\\hat{Y}$ return $\\hat{Y}$ , unfair_feature_set\n\nand a positive value will indicate that the $k ^ { t h }$ feature contributes to unfairness. This is because $\\hat { y } _ { z } ^ { k }$ captures the exclusion of the $k ^ { t h }$ feature (zeroed out attention weight for that feature) from the decision-making process. If the value is positive, it indicates that not having this feature makes the bias lower than when we include it. Notice here, we focus on global attribution, so we measure this over all the samples; however, this can also be turned into local attribution by focusing on individual sample $i$ only.\n\n[Section: 2.3 Mitigating Bias by Removing Unfair Features]",
      "referring_paragraphs": [
        "The resulting representation, $r$ , is passed to the feed-forward layers for classification. In this work, we have used two feedforward layers (See Fig. 1 for overall architecture).",
        "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned.",
        "As shown in Table 1, our post-processing mitigation technique provides lower TRPD while being more accurate, followed by the technique that masks the gendered words before training.",
        "We also used different bias mitigation strategies to compare against our mitigation strategy, such\n\nTable 1: Difference of the True Positive Rates (TPRD) amongst different genders for the dentist and nurse occupations on the biosbias dataset."
      ],
      "figure_type": "architecture",
      "sub_figures": [
        "(a) Classification model.",
        "(b) Attribution framework.",
        "2109.03952_page0_fig1.jpg"
      ],
      "quality_score": 1.0,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig1.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_3",
      "figure_number": 2,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg",
      "image_filename": "2109.03952_page0_fig2.jpg",
      "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
      "context_before": "First, we test our method’s ability to capture correct attributions in controlled experiments with synthetic data (described in Sec. 3.2). We also conduct a similar experiment with UCI Adult and Heritage Health datasets which can be\n\nfound in the appendix. Fig. 2 summarizes our results by visualizing the attributions, which we now discuss.\n\nIn Scenario 1, as expected, $f _ { 2 }$ is correctly attributed to being responsible for the accuracy and removing it hurts the accuracy drastically. Similarly, $f _ { 1 }$ is correctly shown to be responsible for unfairness and removing it creates a fairer outcome. Ideally, the model should not be using any information about $f _ { 1 }$ as it is independent of the task, but it does. Therefore, by removing $f _ { 1 }$ , we can ensure that information is not used and hence outcomes are fair. Lastly, as expected, $f _ { 3 }$ was the irrelevant feature, and its effects on accuracy and fairness are negligible. Another interesting observation is that $f _ { 2 }$ is helping the model achieve fairness since its exclusion means that the model should rely on $f _ { 1 }$ for decision making, resulting in more bias; thus, removing $f _ { 2 }$ harms accuracy and fairness as expected.\n\nIn Scenario 2, our framework captures the effect of indirect discrimination. We can see that removing $f _ { 2 }$ reduces bias as well as accuracy drastically. This is because $f _ { 2 }$ is the predictive feature, but due to its correlation with $f _ { 1 }$ , it can also indirectly affect the model’s fairness. More interestingly, although $f _ { 1 }$ is the sensitive feature, removing it does not play a drastic role in fairness or the accuracy. This is an important finding as it shows why removing $f _ { 1 }$ on its own can not give us a fairer model due to the existence of correlations to other features and indirect discrimination. Overall, our results are intuitive and thus validate our assumption that attention-based framework can provide reliable feature attributions for the fairness and accuracy of the model.\n\n[Section: 4.2 Attention as a Mitigation Technique]\n\nAs we have highlighted earlier, understanding how the information within features interact and contribute to the decision making can be used to design effective bias mitigation strategies. One such example was shown in Sec. 4.1. Often realworld datasets have features which cause indirect discrimination, due to which fairness can not be achieved by simply eliminating the sensitive feature from the decision process. Using the attributions derived from our attention-based attribution framework, we propose a post-processing mitigation strategy. Our strategy is to intervene on attention weights as discussed in Sec. 2.3. We first attribute and identify the features responsible for the unfairness of the outcomes, i.e., all the features whose exclusion will decrease the bias compared to the original model’s outcomes and gradually decrease their attention weights to zero as also outlined in Algorithm 1. We do this by first using the whole fraction of the attention weights learned and gradually use less fraction of the weights until the weights are completely zeroed out.\n\nFor all the baselines described in Sec. 3.3, we used the approach outlined in Gupta et al. (2021) for training a downstream classifier and evaluating the accuracy/fairness tradeoffs. The downstream classifier was a 1-hidden-layer MLP with 50 neurons along with ReLU activation function. Our experiments were performed on Nvidia GeForce RTX 2080. Each method was trained with five different seeds, and we report the average accuracy and fairness measure as statistical parity difference (SPD). Results for other fairness notions can be found in the appendix. CVIB, MaxEnt-ARL,",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 2: Results from the synthetic datasets.",
        "Although most of the features in the Adult datasets are self-descriptive, Heritage Health dataset includes some abbreviations that we list in Table 2 for the ease of interpreting each feature’s meaning."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2109.03952_page0_fig3.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig3.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_5",
      "figure_number": 3,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg",
      "image_filename": "2109.03952_page0_fig4.jpg",
      "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.",
      "context_before": "",
      "context_after": "[Section: 4.3 Experiments with Non-Tabular Data]\n\nIn addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets. To put this to the test, we applied our model to mitigate bias in text-based data. We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset. We compare our approach with the debiasing technique proposed in the original paper (De-Arteaga et al. 2019), which works by masking the gender-related words and then training the model on this masked data. As discussed earlier, such a method is computationally inefficient. It requires re-training the model or creating a new masked dataset, each time it is required to de-\n\nbias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the bias, we used the same measure as in (De-Arteaga et al. 2019) which is based on the equality of opportunity notion of fairness (Hardt et al. 2016) and reported the True Positive Rate Difference (TPRD) for each occupation amongst different genders. As shown in Table 1, our post-processing mitigation technique provides lower TRPD while being more accurate, followed by the technique that masks the gendered words before training. Although both methods reduce the bias compared to a model trained on raw bios without applying any mask or invariance to gendered words, our post-processing method is more effective. Fig. 4 also highlights qualitative differences between models in terms of their most attentive features for the prediction task. As shown in the results, our post-processing technique is able to use more meaningful words, such as R.N. (registered nurse) to predict the outcome label nurse compared to both baselines, while the non-debiased model focuses on gendered words.\n\n[Section: 5 Related Work]",
      "referring_paragraphs": [
        "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.",
        "Table 3: Adult results on post-processing approach from Hardt et al."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2109.03952_page0_fig5.jpg"
      ],
      "quality_score": 0.8,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig5.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_7",
      "figure_number": 4,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg",
      "image_filename": "2109.03952_page0_fig6.jpg",
      "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”",
      "context_before": "In addition to providing interpretability, our approach is flexible and useful for controlling fairness in modalities other than tabular datasets. To put this to the test, we applied our model to mitigate bias in text-based data. We consider the biosbias dataset (De-Arteaga et al. 2019), and use our mitigation technique to reduce observed biases in the classification task performed on this dataset. We compare our approach with the debiasing technique proposed in the original paper (De-Arteaga et al. 2019), which works by masking the gender-related words and then training the model on this masked data. As discussed earlier, such a method is computationally inefficient. It requires re-training the model or creating a new masked dataset, each time it is required to de-\n\nbias the model against different attributes, such as gender vs. race. For the baseline pre-processing method, we masked the gender-related words, such as names and gender words, as provided in the biosbias dataset and trained the model on the filtered dataset. On the other hand, we trained the model on the raw bios for our post-processing method and only manipulated attention weights of the gender words during the testing process as also provided in the biosbias dataset. In order to measure the bias, we used the same measure as in (De-Arteaga et al. 2019) which is based on the equality of opportunity notion of fairness (Hardt et al. 2016) and reported the True Positive Rate Difference (TPRD) for each occupation amongst different genders. As shown in Table 1, our post-processing mitigation technique provides lower TRPD while being more accurate, followed by the technique that masks the gendered words before training. Although both methods reduce the bias compared to a model trained on raw bios without applying any mask or invariance to gendered words, our post-processing method is more effective. Fig. 4 also highlights qualitative differences between models in terms of their most attentive features for the prediction task. As shown in the results, our post-processing technique is able to use more meaningful words, such as R.N. (registered nurse) to predict the outcome label nurse compared to both baselines, while the non-debiased model focuses on gendered words.\n\n[Section: 5 Related Work]\n\nFairness. The research in fairness concerns itself with various topics, such as defining fairness metrics, proposing solutions for bias mitigation, and analyzing existing harms in various systems (Mehrabi et al. 2021). In this work, we utilized different metrics that were introduced previously, such as statistical parity (Dwork et al. 2012), equality of opportunity and equalized odds (Hardt et al. 2016), to measure the amount of bias. We also used different bias mitigation strategies to compare against our mitigation strategy, such\n\nTable 1: Difference of the True Positive Rates (TPRD) amongst different genders for the dentist and nurse occupations on the biosbias dataset. Our introduced post-processing method is the most effective in reducing the disparity for both occupations compared to the pre-processing technique.   \n\n<table><tr><td>Method</td><td>Dentist TPRD (stdev)</td><td>Nurse TPRD (stdev)</td><td>Accuracy (stdev)</td></tr><tr><td>Post-Processing (Ours)</td><td>0.0202 (0.010)</td><td>0.0251 (0.020)</td><td>0.951 (0.013)</td></tr><tr><td>Pre-Processing</td><td>0.0380 (0.016)</td><td>0.0616 (0.025)</td><td>0.946 (0.011)</td></tr><tr><td>Not Debiased Model</td><td>0.0474 (0.025)</td><td>0.1905 (0.059)</td><td>0.958 (0.011)</td></tr></table>",
      "context_after": "[Section: 6 Discussion]\n\nIn this work, we analyzed how attention weights contribute to fairness and accuracy of a predictive model. To do so, we proposed an attribution method that leverages the attention mechanism and showed the effectiveness of this approach on both tabular and text data. Using this interpretable attribution framework we then introduced a post-processing bias mitigation strategy based on attention weight manipulation. We validated the proposed framework by conducting experiments with different baselines, as well as fairness metrics, and different data modalities.\n\nAlthough our work can have a positive impact in allowing to reason about fairness and accuracy of models and reduce their bias, it can also have negative societal consequences if used unethically. For instance, it has been previously shown that interpretability frameworks can be used as a means for fairwashing which is when malicious users generate fake explanations for their unfair decisions to justify them (Anders et al. 2020). In addition, previously it has been shown that interpratability frameworks are vulnerable against adversarial attacks (Slack et al. 2020). We acknowledge that our framework may also be targeted by malicious users for malicious intent that can manipulate attention weights to either generate fake explanations or unfair outcomes. An important future direction can be to analyze and improve robustness of our framework along with others.\n\n[Section: References]",
      "referring_paragraphs": [
        "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts.",
        "<table><tr><td></td><td>Accuracy</td><td>SPD</td><td>Accuracy</td><td>EQOP</td><td>Accuracy</td><td>EQOD</td></tr><tr><td>Attention (Ours)</td><td>0.77 (0.006)</td><td>0.012 (0.003)</td><td>0.81 (0.013)</td><td>0.020 (0.019)</td><td>0.81 (0.021)</td><td>0.027 (0.023)</td></tr><tr><td>Hardt et al.</td><td>0.77 (0.012)</td><td>0.013 (0.005)</td><td>0.83 (0.005)</td><td>0.064 (0.016)</td><td>0.81 (0.007)</td><td>0.047 (0.014)</td></tr></table>\n\nTable 4: Heritage Health results on post-processing approach from Hardt et al."
      ],
      "figure_type": "plot",
      "sub_figures": [],
      "quality_score": 0.95,
      "metadata": {}
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_8",
      "figure_number": 5,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg",
      "image_filename": "2109.03952_page0_fig7.jpg",
      "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ",
      "context_before": "0HJDQ\u0003 ZDV\u0003 ERUQ\u0003 DQG\u0003 UDLVHG\u0003 LQ\u0003 $UOLQJWRQ\u000f\u0003 7H[DV\u0011\u0003 6KH\u0003 JUDGXDWHG\u0003IURP\u0003%D\\ORU\u00038QLYHUVLW\\\u0003/RXLVH\u0003+HUULQJWRQ\u00036FKRRO\u0003 RI\u0003 1XUVLQJ\u0003 LQ\u0003 \u0015\u0013\u0014\u0018\u0003 ZLWK\u0003 KHU\u0003 %DFKHORUV\u0003 RI\u0003 6FLHQFH\u0003 LQ\u0003 1XUVLQJ\u0011\u0003 )ROORZLQJ\u0003 JUDGXDWLRQ\u000f\u0003 VKH\u0003 FRPSOHWHG\u0003 D\u0003 FDUGLDF\u0003 WHOHPHWU\\\u0003LQWHUQVKLS\u0003DW\u00037H[DV\u0003+HDOWK\u00035HVRXUFHV\u0003)RUW\u0003:RUWK\u0011\u0003 6KH\u0003 FRQWLQXHG\u0003 WR\u0003 ZRUN\u0003 WKHUH\u0003 IRU\u0003 WZR\u0003 \\HDUV\u0003 WDNLQJ\u0003 FDUH\u0003 RI\u0003 SRVWVXUJLFDO\u0003 FDUGLDF\u0003 SDWLHQWV\u0003 DQG\u0003 WDNLQJ\u0003 DQ\u0003 DFWLYH\u0003 UROH\u0003 LQ\u0003 SDWLHQW\u0003 HGXFDWLRQ\u0003 FRPPLWWHHV\u0011\u0003 6LQFH\u0003 WKDW\u0003 WLPH\u000f\u0003 VKH\u0003 KDV\u0003 OHIW\u0003 WKH\u0003 KRVSLWDO\u0003 VHWWLQJ\u0003 DQG\u0003 KDV\u0003 MRLQHG\u0003 WKH\u0003 SUDFWLFH\u0011\u0003 0HJDQ\u0003 LV\u0003 UHFHQWO\\\u0003 PDUULHG\u0003 DQG\u0003 KDV\u0003 D\u0003 UHVFXH\u0003 &KLKXDKXD\u0003 7HUULHU\u0003 PL[\u0003 SXS\u0011\u0003 ,Q\u0003 KHU\u0003 IUHH\u0003 WLPH\u000f\u0003 VKH\u0003 ORYHV\u0003 VSHQGLQJ\u0003 WLPH\u0003 ZLWK\u0003 IDPLO\\\u0003 DQG\u0003 IULHQGV\u000f\u0003 WU\\LQJ\u0003 QHZ\u0003 IRRGV\u000f\u0003 DQG\u0003 FDWFKLQJ\u0003 XS\u0003 RQ\u0003 D\u0003 JRRG ERRN\u0011\n\n[Section: 1RW\u0003'HELDVHG\u00030RGHO]\n\n0HJDQ\u0003 ZDV\u0003 ERUQ\u0003 DQG\u0003 UDLVHG\u0003 LQ\u0003 $UOLQJWRQ\u000f\u0003 7H[DV\u0011\u0003 6KH\u0003 JUDGXDWHG\u0003IURP\u0003%D\\ORU\u00038QLYHUVLW\\\u0003/RXLVH\u0003+HUULQJWRQ\u00036FKRRO\u0003 RI\u0003 1XUVLQJ\u0003 LQ\u0003 \u0015\u0013\u0014\u0018\u0003 ZLWK\u0003 KHU\u0003 %DFKHORUV\u0003 RI\u0003 6FLHQFH\u0003 LQ\u0003 1XUVLQJ\u0011\u0003 )ROORZLQJ\u0003 JUDGXDWLRQ\u000f\u0003 VKH\u0003 FRPSOHWHG\u0003 D\u0003 FDUGLDF\u0003 WHOHPHWU\\\u0003LQWHUQVKLS\u0003DW\u00037H[DV\u0003+HDOWK\u00035HVRXUFHV\u0003)RUW\u0003:RUWK\u0011\u0003 6KH\u0003 FRQWLQXHG\u0003 WR\u0003 ZRUN\u0003 WKHUH\u0003 IRU\u0003 WZR\u0003 \\HDUV\u0003 WDNLQJ\u0003 FDUH\u0003 RI\u0003 SRVWVXUJLFDO\u0003 FDUGLDF\u0003 SDWLHQWV\u0003 DQG\u0003 WDNLQJ\u0003 DQ\u0003 DFWLYH\u0003 UROH\u0003 LQ\u0003 SDWLHQW\u0003 HGXFDWLRQ\u0003 FRPPLWWHHV\u0011\u0003 6LQFH\u0003 WKDW\u0003 WLPH\u000f\u0003 VKH\u0003 KDV\u0003 OHIW\u0003 WKH\u0003 KRVSLWDO\u0003 VHWWLQJ\u0003 DQG\u0003 KDV\u0003 MRLQHG\u0003 WKH\u0003 SUDFWLFH\u0011\u0003 0HJDQ\u0003 LV\u0003 UHFHQWO\\\u0003 PDUULHG\u0003 DQG\u0003 KDV\u0003 D\u0003 UHVFXH\u0003 &KLKXDKXD\u0003 7HUULHU\u0003 PL[\u0003 SXS\u0011\u0003 ,Q\u0003 KHU\u0003 IUHH\u0003 WLPH\u000f\u0003 VKH\u0003 ORYHV\u0003 VSHQGLQJ\u0003 WLPH\u0003 ZLWK\u0003 IDPLO\\\u0003 DQG\u0003 IULHQGV\u000f\u0003 WU\\LQJ\u0003 QHZ\u0003 IRRGV\u000f\u0003 DQG\u0003 FDWFKLQJ\u0003 XS\u0003 RQ\u0003 D\u0003 JRRG\u0003 ERRN\u0011",
      "context_after": "",
      "referring_paragraphs": [
        "We also included some additional qualitative results from the experiments on non-tabular data in Fig. 5.",
        "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   \nAccuracy vs EQOP (UCI Adult)"
      ],
      "figure_type": "architecture",
      "sub_figures": [],
      "quality_score": 0.8,
      "metadata": {}
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_9",
      "figure_number": 6,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg",
      "image_filename": "2109.03952_page0_fig8.jpg",
      "caption": "Figure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.",
      "context_before": "",
      "context_after": "Accuracy vs EQOP (Heritage Health)   \nFigure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets.\n\nTable 3: Adult results on post-processing approach from Hardt et al. vs our attention method when all problematic features are zeroed out.   \n\n<table><tr><td></td><td>Accuracy</td><td>SPD</td><td>Accuracy</td><td>EQOP</td><td>Accuracy</td><td>EQOD</td></tr><tr><td>Attention (Ours)</td><td>0.77 (0.006)</td><td>0.012 (0.003)</td><td>0.81 (0.013)</td><td>0.020 (0.019)</td><td>0.81 (0.021)</td><td>0.027 (0.023)</td></tr><tr><td>Hardt et al.</td><td>0.77 (0.012)</td><td>0.013 (0.005)</td><td>0.83 (0.005)</td><td>0.064 (0.016)</td><td>0.81 (0.007)</td><td>0.047 (0.014)</td></tr></table>\n\nTable 4: Heritage Health results on post-processing approach from Hardt et al. vs our attention method when all problematic features are zeroed out.   \n\n<table><tr><td></td><td>Accuracy</td><td>SPD</td><td>Accuracy</td><td>EQOP</td><td>Accuracy</td><td>EQOD</td></tr><tr><td>Attention (Ours)</td><td>0.68 (0.004)</td><td>0.04 (0.015)</td><td>0.68 (0.015)</td><td>0.15 (0.085)</td><td>0.68 (0.015)</td><td>0.10 (0.085)</td></tr><tr><td>Hardt et al.</td><td>0.68 (0.005)</td><td>0.05 (0.018)</td><td>0.75 (0.001)</td><td>0.20 (0.033)</td><td>0.69 (0.012)</td><td>0.19 (0.031)</td></tr></table>",
      "referring_paragraphs": [
        "Accuracy vs EQOP (Heritage Health)   \nFigure 6: Accuracy vs equality of opportunity curves for UCI Adult and Heritage Health datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2109.03952_page0_fig9.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig8.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig9.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_11",
      "figure_number": 7,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg",
      "image_filename": "2109.03952_page0_fig10.jpg",
      "caption": "Figure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
      "context_before": "Accuracy vs EQOD (UCI Adult)",
      "context_after": "Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets.",
      "referring_paragraphs": [
        "Accuracy vs EQOD (Heritage Health)   \nFigure 7: Accuracy vs equalized odds curves for UCI Adult and Heritage Health datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2109.03952_page0_fig11.jpg"
      ],
      "quality_score": 0.85,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig10.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig11.jpg"
        ],
        "panel_count": 2
      }
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_13",
      "figure_number": 8,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig12.jpg",
      "image_filename": "2109.03952_page0_fig12.jpg",
      "caption": "Figure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.",
      "context_before": "Top Problematic Features from Adult",
      "context_after": "Top Problematic Features from Health   \nFigure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.",
      "referring_paragraphs": [
        "Top Problematic Features from Health   \nFigure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets."
      ],
      "figure_type": "other",
      "sub_figures": [],
      "quality_score": 0.85,
      "metadata": {}
    },
    {
      "doc_id": "2109.03952",
      "figure_id": "2109.03952_fig_14",
      "figure_number": 9,
      "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg",
      "image_filename": "2109.03952_page0_fig13.jpg",
      "caption": "Figure 9: Results from the real-world datasets. Note that in our $\\hat { y } _ { z }$ notation we replaced indexes with actual feature names for clarity in these results on real-world datasets as there is not one universal indexing schema, but the feature names are more universal and discriptive for this case. Labels on the points represent the feature name that was removed (zeroed out) according to our $\\hat { y } _ { z }$ notation. The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.",
      "context_before": "Top Problematic Features from Health   \nFigure 8: Top three features for each fairness definition removing which caused the most benefit in improving the corresponding fairness definition. The percentage of improvement upon removal is marked on the $y$ -axis for adult and heritage health datasets.",
      "context_after": "",
      "referring_paragraphs": [
        "Figure 9: Results from the real-world datasets."
      ],
      "figure_type": "plot",
      "sub_figures": [
        "2109.03952_page0_fig14.jpg"
      ],
      "quality_score": 0.7,
      "metadata": {
        "sub_image_paths": [
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig13.jpg",
          "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig14.jpg"
        ],
        "panel_count": 2
      }
    }
  ]
}