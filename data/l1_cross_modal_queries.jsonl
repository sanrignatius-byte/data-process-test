{"query_id": "l1_1104.3913_1104.3913_fig_1_0", "query": "How does Figure 2 visually represent the definition S₀ = G₀ ∩ S from the caption?", "answer": "The dashed line separates G₀ (above) from G₁ (below), with S₀ shown as the portion of S within G₀ (top half of the circle) and S₁ as the portion within G₁ (bottom half). This matches the caption's definition of S₀ as the intersection of S and G₀.", "doc_id": "1104.3913", "figure_id": "1104.3913_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg", "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$", "figure_type": "diagram", "evidence_from_figure": "S₀ label above dashed line, S₁ label below dashed line", "evidence_from_text": "Caption stating S₀ = G₀ ∩ S", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1104.3913_1104.3913_fig_1_1", "query": "Why are members of S over-represented in G₀ and under-represented in G₁ as mentioned in the text?", "answer": "The figure shows S₀ (part of S in G₀) as the upper portion of the circle above the dashed line, while S₁ (in G₁) is the lower portion. This visual partition aligns with the text's claim that S is over-represented in G₀ and under-represented in G₁.", "doc_id": "1104.3913", "figure_id": "1104.3913_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1104.3913/1104.3913/hybrid_auto/images/1104.3913_page0_fig0.jpg", "caption": "Figure 2: $S _ { 0 } = G _ { 0 } \\cap S$ , $T _ { 0 } = G _ { 0 } \\cap T$", "figure_type": "diagram", "evidence_from_figure": "S₀ above dashed line, S₁ below dashed line", "evidence_from_text": "Text stating 'members of S are over-represented in G₀ and under-represented in G₁'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_3_2", "query": "What is the total number of tweets collected from the Streaming API during the study period, and how does the daily trend in Figure 2 relate to this total?", "answer": "The text states 528,592 tweets were collected from the Streaming API. The figure shows daily fluctuations (e.g., peaks around 40,000) that collectively sum to this total.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg", "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.", "figure_type": "plot", "evidence_from_figure": "Daily tweet counts for Streaming API (dotted line) across the date range", "evidence_from_text": "Text states '528,592 tweets from the Streaming API'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_3_3", "query": "On December 27, 2011, which data source had a higher peak tweet count according to Figure 2, and what was the approximate count?", "answer": "Firehose had a peak of approximately 90,000 tweets on December 27, 2011.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg", "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.", "figure_type": "plot", "evidence_from_figure": "Peak of solid red line on Dec 27", "evidence_from_text": "Text confirms Firehose data collection but does not specify peak values", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_3_4", "query": "Why does Firehose show higher daily tweet counts than Streaming API in Figure 2 despite identical parameters as described in the text?", "answer": "Firehose provides a full dataset (100% sample), while Streaming API is a probabilistic sample (e.g., 1% sample), explaining the higher counts despite identical parameters.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig2.jpg", "caption": "Figure 2: Raw tweet counts for each day from both the Streaming API and the Firehose.", "figure_type": "plot", "evidence_from_figure": "Consistently higher Firehose line vs. Streaming API", "evidence_from_text": "Text states 'exactly the same parameters' but implies data source differences", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_4_5", "query": "What is the median coverage value shown in Figure 3, and which day was selected for analysis based on this median value according to the text?", "answer": "The median coverage value is approximately 0.3 (from the box plot's median line), and December 29th was selected as the median day for correlation analysis.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg", "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.", "figure_type": "plot", "evidence_from_figure": "Median line within the box plot at ~0.3", "evidence_from_text": "Text states: 'median (December 29th)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_4_6", "query": "How do the whiskers in Figure 3 relate to the selection of days for correlation analysis as described in the text?", "answer": "The whiskers represent extreme coverage values, which the text uses to select days with minimum (December 27th) and upper quartile (December 18th) coverage levels for analysis.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig3.jpg", "caption": "Figure 3: Distribution of coverage for the Streaming data by day. Whiskers indicate extreme values.", "figure_type": "plot", "evidence_from_figure": "Whiskers extending to extreme values", "evidence_from_text": "Text lists 'minimum (December 27th)' and 'upper quartile (December 18th)' as selected days", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_5_7", "query": "Why does the correlation coefficient τ_β drop sharply between n=0 and n=200 in Figure 4?", "answer": "The sharp drop indicates low coverage of top hashtags in the Streaming API data relative to Firehose, as explained by the text's description of τ_β calculation involving concordant/discordant pairs. Initial low n values reflect insufficient sampling to capture Firehose characteristics.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_5", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg", "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .", "figure_type": "plot", "evidence_from_figure": "The steep decline in all data series between n=0 and n=200", "evidence_from_text": "Text defining τ_β as a rank correlation statistic measuring concordant/discordant pairs", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_5_8", "query": "What does the convergence of all lines at n > 400 in Figure 4 imply about the Streaming API's ability to represent Firehose data?", "answer": "The convergence indicates that with sufficient coverage (n > 400), the Streaming API's top hashtag distribution stabilizes to match Firehose characteristics, as referenced in the text's goal of understanding how sampling affects results.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_5", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig4.jpg", "caption": "Figure 4: Relationship between $n$ - number of top hashtags, and the correlation coefficient, $\\tau _ { \\beta }$ .", "figure_type": "plot", "evidence_from_figure": "All series (Min, Q1, Median, Q3, Max) converging near τ_β = 0.6 for n > 400", "evidence_from_text": "Text stating the investigation aims to understand 'how well the characteristics of the sampled data match those of the Firehose'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_6_9", "query": "What do the Min, Q1, Median, Q3, and Max lines represent in Figure 5?", "answer": "They represent the minimum, first quartile, median, third quartile, and maximum correlation coefficients (tau_beta) across different random samples of top hashtags, as indicated by the legend and the caption's reference to 'different levels of coverage'.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_6", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg", "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.", "figure_type": "plot", "evidence_from_figure": "Legend labels (Min, Q1, Median, Q3, Max) and line trends", "evidence_from_text": "Caption stating 'different levels of coverage' and text defining tau_beta", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_6_10", "query": "Why does tau_beta decrease as n increases in Figure 5, and how does this relate to topic modeling?", "answer": "The decreasing tau_beta as n increases suggests reduced correlation between sampled hashtags and Firehose data, implying that larger sets of top hashtags may include less relevant topics. This aligns with the text's discussion of topic modeling for large Twitter datasets where increasing n could dilute topic coherence.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_6", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig5.jpg", "caption": "Figure 5: Random sampling of Firehose data. Relationship between n - number of top hashtags, and $\\tau _ { \\beta }$ - the correlation coefficient for different levels of coverage.", "figure_type": "plot", "evidence_from_figure": "Downward trend of all lines as n increases", "evidence_from_text": "Text stating topic modeling is used for large Twitter datasets and the definition of tau_beta", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_10_11", "query": "In Figure 6, what is the maximum divergence value observed despite the x-axis extending to 0.20?", "answer": "≤0.15, as stated in the text caption.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_10", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg", "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ", "figure_type": "plot", "evidence_from_figure": "X-axis extends to 0.20 with a negligible bar at 0.15-0.20", "evidence_from_text": "Caption states 'No divergence was > 0.15'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_10_12", "query": "What does the histogram's left-skewed shape in Figure 6 imply about the average divergence (μ) of matched topics?", "answer": "The left-skewed distribution suggests μ is low, as most values cluster near zero.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_10", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig9.jpg", "caption": "Figure 6: The Jensen-Shannon divergence of the matched topics at different levels of coverage. The $\\mathbf { X }$ -axis is the binned divergence. No divergence was $> 0 . 1 5$ . The y-axis is the count of each bin. $\\mu$ is the average divergence of the matched topics, $\\sigma$ is the standard deviation.   ", "figure_type": "plot", "evidence_from_figure": "Tall bar at low divergence (0.00-0.05) with decreasing counts toward higher values", "evidence_from_text": "Text defines μ as the average divergence of matched topics", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_14_13", "query": "What does the red line's position relative to the blue curve in Figure 7 indicate about the Streaming data's Jensen-Shannon divergence?", "answer": "The red line positioned to the right of the blue curve's peak indicates that the Streaming data's average divergence is significantly higher than the mean of the random data distribution, quantified by the z-score as the number of standard deviations from the mean.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_14", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg", "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.", "figure_type": "plot", "evidence_from_figure": "red line's position relative to blue curve's peak", "evidence_from_text": "caption defining z as standard deviations from random data mean", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1306.5204_1306.5204_fig_14_14", "query": "How is the z-score for the Streaming data calculated in Figure 7?", "answer": "The z-score is calculated as the number of standard deviations the Streaming data's average (red line) is from the mean of the random data's distribution (blue curve), as defined in the figure caption.", "doc_id": "1306.5204", "figure_id": "1306.5204_fig_14", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1306.5204/1306.5204/hybrid_auto/images/1306.5204_page0_fig13.jpg", "caption": "Figure 7: The distribution of average Jensen-Shannon divergences in the random data (blue curve), with the single average obtained through the Streaming data (red, vertical line). $z$ indicates the number of standard deviations the Streaming data is from the mean of the random samples.", "figure_type": "plot", "evidence_from_figure": "red line's location on x-axis relative to blue curve", "evidence_from_text": "caption stating 'z indicates the number of standard deviations the Streaming data is from the mean of the random samples'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_1_15", "query": "What date does the peak in Figure 1 occur, and how does this align with the Gezi Protests' timeline as discussed in the text?", "answer": "The peak occurs on May 28 GHT. This aligns with the text's context of analyzing protest-related hashtags, where the peak likely corresponds to heightened protest activity during the Gezi Protests.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg", "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)", "figure_type": "plot", "evidence_from_figure": "Peak date (May 28 GHT) on x-axis", "evidence_from_text": "Text mentions 'Gezi Protests' as the subject of hashtag analysis", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_1_16", "query": "How does the Topsy data source in Figure 1 relate to the text's claim about Twitter dominating big data research?", "answer": "Topsy is a Twitter analytics platform, so the data reflects Twitter-based hashtag usage. This directly supports the text's assertion that big data research focuses disproportionately on Twitter, as seen in ICWSM papers.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig0.jpg", "caption": "Figure 1: The frequency of top 20 hashtags associated with Gezi Protests. (Banko and Babacan, 2013)", "figure_type": "plot", "evidence_from_figure": "Topsy logo in top-right corner", "evidence_from_text": "Text states 'big data research focuses disproportionately on Twitter' and cites ICWSM papers using Twitter data", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_2_17", "query": "Why does Figure 2 show 1,404 retweets but the caption states it was retweeted 'mostly in disgust'?", "answer": "The high retweet count (1,404) reflects quantitative engagement, but the text explains retweets can have contradictory meanings—here, negative sentiment (disgust) despite the metric suggesting popularity. This illustrates how machine-driven data fails to capture contextual nuances.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig1.jpg", "caption": "Figure 2: Retweeted widely, but mostly in disgust", "figure_type": "photo", "evidence_from_figure": "retweet count (1,404) and caption text", "evidence_from_text": "text stating 'the same act can have multiple, even contradictory meanings' and 'interpreting online imprints engages layers of complexity'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_2_18", "query": "How does Figure 2 exemplify the 'engagement invisible to machines' concept from the text?", "answer": "The tweet’s high retweet count (1,404) would typically signal positive engagement to algorithms, but the text clarifies the retweets were driven by disgust. This demonstrates how machine analyses miss contextual sentiment, as discussed in the section on subtweets and hate-links.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig1.jpg", "caption": "Figure 2: Retweeted widely, but mostly in disgust", "figure_type": "photo", "evidence_from_figure": "retweet count (1,404) and negative sentiment in caption", "evidence_from_text": "text stating 'All these practices can blind big data analyses to this mode of activity and engagement' and 'subtweets, hate-links, screen captures'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_3_19", "query": "How does Figure 3 visually demonstrate the text's definition of subtweeting as 'algorithmically invisible'?", "answer": "The tweets in Figure 3 reference each other without naming individuals (e.g., 'Bazları fosseptik çukurunda gezelerin...' and similar Turkish text), matching the text's definition of subtweeting as a reference without explicit name mention that would evade algorithmic detection.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg", "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.", "figure_type": "example", "evidence_from_figure": "Absence of named individuals in tweets and mutual references", "evidence_from_text": "Definition stating subtweeting involves 'referring to a person algorithmically invisible to that person'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_3_20", "query": "Why were the subtweet exchange in Figure 3 reported in newspapers despite being algorithmically invisible?", "answer": "The exchange was reported because the context was clear to humans familiar with the conversation (per text), even though machine algorithms would miss it due to the absence of explicit names (per text's definition of subtweeting).", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig2.jpg", "caption": "Figure 3: Two peeople “subtweetting” each other  without mentionning names. Thee exchange was cclear enough, hoowever, to be re-- ported in nnewspapers.", "figure_type": "example", "evidence_from_figure": "Mutual reference between tweets without names", "evidence_from_text": "Text stating 'the exchange was clear enough... to be reported in newspapers' despite algorithmic invisibility", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_4_21", "query": "What does 'caps' refer to in Figure 4, and how does the embedded screenshot demonstrate this practice?", "answer": "'Caps' refers to Twitter users referencing others' tweets through screen captures instead of direct quotes or mentions. The embedded screenshot in Figure 4 shows reply threads visually embedded within the original tweet, illustrating how screen captures obscure algorithmic engagement tracking.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg", "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”", "figure_type": "photo", "evidence_from_figure": "Embedded screenshot of reply threads showing visual references instead of quoted text", "evidence_from_text": "Text defines 'caps' as screen captures replacing quotes/mentions and notes they 'add to the invisibility of engagement to algorithms'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_4_22", "query": "Why is the 'caps' practice described as 'algorithmically invisible' in Figure 4?", "answer": "The practice is algorithmically invisible because screen captures (like the embedded replies in Figure 4) bypass Twitter's algorithmic detection systems that track direct quotes, mentions, or link-based interactions. This makes engagement patterns undetectable to automated systems.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig3.jpg", "caption": "Figure 4. This ppractice is so wwidespread thaat a single hourr following the saame purposive  sample resultted in more thhan 300 instancees in which useers employed suuch “caps.”", "figure_type": "photo", "evidence_from_figure": "Embedded screenshot showing replies presented as visual references rather than algorithmic-friendly text", "evidence_from_text": "Text states 'Usiing screen capttures rather thaan quotes is an other practice thhat adds to thhe invisibility of engagemennt to algorithmss'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_5_23", "query": "What specific engagement metric in Figure 5 is referenced in the text as being manipulated by activists to create trends?", "answer": "The retweet count (64) is the metric, as the text states activists attempt to make hashtags trend by manipulating metrics like retweets.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg", "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.", "figure_type": "example", "evidence_from_figure": "64 retweets", "evidence_from_text": "political activists... often undertake deliberate attempts to make a hashtag 'trend'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_5_24", "query": "How does the 'Following' button in Figure 5 illustrate the concept of 'reflexivity' discussed in the text after the figure?", "answer": "The 'Following' button represents a user's action (following the account), which exemplifies how humans respond to metrics (the count of followers/following) as part of reflexivity, where users actively engage with the metrics they are being measured by.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig4.jpg", "caption": "Figure 5: Cleaar meaning onlyy in context and ttime.", "figure_type": "example", "evidence_from_figure": "Following button", "evidence_from_text": "Unlike disease vectors... humans understand, evaluate and respond to the same metrics that big data researchers are measuring", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_6_25", "query": "What is the hashtag used in Figure 6, and how does it relate to the text's explanation of political activists gaming social media metrics?", "answer": "The hashtag is #stoplyingCNN, which exemplifies the text's claim that activists deliberately create trends to influence social media metrics.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg", "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]", "figure_type": "photo", "evidence_from_figure": "#stoplyingCNN visible in the tweet", "evidence_from_text": "text states activists make hashtags trend to game metrics", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_6_26", "query": "How does the 4,206 retweets in Figure 6 support the text's argument about humans gaming metrics?", "answer": "The high retweet count demonstrates the campaign's success in trending, aligning with the text's assertion that activists intentionally manipulate metrics to gain attention.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg", "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]", "figure_type": "photo", "evidence_from_figure": "4,206 retweets shown in the figure", "evidence_from_text": "text explains humans deliberately make hashtags trend", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1403.7400_1403.7400_fig_6_27", "query": "What country is the mayor from in Figure 6, and how does this compare to the text's example of Bahrain?", "answer": "The mayor is from Turkey (Ankara), while the text uses Bahrain as a separate example, illustrating that hashtag campaigns occur in multiple countries.", "doc_id": "1403.7400", "figure_id": "1403.7400_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1403.7400/1403.7400/hybrid_auto/images/1403.7400_page0_fig5.jpg", "caption": "Figure 6: Ankkara Mayor leadsds a hashtag campmpaign that will eeventually trendd worldwide. [Tr anslation: Yes…… I’m announcingg ur hashtag. #sstoplyingCNN]", "figure_type": "photo", "evidence_from_figure": "caption states 'Ankkara Mayor'", "evidence_from_text": "text mentions Bahrain as an example of countries with such campaigns", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1409.0575_1409.0575_fig_1_28", "query": "How does Figure 2 illustrate the 'fine-grained object classification' capability mentioned in the text, specifically regarding the replacement of 90 synsets with dog breeds in ILSVRC2012?", "answer": "Figure 2 shows diverse object categories (e.g., Candle, Oyster) with varying attributes, demonstrating the dataset's ability to distinguish subtle differences. The text explains that replacing synsets with dog breeds (e.g., 120 breeds) enables fine-grained evaluation, which aligns with the figure's detailed categorization of objects based on attributes like Color Distinctiveness and Shape Distinctiveness.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg", "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.", "figure_type": "example", "evidence_from_figure": "Examples of object categories with varying attributes (e.g., 'Color Distinctiveness' row showing Mug vs. Red Wine)", "evidence_from_text": "In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1409.0575_1409.0575_fig_1_29", "query": "What is the significance of the 'Real-world Size' attribute in Figure 2, and how does it relate to the dataset construction methodology described in the text?", "answer": "The 'Real-world Size' attribute shows examples ranging from small (Orange) to large (Airliner), indicating the dataset includes objects of varying physical sizes. This aligns with the text's description of collecting diverse images via multiple search engines and queries to ensure comprehensive coverage for classification tasks.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig0.jpg", "caption": "Figure 2. The synsets have remained consistent since year 2012. Appendix A provides the complete list of object categories used in ILSVRC2012-2014.", "figure_type": "example", "evidence_from_figure": "Real-world Size row with Low/High scale and examples like Orange (Low) and Airliner (High)", "evidence_from_text": "collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1409.0575_1409.0575_fig_75_30", "query": "In Figure 11, what type of objects are depicted, and how does this relate to the 'optimistic' method performance metrics described in the text?", "answer": "The figure shows pencils as example objects from the dataset. The 'optimistic' method refers to the best performance of any entry submitted to ILSVRC2012-2014, which is evaluated on such objects.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_75", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg", "caption": "Figure 11.", "figure_type": "photo", "evidence_from_figure": "Visual content of pencils in Figure 11", "evidence_from_text": "Fig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1409.0575_1409.0575_fig_75_31", "query": "How does the visual arrangement of pencils in Figure 11 relate to the 'average scale of object' metric mentioned in the text?", "answer": "The bundled pencils exemplify objects whose average scale (fraction of image area occupied) is measured in Fig. 13, where larger object instances like this bundle would have higher scale values.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_75", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig74.jpg", "caption": "Figure 11.", "figure_type": "photo", "evidence_from_figure": "Bundled arrangement of pencils in Figure 11", "evidence_from_text": "Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1409.0575_1409.0575_fig_93_32", "query": "What does the black circle on the 'Low Man-made' bar represent in Figure 1?", "answer": "The black circle represents the average accuracy of the model on all object classes that fall into the 'Low Man-made' bin, as specified in the figure caption.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_93", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg", "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.", "figure_type": "plot", "evidence_from_figure": "Black circle positioned atop the 'Low Man-made' bar", "evidence_from_text": "Caption states: 'The black circle is the average accuracy of the model on all object classes that fall into each bin.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1409.0575_1409.0575_fig_93_33", "query": "Why are color bars present for all bins in Figure 1 despite the caption noting some bins may be missing them?", "answer": "All bins in this figure have at least 5 object classes remaining after scale normalization, satisfying the condition for color bar inclusion mentioned in the caption.", "doc_id": "1409.0575", "figure_id": "1409.0575_fig_93", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1409.0575/1409.0575/hybrid_auto/images/1409.0575_page0_fig92.jpg", "caption": "Figure 1. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars show the model accuracy averaged across the remaining classes. Error bars show the $9 5 \\%$ confidence interval obtained with bootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization. For example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain.", "figure_type": "plot", "evidence_from_figure": "All four bins ('None'/'Low' under Man-made/Natural) show color bars", "evidence_from_text": "Caption states: 'Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_1_34", "query": "In Figure 1, what is the 95th percentile score for male students in the repaired distribution (black curve), as specified in the text?", "answer": "625", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg", "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.", "figure_type": "plot", "evidence_from_figure": "Position of the black curve's 95th percentile point", "evidence_from_text": "Text states: 'Male students who originally had scores in the 95th percentile... are given scores of 625 in the 95th percentile of the new distribution'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_1_35", "query": "According to the text, the repaired distribution (black curve) has σ=75. How does the width of the black curve in Figure 1 compare to the original male (red) and female (blue) distributions?", "answer": "The black curve's spread is intermediate between the red curve (σ=50) and blue curve (σ=100), consistent with σ=75.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig0.jpg", "caption": "Fig. 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores (Y) for $X =$ female, with $\\mu = 5 5 0 , \\sigma = 1 0 0 ,$ , while the red curve shows the distribution of SAT scores for $X \\ = \\ \\mathtt { m a l e }$ , with $\\mu = 4 0 0 , \\sigma = 5 0 .$ The resulting fully repaired data is the distribution in black, with $\\mu = 4 7 5 , \\sigma = 7 5$ . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in $\\bar { Y } _ { \\cdot }$ , while women with scores of 625 in $\\bar { Y }$ originally had scores of 750.", "figure_type": "plot", "evidence_from_figure": "Visual comparison of curve widths", "evidence_from_text": "Text provides σ values: black curve σ=75, red curve σ=50, blue curve σ=100", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_2_36", "query": "In Figure 2, do all data points to the right of the vertical BER threshold line have a DI value above τ=0.8?", "answer": "Yes, as per the certification algorithm guarantee stated in the text, points to the right of the BER threshold are guaranteed to be above τ=0.8, which is visually confirmed by the figure.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg", "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.", "figure_type": "plot", "evidence_from_figure": "Data points to the right of the vertical line in all panels fall above the horizontal DI=0.8 line", "evidence_from_text": "The caption states: 'the certification algorithm guarantees that points to the right of the BER threshold are also above τ=0.8'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_2_37", "query": "According to the figure caption, what is stated about geometric repair results compared to combinatorial repair results in Figure 2?", "answer": "The caption states that geometric repair results follow the same pattern as the combinatorial repair results shown in the figure.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig1.jpg", "caption": "Fig. 2: Lack of predictability (BER) of the protected attributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above $\\tau = 0 . 8$ , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern.", "figure_type": "plot", "evidence_from_figure": "The figure shows combinatorial repair results", "evidence_from_text": "The caption states: 'we only show results using the combinatorial repair, but the geometric repair results follow the same pattern'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_3_38", "query": "What is the significance of the red vertical line in Figure 3, and how does it relate to the legal disparate impact threshold mentioned in the text?", "answer": "The red vertical line at DI = 0.8 represents the legal threshold τ = 0.8, where only points with DI ≥ 0.8 are considered legal as per the text's definition.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg", "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.", "figure_type": "plot", "evidence_from_figure": "Red vertical line at DI = 0.8 in all plots", "evidence_from_text": "Text states 'only points with DI ≥ τ = 0.8 are legal'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_3_39", "query": "How does the utility (1-BER) trend with increasing Disparate Impact for the Geometric Repair method on the German Credit dataset, and why does this trend occur?", "answer": "Utility (1-BER) decreases as Disparate Impact increases for Geometric Repair on German Credit, which aligns with the text stating that utility decays as fairness (DI) increases.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig2.jpg", "caption": "Fig. 3: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and German Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with $\\mathsf { D } \\mathsf { I } \\ge \\tau = 0 . 8$ are legal. $\\mathsf { D } \\mathsf { I } = 1 . 0$ represents full fairness.", "figure_type": "plot", "evidence_from_figure": "Downward trend in Geometric Repair/German Credit plot", "evidence_from_text": "Text states 'results demonstrate the expected decay over utility as fairness increases'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_4_40", "query": "Why does the 'Race and Gender' line in Figure 4 align closely with the 'Gender' line at high Disparate Impact values (e.g., DI=1.0), despite combining both attributes?", "answer": "The text states that utility loss over the joint distribution is close to the maximum of utility loss over each protected attribute. Since Gender has a higher utility loss than Race, the joint distribution's utility loss matches Gender's higher loss, explaining the alignment.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg", "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.", "figure_type": "plot", "evidence_from_figure": "The 'Race and Gender' line's value at DI=1.0 matches the 'Gender' line's value (~0.719)", "evidence_from_text": "The text states: 'the utility loss over the joint distribution is close to the maximum of the utility loss over each protected attribute considered on its own'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_4_41", "query": "How does the utility trend for the 'Race' attribute in Figure 4 compare to the 'Gender' attribute, and what does this imply about their utility losses?", "answer": "The 'Race' line shows consistently higher utility (less loss) than the 'Gender' line across all Disparate Impact values. This implies Race has a lower utility loss than Gender, making Gender the dominant factor in the joint distribution's utility loss.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig3.jpg", "caption": "Fig. 4: Disparate impact (DI) vs. utility (1-BER) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric repairs follow the same pattern.", "figure_type": "plot", "evidence_from_figure": "Race line remains above Gender line throughout the plot", "evidence_from_text": "The text states the joint distribution's utility loss approaches the maximum of individual attribute losses (Gender's higher loss)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_5_42", "query": "What does the pink plus sign (LFR) represent in the 'Previous Work' section of Figure 5, and how does its performance compare to the geometric approach for the Adult Income Data?", "answer": "The pink plus sign (LFR) represents Learned Fair Representations [26]. In the Adult Income Data subplot, LFR shows moderate accuracy (≈0.70-0.75) with lower fairness than some geometric approach methods, consistent with the text's observation about algorithm-dependent performance differences.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "evidence_from_figure": "Pink plus sign in Previous Work column, Adult Income Data row", "evidence_from_text": "Legend: LFR, Learned Fair Representations [26]; text discussing algorithm-dependent performance differences", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1412.3756_1412.3756_fig_5_43", "query": "Why does the SVM method (red circles) show higher accuracy in the Geometric approach for the German Credit Data compared to the Combinatorial approach?", "answer": "The SVM method shows higher accuracy in the Geometric approach due to algorithm-specific interactions with the repair method, as highlighted in the text's statement about 'substantial differences in performance depending on specific algorithms,' indicating the need for further study into these relationships.", "doc_id": "1412.3756", "figure_id": "1412.3756_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1412.3756/1412.3756/hybrid_auto/images/1412.3756_page0_fig4.jpg", "caption": "Fig. 5: Zemel fairness vs. accuracy from our combinatorial and geometric partial repairs 0.65 0.80 0.85 0.90 0.95 1.00 LFR0.30 0.85 0.90 0.95 1.00as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; 0.80 0.85 0.90 0.95 1.00 0.8 0.9 1.0 1.1LFR, Learned Fair Representations [26]; FNB, Fair Na¨ıve Bayes [8]; GNB, Gaussian 0.65Na¨ıve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine.", "figure_type": "plot", "evidence_from_figure": "SVM red circles in Geometric German Credit Data (higher accuracy) vs. Combinatorial German Credit Data (lower accuracy)", "evidence_from_text": "Text stating 'Our experiments show a substantial difference in the performance of our repair algorithm depending on the specific algorithms we chose'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1511.00830_1511.00830_fig_1_44", "query": "Which node in Figure 1 represents a nuisance variable according to the text's definitions of uninformative dimensions?", "answer": "Node Z represents the nuisance variable.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg", "caption": "Figure 1: Unsupervised model", "figure_type": "diagram", "evidence_from_figure": "Node Z has an arrow pointing to X, indicating its role in the model's structure.", "evidence_from_text": "The text states: 'Uninformative dimensions are often called 'noise' or 'nuisance variables'.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1511.00830_1511.00830_fig_1_45", "query": "How does the unsupervised model in Figure 1 remove uninformative factors as described in the text?", "answer": "The model processes input data (S) to generate X while excluding the nuisance variable (Z) from the final representation.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig0.jpg", "caption": "Figure 1: Unsupervised model", "figure_type": "diagram", "evidence_from_figure": "Node Z is connected to X, implying it is part of the input but not the final output.", "evidence_from_text": "The text states: 'Many machine learning algorithms can be understood in this way: removing the factors of variation that are uninformative...' ", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1511.00830_1511.00830_fig_2_46", "query": "What is the role of z1 in the semi-supervised model of Figure 2, given the text's definition of z as the latent variable modeling remaining information?", "answer": "z1 serves as a component of the latent variable structure (z) that models remaining information, receiving inputs from y and z2 while influencing the observed variable x, as shown by its connections in the diagram and the text's focus on z as the primary latent variable.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg", "caption": "Figure 2: Semi-supervised model", "figure_type": "diagram", "evidence_from_figure": "z1's connections to y, z2, and x", "evidence_from_text": "definition of z as the continuous latent variable modeling remaining information", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1511.00830_1511.00830_fig_2_47", "query": "How does the observed variable s contribute to the model in Figure 2, as described in the text's explanation of undesired variations?", "answer": "s directly influences x (as shown by the arrow from s to x) but represents undesired variations to be removed, consistent with the text's description of s as the source of variations that the model aims to factor out.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig1.jpg", "caption": "Figure 2: Semi-supervised model", "figure_type": "diagram", "evidence_from_figure": "arrow from s to x", "evidence_from_text": "description of s as 'variations that we want to remove'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1511.00830_1511.00830_fig_16_48", "query": "In Figure 5a, why do the data points cluster primarily by lighting conditions?", "answer": "The text explains that lighting conditions (s) are 'well identifiable with almost perfect accuracy from both RF and LR' in the original representation x, causing clustering by lighting as seen in the t-SNE plot.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_16", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg", "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.", "figure_type": "plot", "evidence_from_figure": "Grouped numerical labels (e.g., 1, 2, 3) in panel (a)", "evidence_from_text": "Text states: 'on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1511.00830_1511.00830_fig_16_49", "query": "How does the VFAE's latent space (Figure 5b) demonstrate invariance to lighting?", "answer": "The text states that VFAE learns representations invariant to lighting while retaining information; in Figure 5b, reduced clustering by lighting labels compared to panel (a) shows the model successfully disentangles lighting from identity features.", "doc_id": "1511.00830", "figure_id": "1511.00830_fig_16", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1511.00830/1511.00830/hybrid_auto/images/1511.00830_page0_fig15.jpg", "caption": "Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent $\\mathbf { z } _ { 1 }$ from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.", "figure_type": "plot", "evidence_from_figure": "Less distinct lighting-based clustering in panel (b)", "evidence_from_text": "Text explains: 'learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1602.05352_1602.05352_fig_1_50", "query": "Why does the naive estimator fail for Y₁ as shown in Figure 1?", "answer": "The naive estimator fails because it evaluates predictions on observed ratings without accounting for selection bias. In Figure 1, Y₁ overestimates Drama ratings (e.g., Horror Lovers' Drama rating is 5 in Y₁ vs. 3 in Y), but the observed data is MNAR, causing standard metrics like MAE/MSE to misjudge performance.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg", "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .", "figure_type": "table", "evidence_from_figure": "Y₁ shows Drama ratings of 5 for both user types vs. 3 in Y", "evidence_from_text": "Text states 'naive estimator leads to gross misjudgment for Y₁ and Y₂' due to MNAR selection bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1602.05352_1602.05352_fig_1_51", "query": "How does the Drama rating for Horror Lovers differ between Y and Y₁ in Figure 1?", "answer": "In Y, Horror Lovers' Drama rating is 3, while in Y₁ it is 5. This discrepancy demonstrates how selection bias (MNAR data) causes the naive estimator to misjudge prediction accuracy.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig0.jpg", "caption": "Figure 1. Movie-Lovers toy example. Top row: true rating matrix $Y$ , propensity matrix $P$ , observation indicator matrix $O$ . Bottom row: two rating prediction matrices $\\hat { Y _ { 1 } }$ and $\\hat { Y } _ { 2 }$ , and intervention indicator matrix $\\hat { Y } _ { 3 }$ .", "figure_type": "table", "evidence_from_figure": "Y has 3 in the Drama column for Horror Lovers; Y₁ has 5", "evidence_from_text": "Text explains that MNAR data leads to 'gross misjudgment' in evaluation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1602.05352_1602.05352_fig_4_52", "query": "In Figure 2's right subplot (DCG), why does the IPS estimator's RMSE decrease as alpha increases towards 1.0, despite the increasing selection bias?", "answer": "The IPS estimator corrects for selection bias using propensity scores, so as bias increases (alpha), its performance improves relative to Naive. The text explains that IPS is used in cross-validation with scaled propensities to account for bias.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg", "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.", "figure_type": "plot", "evidence_from_figure": "IPS line trending downward as alpha approaches 1.0 in the right subplot", "evidence_from_text": "Cross-validation with scaled propensities for IPS to handle selection bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1602.05352_1602.05352_fig_4_53", "query": "How does the scaling of propensities in the training folds (as described in the text) affect the Naive estimator's RMSE trend in the left subplot (MSE)?", "answer": "The Naive estimator does not use propensity scores, so the scaling has no effect on its RMSE trend, which is driven by the underlying selection bias.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig3.jpg", "caption": "Figure 2. RMSE of the estimators in the experimental setting as the observed ratings exhibit varying degrees of selection bias.", "figure_type": "plot", "evidence_from_figure": "Naive line's downward trend in the left subplot as alpha increases", "evidence_from_text": "Propensity scaling is specific to the IPS estimator during cross-validation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1602.05352_1602.05352_fig_5_54", "query": "Why does MF-IPS outperform MF-Naive in Figure 3 (left) when α > 0.2?", "answer": "MF-IPS incorporates inverse propensity weighting to address selection bias, which becomes more critical as α increases (indicating higher bias severity). MF-Naive ignores bias, leading to higher MSE. This aligns with the text's explanation that MF-IPS corrects for selection bias in the observed ratings.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg", "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).", "figure_type": "plot", "evidence_from_figure": "MF-IPS line consistently below MF-Naive for α > 0.2", "evidence_from_text": "Section 6.3 explains MF-IPS handles selection bias via ERM", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1602.05352_1602.05352_fig_5_55", "query": "What does the x-axis label 'α' represent in the context of Figure 3 (left)?", "answer": "α represents the severity of selection bias in observed ratings, as stated in the caption: 'as the observed ratings exhibit varying degrees of selection bias (left)'. Higher α values correspond to stronger selection bias.", "doc_id": "1602.05352", "figure_id": "1602.05352_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1602.05352/1602.05352/hybrid_auto/images/1602.05352_page0_fig4.jpg", "caption": "Figure 3. Prediction error (MSE) of matrix factorization methods as the observed ratings exhibit varying degrees of selection bias (left) and as propensity estimation quality degrades (right).", "figure_type": "plot", "evidence_from_figure": "x-axis labeled 'α'", "evidence_from_text": "Caption specifies 'selection bias severity' and Section 6.3 context", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_1_56", "query": "What is the total number of subreddits on Reddit as of June 21, 2015 according to the text, and how does the 'r/IAmA' subreddit in Figure 1 exemplify a specific subreddit purpose?", "answer": "The text states there were 853,000 subreddits as of June 21, 2015. Figure 1 shows 'r/IAmA' as a subreddit where users ask questions and a public figure (Patrick Stewart) answers them, demonstrating a discussion-based purpose distinct from content-sharing subreddits.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg", "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.", "figure_type": "photo", "evidence_from_figure": "r/IAmA subreddit name and its 'Ask Me Anything' purpose", "evidence_from_text": "853,000 subreddits as of June 21, 2015 and statement about subreddits focusing on particular purposes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_1_57", "query": "How does the 'IAmA' subreddit's purpose, as referenced in the figure's caption, align with the text's description of subreddits focusing on particular purposes?", "answer": "The text states subreddits have specific purposes, and Figure 1's caption describes 'IAmA' as a platform for 'ask me anything' sessions where users ask questions and the featured person answers, exemplifying a discussion-focused subreddit purpose.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig0.jpg", "caption": "Figure 1: Reddit interface when visualizing a submission. This is Patrick Stewart’s “AmA” (ask me anything) in “IAmA” (I am a), a submission where he answers users’ questions in the comments. We can see the most upvoted comment and Patrick’s answer right below.", "figure_type": "photo", "evidence_from_figure": "Figure 1 caption stating 'ask me anything' in 'IAmA'", "evidence_from_text": "Text statement: 'each of which focuses on a particular purpose'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_2_58", "query": "What is the definition of an 'active user' in Figure 2(a), and how does it influence the cumulative growth shown?", "answer": "An active user is defined as one with at least one post in a monthly bin. This definition causes the cumulative user count to grow steadily as more users become active over time, resulting in a higher cumulative number of users compared to subreddits.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg", "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.", "figure_type": "plot", "evidence_from_figure": "The cumulative growth trend of users (purple line) exceeding subreddits (teal line) from 2008 onward.", "evidence_from_text": "Active user or subreddit is one that had at least one post in the time bin (discretized by month).", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_2_59", "query": "Why does Figure 2(a) start at 2008 instead of October 2007 as mentioned in the dataset description?", "answer": "The figure starts at 2008 because the cumulative growth data from October 2007 to 2008 is minimal and not visually significant on the logarithmic scale, so the plot focuses on the period where growth is more pronounced.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig1.jpg", "caption": "Figure 2: Figure (a) shows the cumulative growth of Reddit for users and subreddits. Figure (b) shows the number of active users and subreddits in Reddit over time. An active user or subreddit is one that had at least one post (comment or submission) in the time bin we used—here, discretized by month.", "figure_type": "plot", "evidence_from_figure": "X-axis starts at 2008.", "evidence_from_text": "Dataset compiled from October 2007 until May 2015.", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_4_60", "query": "What is the average number of posts per active user in 2010 according to Figure 3(a), and how does the exclusion of inactive users (as per the caption) affect this value?", "answer": "In 2010, the average was approximately 16-17 posts per active user. The exclusion of inactive users ensures the metric reflects only engaged users, making the value representative of active participation.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg", "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "figure_type": "plot", "evidence_from_figure": "The line at 2010 shows a value of 16-17 posts per user", "evidence_from_text": "Caption states 'we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_4_61", "query": "How does the dataset's time frame (October 2007 to May 2015) relate to the x-axis range of Figure 3(a) (2008-2014), and what does this imply about the data's coverage?", "answer": "The x-axis starts at 2008 because the analysis begins after sufficient data accumulation (e.g., first full year), even though the dataset starts in 2007. This implies the figure covers the period where monthly averages are reliably computed, excluding the initial partial year.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg", "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "figure_type": "plot", "evidence_from_figure": "X-axis starts at 2008", "evidence_from_text": "Text states dataset spans October 2007 to May 2015", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_4_62", "query": "What preprocessing step mentioned in the text is directly relevant to the metric 'average number of posts per active user' shown in Figure 3(a)?", "answer": "The exclusion of posts by deleted users (and other filtered posts) ensures the metric only includes active users, directly impacting the calculation of average posts per active user.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig3.jpg", "caption": "Figure 3: In Figure (a), monthly average posts per active user over clock time. In Figure (b), monthly average posts per active users in the user-time referential, i.e., message creation time is measured relative to the user’s first post. Each tick in the x-axis is one year. In both figures (and all later figures), we consider only active users during each month; users that are either temporarily or permanently away from Reddit are not included.", "figure_type": "plot", "evidence_from_figure": "Metric is 'average number of posts per active user'", "evidence_from_text": "Text states 'we did light preprocessing to filter out posts by deleted users...'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_6_63", "query": "In Figure 4a, how does the 2008 cohort's average posts per user compare to the Overall average line after 2010?", "answer": "The 2008 cohort's line consistently exceeds the Overall average line after 2010, indicating higher activity levels. This aligns with the text's context that the Overall line serves as a comparison baseline for cohort-specific trends.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg", "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.", "figure_type": "plot", "evidence_from_figure": "2008 cohort line (solid green) and Overall line (black) after 2010", "evidence_from_text": "Figure 4 caption stating 'black line represents the overall average' and text discussing comparison methodology", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_6_64", "query": "Why does the 2008 cohort's line show a sharp increase around 2009?", "answer": "The sharp increase aligns with Reddit's 'initial extremely rapid expansion from 2008–2009' mentioned in the text, where early adopters (2008 cohort) likely contributed to heightened activity during this growth phase.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig5.jpg", "caption": "Figure 4: Figure (a) shows the average number of posts per active user over clock time and Figure (b) per active user in the user-time referential, both segmented by users’ cohorts. The user cohort is defined by the year of the user’s creation time. For comparison, the black line in Figure (a) represents the overall average.", "figure_type": "plot", "evidence_from_figure": "2008 cohort line's steep rise around 2009", "evidence_from_text": "Text stating 'After an initial extremely rapid expansion from 2008–2009'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_8_65", "query": "In Figure 5, what is the approximate average number of posts per user for cohort 0 at time 0?", "answer": "Approximately 7 posts per user, as observed from the green solid line at time 0 on the x-axis.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg", "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.", "figure_type": "plot", "evidence_from_figure": "Value of the green solid line (cohort 0) at x=0", "evidence_from_text": "Definition of cohort 0 as users surviving up to 1 year", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_8_66", "query": "Why do newer users (cohort 0) not reach the activity levels of older users (cohort 4) according to Figure 5 and the text?", "answer": "Cohort 0 starts with lower activity (≈7 posts) and declines rapidly, while cohort 4 starts higher (≈28 posts) and remains elevated longer. The text explains this cohort effect, indicating new users never match older users' sustained activity.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig7.jpg", "caption": "Figure 5: Each Figure corresponds to one cohort, from 2010 to 2012, left to right. The users for each cohort are further divided in groups based on how long they survived: users that survived up to 1 year are labeled 0, from 1 to 2 years are labeled 1, and so on. For all cohorts, longer-tenured users started at higher activity levels than shorter-tenured ones.", "figure_type": "plot", "evidence_from_figure": "Trend comparison between cohort 0 and 4 lines over time", "evidence_from_text": "Section 4.2 stating 'new cohorts do not catch up' and 'significant cohort effect'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_13_67", "query": "What does the '0' cohort line in Figure 6b show, and how does it relate to the text's claim about cohort trends?", "answer": "The '0' cohort line shows a sharp initial drop in comment length followed by an increase. This aligns with the text's claim that 'for any individual cohort, it increases after a sharp initial drop' despite the overall average decreasing.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_13", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg", "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.", "figure_type": "plot", "evidence_from_figure": "Line 0's trend (initial drop then increase)", "evidence_from_text": "Text states: 'for any individual cohort, it increases after a sharp initial drop'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_13_68", "query": "Why does the overall average comment length decrease over time despite individual cohorts showing an increase after an initial drop?", "answer": "The overall average decreases because users who start as longer commenters (cohorts 0-4) are more likely to leave the network, leaving behind users with shorter comments. This is explained in the text: 'people who start out as longer commenters are more likely to leave.'", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_13", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig12.jpg", "caption": "Figure 6: Figure (a) shows the average comment length over clock time and Figure (b) from the user-referential time. Both figures show the cohorted trends. The overall average length per comment decreases over time, although for any individual cohort, it increases after a sharp initial drop. Figures (c), (d) and (e), similar to Figure 5, show the monthly average comment length for active users in the cohorts of 2010, 2011 and 2012, segmented by the number of years that the user survived in the network. Opposite the analysis for average posts, which showed that low-activity users were the first to leave Reddit, here, people who start out as longer commenters are more likely to leave.", "figure_type": "plot", "evidence_from_figure": "Cohort lines show initial drop then increase", "evidence_from_text": "Text states: 'people who start out as longer commenters are more likely to leave'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_19_69", "query": "Why does the 2008 cohort (labeled as cohort 0 in Figure 7b) show a lower comment per submission ratio than later cohorts at the end of the time period?", "answer": "The text explains that unlike the previous analysis of average posts per month, the early 2008 cohort ends up below later cohorts in Figure (b). Visually, cohort 0's line drops below other cohorts near the end of the x-axis (time ≈ 5), consistent with the text's observation about the 2008 cohort's declining ratio.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_19", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg", "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).", "figure_type": "plot", "evidence_from_figure": "Cohort 0's line trending downward relative to other cohorts at x ≈ 5", "evidence_from_text": "The text states: 'the early 2008 cohort ends up below the later cohorts in Figure (b)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1603.07025_1603.07025_fig_19_70", "query": "Which cohort in Figure 7b shows the highest initial comment per submission ratio (at time = 0) for users who survive longer?", "answer": "Cohort 5 (the highest-numbered group in the legend) shows the highest initial ratio, as its line starts at the top of the plot at time = 0. This aligns with the text stating that users who survive longer 'start out with a high comment-to-submission ratio'.", "doc_id": "1603.07025", "figure_id": "1603.07025_fig_19", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1603.07025/1603.07025/hybrid_auto/images/1603.07025_page0_fig18.jpg", "caption": "Figure 7: Figure (a) shows the average comment per submission ratio over clock time for the cohorts and the overall average. Figure (b) shows the average comment per submission from the user-referential time for the cohorts. Figures (c), (d), (e) and (f), similarly to Figure 5, shows the 2008, 2009, 2010, and 2011 cohorts, segmented by the number of years a user in the cohort survived. As with average posts per month, users who stay active longer appear to start their careers with a relatively higher comments per submission ratio than users who abandon Reddit sooner. Unlike that analysis, however, the early 2008 cohort ends up below the later cohorts in Figure (b).", "figure_type": "plot", "evidence_from_figure": "Cohort 5's line begins at the highest y-value (≈15) at x = 0", "evidence_from_text": "The text states: 'users who survive the longest in each cohort are the ones who hit the ground running. They start out with a high comment-to-submission ratio'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_1_71", "query": "What is the Spearman correlation coefficient between the she-he axes of the two embeddings in Figure 4?", "answer": "0.81", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_1", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg", "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).", "figure_type": "plot", "evidence_from_figure": "Scatter plot showing data points across both axes", "evidence_from_text": "Figure caption stating 'Spearman ρ = 0.81'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_1_72", "query": "Which embedding is plotted on the x-axis in Figure 4?", "answer": "w2vNEWS", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_1", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg", "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).", "figure_type": "plot", "evidence_from_figure": "X-axis label 'she-he axis of w2vNEWS embedding'", "evidence_from_text": "Text stating 'the embedding we refer to in this paper is the aforementioned w2vNEWS embedding'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_1_73", "query": "How is the she-he axis direction computed for the embeddings in Figure 4?", "answer": "Using the cosine of the angle between normalized vectors (dot product)", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_1", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig0.jpg", "caption": "Figure 4: Comparing the bias of two different embeddings–the w2vNEWS and the GloVe web-crawl embedding. In each embedding, the occupation words are projected onto the she-he direction. Each dot corresponds to one occupation word; the gender bias of occupations is highly consistent across embeddings (Spearman $\\rho = 0 . 8 1$ ).", "figure_type": "plot", "evidence_from_figure": "She-he axis labels on both axes", "evidence_from_text": "Text explaining 'cos(w1, w2) = w1 · w2' for normalized vectors", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_2_74", "query": "What percentage of variance does component 0 explain in Figure 6 left, and how does this compare to random unit vectors as described in the text?", "answer": "Component 0 explains approximately 60% of variance. The text states that random unit vectors (Figure 6 right) show a more uniform distribution, implying significantly lower variance for the top component compared to the gender pair differences.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_2", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg", "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).", "figure_type": "plot", "evidence_from_figure": "Component 0 bar height (~0.6)", "evidence_from_text": "Description of random unit vectors as 'for comparison' and 'significantly more variance' for gender pair differences", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_2_75", "query": "Why does component 0 in Figure 6 left explain far more variance than other components?", "answer": "The text explains that gender pair differences are not parallel due to systematic biases (e.g., different gender-specific biases and polysemy), creating a dominant direction that explains most variance, unlike random noise which would distribute variance evenly.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_2", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig1.jpg", "caption": "Figure 6: Left: the percentage of variance explained in the PCA of these vector differences (each difference normalized to be a unit vector). The top component explains significantly more variance than any other. Right: for comparison, the corresponding percentages for random unit vectors (figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).", "figure_type": "plot", "evidence_from_figure": "Dominant height of component 0 bar", "evidence_from_text": "Explanation of non-parallel gender pair differences and the statement 'there is a single direction that explains the majority of variance'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_4_76", "query": "What is the significance of the horizontal dashed line in Figure 7?", "answer": "The horizontal dashed line separates gender-neutral words (above the line) from gender-specific words (below the line), as specified in the caption.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_4", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg", "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "figure_type": "plot", "evidence_from_figure": "The horizontal dashed line divides the word cloud into upper and lower regions", "evidence_from_text": "Caption states: 'gender neutral words above the line and gender specific words below the line'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_4_77", "query": "Why are words like 'actresses' and 'ladies' located below the horizontal line in Figure 7?", "answer": "They are gender-specific words, as the caption defines gender-specific words as those positioned below the horizontal line.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_4", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg", "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "figure_type": "plot", "evidence_from_figure": "'actresses' and 'ladies' appear below the dashed line", "evidence_from_text": "Caption specifies gender-specific words are below the line", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_4_78", "query": "How does the hard debiasing algorithm affect gender-neutral words in Figure 7?", "answer": "It collapses gender-neutral words above the horizontal line to the vertical (y-axis), removing their association with the gender difference (x-axis).", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_4", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig3.jpg", "caption": "Figure 7: Selected words projected along two axes: $x$ is a projection onto the difference between the embeddings of the words he and she, and $y$ is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.", "figure_type": "plot", "evidence_from_figure": "Words above the line are gender-neutral", "evidence_from_text": "Text states: 'the words above the horizontal line would all be collapsed to the vertical line'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_5_79", "query": "At 140 generated analogies in Figure 8, how many stereotypic analogies does the soft-debiased model produce, and how does this compare to its analogy score of 56.8 in Table 1?", "answer": "At 140 generated analogies, the soft-debiased model produces approximately 24 stereotypic analogies (Figure 8). This aligns with its analogy score of 56.8 (Table 1), indicating reduced stereotypic analogies while maintaining high performance in generating appropriate analogies.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_5", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg", "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.", "figure_type": "plot", "evidence_from_figure": "Red triangle at x=140 shows y≈24", "evidence_from_text": "Table 1 shows soft-debiased analogy score = 56.8", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1607.06520_1607.06520_fig_5_80", "query": "The text states 29 analogies were rated as gender-stereotypical (Figure 8). What is the value of the 'before' model at 150 generated analogies in Figure 8, and how does it compare to this number?", "answer": "At 150 generated analogies, the 'before' model produces approximately 28 stereotypic analogies (Figure 8), closely matching the 29 stereotypic analogies mentioned in the text.", "doc_id": "1607.06520", "figure_id": "1607.06520_fig_5", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1607.06520/1607.06520/hybrid_auto/images/1607.06520_page0_fig4.jpg", "caption": "Figure 8: Number of stereotypical (Left) and appropriate (Right) analogies generated by wordembeddings before and after debiasing.", "figure_type": "plot", "evidence_from_figure": "Blue dot at x=150 shows y≈28", "evidence_from_text": "Text states '29 analogies were rated as exhibiting gender stereotype'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1608.07187_1608.07187_fig_1_81", "query": "What does the Pearson’s correlation coefficient ρ = 0.90 (stated in the text) indicate about the relationship between the percentage of women in occupations and the strength of association with female gender as shown in the scatter plot?", "answer": "The ρ = 0.90 indicates a strong positive correlation, visually represented by the upward trend of data points in the scatter plot where higher percentages of women in occupations correspond to more positive association with female gender.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Upward trend of data points as x-axis increases", "evidence_from_text": "Pearson’s correlation coefficient ρ = 0.90 with p-value < 10^-18", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1608.07187_1608.07187_fig_1_82", "query": "How does the negative y-axis value for blue data points in the figure relate to the text’s finding that 'female terms are less associated with the sciences'?", "answer": "The blue data points (representing science occupations) have negative y-axis values, indicating a strong negative association with female gender, which aligns with the text’s description that female terms are less associated with the sciences.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Blue data points located below y=0", "evidence_from_text": "Original finding: 'female terms are less associated with the sciences'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1608.07187_1608.07187_fig_3_83", "query": "In Figure 3, what do the dashed lines between 'queen' and 'king' represent, and how does the text explain this relationship?", "answer": "The dashed lines represent constant vector differences between gendered word pairs in the GloVe embedding space. The text states these algebraic relationships capture gender-related semantic differences where pairs differing only by gender map to vectors with consistent vector differences.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg", "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.", "figure_type": "plot", "evidence_from_figure": "Dashed lines connecting 'queen' and 'king'", "evidence_from_text": "Figure caption: 'pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1608.07187_1608.07187_fig_3_84", "query": "How does the Pearson’s correlation coefficient ρ = 0.84 relate to the visual structure of Figure 3?", "answer": "The ρ = 0.84 correlation measures the relationship between the y-axis (gender bias calculation) and the actual gender distribution of names. The figure’s y-axis visually reflects this bias, with words like 'woman' and 'man' positioned to indicate the direction of gender bias.", "doc_id": "1608.07187", "figure_id": "1608.07187_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1608.07187/1608.07187/hybrid_auto/images/1608.07187_page0_fig2.jpg", "caption": "Figure 3. A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014). The lines illustrate algebraic relationships between related words: pairs of words that differ only by gender map to pairs of vectors whose vector difference is roughly constant. Similar algebraic relationships have been shown for other semantic relationships, such as countries and their capital cities, companies and their CEOs, or simply different forms of the same word.", "figure_type": "plot", "evidence_from_figure": "Y-axis positioning of gendered words (e.g., 'woman' above 'man')", "evidence_from_text": "Text: 'The y-axis reflects our calculation of the bias for how male or female each of the names is... Pearson’s correlation coefficient of ρ = 0.84'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_1_85", "query": "What does the '+' symbol represent in Figure 1's left panel (equalized odds predictor) according to the legend?", "answer": "Result for Ŷ = Ŷ", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg", "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "figure_type": "diagram", "evidence_from_figure": "The '+' symbol is explicitly labeled in the legend as 'Result for Ŷ = Ŷ'", "evidence_from_text": "The text defines gamma_a(Ŷ) components as FPR/TPR, where the '+' symbol corresponds to the original predictor's performance", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_1_86", "query": "Why is the equal-odds optimum (star symbol) located within the overlap region in Figure 1's left panel?", "answer": "The overlap region represents the intersection of achievable regions for A=0 and A=1, which is required for equalized odds since both FPR and TPR must be equal across groups.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg", "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "figure_type": "diagram", "evidence_from_figure": "The star symbol is positioned within the teal 'Overlap' region", "evidence_from_text": "The text states that equalized odds requires matching both FPR and TPR across groups (via gamma_a components)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_1_87", "query": "How does the '×' symbol in Figure 1 relate to the flipped predictor (Ŷ = 1 - Ŷ) in the context of the linear program?", "answer": "The '×' symbol represents the performance metrics when the predictor is flipped, which swaps FPR and TPR values across groups as defined by gamma_a(Ŷ).", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig0.jpg", "caption": "Figure 1: Finding the optimal equalized odds predictor (left), and equal opportunity predictor (right).", "figure_type": "diagram", "evidence_from_figure": "The '×' symbol is labeled 'Result for Ŷ = 1 - Ŷ' in the legend", "evidence_from_text": "The text defines gamma_a(Ŷ) components as FPR (first) and TPR (second), where flipping Ŷ inverts these metrics", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_4_88", "query": "In Figure 2, how does the horizontal gap between the ROC curve and tangent line relate to the cost for the equal opportunity predictor?", "answer": "The horizontal gap is proportional to the cost for the equal opportunity predictor, as stated in the text: 'the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line'.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg", "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.", "figure_type": "plot", "evidence_from_figure": "Horizontal gap between blue ROC curve and black tangent line", "evidence_from_text": "Text states cost is proportional to this gap for equal opportunity predictor", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_4_89", "query": "Why is the cost function convex for the equal opportunity predictor according to Figure 2 and the text?", "answer": "The convexity of the ROC curve (visible in Figure 2) causes the cost function to be convex, as the text explains: 'the cost... is a convex function of the true positive rate' due to the ROC curve's shape.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig3.jpg", "caption": "Figure 2: Finding the optimal equalized odds threshold predictor (middle), and equal opportunity threshold predictor (right). For the equal opportunity predictor, within each group the cost for a given true positive rate is proportional to the horizontal gap between the ROC curve and the profit-maximizing tangent line (i.e., the two curves on the left plot), so it is a convex function of the true positive rate (right). This lets us optimize it efficiently with ternary search.", "figure_type": "plot", "evidence_from_figure": "Convex shape of blue ROC curve", "evidence_from_text": "Text states cost is convex as a function of true positive rate", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_7_90", "query": "In Figure 3, how does the node A (protected attribute) relate to the equalized odds predictor construction mentioned in Corollary 5.3?", "answer": "Node A connects to X' and Y*, representing the protected group's influence on transformed features and outcomes. Corollary 5.3 states that optimal equalized odds predictors require leveraging A's relationship with R (Bayes regressor), as A defines the groups whose true positive rates must be equalized.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_7", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg", "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.", "figure_type": "diagram", "evidence_from_figure": "A → X', A → Y* connections", "evidence_from_text": "Corollary 5.3: 'optimal equalized odds predictor can be derived from R and A'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_7_91", "query": "How does the node Y* in Figure 3 correspond to the 'conditional ROC curves' referenced in the text?", "answer": "Y* represents the outcome for the protected group. The text states that equal opportunity requires matching true positive rates across groups via conditional ROC curves, where Y* is the protected group's outcome variable (linked to A in the diagram).", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_7", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig6.jpg", "caption": "Figure 3: Graphical model for the proof of Proposition 5.2.", "figure_type": "diagram", "evidence_from_figure": "A → Y* arrow", "evidence_from_text": "Text: 'find points on the conditional ROC curves that have the same true positive rates in both groups'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_9_92", "query": "Why does Scenario II (Figure 5) lead to indistinguishability from a scenario where R* is based directly on A using oblivious tests, as stated in the text?", "answer": "The figure shows R* depends on X3 (influenced by A), but oblivious tests only analyze Y and R*. The text explains this structural similarity (A → X3 → R*) makes it indistinguishable from direct A → R* paths, as marginal distributions of Y given R* appear identical.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_9", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg", "caption": "Figure 5: Graphical model for Scenario II.", "figure_type": "diagram", "evidence_from_figure": "Arrows from X3 to R* and A to X3", "evidence_from_text": "Text states 'the two scenarios are indistinguishable using any oblivious test' and 'no test based only on target labels'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_9_93", "query": "How does R̃ function as a surrogate for A in Figure 5, and why is this relevant to the text's discussion of Scenario II?", "answer": "R̃ is directly connected to A (A → R̃), making it a surrogate. The text explains Scenario II (with R* based on X3) is indistinguishable from scenarios where R* is based on A or its surrogate (R̃), highlighting equalized odds' failure to capture causal differences.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_9", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig8.jpg", "caption": "Figure 5: Graphical model for Scenario II.", "figure_type": "diagram", "evidence_from_figure": "Arrow from A to R̃", "evidence_from_text": "Text states 'optimal score R* is in one case based directly on A or its surrogate'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_10_94", "query": "In Figure 6, why do scenarios I and II have identical ROC curves despite different directed dependency structures?", "answer": "Scenarios I and II are indistinguishable by oblivious tests because their structures produce identical conditional ROC curves for both values of A, as required by equalized odds. The text explains that oblivious tests cannot differentiate them despite differing paths (e.g., A→X₃→Y vs. A→X₁→X₂→Y).", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_10", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg", "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.", "figure_type": "diagram", "evidence_from_figure": "", "evidence_from_text": "", "requires_figure": "the different paths from A to Y (X₃ vs. X₂)", "requires_text": "the explanation that oblivious tests cannot distinguish the scenarios despite structural differences"}
{"query_id": "l1_1610.02413_1610.02413_fig_10_95", "query": "According to Definition 6.2, what condition must the score R satisfy for identical conditional ROC curves in Figure 6's structures?", "answer": "The score R must have conditional ROC curves that agree for both values of A (C_a(t) = C), as defined in Section 4.2. Figure 6's structures ensure this by maintaining equivalent predictive relationships for A across scenarios.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_10", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig9.jpg", "caption": "Figure 6: Two possible directed dependency structures for the variables in scenarios I and II. The undirected (infrastructure graph) versions of both graphs are also possible.", "figure_type": "diagram", "evidence_from_figure": "", "evidence_from_text": "", "requires_figure": "the directed dependencies from A to Y via X₃/X₂", "requires_text": "Definition 6.2 and the explanation that equalized odds requires identical conditional ROC curves"}
{"query_id": "l1_1610.02413_1610.02413_fig_12_96", "query": "How does the monotonic trend in Figure 7's curves support the text's claim that P(Y | X3) is monotone in X3 for scenario II?", "answer": "The consistent upward trend across all ethnic groups in Figure 7 demonstrates that non-default rates decrease as FICO score (X3) increases, directly aligning with the text's assertion of monotonicity in P(Y | X3).", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_12", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg", "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.", "figure_type": "plot", "evidence_from_figure": "All four group curves show increasing non-default rates with rising FICO scores", "evidence_from_text": "Text states 'P(Y | X3) is monotone in X3' for scenario II", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_12_97", "query": "Why does the text state that Y | X3 matches Y | R* from scenario I, and how does Figure 7 illustrate this?", "answer": "Figure 7's non-default rate curves for X3 (FICO score) exhibit the same monotonic pattern as R* in scenario I, confirming the conditional distribution match described in the text.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_12", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig11.jpg", "caption": "Figure 7: These two marginals, and the number of people per group, constitute our input data.", "figure_type": "plot", "evidence_from_figure": "Curves for all groups show identical monotonic relationship between FICO score and non-default rate", "evidence_from_text": "Text states 'the conditional distribution Y | X3 matched that of Y | R* from scenario I'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_14_98", "query": "What does the shaded gray area in Figure 8 represent for the Black group, and how does it compare to White and Asian groups according to the text?", "answer": "The shaded area represents Pr[Ŷ=1|Y=1,A] (probability a non-defaulter qualifies for a loan), which is smaller for Black than White/Asian, indicating Black non-defaulters are less likely to qualify.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_14", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg", "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.", "figure_type": "plot", "evidence_from_figure": "gray shaded area on right side of plot and Black line position below White/Asian lines", "evidence_from_text": "caption stating shaded area = Pr[Ŷ=1|Y=1,A] and text stating Black non-defaulters are less likely to qualify", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_14_99", "query": "Why does the common FICO threshold of 620 violate fairness definitions as explained in the text and shown in Figure 8?", "answer": "Because Black non-defaulters have a lower non-default rate at 620 than White/Asian groups, resulting in fewer Black non-defaulters qualifying for loans despite the same threshold.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_14", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig13.jpg", "caption": "Figure 8: The common FICO threshold of 620 corresponds to a non-default rate of $8 2 \\%$ Rescaling the $x$ axis to represent the within-group thresholds (right), $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid Y = 1 , A ]$ is the fraction of the area under the curve that is shaded. This means black non-defaulters are much less likely to qualify for loans than white or Asian ones, so a race blind score threshold violates our fairness definitions.", "figure_type": "plot", "evidence_from_figure": "Black line position below White/Asian lines at 620", "evidence_from_text": "text stating Black non-defaulters are much less likely to qualify for loans than White/Asian", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_16_100", "query": "Why does the Equal odds method show a line segment instead of a single point in Figure 9?", "answer": "The Equal odds method shows a line segment because it does not define a single threshold but a range where the probability of positive prediction increases; the figure visualizes this range as a horizontal line segment for each racial group.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_16", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg", "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.", "figure_type": "plot", "evidence_from_figure": "Line segment for Equal odds category", "evidence_from_text": "Caption explanation: 'the equal odds method does not give a single threshold, but instead Pr[Ŷ=1|R,A] increases over some not uniquely defined range'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_16_101", "query": "For the Hispanic race in Figure 9, which fairness methods' thresholds lie between Max profit and Demography thresholds?", "answer": "The Equal opportunity (Opportunity category) and Equal odds methods' thresholds for Hispanic lie between Max profit and Demography thresholds.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_16", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig15.jpg", "caption": "Figure 9: FICO thresholds for various definitions of fairness. The equal odds method does not give a single threshold, but instead $\\operatorname* { P r } [ \\widehat { Y } = 1 \\mid R , A ]$ increases over some not uniquely defined range; we pick the one containing the fewest people. Observe that, within each race, the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds.", "figure_type": "plot", "evidence_from_figure": "Hispanic red squares in Opportunity and Equal odds categories between Max profit and Demography", "evidence_from_text": "Text: 'the equal opportunity threshold and average equal odds threshold lie between the max profit threshold and equal demography thresholds'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_18_102", "query": "In Figure 10, why does the Black group's ROC curve fall below the White group's curve in the x < 0.2 region, and how does this relate to the 'equal odds' fairness criterion mentioned in the caption?", "answer": "The Black group's ROC curve lies below the White group's in this region, indicating lower true positive rates (TPR) for the same false positive rate (FPR). This aligns with 'equal odds' requiring a point below all group curves, as the Black group's lower curve sets the minimum achievable TPR across groups.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg", "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.", "figure_type": "plot", "evidence_from_figure": "Black curve position relative to White curve in x < 0.2 region", "evidence_from_text": "Caption stating 'Equal odds picks a point below all lines'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_18_103", "query": "How does the concept of 'equality of opportunity' (text) apply to the ROC curves in Figure 10 when selecting points along horizontal lines?", "answer": "Equality of opportunity selects points with identical true positive rates (horizontal lines), meaning the same TPR across groups. The ROC curves show that different groups achieve different TPRs for the same FPR, so horizontal lines intersect each curve at distinct FPR values.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig17.jpg", "caption": "Figure 10: The ROC curve for using FICO score to identify non-defaulters. Within a group, we can achieve any convex combination of these outcomes. Equality of opportunity picks points along the same horizontal line. Equal odds picks a point below all lines.", "figure_type": "plot", "evidence_from_figure": "Horizontal alignment of points across curves", "evidence_from_text": "Caption stating 'Equality of opportunity picks points along the same horizontal line'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_20_104", "query": "Which fairness metric has the highest fraction of non-defaulters getting loans for the Asian group in Figure 11, and how does this relate to the 'profit' concept mentioned in the figure caption?", "answer": "Max profit (blue bar) achieves the highest fraction (~0.85) for Asian non-defaulters. This aligns with the caption's 'profit achievable' concept, as Max profit prioritizes financial gain, potentially enabling higher loan access for non-defaulters in groups where default risk is low.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_20", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg", "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.", "figure_type": "plot", "evidence_from_figure": "Blue bar height for Asian group", "evidence_from_text": "Figure caption: 'On the right, we see the profit achievable for each notion of fairness'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.02413_1610.02413_fig_20_105", "query": "What is the fraction of non-defaulters getting loans for the Black group under the 'Demography' metric in Figure 11, and what does the text imply about this metric's impact on loan access?", "answer": "The Demography metric shows a fraction of ~0.9 for Black non-defaulters. The text implies this metric prioritizes demographic representation, leading to higher loan access for Black non-defaulters compared to profit-focused metrics like Max profit.", "doc_id": "1610.02413", "figure_id": "1610.02413_fig_20", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.02413/1610.02413/hybrid_auto/images/1610.02413_page0_fig19.jpg", "caption": "Figure 11: On the left, we see the fraction of non-defaulters that would get loans. On the right, we see the profit achievable for each notion of fairness, as a function of the false positive/negative trade-off.", "figure_type": "plot", "evidence_from_figure": "Yellow bar height for Black group", "evidence_from_text": "Text: 'the left side of Figure 11 shows the fraction of people that wouldn’t default that would qualify for loans by the various metrics'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.07524_1610.07524_fig_1_106", "query": "How does Figure 1 support the claim that COMPAS RPI adheres to the test fairness condition?", "answer": "The figure shows similar recidivism rate trends for Black and White defendants across all COMPAS decile scores, with overlapping confidence intervals. This visual evidence aligns with the text's assertion that COMPAS RPI adheres to the test fairness condition by demonstrating comparable risk assessment outcomes across racial groups.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg", "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.", "figure_type": "plot", "evidence_from_figure": "Similar bar heights and overlapping confidence intervals for Black and White across all decile scores", "evidence_from_text": "Text states COMPAS RPI adheres to test fairness condition and mentions Flores et al. verification", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.07524_1610.07524_fig_1_107", "query": "What does the trend in recidivism rates for Black and White defendants at decile score 8 (Figure 1) imply about COMPAS fairness?", "answer": "At decile score 8, both racial groups show similar recidivism rates (approximately 0.75), with overlapping confidence intervals. This supports the text's claim that COMPAS adheres to the test fairness condition, as it indicates no significant racial disparity in risk assessment at this score.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig0.jpg", "caption": "Figure 1: Plot shows $\\mathbb { P } ( Y = 1 \\mid S = s , R )$ for the COM-PAS decile score, with $R \\in \\{ \\mathrm { B l a c k } , \\mathrm { W h i t e } \\}$ . Error bars represent 95% confidence intervals.", "figure_type": "plot", "evidence_from_figure": "Bar heights and confidence intervals for Black/White at decile 8", "evidence_from_text": "Text describes COMPAS RPI adherence to fairness condition and mentions logistic regression verification", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.07524_1610.07524_fig_2_108", "query": "In Figure 2, why does the Black group have a higher false positive rate than the White group in the '7-10' prior count category?", "answer": "The text explains that when recidivism prevalence differs across groups, the higher prevalence group (Black) will have a higher FPR. Since Black defendants have higher recidivism prevalence, their FPR is higher in this category.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg", "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ", "figure_type": "plot", "evidence_from_figure": "The '7-10' prior count category shows a higher gray bar (Black) than yellow bar (White)", "evidence_from_text": "The text states that higher recidivism prevalence leads to higher FPR", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.07524_1610.07524_fig_2_109", "query": "According to the text, what explains the trend where Black defendants have higher false positive rates than White defendants across all prior count categories in Figure 2?", "answer": "The text states that higher recidivism prevalence in the Black group leads to higher FPR, which explains the observed trend in Figure 2.", "doc_id": "1610.07524", "figure_id": "1610.07524_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.07524/1610.07524/hybrid_auto/images/1610.07524_page0_fig1.jpg", "caption": "Figure 2: False positive rates across prior record count for defendants charged with a Misdemeanor offense. Plot is based on assessing a defendant as “high-risk” if their COMPAS decile score is $> 4$ . Error bars represent 95% confidence intervals.   ", "figure_type": "plot", "evidence_from_figure": "Black bars are consistently taller than White bars across all prior count categories", "evidence_from_text": "The text explains that higher recidivism prevalence results in higher FPR", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_1_110", "query": "What is the false positive rate for female pedestrians in Figure 1, using the text's definition of false positive rate as a fraction over ground truth labels?", "answer": "The false positive rate for female pedestrians is calculated by dividing the number of female pedestrians incorrectly stopped (classifier decision 1 where ground truth is 0) by the total number of female pedestrians without illegal weapons (ground truth 0).", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg", "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.", "figure_type": "table", "evidence_from_figure": "Table structure showing female pedestrians' decisions and ground truth labels", "evidence_from_text": "Definition of false positive rate as fraction over ground truth labels", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_1_111", "query": "How does disparate impact manifest in Figure 1's classifier decisions, according to the text's explanation of disproportionate outcomes for sensitive attribute groups?", "answer": "Disparate impact manifests when classifier decisions (e.g., stopping pedestrians) disproportionately benefit or harm female pedestrians compared to male pedestrians, as seen in unequal misclassification rates across gender groups in Figure 1.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig0.jpg", "caption": "Figure 1: Decisions of three fictitious classifiers $\\mathbf { C _ { 1 } }$ , C2 and $\\mathbf { C _ { 3 } }$ ) on whether (1) or not (0) to stop a pedestrian on the suspicion of possessing an illegal weapon. Gender is a sensitive attribute, whereas the other two attributes (suspicious bulge in clothing and proximity to a crime scene) are non-sensitive. Ground truth on whether the person is actually in possession of an illegal weapon is also shown.", "figure_type": "table", "evidence_from_figure": "Gender-based decision outcomes in the table", "evidence_from_text": "Text definition of disparate impact as disproportionate outcomes for sensitive attribute groups", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_2_112", "query": "In Figure 2(a), what happens to the false positive rate for group z=1 as the covariance multiplicative factor decreases from 1 to 0, and how does this relate to the text's explanation of fairness constraints?", "answer": "The false positive rate for group z=1 increases slightly from ~0.05 to ~0.1 as the covariance factor decreases. According to the text, this occurs because fairness constraints rotate the decision boundary, moving well-classified examples with z=1 into the positive class (increasing false positives) while reducing false positives for z=0.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg", "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.", "figure_type": "plot", "evidence_from_figure": "Pink square trend showing slight increase in FPR for z=1 as m decreases", "evidence_from_text": "Text stating fairness constraints move well-classified z=1 examples into positive class", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_2_113", "query": "According to the text, why does the false positive rate for group z=0 decrease as the covariance multiplicative factor decreases in Figure 2(a)?", "answer": "The text explains that fairness constraints cause the decision boundary to rotate, moving previously misclassified examples with z=0 into the negative class, which directly reduces their false positive rate as seen in the blue cross trend.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg", "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.", "figure_type": "plot", "evidence_from_figure": "Blue cross trend showing decreasing FPR for z=0 as m decreases", "evidence_from_text": "Text describing how fairness constraints move misclassified z=0 examples to negative class", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_2_114", "query": "How does Figure 2(a) demonstrate the scenario where D_FPR ≠ 0 and D_FNR = 0 as described in the text?", "answer": "The figure shows disparate FPRs initially (z=0 has higher FPR than z=1), satisfying D_FPR ≠ 0. As the covariance threshold decreases, FPRs converge, which aligns with the text's explanation that fairness constraints equalize FPRs while maintaining equal FNRs (D_FNR = 0).", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig1.jpg", "caption": "Figure 2: [Synthetic data] Panel (a) shows that decreasing the covariance threshold causes the false positive rates for both groups to become similar. Panel (b) shows that an increasing degree of fairness corresponds to a steady decrease in accuracy. Panel (c) shows the original decision boundary (solid line) and fair decision boundary (dashed line), along with corresponding accuracy and false positive rates for groups $z = 0$ (crosses) and $z = 1$ (circles). Fairness constraints cause the original decision boundary to rotate such that previously misclassified examples with $z = 0$ are moved into the negative class (decreasing false positives), while well-classified examples with $z = 1$ are moved into the positive class (increasing false positives), leading to equal false positive rates for both groups.", "figure_type": "plot", "evidence_from_figure": "Initial divergence of FPRs between z=0 and z=1 with convergence at lower m values", "evidence_from_text": "Text defining scenario where D_FPR ≠ 0 and D_FNR = 0", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_6_115", "query": "Why does the FPR-constrained classifier (blue dashed line) in Figure 3 show a lower FNR than the unconstrained classifier (cyan solid line), despite having a higher FPR?", "answer": "The text explains that D_FPR and D_FNR have opposite signs, meaning reducing FPR disparity can improve FNR disparity. The figure shows the constrained classifier (FPR=0.26, FNR=0.24) has higher FPR but lower FNR than the unconstrained classifier (FPR=0.14, FNR=0.31), consistent with this relationship.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg", "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ", "figure_type": "plot", "evidence_from_figure": "FPR/FNR values of blue dashed vs. cyan solid lines", "evidence_from_text": "Statement that D_FPR and D_FNR have opposite signs", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_6_116", "query": "How does the accuracy of the FPR-constrained classifier compare to the unconstrained classifier in Figure 3, and what does this imply about fairness-accuracy trade-offs?", "answer": "The unconstrained classifier has Acc=0.78 while the FPR-constrained classifier has Acc=0.75. This small accuracy reduction (0.03) suggests a minor trade-off where improving FPR fairness slightly reduces overall accuracy, as implied by the text's discussion of fairness constraints.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig5.jpg", "caption": "Figure 3: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have opposite signs. Removing disparate mistreatment on FPR can potentially help remove disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time leads to very similar results.   ", "figure_type": "plot", "evidence_from_figure": "Accuracy values in legend", "evidence_from_text": "Text stating 'removing disparate mistreatment on both at the same time leads to very similar results'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_9_117", "query": "In Figure 4, why does the fair constrained classifier (dashed line) have a lower FPR but higher FNR than the unconstrained classifier (solid line), as shown in the legend?", "answer": "The text states that removing disparate mistreatment on FPR can increase FNR (caption), which aligns with the legend values: FPR drops from 0.08 to 0.07 (fair constrained) but FNR rises from 0.12 to 0.14. This trade-off occurs because fairness constraints prioritize reducing FPR at the cost of higher FNR.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_9", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg", "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.", "figure_type": "plot", "evidence_from_figure": "Legend values for FPR/FNR of dashed vs. solid lines", "evidence_from_text": "Caption stating 'Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1610.08452_1610.08452_fig_9_118", "query": "What does the 0.77 accuracy value for the fair constrained classifier in Figure 4 indicate about the trade-off between fairness and accuracy, according to the text?", "answer": "The text explains that controlling both FPR and FNR (as in the fair constrained classifier) causes a larger accuracy drop (caption), matching the 0.77 accuracy vs. 0.80 for the unconstrained classifier. This demonstrates the trade-off where fairness constraints reduce overall accuracy.", "doc_id": "1610.08452", "figure_id": "1610.08452_fig_9", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1610.08452/1610.08452/hybrid_auto/images/1610.08452_page0_fig8.jpg", "caption": "Figure 4: [Synthetic data] $D _ { F P R }$ and $D _ { F N R }$ have the same sign. Removing disparate mistreatment on FPR can potentially increase disparate mistreatment on FNR. Removing disparate mistreatment on both at the same time causes a larger drop in accuracy.", "figure_type": "plot", "evidence_from_figure": "Acc=0.77 for dashed line in legend", "evidence_from_text": "Caption stating 'Removing disparate mistreatment on both at the same time causes a larger drop in accuracy'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07438_1611.07438_fig_1_119", "query": "In Figure 1, what causal path from gender to admission is directly checked for discrimination according to Section 3.1's criterion?", "answer": "The direct path gender → admission is checked for direct discrimination, as Section 3.1 specifies that violations of |ΔP|_b < τ for subpopulations would indicate liability for direct discrimination.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg", "caption": "Figure 1: Causal graph of an example university admission system.", "figure_type": "diagram", "evidence_from_figure": "Arrow from gender to admission", "evidence_from_text": "Section 3.1's definition of 'direct discrimination' and |ΔP|_b criterion", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07438_1611.07438_fig_1_120", "query": "How does the causal graph in Figure 1 inform the examination of block sets B for non-discrimination?", "answer": "The graph's structure (e.g., gender → major → admission) defines potential block sets B where discrimination might occur through mediated pathways, requiring analysis of subpopulations defined by these causal relationships.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig0.jpg", "caption": "Figure 1: Causal graph of an example university admission system.", "figure_type": "diagram", "evidence_from_figure": "Gender → major → admission path", "evidence_from_text": "Section 3.1's requirement to examine block sets B for |ΔP|_b < τ", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07438_1611.07438_fig_2_121", "query": "Which node is identified as the protected attribute in Figure 2, and what is its color according to the caption?", "answer": "The protected attribute is the 'sex' node, which is colored red as specified in the figure caption.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg", "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.", "figure_type": "diagram", "evidence_from_figure": "Red-colored node labeled 'sex'", "evidence_from_text": "Caption states 'the red node represents the protected attribute'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07438_1611.07438_fig_2_122", "query": "How many nodes are included in set Q according to the figure caption, and which specific nodes are they?", "answer": "Set Q contains 9 nodes: race, age, native_country, edu, hour, work_class, marital, occupation, and relationship.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig1.jpg", "caption": "Figure 2: Causal graph for Adult dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q.", "figure_type": "diagram", "evidence_from_figure": "All green-colored nodes in the graph", "evidence_from_text": "Caption states 'the green nodes represent set Q'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07438_1611.07438_fig_3_123", "query": "Why are 'age' and 'edu' colored green in Figure 3, and what does the text say about their role in the causal graph?", "answer": "Green nodes represent set Q in the causal graph. The text states that set Q is part of the methodology for analyzing discrimination, where these variables are used to identify indirect causal paths affecting the decision variable (occupation) through the protected attribute (sex).", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg", "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.", "figure_type": "diagram", "evidence_from_figure": "Green nodes labeled 'age' and 'edu' with arrows connecting to other variables", "evidence_from_text": "The figure caption defines green nodes as 'set Q' and the surrounding text discusses its role in discrimination analysis", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07438_1611.07438_fig_3_124", "query": "How does the red 'sex' node influence the blue 'occupation' node in Figure 3, and what is the significance of this relationship in the paper's methodology?", "answer": "The red 'sex' node has direct and indirect causal paths to 'occupation' via variables like 'age', 'edu', and 'economic_state'. The text explains this relationship is critical for identifying discrimination, as the protected attribute (sex) affects the decision variable (occupation) through mediated pathways.", "doc_id": "1611.07438", "figure_id": "1611.07438_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07438/1611.07438/hybrid_auto/images/1611.07438_page0_fig2.jpg", "caption": "Figure 3: Causal graph for Dutch Census dataset: the red node represents the protected attribute, the blue node represents the decision, the green nodes represent set Q, and the black nodes represent the others.", "figure_type": "diagram", "evidence_from_figure": "Arrows from 'sex' to 'occupation' and intermediate nodes (e.g., 'age', 'edu')", "evidence_from_text": "The text describes how protected attributes influence decisions through causal pathways in discrimination analysis", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_1_125", "query": "What does the path-specific effect of Race on Loan via Zip_code represent in Figure 1 according to the paper's methodology?", "answer": "The path-specific effect via Zip_code represents indirect discrimination where Race influences Loan through Zip_code (e.g., racial segregation in zip codes), as defined in the text as path-specific effects distinguishing direct/indirect discrimination.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg", "caption": "Figure 1: The toy model.", "figure_type": "diagram", "evidence_from_figure": "Path from Race → Zip_code → Loan", "evidence_from_text": "Definition of path-specific effects for modeling direct/indirect discrimination", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_1_126", "query": "How does the absence of a direct Race→Loan arrow in Figure 1 relate to the paper's concept of direct discrimination?", "answer": "The absence implies no direct discrimination path exists in this model; all discrimination must be indirect (via Zip_code or Income), aligning with the paper's distinction between direct (no mediators) and indirect (mediated) effects.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig0.jpg", "caption": "Figure 1: The toy model.", "figure_type": "diagram", "evidence_from_figure": "No direct arrow from Race to Loan", "evidence_from_text": "Discussion of direct vs. indirect discrimination as path-specific effects", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_2_127", "query": "Does Figure 2 satisfy the recanting witness criterion as stated in the caption?", "answer": "Yes, the caption explicitly states that Figure 2 is an example where the recanting witness criterion is satisfied.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg", "caption": "Figure 2: An example with the recanting witness criterion satisfied.", "figure_type": "diagram", "evidence_from_figure": "Structure of the DAG with nodes X, Z₁, Z₂, Y and arrows", "evidence_from_text": "Caption: 'Figure 2: An example with the recanting witness criterion satisfied.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_2_128", "query": "Which paths from X to Y are considered for the total causal effect in Figure 2 according to the text?", "answer": "The paths X→Z₁→Y and X→Z₁→Z₂→Y are considered, as they represent all causal routes from X to Y in the DAG.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg", "caption": "Figure 2: An example with the recanting witness criterion satisfied.", "figure_type": "diagram", "evidence_from_figure": "Arrows showing X→Z₁→Y and X→Z₁→Z₂→Y", "evidence_from_text": "Text mentions 'total causal effect of X on Y' and do-calculus marginalization over V \\ {X,Y}", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_2_129", "query": "What role does Z₁ play in the causal structure of Figure 2?", "answer": "Z₁ acts as a mediator between X and Y, influencing Y both directly (X→Z₁→Y) and indirectly through Z₂ (X→Z₁→Z₂→Y).", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig1.jpg", "caption": "Figure 2: An example with the recanting witness criterion satisfied.", "figure_type": "diagram", "evidence_from_figure": "Z₁'s connections to X, Z₂, and Y", "evidence_from_text": "Text discusses 'path-specific effects' and 'direct and indirect discrimination' in the context of causal models", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_4_130", "query": "How many direct paths from 'sex' to 'income' are shown in Figure 3, and what does the figure caption specify about the green path's meaning?", "answer": "Figure 3 shows one direct path from 'sex' to 'income' (the green arrow), which the caption specifies represents direct causal relationships.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg", "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "evidence_from_figure": "Green arrow connecting 'sex' to 'income'", "evidence_from_text": "Caption: 'the green path represents the direct path'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_4_131", "query": "Which attribute mediates indirect paths from 'age' to 'income' in Figure 3, as indicated by the blue paths and the figure caption?", "answer": "Marital status mediates indirect paths from 'age' to 'income', as blue paths pass through it per the caption's explanation.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig3.jpg", "caption": "Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "evidence_from_figure": "Blue arrows from 'age' to 'income' via 'marital_status'", "evidence_from_text": "Caption: 'blue paths represent indirect paths passing through marital status'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_5_132", "query": "Why are the green and blue paths in Figure 4 significant for the causal analysis of the Dutch dataset as described in the text?", "answer": "Green paths represent direct causal relationships, while blue paths indicate indirect relationships mediated through marital status. The text explains that binarizing attributes reduced computational complexity, enabling the PC algorithm to model these causal paths effectively.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_5", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg", "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "evidence_from_figure": "green and blue arrows in the network", "evidence_from_text": "we binarize each attribute’s domain values into two classes to reduce the domain sizes. We use three tiers in the partial order for temporal priority", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1611.07509_1611.07509_fig_5_133", "query": "How does the temporal priority tier mentioned in the text influence the causal paths in Figure 4?", "answer": "The first-tier attributes (sex, age, country_birth) act as initial causal drivers, with arrows originating from them to influence downstream variables like education level and economic status, as seen in the network structure.", "doc_id": "1611.07509", "figure_id": "1611.07509_fig_5", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1611.07509/1611.07509/hybrid_auto/images/1611.07509_page0_fig4.jpg", "caption": "Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect paths passing through marital status.", "figure_type": "diagram", "evidence_from_figure": "arrows from sex, age, country_birth to other nodes", "evidence_from_text": "three tiers in the partial order for temporal priority: sex, age, native country, race are defined in the first tier", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1701.08230_1701.08230_fig_2_134", "query": "What does the dashed line at 25% in Figure 1 indicate about the detention rate for Broward County data, as referenced in the text?", "answer": "The dashed line at 25% corresponds to a threshold that results in detaining 30% of defendants, which violates fairness metrics like statistical parity and predictive equality as stated in the text.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg", "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.", "figure_type": "plot", "evidence_from_figure": "Dashed vertical line at 25% on x-axis", "evidence_from_text": "Text states 'using a single threshold which detains 30% of defendants in Broward County violates statistical parity'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1701.08230_1701.08230_fig_2_135", "query": "Why does the simulated data (red curve) in Figure 1 have a different shape than the Broward County data (blue curve) despite equal means?", "answer": "The simulated data is drawn from two beta distributions with equal means but different variances, resulting in a flatter distribution compared to the more peaked Broward County data, as explained in the caption.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig1.jpg", "caption": "Figure 1: Top: distribution of risk scores for Broward County data (le), and simulated data drawn from two beta distributions with equal means (right). Bottom: using a single threshold which detains $3 0 \\%$ of defendants in Broward County violates statistical parity (as measured by detention rate), predictive equality (false positive rate), and conditional statistical parity (detention rate conditional on number of prior arrests). We omit the last measure for the simulated data since that would require making additional assumptions about the relationship of priors and risk in the hypothetical populations.", "figure_type": "plot", "evidence_from_figure": "Red curve is flatter than blue curve", "evidence_from_text": "Caption states 'simulated data drawn from two beta distributions with equal means (right)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1701.08230_1701.08230_fig_5_136", "query": "Why does Figure 2 support the claim that COMPAS scores are calibrated as described in the text?", "answer": "The overlapping red and blue lines across all risk scores show that Black and White defendants with the same risk score have nearly identical recidivism rates, aligning with the text's definition of calibration (equal recidivism likelihood for same risk score).", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg", "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.", "figure_type": "plot", "evidence_from_figure": "Overlapping lines for Black and White defendants across all risk scores", "evidence_from_text": "Text states: 'White and black defendants with the same risk score are roughly equally likely to reoffend, indicating that the scores are calibrated.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1701.08230_1701.08230_fig_5_137", "query": "What does the gray band in Figure 2 represent, and how does it relate to the study's conclusion about racial fairness?", "answer": "The gray bands represent 95% confidence intervals. Their overlap with both racial groups' lines confirms that observed recidivism rate differences are not statistically significant, supporting the conclusion that COMPAS scores are calibrated (racially fair).", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig4.jpg", "caption": "Figure 2: Recidivism rate by COMPAS risk score and race. White and black defendants with the same risk score are roughly equally likely to reoend, indicating that the scores are calibrated. e $y$ -axis shows the proportion of defenydants re-arrested for any crime, including non-violent offenses; the gray bands show $9 5 \\%$ condence intervals.", "figure_type": "plot", "evidence_from_figure": "Gray shaded regions around both lines", "evidence_from_text": "Caption states: 'the gray bands show 95% confidence intervals'; text concludes scores are calibrated (indicating racial fairness).", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1701.08230_1701.08230_fig_6_138", "query": "In Figure 3, how does the red line's position relative to the dashed vertical line demonstrate that the altered risk scores are less informative?", "answer": "The red line lies entirely to the left of the dashed threshold line, meaning no defendants have risk scores above the threshold. This violates the text's explanation that altered scores 'guarantee no defendants fall above the detention threshold' and are 'less informative' because they fail to reflect true risk variability.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg", "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).", "figure_type": "plot", "evidence_from_figure": "red line's entire distribution left of dashed vertical line", "evidence_from_text": "text states altered scores 'guarantee no defendants fall above the detention threshold' and are 'less informative'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1701.08230_1701.08230_fig_6_139", "query": "Why does the text claim calibration is insufficient to assess discrimination using Figure 3?", "answer": "The figure shows both distributions (black and red) are calibrated (as stated in the caption), but the red line's altered shape ensures no defendants exceed the threshold. The text explains this demonstrates that calibration alone cannot detect discrimination because the scores are manipulated to produce misleading outcomes despite statistical calibration.", "doc_id": "1701.08230", "figure_id": "1701.08230_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1701.08230/1701.08230/hybrid_auto/images/1701.08230_page0_fig5.jpg", "caption": "Figure 3: Calibration is insucient to assess discrimination. In the le plot, the black line shows the distribution of risk in a hypothetical population, and the red line shows strategically altered risk estimates in the same population. Both sets of risk scores are calibrated (right plot), but the altered risk scores are less informative and as a result guarantee that no defendants fall above the detention threshold (dashed vertical line).", "figure_type": "plot", "evidence_from_figure": "two curves with different shapes but shared calibration (implied by caption)", "evidence_from_text": "text states 'calibration is insufficient to assess discrimination' and explains altered scores 'are less informative'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1703.06856_1703.06856_fig_8_140", "query": "In Figure 3, which path from Race to Searched represents the 'direct' path as defined in the text for controlled direct effects?", "answer": "The direct arrow from Race to Searched, as the text defines direct paths as those in P_G_A (not mediated by Criminality)", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "evidence_from_figure": "Arrow connecting Race to Searched", "evidence_from_text": "Definition of 'direct paths' as P_G_A in the text", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1703.06856_1703.06856_fig_8_141", "query": "Which variable in Figure 3 corresponds to the 'weapon was found' data point mentioned in the text section 'Section: S6 Case Study: NYC Stop-and-Frisk Data'?", "answer": "Weapon, as it is directly connected to Criminality and explicitly referenced in the text as a recorded data point", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_8", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig7.jpg", "caption": "Figure 3: A causal model for the stop and frisk dataset.", "figure_type": "diagram", "evidence_from_figure": "Weapon node connected to Criminality", "evidence_from_text": "Text description of NYPD recording 'if a weapon was found'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1703.06856_1703.06856_fig_9_142", "query": "In Figure 4, why does the 'Arrest if White (counterfactual)' map show higher red dot density in the yellow-circled area compared to the 'Arrested (data)' map?", "answer": "The model predicts higher arrest rates for White individuals in this area than observed in reality, as the counterfactual simulates arrests under the assumption that all individuals are White, revealing racial disparities in the arrest process.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_9", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg", "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.", "figure_type": "plot", "evidence_from_figure": "Visual comparison of red dot density in yellow-circled area between third and second maps", "evidence_from_text": "Text explains the model computes counterfactual arrests based on race manipulation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1703.06856_1703.06856_fig_9_143", "query": "What is the total number of male stops in the dataset used for Figure 4, and how does the 'Arrest if Black Hispanic (counterfactual)' map illustrate the model's findings?", "answer": "The dataset contains 38,609 male stops (from 2014). The fourth map shows higher arrest density for Black Hispanic counterfactuals in certain areas, indicating the model's prediction of increased arrests for Black Hispanic individuals compared to actual arrest patterns.", "doc_id": "1703.06856", "figure_id": "1703.06856_fig_9", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1703.06856/1703.06856/hybrid_auto/images/1703.06856_page0_fig8.jpg", "caption": "Figure 4: How race affects arrest. The above maps show how altering one’s race affects whether or not they will be arrested, according to the model. The left-most plot shows the distribution of White and Black Hispanic populations in the stop-and-frisk dataset. The second plot shows the true arrests for all of the stops. Given our model we can compute whether or not every individual in the dataset would be arrest had they been white. We show this counterfactual in the third plot. Similarly, we can compute this counterfactual if everyone had been Black Hispanic, as shown in the fourth plot.", "figure_type": "plot", "evidence_from_figure": "Fourth map's red dot distribution in purple-circled areas", "evidence_from_text": "Text states '38,609 records' and describes counterfactual modeling", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1705.10378_1705.10378_fig_4_144", "query": "In Figure 2(a), which node corresponds to 'prior convictions' as mentioned in the text?", "answer": "M", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg", "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.", "figure_type": "diagram", "evidence_from_figure": "Node M in the causal graph", "evidence_from_text": "Text states: 'prior convictions is the mediator M'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1705.10378_1705.10378_fig_4_145", "query": "What does the green curved arrow from M to Y in Figure 2(a) represent in the COMPAS dataset's causal model?", "answer": "It represents the direct effect of the mediator (prior convictions) on the outcome (recidivism), which can be regularized without compromising fair inferences when estimating NDE.", "doc_id": "1705.10378", "figure_id": "1705.10378_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1705.10378/1705.10378/hybrid_auto/images/1705.10378_page0_fig3.jpg", "caption": "Figure 2: Causal graphs for (a) the COMPAS dataset, and (b) the Adult dataset.", "figure_type": "diagram", "evidence_from_figure": "Green curved arrow from M to Y", "evidence_from_text": "Text states: 'the part of the model involving the outcome Y may be regularized... if the NDE quantifying discrimination is estimated using methods robust to misspecification of the Y model'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02409_1706.02409_fig_4_146", "query": "Why does Figure 1 contain six curves for the dataset shown?", "answer": "The six curves correspond to three fairness notions (group, individual, hybrid) for binary-valued targets, each evaluated with single or separate models, as explained in the text: 'For datasets with binary-valued targets... three fairness notions... for each examine building a single model or separate models... yielding a total of six curves.'", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg", "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.", "figure_type": "plot", "evidence_from_figure": "Six distinct curves in the plot", "evidence_from_text": "Text section explaining six curves for binary targets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02409_1706.02409_fig_4_147", "query": "At Fairness Loss = 0.03, how does the MSE of the 'individual, single' model compare to the 'group, single' model?", "answer": "The 'individual, single' model (purple curve) has a higher MSE than the 'group, single' model (red curve) at Fairness Loss = 0.03, as observed in the plot. The text confirms individual fairness is one of the three fairness notions for binary targets.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig3.jpg", "caption": "Figure 1: Efficient frontiers of accuracy vs. fairness for each dataset. For datasets with binary-valued targets (logistic regression), we consider three fairness notions (group, individual and hybrid), and for each examine building a single model or separate models for each group, yielding a total of six curves. For real-valued targets (linear regression), we consider two fairness notions (group and individual), and again single or separate models, yielding a total of four curves.", "figure_type": "plot", "evidence_from_figure": "Curves for 'individual, single' and 'group, single' at x=0.03", "evidence_from_text": "Text defining individual fairness as a binary-target fairness notion", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02409_1706.02409_fig_10_148", "query": "In Figure 2, what is the Price of Fairness for Group, separate vs. Group, single when α=0.01?", "answer": "Group, separate has a higher Price of Fairness than Group, single at α=0.01 (≈1.5 vs. ≈1.4).", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_10", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg", "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.", "figure_type": "plot", "evidence_from_figure": "Bar heights for Group, separate (blue) and Group, single (red) at α=0.01", "evidence_from_text": "Text confirms 'single and separate model case' for Group regularizers", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02409_1706.02409_fig_10_149", "query": "Which fairness regularizer shows the highest Price of Fairness at α=0.02 in Figure 2?", "answer": "Hybrid, separate (magenta) shows the highest Price of Fairness at α=0.02.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_10", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg", "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.", "figure_type": "plot", "evidence_from_figure": "Tallest bar at α=0.02 corresponds to magenta (Hybrid, separate)", "evidence_from_text": "Text specifies 'Hybrid' as one of the fairness regularizer types", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02409_1706.02409_fig_10_150", "query": "At α=0.05, how does Individual, single compare to Individual, separate in Figure 2?", "answer": "Individual, single (green) has a higher Price of Fairness than Individual, separate (cyan) at α=0.05.", "doc_id": "1706.02409", "figure_id": "1706.02409_fig_10", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02409/1706.02409/hybrid_auto/images/1706.02409_page0_fig9.jpg", "caption": "Figure 2: The “Price of Fairness” across data sets, for each type of fairness regularizer, in both the single and separate model case.", "figure_type": "plot", "evidence_from_figure": "Green bar height exceeds cyan bar height at α=0.05", "evidence_from_text": "Text confirms 'individual' regularizer comparisons between single/separate models", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_1_151", "query": "In Figure 1, does the arrow from A to R indicate a direct causal effect of gender on admission decision, as discussed in the text?", "answer": "Yes, the arrow from A to R shows a direct causal effect. The text explains this direct effect is critical because it cannot be attributed to the mediator variable X (department choice).", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "evidence_from_figure": "Arrow from A to R", "evidence_from_text": "Text states: 'the direct effect of the protected attribute (here, gender A) on the decision (here, college admission R) that cannot be ascribed to a resolving variable such as department choice X'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_1_152", "query": "According to Figure 1 and the text, what role does department choice (X) play in the causal relationship between gender (A) and admission decision (R)?", "answer": "X acts as a mediator: gender (A) influences department choice (X), which in turn influences admission decision (R). The text confirms this by stating X is 'affected by gender A' and 'affects R'.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig0.jpg", "caption": "Figure 1: The admission decision $R$ does not only directly depend on gender $A$ , but also on department choice $X$ , which in turn is also affected by gender $A$ .", "figure_type": "diagram", "evidence_from_figure": "Arrows from A→X and X→R", "evidence_from_text": "Text states: 'admission decision R does not only directly depend on gender A, but also on department choice X, which in turn is also affected by gender A'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_2_153", "query": "Why does R* exhibit unresolved discrimination in the right graph of Figure 2 but not in the left graph?", "answer": "The right graph contains additional paths (e.g., X₁→X₂→Y) that create unresolved discrimination in R*, while the left graph's structure (with X₁ directly influencing Y) allows X₁ to resolve discrimination as a resolving variable.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg", "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.", "figure_type": "diagram", "evidence_from_figure": "left graph structure (X₁→Y, X₁→R*, X₂→R*)", "evidence_from_text": "text states unresolved discrimination occurs in the right graph along red paths but not the left when X₁ is a resolving variable", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_2_154", "query": "How does the presence of X₁ as a resolving variable affect discrimination in R* in the left graph of Figure 2?", "answer": "X₁ acts as a resolving variable in the left graph, preventing unresolved discrimination in R* by directly mediating the relationship between A and Y without indirect paths that would cause discrimination.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig1.jpg", "caption": "Figure 2: Two graphs that may generate the same joint distribution for the Bayes optimal unconstrained predictor $R ^ { * }$ . If $X _ { 1 }$ is a resolving variable, $R ^ { * }$ exhibits unresolved discrimination in the right graph (along the red paths), but not in the left one.", "figure_type": "diagram", "evidence_from_figure": "X₁'s direct arrows to Y and R* in the left graph", "evidence_from_text": "text explains that X₁ resolves discrimination in the left graph but not the right", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_4_155", "query": "Why does the edge from P to R change from tilde g to g between the left and right graphs in Figure 3?", "answer": "The adjustment from tilde g to g cancels the influence of P on R via the P→X→R pathway, as explained in the text: 'adjusting P→R to cancel the influence along P→X→R in the intervened graph'.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "evidence_from_figure": "Edge labels 'tilde g' (left) and 'g' (right) on P→R", "evidence_from_text": "Text states: 'adjusting P→R to cancel the influence along P→X→R'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_4_156", "query": "What does the red circle around P in the right graph of Figure 3 indicate?", "answer": "The red circle indicates an intervention on the proxy P in the intervened graph, as part of the procedure to eliminate its overall influence on the prediction R while preserving A's influence.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig3.jpg", "caption": "Figure 3: A template graph $\\tilde { \\mathcal { G } }$ for proxy discrimination (left) with its intervened version $\\mathcal { G }$ (right). While from the benevolent viewpoint we do not generically prohibit any influence from $A$ on $R$ , we want to guarantee that the proxy $P$ has no overall influence on the prediction, by adjusting $P  R$ to cancel the influence along $P $ $X  R$ in the intervened graph.", "figure_type": "diagram", "evidence_from_figure": "Red circle around P in the right graph", "evidence_from_text": "Text describes the right graph as 'its intervened version G' and states the goal is to 'guarantee that the proxy P has no overall influence on the prediction'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_6_157", "query": "What does the right side of Figure 5 represent in terms of causal intervention?", "answer": "The right side represents the graph after intervening on P (denoted as do(P)), which blocks all causal paths from P to other variables. This is confirmed by the caption stating it corresponds to 'an intervention on P' and the text explaining intervention blocks paths through the intervened variable.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_6", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg", "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.", "figure_type": "diagram", "evidence_from_figure": "Right panel with intervention on P", "evidence_from_text": "Caption: 'Right: The graph corresponding to an intervention on P' and text discussing intervention effects", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1706.02744_1706.02744_fig_6_158", "query": "How does the DAG node relate to the structural equation model described in the text?", "answer": "The DAG node represents any sub-DAG of the original graph containing arbitrary variables compatible with the arrows. This aligns with the text's description of the DAG as part of the structural equation model where variables are connected through causal relationships.", "doc_id": "1706.02744", "figure_id": "1706.02744_fig_6", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1706.02744/1706.02744/hybrid_auto/images/1706.02744_page0_fig5.jpg", "caption": "Figure 5: Left: A generic graph $\\tilde { \\mathcal { G } }$ to describe proxy discrimination. Right: The graph corresponding to an intervention on $P$ . The circle labeled “DAG” represents any sub-DAG of $\\tilde { \\mathcal { G } }$ and $\\mathcal { G }$ containing an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can, but do not have to be present in a given scenario.", "figure_type": "diagram", "evidence_from_figure": "DAG node label and description in caption", "evidence_from_text": "Text stating 'the circle labeled 'DAG' represents any sub-DAG of tilde G and G containing an arbitrary number of variables'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.00574_1707.00574_fig_2_159", "query": "What is the value of τ at α=1.0 and β=0.4 in Figure 1(c)?", "answer": "Approximately 0.7 (orange color in the heatmap corresponds to τ ≈ 0.7 on the color bar)", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg", "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.", "figure_type": "plot", "evidence_from_figure": "Color at coordinates (α=1.0, β=0.4) in the heatmap", "evidence_from_text": "Caption identifies part (c) as τ vs α and β", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.00574_1707.00574_fig_2_160", "query": "How does τ change with β when α=0.5 in Figure 1(c)?", "answer": "τ decreases as β increases (from yellow to black in the row α=0.5)", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg", "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.", "figure_type": "plot", "evidence_from_figure": "Color progression in the row α=0.5", "evidence_from_text": "Caption states τ depends on α and β", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.00574_1707.00574_fig_2_161", "query": "Why does the maximum τ occur at α=0.0 and β=0.0 in Figure 1(c)?", "answer": "Lower popularity bias (α) and lower β maximize faithfulness, as τ peaks at these minimum parameter values", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig1.jpg", "caption": "Figure 1: Effects of popularity bias on average quality and faithfulness.. (a) Heatmap of average quality $q$ as a function of $\\alpha$ and $\\beta$ , showing that $q$ reaches a maximum for $\\alpha = 1$ and $\\beta \\approx 0 . 4$ , while for $\\alpha = 3$ the maximum is attained for a lower $\\beta$ . ( $b$ ) The location of the maximum $q$ as a function of $\\beta$ depends on $\\alpha$ , here shown for $\\alpha = 0 , 0 . 5 , 1 . 0$ . (c) Faithfulness $\\tau$ of the algorithm as a function of $\\alpha$ and $\\beta$ . ( $d$ ) $\\tau$ as a function of $\\beta$ for the same three values of $\\alpha$ . Standard errors are shown in panels $( b , d )$ and are smaller than the markers.", "figure_type": "plot", "evidence_from_figure": "Peak τ value at (α=0.0, β=0.0)", "evidence_from_text": "Caption explains τ reflects faithfulness under popularity bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.00574_1707.00574_fig_5_162", "query": "Why does the α=2 case converge earlier to a lower average quality than α=1 in Figure 2, as explained in the text?", "answer": "The text states that with less exploration (α=2), the system converges early to sub-optimal quality due to reduced exploration of high-quality items.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg", "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.", "figure_type": "plot", "evidence_from_figure": "α=2 (black line) shows faster convergence to lower q̄ plateau", "evidence_from_text": "With less exploration the system converges early to sub-optimal quality", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.00574_1707.00574_fig_5_163", "query": "How does β=0.1 in Figure 2 relate to the text's discussion of popularity bias in cultural markets?", "answer": "β=0.1 represents the specific popularity bias parameter used in the study, where higher β would increase reliance on popularity, affecting quality dynamics as described in the text.", "doc_id": "1707.00574", "figure_id": "1707.00574_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.00574/1707.00574/hybrid_auto/images/1707.00574_page0_fig4.jpg", "caption": "Figure 2: Temporal evolution of average quality. Average quality $q$ is traced over time for different values of popularity bias $\\beta$ , in two cases of higher and lower exploration ( $\\alpha = 1$ and $\\alpha = 2$ , respectively). Error bars represent standard errors across runs. With less exploration the system converges early to sub-optimal quality.", "figure_type": "plot", "evidence_from_figure": "β=0.1 label in figure", "evidence_from_text": "Success in these markets may strongly depend on... popularity of an item is hugely advantageous", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.09457_1707.09457_fig_1_164", "query": "In Figure 1, which image shows 'AGENT: MAN' and what percentage of cooking images have 'MAN' as the agent role after calibration according to the text?", "answer": "The fifth image shows 'AGENT: MAN'; after calibration, 20% of cooking images have 'MAN' as the agent role.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg", "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.", "figure_type": "example", "evidence_from_figure": "Last image's table with AGENT: MAN", "evidence_from_text": "After applying our methods, man appears in the agent role of 20% of cooking images", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.09457_1707.09457_fig_1_165", "query": "How does the calibration method reduce bias amplification in the agent role for cooking images, as shown in Figure 1?", "answer": "The calibration method increases the agent role percentage for 'MAN' from 16% (after CRF) to 20%, reducing bias amplification by 25% while maintaining performance.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig0.jpg", "caption": "Figure 1: Five example images from the imSitu visual semantic role labeling (vSRL) dataset. Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman. In the imSitu training set, $33 \\%$ of cooking images have man in the agent role while the rest have woman. After training a Conditional Random Field (CRF), bias is amplified: man fills $16 \\%$ of agent roles in cooking images. To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions. After applying our methods, man appears in the agent role of $20 \\%$ of cooking images, reducing the bias amplification by $2 5 \\%$ , while keeping the CRF vSRL performance unchanged.", "figure_type": "example", "evidence_from_figure": "Examples showing WOMAN and MAN as agents", "evidence_from_text": "After calibration, man appears in 20% of cooking images, reducing bias amplification by 25%", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.09457_1707.09457_fig_2_166", "query": "Why does the verb 'shopping' have a training gender ratio of ~0.2 but a predicted gender ratio of ~0.1 in Figure 2(a)?", "answer": "The model amplifies the existing female bias in the training data (training ratio = 0.2), resulting in a more extreme female bias in predictions (predicted ratio = 0.1), consistent with the text's claim of bias amplification after training on biased data.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg", "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.", "figure_type": "plot", "evidence_from_figure": "Data point for 'shopping' at (x=0.2, y=0.1) below the blue line", "evidence_from_text": "Text states 'significant bias amplification after training on biased training data' and describes bias amplification related to initial bias size", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.09457_1707.09457_fig_2_167", "query": "What percentage of verbs have a training gender ratio ≥0.7 according to the text?", "answer": "46.95% of verbs favor a gender with a bias of at least 0.7, as stated in the text.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig1.jpg", "caption": "Figure 2: Gender bias analysis of imSitu vSRL and MS-COCO MLC. (a) gender bias of verbs toward man in the training set versus bias on a predicted development set. (b) gender bias of nouns toward man in the training set versus bias on the predicted development set. Values near zero indicate bias toward woman while values near 0.5 indicate unbiased variables. Across both dataset, there is significant bias toward males, and significant bias amplification after training on biased training data.", "figure_type": "plot", "evidence_from_figure": "X-axis represents training gender ratio, with verbs ≥0.7 shown on the right side of the plot", "evidence_from_text": "Text states '46.95% of verbs favor a gender with a bias of at least 0.7'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.09457_1707.09457_fig_7_168", "query": "In Figure 3, how does RBA affect the number of violations (red points) at a training gender ratio of 0.7 according to the text?", "answer": "The text reports a 5% reduction in violations with RBA, which is visually evident in the figure where red points decrease at x=0.7.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_7", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg", "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.", "figure_type": "plot", "evidence_from_figure": "red points at x=0.7", "evidence_from_text": "reduced the number of objects whose bias exceeds the original training bias by 5% (Viol.)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1707.09457_1707.09457_fig_7_169", "query": "What does the 0.05 margin in Figure 3 indicate, and how does it relate to the 'Viol.' metric in the text?", "answer": "The dotted blue lines represent the 0.05 margin threshold; violations (red points) exceed this margin, and the 'Viol.' metric quantifies these violations, which the text states is reduced by 5% with RBA.", "doc_id": "1707.09457", "figure_id": "1707.09457_fig_7", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1707.09457/1707.09457/hybrid_auto/images/1707.09457_page0_fig6.jpg", "caption": "Figure 3: Results of reducing bias amplification using RBA on imSitu vSRL and MS-COCO MLC. Figures 3(a)-(d) show initial training set bias along the x-axis and development set bias along the yaxis. Dotted blue lines indicate the 0.05 margin used in RBA, with points violating the margin shown in red while points meeting the margin are shown in green. Across both settings adding RBA significantly reduces the number of violations, and reduces the bias amplification significantly. Figures 3(e)-(f) demonstrate bias amplification as a function of training bias, with and without RBA. Across all initial training biases, RBA is able to reduce the bias amplification.", "figure_type": "plot", "evidence_from_figure": "dotted blue lines as 0.05 margin", "evidence_from_text": "Dotted blue lines indicate the 0.05 margin used in RBA... (Viol.)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_3_170", "query": "What do the blue and red lines in Figure 1 represent, and how do they relate to the concepts of calibration classifiers for two groups?", "answer": "The blue line represents H1* (calibrated classifiers for group 1), and the red line represents H2* (calibrated classifiers for group 2), as defined in the caption.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_3", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg", "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ", "figure_type": "plot", "evidence_from_figure": "Blue and red lines within the triangular plot", "evidence_from_text": "Caption states 'H1*, H2* are the set of cal. classifiers for the two groups'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_3_171", "query": "How do the points labeled h1 and h2 in Figure 1 correspond to the trivial classifiers mentioned in the text?", "answer": "h1 and h2 correspond to the trivial classifiers h^μ1 and h^μ2, which are specific points on the calibration lines for the two groups.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_3", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig2.jpg", "caption": "Figure 1: Calibration, trivial classifiers, and equal-cost constraints – plotted in the false-pos./false-neg. plane. $\\mathcal { H } _ { 1 } ^ { \\ast } , \\mathcal { H } _ { 2 } ^ { \\ast }$ are the set of cal. classifiers for the two groups, and $h ^ { \\mu _ { 1 } } , h ^ { \\mu _ { 2 } }$ are trivial classifiers.   ", "figure_type": "plot", "evidence_from_figure": "Black dots labeled h1 and h2 on the plot", "evidence_from_text": "Caption identifies 'h^μ1, h^μ2 are trivial classifiers'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_6_172", "query": "In Figure 2, what does the point h̃₂ indicate about the relaxation method described in Section 4?", "answer": "h̃₂ represents a model that satisfies the single equal-cost constraint while maintaining calibration for each group, as part of the relaxation path h^μ₂ introduced in Section 4.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_6", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg", "caption": "Figure 2: Calibration-Preserving Parity through interpolation.", "figure_type": "plot", "evidence_from_figure": "Position of h̃₂ on the dashed line h^μ₂", "evidence_from_text": "Section 4's description of a relaxation maintaining calibration while satisfying a single equal-cost constraint", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_6_173", "query": "How does the blue line h₁ relate to the Equalized Odds constraints mentioned in the text?", "answer": "h₁ represents the set of models satisfying Equalized Odds by equalizing false-positives and false-negatives, as defined in the text where c_fp(h_t) = c_fn(h_t).", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_6", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig5.jpg", "caption": "Figure 2: Calibration-Preserving Parity through interpolation.", "figure_type": "plot", "evidence_from_figure": "Blue line labeled h₁ with a black dot", "evidence_from_text": "Text stating 'Equalized Odds sets constraints to equalize false-positives c_fp(h_t) and false-negatives c_fn(h_t)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_9_174", "query": "How does the linear interpolation equation for g₂(\\tilde{h}_2) in the text relate to the position of the diamond markers in the \"Calib. + Equal F.N.\" graph?", "answer": "The diamond markers in the Calib. + Equal F.N. graph correspond to classifiers where the cost of h₂ is a linear interpolation between h₂ and h^μ₂, as defined by the equation, positioning them along the line connecting these two points.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg", "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "figure_type": "plot", "evidence_from_figure": "Diamond markers in the right subplot (Calib. + Equal F.N.)", "evidence_from_text": "Equation g₂(\\tilde{h}_2) = (1 - α)g₂(h₂) + αg₂(h^μ₂)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_9_175", "query": "What do the points on the \"Equal Odds (Derived)\" graph represent according to the figure caption and the text's discussion of hyperparameter modification?", "answer": "The points on the \"Equal Odds (Derived)\" graph represent classifiers achieved by modifying constraint hyperparameters, as stated in the figure caption and the text's discussion of hyperparameter adjustments.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg", "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "figure_type": "plot", "evidence_from_figure": "Points on the left subplot (Equal Odds) labeled as 'trained' classifiers", "evidence_from_text": "Text before figure: 'classifiers achieved by modifying constraint hyperparameters'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1709.02012_1709.02012_fig_9_176", "query": "Why do the diamonds in the \"Calib. + Equal F.N.\" graph exhibit higher Generalized F.N. rates compared to the points in the Equalized Odds graph, as discussed in the text?", "answer": "The diamonds in the Calib. + Equal F.N. graph show higher Generalized F.N. rates than the points in the Equalized Odds graph, indicating performance degradation as explained in the text's claim that calibration and equal cost constraints often result in such degradation.", "doc_id": "1709.02012", "figure_id": "1709.02012_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1709.02012/1709.02012/hybrid_auto/images/1709.02012_page0_fig8.jpg", "caption": "Figure 3: Generalized F.P. and F.N. rates for two groups under Equalized Odds and the calibrated relaxation. Diamonds represent post-processed classifiers. Points on the Equalized Odds (trained) graph represent classifiers achieved by modifying constraint hyperparameters.", "figure_type": "plot", "evidence_from_figure": "Diamonds in right subplot have higher F.N. rates than left subplot points", "evidence_from_text": "Text after figure: 'imposing calibration and an equal cost constraint... results in performance degradation'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.08615_1710.08615_fig_1_177", "query": "How does the trend in Figure 2 (probability of retweeting increasing then fluctuating with friends) illustrate Simpson's paradox as described in the abstract?", "answer": "The abstract states Simpson's paradox occurs when aggregated data trends differ from subgroups. The figure's aggregated trend shows increasing retweet probability with friends, but subgroup analysis might reveal contrasting trends (e.g., users with few friends may have low retweet rates while others have higher), demonstrating how aggregated data masks subgroup differences.", "doc_id": "1710.08615", "figure_id": "1710.08615_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "The upward trend followed by fluctuations in retweet probability as friends increase", "evidence_from_text": "Abstract's explanation of Simpson's paradox: 'trends observed in aggregated data may differ from underlying subgroups'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.08615_1710.08615_fig_1_178", "query": "Why does the fluctuating trend after 50 friends in Figure 2 suggest heterogeneity, as mentioned in the abstract?", "answer": "Heterogeneity implies subgroups with varying behaviors. The post-50 fluctuation suggests the aggregated data combines subgroups with opposing trends (e.g., users with very high friend counts may have inconsistent retweet behaviors), which aligns with the abstract's claim that heterogeneity predisposes analysis to Simpson's paradox.", "doc_id": "1710.08615", "figure_id": "1710.08615_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.08615/1710.08615/hybrid_auto/images/1710.08615_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Fluctuations in the data series beyond 50 friends", "evidence_from_text": "Abstract's description of heterogeneity: 'subgroups within the population under study that vary in size and behavior'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.11214_1710.11214_fig_1_179", "query": "In Figure 1, what role does the 'preference' label connected to the user silhouette play in confounding as described in the text?", "answer": "The 'preference' label represents confounding factors that influence both recommendations (via past interactions) and current user interactions, as stated in the text. This creates a feedback loop where user preferences are both shaped by and shape the recommendation system.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg", "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.", "figure_type": "diagram", "evidence_from_figure": "Speech bubble labeled 'preference' connected to the user silhouette", "evidence_from_text": "Text states: 'User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.11214_1710.11214_fig_1_180", "query": "How does the 'interaction' arrow in Figure 1 relate to the concept of algorithmic confounding explained in the text?", "answer": "The 'interaction' arrow (user → platform) represents data used to train the recommendation model. As the text explains, this data is confounded because it reflects user behavior influenced by prior recommendations, violating the assumption of independent observations in model training.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig0.jpg", "caption": "Figure 1: The feedback loop between user behavior and algorithmic recommendation systems. Confounding occurs when a platform attempts to model user behavior without accounting for recommendations. User preferences act as confounding factors, influencing both recommendations (through past interactions) and current interactions.", "figure_type": "diagram", "evidence_from_figure": "Arrow labeled 'interaction' from user to platform", "evidence_from_text": "Text states: 'the underlying recommendation model is trained using data that are confounded by algorithmic recommendations from a previously deployed system.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.11214_1710.11214_fig_2_181", "query": "How does the clustered high-utility pattern in Figure 2 relate to the claim that matrix factorization can 'easily capture' the structure?", "answer": "The clustered darker regions in the matrix (e.g., the dense block around items 2500-5000) represent structured user preferences that matrix factorization can decompose into latent features, as explained in the text's section on recommendation algorithms.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg", "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.", "figure_type": "plot", "evidence_from_figure": "Dense clusters of darker green regions in the utility matrix", "evidence_from_text": "Section 5.1's description of matrix factorization as a method that captures underlying structures", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.11214_1710.11214_fig_2_182", "query": "Why does the sparse structure of Figure 2 match 'commonly accepted intuitions about user behavior'?", "answer": "The sparsity (light regions) reflects real-world disproportionate preferences where users interact with only a small subset of items, as stated in the text: 'the distribution of user preferences is disproportionate, like the real world'.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig1.jpg", "caption": "Figure 2: Example true utility matrix  for simulated data; Vdarker is higher utility. The distribution of user preferences is disproportionate, like the real world, and the structure is easily captured with matrix factorization.", "figure_type": "plot", "evidence_from_figure": "Sparse light areas dominating the matrix", "evidence_from_text": "The paragraph stating 'the resulting matrix of true utility is sparse (e.g., figure 2), which matches commonly accepted intuitions about user behavior'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.11214_1710.11214_fig_4_183", "query": "Why does the 'popularity' algorithm in Figure 4 show the highest increase in Jaccard index compared to other algorithms?", "answer": "The 'popularity' algorithm shows the highest Jaccard index change because it maximizes global homogenization, as stated in the figure caption: 'Popularity increases homogenization the most globally.'", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg", "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.", "figure_type": "plot", "evidence_from_figure": "popularity line's steep upward trend", "evidence_from_text": "caption stating 'Popularity increases homogenization the most globally'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1710.11214_1710.11214_fig_4_184", "query": "How does the homogenization trend for 'popularity' in Figure 4 relate to utility loss for minority users mentioned in Section 5.3?", "answer": "The high homogenization (Jaccard index increase) for 'popularity' correlates with utility loss for minority users, as the text explains that users with poorly captured preferences may see 'lesser improvements or even decreases in utility' when homogenization occurs.", "doc_id": "1710.11214", "figure_id": "1710.11214_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1710.11214/1710.11214/hybrid_auto/images/1710.11214_page0_fig3.jpg", "caption": "Figure 4: For the repeated training case, change in Jaccard index of user behavior relative to ideal behavior; users paired randomly. Popularity increases homogenization the most globally, but all non-random recommendation algorithms also homogenize users globally.", "figure_type": "plot", "evidence_from_figure": "popularity line's significant upward trend", "evidence_from_text": "Section 5.3 stating 'minority users may see lesser improvements or even decreases in utility when homogenization occurs'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.05144_1711.05144_fig_1_185", "query": "In Figure 1(a), which curve corresponds to the largest γ value, and what does the text imply about the relationship between γ and the Learner's error?", "answer": "The topmost curve at iteration 0 (highest initial error) corresponds to the largest γ value. The text implies higher γ values (stricter fairness constraints) result in higher initial error for the Learner's model.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg", "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.", "figure_type": "plot", "evidence_from_figure": "Topmost curve at iteration 0 with highest initial error", "evidence_from_text": "Text states γ controls fairness constraints and mentions 'cost sensitive classification oracle' for balancing accuracy and fairness", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.05144_1711.05144_fig_1_186", "query": "What does the convergence of error curves to ~0.15 by iteration 800 indicate about the Learner's model performance, as described in the text?", "answer": "The convergence indicates the Learner's model stabilizes to a consistent error rate, reflecting the FairFictPlay algorithm's ability to balance fairness and accuracy across iterations.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig0.jpg", "caption": "Figure 1: Evolution of the error and unfairness of Learner’s classifier across iterations, for varying choices of γ. (a) Error $\\varepsilon _ { t }$ of Learner’s model vs iteration t. (b) Unfairness $\\gamma _ { t }$ of subgroup found by Auditor vs. iteration $t$ , as measured by Definition 2.3. See text for details.", "figure_type": "plot", "evidence_from_figure": "All curves converge to ~0.15 by iteration 800", "evidence_from_text": "Text describes FairFictPlay's implementation of cost-sensitive classification and auditing oracles for stable model performance", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.05144_1711.05144_fig_3_187", "query": "How does the color coding in Figure 2(a) correspond to the input parameter γ as described in the text?", "answer": "The colors represent different values of γ, where each color cluster corresponds to a specific γ value as explained in the caption stating 'color coded by varying values of the input parameter γ'.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg", "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.", "figure_type": "plot", "evidence_from_figure": "Color-coded data points forming distinct clusters along the Pareto frontier", "evidence_from_text": "Caption stating 'color coded by varying values of the input parameter γ'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.05144_1711.05144_fig_3_188", "query": "Why does the aggregate Pareto frontier in Figure 2(b) appear smoother than the per-gamma frontiers in Figure 2(a)?", "answer": "The text explains that Figure 2(b) uses γ values sampled more densely to achieve a smoother frontier, whereas Figure 2(a) shows discrete γ values that create visible gaps between clusters.", "doc_id": "1711.05144", "figure_id": "1711.05144_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.05144/1711.05144/hybrid_auto/images/1711.05144_page0_fig2.jpg", "caption": "Figure 2: (a) Pareto-optimal error-unfairness values, color coded by varying values of the input parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same range but are sampled more densely to get a smoother frontier. See text for details.", "figure_type": "plot", "evidence_from_figure": "Figure 2(a) shows clustered color groups with visible gaps between them", "evidence_from_text": "Caption stating 'γ values cover the same range but are sampled more densely to get a smoother frontier' for Figure 2(b)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.07076_1711.07076_fig_1_189", "query": "How does the DLP (dashed line) in Figure 1 address impact disparity as defined in the text?", "answer": "Impact disparity occurs when outcomes differ across subgroups (e.g., men vs. women). The DLP uses hair length (an irrelevant attribute) to shift the decision boundary, equalizing hiring rates despite the unconstrained classifier favoring candidates with more work experience.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg", "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.", "figure_type": "plot", "evidence_from_figure": "Dashed blue diagonal line crossing data points", "evidence_from_text": "Definition of impact disparity as 'outcomes differ across subgroups (even unintentionally)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.07076_1711.07076_fig_1_190", "query": "Why does the DLP in Figure 1 flip decisions for some short-haired women?", "answer": "The DLP's boundary (dashed line) reclassifies candidates based on hair length. Short-haired women fall on the side of the DLP boundary that rejects them, whereas the unconstrained classifier (vertical line) would have hired them, as explained in the caption's statement about 'hurting some short-haired women, flipping their decisions to reject'.", "doc_id": "1711.07076", "figure_id": "1711.07076_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.07076/1711.07076/hybrid_auto/images/1711.07076_page0_fig0.jpg", "caption": "Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data (see §4.1). An unconstrained classifier (vertical line) hires candidates based on work experience, yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired women, flipping their decisions to reject, and helps some long-haired men.", "figure_type": "plot", "evidence_from_figure": "Blue circles (women) near the DLP boundary", "evidence_from_text": "Caption: 'The DLP hurts some short-haired women, flipping their decisions to reject'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_1_191", "query": "What percentage of Open Images are from the US according to the figure, and how does the text explain the implications for models in the developing world?", "answer": "The US accounts for 32.1% of Open Images. The text states that non-representative data can harm model performance in the developing world.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg", "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ", "figure_type": "plot", "evidence_from_figure": "US segment labeled 32.1%", "evidence_from_text": "if these data sets are not representative of the locations of interest, predictive performance of models may suffer", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_1_192", "query": "What proxy information did the authors use to determine country of origin for images in Open Images, as referenced in the text and shown in Figure 1?", "answer": "The authors used textual/contextual information and URL metadata as proxies to determine country of origin.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg", "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ", "figure_type": "plot", "evidence_from_figure": "Country distribution chart from proxy data", "evidence_from_text": "proxy information such as textual / contextual information and URL metadata provided by a service allowed us to recover reasonably reliable location information", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_1_193", "query": "How does the text describe the relationship between Open Images and ImageNet distributions in Figure 1?", "answer": "The text states ImageNet had lower coverage but a similar distribution dominated by a few countries.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig0.jpg", "caption": "Figure 1: Fraction of Open Images and ImageNet images from each country. In both data sets, top represented locations include the US and Great Britain. Countries are represented by their two-letter ISO country codes.   ", "figure_type": "plot", "evidence_from_figure": "Open Images distribution shown in Figure 1", "evidence_from_text": "We had lower coverage for ImageNet, but the distribution was similarly dominated by a small number of countries, as shown in Figure 1", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_4_194", "query": "Why does the 'rater images' density plot in Figure 3 show a leftward shift compared to 'imagernet images'?", "answer": "The leftward shift indicates lower log-likelihood values for Hyderabad crowdsourced images, reflecting reduced model confidence. As explained in the text, this occurs because these images lack geo-diversity present in standard test sets, making them harder for models trained on ImageNet/Open Images to recognize correctly.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg", "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.", "figure_type": "plot", "evidence_from_figure": "Leftward shift of green ('rater images') curve relative to blue ('imagernet images') curve", "evidence_from_text": "Text states Hyderabad crowdsourced images are 'dramatically less likely to be recognized correctly by these models' due to geo-diversity issues", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_4_195", "query": "What does the peak location of the 'imagernet images' curve in Figure 3 imply about model performance?", "answer": "The peak at higher log-likelihood values (less negative) for 'imagernet images' implies greater model confidence and better recognition performance, as these images are part of standard test sets with sufficient geo-diversity compared to crowdsourced Hyderabad images.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig3.jpg", "caption": "Figure 3: Density plots of log-likelihood attributed for groom, bridegroom images crowdsourced by raters in Hyderabad, India, as scored by a model trained on ImageNet (left) and Open Images (center), as compared to images in the standard test sets. In both cases, the images provided by Hyderabad-located crowdsourcing are dramatically less likely to be recognized correctly by these models. The plot on right shows a similar trend for the woman class in OpenImages which has no corresponding class in ImageNet.", "figure_type": "plot", "evidence_from_figure": "Blue curve ('imagernet images') peak position near 0 log-likelihood", "evidence_from_text": "Text explains standard test sets have better geo-diversity, leading to higher recognition accuracy", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_7_196", "query": "In Figure 4, why does the United States curve show the highest peak compared to other countries?", "answer": "The text explains that the model was trained on Open Images, which lacks geo-diversity. US images have higher likelihoods because the training data overrepresents US locations, making the model better at classifying US images (as stated in the caption: 'Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US').", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg", "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "figure_type": "plot", "evidence_from_figure": "United States curve peak is highest", "evidence_from_text": "Text states lack of geo-diversity causes non-US images to have lower likelihoods", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_7_197", "query": "How does Figure 4 support the text's claim that Open Images lacks geo-diversity for developing world representation?", "answer": "The figure shows significantly lower log-likelihood densities for countries like Ethiopia, Nigeria, and Pakistan (developing nations) compared to the United States. This aligns with the text's conclusion that standard datasets fail to represent developing world contexts adequately.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg", "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "figure_type": "plot", "evidence_from_figure": "Low-density curves for Ethiopia/Nigeria/Pakistan vs. US", "evidence_from_text": "Text states 'standard open source data sets... may not have sufficient geo-diversity for broad representation across the developing world'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_7_198", "query": "What does the log-likelihood trend in Figure 4 imply about model performance for non-US bridegroom images?", "answer": "The lower log-likelihood values for non-US countries (e.g., Ethiopia, Nigeria) indicate poorer model performance on these images, consistent with the text's assertion that geo-diverse training data is critical for equitable classification across regions.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig6.jpg", "caption": "Figure 4: Density plots of log-likelihood attributed by the models trained on Open Images for images drawn from the groom, bridegroom, butcher, greengrocer, and police officer categories. Groom images with non-US location tags tend to have lower likelihoods than the groom images from the US.", "figure_type": "plot", "evidence_from_figure": "Non-US curves are shifted left with lower peaks", "evidence_from_text": "Text explains 'lack of geo-diversity in training data impacts classification performance on images drawn from a broader set of locations'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_10_199", "query": "What does the horizontal axis in Figure 5 represent, and why do Ethiopia and Pakistan have more scattered images compared to the United States and Australia?", "answer": "The horizontal axis represents the log-likelihood that the Open Images-trained classifier assigns to the bridegroom class. Ethiopia and Pakistan show more scattered distributions because the Open Images dataset lacks geo-diversity for developing-world representation, as explained in the text.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_10", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg", "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.", "figure_type": "plot", "evidence_from_figure": "Scattered vs. concentrated image distributions for Ethiopia/Pakistan vs. US/Australia", "evidence_from_text": "Text states Open Images lacks geo-diversity for developing-world representation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1711.08536_1711.08536_fig_10_200", "query": "Which countries' images are classified as bridegrooms with the highest log-likelihood scores in Figure 5, and how does this relate to the paper's claim about data set limitations?", "answer": "United States and Australia show the highest log-likelihood scores (rightmost cluster). This aligns with the paper's claim that Open Images lacks geo-diversity, causing inconsistent classification for developing-world countries like Ethiopia and Pakistan.", "doc_id": "1711.08536", "figure_id": "1711.08536_fig_10", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1711.08536/1711.08536/hybrid_auto/images/1711.08536_page0_fig9.jpg", "caption": "Figure 5: Photos of bridegrooms from different countries aligned by the log-likelihood that the classifier trained on Open Images assigns to the bridegroom class. Images from Ethiopia and Pakistan are not classified as consistently as images from the United States and Australia.", "figure_type": "plot", "evidence_from_figure": "Concentrated right-side clusters for US/Australia", "evidence_from_text": "Text states Open Images data sets may not have sufficient geo-diversity for broad representation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_1_201", "query": "How many instances of Simpson's paradox were identified in the Stack Exchange data, and does Figure 1 represent one of them?", "answer": "Seven instances were identified, and Figure 1 represents one of them as explicitly labeled in the caption and referenced in the text.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg", "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ", "figure_type": "plot", "evidence_from_figure": "Figure 1 is labeled as 'Simpson’s paradox in Stack Exchange data'", "evidence_from_text": "Text states 'our method identifies seven as instance of paradox. These are listed in Table 1'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_1_202", "query": "Why does Figure 1 show an upward trend in acceptance probability with answer position while the text states disaggregated data reverses this trend?", "answer": "The upward trend in Figure 1 (aggregated data) contrasts with the downward trend in disaggregated data (by session length), demonstrating Simpson's paradox where aggregation masks the true relationship.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg", "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ", "figure_type": "plot", "evidence_from_figure": "Dashed line in Figure 1 shows upward trend", "evidence_from_text": "Text states 'when data is disaggregated by session length (b), the trend reverses'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_1_203", "query": "What does the upward trend in Figure 1's aggregated data indicate about the previously reported finding that acceptance probability decreases with answer position?", "answer": "The upward trend in Figure 1 (aggregated data) contradicts the previous finding (disaggregated data), illustrating Simpson's paradox where aggregated data misrepresents the true relationship.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig0.jpg", "caption": "Figure 1: Simpson’s paradox in Stack Exchange data. Both plots show the probability an answer is accepted as the best answer to a question as a function of its position within user’s activity session. (a) Acceptance probability calculated over aggregated data has an upward trend, suggesting that answers written later in a session are more likely to be accepted as best answers. However, when data is disaggregated by session length (b), the trend reverses. Among answers produced during sessions of the same length (dierent colors represent dierent-length sessions), later answers are less likely to be accepted as best answers.   ", "figure_type": "plot", "evidence_from_figure": "Dashed line shows upward trend", "evidence_from_text": "Text states 'the previously reported finding that acceptance probability decreases with answer position [9] is an instance of Simpson's paradox'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_4_204", "query": "In Figure 2, why does the disaggregated data (b) show a downward trend while the aggregated data (a) shows an upward trend?", "answer": "The disaggregated data (b) reveals that within each reputation bin, more experienced users (higher number of answers) have lower acceptance probabilities, while aggregated data (a) masks this by combining all bins where higher-reputation users (who have more answers) dominate.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg", "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.", "figure_type": "plot", "evidence_from_figure": "Downward-sloping lines for each reputation bin", "evidence_from_text": "Caption states aggregated data (a) has upward trend, disaggregated (b) reverses", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_4_205", "query": "What does the color coding in Figure 2 represent, and how does it explain the Simpson's paradox?", "answer": "Colors represent reputation bins; each bin's downward trend (more answers → lower acceptance) reverses the aggregated upward trend, demonstrating Simpson's paradox where the overall trend is misleading without disaggregation.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig3.jpg", "caption": "Figure 2: Novel Simpson’s paradox discovered in Stack Exchange data. Plots show the probability an answer is accepted as best answer as a function of the number of lifetime answers written by user over his or her tenure. (a) Acceptance probability calculated over aggregated data has an upward trend, with answers written by more experienced users (who have already posted more answers) more likely to be accepted as best answers. However, when data is disaggregated by reputation (b), the trend reverses. Among answers written by users with the same reputation (dierent colors represent reputation bins), those posted by users who had already written more answers are less likely to be accepted as best answers.", "figure_type": "plot", "evidence_from_figure": "Different colors for data points and lines", "evidence_from_text": "Text states 'different colors represent reputation bins'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_5_206", "query": "How does the color gradient in Figure 3(a) illustrate the Simpson's paradox described in the text, specifically regarding the relationship between Reputation and Number of Answers?", "answer": "The color gradient shows higher acceptance probability (red) at higher numbers of answers and reputation, but Simpson's paradox implies that this trend may reverse when accounting for hidden variables. The text explains that the probability mass normalization for session lengths (Number of Answers) affects the observed average trend, creating a misleading correlation without subgroup analysis.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg", "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "evidence_from_figure": "Color gradient showing acceptance probability vs. reputation and number of answers", "evidence_from_text": "Explanation of probability mass normalization for sessions of same length", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_5_207", "query": "Why does the high density of data points at higher 'Number of Answers' (x-axis) in Figure 3(a) impact the observed acceptance probability trend, as referenced in the text?", "answer": "The text states that probability mass is normalized for sessions of length 'a', meaning higher data point density at larger 'Number of Answers' values biases the average acceptance probability. This density explains the upward trend in the color gradient, where more data points at high answer counts dominate the observed correlation.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig4.jpg", "caption": "Figure 3: Analysis of the Simpson’s paradox Reputation – Number of Answers variable pair. (a) Average acceptance probability as a function of two variables. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "evidence_from_figure": "Dense cluster of data points at higher x-axis values", "evidence_from_text": "Description of probability mass normalization for session lengths", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_7_208", "query": "Why does the aggregated data (black line) in Figure 4 show an upward trend despite subgroup-specific trends varying, as explained in the text?", "answer": "The text states that the probability mass function decreases for the smallest acceptance probability but increases for higher values at a greater rate, resulting in an upward trend when aggregated.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg", "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.", "figure_type": "plot", "evidence_from_figure": "Black line's upward trend", "evidence_from_text": "Explanation of probability mass function dynamics", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_7_209", "query": "How does the variance of the new Reputation Rate feature compare to Fig. 2b, and what does this imply about its utility?", "answer": "The caption states the new feature has much smaller variance than Fig. 2b, suggesting it is a better proxy for answerer performance.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig6.jpg", "caption": "Figure 4: Relationship between acceptance probability and Reputation Rate, a new measure of user performance de-ned as reputation per number of answers users wrote over their entire tenure. Each line represents a subgroup with a dierent reputation score. e much smaller variance compared to Fig. 2b suggests that the new feature is a good proxy of answerer performance.", "figure_type": "plot", "evidence_from_figure": "Figure 4 caption text", "evidence_from_text": "Context about Fig. 2b and proxy quality", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_8_210", "query": "Why does multivariate logistic regression fail to detect the trend in Figure 5(a) as described in the text?", "answer": "Multivariate logistic regression fails because the observed trend in Figure 5(a) is an artifact of Simpson's paradox, where the aggregated data masks the true relationships within subgroups. As explained in the text, the method identifies pairs where trends disappear when disaggregated by conditioning on a second variable, which logistic regression does not account for.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg", "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "evidence_from_figure": "The curved pattern of high acceptance probability in the middle of the plot (Figure 5(a))", "evidence_from_text": "The claim that 'multivariate logistic regression cannot find' the trend in the data due to subgroup structures", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.04385_1801.04385_fig_8_211", "query": "How does the data point distribution in Figure 5(a) relate to the study's conclusion about Simpson's paradox?", "answer": "The dense clustering of data points in the middle range of Answer Position and Time Since Previous Answer (Figure 5(a)) creates a misleading aggregated trend. This distribution exemplifies Simpson's paradox, where the overall trend (high acceptance probability) reverses or disappears when data is disaggregated into subgroups, as the study's method identifies.", "doc_id": "1801.04385", "figure_id": "1801.04385_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.04385/1801.04385/hybrid_auto/images/1801.04385_page0_fig7.jpg", "caption": "Figure 5: A pair which multivariate logistic regression cannot nd in the data. (a) Average acceptance probability as a function of Answer Position and Time Since Previous Answer. (b) e distribution of the number of data points contributing to the value of the outcome variable for each pair of variable values.", "figure_type": "plot", "evidence_from_figure": "The concentration of data points forming a curved peak in the plot", "evidence_from_text": "The study's conclusion that 'a trend in an outcome as a function of one variable disappears or reverses itself when disaggregated by conditioning on the second variable'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.07593_1801.07593_fig_1_212", "query": "In Figure 1, what is the role of L_A(ẑ, z) in the adversarial debiasing framework described in the text?", "answer": "L_A(ẑ, z) is the loss function for the Adversary network, which attempts to predict Z from the Predictor's output ŷ. The text explains this adversarial component is designed to detect and mitigate biases by training the Predictor to minimize L_P while the Adversary maximizes L_A, thus reducing information about sensitive attributes.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg", "caption": "Figure 1: The architecture of the adversarial network.", "figure_type": "diagram", "evidence_from_figure": "L_A(ẑ, z) label connected to the Adversary block", "evidence_from_text": "Section 3 describes the Adversary's role in predicting Z and the adversarial debiasing objective", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.07593_1801.07593_fig_1_213", "query": "How does the Predictor's loss function L_P(ŷ, y) relate to the gradient-based training method mentioned in the text?", "answer": "The Predictor minimizes L_P(ŷ, y) using gradient-based methods like stochastic gradient descent (SGD), as stated in the text. Figure 1 visually shows L_P as the loss applied to the Predictor's output ŷ, which drives the weight updates W during training to improve prediction accuracy for Y given X.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig0.jpg", "caption": "Figure 1: The architecture of the adversarial network.", "figure_type": "diagram", "evidence_from_figure": "L_P(ŷ, y) label above the Predictor's output", "evidence_from_text": "Text states 'the model is trained by attempting to modify weights W to minimize some loss L_P... using a gradient-based method such as stochastic gradient descent'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.07593_1801.07593_fig_2_214", "query": "In Figure 2, why does the direction labeled 'g + h' help the adversary according to the text?", "answer": "Without the projection term, the predictor moves in the 'g + h' direction, which aligns with the adversary's gradient (h = -α∇_W L_A). This direction inadvertently improves the adversary's ability to predict sensitive attributes, as explained in the text.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg", "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.", "figure_type": "diagram", "evidence_from_figure": "the 'g + h' vector label", "evidence_from_text": "the statement that 'g + h' helps the adversary", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.07593_1801.07593_fig_2_215", "query": "How does the term 'proj_h g' in Figure 2 relate to the adversary's gradient h = -α∇_W L_A?", "answer": "The projection term 'proj_h g' modifies the predictor's gradient by removing components aligned with the adversary's gradient (h), ensuring the predictor avoids directions that benefit the adversary. This is critical for achieving fairness as defined in the text.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg", "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.", "figure_type": "diagram", "evidence_from_figure": "the 'proj_h g' vector label", "evidence_from_text": "the explanation that projection prevents movement toward adversary-helpful directions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1801.07593_1801.07593_fig_2_216", "query": "What does the vector 'g = ∇_W L_P' represent in Figure 2, and how does it interact with the adversary's gradient?", "answer": "The vector 'g = ∇_W L_P' represents the predictor's gradient for minimizing its loss. The adversary's gradient (h) interacts with it to form 'g + h', which the text states benefits the adversary. Projection ensures the predictor's update avoids this harmful interaction.", "doc_id": "1801.07593", "figure_id": "1801.07593_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1801.07593/1801.07593/hybrid_auto/images/1801.07593_page0_fig1.jpg", "caption": "Figure 2: Diagram illustrating the gradients in Eqn. 1 and the relevance of the projection term $\\mathrm { p r o j } _ { h } g$ . Without the projection term, in the pictured scenario, the predictor would move in the direction labelled $g + h$ in the diagram, which actually helps the adversary. With the projection term, the predictor will never move in a direction that helps the adversary.", "figure_type": "diagram", "evidence_from_figure": "the 'g = ∇_W L_P' label", "evidence_from_text": "the description of adversarial debiasing and gradient interactions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_1_217", "query": "What is the role of node C in Figure 1(a) as described in the caption?", "answer": "Node C acts as a confounder for the causal effect of A on Y, as stated in the caption's description of Figure 1(a).", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg", "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .", "figure_type": "diagram", "evidence_from_figure": "Arrows from C to A and C to Y", "evidence_from_text": "Caption: 'GCM with a confounder C for the causal effect of A on Y'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_1_218", "query": "How does the causal structure in Figure 1(a) differ from the description of part (b) in the caption?", "answer": "Figure 1(a) shows a confounder C for A→Y, while part (b) describes a model with one direct and one indirect causal path from A to Y (e.g., A→M→Y).", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig0.jpg", "caption": "Figure 1. (a): GCM with a confounder $C$ for the causal effect of $A$ on $Y$ . (b): GCM with one direct and one indirect causal path from $A$ to $Y$ . (c): GCM with a confounder $C$ for the effect of $M$ on $Y$ .", "figure_type": "diagram", "evidence_from_figure": "Arrows from C to A, C to Y, and A to Y", "evidence_from_text": "Caption: '(b): GCM with one direct and one indirect causal path from A to Y'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_4_219", "query": "In Figure 2(c), which paths (indicated by green arrows) correspond to the path-specific effect (PSE) calculation described in the text?", "answer": "The green paths from A→W and W→Y, as these represent the causal pathways contributing to the PSE, which quantifies the difference in outcomes when A is set to different values.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg", "caption": "Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).", "figure_type": "diagram", "evidence_from_figure": "Green arrows connecting A→W and W→Y", "evidence_from_text": "Text states: 'The PSE, namely difference between this quantity and the mean of the effect of A = a'...'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_4_220", "query": "How does the variable W in Figure 2(c) relate to the latent variable ε_l mentioned in the text?", "answer": "W corresponds to the observed variable L mentioned in the text, with ε_l representing the unobserved Gaussian term affecting L (W) in the GCM.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig3.jpg", "caption": "Figure 2. (a)-(b): GCMs in which we are interested in the effects along the green paths. (c): GCM corresponding to Eq. (3).", "figure_type": "diagram", "evidence_from_figure": "Node labeled W with incoming arrows from A and M", "evidence_from_text": "Text states: 'The variables A, C, M, L and Y are observed, whilst ε_a, ε_c, ε_m and ε_l are unobserved...' and 'the GCM of Fig. 2(c) with extra latent variables H_m'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_7_221", "query": "In Figure 3(a), the black stars (A=0) peak at 0 while the red crosses (A=1) peak at -2. How does this shift relate to the MMD criterion mentioned in the text?", "answer": "The peak shift demonstrates dependence of $ p(H_m|A) $ on $ A $, which the MMD criterion aims to minimize. The text states MMD reduces $ A $'s influence on $ H_m $, so this figure illustrates the scenario before applying MMD.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_7", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg", "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .", "figure_type": "plot", "evidence_from_figure": "Peak positions of A=0 (0) and A=1 (-2)", "evidence_from_text": "MMD criterion to reduce dependence on A", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_7_222", "query": "Why does Figure 3(a) show different distributions for A=0 and A=1, and how does this connect to the GCM modification with latent variable $ H_m $?", "answer": "The distribution differences reflect $ H_m $'s dependence on $ A $, as per the GCM modification introducing $ H_m $ for $ M $ generation. The text explains this latent variable captures $ A $'s influence, which MMD later mitigates.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_7", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig6.jpg", "caption": "Figure 3. (a): Empirical distribution of $\\epsilon _ { m } ^ { n }$ for the case in which $m ^ { n }$ is generated by Eq. (3) with an extra non-linear term $f ( A , C )$ (continuous lines). Histograms of $\\tilde { p } ( H _ { m } | A )$ (crossed lines), see (b). (b): Modification of the GCM corresponding to Eq. (3) to include an explicit latent variable $H _ { m }$ for the generation of $M$ .", "figure_type": "plot", "evidence_from_figure": "Distinct histograms for A=0 and A=1", "evidence_from_text": "GCM modification with explicit $ H_m $ for $ M $ generation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_9_223", "query": "In Figure 4, which variables directly influence M according to the diagram, and how does the text describe the modeling of p_θ(M|A,C,H_m)?", "answer": "The diagram shows solid arrows from A, C, and H_m to M. The text states p_θ(M|A,C,H_m) is modeled as a function (e.g., neural network) for categorical M or a Gaussian distribution for continuous M.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_9", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg", "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.", "figure_type": "diagram", "evidence_from_figure": "Arrows from A, C, H_m to M", "evidence_from_text": "p_θ(M|A,C,H_m) = f_θ(A,C,H_m) for categorical M", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_9_224", "query": "How does the MMD penalization approach in the text relate to the latent variables H_m, H_l, H_r in Figure 4?", "answer": "The MMD penalization (text) ensures q(H|A) does not depend on A, which applies to latent variables H_m, H_l, H_r (figure) to enforce independence between latent factors and input A.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_9", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig8.jpg", "caption": "Figure 4. (a): GCM for the UCI Adult dataset. (b): GCM for the UCI German Credit dataset.", "figure_type": "diagram", "evidence_from_figure": "Nodes H_m, H_l, H_r", "evidence_from_text": "MMD penalization approach to ensure q(H|A) does not depend on A", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_17_225", "query": "Why is the causal effect along the green path A→Y not identifiable using only observed variables in Figure 7(a)?", "answer": "The path is blocked by the unobserved confounder C (gray node), which is not observed, making the causal effect non-identifiable with observed variables alone.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_17", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg", "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.", "figure_type": "diagram", "evidence_from_figure": "Green arrow from A to Y and gray node C", "evidence_from_text": "The causal effect along the green path A→Y cannot be identified by only using observed variables", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_17_226", "query": "How does the ADMG in Figure 7(b) represent the unobserved confounder C from Figure 7(a)?", "answer": "The ADMG replaces the unobserved confounder C with red bidirected links to indicate the presence of an unobserved common cause, as defined in the text.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_17", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig16.jpg", "caption": "Figure 7. (a): GCM with an unobserved confounder $C$ indicated with a gray node. (b): ADMG corresponding to (a). The causal effect along the green path $A  Y$ cannot be identified by only using observed variables.", "figure_type": "diagram", "evidence_from_figure": "Figure 7(b) as the ADMG corresponding to Figure 7(a)", "evidence_from_text": "ADMG contains red bidirected links indicating unobserved common causes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_36_227", "query": "What is the number of mixture components in the prior distribution for $H_m$ as stated in the text related to Figure 8(a)?", "answer": "10", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_36", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg", "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.", "figure_type": "plot", "evidence_from_figure": "Figure 8(a) corresponds to $H_m$", "evidence_from_text": "Text states 'mixture of two-dimensional Gaussians with ten mixture components'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_36_228", "query": "Why does the histogram in Figure 8(a) exhibit a single peak despite the prior being a mixture of Gaussians?", "answer": "The variational posterior is a single two-dimensional Gaussian (per text), resulting in a unimodal marginal distribution", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_36", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg", "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.", "figure_type": "plot", "evidence_from_figure": "Unimodal histogram shape", "evidence_from_text": "Text states 'variational posterior distribution q we used a two-dimensional Gaussian'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_36_229", "query": "What is the dimensionality of the latent variable $H_m$ for which the histogram in Figure 8(a) was generated?", "answer": "Two-dimensional", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_36", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig35.jpg", "caption": "Figure 8. (a): Histograms of $\\tilde { q } ( H _ { m } | A )$ (two-dimensional), $\\tilde { q } ( H _ { l } | A )$ (two-dimensional), and $\\tilde { q } ( H _ { r } | A )$ (six-dimensional) after 5,000 training steps. (b): Prior distributions $p ( H _ { m } )$ , $p ( H _ { l } )$ , and $p ( H _ { r } )$ corresponding to mixtures of ten two-dimensional Gaussians.", "figure_type": "plot", "evidence_from_figure": "Figure 8(a) corresponds to $H_m$", "evidence_from_text": "Text states '$\tilde{q}(H_m | A)$ (two-dimensional)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_46_230", "query": "In Figure 9, what do the red histograms in the first row represent regarding training steps, and how does this relate to the figure caption?", "answer": "The red histograms in the first row correspond to 2,000 training steps, as explicitly stated in the figure caption.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_46", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg", "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).", "figure_type": "plot", "evidence_from_figure": "Red histograms in the first row", "evidence_from_text": "Caption states '2,000 (first row)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1802.08139_1802.08139_fig_46_231", "query": "How many histogram pairs are visible for the 'savings' category in Figure 9, and why does the text specify it has two dimensions?", "answer": "Two histogram pairs are visible for savings, consistent with the text's specification of 'two dimensions' for this category.", "doc_id": "1802.08139", "figure_id": "1802.08139_fig_46", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1802.08139/1802.08139/hybrid_auto/images/1802.08139_page0_fig45.jpg", "caption": "Figure 9. Histograms of $\\tilde { q } ( H _ { s } | A )$ after 2,000 (first row) and 8,000 (second row) training steps. From left to right: status of checking account (two dimensions), savings (two dimensions), and housing (one dimension).", "figure_type": "plot", "evidence_from_figure": "Two distinct histogram pairs for savings", "evidence_from_text": "Caption states 'savings (two dimensions)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_4_232", "query": "In Figure 2, why does the EqOpt utility curve peak at β_EqOpt, and how does this relate to Assumption 1's condition on institution utilities?", "answer": "The peak at β_EqOpt represents the selection rate where EqOpt maximizes utility under the fairness constraint. Assumption 1 states that institution utility is more stringent than expected score changes (u(x) > 0 ⇒ Δ(x) > 0), which explains why the utility curve peaks at this optimal selection rate rather than at the maximum possible value.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg", "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.", "figure_type": "plot", "evidence_from_figure": "Peak position of EO curve at β_EqOpt", "evidence_from_text": "Assumption 1's utility constraint condition", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_4_233", "query": "How does the positioning of β_DemParity relative to β_EqOpt in Figure 2 inform the comparison between DemParity and EqOpt discussed in Section 3.2?", "answer": "β_DemParity lies to the right of β_EqOpt on the selection rate axis, indicating DemParity requires a higher selection rate to maximize utility compared to EqOpt. This visual difference aligns with Section 3.2's assertion that comparing these fairness constraints depends on full distributions, as their optimal selection rates differ.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_4", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig3.jpg", "caption": "Figure 2: Both outcomes $\\Delta \\pmb { \\mu }$ and institution utilities $\\boldsymbol { u }$ can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.", "figure_type": "plot", "evidence_from_figure": "β_DemParity's position relative to β_EqOpt", "evidence_from_text": "Section 3.2's discussion on distribution dependence", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_6_234", "query": "Why does the EO constraint curve in Figure 3 differ in shape from the DP constraint line?", "answer": "The EO constraint is a curve because it is defined by the graph of G^(A→B), while the DP constraint is a straight line with slope 1 as stated in the figure caption.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg", "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).", "figure_type": "plot", "evidence_from_figure": "EO curve is visibly curved, DP line is straight", "evidence_from_text": "Caption states DP has slope 1 and EO is graph of G^(A→B)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_6_235", "query": "What does the 'MU' label in Figure 3 correspond to according to the text?", "answer": "MU represents the maximum utility (MaxUtil) optimal solution, as indicated by the text's reference to 'MaxUtil in is represented by a horizontal line through the MU optimal solution'.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig5.jpg", "caption": "Figure 3: Considering the utility as a function of selection rates, fairness constraints correspond to restricting the optimization to one-dimensional curves. The DemParity (DP) constraint is a straight line with slope 1, while the EqOpt (EO) constraint is a curve given by the graph of $G ^ { ( \\mathsf { A } \\to \\mathsf { B } ) }$ . The derivatives considered throughout Section 6 are taken with respect to the selection rate $\\beta _ { \\mathsf { A } }$ (horizontal axis); projecting the EO and DP constraint curves to the horizontal axis recovers concave utility curves such as those shown in the lower panel of Figure 2 (where MaxUtil in is represented by a horizontal line through the MU optimal solution).", "figure_type": "plot", "evidence_from_figure": "MU point is marked on the EO curve", "evidence_from_text": "Text describes MU as the MaxUtil optimal solution", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_12_236", "query": "In Figure 5(a), which criterion's threshold leads to active harm when bank utilities are set to u_-/u_+ = -4?", "answer": "DemParity (DP)", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_12", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg", "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.", "figure_type": "plot", "evidence_from_figure": "Teal dashed line labeled 'DP' intersects the CDFs at a score where active harm (brown dotted line) occurs", "evidence_from_text": "Text states 'in (a) DemParity causes active harm while in (b) it does not'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_12_237", "query": "What is the profit/loss ratio for the scenario where DemParity causes active harm?", "answer": "1/10", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_12", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig11.jpg", "caption": "Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) $\\frac { u _ { - } } { u _ { + } } = - 4$ and (b) $\\frac { u _ { - } } { u _ { + } } = - 1 0$ . The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.", "figure_type": "plot", "evidence_from_figure": "Figure header shows 'Profit/Loss Ratio: 1/10'", "evidence_from_text": "Text specifies the scenario corresponds to (a) u_-/u_+ = -4", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_16_238", "query": "In Figure 6, how do the utility maxima positions for the DP and EO curves determine the decision rule thresholds?", "answer": "The relative positions of the utility maxima (peaks) of the DP and EO curves determine the decision rule thresholds, as stated in the figure caption. The DP curve peaks at a lower selection rate than the EO curve, indicating different optimal thresholds for each fairness criterion.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_16", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg", "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.", "figure_type": "plot", "evidence_from_figure": "Positions of peaks in DP and EO curves", "evidence_from_text": "Caption stating 'relative positions of utility maxima determine decision rule thresholds'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1803.04383_1803.04383_fig_16_239", "query": "How does the fixed ratio u_-/u_+ = -4 (mentioned in the figure caption) affect the shape of the utility curves in Figure 6?", "answer": "The fixed ratio u_-/u_+ = -4 constrains the utility function's parameters, causing the DP and EO curves to exhibit distinct shapes. The DP curve's earlier peak suggests a higher utility at lower selection rates compared to the EO curve, reflecting the impact of this ratio on fairness trade-offs.", "doc_id": "1803.04383", "figure_id": "1803.04383_fig_16", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1803.04383/1803.04383/hybrid_auto/images/1803.04383_page0_fig15.jpg", "caption": "Figure 6: The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold $\\frac { u _ { - } } { u _ { + } } = - 4$ as fixed.", "figure_type": "plot", "evidence_from_figure": "Shape and peak positions of DP and EO curves", "evidence_from_text": "Caption stating 'u_-/u_+ = -4 as fixed' and Lemma C.3 context on group-specific utility calculations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.06876_1804.06876_fig_3_240", "query": "In Figure 1, why is the solid purple line used for 'the secretary' in the first sentence while the dashed purple line connects 'him' in the second sentence?", "answer": "The solid purple line represents pro-stereotypical scenarios (e.g., female secretaries), while the dashed purple line represents anti-stereotypical scenarios (e.g., male secretaries). The text explains that systems must handle both equally to pass the bias test.", "doc_id": "1804.06876", "figure_id": "1804.06876_fig_3", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg", "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.", "figure_type": "example", "evidence_from_figure": "solid/dashed purple line connections", "evidence_from_text": "section 2 describes pro-stereotypical (solid) vs. anti-stereotypical (dashed) scenarios", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.06876_1804.06876_fig_3_241", "query": "How does Figure 1 demonstrate the 'gender of the pronominal reference being irrelevant for co-reference decision' as stated in the caption?", "answer": "The figure shows identical co-reference links (physician → secretary) with different pronouns ('the secretary' vs. 'him'), where the gender of the pronoun (female vs. male) does not affect the coreference relationship. The text clarifies that systems must correctly resolve both scenarios to avoid bias.", "doc_id": "1804.06876", "figure_id": "1804.06876_fig_3", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.06876/1804.06876/hybrid_auto/images/1804.06876_page0_fig2.jpg", "caption": "Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.", "figure_type": "example", "evidence_from_figure": "same co-reference relationship with gendered pronouns", "evidence_from_text": "caption states gender of pronoun is irrelevant for co-reference decisions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.09301_1804.09301_fig_1_242", "query": "In Figure 1, why does the coreference resolution for 'his' and 'their' succeed but fail for 'her'?", "answer": "The system resolves male ('his') and neutral ('their') pronouns as coreferent with 'The surgeon' but fails for female ('her'), indicating gender bias in the rule-based coreference system.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg", "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.", "figure_type": "example", "evidence_from_figure": "Coref links for 'his' and 'their' exist but not for 'her'", "evidence_from_text": "Text explains gender bias in coreference systems and Winogender schemas", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.09301_1804.09301_fig_1_243", "query": "What pronouns in Figure 1 correspond to the 'male', 'neutral', and 'female' categories mentioned in the text?", "answer": "The pronouns 'his' (male), 'their' (neutral), and 'her' (female) form the minimal pair sentences demonstrating gender bias.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig0.jpg", "caption": "Figure 1: Stanford CoreNLP rule-based coreference system resolves a male and neutral pronoun as coreferent with “The surgeon,” but does not for the corresponding female pronoun.", "figure_type": "example", "evidence_from_figure": "Three sentences with 'his', 'their', 'her' labeled as 'Mention'", "evidence_from_text": "Text defines Winogender schemas as minimal pairs differing by pronoun gender", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.09301_1804.09301_fig_3_244", "query": "Why do most data points in Figure 3 lie below the 45-degree line, and what does this imply about the gender statistics from Bergsma and Lin (2006)?", "answer": "Most points lie below the 45-degree line because Bergsma and Lin (2006) systematically underestimates female percentages compared to BLS data, indicating a male-skewed bias in their text-based gender statistics.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg", "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .", "figure_type": "plot", "evidence_from_figure": "Data points below the 45-degree line", "evidence_from_text": "Caption states 'the former has systematically lower female percentages; most points lie well below the 45-degree line'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.09301_1804.09301_fig_3_245", "query": "What does the Pearson Γ = 0.67 value in Figure 3 indicate about the relationship between Bergsma and Lin (2006) and BLS 2015 data?", "answer": "The Pearson Γ = 0.67 indicates a moderate positive correlation between the datasets, but the systematic deviation below the 45-degree line shows that while correlated, Bergsma and Lin's data exhibits a male-skewed bias relative to actual employment statistics.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig2.jpg", "caption": "Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the 45-degree line (dotted). Regression line and $9 5 \\%$ confidence interval in blue. Pearson $\\Gamma = 0 . 6 7$ .", "figure_type": "plot", "evidence_from_figure": "Pearson Γ = 0.67 label", "evidence_from_text": "Caption mentions correlation but systematic skew; text explains bias in NLP pipelines", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.09301_1804.09301_fig_4_246", "query": "What does a y-value of 50 indicate in Figure 4, based on the figure caption?", "answer": "A y-value of 50 indicates the system prefers female pronouns for the occupation 50 percentage points more than male pronouns (e.g., 75% female vs 25% male resolution), as defined in the caption.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg", "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.", "figure_type": "plot", "evidence_from_figure": "y-axis scale and value of 50", "evidence_from_text": "caption definition of y-axis values (\"a value of 100 (maximum female bias)... a score of -100 (maximum male bias)... value of 0 indicates no gender differential\")", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1804.09301_1804.09301_fig_4_247", "query": "Why does the NEURAL system's regression line lie above the y=0 line in Figure 4?", "answer": "The y=0 line represents no gender bias; the NEURAL system's regression line above y=0 indicates it exhibits female bias across occupations, as the y-axis measures preference for female pronouns.", "doc_id": "1804.09301", "figure_id": "1804.09301_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1804.09301/1804.09301/hybrid_auto/images/1804.09301_page0_fig3.jpg", "caption": "Figure 4: These two plots show how gender bias in coreference systems corresponds with occupational gender statistics from the U.S Bureau of Labor Statistics (left) and from text as computed by Bergsma and Lin (2006) (right); each point represents one occupation. The y-axes measure the extent to which a coref system prefers to match female pronouns with a given occupation over male pronouns, as tested by our Winogender schemas. A value of 100 (maximum female bias) means the system always resolved female pronouns to the given occupation and never male pronouns $( 1 0 0 \\% - 0 \\% )$ ; a score of -100 (maximum male bias) is the reverse; and a value of 0 indicates no gender differential. Recall the Winogender evaluation set is gender-balanced for each occupation; thus the horizontal dotted black line $\\scriptstyle ( \\mathrm { y = 0 } )$ in both plots represents a hypothetical system with $100 \\%$ accuracy. Regression lines with $9 5 \\%$ confidence intervals are shown.", "figure_type": "plot", "evidence_from_figure": "NEURAL regression line position relative to y=0", "evidence_from_text": "caption explanation that y=0 is neutral and positive values indicate female bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_2_248", "query": "Why do the error bars for answer position 19 in Figure 1(a) appear longer than those for position 10?", "answer": "Because there are fewer data samples in the bin for answer position 19, leading to increased noise as noted in the text.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg", "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "error bar lengths at positions 19 and 10", "evidence_from_text": "Note that the outcome becomes noisy when there are few samples", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_2_249", "query": "According to the text, what is the implication of having few data samples in a bin for the reliability of the data in Figure 1(a)?", "answer": "The outcome becomes noisy, reducing reliability when there are few samples.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig1.jpg", "caption": "Figure 1: Disaggregation of Stack Exchange data. (a) The heat map shows the probability the answer is accepted as a function of its answer position within a session, with the horizontal bands corresponding to the different subgroups, conditioned on total number of answers the user has written. (b) Number of data samples within each bin of the heat map. Note that the outcome becomes noisy when there are few samples. The trends in performance as a function of answer position in (c) disaggregated data and (d) aggregate data. Error bars in (c) and (d) show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "error bars in bins with low sample counts (e.g., position 19)", "evidence_from_text": "Note that the outcome becomes noisy when there are few samples", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_14_250", "query": "Why does the first attempt subgroup (red crosses) plateau at ~0.8 in Figure 4c?", "answer": "The plateau at ~0.8 occurs because, as explained in the text, lessons exceeding 20 new words prevent perfect performance, making 0.8 the maximum achievable performance level.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_14", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg", "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "evidence_from_figure": "red crosses plateauing at ~0.8", "evidence_from_text": "text states 'After 20 new words are shown in a lesson, users can no longer answer all the words correctly'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_14_251", "query": "What does the downward trend in the fifth attempt subgroup (blue diamonds) from month 7 to 12 indicate about lesson difficulty?", "answer": "The downward trend reflects increasing lesson difficulty beyond 20 new words, where users cannot maintain high performance as more distinct words are introduced, per the text's explanation of performance limitations.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_14", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig13.jpg", "caption": "Figure 4: Disaggregation of Khan Academy data showing performance as a function of month, conditioned on five first attempts. (a) The heat map shows average performance as a function of the month. (b) Number of data samples within each subgroup. The trends in (c) the disaggregated data and in (d) aggregated data.", "figure_type": "plot", "evidence_from_figure": "blue diamonds declining after month 7", "evidence_from_text": "text explains 'as lessons become more difficult—more distinct words are introduced—it becomes more difficult for users to have perfect performance'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_21_252", "query": "What does the diagonal red region in Figure 5a represent according to the text?", "answer": "The diagonal red region represents 'perfect lessons' where users answered all words they were shown correctly, as stated in the text.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_21", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg", "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "diagonal red region in heat map", "evidence_from_text": "text stating 'along the diagonal show perfect lessons, where users answered all words they were shown correctly'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_21_253", "query": "Why does performance decrease after 20 distinct words per lesson in Figure 5a?", "answer": "After 20 distinct words are introduced, users can no longer answer all words correctly due to increased lesson difficulty, as explained in the text.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_21", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg", "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "performance drop beyond y-axis value ~10^1.3 (≈20 distinct words)", "evidence_from_text": "text stating 'After 20 new words are shown in a lesson, users can no longer answer all the words correctly'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_21_254", "query": "What performance value corresponds to 'perfect lessons' in Figure 5a?", "answer": "A performance value of 0.7 corresponds to perfect lessons, as the highest value on the color bar aligns with the diagonal red region described in the text.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_21", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig20.jpg", "caption": "Figure 5: Disaggregation of Duolingo data. (a) The heat map shows performance, as a function of how many lessons the user completed, conditioned on how many of the five first lessons were answered correctly. (b) Number of data samples within each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "color bar showing 0.7 as maximum performance", "evidence_from_text": "text defining perfect lessons as cases where all words were answered correctly", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_22_255", "query": "What does the 'Count' in Figure 6 (b) represent, and how is it related to the 'performance' metric described in the text?", "answer": "The 'Count' in Figure 6 (b) represents the number of data samples in each bin. It contextualizes the performance metric in part (a), where performance is defined as the probability of answering all words correctly. Higher counts in specific bins indicate more reliable estimates of performance for those lesson-word combinations.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_22", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg", "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "Color bar labeled 'Count' and axes 'Lesson Correct'/'Distinct Words'", "evidence_from_text": "Definition of performance as 'probability to answer all the words correctly' in caption", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03094_1805.03094_fig_22_256", "query": "How does the heat map's data distribution in Figure 6 (b) relate to the discussion of initial performance in the text?", "answer": "The heat map shows higher data counts in regions with lower 'Lesson Correct' and 'Distinct Words' values, corresponding to early lessons. This aligns with the text's observation that initial performance (first five lessons) differentiates subgroups, as these regions likely represent users' early learning stages.", "doc_id": "1805.03094", "figure_id": "1805.03094_fig_22", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03094/1805.03094/hybrid_auto/images/1805.03094_page0_fig21.jpg", "caption": "Figure 6: Disaggregation of Duolingo data showing performance as a function of lesson Correct (a) The heat map shows performance, i.e., probability to answer all the words correctly, conditioned on the number of distinct words in the lesson. (b) Number of data samples in each bin of the heat map. Trends in (c) the disaggregated data and in (d) aggregate data. Errors bars show $9 5 \\%$ confidence interval.", "figure_type": "plot", "evidence_from_figure": "Concentration of darker green (higher count) in lower-left region of heat map", "evidence_from_text": "Discussion of 'initial performance' as a conditioning variable for subgroup differentiation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_1_257", "query": "In Figure 1, where does the 'Dataset Nutrition Label' generation occur in the pipeline, and what is its purpose according to the text?", "answer": "The 'Dataset Nutrition Label' generation occurs during the 'Dataset Preprocessing' stage (indicated by the red arrow). According to the text, its purpose is to provide a standardized diagnostic framework for analyzing dataset quality before model development to avoid problematic AI outcomes from biased or incomplete data.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg", "caption": "Figure 1.​ Model Development Pipeline", "figure_type": "diagram", "evidence_from_figure": "Red arrow labeled 'Interrogating data quality & generating \"nutrition label\"' pointing to 'Dataset Preprocessing'", "evidence_from_text": "Text states: 'The Dataset Nutrition Label... lowers the barrier to standardized data analysis by providing a distilled yet comprehensive overview of dataset \"ingredients\" before AI model development.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_1_258", "query": "How does the 'Data Pipeline' feedback loop in Figure 1 address the problem mentioned in the text about AI systems being scrutinized only after deployment?", "answer": "The feedback loop from 'Deployment' back to 'Dataset' enables iterative refinement of the dataset based on post-deployment insights. This aligns with the text's observation that models often face scrutiny only after deployment, allowing the pipeline to correct data issues identified during real-world usage.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg", "caption": "Figure 1.​ Model Development Pipeline", "figure_type": "diagram", "evidence_from_figure": "Curved arrow labeled 'Data Pipeline' connecting 'Deployment' to 'Dataset'", "evidence_from_text": "Text states: 'Models often come under scrutiny only after they are built, trained, and deployed.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_1_259", "query": "What is the relationship between the 'Dataset Nutrition Label' and the 'Data Pipeline' as shown in Figure 1 and described in the text?", "answer": "The 'Dataset Nutrition Label' is a diagnostic component integrated into the 'Data Pipeline' during the 'Dataset Preprocessing' stage. The text explains it provides standardized data analysis before model development, making it a critical step within the pipeline to ensure data quality prior to model training.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig0.jpg", "caption": "Figure 1.​ Model Development Pipeline", "figure_type": "diagram", "evidence_from_figure": "Red arrow linking 'Interrogating data quality & generating \"nutrition label\"' to 'Dataset Preprocessing'", "evidence_from_text": "Text states: 'The Dataset Nutrition Label... provides a distilled yet comprehensive overview of dataset \"ingredients\" before AI model development.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_2_260", "query": "In Figure 2A, what percentage of respondents selected 'Yes' for having best practices, and how does this relate to the survey's conclusion about data analysis practices?", "answer": "26.5% (9 respondents), which indicates less than a third of organizations have established best practices, supporting the survey's conclusion that inadequate data analysis practices are prevalent.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg", "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data", "figure_type": "plot", "evidence_from_figure": "26.5% (9) for 'Yes' category", "evidence_from_text": "Text states survey results 'lend credence to this problem' (lack of best practices)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_2_261", "query": "How many respondents answered 'In Development' in Figure 2A, and what does the text imply about this category's relevance to algorithmic accountability?", "answer": "3 respondents (8.8%), which suggests limited progress in developing best practices despite the text's focus on algorithmic accountability as a key field for addressing AI fairness.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig1.jpg", "caption": "Figure 2.​ (A) Survey results about data analysis best practices in respondents’ organizations and (B) Survey results about how respondents learned to analyze data", "figure_type": "plot", "evidence_from_figure": "8.8% (3) for 'In Development'", "evidence_from_text": "Text cites 'algorithmic accountability' as a salient field for the study", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_4_262", "query": "Which component in Figure 3 handles server-side processing according to the text?", "answer": "The data host is responsible for server-side processing, as the text states that sophisticated analyses requiring additional computational power are reserved for server-side processing, which aligns with the data host's role in the ecosystem.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg", "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.", "figure_type": "architecture", "evidence_from_figure": "data host component connected to label generator via blue arrow", "evidence_from_text": "text states server-side processing is reserved for 'more specialized and sophisticated analyses requiring additional computational power'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_4_263", "query": "How does the 'link to label' element function in the label retrieval process?", "answer": "The 'link to label' enables label retrieval from the repository to the viewer, as shown by the red arrow in Figure 3, and supports the text's description of the label viewer as a core component for accessing generated labels.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig3.jpg", "caption": "Figure 3.​ Architecture of the proposed Data Nutrition Label ecosystem.", "figure_type": "architecture", "evidence_from_figure": "red arrow labeled 'link to label' connecting repository to viewer", "evidence_from_text": "text states the ecosystem includes a 'label viewer' as one of its main components", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_5_264", "query": "What is the purpose of the interactive dropdown menus in Figure 4, as described in the text?", "answer": "The dropdown menus allow users to select variable pairs for comparison, enabling browser-based statistical analysis of datasets under 100K rows as stated in the text.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_5", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg", "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.", "figure_type": "plot", "evidence_from_figure": "Dropdowns labeled 'Variable A' and 'Variable B' for selecting product_name and recipient_state", "evidence_from_text": "Text states: 'the viewer is able to choose the variable pair being compared to one another'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_5_265", "query": "According to the text, what computational limitation is addressed by the browser-based analysis in Figure 4?", "answer": "Browser-based analysis handles datasets <100K rows for simple statistical operations (e.g., histograms, correlations), while server-side processing is reserved for more complex analyses.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_5", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig4.jpg", "caption": "Figure 4.​ Prototype Label demonstrating the Pair Plot module and highlighting the interactive dropdown menus for selecting variables.", "figure_type": "plot", "evidence_from_figure": "Pair Plot module with interactive elements for variable selection", "evidence_from_text": "Text states: 'Simple statistical analyses... carried out directly in the browser, given tabular datasets of <100K rows'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_6_266", "query": "Why does California (CA) have the highest bar in Figure 5, and how does the Probabilistic Model module generate this distribution according to the text?", "answer": "CA has the highest bar because the Probabilistic Model module generates synthetic data using BayesDB, as stated in the text. The figure exemplifies this by showing a hypothetical distribution where CA's probability exceeds other states, reflecting the model's output.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_6", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg", "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.", "figure_type": "plot", "evidence_from_figure": "CA's bar height is the tallest in the chart", "evidence_from_text": "Text states the module 'generates synthetic data by utilizing the aforementioned BayesDB backend'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_6_267", "query": "What is the purpose of the Probabilistic Model module as described in the text, and how does Figure 5 demonstrate this purpose?", "answer": "The module's purpose is to generate synthetic data (as per text), and Figure 5 demonstrates this by visualizing a hypothetical payment distribution for Eliquis across states, showing how the model produces probabilistic outputs for analysis.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_6", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig5.jpg", "caption": "Figure 5.​ Prototype Label demonstrating the Probabilistic Model module and showcasing a hypothetical distribution for payments made towards the drug \"Eliquis\" across different states.", "figure_type": "plot", "evidence_from_figure": "Bar chart showing state-specific probabilities for Eliquis payments", "evidence_from_text": "Text states the module 'attempts to generate synthetic data by utilizing the aforementioned BayesDB backend'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_7_268", "query": "Which demographic group shows the strongest negative correlation with total_amount_of_payment_usdollars in Figure 6, and how does the text explain the significance of such correlations for model development?", "answer": "The 'Rural fraction' group shows the strongest negative correlation (darkest bar in top chart). The text explains this demonstrates the module's ability to prompt critical questions during preprocessing, helping identify relationships that warrant further analysis without sacrificing thoroughness.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg", "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.", "figure_type": "plot", "evidence_from_figure": "darkest bar in top chart under 'total_amount_of_payment_usdollars'", "evidence_from_text": "text states module 'prompts critical questions and interrogation in the preprocessing phase' and 'expedites decision making'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.03677_1805.03677_fig_7_269", "query": "How do the two variables (total_amount_of_payment_usdollars and spend_per_person) in Figure 6 align with the text's claim that the Ground Truth Correlations module 'expedites decision making'?", "answer": "The variables demonstrate the module's ability to provide initial evidence of demographic relationships (e.g., 'Rural fraction' negatively correlates with payment amounts, while 'White alone' positively correlates with spending). The text explains this enables data specialists to focus on high-impact correlations, saving time without compromising quality.", "doc_id": "1805.03677", "figure_id": "1805.03677_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.03677/1805.03677/hybrid_auto/images/1805.03677_page0_fig6.jpg", "caption": "Figure 6.​ The negative (top) and positive (bottom) correlations to demographics produced by the Ground Truth Correlations module.", "figure_type": "plot", "evidence_from_figure": "two distinct variable plots showing correlation directions", "evidence_from_text": "text states module 'expedites decision making, which saves time... without sacrificing quality or thoroughness'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.05859_1805.05859_fig_1_270", "query": "What does the arrow from A to Z in Figure 1(a) imply about the causal relationship between A and Z according to the intervention definition provided in the text?", "answer": "The arrow implies A is a direct cause of Z. According to the text, an intervention on A (e.g., setting A = v) would override its structural equation, affecting Z's distribution via the causal path A→Z.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .", "figure_type": "diagram", "evidence_from_figure": "Arrow from A to Z", "evidence_from_text": "Definition of intervention: 'overriding f_i(·) with V_i = v'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.05859_1805.05859_fig_1_271", "query": "How does the presence of U_Z as an exogenous variable in Figure 1(a) relate to the concept of untestable assumptions discussed in the text after the figure?", "answer": "U_Z is an unobserved background variable influencing Z. The text states structural equations depending on observed variables only cannot be tested without untestable assumptions, highlighting U_Z's role in this limitation.", "doc_id": "1805.05859", "figure_id": "1805.05859_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.05859/1805.05859/hybrid_auto/images/1805.05859_page0_fig0.jpg", "caption": "Figure 1: (a) A causal graph for three observed variables $A , Y , Z$ . (b) A joint representation with explicit background variables, and two counterfactual alternatives where $A$ is intervened at two different levels. (c) Similar to (b), where the interventions take place on $Y$ .", "figure_type": "diagram", "evidence_from_figure": "U_Z node with arrow to Z", "evidence_from_text": "Section 3.3: 'Unless structural equations depend on observed variables only, they cannot be tested...'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.09458_1805.09458_fig_1_272", "query": "Why does the VFAE method have a higher adversarial loss than the Proposed method when n layers = 0?", "answer": "The VFAE method has a higher adversarial loss at n layers = 0 because it uses a modified VAE loss without a predictor branch (p(y|z)), while the Proposed method incorporates a predictor branch in its modified VIB framework, which improves adversarial robustness.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg", "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.", "figure_type": "plot", "evidence_from_figure": "VFAE's red bar (n=0) is taller than Proposed's red bar", "evidence_from_text": "Text states VFAE uses modified VAE loss without predictor branch, while Proposed uses modified VIB with predictor branch", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.09458_1805.09458_fig_1_273", "query": "How does the adversarial loss for the Xie method change as n layers increases from 0 to 3?", "answer": "The adversarial loss for the Xie method increases as n layers increases from 0 to 3, indicating reduced adversarial robustness with deeper architectures, possibly due to limitations in how Xie's method leverages additional layers.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig0.jpg", "caption": "Figure 1: On the left we display the adversarial loss (the accuracy of the adversary on $c$ ) and predictive accurracy on $y$ for three methods, plus the majority-class baseline, on both Adult and German datasets. For adv. loss lower is better, while for pred. acc. higher is better. On the right we plot adversarial loss by varying adversarial strength (indicated by color), parameterized by the number of layers from zero (logistic regression) to three. All evaluations are performed on the hold-out test sets.", "figure_type": "plot", "evidence_from_figure": "Xie's bars increase in height from n=0 to n=3", "evidence_from_text": "Text describes Xie as a method with specific architectural constraints that may not scale well with increasing layers", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.09458_1805.09458_fig_2_274", "query": "In Figure 2's VFAE plot, what does the red color indicate about the data points?", "answer": "The red color represents the majority class as specified in the caption.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg", "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.", "figure_type": "plot", "evidence_from_figure": "Red data points visible in the plot", "evidence_from_text": "Caption states 'where red is the majority class'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.09458_1805.09458_fig_2_275", "query": "Why was the Adult dataset used for Figure 2 while the subsequent section discusses MNIST?", "answer": "Figure 2 evaluates latent space structure on the Adult dataset, whereas the subsequent section applies the method to MNIST for unsupervised image manipulation.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig1.jpg", "caption": "Figure 2: t-SNE plots for the latent encodings of (Left to Right) the VFAE, Xie et al., and our proposed method on the Adult dataset (first 1000 pts., test split). The value of the $c$ variable is provided as color, where red is the majority class.", "figure_type": "plot", "evidence_from_figure": "Caption specifies 'Adult dataset'", "evidence_from_text": "Text after figure states 'on the MNIST dataset' for unsupervised manipulation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.09458_1805.09458_fig_5_276", "query": "How does Figure 3 demonstrate stylistic invariance using the MNIST dataset?", "answer": "The figure shows that the 'Generated' column preserves the handwriting style of the 'Real' column while changing digit labels (e.g., the top row's '9' in 'Real' maps to '0' in 'Generated'). This illustrates how the latent space z captures style information independent of digit labels, as explained in the text where z is invariant to the digit class c.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg", "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.", "figure_type": "example", "evidence_from_figure": "Visual similarity between 'Real' and 'Generated' columns despite different digit labels", "evidence_from_text": "Text describing z as invariant to digit label c and the method of generating c' using z", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.09458_1805.09458_fig_5_277", "query": "Why does the text claim adversarial training is unnecessary for this invariant representation task?", "answer": "The text states that the generated digits in Figure 3 successfully replicate the style of real digits without adversarial training. The figure's results show that the derived method achieves stylistic similarity (e.g., consistent stroke patterns across columns), making adversarial training redundant despite its theoretical equivalence mentioned in Section 2.1.2.", "doc_id": "1805.09458", "figure_id": "1805.09458_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.09458/1805.09458/hybrid_auto/images/1805.09458_page0_fig4.jpg", "caption": "Figure 3: We demonstrate the ability to generate stylistically similar images of varying classes using the MNIST dataset. The left column is mapped into $z$ that is invariant to its digit label c. We then can generate an image using $z$ and any other specified digit, $c ^ { \\prime }$ , as show on the right.", "figure_type": "example", "evidence_from_figure": "Generated digits matching real digit styles in the right column", "evidence_from_text": "Section discussing adversarial training's complexity and the conclusion that it's unnecessary for this specific task", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_1_278", "query": "How does the Discriminator in Figure 1 contribute to reducing the risk difference (disc(D)) in synthetic datasets as shown in Table 1?", "answer": "The Discriminator evaluates whether input data is real (x) or generated (G(z)), forcing the Generator to produce synthetic data that mimics the statistical properties of the real dataset. This adversarial training reduces the risk difference (disc(D)), as seen in Table 1 where SYN2-NFGANI achieves disc(D) = 0.0025±0.0007.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg", "caption": "Figure 1: Illustration of generative adversarial networks", "figure_type": "diagram", "evidence_from_figure": "Discriminator's role in distinguishing real vs. generated data", "evidence_from_text": "Definition 1 (risk difference disc(D)) and text discussing fairness metrics", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_1_279", "query": "According to Table 1, SYN4-FairGAN has a disc(D) of 0.0062±0.0037. How does the GAN architecture in Figure 1 enable this fairness metric?", "answer": "The FairGAN architecture (Figure 1) uses adversarial training where the Discriminator is optimized to enforce statistical parity, guiding the Generator to produce synthetic data with reduced risk difference. This results in disc(D) values close to zero, as observed in SYN4-FairGAN's 0.0062±0.0037.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig0.jpg", "caption": "Figure 1: Illustration of generative adversarial networks", "figure_type": "diagram", "evidence_from_figure": "GAN structure with Generator-Discriminator interaction", "evidence_from_text": "Text describing FairGAN's role in classification fairness and risk difference metrics", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_2_280", "query": "What is the specific role of D2 in FairGAN (Figure 2) and how does its input structure relate to the training objective described in the text?", "answer": "D2 discriminates between generated data conditional on ŝ=0 and ŝ=1, ensuring the generator respects protected attribute fairness. The text explains that G is trained to fool discriminators, so D2 enforces that the synthetic data maintains appropriate conditional distributions for protected attributes.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg", "caption": "Figure 2: The Structure of FairGAN", "figure_type": "diagram", "evidence_from_figure": "D2's inputs labeled (x̂, ŷ|ŝ=0) and (x̂, ŷ|ŝ=1)", "evidence_from_text": "Text states G is trained to 'fool the discriminator unable to distinguish generated data from real data' and mentions fairness via protected attributes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_2_281", "query": "How does the generator G_dec (Figure 2) utilize the protected attribute P_s and noise P_z as explained in the text?", "answer": "G_dec integrates P_s (protected attribute) and P_z (noise) as inputs to generate synthetic data. The text explains that G generates salient representations (incorporating P_s) while the decoder reconstructs the input, ensuring the protected attribute is preserved in the synthetic data.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig1.jpg", "caption": "Figure 2: The Structure of FairGAN", "figure_type": "diagram", "evidence_from_figure": "G_dec node connected to P_s and P_z inputs", "evidence_from_text": "Text states 'the generator G is trained to generate the salient representations' and 'the decoder seeks to reconstruct the original input'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_5_282", "query": "In Figure 3(a), which line represents P_data(x|s=1)?", "answer": "The green line represents P_data(x|s=1), as indicated by the legend and the caption's description of real data distributions.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg", "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.", "figure_type": "plot", "evidence_from_figure": "Green line in the legend and plot", "evidence_from_text": "Caption stating '(a) shows the distributions P_data(x) (black), P_data(x|s=1) (green) and P_data(x|s=0) (red) of real data'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_5_283", "query": "Why is Figure 3(a) included in the comparison of FairGAN and the NaïveFairGAN variants?", "answer": "Figure 3(a) provides the real data distribution baseline for evaluating how well synthetic datasets generated by FairGAN and the NaïveFairGAN variants match the original data, which is critical for assessing fair data generation.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig4.jpg", "caption": "Figure 3: Comparing FairGAN, NaïveFairGAN-I and NaïveFairGAN-II on a toy dataset. (a) shows the distributions $P _ { \\mathbf { d a t a } } ( x )$ (black), $P _ { \\bf d a t a } ( x | s = 1 )$ (green) and $P _ { \\bf d a t a } ( x | s = 0 )$ (red) of real data; (b), (c) and (d) are distributions $P _ { G } ( x ) , P _ { G } ( x | s = 1 )$ and $P _ { G } ( x | s = 0 )$ P x s P x s Pof synthetic datasets generated by NaïveFairGAN-I, NaïveFairGAN-II and FairGAN separately.", "figure_type": "plot", "evidence_from_figure": "Real data distributions shown in part (a)", "evidence_from_text": "Text stating 'We evaluate the performance of FairGAN on fair data generation and fair classification' and caption explaining Figure 3's structure", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_22_284", "query": "In Figure 5, what does the diagonal line represent in the context of the study's fairness evaluation?", "answer": "The diagonal line represents the ideal case where the synthetic dataset's probability distributions exactly match the real dataset's, indicating identical quality for fair classification.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_22", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg", "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.", "figure_type": "plot", "evidence_from_figure": "Diagonal line in the plot", "evidence_from_text": "Caption stating 'the diagonal line indicates the ideal case where the real and synthetic data show identical quality'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_22_285", "query": "How does the clustering of data points near the diagonal in Figure 5 relate to the synthetic dataset's suitability for fair classification?", "answer": "The clustering near the diagonal indicates the synthetic dataset accurately replicates the real data's probability distributions, supporting its use for fair classification as evaluated in Section 5.3.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_22", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig21.jpg", "caption": "Figure 5: Dimension-wise probability distributions. Each dot represents one attribute. The $\\mathbf { x }$ -axis represents the Bernoulli success probability for the real dataset. The y-axis represents the probability for the synthetic dataset generated by each model. The diagonal line indicates the ideal case, where the real and synthetic data show identical quality.", "figure_type": "plot", "evidence_from_figure": "Data points clustering along the diagonal", "evidence_from_text": "Section 5.3 discussing fairness evaluation with synthetic datasets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_25_286", "query": "In Figure 6, how does the risk difference change as the lambda parameter increases from 0.0 to 2.0, and what does this imply about FairGAN's fairness performance?", "answer": "The risk difference decreases sharply from ~0.18 to ~0.04 as lambda increases, indicating improved fairness. This aligns with the text stating lambda controls fair data generation, where higher values reduce disparate impact.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_25", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg", "caption": "Figure 6: The sensitivity analysis of FairGAN with various", "figure_type": "plot", "evidence_from_figure": "Blue line trend (decreasing from 0.18 to 0.04) and x-axis parameter range", "evidence_from_text": "Text states 'λ for fair data generation' and FairGAN's goal to eliminate disparate impact", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1805.11202_1805.11202_fig_25_287", "query": "Why does the Euclidean distance remain nearly constant at low values in Figure 6, and how does this relate to FairGAN's data utility?", "answer": "The Euclidean distance stays near 0.02, indicating minimal distortion between synthetic and real data. This supports FairGAN's claim of retaining 'high data utility' (text), as low Euclidean distance preserves data similarity while ensuring fairness.", "doc_id": "1805.11202", "figure_id": "1805.11202_fig_25", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1805.11202/1805.11202/hybrid_auto/images/1805.11202_page0_fig24.jpg", "caption": "Figure 6: The sensitivity analysis of FairGAN with various", "figure_type": "plot", "evidence_from_figure": "Flat orange line at ~0.02", "evidence_from_text": "Text states FairGAN retains 'high data utility' while eliminating discrimination", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_1_288", "query": "In Figure 1, how does the distribution of accepted individuals (circles) in the Blue/Male quadrant versus Blue/Female quadrant demonstrate a violation of rich subgroup fairness?", "answer": "The Blue/Male quadrant has a higher density of accepted individuals (circles) than Blue/Female, violating the rich subgroup fairness constraint of equalizing false positive rates across gender within the Blue group as defined in the text.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg", "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]", "figure_type": "example", "evidence_from_figure": "Uneven circle density between Blue/Male and Blue/Female quadrants", "evidence_from_text": "Text defines rich subgroup fairness as requiring statistical constraints (e.g., equal false positive rates) across subgroups", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_1_289", "query": "According to the text, why is the Green/Male quadrant in Figure 1 significant for understanding rich subgroup fairness?", "answer": "The Green/Male quadrant shows a lower density of accepted individuals compared to Green/Female, illustrating a violation of equal false positive rates across gender within the Green group, which rich subgroup fairness requires to hold over all subgroups.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig0.jpg", "caption": "Figure 1: Fairness Gerrymandering: A Toy Example [Kearns et al., 2018]", "figure_type": "example", "evidence_from_figure": "Lower circle density in Green/Male vs Green/Female", "evidence_from_text": "Text specifies rich subgroup fairness demands constraints to hold across combinatorially large subgroups", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_3_290", "query": "What do the dashed horizontal lines in Figure 2 (this plot) represent?", "answer": "The dashed horizontal lines correspond to varying input gamma values used in the experiment, as explained in the figure caption.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg", "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .", "figure_type": "plot", "evidence_from_figure": "Dashed horizontal lines on the gamma_t plot", "evidence_from_text": "Figure caption stating 'Dashed horizontal lines on γ_t plots correspond to varying values of γ'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_3_291", "query": "How does the convergence of gamma_t relate to the trade-off between error and fairness for the Law School dataset?", "answer": "Higher input gamma values lead to higher gamma_t convergence levels, demonstrating the trade-off where increased fairness violation (gamma_t) corresponds to lower error, as described in the text.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig2.jpg", "caption": "Figure 2: Error $\\varepsilon _ { t }$ and fairness violation $\\gamma _ { t }$ for Law School dataset (panels (a) and (b)) and Adult data set (panels (c) and (d)), for values of input $\\gamma$ ranging from 0 to 0.03. Dashed horizontal lines on $\\gamma _ { t }$ plots correspond to varying values of $\\gamma$ .", "figure_type": "plot", "evidence_from_figure": "Gamma_t convergence toward different dashed lines based on input gamma", "evidence_from_text": "Text stating 'varying the input gamma provides an appealing trade-off between error and fairness'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_11_292", "query": "Which dataset does the figure titled 'adult, marginal fairness' correspond to, and what is its position in the dataset ordering mentioned in the text?", "answer": "The dataset is Adult, which is the third in the ordering (Communities and Crime, Law School, Adult, Student).", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_11", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg", "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.", "figure_type": "plot", "evidence_from_figure": "Title 'adult, marginal fairness'", "evidence_from_text": "Ordering of datasets is Communities and Crime, Law School, Adult, and Student", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_11_293", "query": "What does the blue data series in the left column of Figure 3 represent, and how does it relate to the MARGINAL algorithm's results shown in the right column figure titled 'adult, marginal fairness'?", "answer": "The blue series in the left column represents the MARGINAL algorithm's error and subgroup fairness violation across all four datasets. The right column figure for Adult shows the specific data points for the Adult dataset as part of the MARGINAL algorithm's results across all datasets.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_11", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig10.jpg", "caption": "Figure 3: Left column: The red points show the Pareto frontier of error (x axis) and subgroup fairness violation (y axis) for the SUBGROUP algorithm across all four data sets, while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm. Right column: The error and marginal fairness violation for the MARGINAL algorithm across all four data sets. Ordering of datasets is Communities and Crime, Law School, Adult, and Student.", "figure_type": "plot", "evidence_from_figure": "Right column figure labeled 'adult, marginal fairness'", "evidence_from_text": "Left column: blue points show MARGINAL algorithm results; Right column: MARGINAL algorithm across all four datasets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_19_294", "query": "What does the z-axis label 'gamma disparity' in Figure 4 correspond to according to the text?", "answer": "The z-axis represents the false positive discrepancy for the subgroup.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_19", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg", "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12", "figure_type": "plot", "evidence_from_figure": "z-axis label 'gamma disparity'", "evidence_from_text": "text states 'the corresponding z value is the current false positive discrepancy for the subgroup'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_19_295", "query": "At t = 31 (as shown in the figure title), what is the maximum value of gamma disparity observed?", "answer": "Approximately 0.03.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_19", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig18.jpg", "caption": "Figure 4: Evolution of discrimination surface for the SUBGROUP algorithm from $t = 1 \\ldots 1 3 0 1$ . Each point in the plane corresponds to a different subgroup over two protected attributes, and the corresponding $z$ value is the current false positive discrepancy for the subgroup. 12", "figure_type": "plot", "evidence_from_figure": "peak of the surface at z ≈ 0.03", "evidence_from_text": "text specifies t=31 as part of the algorithm's evolution from t=1 to 1301", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_23_296", "query": "Why does the trajectory in Figure 5 for gamma=0.005 show a sharp increase in gamma_t after error=0.35, as described in the text's discussion of subgroup fairness?", "answer": "The sharp increase reflects the algorithm's adjustment to enforce subgroup fairness constraints when error exceeds 0.35, as the text emphasizes the necessity of explicitly enforcing subgroup fairness (not just marginal fairness) to maintain the target gamma value.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_23", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg", "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .", "figure_type": "plot", "evidence_from_figure": "Sharp increase in gamma_t after error=0.35 and dashed line at gamma=0.005", "evidence_from_text": "Text states 'necessity of explicitly enforcing subgroup (as opposed to only marginal) fairness' and mentions gamma values in experiments", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1808.08166_1808.08166_fig_23_297", "query": "What does the dashed horizontal line at gamma=0.005 in Figure 5 represent in the context of the paper's experimental setup?", "answer": "The dashed line represents the target gamma value (0.005) specified in the experiment, as the text lists gamma ∈ {0.001, 0.005, ...} and the figure title confirms this trajectory corresponds to gamma = 0.005.", "doc_id": "1808.08166", "figure_id": "1808.08166_fig_23", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1808.08166/1808.08166/hybrid_auto/images/1808.08166_page0_fig22.jpg", "caption": "Figure 5: $\\langle \\varepsilon _ { t } , \\gamma _ { t } \\rangle$ trajectories for Communities and Crime, for $\\gamma \\in \\{ 0 . 0 0 1 , 0 . 0 0 5 , 0 . 0 0 9 , 0 . 0 2 2 \\}$ .", "figure_type": "plot", "evidence_from_figure": "Dashed line at gamma=0.005", "evidence_from_text": "Text states 'gamma ∈ {0.001, 0.005, 0.009, 0.022}' and figure caption specifies 'gamma = 0.005'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.01496_1809.01496_fig_1_298", "query": "In Figure 1, what does the cosine similarity value of the word 'nurse' indicate about its gender bias according to the caption?", "answer": "The word 'nurse' has a negative cosine similarity value, indicating a bias towards female as specified in the caption.", "doc_id": "1809.01496", "figure_id": "1809.01496_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg", "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.", "figure_type": "plot", "evidence_from_figure": "Position of 'nurse' on the negative x-axis", "evidence_from_text": "Caption stating 'negative values represent a bias towards female'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.01496_1809.01496_fig_1_299", "query": "According to the text, why is the vector v_g treated as fixed during training, and how does this affect the cosine similarity values in Figure 1?", "answer": "v_g is fixed to reduce computational complexity (as per the text), allowing the cosine similarity to consistently measure bias against a stable gender direction, which is reflected in the distribution of values in Figure 1.", "doc_id": "1809.01496", "figure_id": "1809.01496_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.01496/1809.01496/hybrid_auto/images/1809.01496_page0_fig0.jpg", "caption": "Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In each figure, negative values represent a bias towards female, otherwise male.", "figure_type": "plot", "evidence_from_figure": "Distribution of cosine similarity values across labeled words", "evidence_from_text": "Section 4 stating 'we assume v_g is a fixed vector... to reduce computational complexity'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_1_300", "query": "Why does Google Translate assign male pronouns to 'engineer' and 'CEO' but female pronouns to 'nurse' and 'baker' in Figure 1?", "answer": "Because the translation tool reflects societal gender inequality by using examples from society, which associates male-dominated fields with male pronouns and female-dominated fields with female pronouns, as explained in the text's assumption that translation tools 'inevitably retain some of that bias'.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg", "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.", "figure_type": "example", "evidence_from_figure": "Gendered pronouns paired with specific occupations (e.g., 'he is an engineer' vs. 'she's a nurse')", "evidence_from_text": "Text states translation tools reflect 'societal inequality' and 'poll from examples that society produced'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_1_301", "query": "What does reference [40] in the figure caption support?", "answer": "Reference [40] supports the claim that occupations like engineer and CEO are traditionally male-dominated, as the text explains that translation tools reflect societal gender biases in such fields.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig0.jpg", "caption": "Figure 1: Translating sentences from a gender neutral language such as Hungarian to English provides a glimpse into the phenomenon of gender bias in machine translation. This screenshot from Google Translate shows how occupations from traditionally male-dominated fields [40] such as scholar, engineer and CEO are interpreted as male, while occupations such as nurse, baker and wedding organizer are interpreted as female.", "figure_type": "example", "evidence_from_figure": "Caption mentions [40] alongside male-dominated fields (engineer, CEO)", "evidence_from_text": "Text states translation tools 'reflect at most the inequality existent in society'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_2_302", "query": "Why is the STEM category concentrated at X=0 in Figure 2, and how does the text explain the exclusion of languages like Korean and Nepali?", "answer": "The STEM category's concentration at X=0 in Figure 2 reflects the exclusion of languages like Korean and Nepali (which may have different pronoun structures), as explained in the text's rationale for omitting these languages due to technical difficulties with their scripts and complex grammar.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg", "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table", "figure_type": "plot", "evidence_from_figure": "STEM bar segment at X=0", "evidence_from_text": "Text states Korean/Nepali were omitted due to technical difficulties and complex grammar", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_2_303", "query": "How does the inverse distribution in Figure 2 relate to gender-neutral pronouns in Figure 4 according to the text?", "answer": "The inverse distribution of female pronouns in Figure 2 is mirrored in Figure 4's gender-neutral pronouns data, as explicitly stated in the text: 'the number of female pronouns is inversely distributed – which is mirrored in the data for gender-neutral pronouns in Figure 4'.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig1.jpg", "caption": "Figure 2: The data for the number of translated female pronouns per merged occupation category totaled among languages suggests and inverse distribution. STEM fields are nearly exclusively concentrated at $X = 0$ , while more evenly distributed in fields such as production and healthcare (See Table", "figure_type": "plot", "evidence_from_figure": "Inverse distribution pattern (STEM at X=0, other categories spread)", "evidence_from_text": "Text explicitly links Figure 2's inverse distribution to Figure 4", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_3_304", "query": "What is the peak value of the '# Translated Male Pronouns' distribution in Figure 3, and how does the text describe its shape?", "answer": "The peak occurs at X = 6, and the text describes it as a skew normal distribution.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg", "caption": "Figure 3: In contrast to Figure   ", "figure_type": "plot", "evidence_from_figure": "Peak at X = 6 on the x-axis", "evidence_from_text": "Text states 'peak at X = 6' and 'skew normally distributed'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_3_305", "query": "Which category is most prominent in the right tail (X ≥ 6) of Figure 3, and how does this align with the text's statement about STEM fields?", "answer": "STEM (yellow) is most prominent in X ≥ 6, aligning with the text's claim that STEM fields concentrate to the right.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig2.jpg", "caption": "Figure 3: In contrast to Figure   ", "figure_type": "plot", "evidence_from_figure": "Yellow bars (STEM) dominate the right tail (X ≥ 6)", "evidence_from_text": "Text states 'STEM fields concentrate mainly to the right (X ≥ 6)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_4_306", "query": "In Figure 4, why are STEM fields predominantly concentrated at X=0? What does this indicate about gender-neutral pronouns in STEM occupations?", "answer": "This concentration indicates that STEM occupations have very few gender-neutral pronouns, reflecting their scarcity in language data as stated in the caption: 'The scarcity of gender-neutral pronouns is manifest in their histogram.'", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg", "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .", "figure_type": "plot", "evidence_from_figure": "STEM category dominates the X=0 bar in the stacked histogram", "evidence_from_text": "Caption states 'STEM fields are predominantly concentrated at X = 0' due to scarcity", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_4_307", "query": "How does the distribution of gender-neutral pronouns in Figure 4 differ from male pronouns in Figure 3 as described in the text?", "answer": "Figure 4 shows STEM concentrated at X=0 (scarcity), while Figure 3 shows male pronouns peaking at X=6 with STEM concentrated at X ≥6. This contrast highlights how gender-neutral pronouns are scarce in STEM (Figure 4) versus male pronouns being more prevalent (Figure 3).", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig3.jpg", "caption": "Figure 4: The scarcity of gender-neutral pronouns is manifest in their histogram. Once again, STEM fields are predominantly concentrated at $X = 0$ .", "figure_type": "plot", "evidence_from_figure": "Figure 4's STEM segment at X=0", "evidence_from_text": "Text states Figure 3 has 'skew normal distribution with peak at X=6' and 'STEM fields concentrate mainly to the right (X ≥6)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_5_308", "query": "Why does the Male pronoun category show a wider distribution across translated pronoun counts in Figure 5, given the text states male defaults are the second-to-most prominent in STEM fields?", "answer": "The spread of Male pronouns across higher translated pronoun counts reflects the prevalence of male defaults in STEM occupations, as noted in the text. This distribution indicates variability in how male pronouns are translated across different job roles, consistent with male defaults being second-to-most prominent after Legal fields.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg", "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).", "figure_type": "plot", "evidence_from_figure": "Male pronoun bars distributed across 6-10 translated pronouns", "evidence_from_text": "male defaults are the second-to-most prominent (after Legal)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_5_309", "query": "What does the high count of occupations with 0 translated pronouns for Female and Neutral in Figure 5 imply about gender representation in STEM fields?", "answer": "The high count of occupations with 0 translated pronouns for Female and Neutral suggests limited gender-neutral or female-specific pronoun translations in STEM occupations, aligning with the text's assertion that male defaults dominate the field. This indicates underrepresentation of non-male gender pronouns in professional language contexts.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig4.jpg", "caption": "Figure 5: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the STEM (Science, Technology, Engineering and Mathematics) field, in which male defaults are the second-to-most prominent (after Legal).", "figure_type": "plot", "evidence_from_figure": "Tall bars for Female/Neutral at 0 translated pronouns", "evidence_from_text": "male defaults are the second-to-most prominent in STEM", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_6_310", "query": "Which gender category has the highest count of occupations with 2 translated pronouns in the Healthcare field according to Figure 6?", "answer": "Neutral pronouns (pink bar)", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg", "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.", "figure_type": "plot", "evidence_from_figure": "Pink bar at x=2 is the tallest among gender categories", "evidence_from_text": "Caption specifies Healthcare field context", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_6_311", "query": "Why does the text state male defaults are least prominent in the Healthcare field? Refer to Figure 6.", "answer": "Male pronoun bars (yellow) are consistently the shortest across all pronoun counts in Figure 6, indicating fewer occupations use male translated pronouns.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig5.jpg", "caption": "Figure 6: Histograms for the distribution of the number of translated female, male and gender neutral pronouns totaled among languages are plotted side by side for job occupations in the Healthcare field, in which male defaults are least prominent.", "figure_type": "plot", "evidence_from_figure": "Yellow bars are shortest for all x-values", "evidence_from_text": "Caption explicitly states 'male defaults are least prominent'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_7_312", "query": "Why do the bars in Figure 7 not sum to 100%, as mentioned in the caption?", "answer": "The bars do not sum to 100% because some translated sentences lacked gender pronouns, as explained in the caption.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_7", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg", "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively", "figure_type": "plot", "evidence_from_figure": "Bars do not reach 100% on the y-axis", "evidence_from_text": "Caption states 'bars do not add up to 100% due to sentences without gender pronouns'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_7_313", "query": "Which occupation category has the highest proportion of male pronouns in Figure 7, and how does this align with the text's description of male defaults?", "answer": "Legal has the highest proportion of male pronouns, aligning with the text's statement that Legal exhibits a predominance of male defaults.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_7", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig6.jpg", "caption": "Figure 7: Bar plots show how much of the distribution of translated gender pronouns for each occupation category (grouped as in Table 7) is composed of female, male and neutral terms. Legal and STEM fields exhibit a predominance of male defaults and contrast with Healthcare and Education, with a larger proportion of female and neutral pronouns. Note that in general the bars do not add up to $1 0 0 \\%$ , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun. Categories are sorted with respect to the proportions of male, female and neutral translated pronouns respectively", "figure_type": "plot", "evidence_from_figure": "Legal bar's pink segment is the tallest", "evidence_from_text": "Text states 'Legal and STEM fields exhibit a predominance of male defaults'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_8_314", "query": "In Figure 8, what does the darker shade of blue indicate about the statistical significance of Language-Category pairs?", "answer": "It indicates pairs where the complementary null hypothesis is rejected, as explained in the text.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "evidence_from_figure": "Darker blue regions in the heatmap", "evidence_from_text": "Text states 'Language-Category pairs for which the complementary null hypothesis is rejected are painted in a darker shade of blue'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_8_315", "query": "Why are the axes in Figure 8 sorted to concentrate higher probabilities in the bottom right corner?", "answer": "To facilitate visual identification of high-probability pairs, as stated in the figure caption.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "evidence_from_figure": "Concentration of red (high probability) in the bottom-right corner", "evidence_from_text": "Caption states 'both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_8_316", "query": "Which language shows the highest probability for the Healthcare category in Figure 8, and what does this imply about the null hypothesis?", "answer": "Finnish shows the highest probability (red), implying the complementary null hypothesis is not rejected for this pair.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_8", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig7.jpg", "caption": "Figure 8: Heatmap for the translation probability into female pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to $1 0 0 \\%$ (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "evidence_from_figure": "Finnish row in Healthcare column is red", "evidence_from_text": "Text states darker blue indicates rejected null hypotheses, so red implies non-rejection", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_10_317", "query": "In Figure 10, what does the blue color in the 'Malay' row for 'Education' category indicate, and how does this align with the study's probability scale described in the text?", "answer": "The blue color indicates 0% probability of translation into gender-neutral pronouns, consistent with the text's description of blue representing 0% probability.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_10", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg", "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "evidence_from_figure": "Blue cell at intersection of 'Malay' and 'Education'", "evidence_from_text": "Probabilities range from 0% (blue) to 100% (red)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_10_318", "query": "How does the concentration of high probabilities in the bottom-right of Figure 10 relate to the text's claim about language groups visualizing cultural effects on gender bias?", "answer": "The bottom-right concentration shows higher translation probabilities for certain language-category pairs, which the text suggests reflects cultural influences on gender bias, supporting the need for language group analysis.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_10", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig9.jpg", "caption": "Figure 10: Heatmap for the translation probability into gender neutral pronouns for each pair of language and occupation category. Probabilities range from 0% (blue) to 100% (red), and both axes are sorted in such a way that higher probabilities concentrate on the bottom right corner.", "figure_type": "plot", "evidence_from_figure": "Red/peach colors concentrated in bottom-right corner", "evidence_from_text": "coalescing data by language groups... helps visualize the effect of different cultures in the genesis of gender bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_11_319", "query": "Which languages have the lowest female pronoun percentages according to Figure 11 and the text?", "answer": "Japanese (0.196%) and Chinese (1.865%) have the lowest female pronoun percentages, as stated in the text and visually confirmed by the minimal yellow segments in their bars.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_11", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg", "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.", "figure_type": "plot", "evidence_from_figure": "Minimal yellow segments in Japanese and Chinese bars", "evidence_from_text": "Text states 'female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_11_320", "query": "Why do some bars in Figure 11 not sum to 100%, and which language is specifically mentioned as having the most untranslated sentences?", "answer": "Some bars do not sum to 100% due to untranslated sentences where gender pronouns could not be determined. Basque is highlighted in the text as having the most untranslated sentences, evidenced by its incomplete bar.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_11", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig10.jpg", "caption": "Figure 11: The distribution of pronominal genders per language also suggests a tendency towards male defaults, with female pronouns reaching as low as 0.196% and 1.865% for Japanese and Chinese respectively. Once again not all bars add up to 100% , as there is a fair amount of translated sentences for which we cannot obtain a gender pronoun, particularly in Basque. Among all tested languages, Basque was the only one to yield more gender neutral than male pronouns, with Bengali and Yoruba following after in this order. Languages are sorted with respect to the proportions of male, female and neutral translated pronouns respectively.", "figure_type": "plot", "evidence_from_figure": "Basque bar does not reach 100%", "evidence_from_text": "Text states 'not all bars add up to 100%... particularly in Basque'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_12_321", "query": "Which adjectives in Figure 12 show a high female pronoun association, and what methodology was used to select these adjectives according to the text?", "answer": "Shy, Desirable, Sad, and Dumb show high female association (yellow segments); the text states they were filtered from the COCA corpus based on applicability to the sentence template.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_12", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg", "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.", "figure_type": "plot", "evidence_from_figure": "Adjectives with prominent yellow segments (Female)", "evidence_from_text": "Text states adjectives were filtered from COCA corpus based on sentence template applicability", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_12_322", "query": "How does the distribution of male pronouns for 'Proud' in Figure 12 align with the text's claim about gender bias in translation?", "answer": "Figure 12 shows 'Proud' has ~70% male pronouns (pink segment), aligning with the text's claim that it is 'almost exclusively translated with male pronouns' due to stereotypical gender roles.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_12", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig11.jpg", "caption": "Figure 12: The distribution of pronominal genders for each word in Table 5 shows how stereotypical gender roles can play a part on the automatic translation of simple adjectives. One can see that adjectives such as Shy and Desirable, Sad and Dumb amass at the female side of the spectrum, contrasting with Proud, Guilty, Cruel and Brave which are almost exclusively translated with male pronouns.", "figure_type": "plot", "evidence_from_figure": "Male pronoun percentage for 'Proud' bar", "evidence_from_text": "Text states 'Proud... are almost exclusively translated with male pronouns'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_13_323", "query": "In Figure 13, how do the blue bars (Google Translate Female %) in quantiles 10-12 compare to yellow bars (BLS Female Participation %), and what does the text say about this for the gender bias hypothesis?", "answer": "Blue bars are very low in quantiles 10-12 while yellow bars are relatively high. The text states this lack of correspondence undermines the hypothesis that Google Translate's bias reflects low female participation in jobs.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_13", "figure_number": 13, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg", "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.", "figure_type": "plot", "evidence_from_figure": "Blue bars in quantiles 10-12 are minimal; yellow bars are elevated", "evidence_from_text": "Text states 'the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_13_324", "query": "Which quantile shows the highest Google Translate Female % (blue), and how does the text interpret this in relation to male-dominated fields?", "answer": "Quantile 1 shows the highest blue bar. The text explains this contradicts the hypothesis, as male-dominated fields (low female participation) would align with higher blue values in low quantiles, but the data shows blue peaks early while yellow increases later.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_13", "figure_number": 13, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig12.jpg", "caption": "Figure 13: Women participation ( $\\%$ ) data obtained from the U.S. Bureau of Labor Statistics allows us to assess whether the Google Translate bias towards male defaults is at least to some extent explained by small frequencies of female workers in some job positions. Our data does not make a very good case for that hypothesis: the total frequency of translated female pronouns (in blue) for each 12-quantile does not seem to respond to the higher proportion of female workers (in yellow) in the last quantiles.", "figure_type": "plot", "evidence_from_figure": "Blue bar is tallest at quantile 1", "evidence_from_text": "Text notes the hypothesis expects blue to correlate with yellow in low quantiles (male-dominated fields), but the data shows blue peaks early while yellow rises later", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_14_325", "query": "What gendered translation does the 'Before' version of Google Translate (Figure 14) provide for 'o bir doktor', and why is this significant per the text?", "answer": "The translation is 'he is a doctor'. This reflects the male default bias in pre-2018 Google Translate, which the text explains stems from training data and societal biases, as evidenced by the lack of feminine options in the older interface.", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_14", "figure_number": 14, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg", "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.", "figure_type": "example", "evidence_from_figure": "Translation output 'he is a doctor' in blue box", "evidence_from_text": "Text states 'statistical translation tools... exhibit gender biases and a strong tendency towards male defaults'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02208_1809.02208_fig_14_326", "query": "When did Google Translate introduce the gender fairness feature shown in Figure 14's 'After' version?", "answer": "December 6, 2018, as stated in the text: 'On December 6, 2018 the company’s policy changed, and a statement was released detailing their efforts to reduce gender bias...'", "doc_id": "1809.02208", "figure_id": "1809.02208_fig_14", "figure_number": 14, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02208/1809.02208/hybrid_auto/images/1809.02208_page0_fig13.jpg", "caption": "Figure 14: Comparison between the GUI of Google Translate before (left) and after (right) the introduction of the new feature intended to promote gender fairness in translation. The results described in this paper relate to the older version.", "figure_type": "example", "evidence_from_figure": "Figure 14 caption references 'after (right)' version with gender fairness feature", "evidence_from_text": "Text explicitly cites December 6, 2018 as the policy change date", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02244_1809.02244_fig_6_327", "query": "At what utility parameter θ does the Optimal Fair policy's expected utility surpass the Observed policy in Figure 4?", "answer": "Approximately θ = 2.5. The text explains that the Optimal Fair policy is designed to maximize utility under fairness constraints, which becomes advantageous as θ (the fairness weight) increases beyond this threshold.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg", "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "evidence_from_figure": "Intersection point of dashed and dotted lines at θ ≈ 2.5", "evidence_from_text": "Theorem 1 discusses path-specific effects (PSE^{sy}, PSE^{sak_k}) that quantify fairness-utility trade-offs", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.02244_1809.02244_fig_6_328", "query": "Why does the Optimal Fair policy's utility decline more slowly than the Optimal unfair policy as θ increases in Figure 4?", "answer": "The Optimal Fair policy is designed to maintain fairness constraints, making it less sensitive to θ increases. The text explains that θ modulates path-specific effects (PSE^{sy}, PSE^{sak_k}), where fairness-optimized policies better balance utility trade-offs.", "doc_id": "1809.02244", "figure_id": "1809.02244_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.02244/1809.02244/hybrid_auto/images/1809.02244_page0_fig5.jpg", "caption": "Figure 4. The relative utility of policies for the COMPAS data as a function of the utility parameter $\\theta$ .", "figure_type": "plot", "evidence_from_figure": "Dashed line (Optimal Fair) has a shallower slope than solid line (Optimal unfair)", "evidence_from_text": "Theorem 1's latent projection identifies path-specific effects governing utility-fairness trade-offs", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.04737_1809.04737_fig_1_329", "query": "In Figure 1, the δ=logistic curve shows a sharp transition at x=0. How does this relate to the text's discussion of fairness constraints involving RD(f) ≤ τ?", "answer": "The sharp transition of δ=logistic at x=0 reflects the non-convex nature of the fairness constraint, as the text explains that using a single surrogate function for both RD(f) ≤ τ and -RD(f) ≤ τ creates conflicting convex-concave behavior, making optimization challenging.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg", "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .", "figure_type": "plot", "evidence_from_figure": "δ=logistic curve's steep transition at x=0", "evidence_from_text": "Text states 'two constraints RD(f) ≤ τ and -RD(f) ≤ τ are opposite... replacing indicator functions with a single surrogate function will result in a convex-concave problem'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.04737_1809.04737_fig_1_330", "query": "How does the κ=hinge line's behavior (flat for x<0, linear for x>0) align with the text's explanation of surrogate functions replacing indicator functions?", "answer": "The κ=hinge's piecewise linear shape is a standard surrogate for indicator functions in optimization, but as the text notes, applying it to dual constraints (RD(f) ≤ τ and -RD(f) ≤ τ) creates convex-concave conflicts, which the figure's sharp transition visually exemplifies.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig0.jpg", "caption": "Figure 1: Examples of $\\kappa ( \\cdot )$ and $\\delta ( \\cdot )$ .", "figure_type": "plot", "evidence_from_figure": "κ=hinge's flat region for x<0 and linear slope for x>0", "evidence_from_text": "Text states 'replacing all indicator functions with a single surrogate function will result in a convex-concave problem'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.04737_1809.04737_fig_2_331", "query": "How does the clustering of female predictions at y=-0.5 in Figure 2 relate to the definition of H_δ^+ (η) as the maximal conditional δ-risk difference in the text?", "answer": "The female predictions at y=-0.5 represent a concentration of values that contribute to the maximal conditional δ-risk difference H_δ^+ (η), which is defined under the interval condition α(η_S - p) ≥ 0 in the text.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg", "caption": "Figure 2: Two classifiers and their predictions.", "figure_type": "plot", "evidence_from_figure": "blue dots clustered at y=-0.5", "evidence_from_text": "definition of H_δ^+ (η) as maximal conditional δ-risk difference", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.04737_1809.04737_fig_2_332", "query": "Why are the male classifier's predictions at y=-1.5 in Figure 2 significant for the constrained optimization problem R D_κ(h) ≤ ψ_κ(c1 - R D^-) + R D_κ^-?", "answer": "The male predictions at y=-1.5 contribute to the risk difference calculation, which must satisfy the constraint where the RHS is a constant for the dataset, ensuring the optimization remains convex as stated in the text.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig1.jpg", "caption": "Figure 2: Two classifiers and their predictions.", "figure_type": "plot", "evidence_from_figure": "yellow dots at y=-1.5", "evidence_from_text": "constraint R D_κ(h) ≤ ψ_κ(c1 - R D^-) + R D_κ^- and convexity claim", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.04737_1809.04737_fig_4_333", "query": "At a risk difference of 0.3, which method has the lowest empirical loss, and what does the text imply about the fairness-accuracy trade-off for this method?", "answer": "Zafar-1 has the lowest empirical loss at risk difference 0.3. The text implies that higher risk difference (less fair) allows lower empirical loss, reflecting the trade-off between fairness and accuracy in classifier design.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg", "caption": "Figure 3: Comparison of fair classifiers.", "figure_type": "plot", "evidence_from_figure": "Zafar-1's solid blue line at x=0.3 shows the lowest y-value (empirical loss)", "evidence_from_text": "Text describes maximal/minimal risk difference classifiers and the sufficiency criterion for fairness", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.04737_1809.04737_fig_4_334", "query": "How does Our Method compare to Zafar-2 at a risk difference of 0.1 in terms of empirical loss, and what does this reveal about the sufficiency criterion?", "answer": "Our Method has higher empirical loss than Zafar-2 at risk difference 0.1. This suggests Zafar-2 better satisfies the sufficiency criterion by achieving lower loss at similar risk differences, indicating a more efficient fairness-accuracy trade-off.", "doc_id": "1809.04737", "figure_id": "1809.04737_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.04737/1809.04737/hybrid_auto/images/1809.04737_page0_fig3.jpg", "caption": "Figure 3: Comparison of fair classifiers.", "figure_type": "plot", "evidence_from_figure": "Our Method's dotted red line is above Zafar-2's dashed green line at x=0.1", "evidence_from_text": "Text explains the sufficiency criterion for fair classifiers and mentions comparing methods in Table 3", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_5_335", "query": "What do the 'upper-left' and 'lower-right' labels in Figure 2(a)'s legend represent?", "answer": "They represent distinct lighting conditions of the Extended Yale-B dataset, as confirmed by the figure's legend and the caption specifying that Figure 2(a) visualizes raw data with lighting variations.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg", "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).", "figure_type": "plot", "evidence_from_figure": "Legend entries for 'upper-left' and 'lower-right' with corresponding markers", "evidence_from_text": "Caption stating Figure 2(a) shows raw data from Extended Yale-B with lighting condition labels", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_5_336", "query": "Why does the t-SNE visualization of raw data (Figure 2a) cluster by lighting conditions?", "answer": "The raw data inherently contains variations due to different lighting conditions in the Extended Yale-B dataset, which the t-SNE algorithm visualizes as distinct clusters, as the dataset is designed to capture such lighting variations.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig4.jpg", "caption": "Figure 2: Extended Yale-B – t-SNE visualization of (a) raw data, (b) $e _ { 2 }$ labeled by lighting condition, (c) $e _ { 1 }$ labeled by lighting condition, and (d) $e _ { 1 }$ labeled by subject-ID (numerical markers, not colors).", "figure_type": "plot", "evidence_from_figure": "Clustering of data points matching legend categories", "evidence_from_text": "Caption describing Figure 2(a) as raw data from Extended Yale-B, which includes lighting variations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_13_337", "query": "In Figure 3(b), which reconstruction method (e₁ or e₂) shows more accurate details for the chair images, and how does this relate to the text's claim about the unsupervised adversarial model outperforming CAI?", "answer": "e₂ shows more accurate details (e.g., sharper edges and clearer textures), aligning with the text's assertion that the unsupervised adversarial model outperforms CAI by eliminating reliance on supervised rotation angle information.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_13", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg", "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "evidence_from_figure": "Visual comparison of e₁ vs e₂ columns in the chair dataset (b)", "evidence_from_text": "Text states unsupervised model outperforms CAI using supervised rotation angle information", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_13_338", "query": "What do the columns in Figure 3(b) represent, and how does the unsupervised adversarial model described in the text enable high-quality reconstructions?", "answer": "The columns represent real images, reconstructions from e₁, and reconstructions from e₂. The unsupervised model leverages adversarial learning to generate realistic reconstructions without requiring labeled rotation angles, as opposed to supervised methods like CAI.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_13", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig12.jpg", "caption": "Figure 3: Reconstruction from $e _ { 1 }$ and $e _ { 2 }$ for (a) Extended Yale B and (b) Chairs. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "evidence_from_figure": "Columns labeled as real, e₁, e₂", "evidence_from_text": "Text explains unsupervised model outperforms CAI by eliminating supervised rotation angle requirements", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_18_339", "query": "What does the color coding in Figure 4 (a) represent, and how does it relate to the MNIST-ROT dataset?", "answer": "The colors correspond to digits 0-9 in the MNIST-ROT dataset. Each cluster represents a digit, with variations in rotation causing overlapping clusters in the raw data visualization.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_18", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg", "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ", "figure_type": "plot", "evidence_from_figure": "Colored clusters labeled by legend (0-9)", "evidence_from_text": "Figure 4 caption states it's MNIST-ROT raw data visualization", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_18_340", "query": "What is the CAI value for the 'Ours' method at ±55° angle, and how does this relate to the cluster separation in Figure 4 (a)?", "answer": "The CAI value is 0.856. This metric reflects improved cluster separation for the 'Ours' method compared to raw data (Figure 4a), which shows more overlapping clusters due to rotation.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_18", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig17.jpg", "caption": "Figure 4: MNIST-ROT – t-SNE visualization of (a) raw data and (b) $e _ { 1 }$   ", "figure_type": "plot", "evidence_from_figure": "Figure 4 (a) shows overlapping clusters for raw data", "evidence_from_text": "Table shows CAI values for different methods and angles", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_19_341", "query": "In Figure 5, why does the proposed model's embedding (a) show more distinct clusters for digit 7 compared to baseline (b)?", "answer": "The proposed model's adversarial invariance mechanism enhances separation of unobserved rotation variations (e.g., ±55°), while the baseline lacks robustness to such synthetic augmentations. This is explained in the text as leveraging synthetic data to learn invariance for unobserved nuisance factors.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_19", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg", "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .", "figure_type": "plot", "evidence_from_figure": "Cluster separation for digit 7 in panels (a) vs. (b)", "evidence_from_text": "Section 5.2 on synthetic data augmentation and model robustness", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_19_342", "query": "What specific angles were used for training the models in Figure 5, and how does this relate to the visualization angle of ±55?", "answer": "Training used Θ = {0, ±22.5, ±45} (text), while visualization is for Θ = {±55}. The text explains this setup tests generalization to unobserved variations via synthetic augmentation, which the proposed model handles better than the baseline.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_19", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig18.jpg", "caption": "Figure 5: t-SNE visualization of MNIST-ROT $e _ { 1 }$ embedding for the proposed Unsupervised Adversarial Invariance model (a) & (c), and baseline model $B _ { 0 }$ (b) & (d). Models trained on $\\Theta = \\{ 0 , \\pm 2 2 . 5 , \\pm 4 5 \\}$ . Visualization generated for $\\Theta = \\{ \\pm 5 5 \\}$ .", "figure_type": "plot", "evidence_from_figure": "Figure caption states visualization angle as ±55", "evidence_from_text": "Training angles listed in the caption and Section 5.2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_22_343", "query": "In Figure 6, how do the reconstructions from e₁ and e₂ demonstrate the model's ability to handle rotation invariance as discussed in Section 5.2?", "answer": "The reconstructions from e₁ and e₂ (middle/right columns) show that the model can accurately recover rotated digits from latent representations, illustrating how synthetic rotation augmentation enables learning rotation-invariant features without explicit training on all rotations.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_22", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg", "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "evidence_from_figure": "Visual comparison of real vs. reconstructed digits across rotation angles", "evidence_from_text": "Section 5.2's discussion of synthetic augmentation for nuisance factor invariance", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1809.10083_1809.10083_fig_22_344", "query": "What does the MNIST-ROT reconstruction quality in Figure 6 imply about the model's generalization to unobserved rotations?", "answer": "The consistent reconstruction of rotated digits (e.g., rotated '1' and '8') from e₁/e₂ indicates the model generalizes well to unseen rotation angles, validating the effectiveness of synthetic data augmentation for learning rotation invariance as described in the text.", "doc_id": "1809.10083", "figure_id": "1809.10083_fig_22", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1809.10083/1809.10083/hybrid_auto/images/1809.10083_page0_fig21.jpg", "caption": "Figure 6: MNIST-ROT – reconstruction from $e _ { 1 }$ and $e _ { 2 }$ , (c) e. Columns in each block reflect (left to right): real, reconstruction from $e _ { 1 }$ and that from $e _ { 2 }$ .", "figure_type": "example", "evidence_from_figure": "Reconstruction accuracy across diverse rotated digit examples", "evidence_from_text": "Section 5.2's assertion that synthetic augmentation addresses unobserved variations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_1_345", "query": "What does the 'fair pre-processor' do according to Figure 1 and the text?", "answer": "It transforms the original dataset into a fairer dataset using a pre-processing algorithm to mitigate bias.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg", "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.", "figure_type": "diagram", "evidence_from_figure": "Arrow from Original Dataset to Transformed Dataset via fair pre-processor", "evidence_from_text": "Figure caption states 'transforming it into a fairer dataset using a fair pre-processing algorithm'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_1_346", "query": "According to the text, where does the BinaryLabelDatasetMetric apply in Figure 1?", "answer": "It applies to the left half of Figure 1, examining the original or transformed datasets.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg", "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.", "figure_type": "diagram", "evidence_from_figure": "Left half shows Original Dataset and Transformed Dataset components", "evidence_from_text": "Text states 'typically applied in the left half of Figure 1 to either the original dataset or the transformed dataset'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_1_347", "query": "What is the purpose of the 'fair post-processor' in the pipeline as described in the figure and text?", "answer": "It generates the Fair Predicted Dataset for fairness metric evaluation, as metrics are calculated on predicted datasets per the text.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig0.jpg", "caption": "Figure 1. The fairness pipeline. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the transformed and predicted datasets. Many other instantiations are also possible.", "figure_type": "diagram", "evidence_from_figure": "Arrow from classifier to fair post-processor leading to Fair Predicted Dataset", "evidence_from_text": "Text states 'Metrics can be calculated on the original, transformed, and predicted datasets'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_2_348", "query": "What is the ratio of favorable to total for the 32-37 age group in Figure 2, and how does this relate to the 'privileged' status mentioned in the text?", "answer": "The 32-37 age group has a ratio of approximately 0.25, which aligns with the text's definition of older groups as 'privileged' (43-58 range), indicating higher favorability in older age segments.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg", "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.", "figure_type": "plot", "evidence_from_figure": "Height of the bar for 32-37 (≈0.25)", "evidence_from_text": "Text states older groups (43-58) are 'privileged' with higher favorability", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_2_349", "query": "According to the text, what age range defines the 'older (privileged)' group in the German Credit dataset, and why does Figure 2 not include age groups beyond 32-37?", "answer": "The older (privileged) group is defined as 43-58 in the text, while Figure 2 only displays up to 32-37 because it focuses on the younger (unprivileged) group's age breakdown (17-27 range).", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig1.jpg", "caption": "Figure 2. Protected attribute bias localization in (a) younger (unprivileged), and (b) older (privileged) groups in the German Credit dataset. The 17–27 range in the younger group and the 43– 58 range in the older group would be localized by the approach.", "figure_type": "plot", "evidence_from_figure": "X-axis limits to 32-37", "evidence_from_text": "Caption specifies 43-58 as the older group", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_4_350", "query": "In Figure 3, which county shows the highest DI fairness metric as per the text, and what specific bar chart features indicate this?", "answer": "Hartford County. The figure shows Hartford's orange (black) and green (Hispanic) bars significantly exceeding the blue (white) bar, indicating higher search rates for unprivileged groups. The text states Hartford's DI metric is higher than others.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg", "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.", "figure_type": "plot", "evidence_from_figure": "Hartford's orange and green bar heights relative to blue", "evidence_from_text": "Text states 'the ratio of search rates for the unprivileged groups... is higher than the same metric in Middlesex County and others'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_4_351", "query": "How does the bar chart for Middlesex County visually differ from Hartford County in terms of search rates, and what does the text say about their DI metrics?", "answer": "Middlesex has lower orange/green bars (black/Hispanic search rates) relative to blue (white), while Hartford shows the opposite. The text states Hartford's DI metric is higher than Middlesex's.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig3.jpg", "caption": "Figure 3. Feature bias localization in the Stanford Open Policing dataset for Connecticut, with county name as the feature and race as the protected attribute. In Hartford County, the ratio of search rates for the unprivileged groups (black and Hispanic) in proportion to the search rate for the privileged group (this ratio is the DI fairness metric) is higher than the same metric in Middlesex County and others. The approach would localize Hartford County.", "figure_type": "plot", "evidence_from_figure": "Middlesex's shorter orange/green bars compared to Hartford", "evidence_from_text": "Text states 'the ratio... is higher than the same metric in Middlesex County and others'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_7_352", "query": "In Figure 4, why do the orange bars for 'adult' dataset under 'sex race' attribute show DI values closer to 1 compared to blue bars?", "answer": "Pre-processing algorithms transform the original dataset to reduce bias, as explained in the text. The orange bars (after pre-processing) reflect improved fairness metrics, with DI values approaching the ideal target of 1.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg", "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ", "figure_type": "plot", "evidence_from_figure": "Orange bar for 'adult'/'sex race' showing DI ≈ 0.95", "evidence_from_text": "Text states: 'fair pre-processing algorithms transform the original dataset' and 'ideal fair value of DI is 1'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_7_353", "query": "What does the dark gray bar in Figure 4 represent, and how does it relate to the text's description of '±1 standard deviation'?", "answer": "The dark gray bars represent the range of ±1 standard deviation from the mean DI value. This aligns with the text's definition that these bars indicate 'the extent of ±1 standard deviation' for the metrics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig6.jpg", "caption": "Figure 4. Statistical Parity Difference (SPD) and Disparate Impact (DI) before (blue bar) and after (orange bar) applying pre-processing algorithms on various datasets for different protected attributes. The dark gray bars indicate the extent of $\\pm 1$ standard deviation. The ideal fair value of SPD is 0 and DI is 1.   ", "figure_type": "plot", "evidence_from_figure": "Dark gray bars adjacent to blue/orange bars", "evidence_from_text": "Caption states: 'The dark gray bars indicate the extent of ±1 standard deviation'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_10_354", "query": "Why do LR, Optimized pre-processing points in the top panel show disparate impact near 1 but lower balanced accuracy compared to the bottom panel?", "answer": "The top panel represents the scenario before bias mitigation, where achieving near-ideal fairness (disparate impact ≈1) comes at the cost of lower balanced accuracy. After mitigation (bottom panel), the model improves accuracy while maintaining fairness, as seen by the shift toward higher balanced accuracy while still approaching the ideal disparate impact of 1.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_10", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg", "caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.", "figure_type": "plot", "evidence_from_figure": "LR, Optimized pre-processing points in top panel have disparate impact near 1 but lower y-values (balanced accuracy) than bottom panel", "evidence_from_text": "Ideal fair value of disparate impact is 1; bias mitigation algorithms improve performance while maintaining fairness", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_10_355", "query": "According to the figure caption, what does the ideal value of 1 for disparate impact signify, and how does it relate to the bottom panel data points?", "answer": "The ideal value of 1 for disparate impact signifies perfect fairness (no bias), as defined in the caption. In the bottom panel, several data points (e.g., LR, Equal odds postprocessing) approach 1 on the x-axis, indicating that bias mitigation successfully improved fairness toward the ideal while maintaining balanced accuracy.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_10", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig9.jpg", "caption": "Figure 5. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Dataset: Adult, Protected attribute: race.", "figure_type": "plot", "evidence_from_figure": "LR, Equal odds postprocessing data point in bottom panel has disparate impact ≈1", "evidence_from_text": "Ideal fair value of disparate impact is 1", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_14_356", "query": "What does the 'Fair' threshold line at 1.0 indicate in Figure 6, and how does it relate to the mitigation process described in the text?", "answer": "The 'Fair' threshold line at 1.0 represents the benchmark for fairness in the Disparate Impact metric. The text explains that adversarial debiasing mitigation aims to improve fairness, as evidenced by the mitigated value (0.85) moving closer to this threshold than the original value (0.59).", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "evidence_from_figure": "Fair threshold line at 1.0", "evidence_from_text": "Context of Disparate Impact as a fairness metric and mitigation's purpose", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_14_357", "query": "How does the 'mitigated' bar in Figure 6 compare to the 'original' bar in terms of the Disparate Impact metric, as referenced in the text?", "answer": "The 'mitigated' bar (0.85) is higher than the 'original' bar (0.59), indicating an improvement in the Disparate Impact metric after adversarial debiasing mitigation, as described in the text.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig13.jpg", "caption": "Figure 6. Graphs from the interactive web experience showing one of the metrics, for one of the datasets, before and after mitigation.", "figure_type": "plot", "evidence_from_figure": "Values of original (0.59) and mitigated (0.85) bars", "evidence_from_text": "Mention of adversarial debiasing mitigation improving the metric", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_15_358", "query": "What is the role of GenericPreProcessing in the AIF360 pipeline as depicted in Figure 7?", "answer": "GenericPreProcessing serves as a placeholder for bias mitigation algorithms in the preprocessing stage, connecting to the Transformer class in the diagram. The text clarifies it is not an actual class but represents real algorithms implemented for bias mitigation.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "evidence_from_figure": "Connection between GenericPreProcessing and Transformer with '0..*' multiplicity", "evidence_from_text": "Text states: 'GenericPreProcessing... are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_15_359", "query": "How do metametrics like difference() function within the class hierarchy shown in Figure 7?", "answer": "Metametrics like difference() act on other metrics (e.g., DatasetMetric) to compute derived values such as 'true positive rate difference'. The figure shows DatasetMetric containing difference() as a method, while the text explains these are 'metametrics' that operate on other metrics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "evidence_from_figure": "DatasetMetric class containing 'difference()' method", "evidence_from_text": "Text states: 'Some methods are 'metametrics' — such as difference(), ratio(), total(), average() — that act on other metrics to get, e.g. true positive rate difference.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_15_360", "query": "Why are methods like memoize() and addmetadata not explicitly listed as members in the Metric class?", "answer": "The text explains these are Python decorator functions automatically applied to all functions in their respective classes. The figure omits them because the caption states: 'inherited members and methods are not shown (but overridden ones are)'.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_15", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig14.jpg", "caption": "Figure 7. Class abstractions for a fair machine learning pipeline, as implemented in AIF360. This figure is meant to provide a visual sense of the class hierarchy, many details and some methods are omitted. For brevity, inherited members and methods are not shown (but overridden ones are) nor are aliases such as recall() for true positive rate(). Some methods are “metametrics” — such as difference(), ratio(), total(), average(), maximum() — that act on other metrics to get, e.g. true positive rate difference(). The metric explainer classes use the same method signatures as the metric classes (not enumerated) but provide further description for the values. The GenericPreProcessing, GenericInProcessing, and GenericPostProcessing are not actual classes but serve as placeholders here for the real bias mitigation algorithms we implemented. Finally, memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.", "figure_type": "diagram", "evidence_from_figure": "Metric class showing only '-memoize()' with a diamond symbol", "evidence_from_text": "Text states: 'memoize and addmetadata are Python decorator functions that are automatically applied to every function in their respective classes.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_16_361", "query": "In Figure 8's bottom panel, the 'Equal opportunity difference' plot shows LR, Re-weighting has a mean value near 0. How does this relate to the text's explanation of equal opportunity as the difference in true positive rates?", "answer": "The mean value near 0 for LR, Re-weighting in the Equal opportunity difference plot indicates equal true positive rates between groups, aligning with the text's definition that 0 implies both groups have equal benefit.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_16", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg", "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ", "figure_type": "plot", "evidence_from_figure": "LR, Re-weighting data point in bottom panel's Equal opportunity difference plot", "evidence_from_text": "Text explaining 'equal opportunity as the difference in true positive rates' and 0 implies equal benefit", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_16_362", "query": "Why does the ideal fair value for Disparate Impact differ from other fairness metrics in Figure 8?", "answer": "Disparate impact's ideal value is 1 (as stated in the caption), whereas other metrics' ideal is 0. This difference exists because disparate impact measures a ratio (privileged/unprivileged) where 1 indicates no disparity, while other metrics measure differences (e.g., 0 for equal benefit).", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_16", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig15.jpg", "caption": "Figure 8. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: Adult, Protected attribute: sex.   ", "figure_type": "plot", "evidence_from_figure": "Disparate impact x-axis scale and caption's note on ideal values", "evidence_from_text": "Caption stating 'ideal fair value of disparate impact is 1, whereas for all other metrics it is 0'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_18_363", "query": "In Figure 10's bottom panel, which algorithm shows the highest mean balanced accuracy for the 'Disparate impact' metric, and why is its ideal value 1 according to the text?", "answer": "RF, Equal odds postprocessing shows the highest mean balanced accuracy (≈0.72). The text explains that disparate impact's ideal value is 1 because it measures proportional representation (e.g., 1:1 ratio of favorable outcomes across groups).", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "evidence_from_figure": "Bottom panel 'Disparate impact' subplot showing highest circle (mean) for RF, Equal odds postprocessing", "evidence_from_text": "Text states 'ideal fair value of disparate impact is 1'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_18_364", "query": "The text states the ideal value for 'Equal opportunity difference' is 0. In Figure 10's top panel, which algorithm's data points are closest to this ideal value?", "answer": "LR, Prejudice remover has data points clustered near 0 in the top panel's 'Equal opportunity difference' subplot, indicating minimal fairness violation.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_18", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig17.jpg", "caption": "Figure 10. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: german, Protected attribute: age.   ", "figure_type": "plot", "evidence_from_figure": "Top panel 'Equal opportunity difference' subplot showing LR, Prejudice remover data points centered at 0", "evidence_from_text": "Text specifies 'ideal value of 0 for all other metrics'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_20_365", "query": "In Figure 12, what is the approximate mean balanced accuracy for LR.Optimized pre-processing in the top panel (before mitigation) for the COMPAS dataset, and how does this relate to the accuracy change reported for the Adult census income dataset in the text after the figure?", "answer": "The top panel shows LR.Optimized pre-processing (orange) with a mean balanced accuracy of ~0.68. The text states Adult census income accuracy changed from 82% to 74%, but these are different datasets—COMPAS results cannot be directly compared to Adult's accuracy change.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_20", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg", "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.", "figure_type": "plot", "evidence_from_figure": "LR.Optimized pre-processing data point in top-left subplot (Statistical parity difference)", "evidence_from_text": "Text states 'Accuracy after mitigation changed from 82% to 74%' for Adult dataset", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.01943_1810.01943_fig_20_366", "query": "Which fairness metric in Figure 12 shows the largest deviation from the ideal value for LR.Optimized pre-processing in the COMPAS dataset, and how does this align with the text's claim about bias reduction for the Adult census income dataset?", "answer": "The Disparate impact metric (top-right subplot) shows the largest deviation (mean ~0.7 vs. ideal 1). The text states bias was reduced for 1 of 2 biased metrics in Adult, but 1 of 5 still showed bias—this indicates mitigation success varies across metrics.", "doc_id": "1810.01943", "figure_id": "1810.01943_fig_20", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.01943/1810.01943/hybrid_auto/images/1810.01943_page0_fig19.jpg", "caption": "Figure 12. Fairness vs. Balanced Accuracy before (top panel) and after (bottom panel) applying various bias mitigation algorithms. Four different fairness metrics are shown. In most cases two classifiers (Logistic regression - LR or Random forest classifier - RF) were used. The ideal fair value of disparate impact is 1, whereas for all other metrics it is 0. The circles indicate the mean value and bars indicate the extent of $\\pm 1$ standard deviation. Data set: compas, Protected attribute: race.", "figure_type": "plot", "evidence_from_figure": "LR.Optimized pre-processing (orange) data point in Disparate impact subplot", "evidence_from_text": "Text states 'Bias against unprivileged group was reduced to acceptable levels for 1 of 2 previously biased metrics (1 of 5 metrics still indicate bias)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_1_367", "query": "What does the N value (1412846) in Figure 1 represent, and how does it relate to the corpora described in the text?", "answer": "The N value (1412846) represents the total number of documents in the NYT corpus used for this experiment. The text specifies that the NYT setup is one of two corpora analyzed, with the other being a Wikipedia dump.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg", "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.", "figure_type": "plot", "evidence_from_figure": "N value in the statistics box (1412846)", "evidence_from_text": "Text mentions 'two corpora' and identifies the NYT setup as one of them", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_1_368", "query": "Why are documents in the tails of the histogram in Figure 1 used for constructing targeted perturbation sets?", "answer": "Documents in the tails are used because they correspond to the greatest differential bias of removal, as stated in the text: 'the documents whose removals were predicted (by our method) to cause the greatest differential bias'.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig0.jpg", "caption": "Figure 1. Histogram of the approximated differential bias of removal for every document in our NYT setup, considering WEAT1, measured in percent change from the baseline mean.", "figure_type": "plot", "evidence_from_figure": "Histogram tails (extreme left/right values)", "evidence_from_text": "Text explicitly links tails to 'greatest differential bias' for perturbation sets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_2_369", "query": "What does the vertical dotted line in Figure 2 represent according to the caption, and where is it positioned on the x-axis?", "answer": "The vertical dotted line represents the baseline (unperturbed) mean validated effect size, positioned at x=0.5 on the x-axis.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg", "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.", "figure_type": "plot", "evidence_from_figure": "vertical dotted line at x=0.5", "evidence_from_text": "caption states 'the baseline (unperturbed) mean is shown with a vertical dotted line'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_2_370", "query": "How does the alignment of data points with the red dashed line in Figure 2 support the claim that the method accurately approximates the impact of document removal compared to the PPMI baseline in Section 5.3?", "answer": "The data points closely align with the red dashed line (y=x), indicating high correlation between approximated and validated effect sizes, which supports the claim of accurate approximation compared to the PPMI baseline.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig1.jpg", "caption": "Figure 2. Approximated and ground truth WEAT bias effect size due to the removal of various perturbation sets for our NYT corpus, considering WEAT1. Each point describes the mean effect size of one set; error bars depict one standard deviation; the baseline (unperturbed) mean is shown with a vertical dotted line.", "figure_type": "plot", "evidence_from_figure": "data points near red dashed line", "evidence_from_text": "Section 5.3 discusses comparison to the PPMI baseline", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_3_371", "query": "In Figure 3, the 'decrease-1000' perturbation set shows WEAT effect size values predominantly below 0. How does this relate to the text's statement that '0.7% of articles can reverse the WEAT effect size in the New York Times'?", "answer": "The negative WEAT effect size values for 'decrease-1000' indicate reversal of bias direction, which corresponds to the text's claim that 0.7% of articles in this perturbation set reverse the effect.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg", "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)", "figure_type": "plot", "evidence_from_figure": "Negative WEAT effect size values for 'decrease-1000' perturbation set", "evidence_from_text": "Text stating '0.7% of articles can reverse the WEAT effect size in the New York Times'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_3_372", "query": "How does the alignment of 'approx. mean' (blue squares) with 'gnd. truth mean' (red diamonds) in Figure 3 support the text's assertion that the method 'accurately approximates the impact of document removal'?", "answer": "The close alignment of blue squares and red diamonds across all perturbation sets demonstrates the method's accuracy in approximating the true effect size, as stated in the text.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig2.jpg", "caption": "Figure 3. Approximated and ground truth differential bias of removal for every perturbation set. Results for different perturbation sets arranged vertically, named as type - size (number of documents removed). (NYT - WEAT1)", "figure_type": "plot", "evidence_from_figure": "Consistent positioning of blue squares and red diamonds for each perturbation set", "evidence_from_text": "Text claiming the method 'accurately approximates the impact of document removal'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_4_373", "query": "In Figure 4, what is the WEAT effect size for the word2vec model when the perturbation set is 'decrease-3000'?", "answer": "The WEAT effect size for word2vec at 'decrease-3000' is approximately 0.5. This is observed from the red circle data point at the corresponding y-axis label, as described in the text where word2vec embeddings are trained with comparable hyperparameters.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg", "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)", "figure_type": "plot", "evidence_from_figure": "Red circle data point at 'decrease-3000' y-axis label", "evidence_from_text": "Text states word2vec embeddings were trained with comparable hyperparameters for perturbation sets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_4_374", "query": "Why does the GloVe model show a larger WEAT effect size than word2vec for the 'increase-10000' perturbation set in Figure 4?", "answer": "The text explains that influential documents identified by the method have a strong impact on GloVe embeddings. In Figure 4, the black square (GloVe) at 'increase-10000' shows a higher WEAT effect size (≈1.8) compared to word2vec's red circle (≈1.2), indicating GloVe is more sensitive to these documents' removal.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig3.jpg", "caption": "Figure 4. The effects of removing the different perturbation sets (most impactful documents as identified by our method) on the WEAT bias in: our GloVe embeddings, the PPMI representation, and word2vec embeddings with comparable hyper-parameters; error bars represent one standard deviation. (NYT - WEAT1)", "figure_type": "plot", "evidence_from_figure": "GloVe (black square) vs. word2vec (red circle) positions at 'increase-10000'", "evidence_from_text": "Text states 'the documents identified as influential... have a strong impact on the WEAT effect size in GloVe embeddings'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_7_375", "query": "What is the mean differential bias (μ) for the Wiki setup (WEAT1) as shown in Figure 6?", "answer": "-0.00236", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "evidence_from_figure": "Statistics box showing μ: -0.00236", "evidence_from_text": "Caption specifies 'Wiki setup (top)' and 'WEAT1 (left)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_7_376", "query": "How many documents were analyzed in the Wiki setup (WEAT1) according to Figure 6?", "answer": "29,344", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "evidence_from_figure": "Statistics box showing N: 29344", "evidence_from_text": "Caption identifies the top panel as 'Wiki setup'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_7_377", "query": "What does the standard deviation (σ) of 0.96661 indicate about the differential bias distribution in the Wiki setup (WEAT1) in Figure 6?", "answer": "The data has moderate spread around the mean, with most values concentrated near 0%.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_7", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig6.jpg", "caption": "Figure 6. Histogram of the approximated differential bias of removal for every document in our Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right), measured in percent change from the corresponding mean baseline bias.", "figure_type": "plot", "evidence_from_figure": "Statistics box showing σ: 0.96661", "evidence_from_text": "Caption confirms the setup context (Wiki/WEAT1)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_13_378", "query": "What does the vertical dotted line in Figure 7 represent according to the text?", "answer": "The vertical dotted line represents the baseline means, as specified in the figure caption.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_13", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg", "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.", "figure_type": "plot", "evidence_from_figure": "Vertical dotted line on the plot", "evidence_from_text": "Caption states: 'the baseline means are shown with vertical dotted lines'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_13_379", "query": "How does the correlation between approximated and ground truth effect sizes relate to Table 4?", "answer": "The correlation is visualized via the least squares line in Figure 7 and quantified in Table 4, as referenced in the figure caption.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_13", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg", "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.", "figure_type": "plot", "evidence_from_figure": "Red dashed line (least squares fit) showing correlation trend", "evidence_from_text": "Caption states: 'correlations in Table 4'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_13_380", "query": "What methodology does the figure illustrate regarding perturbation set removal?", "answer": "The figure illustrates the effect of removing non-random perturbation sets on WEAT bias effect size, comparing approximated vs. ground truth values as described in the caption.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_13", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig12.jpg", "caption": "Figure 7. Approximated vs. ground truth WEAT bias effect size due to the removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); points plot the means; error bars depict one standard deviation; dashed line shows least squares; the baseline means are shown with vertical dotted lines; correlations in Table 4.", "figure_type": "plot", "evidence_from_figure": "Scatter plot showing approximated vs. ground truth effect sizes", "evidence_from_text": "Caption states: 'removal of each (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_15_381", "query": "In Figure 8, what is the value of the baseline mean on the WEAT effect size axis?", "answer": "0.0", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_15", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg", "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines", "figure_type": "plot", "evidence_from_figure": "Vertical dotted line positioned at x=0.0", "evidence_from_text": "Caption states 'baseline means are shown with vertical dotted lines'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_15_382", "query": "For the 'increase-300' category, how do the 'approx. mean' (blue diamonds) and 'gnd. truth mean' (red diamonds) compare in WEAT effect size?", "answer": "The 'approx. mean' is lower than the 'gnd. truth mean' (approx. mean ≈ 0.2, gnd. truth mean ≈ 0.5)", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_15", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig14.jpg", "caption": "Figure 8. Approximated and ground truth differential bias of removal for every (non-random) perturbation set in Wiki setup (top) and NYT setup (bottom), considering WEAT1 (left) and WEAT2 (right); the baseline means are shown with vertical dotted lines", "figure_type": "plot", "evidence_from_figure": "Blue diamonds at ~0.2, red diamonds at ~0.5 for increase-300", "evidence_from_text": "Caption explains comparison between approximation and ground truth for differential bias of removal", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_18_383", "query": "What is the r² value for the correlation between GloVe WEAT and PPMI WEAT in Figure 9's left plot?", "answer": "0.725", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_18", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg", "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).", "figure_type": "plot", "evidence_from_figure": "Scatter plot showing the relationship between the two WEAT metrics", "evidence_from_text": "Caption stating 'r² = 0.725 (left)' for GloVe vs PPMI correlation", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03611_1810.03611_fig_18_384", "query": "How does the correlation strength differ between GloVe-PPMI (r²=0.725) and word2vec (r²=0.803) in Figure 9?", "answer": "The word2vec embedding correlation (r²=0.803) is stronger than GloVe-PPMI (r²=0.725), suggesting better alignment between WEAT scores and word2vec embeddings under comparable hyper-parameters.", "doc_id": "1810.03611", "figure_id": "1810.03611_fig_18", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03611/1810.03611/hybrid_auto/images/1810.03611_page0_fig17.jpg", "caption": "Figure 9. The correlation of the WEAT as measured in our NYT GloVe embeddings versus the corpus’ PPMI representation in 2000 randomly generated word sets, $r ^ { 2 } = 0 . 7 2 5$ (left); versus when measured in word2vec embeddings with comparable hyper-parameters, $r ^ { 2 } = 0 . 8 0 { \\dot { 3 } }$ (right).", "figure_type": "plot", "evidence_from_figure": "Left plot showing GloVe-PPMI scatter distribution", "evidence_from_text": "Caption comparing r² values for GloVe vs PPMI (0.725) and word2vec (0.803)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_1_385", "query": "In Figure 1, which demographic group has the highest false positive rate at 0.5, and how does the ethical consideration about using public figures as data sources relate to this finding?", "answer": "The old-male group has the highest FPR (~0.09). The ethical consideration states that data is based on public figures (celebrities), which may underrepresent older males in real-world populations, potentially explaining the elevated FPR for this group.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "old-male's dot is the rightmost, indicating highest FPR", "evidence_from_text": "Text states: 'Faces and annotations based on public figures (celebrities). No new information is inferred or annotated.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_1_386", "query": "Why is the 0.5 threshold specified in Figure 1, and how does the 'Caveats and Recommendations' section address potential issues with this threshold?", "answer": "The 0.5 threshold is the classification boundary for binary decisions. The 'Caveats and Recommendations' section likely discusses how fixed thresholds can amplify disparities across demographics, as seen in the varying FPRs (e.g., old-male vs. female).", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Title specifies '@ 0.5' and shows FPR variation across groups", "evidence_from_text": "Section title 'Caveats and Recommendations' implies discussion of methodological limitations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_2_387", "query": "In Figure 2, which demographic group has the highest False Negative Rate, and what does the text state about the model's limitations regarding gender classification?", "answer": "The 'old-male' group has the highest FNR (~0.10). The text states the model uses binary gender classes (male/female) and notes further work is needed to evaluate across a gender spectrum.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg", "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.", "figure_type": "plot", "evidence_from_figure": "old-male group's FNR peak (~0.10)", "evidence_from_text": "text states 'gender classes are binary... Further work needed to evaluate across a spectrum of genders'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_2_388", "query": "Why does the False Negative Rate for the 'old' group exceed that of the 'young' group in Figure 2, and how does the text explain the model's limitations related to demographic factors?", "answer": "The 'old' group shows higher FNR than 'young' (e.g., ~0.08 vs ~0.06). The text identifies race/skin type as a source of disproportionate errors, which may underlie these disparities.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg", "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.", "figure_type": "plot", "evidence_from_figure": "FNR comparison between 'old' and 'young' groups", "evidence_from_text": "text states 'Does not capture race or skin type, which has been reported as a source of disproportionate errors'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_2_389", "query": "What does the text identify as a source of disproportionate errors in the model, and how does Figure 2 visually represent this?", "answer": "The text identifies race/skin type as a source of errors. Figure 2 shows varying FNR across groups (e.g., old-male vs young-female), illustrating disparities likely linked to unaccounted factors like race/skin type.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig1.jpg", "caption": "Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset.", "figure_type": "plot", "evidence_from_figure": "disparate FNR values across demographic groups", "evidence_from_text": "text states 'Does not capture race or skin type, which has been reported as a source of disproportionate errors'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_6_390", "query": "Which unitary group has the highest AUC in Version 5 of the Perspective API's toxicity detector (Figure 3), and how does the text describe the significance of this metric for model transparency?", "answer": "The 'lesbian' group has the highest AUC (~0.99). The text states that model cards like Figure 3 provide quantitative analyses to ensure transparency in performance metrics across demographic groups.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg", "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.", "figure_type": "plot", "evidence_from_figure": "Highest bar for 'lesbian' group", "evidence_from_text": "Text states model cards include quantitative analyses for transparency", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.03993_1810.03993_fig_6_391", "query": "What is the AUC for the 'white' group in Version 5 (Figure 3), and how does the text explain the implications of performance disparities across unitary groups?", "answer": "The 'white' group has an AUC of ~0.97. The text indicates that such disparities in performance metrics highlight potential biases requiring mitigation in the toxicity detection model.", "doc_id": "1810.03993", "figure_id": "1810.03993_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.03993/1810.03993/hybrid_auto/images/1810.03993_page0_fig5.jpg", "caption": "Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector.", "figure_type": "plot", "evidence_from_figure": "Bar height for 'white' group", "evidence_from_text": "Text references model card context for bias analysis", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.08683_1810.08683_fig_1_392", "query": "In Figure 2, what do the binary labels (e.g., 0100, 0000) near the data points correspond to, as referenced in the caption?", "answer": "The binary labels correspond to the text labels P, D, F, and S mentioned in the caption, representing different model configurations for the Gender feature.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg", "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "evidence_from_figure": "Binary strings (e.g., 0100, 0000) near data points", "evidence_from_text": "Caption stating 'text close to the symbols in plot are P, D, F, and S'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.08683_1810.08683_fig_1_393", "query": "How does the MTL model (red circles) perform on the Gender feature in terms of Normalized EoD and Averaged Group Error?", "answer": "The MTL model shows varying Normalized EoD values across configurations, with some points achieving lower Averaged Group Error (e.g., 0000) while others have higher error (e.g., 1010), indicating trade-offs between fairness and accuracy.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig0.jpg", "caption": "Figure 2: Adult dataset: complete results set for Gender (text close to the symbols in plot are P, D, F, and S).", "figure_type": "plot", "evidence_from_figure": "Red circles' positions on axes", "evidence_from_text": "Context that Gender is a sensitive feature in the Adult dataset", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.08683_1810.08683_fig_4_394", "query": "In Figure 5, how does EOd change as λ increases in the Gender - D=1 subplot, and what does the text imply about the trade-off between accuracy and fairness?", "answer": "EOd increases as λ increases from 0 to 1 in the Gender - D=1 subplot. The text implies this trade-off occurs because higher λ values (model complexity) improve accuracy but reduce fairness, as the model becomes more sensitive to the sensitive feature.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_4", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg", "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .", "figure_type": "plot", "evidence_from_figure": "Bottom subplot's orange line (EOd) trends upward with increasing λ", "evidence_from_text": "Text states 'tension between accuracy gains... and potential inapplicability' and discusses fairness constraints", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1810.08683_1810.08683_fig_4_395", "query": "What does the parameter S=0 in the figure caption correspond to in the legend, and how does the text explain this configuration?", "answer": "S=0 corresponds to the 'f0' entries in the top subplot's legend (Gender - D=0). The text explains this as a configuration where the sensitive feature is not used in the model, resolving tension between accuracy gains and applicability concerns.", "doc_id": "1810.08683", "figure_id": "1810.08683_fig_4", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1810.08683/1810.08683/hybrid_auto/images/1810.08683_page0_fig3.jpg", "caption": "Figure 5: Adult Dataset: ACC and EOd of MTL, when we fix $\\theta$ and $\\rho$ to be the best values found during the validation procedure and we vary  with $P { = } 0$ , $F { = } 1$ , and $S { = } 0$ .", "figure_type": "plot", "evidence_from_figure": "Top subplot legend shows 'ACC f0' and 'EOd f0'", "evidence_from_text": "Text states 'we have suggested to first predict the sensitive feature... and then use the predicted value' when S=0", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_1_396", "query": "In Figure X, which population (male or female) has higher reconstruction error on the LFW dataset, and how does this align with the text's example of women versus men?", "answer": "Males have higher reconstruction error. This aligns with the text's example where population A (men) exhibits higher error than population B (women), supporting the study's claim about PCA's differential fidelity across populations.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Male RE PCA line is consistently above Female RE PCA line", "evidence_from_text": "Text states 'PCA has higher reconstruction error on population A than on B (for example, women versus men)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_1_397", "query": "How does the figure's trend support the text's claim that PCA can produce different fidelity for different populations?", "answer": "The figure shows that Male RE PCA consistently exceeds Female RE PCA across all feature counts, demonstrating that PCA's reconstruction error differs between genders. This supports the text's assertion that PCA can inadvertently create unequal fidelity for different populations.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Both lines decrease with features but Male line remains higher", "evidence_from_text": "Text states 'PCA has higher reconstruction error on population A than on B (for example, women versus men)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_2_398", "query": "At 2.5 features, what is the difference in reconstruction error between male and female PCA on LFW according to Figure 1 left, and how does this relate to the text's mention of a 'noticeable gap' at lower dimensions?", "answer": "At 2.5 features, male error is ~32 and female error is ~30, a 2-point gap. This aligns with the text's claim of a noticeable gap at lower dimensions.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg", "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).", "figure_type": "plot", "evidence_from_figure": "Data points at 2.5 features (male ~32, female ~30)", "evidence_from_text": "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_2_399", "query": "How does the figure illustrate the text's statement that 'at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women'?", "answer": "The figure shows the largest vertical separation between the male and female lines at low feature counts (e.g., 2.5), with the gap narrowing as features increase, matching the text's description.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig1.jpg", "caption": "Figure 1: Left: Average reconstruction error of PCA on labeled faces in the wild data set (LFW), separated by gender. Right: The same, but sampling 1000 faces with men and women equiprobably (mean over 20 samples).", "figure_type": "plot", "evidence_from_figure": "Widest gap between lines at leftmost part of plot (low feature counts)", "evidence_from_text": "As we already saw in Figure 1 left, at lower dimensions, there is a noticeable gap between PCA’s average reconstruction error for men and women", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_6_400", "query": "What is the reconstruction error for Female RE Fair PCA at 2.5 features in Figure 3, and how does this relate to Theorem 5.2's claim about optimal fairness objectives?", "answer": "The reconstruction error is approximately 30. This aligns with Theorem 5.2, which states a polynomial-time algorithm exists to find a projection achieving optimal fairness objectives, explaining why Fair PCA reduces error compared to vanilla PCA.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg", "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "evidence_from_figure": "Female RE Fair PCA line at x=2.5 features", "evidence_from_text": "Theorem 5.2 on polynomial-time algorithm for optimal fairness", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_6_401", "query": "Why do the Male and Female RE Fair PCA lines in Figure 3 overlap almost perfectly, and how does this relate to the text's statement about two-group fairness?", "answer": "The lines overlap because for two groups (Male/Female), Fair PCA can assign equal loss to all groups (as stated in the text's contrast with multi-group cases), resulting in similar reconstruction errors.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_6", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig5.jpg", "caption": "Figure 3: Reconstruction error of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "evidence_from_figure": "Male and Female RE Fair PCA lines' near-identical trajectory", "evidence_from_text": "Text stating 'In contrast to the case of two groups, when there are more than two groups...'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_8_402", "query": "In Figure 4, why does the 'Fair loss' curve (green) have a higher loss than the 'Male loss PCA' curve (purple) at all feature counts, and how does this relate to the proof of Lemma 4.8?", "answer": "The Fair loss curve is higher than Male loss PCA due to fairness constraints in Fair PCA that balance loss across groups. Lemma 4.8 proves that Fair PCA's objective function $g_A$ achieves its global minimum at the top $d$ eigenvectors of $A^T A$, but fairness constraints prevent it from reaching the lower loss of standard Male PCA.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_8", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg", "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "evidence_from_figure": "Green line is consistently above purple line across all x-values", "evidence_from_text": "Lemma 4.8 discusses $g_A$'s minima and its relation to eigenvectors of $A^T A$", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.00103_1811.00103_fig_8_403", "query": "At 15 features in Figure 4, what is the difference in loss between 'Female loss PCA' (blue) and 'Fair loss' (green), and how does this support the effectiveness of Fair PCA?", "answer": "At 15 features, Female loss PCA is ~0.55 while Fair loss is ~0.22, a difference of ~0.33. This supports Fair PCA's effectiveness because the text states that Fair PCA minimizes loss while satisfying fairness constraints, reducing the disparity between male and female losses.", "doc_id": "1811.00103", "figure_id": "1811.00103_fig_8", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.00103/1811.00103/hybrid_auto/images/1811.00103_page0_fig7.jpg", "caption": "Figure 4: Loss of PCA/Fair PCA on LFW and the Default Credit data set.", "figure_type": "plot", "evidence_from_figure": "Blue line at 15 features ≈0.55, green line ≈0.22", "evidence_from_text": "Lemma 4.8's proof of $g_A$'s optimization under fairness constraints", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_1_404", "query": "What statistical significance level does the '***' symbol indicate for the 'Ratio' vs 'Equal' comparison in Treatment 2 of Figure 1?", "answer": "p < 0.001, as defined in the figure caption.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg", "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .", "figure_type": "plot", "evidence_from_figure": "The '***' marker above the Ratio-Equal comparison in Treatment 2", "evidence_from_text": "Figure caption defining *** as p < 0.001", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_1_405", "query": "Why was H1A supported in all treatments according to the text, based on Figure 1's data for Treatment 3?", "answer": "Participants rated 'Ratio' as more fair than 'Equal' (6.91 vs 4.69) with *** significance (p < 0.001), aligning with H1A's prediction.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig0.jpg", "caption": "Figure 1: Comparison of means (with $9 5 \\%$ CI) for Study 1. Where * signifies p $< 0 . 0 5$ , $^ { \\ast \\ast } \\mathrm { p } < 0 . 0 1$ , and ** $\\mathfrak { i } \\mathrm { p } < 0 . 0 0 1$ .", "figure_type": "plot", "evidence_from_figure": "Mean ratings and *** marker for Ratio vs Equal in Treatment 3", "evidence_from_text": "Text stating H1A was supported due to Ratio being rated more fair than Equal", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_2_406", "query": "Why is the 'Equal' decision rated as more fair than the 'All A' decision in Treatment 2 only when the candidate with the higher repayment rate was white?", "answer": "The text specifies this finding occurs only when the higher repayment rate candidate is white, while Figure 2 shows a significant difference (***, p < 0.001) between 'Equal' and 'All A' in Treatment 2, consistent with the text's race-specific condition.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg", "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "evidence_from_figure": "Significant difference (***, p < 0.001) between 'Equal' and 'All A' in Treatment 2", "evidence_from_text": "Text states 'Participants also rated the 'Equal' decision as more fair than the 'All A' decision in Treatment 2, but only when the candidate with the higher repayment rate was white'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_2_407", "query": "What does the *** symbol indicate in Figure 2 for the 'Ratio' vs 'Equal' comparison in Treatment 3?", "answer": "The *** symbol indicates p < 0.001, as defined in the figure caption, and Figure 2 shows this significance for the 'Ratio' vs 'Equal' comparison in Treatment 3.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig1.jpg", "caption": "Figure 2: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is white). Where * signifies p $< 0 . 0 5$ , ** p $< 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "evidence_from_figure": "Presence of *** above 'Ratio' vs 'Equal' in Treatment 3", "evidence_from_text": "Figure caption states: '*** p < 0.001'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_3_408", "query": "In Figure 3, what is the fairness rating difference between 'Ratio' and 'Equal' in Treatment 2, and what does the *** marker signify according to the text?", "answer": "The fairness rating for 'Ratio' is 6.00 and for 'Equal' is 5.29 in Treatment 2, with a difference of 0.71. The *** marker signifies p < 0.001, supporting H1A as stated in the text where 'Ratio' was viewed as more fair than 'Equal' in Treatments 2-4.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg", "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "evidence_from_figure": "Mean values for Ratio (6.00) and Equal (5.29) in Treatment 2 with *** marker", "evidence_from_text": "Text states 'participants viewed the “Ratio” decision as more fair than the “Equal” decision in Treatments 2, 3, and 4... in support of H1A'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_3_409", "query": "Why did 'All A' receive higher fairness ratings than 'Equal' in Treatment 4 when the higher repayment rate candidate was Black, as shown in Figure 3?", "answer": "In Treatment 4, 'All A' had a rating of 5.44 vs. 4.68 for 'Equal' (p < 0.05). The text explains this occurred because 'when the difference between repayment rates was larger (Treatments 3 and 4), participants viewed “All A” as more fair than “Equal” but only when the candidate with the higher repayment rate was Black'.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig2.jpg", "caption": "Figure 3: Comparison of means (with $9 5 \\%$ CI) for Study 2 (when the individual with the higher loan repayment rate is black). Where * signifies p $< 0 . 0 5$ , $^ { * * } \\mathrm {  ~ p ~ } { < } 0 . 0 1$ , and $\\ast \\ast \\ast _ { \\mathrm { ~ p ~ } }$ ${ < } 0 . 0 0 1$ .", "figure_type": "plot", "evidence_from_figure": "Mean values for All A (5.44) and Equal (4.68) in Treatment 4 with * marker", "evidence_from_text": "Text states 'When the difference... was larger (Treatments 3 and 4), participants viewed “All A”... as more fair than “Equal” but only when the candidate with the higher repayment rate was Black'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_4_410", "query": "What percentage of participants in Study 1 were under 30 years old, and does the text specify any age restrictions for eligibility?", "answer": "29% (1% Under 20 + 28% 20-29). The text states eligibility was restricted to U.S.-based MTurkers only, with no age restrictions mentioned.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "Under 20 (1%) and 20-29 (28%) segments", "evidence_from_text": "Eligibility criteria: 'located in the United States of America' and 'MTurker could only participate in one of the two studies'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_4_411", "query": "Which age group has the highest percentage in Figure 6, and how does this relate to the study's participant selection process described in the text?", "answer": "30-39 (30%). The text specifies that participants were selected via MTurk based on U.S. location only, with no age-based criteria applied.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_4", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig3.jpg", "caption": "Figure 6: Age distribution of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "30-39 segment (30%)", "evidence_from_text": "Eligibility criteria: 'Amazon Mechanical Turk workers had to be located in the United States of America'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_6_412", "query": "What percentage of participants in Study 1 were female according to Figure 8, and how does the 'Education' section (text before figure) contextualize this distribution?", "answer": "56% female; the Education section indicates the study was conducted in an educational context, which may explain the higher female representation.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_6", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg", "caption": "Figure 8: Gender breakdown of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "Female segment labeled 56%", "evidence_from_text": "Text before figure: [Section: Education]", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_6_413", "query": "In Figure 8, the male participants account for 44% of the sample. How does the 'Education' section (text before figure) relate to this gender distribution?", "answer": "The Education section contextualizes the study within educational settings, where gender imbalances in participation may occur due to field-specific recruitment practices.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_6", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig5.jpg", "caption": "Figure 8: Gender breakdown of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "Male segment labeled 44%", "evidence_from_text": "Text before figure: [Section: Education]", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_7_414", "query": "What percentage of participants in Study 1 identified as Independent according to Figure 9?", "answer": "33%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_7", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig6.jpg", "caption": "Figure 9: Political Affiliation of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "The 'Independent' segment labeled with 33%", "evidence_from_text": "Text before figure confirms 'Study 1' context", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_7_415", "query": "What is the total percentage of participants who identified with either the Democratic or Republican Party in Study 1 according to Figure 9?", "answer": "62%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_7", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig6.jpg", "caption": "Figure 9: Political Affiliation of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "Democratic Party (34%) + Republican Party (28%)", "evidence_from_text": "Text confirms 'Study 1' context for the data", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_8_416", "query": "Which state had the highest participant count in Study 1 according to Figure 11, and what does the Political Affiliation section state about the recruitment strategy?", "answer": "Pennsylvania (PA) had the highest count (24 participants), and the Political Affiliation section indicates participants were recruited via university networks, explaining higher representation in states with major academic institutions.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_8", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg", "caption": "Figure 11: Breakup by state of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "PA bar is the tallest (24 participants)", "evidence_from_text": "Text states Study 1 used university-based recruitment", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_8_417", "query": "How does the participant distribution in Figure 11 align with the political leanings discussed in the Political Affiliation section?", "answer": "The distribution aligns with the section's claim that Democratic-leaning states (e.g., PA) had higher participation due to targeted outreach in politically active regions.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_8", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig7.jpg", "caption": "Figure 11: Breakup by state of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "PA (24 participants) is the most represented state", "evidence_from_text": "Political Affiliation section links state political leanings to recruitment outcomes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_9_418", "query": "What percentage of participants identified as White in Figure 10, and how does this relate to the 'State breakup' mentioned in the text?", "answer": "82% of participants identified as White in Figure 10. The text indicates that Figure 11 (state breakup) follows this racial breakdown, suggesting the race data serves as a demographic baseline before analyzing state-level distributions.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_9", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig8.jpg", "caption": "Figure 10: Race of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "White segment labeled 82%", "evidence_from_text": "Text states 'State breakup Figure 11: Breakup by state of the participants in Study 1'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_9_419", "query": "According to Figure 10, what is the percentage of participants in the 'American Indian' category, and why might this category be included in the study's demographic analysis?", "answer": "0% of participants identified as American Indian in Figure 10. The text's reference to Figure 11 (state breakup) implies the study aims for comprehensive demographic representation, where even categories with minimal representation are documented for methodological transparency.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_9", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig8.jpg", "caption": "Figure 10: Race of the participants in Study 1.", "figure_type": "plot", "evidence_from_figure": "American Indian segment labeled 0%", "evidence_from_text": "Text describes Figure 11 as part of the study's 'breakup' structure, indicating systematic demographic reporting", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_10_420", "query": "What percentage of participants in Study 1 lived in suburban communities according to Figure 12?", "answer": "49%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_10", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg", "caption": "Figure 12: Residential breakdown of the participants in Study 1.", "figure_type": "diagram", "evidence_from_figure": "The 'Suburban comm.' segment shows 49%", "evidence_from_text": "Text confirms the figure relates to Study 1", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_10_421", "query": "What is the combined percentage of urban and rural community participants in Study 1 based on Figure 12?", "answer": "51%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_10", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg", "caption": "Figure 12: Residential breakdown of the participants in Study 1.", "figure_type": "diagram", "evidence_from_figure": "City or urban (26%) + Rural community (25%)", "evidence_from_text": "Text identifies the figure as part of Study 1", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_10_422", "query": "Which residential category had the lowest percentage of participants in Study 1 per Figure 12?", "answer": "Rural community", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_10", "figure_number": 12, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig9.jpg", "caption": "Figure 12: Residential breakdown of the participants in Study 1.", "figure_type": "diagram", "evidence_from_figure": "Rural community segment shows 25%", "evidence_from_text": "Text confirms the study context", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_12_423", "query": "What percentage of participants have a 'Bachelor's deg.' in Figure 13, and how does the text describe the purpose of Study 2?", "answer": "37%. The text states Study 2 focuses on demographic information of participants, with Figure 13 (labeled as age distribution) unexpectedly showing education levels, while Figure 14 explicitly addresses education.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_12", "figure_number": 13, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg", "caption": "Figure 13: Age distribution of the participants in Study 2.   ", "figure_type": "diagram", "evidence_from_figure": "Bachelor's deg. segment (37%)", "evidence_from_text": "Text before figure: 'Study 2: Demographic information of the participants Age'; Text after figure: 'Education Figure 14: Education of the participants in Study 2.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_12_424", "query": "What is the smallest education category in Figure 13, and what does the text indicate about the relationship between Figures 13 and 14?", "answer": "No high sch. deg. (0%). The text implies Figure 13 (labeled as age distribution) contains education data, while Figure 14 explicitly focuses on education, suggesting inconsistent labeling or a shift in demographic focus.", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_12", "figure_number": 13, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig11.jpg", "caption": "Figure 13: Age distribution of the participants in Study 2.   ", "figure_type": "diagram", "evidence_from_figure": "No high sch. deg. segment (0%)", "evidence_from_text": "Text after figure: 'Education Figure 14: Education of the participants in Study 2.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_13_425", "query": "What percentage of participants in Study 2 identified as female according to Figure 15?", "answer": "58%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_13", "figure_number": 15, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig12.jpg", "caption": "Figure 15: Gender breakdown of the participants in Study 2.", "figure_type": "plot", "evidence_from_figure": "The 'Female' segment labeled with 58%", "evidence_from_text": "The text confirms the figure pertains to Study 2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_13_426", "query": "What is the combined percentage of male and female participants in Study 2 as shown in Figure 15?", "answer": "100%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_13", "figure_number": 15, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig12.jpg", "caption": "Figure 15: Gender breakdown of the participants in Study 2.", "figure_type": "plot", "evidence_from_figure": "Male (42%) + Female (58%) = 100%", "evidence_from_text": "The text specifies the data relates to Study 2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_14_427", "query": "What percentage of participants in Study 2 identified as Democratic Party according to Figure 16?", "answer": "43%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_14", "figure_number": 16, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg", "caption": "Figure 16: Political Affiliation of the participants in Study 2.", "figure_type": "plot", "evidence_from_figure": "The Democratic Party segment is labeled 43%", "evidence_from_text": "The caption confirms the data pertains to Study 2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_14_428", "query": "What is the combined percentage of participants identifying as Republican Party or Independent in Study 2?", "answer": "51%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_14", "figure_number": 16, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig13.jpg", "caption": "Figure 16: Political Affiliation of the participants in Study 2.", "figure_type": "plot", "evidence_from_figure": "Republican Party (26%) + Independent (25%)", "evidence_from_text": "The text confirms this data is from Study 2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_15_429", "query": "What percentage of participants identified as White in Study 2 according to Figure 17 and the Race section?", "answer": "74%", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_15", "figure_number": 17, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg", "caption": "Figure 17: Race of the participants in Study 2.   ", "figure_type": "plot", "evidence_from_figure": "White segment labeled 74%", "evidence_from_text": "Text after figure is [Section: Race], confirming the figure's focus on race", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.03654_1811.03654_fig_15_430", "query": "Which racial group had the lowest percentage of participants in Study 2, as shown in Figure 17 and the Race section?", "answer": "Native Hawaiian and American Indian (0%)", "doc_id": "1811.03654", "figure_id": "1811.03654_fig_15", "figure_number": 17, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.03654/1811.03654/hybrid_auto/images/1811.03654_page0_fig14.jpg", "caption": "Figure 17: Race of the participants in Study 2.   ", "figure_type": "plot", "evidence_from_figure": "Native Hawaiian and American Indian segments both labeled 0%", "evidence_from_text": "Text after figure is [Section: Race], confirming the figure's topic", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.10104_1811.10104_fig_1_431", "query": "What do the shaded regions in Figure 1 represent in Petersen and Novick's fairness criteria as discussed in Section 2.1?", "answer": "The shaded regions represent areas where the marginal distributions of test scores and ground truth scores differ between subgroups π₁ and π₂, indicating potential unfairness in test outcomes.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg", "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.", "figure_type": "diagram", "evidence_from_figure": "Shaded regions near π₁ and π₂ curves on both axes", "evidence_from_text": "Section 2.1 mentions 'fairness of tests for black and white students' and Cleary's work on test bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.10104_1811.10104_fig_1_432", "query": "How does the relationship between x* and y* in Figure 1 relate to the shift from unfairness to fairness described in Section 2.2?", "answer": "x* and y* represent threshold points where fairness criteria are evaluated; their alignment illustrates how the 1970s shift focused on establishing objective fairness metrics rather than merely identifying bias.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig0.jpg", "caption": "Figure 1: Petersen and Novick’s [52] original figures demonstrating fairness criteria. The marginal distributions of test scores and ground truth scores for subgroups $\\pi _ { 1 }$ and $\\pi _ { 2 }$ are shown by the axes.", "figure_type": "diagram", "evidence_from_figure": "Arrows connecting x* (test axis) and y* (vertical axis)", "evidence_from_text": "Section 2.2 states 'work began to arise that parallels the recent evolution of work in ML fairness, marking a change in framing from unfairness to fairness'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.10104_1811.10104_fig_3_433", "query": "What does the curve labeled '1' in Figure 2 represent in Darlington's fairness framework, and how does it relate to the fixed demographic correlation of 0.2 mentioned in the caption?", "answer": "Curve '1' represents Darlington's criterion where fair values of r_cx decrease as r_xy increases. This illustrates that higher test score-ground truth correlation requires lower culture-test score correlation to maintain fairness, with the demographic correlation fixed at 0.2 as stated in the caption.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg", "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)", "figure_type": "plot", "evidence_from_figure": "Curve 1's downward trend showing r_cx decreasing with increasing r_xy", "evidence_from_text": "Caption stating the demographic correlation is fixed at 0.2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.10104_1811.10104_fig_3_434", "query": "Why did research on quantitative fairness definitions stop after 1976 according to the text, and how does Figure 2 illustrate the complexity of this research?", "answer": "Research halted because studies failed to provide unequivocal evidence of fairness (text: 'research over the last 30 years has not supplied any analyses to unequivocally indicate fairness or unfairness'). Figure 2 illustrates this complexity through multiple fairness conditions (lines 1-4), showing no single solution exists for fair r_cx across varying r_xy.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig2.jpg", "caption": "Figure 2: Darlington’s original graph of fair values of the correlation between culture and test score $r _ { C X }$ in Darlingrton’s notation), plotted against the correlation between test score and ground truth $( r _ { X Y } )$ , according to his definitions r(1–4). (The correlation between the demographic and target variables is assumed here to be fixed at 0.2.)", "figure_type": "plot", "evidence_from_figure": "Multiple lines (1-4) representing different fairness conditions", "evidence_from_text": "Text stating research failed to indicate fairness/unfairness", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.10104_1811.10104_fig_4_435", "query": "In Figure 3, which gender shows higher percent correct at SAT-Verbal scores below 400, and how does this relate to the 'equalized odds' fairness criterion mentioned in Section 3.1?", "answer": "Females show higher percent correct below 400 SAT scores. This DIF violates equalized odds, which requires conditional independence of group membership (A) and outcome (Y) given ability (D), as the conditional probability of correct response differs by gender at the same ability level.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg", "caption": "Figure 3: Original graph from [22] illustrating DIF.", "figure_type": "plot", "evidence_from_figure": "Trend of open circles (females) above filled circles (males) below 400 SAT score", "evidence_from_text": "Section 3.1 states equalized odds requires A ⊥ D | Y", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1811.10104_1811.10104_fig_4_436", "query": "How does the DIF observed in Figure 3 inform the recommendation in Section 2.5 to examine items with DIF for bias?", "answer": "The figure shows females outperform males at lower ability levels while males outperform at higher levels, indicating bias in item functioning. This aligns with Section 2.5's recommendation to examine DIF items for bias causes, as such discrepancies suggest unfair test item behavior.", "doc_id": "1811.10104", "figure_id": "1811.10104_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1811.10104/1811.10104/hybrid_auto/images/1811.10104_page0_fig3.jpg", "caption": "Figure 3: Original graph from [22] illustrating DIF.", "figure_type": "plot", "evidence_from_figure": "DIF trend where performance differs by gender across ability levels", "evidence_from_text": "Section 2.5 states items with DIF should be examined for bias and possibly removed", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_1_437", "query": "What criteria were applied to discard faces with a bounding box like the one in Figure 1?", "answer": "Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded, as specified in the caption text.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg", "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "figure_type": "photo", "evidence_from_figure": "red bounding box outlining the face region", "evidence_from_text": "Face region size criteria (50x50) and inter-ocular distance (30 pixels)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_1_438", "query": "Which processing step does the red bounding box in Figure 1 represent in the pipeline?", "answer": "The red bounding box represents the face detection step using Faster-RCNN, as described in the caption where detected faces are processed with DLIB for pose/landmark extraction.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig0.jpg", "caption": "Figure 1: Each candidate photo from YFCC-100M was processed by first detecting the depicted faces with a Convolutional Neural Network (CNN) using the Faster-RCNN based object detector [61]. Then each detected face as in (a) was processed using DLIB [62] to extract pose and landmark points as shown in (b) and subsequently assessed based on the width and height of the face region. Faces with region size less than 50x50 or inter-ocular distance of less than 30 pixels were discarded. Faces with non-frontal pose, or anything beyond being slightly tilted to the left or the right, were also discarded. Finally, an affine transformation was performed using center points of both eyes, and the face was rectified as shown in (c).", "figure_type": "photo", "evidence_from_figure": "red bounding box around the face", "evidence_from_text": "Caption text mentioning 'Faster-RCNN based object detector' and 'processed using DLIB'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_5_439", "query": "What is the purpose of line segment 'a' and point 'b' in Figure 3's facial symmetry measurement process for coding scheme 4?", "answer": "Line segment 'a' connects the inner canthus points (C1-C2) to define the horizontal symmetry baseline, while point 'b' connects the philtrum (C3) to the midpoint of 'a' to establish vertical symmetry alignment for measuring facial symmetry.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg", "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.", "figure_type": "diagram", "evidence_from_figure": "Labels 'a' and 'b' with their geometric relationships to C1-C2 and C3", "evidence_from_text": "Section 4.4 describing coding scheme 4 as 'facial symmetry measures' and the caption explaining 'point a for C1-C2 and point b connecting C3 to the midpoint of point a'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_5_440", "query": "How does the Sobel filter relate to the reference points shown in Figure 3 for coding scheme 4?", "answer": "The Sobel filter is applied after the reference points (C1-C3) are established in Figure 3(a) to extract edge magnitude and orientation (parts b and c of the figure), which are then used to derive edge orientation similarity as part of the facial symmetry measure.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig4.jpg", "caption": "Figure 3: Process for extracting facial symmetry measures for coding scheme 4, starting with (a) rectified face showing face mid-line and reference points for inner canthus (C1 and C2) and philtrum (C3) and line segmented connecting them (point $a$ for C1-C2 and point $b$ connecting C3 to the midpoint of point $a$ ). Additionally, a Sobel filter is used to extract (b) edge magnitude and (c) orientation to derive the measure for edge orientation similarity.", "figure_type": "diagram", "evidence_from_figure": "Figure 3(a) showing reference points C1-C3", "evidence_from_text": "Caption stating 'a Sobel filter is used to extract (b) edge magnitude and (c) orientation' and Section 4.4 describing facial symmetry as the focus of coding scheme 4", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_8_441", "query": "In Figure 4, what do the green lines inside the red polygons around the eyes and lips represent for Coding Scheme 5?", "answer": "The green lines represent the inner regions used to calculate contrast measures via average pixel intensity differences between outer (red) and inner (green) areas as described in the caption.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_8", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg", "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "figure_type": "photo", "evidence_from_figure": "green lines inside red polygons around eyes/lips", "evidence_from_text": "caption states 'computation is based on average pixel intensity differences between outer and inner regions'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_8_442", "query": "How does the contrast measure for the eyebrows region (shown in Figure 4) relate to the study's finding that high-contrast faces are judged younger?", "answer": "The eyebrows region's contrast measure contributes to overall facial contrast, where higher contrast (as computed for this region) aligns with the finding that high-contrast faces are perceived as younger.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_8", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig7.jpg", "caption": "Figure 4: Process for extracting facial regions contrast measures for coding scheme 5. The computation is based on the average pixel intensity differences between the outer and inner regions for the lips, eyes and eyebrows as depicted above.", "figure_type": "photo", "evidence_from_figure": "red polygon outlining eyebrows with green inner lines", "evidence_from_text": "text states 'high-contrast faces were judged to be younger than low-contrast face'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_14_443", "query": "What is the purpose of the 'masked ITA map' (panel g) in Figure 5 for coding scheme 6?", "answer": "The masked ITA map isolates skin regions from non-skin areas to enable accurate skin color extraction in coding scheme 6, as required by the process described in the caption.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_14", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg", "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "figure_type": "diagram", "evidence_from_figure": "Panel g labeled 'masked ITA map'", "evidence_from_text": "The process aims to extract skin color, and masking is a standard step to focus on relevant regions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_14_444", "query": "How does the $L$ channel (panel c) contribute to the ITA-based skin color extraction in coding scheme 6?", "answer": "The $L$ channel (lightness) is a component of the LAB color space used in calculating the Individual Typology Angle (ITA), which is essential for skin color extraction in coding scheme 6.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_14", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg", "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "figure_type": "diagram", "evidence_from_figure": "Panel c labeled '$L$ channel'", "evidence_from_text": "Coding scheme 6 uses ITA, which relies on LAB color space parameters", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_14_445", "query": "What does the 'ITA histogram' (panel h) represent in the context of coding scheme 6?", "answer": "The ITA histogram quantifies the distribution of Individual Typology Angle values to determine skin color thresholds for coding scheme 6.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_14", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig13.jpg", "caption": "Figure 5: Process for extracting skin color for coding scheme 6 based on Individual Typology Angle-based (ITA). (a) Input face (b) skin map (c) $L$ channel (d) $a$ channel (e) $b$ channel (f) ITA map (g) masked ITA map (h) ITA histogram.", "figure_type": "diagram", "evidence_from_figure": "Panel h labeled 'ITA histogram'", "evidence_from_text": "The process involves extracting skin color, and histograms are used for thresholding in color analysis", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_18_446", "query": "In Figure 6, what does the y-axis value of 1.00 for Simpson E indicate about the uniform distribution's evenness as the number of classes increases from 2 to 20?", "answer": "The value of 1.00 indicates perfect evenness (maximum possible) is maintained regardless of the number of classes, as the uniform distribution distributes data equally across all classes.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_18", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg", "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.", "figure_type": "plot", "evidence_from_figure": "Flat line at 1.00 across all # of classes", "evidence_from_text": "Evenness is generally balanced with highest Simpson E of 0.981", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_18_447", "query": "Why is the highest Simpson E value for coding scheme 1 (0.981) lower than the uniform distribution's Simpson E of 1.00 in Figure 6?", "answer": "Coding scheme 1's feature distribution is not perfectly uniform (as evidenced by the 0.981 value), resulting in slightly lower evenness compared to the theoretical maximum shown in Figure 6.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_18", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig17.jpg", "caption": "Figure 6: Illustration of how (a) diversity and (b) evenness varies for a uniform distribution compared to how (c) diversity and (d) evenness varies for a random distribution.", "figure_type": "plot", "evidence_from_figure": "Uniform distribution's Simpson E line at 1.00", "evidence_from_text": "Highest Simpson E value is 0.981 for coding scheme 1", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_38_448", "query": "What is the highest Simpson D value for coding scheme 2 as referenced in the text related to Figure 8?", "answer": "5.888", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_38", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg", "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.", "figure_type": "plot", "evidence_from_figure": "Figure 8 is the bar plot for coding scheme 2", "evidence_from_text": "Text states: 'The highest Simpson D value is 5.888 and the smallest is 5.858' for coding scheme 2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_38_449", "query": "What is the highest Shannon H value for coding scheme 2 as mentioned in the text related to Figure 8?", "answer": "1.782", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_38", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig37.jpg", "caption": "Figure 8: Feature distribution of craniofacial areas (coding scheme 2) for the $D i F$ data set.", "figure_type": "plot", "evidence_from_figure": "Figure 8 is the bar plot for coding scheme 2", "evidence_from_text": "Text states: 'The highest Shannon H value is 1.782 and the lowest is 1.780' for coding scheme 2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_50_450", "query": "What is the x-axis value where the tallest bar occurs in Figure 10(b), and how does this relate to the 'lower evenness' mentioned for coding scheme 4 in the text?", "answer": "The tallest bar occurs around 0.005-0.01. This bimodal peak distribution directly explains the 'lower evenness' described in the text, as evenness measures uniformity—peaks indicate non-uniformity.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_50", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg", "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.", "figure_type": "plot", "evidence_from_figure": "Tallest bar location (0.005-0.01) and bimodal shape", "evidence_from_text": "Text states 'evenness values are lower as well' for coding scheme 4", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_50_451", "query": "How does the distribution shape in Figure 10(b) correlate with the 'moderate diversity' metrics (Simpson D = 5.510, Shannon H = 1.748) for coding scheme 4?", "answer": "The non-uniform bimodal distribution (peaks at 0.005 and 0.01) aligns with moderate diversity metrics—higher diversity would require a flatter distribution, while lower diversity would show a single dominant peak.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_50", "figure_number": 10, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig49.jpg", "caption": "Figure 10: Feature distribution of facial symmetry (coding scheme 4): (a) density difference and (b) edge orientation similarity for the $D i F$ data set.", "figure_type": "plot", "evidence_from_figure": "Bimodal distribution with peaks at 0.005/0.01", "evidence_from_text": "Text states 'diversity value is in a middle range' with specific Simpson D/Shannon H values", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_58_452", "query": "What is the range of the 'value' axis in Figure 11, and how does this relate to the ITA score calculation mentioned in the text?", "answer": "The 'value' axis ranges from -0.02 to 0.01. These contrast values are averaged (as per the text) to compute the ITA score for each face, which is part of the coding scheme 5 analysis.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_58", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg", "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ", "figure_type": "plot", "evidence_from_figure": "x-axis range (-0.02 to 0.01)", "evidence_from_text": "Average the values to give a single ITA score for each face; Table 11 formula", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_58_453", "query": "How does the evenness factor Shannon E of 0.979 mentioned in the text relate to the distribution shown in Figure 11?", "answer": "The distribution in Figure 11 shows a relatively even spread of contrast values (bars of similar heights), which corresponds to the high Shannon E value of 0.979, indicating near-even distribution.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_58", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg", "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ", "figure_type": "plot", "evidence_from_figure": "bar heights showing a balanced distribution", "evidence_from_text": "evenness factor Shannon E is very close to 0.979 indicating that the measures are close to ev", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_58_454", "query": "What is the highest Simpson D value mentioned in the text, and how is it related to the contrast distribution in Figure 11?", "answer": "The Simpson D value of 5.872 reflects the diversity of contrast values across faces as shown in Figure 11's distribution, where multiple distinct contrast ranges are observed.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_58", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig57.jpg", "caption": "Figure 11: Feature distribution of facial regions contrast (coding scheme 5) for the $D i F$ data set.   ", "figure_type": "plot", "evidence_from_figure": "spread of contrast values across the x-axis", "evidence_from_text": "highest Simpson D value is 5.872", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_61_455", "query": "What is the Simpson D value for the age prediction feature distribution in Figure 13(a)?", "answer": "4.368", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_61", "figure_number": 13, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg", "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ", "figure_type": "plot", "evidence_from_figure": "Figure 13(a) is labeled 'age prediction' and corresponds to coding scheme 7", "evidence_from_text": "Text states 'Simpson D and Shannon H values are 4.368 and 1.601'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_61_456", "query": "Which age group has the highest frequency in Figure 13(a)?", "answer": "20-30", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_61", "figure_number": 13, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig60.jpg", "caption": "Figure 13: Feature distribution of (a) age prediction (coding scheme 7) and (b) gender prediction (coding scheme 8) for the $D i F$ data set.   ", "figure_type": "plot", "evidence_from_figure": "The bar for 20-30 is visibly the tallest", "evidence_from_text": "Text lists the age bins as [0-3], [4-12], etc., confirming the group labels", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_65_457", "query": "What is the Shannon H value for the pose distribution in Figure 15(a), and how does the visual dominance of 'Frontal value' relate to this value?", "answer": "The Shannon H value is 0.39 (from text), indicating low diversity due to the extreme dominance of the 'Frontal value' category in the bar chart.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_65", "figure_number": 15, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg", "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).", "figure_type": "plot", "evidence_from_figure": "Dominance of 'Frontal value' bar compared to other categories", "evidence_from_text": "Text states: 'The three class pose distribution has a Shannon H value of 0.39.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1901.10436_1901.10436_fig_65_458", "query": "How does the pose diversity in coding scheme 10 (Figure 15(a)) compare to other coding schemes mentioned in the text?", "answer": "Coding scheme 10 shows lower diversity (Shannon H=0.39) compared to other schemes, which have higher diversity scores as noted in the text.", "doc_id": "1901.10436", "figure_id": "1901.10436_fig_65", "figure_number": 15, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1901.10436/1901.10436/hybrid_auto/images/1901.10436_page0_fig64.jpg", "caption": "Figure 15: Feature distribution of pose and resolution (coding scheme 10) for the $D i F$ data set, including (a) pose, (b) face region bounding box size, (c) intra-ocular distance (IOD).", "figure_type": "plot", "evidence_from_figure": "Skewed distribution with one dominant category", "evidence_from_text": "Text states: 'many of the dimensions of the craniofacial schemes have high scores in diversity relative to the other coding schemes.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.03519_1902.03519_fig_4_459", "query": "In Figure 1, how many blue points are in the second child node, and how does this relate to the (1,3)-fairlet decomposition requirement?", "answer": "The second child node contains 2 blue points. This reflects an intermediate step in the decomposition process, as the final (1,3)-fairlet requirement specifies 3 blue points per red point. The text explains that the algorithm builds fairlets incrementally through hierarchical steps.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg", "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.", "figure_type": "diagram", "evidence_from_figure": "Second child node's blue point count (2)", "evidence_from_text": "Text states (1,3)-fairlet decomposition requires 1 red and 3 blue points per fairlet", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.03519_1902.03519_fig_4_460", "query": "What does the top node in Figure 1 represent in the context of the γ-HST embedding described in the text?", "answer": "The top node represents a node in the γ-HST embedding of the input point set. The text explains that the algorithm first embeds points into a γ-HST, and the figure illustrates how the decomposition operates on this hierarchical structure.", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig3.jpg", "caption": "Figure 1: A run of our algorithm for (1,3)-fairlet decomposition on 8 blue points and 4 red points in $\\mathbb { R } ^ { 2 }$ . Steps (c)-(e) show the three stages of step 1 in FairletDecomposition.", "figure_type": "diagram", "evidence_from_figure": "Top node as root of the hierarchical structure", "evidence_from_text": "Text states 'the first step in our algorithm is to embed the input point set into a γ-HST'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.03519_1902.03519_fig_8_461", "query": "What balance parameter is used for the Bank dataset in Figure 2?", "answer": "(1,2)", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_8", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg", "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.", "figure_type": "plot", "evidence_from_figure": "The figure title explicitly states '(1,2)-fairlet decomposition runtime (Bank)'", "evidence_from_text": "The caption mentions 'algorithms with the specified balance parameter' and the text references the four datasets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.03519_1902.03519_fig_8_462", "query": "According to the text, how does the runtime scale with the number of points as shown in Figure 2?", "answer": "It scales almost linearly", "doc_id": "1902.03519", "figure_id": "1902.03519_fig_8", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.03519/1902.03519/hybrid_auto/images/1902.03519_page0_fig7.jpg", "caption": "Figure 2: Each figure captures the running time of our fairlet decomposition algorithms with the specified balance parameter on different number of sample points from one of the four datasets: Diabetes, Bank, Census and Census II.", "figure_type": "plot", "evidence_from_figure": "The plot shows a near-linear upward trend in runtime as points increase", "evidence_from_text": "The text states 'the empirical runtime of our algorithm scales almost linearly in the number of points'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.07823_1902.07823_fig_2_463", "query": "At λ=0.05, which algorithm has the highest stab value in Figure 1, and how does this align with the text's claim that ZVRG achieves better fairness?", "answer": "KAAS-St has the highest stab value (~55) at λ=0.05, but ZVRG is preferred for fairness despite lower stab values.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg", "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.", "figure_type": "plot", "evidence_from_figure": "KAAS-St's stab value at λ=0.05 (~55)", "evidence_from_text": "Text states ZVRG achieves better fairness than other algorithms", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.07823_1902.07823_fig_2_464", "query": "What is the stab value for ZVRG-St at λ=0.01, and why was ZVRG chosen as a baseline in the study?", "answer": "ZVRG-St has a stab value of ~38 at λ=0.01; ZVRG was chosen due to its superior fairness performance.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig1.jpg", "caption": "Figure 1: stab vs. $\\lambda$ for race attribute.", "figure_type": "plot", "evidence_from_figure": "ZVRG-St's stab value at λ=0.01 (~38)", "evidence_from_text": "Text states ZVRG achieves better fairness than other algorithms", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.07823_1902.07823_fig_3_465", "query": "What is the stab value for KAA5-St at λ=0.03 in Figure 2, and how does this relate to the text's definition of stab_T(A) as the expected number of different predictions?", "answer": "The stab value for KAA5-St at λ=0.03 is approximately 65, which aligns with the text's definition of stab_T(A) as the expected number of different predictions, indicating high variability in predictions at this λ value.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg", "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.", "figure_type": "plot", "evidence_from_figure": "KAA5-St peak at λ=0.03 (y-axis value ~65)", "evidence_from_text": "stab_T(A) is defined as the expected number of different predictions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1902.07823_1902.07823_fig_3_466", "query": "Which model has the lowest stab value at λ=0.05 in Figure 2, and what does this imply about its stability according to the text?", "answer": "GYR-St has the lowest stab value (~35) at λ=0.05, implying higher stability as lower stab_T(A) values indicate fewer expected different predictions, consistent with the text's metric definition.", "doc_id": "1902.07823", "figure_id": "1902.07823_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1902.07823/1902.07823/hybrid_auto/images/1902.07823_page0_fig2.jpg", "caption": "Figure 2: stab vs. $\\lambda$ for sex attribute.", "figure_type": "plot", "evidence_from_figure": "GYR-St line at λ=0.05 (y-axis ~35)", "evidence_from_text": "stab_T(A) measures prediction variability where lower values indicate more stable models", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_1_467", "query": "In Figure 1, what do the textures in the dark gray segment represent according to the caption, and how does this relate to the bias in community detection methods described in the text?", "answer": "The textures represent variations of opinion (caption), and the dark gray segment illustrates low-degree nodes that are ignored by biased methods like CESNA/Louvain (text), leading to omission of relevant information.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg", "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.", "figure_type": "example", "evidence_from_figure": "Textures in dark gray segment", "evidence_from_text": "Text explains methods exclude low-degree nodes, causing bias", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_1_468", "query": "How does the distribution of shapes in the light gray segment of Figure 1 reflect the omission of low-degree nodes by CESNA and Louvain methods?", "answer": "The light gray segment shows larger communities formed by high-degree nodes, while the dark gray segment (omitted low-degree nodes) is excluded by methods like CESNA/Louvain (text), demonstrating the bias in community detection.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig0.jpg", "caption": "Figure 1: Demonstration of versatility of relevant low-degree people in social networks who get ignored by biased community detection methods. Shapes (circle, triangle) represent opinion, and texture represents variations of the opinion. We provide real-world examples of how this manifests in subsequent sections.", "figure_type": "example", "evidence_from_figure": "Light gray segment with more shapes", "evidence_from_text": "Text states methods exclude low-degree nodes, focusing on large communities", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_2_469", "query": "Why does the left side of Figure 2 show a dispersed pattern while the right side has clustered green and pink regions?", "answer": "The left side uses modularity-based coloring (network structure), while the right side uses ground truth political party labels, which may not align with structural communities.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg", "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.", "figure_type": "diagram", "evidence_from_figure": "Left side scattered points vs. right side clustered green/pink regions", "evidence_from_text": "disagreement between network structure (left) and ground truth labels (right)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_2_470", "query": "What does the callout in Figure 2 emphasize, and why is this relevant to the dataset?", "answer": "The callout emphasizes low-degree users, which are part of the seed-user dataset used for ground truth labels.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig1.jpg", "caption": "Figure 2: The 2016 U.S. presidential election seed-user retweet network colored by the political party from modularity. The callout emphasizes the low-degree users.", "figure_type": "diagram", "evidence_from_figure": "Callout highlighting a specific region", "evidence_from_text": "callout emphasizes low-degree users; seed users were used for ground truth labels", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_3_471", "query": "In Figure 3, the callout zooms a component showing disagreement between labeling approaches. What does the text state about the agreement between network-based communities and ground truth labels?", "answer": "The text states there was very low agreement between communities detected using network attributes and ground truth labels, which will be discussed in the 'Community Detection Results' section.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg", "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.", "figure_type": "diagram", "evidence_from_figure": "Callout zoom showing disagreement between two labeling approaches", "evidence_from_text": "Text states 'surprisingly these results had a very low agreement which will be discussed in detail in 'Community Detection Results' section'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_3_472", "query": "According to Figure 3's caption and the text, what method was used to obtain the ground truth labels for the green nodes (Gamergate supporters)?", "answer": "The ground truth labels were obtained by asking Amazon Mechanical Turk workers to label users based on their tweets after filtering the dataset to 8,128 users.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig2.jpg", "caption": "Figure 3: The Gamergate retweet network colored based on the network structure is shown on the left hand side, and the network colored by the ground truth labels is shown on the right hand side. The callout zooms one of the components, showing the disagreement between the two labeling approaches. Purple nodes represent Gamergate opposers and green nodes represent Gamergate supporters.", "figure_type": "diagram", "evidence_from_figure": "Green nodes representing Gamergate supporters", "evidence_from_text": "Text states 'we asked the turkers on Amazon Mechanical Turk to label each of the 8,128 users left based on their tweets'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_6_473", "query": "In Figure 4, what does the green color indicate about CLAN's label for the Gamergate dataset, given the table example where CLAN matches ground truth?", "answer": "Green indicates disagreement between CLAN's label and ground truth, as the table shows CLAN matching ground truth ('Anti Gamerge') in the example (implying red would represent agreement).", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg", "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.", "figure_type": "plot", "evidence_from_figure": "green node distribution in clusters", "evidence_from_text": "table example showing CLAN matching ground truth", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_6_474", "query": "How does CESNA's agreement with ground truth compare to Modularity in Figure 4, based on the table's N/A entries for CESNA?", "answer": "CESNA shows more green nodes (disagreement) than Modularity, consistent with the table's N/A entries indicating CESNA's labels were often unassigned or incorrect.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig5.jpg", "caption": "Figure 4: Networks colored by agreement with the ground truth labels for three methods for the Gamergate dataset.", "figure_type": "plot", "evidence_from_figure": "higher density of green nodes in CESNA's cluster", "evidence_from_text": "table entries showing CESNA as N/A", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_9_475", "query": "In Figure 5, how does the slope parameter influence the network structure of synthetic distributions as tested in the Gamergate dataset?", "answer": "The slope parameter modulates the degree of skew in synthetic distributions. As slope increases (from left to right), the network structure shifts from distinct, non-overlapping clusters (e.g., panel 1) to more overlapping or fragmented communities (e.g., panel 4), demonstrating how slope affects detectable community boundaries.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_9", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg", "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.", "figure_type": "diagram", "evidence_from_figure": "Variation in cluster separation and edge density across panels labeled with 'Slope'", "evidence_from_text": "Text explains slope is used to test resilience to skewed data where communities correlate with degree", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_9_476", "query": "Why were synthetic distributions with different slope values included in Figure 5 according to the paper's methodology?", "answer": "The synthetic distributions with varying slopes were used to test CLAN's resilience to skewed data, where communities might be overlooked if membership correlates with degree (e.g., introverts forming fewer links). The slope parameter simulates different levels of skew to evaluate the method's robustness.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_9", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig8.jpg", "caption": "Figure 5: Synthetic distributions with their corresponding network and obtained results for the Gamergate dataset.", "figure_type": "diagram", "evidence_from_figure": "Four panels showing distinct cluster configurations under different slope conditions", "evidence_from_text": "Text states the goal was to test against 'communities correlated with degree' and 'resilience to skewed data'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_14_477", "query": "How does the 'Slope' parameter affect the integration of sparsely-connected nodes into communities in Figure 6, as explained by the CLAN method in the text?", "answer": "Lower slope values (left panels) in Figure 6 show denser cluster structures where sparsely-connected nodes are integrated into communities, aligning with CLAN's method described in the text to minimize bias by including lowly-connected nodes.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg", "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.", "figure_type": "plot", "evidence_from_figure": "Four panels with varying cluster density under 'Slope' label", "evidence_from_text": "CLAN minimizes bias by including lowly-connected nodes into their true communities", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.08136_1903.08136_fig_14_478", "query": "Which visual pattern in Figure 6 corresponds to CLAN's success in detecting communities for sparsely-connected nodes?", "answer": "The leftmost panel (lowest slope) shows the densest cluster structure, indicating effective integration of sparsely-connected nodes into communities as per CLAN's approach.", "doc_id": "1903.08136", "figure_id": "1903.08136_fig_14", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.08136/1903.08136/hybrid_auto/images/1903.08136_page0_fig13.jpg", "caption": "Figure 6: Synthetic distributions with their corresponding network and obtained results for the 2016 U.S. Presidential Election dataset.", "figure_type": "plot", "evidence_from_figure": "Leftmost panel's dense cluster formation", "evidence_from_text": "CLAN minimizes bias by including lowly-connected nodes into their true communities", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10561_1903.10561_fig_1_479", "query": "In Figure 1, why does the 'sent-angry_black_woman_stereotype' test show a dark blue cell for 'GenSen' but not for 'CBoW'? What does the text say about GenSen's training that might explain this?", "answer": "GenSen's training on multiple tasks (MultiNLI, SNLI, next sentence prediction, translation, constituency) likely enables better capture of nuanced stereotypes, while CBoW's simple word embedding averaging (as described in the text) lacks contextual sensitivity for this specific test.", "doc_id": "1903.10561", "figure_id": "1903.10561_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg", "caption": "Figure 1: Significance of results for all models and tests.", "figure_type": "plot", "evidence_from_figure": "Dark blue cell for 'GenSen' under 'sent-angry_black_woman_stereotype'", "evidence_from_text": "GenSen's training objectives listed in the text", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10561_1903.10561_fig_1_480", "query": "The 'weat9' test shows an insignificant result for 'CBoW' in Figure 1. How does the text's description of CBoW's methodology explain this outcome?", "answer": "CBoW's averaging of word embeddings (described as a 'simple baseline' in the text) fails to capture complex contextual patterns required for the 'weat9' test, resulting in insignificant results.", "doc_id": "1903.10561", "figure_id": "1903.10561_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10561/1903.10561/hybrid_auto/images/1903.10561_page0_fig0.jpg", "caption": "Figure 1: Significance of results for all models and tests.", "figure_type": "plot", "evidence_from_figure": "Light blue cell for 'CBoW' under 'weat9'", "evidence_from_text": "CBoW's methodology as a word embedding average", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_1_481", "query": "Which model does the triangle symbol represent in Figure (a), as specified in the legend text after the figure?", "answer": "DADT", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Triangle symbol (△) data points", "evidence_from_text": "Text after figure: 'Family·MIP △DADT■CART•logistic'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_1_482", "query": "How does the Discrimination metric affect the Accuracy for the logistic model in Figure (a), based on the MIP formulation context?", "answer": "The logistic model (circle symbols) shows a decreasing trend in Accuracy as Discrimination increases, consistent with the MIP's optimization constraints balancing these metrics.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_1", "figure_number": null, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig0.jpg", "caption": "", "figure_type": "plot", "evidence_from_figure": "Circle symbol (•) data points and their trend", "evidence_from_text": "MIP formulation context and legend identification of logistic model", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_4_483", "query": "Which approach does the shaded triangle in Figure 1(d) correspond to according to the legend in the text?", "answer": "The shaded triangle corresponds to the 'MIP' approach as indicated by the legend items in the text (MIP △CART ■Regression...).", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg", "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).", "figure_type": "plot", "evidence_from_figure": "Shaded triangle in Figure 1(d)", "evidence_from_text": "Legend items: 'MIP △CART ■Regression MIP-DTCARTregLR-indLR-grp'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_4_484", "query": "Why is MAE used as the accuracy metric in Figure 1(d) instead of misclassification rate?", "answer": "MAE is used because Figure 1(d) represents results for the regression dataset 'Crime', whereas misclassification rate applies to classification datasets (a)-(c) as stated in the caption.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig3.jpg", "caption": "Figure 1: Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).", "figure_type": "plot", "evidence_from_figure": "MAE (%) on y-axis in Figure 1(d)", "evidence_from_text": "Caption states: 'Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_6_485", "query": "What is the DTI value for MIP at tree depth 4 in Figure 2(b), and how does this relate to the DTDI value of 0 mentioned in the text?", "answer": "The DTI value for MIP at tree depth 4 is approximately 25%. The text states DTDI was 0, suggesting this DTI metric may represent a fairness component where a value of 0 indicates no disparity in protected characteristics.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_6", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg", "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.", "figure_type": "plot", "evidence_from_figure": "DTI line (triangles) for MIP at tree depth 4", "evidence_from_text": "Text stating 'DTD1 was $0' in the experiment", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_6_486", "query": "How does the accuracy trend for MIP compare to CART at tree depth 5 in Figure 2(b), and what does the text imply about fairness trade-offs?", "answer": "MIP's accuracy (circles) is higher than CART's at tree depth 5. The text implies this accuracy advantage aligns with improved fairness (DTD1=0), suggesting MIP achieves better fairness without sacrificing accuracy.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_6", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig5.jpg", "caption": "Figure 2: From left to right: (a) MIP objective value and (b) Accuracy and fairness in dependence of tree depth; (c) Comparison of upper and lower bound evolution while solving MILP problem; and (d) Empirical distribution of $\\gamma ( \\mathbf { x } ) : = \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } , \\mathbf { x } _ { \\mathrm { { p } } } ) - \\mathsf { P } ( y | \\mathbf { x } _ { \\mathrm { { \\overline { { p } } } } } )$ (see Definition 2.5) when $\\mathbf { X }$ is valued in the test set in both CART $\\lambda = 0$ ) and MIP.", "figure_type": "plot", "evidence_from_figure": "Accuracy lines for MIP (circles) and CART (circles) at depth 5", "evidence_from_text": "Text stating DTD1 was 0 and 'likelihood for individuals... is twice as high in MIP'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_9_487", "query": "In Figure 3(a), which approach achieved the highest accuracy on the Default dataset, and how does the text describe the nature of these models?", "answer": "MIP-DT. The text states these are 'the most accurate models with zero discrimination (when available)'", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg", "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.", "figure_type": "plot", "evidence_from_figure": "Tallest bar in Default dataset group (MIP-DT)", "evidence_from_text": "Text: 'Accuracy results for the most accurate models with zero discrimination (when available) are shown in Figure 3.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1903.10598_1903.10598_fig_9_488", "query": "Which approach shows the largest accuracy drop between the Adult Dataset and COMPAS datasets in Figure 3(a), and what does the text imply about this trade-off?", "answer": "log-grp. The text implies this reflects the trade-off between non-discrimination and accuracy for maximally non-discriminative models.", "doc_id": "1903.10598", "figure_id": "1903.10598_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1903.10598/1903.10598/hybrid_auto/images/1903.10598_page0_fig8.jpg", "caption": "Figure 3: Accuracy of maximally non-discriminative models in each approach for (a) classification and (b) regression.", "figure_type": "plot", "evidence_from_figure": "log-grp bar drops from ~75% (Adult Dataset) to ~50% (COMPAS)", "evidence_from_text": "Caption: 'Accuracy of maximally non-discriminative models...'; Text: 'most accurate models with zero discrimination'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1904.03035_1904.03035_fig_1_489", "query": "In Figure 1, how does the 'Bias after Regularization' histogram compare to the 'Bias in Generated Text' histogram, and what role does λ play in this difference?", "answer": "The 'Bias after Regularization' histogram shows reduced bias compared to 'Bias in Generated Text', indicating regularization mitigates bias. λ controls the importance of minimizing bias in the embedding matrix, as stated in the caption, directly influencing this reduction.", "doc_id": "1904.03035", "figure_id": "1904.03035_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg", "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.", "figure_type": "diagram", "evidence_from_figure": "Comparison of bias histograms for generated text with/without regularization", "evidence_from_text": "Caption stating λ controls bias minimization importance and text explaining regularization encourages embeddings to depend minimally on gender", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1904.03035_1904.03035_fig_1_490", "query": "What does the 'Cross Entropy Loss + λ(N,B)' box in Figure 1 represent in the context of the regularization procedure described in the text?", "answer": "It represents the regularization term that modifies the standard loss function to minimize gender bias in embeddings. The text states this procedure encourages embeddings to depend minimally on gender, with λ determining the bias minimization weight.", "doc_id": "1904.03035", "figure_id": "1904.03035_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03035/1904.03035/hybrid_auto/images/1904.03035_page0_fig0.jpg", "caption": "Figure 1: Word level language model is a three layer LSTM model. $\\lambda$ controls the importance of minimizing bias in the embedding matrix.", "figure_type": "diagram", "evidence_from_figure": "Label 'Cross Entropy Loss + λ(N,B)' in the bottom pipeline", "evidence_from_text": "Text explaining regularization encourages embeddings to depend minimally on gender and caption defining λ's role", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1904.03310_1904.03310_fig_1_491", "query": "How many principal components are shown in the left plot of Figure 1 with significant variance, and how does this compare to GloVe as stated in the text?", "answer": "The left plot shows two principal components (PC1 and PC2) with significant variance (26% and 22%, respectively), while the text states ELMo has two gender-related principal components versus GloVe's single component.", "doc_id": "1904.03310", "figure_id": "1904.03310_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg", "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.", "figure_type": "plot", "evidence_from_figure": "Bars at x=0 and x=2 dominate the variance distribution", "evidence_from_text": "Text states: 'Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1904.03310_1904.03310_fig_1_492", "query": "What percentage of explained variance does the first principal component account for in ELMo embeddings according to Figure 1?", "answer": "Approximately 26% (based on the height of the bar at x=0 on the y-axis)", "doc_id": "1904.03310", "figure_id": "1904.03310_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1904.03310/1904.03310/hybrid_auto/images/1904.03310_page0_fig0.jpg", "caption": "Figure 1: Left: Percentage of explained variance in PCA in the embedding differences. Right: Selected words projecting to the first two principle components where the blue dots are the sentences with male context and the orange dots are from the sentences with female context.", "figure_type": "plot", "evidence_from_figure": "Bar height at x=0 aligns with 0.26 on the y-axis", "evidence_from_text": "Text context confirms this figure relates to ELMo's gender bias analysis", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.03674_1905.03674_fig_2_493", "query": "What does the line segment labeled r_y represent in the context of the proof?", "answer": "The line segment r_y represents the distance between points i* and y, which is used in the triangle inequality d(i*, x) ≤ r_y to bound the distance between i* and x.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg", "caption": "Figure 2: Diagram for Proof of Theorem 1", "figure_type": "diagram", "evidence_from_figure": "The vertical line labeled r_y connecting i* and y", "evidence_from_text": "The triangle inequality d(i*, x) ≤ r_y and the condition i ∈ B(x, r_y) ∩ S", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.03674_1905.03674_fig_2_494", "query": "How does the position of point i relate to the sets X and N in Algorithm 1?", "answer": "Point i lies in the intersection of the two circles, indicating it belongs to both B(x, r_y) (a ball centered at x ∈ X) and N, satisfying the condition ∃x ∈ X, ∃i ∈ N such that i ∈ B(x, r_y) ∩ S.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig1.jpg", "caption": "Figure 2: Diagram for Proof of Theorem 1", "figure_type": "diagram", "evidence_from_figure": "Point i located in the overlapping region of the two circles", "evidence_from_text": "The condition '∃x ∈ X and ∃i ∈ N such that i ∈ B(x, r_y) ∩ S' and Algorithm 1's logic for set operations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.03674_1905.03674_fig_7_495", "query": "In Figure 5, why does k-means++ have a lower k-means objective than Local Capture for all k values from 2 to 10?", "answer": "The text explains that k-means++ achieves a lower k-means objective, but to attain the proportionality of Local Capture while maintaining this objective, one must use 2k centers (as the union of both solutions trivially achieves this).", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_7", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg", "caption": "Figure 5: $k$ -means objective", "figure_type": "plot", "evidence_from_figure": "k-means++ (orange squares) consistently plots below Local Capture (blue circles) across all k values", "evidence_from_text": "The text states that using 2k centers allows achieving both Local Capture's proportionality and k-means++'s k-means objective", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.03674_1905.03674_fig_7_496", "query": "What does the text imply about the tradeoff between proportionality and k-means objective when comparing Local Capture and k-means++ as shown in Figure 5?", "answer": "The text implies that the tradeoff is quantified by the number of extra centers needed (e.g., 2k) to achieve both the proportionality of Local Capture and the lower k-means objective of k-means++, as the figure shows k-means++ has a consistently lower objective.", "doc_id": "1905.03674", "figure_id": "1905.03674_fig_7", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.03674/1905.03674/hybrid_auto/images/1905.03674_page0_fig6.jpg", "caption": "Figure 5: $k$ -means objective", "figure_type": "plot", "evidence_from_figure": "k-means++ has lower objective values than Local Capture for all k values", "evidence_from_text": "The text discusses 'how many extra centers' are required to balance proportionality and objective", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_1_497", "query": "In Figure 1, how does the adversarial training between Filters and Discriminators enforce fairness in graph embeddings?", "answer": "The Filters process Node Embeddings into Filtered Embeddings such that the Discriminators cannot classify sensitive attributes (e.g., Gender, Occupation, Age), as the text explains the adversarial training prevents sensitive information inference.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg", "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.", "figure_type": "diagram", "evidence_from_figure": "Arrows showing Filter-to-Discriminator connections with 'D' labels", "evidence_from_text": "Text describing adversarial training to prevent discriminators from classifying sensitive attributes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_1_498", "query": "What is the role of the 'D' labels in the Discriminators section of Figure 1, as described in the text?", "answer": "The 'D' labels denote Discriminators, which are adversarial classifiers trained to detect sensitive attributes from Filtered Embeddings; the Filters are trained to minimize this detection, ensuring embedding invariance.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig0.jpg", "caption": "Figure 1. Overview of our approach: Our goal is to generate graph embeddings that are invariant to particular sensitive attributes (e.g., age or gender). We train a set of “filters” to prevent adversarial discriminators from classifying the sensitive information from the filtered embeddings. After training, these filters can be composed together in different combinations, allowing the flexible generation of embeddings that are invariant w.r.t. any subset of the sensitive attributes.", "figure_type": "diagram", "evidence_from_figure": "Discriminators labeled D Gender, D Occupation, D Age", "evidence_from_text": "Text defining discriminators as adversarial classifiers in the fairness framework", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_2_499", "query": "In Figure 2, what is the RMSE value for the Compositional Adversary at 200 epochs, and how does the text explain its relationship to the baseline?", "answer": "The Compositional Adversary reaches an RMSE of approximately 1.01 at 200 epochs (visible in Figure 2), which is 0.145 higher than the Baseline No Adversary's 0.865 RMSE. The text explains this represents the invariance-accuracy tradeoff where achieving sensitive attribute invariance (via compositional adversarial training) degrades main task accuracy.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg", "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).", "figure_type": "plot", "evidence_from_figure": "Compositional Adversary line at 200 epochs (≈1.01 RMSE)", "evidence_from_text": "Text states RMSE degrades from 0.865 to 1.01 with compositional adversary", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_2_500", "query": "How does the trend of the Baseline No Adversary line in Figure 2 support the text's claim about the invariance-accuracy tradeoff?", "answer": "The Baseline No Adversary line maintains the lowest RMSE (≈0.865) throughout training, while all adversarial methods show higher RMSE. This visually supports the text's claim that adversarial training (for invariance) reduces main task accuracy, as evidenced by the consistent gap between baseline and adversarial methods' RMSE values.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_2", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig1.jpg", "caption": "Figure 2. Performance on the edge prediction (i.e., recommendation) task on MovieLens, using RMSE as in Berg et al. (2017).", "figure_type": "plot", "evidence_from_figure": "Baseline No Adversary line's consistently low RMSE", "evidence_from_text": "Text describes invariance-accuracy tradeoff where adversarial training increases RMSE", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_3_501", "query": "What is the AUC value of the Held Out Compositional model at epoch 50 in Figure 3, and how does this relate to the text's claim about the compositional approach's performance on Reddit?", "answer": "At epoch 50, the Held Out Compositional model has an AUC of ~0.74. This aligns with the text's statement that the compositional approach showed a 'small drop on the performance of the main edge prediction task' on Reddit data, reflecting the invariance-accuracy tradeoff.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg", "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.", "figure_type": "plot", "evidence_from_figure": "Held Out Compositional line at epoch 50", "evidence_from_text": "text stating 'small drop on the performance of the main edge prediction task (Figure 3)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_3_502", "query": "Why does the Non Compositional model outperform the Held Out Compositional model in Figure 3, according to the text's explanation of the invariance-accuracy tradeoff?", "answer": "The Non Compositional model avoids the invariance constraints applied to the Held Out Compositional model, which prioritizes removing sensitive attribute information (e.g., gender/age). This leads to better edge prediction accuracy (higher AUC) but potentially less fairness, as explained in the text's discussion of the tradeoff.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_3", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig2.jpg", "caption": "Figure 3. Performance on the edge prediction (i.e., recommendation) task on the Reddit data. Evaluation is using the AUC score, since there is only one edge/relation type.", "figure_type": "plot", "evidence_from_figure": "Non Compositional vs. Held Out Compositional line trends", "evidence_from_text": "text describing the 'invariance-accuracy tradeoff' and adversarial training methodology", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_4_503", "query": "Why does the Held Out Compositional approach have a higher AUC score than the Non Compositional approach in Figure 4, and what does this imply about its ability to remove sensitive attributes?", "answer": "The Held Out Compositional approach has a higher AUC (≈0.6) than Non Compositional (≈0.5), indicating it is less effective at removing sensitive attributes. Higher AUC scores correlate with better prediction of sensitive attributes, so a higher value means worse removal. This aligns with the text stating compositional approaches performed worse in removing sensitive attribute information.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg", "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.", "figure_type": "plot", "evidence_from_figure": "AUC values for Held Out Compositional and Non Compositional bars", "evidence_from_text": "Text stating compositional approaches performed worse in removing sensitive attributes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_4_504", "query": "What is the performance drop for held-out combinations in Figure 4, and how does it relate to the AUC difference between No Held Out Compositional and Held Out Compositional?", "answer": "The text states a 0.025 performance drop for held-out combinations. In Figure 4, Held Out Compositional (≈0.6) has a slightly lower AUC than No Held Out Compositional (≈0.6), reflecting minimal degradation. This small difference aligns with the text's claim that the compositional approach generalizes well to unseen combinations.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_4", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig3.jpg", "caption": "Figure 4. Ability to predict sensitive attributes on the Reddit data when using various embedding approaches. Bar plots correspond to the average AUC across the 10 binary sensitive attributes.", "figure_type": "plot", "evidence_from_figure": "AUC values for No Held Out Compositional and Held Out Compositional bars", "evidence_from_text": "Text stating 'performance drop for the held-out combinations is very small (0.025)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_5_505", "query": "In Figure 5, what is the AUC value of the Compositional Gender AUC at λ=10^3, and how does it compare to the Gender Baseline AUC at the same λ?", "answer": "At λ=10^3, the Compositional Gender AUC is approximately 0.50, while the Gender Baseline AUC remains at ~0.70, showing a significant performance drop for the compositional model under the adversary.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg", "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$", "figure_type": "plot", "evidence_from_figure": "Blue line at λ=10^3 and red dashed line at same λ", "evidence_from_text": "Caption identifies both metrics and their comparison", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_5_506", "query": "According to the caption, what does the 'compositional adversary' imply about the relationship between λ and model performance?", "answer": "The 'compositional adversary' parameterized by λ shows a tradeoff where increasing λ (stronger adversary) reduces the Compositional Gender AUC while the Gender Baseline AUC remains stable.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_5", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig4.jpg", "caption": "Figure 5. Tradeoff of Gender AUC score on MovieLens1M for a compositional adversary versus different $\\lambda$", "figure_type": "plot", "evidence_from_figure": "Trend of Compositional Gender AUC decreasing with λ", "evidence_from_text": "Caption describes 'tradeoff of Gender AUC score... versus different λ'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_6_507", "query": "Why does the RMSE of the Compositional Adversary increase sharply around λ = 10^2 in Figure 6, and how does this relate to the text's claim that the compositional approach outperformed individually enforced fairness?", "answer": "The sharp RMSE increase around λ=10² occurs because higher λ prioritizes fairness constraints, reducing accuracy on the main task. This aligns with the text stating the compositional approach better removes sensitive attribute information (even though main task RMSE increases).", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg", "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .", "figure_type": "plot", "evidence_from_figure": "Blue line's sharp rise at λ=10^2", "evidence_from_text": "the compositional trained adversary outperformed the individually trained adversaries in terms of removing information about the sensitive attributes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_6_508", "query": "What does the flat Baseline RMSE line in Figure 6 indicate about the individually enforced fairness approach compared to the compositional approach, as described in the text?", "answer": "The flat Baseline RMSE line indicates the individually enforced approach does not adapt to changes in λ, while the compositional approach's RMSE increases with λ, reflecting its focus on fairness (as the text states it outperformed in removing sensitive attributes).", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_6", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig5.jpg", "caption": "Figure 6. RMSE on MoveLens1M with various $\\lambda$ .", "figure_type": "plot", "evidence_from_figure": "Red line remains flat across all λ values", "evidence_from_text": "the compositional approach performed favorably compared to an approach that individually enforced fairness on each individual attribute", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_7_509", "query": "In Figure 7, why does the Compositional Adversary show a lower prediction bias than the Single Adversary for the Gender attribute?", "answer": "The Compositional Adversary's ability to handle unseen combinations of fairness constraints (as explained in the text) allows it to better reduce bias for Gender compared to the Single Adversary, which is limited to a single sensitive attribute.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_7", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg", "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.", "figure_type": "plot", "evidence_from_figure": "Compositional Adversary bar for Gender is significantly shorter than Single Adversary bar", "evidence_from_text": "Text states compositional approach flexibly accommodates unseen combinations without explicit training", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.10674_1905.10674_fig_7_510", "query": "Which sensitive attribute exhibits the highest prediction bias under the Baseline setting in Figure 7, and what does the text imply about the effectiveness of adversarial regularization?", "answer": "Gender shows the highest bias under Baseline (~0.05). The text implies adversarial regularization (especially Compositional Adversary) effectively reduces bias, as seen in the significantly lower bars for Gender under adversarial settings.", "doc_id": "1905.10674", "figure_id": "1905.10674_fig_7", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.10674/1905.10674/hybrid_auto/images/1905.10674_page0_fig6.jpg", "caption": "Figure 7. Prediction Bias for different Sensitive Attributes under three settings in MovieLens1M.", "figure_type": "plot", "evidence_from_figure": "Baseline bar for Gender is the tallest (0.05)", "evidence_from_text": "Text states 'adversarial regularization does indeed drastically reduce prediction bias' and highlights compositional approach's flexibility", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.12843_1905.12843_fig_4_511", "query": "Which model does the orange line in Figure 1 represent, and how does it compare to fair classification on the adult dataset?", "answer": "The orange line represents 'fair reg. (oracle=C5, model=linear)'. On the adult dataset, fair classification slightly outperforms this method as stated in the caption.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg", "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.", "figure_type": "plot", "evidence_from_figure": "Orange line in legend", "evidence_from_text": "Caption states 'Our method dominates... except adult, where fair classification is slightly better'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.12843_1905.12843_fig_4_512", "query": "What does the blue dashed line in Figure 1 represent, and why are confidence intervals shown for constraint violation?", "answer": "The blue dashed line represents 'fair reg. (oracle=LR, model=tree ensemble)'. Confidence intervals for constraint violation are shown to indicate statistical uncertainty in measurements.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_4", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig3.jpg", "caption": "Figure 1. Relative test loss versus the worst constraint violation with respect to SP. Relative losses are computed by subtracting the smallest baseline loss from the actual loss. For our algorithm and fair classification we plot the convex envelope of the predictors obtained on training data at various accuracy–fairness tradeoffs. We show $9 5 \\%$ confidence bands for the relative loss of our method and fair classification, and also show $9 5 \\%$ confidence intervals for constraint violation (the same for all methods). Our method dominates or matches the baselines up to statistical uncertainty on all datasets except adult, where fair classification is slightly better.", "figure_type": "plot", "evidence_from_figure": "Blue dashed line in 'Bottom plots only' section", "evidence_from_text": "Caption mentions '95% confidence intervals for constraint violation (the same for all methods)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.12843_1905.12843_fig_10_513", "query": "What does the red dashed line in the top plots of Figure 2 correspond to according to the legend?", "answer": "fair reg. (oracle=LS, model=tree ensemble)", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_10", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg", "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.", "figure_type": "plot", "evidence_from_figure": "Red dashed line marker in top plots legend", "evidence_from_text": "Text states baseline methods include unconstrained regressors, implying fair reg models are the primary focus", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.12843_1905.12843_fig_10_514", "query": "How does the unconstrained regressor model (green triangle) compare to the fair regression model (red dashed line) in terms of constraint violation?", "answer": "The unconstrained regressor (green triangle) shows 0 constraint violation (x=0), while the fair regression model (red dashed line) exhibits positive constraint violation due to fairness constraints.", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_10", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig9.jpg", "caption": "Figure 2. Training loss versus constraint violation with respect to DP. For our algorithm, we varied the fairness slackness parameter and plot the Pareto frontiers of the sets of returned predictors. For the logistic regression experiments, we also plot the Pareto frontiers of the sets of returned predictors given by fair classification reduction methods.", "figure_type": "plot", "evidence_from_figure": "Green triangle marker for unconstrained reg. (model=tree ensemble) in top plots legend", "evidence_from_text": "Text explains unconstrained regressors serve as baseline methods for comparison with fair models", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.12843_1905.12843_fig_13_515", "query": "In Figure 3, what is the number of oracle calls for the 'Fair reg. (oracle=LS, class=linear)' model when fairness slackness is 0.0?", "answer": "25.0", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_13", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg", "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.", "figure_type": "plot", "evidence_from_figure": "Red line data point at x=0.0 on the plot", "evidence_from_text": "Caption identifies y-axis as '# oracle calls' and x-axis as 'fairness slackness'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1905.12843_1905.12843_fig_13_516", "query": "According to Figure 3 and its caption, what is the minimum number of oracle calls for the 'Fair reg. (oracle=CS, class=linear)' model as fairness slackness approaches 1.0?", "answer": "Approximately 6", "doc_id": "1905.12843", "figure_id": "1905.12843_fig_13", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1905.12843/1905.12843/hybrid_auto/images/1905.12843_page0_fig12.jpg", "caption": "Figure 3. Number of oracle calls versus specified value of fairness slackness.", "figure_type": "plot", "evidence_from_figure": "Orange line plateaus at ~6 oracle calls for x ≥ 0.4", "evidence_from_text": "Caption defines axes to interpret the plateaued region", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_1_517", "query": "In Figure 1, how does the arrow from sensitive latents (b) to sensitive observations (a) enable subgroup demographic parity adjustments during test time as described in the text?", "answer": "The arrow from b to a shows that sensitive latents (b) reconstruct sensitive observations (a). The text explains that modifying b at test time allows the model to adjust for subgroup demographic parity without altering non-sensitive representations, enabling flexible fairness adjustments.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg", "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).", "figure_type": "diagram", "evidence_from_figure": "arrow from b to a", "evidence_from_text": "flexibly fair, meaning they can be easily modified at test time to achieve subgroup demographic parity", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_1_518", "query": "According to Figure 1 and the text, what role do non-sensitive latents (z) play in the model's reconstruction capability?", "answer": "Non-sensitive latents (z) reconstruct non-sensitive observations (x) via the arrow from z to x. The text states these latents form 'compact representations... useful for reconstruction and prediction,' ensuring the model maintains predictive utility while isolating sensitive attributes.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig0.jpg", "caption": "Figure 1. Data flow at train time (1a) and test time (1b) for our model, Flexibly Fair VAE (FFVAE).", "figure_type": "diagram", "evidence_from_figure": "arrow from z to x", "evidence_from_text": "compact representations of datasets that are useful for reconstruction and prediction", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_5_519", "query": "In Figure 2, which model achieves the highest accuracy at Δ_DP = 0.05? How does this relate to the text's description of MLP as a baseline classifier?", "answer": "FFVAE achieves the highest accuracy at Δ_DP = 0.05. This demonstrates that fairness-aware models like FFVAE outperform the baseline MLP (which shows the lowest accuracy for the same Δ_DP), indicating better tradeoffs between fairness and accuracy.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg", "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ", "figure_type": "plot", "evidence_from_figure": "FFVAE's blue line is the highest curve at Δ_DP = 0.05", "evidence_from_text": "MLP is described as a baseline classifier trained directly on input data", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_5_520", "query": "What is the role of the 'a' attribute mentioned in the figure caption when stating 'encoder outputs are modified to remove information about a'?", "answer": "The 'a' attribute is the sensitive demographic feature that models modify to ensure fairness. This aligns with the text's description of sensitive attributes in Communities & Crime (e.g., racePct-Black), where protected attributes are targeted for fairness constraints.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg", "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ", "figure_type": "plot", "evidence_from_figure": "Figure caption states 'encoder outputs are modified to remove information about a'", "evidence_from_text": "Text describes sensitive attributes like racePct-Black as protected demographic features", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_5_521", "query": "Why does the MLP baseline show the lowest accuracy across all Δ_DP values in Figure 2?", "answer": "MLP is trained directly on input data without fairness constraints, making it the unoptimized baseline. The text confirms it serves as a reference point, while other models incorporate fairness-aware modifications that improve accuracy for given fairness levels.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_5", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig4.jpg", "caption": "Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained directly on the input data. For each model, encoder outputs are modified to remove information about a. $y =$ XPosition for each plot.   ", "figure_type": "plot", "evidence_from_figure": "MLP's purple line is consistently the lowest curve", "evidence_from_text": "MLP is described as 'a baseline classifier trained directly on the input data'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_14_522", "query": "In Figure 4, which model achieves the highest accuracy at A_max=0.05, and what sensitive attributes are evaluated in this analysis according to the caption?", "answer": "FFVAE achieves the highest accuracy at A_max=0.05. The sensitive attributes are racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P).", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_14", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg", "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.", "figure_type": "plot", "evidence_from_figure": "The blue line (FFVAE) peaks highest at A_max=0.05 on the x-axis", "evidence_from_text": "Figure caption lists the sensitive attributes as racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_14_523", "query": "How does the β-VAE model's accuracy trend compare to FFVAE at A_max=0.15 in Figure 4, and what does the text imply about the role of A_max in fairness-accuracy tradeoffs?", "answer": "At A_max=0.15, β-VAE shows significantly lower accuracy than FFVAE. The text implies A_max represents the maximum allowed fairness violation across subgroups, where higher values relax fairness constraints.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_14", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig13.jpg", "caption": "Figure 4. Communities & Crime subgroup fairness-accuracy tradeoffs. Sensitive attributes: racePctBlack (R), blackPerCapIncome (B), and pctNotSpeakEnglWell (P). $y =$ violentCrimesPerCaptia.", "figure_type": "plot", "evidence_from_figure": "The red dashed line (β-VAE) is visibly lower than the blue solid line (FFVAE) at A_max=0.15", "evidence_from_text": "Caption describes A_max as part of 'subgroup fairness-accuracy tradeoffs' and the text implies it controls fairness constraints", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_29_524", "query": "What is the accuracy of FFVAE at Δ_pf = 0.15 in Figure 5, and how does this relate to the paper's claim about FFVAE's flexibility for subgroup fairness?", "answer": "The accuracy of FFVAE at Δ_pf = 0.15 is approximately 0.79. This high accuracy supports the paper's claim that FFVAE provides flexibly fair representations by demonstrating effective subgroup fairness across sensitive attributes like Chubby, Eyeglasses, and Male.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_29", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg", "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.", "figure_type": "plot", "evidence_from_figure": "FFVAE line value at Δ_pf = 0.15", "evidence_from_text": "Discussion section stating FFVAE's flexibility for multiple sensitive attributes", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_29_525", "query": "Why does FFVAE outperform β-VAE in accuracy across all Δ_pf values in Figure 5, according to the discussion?", "answer": "FFVAE's structured latent code allows compositional modification for multiple sensitive attributes and their conjunctions at test time, enabling better subgroup fair classification than β-VAE, which lacks this flexibility.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_29", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig28.jpg", "caption": "Figure 5. Celeb-A subgroup fair classification results. Sensitive attributes: Chubby (C), Eyeglasses (E), and Male (M). $y =$ Heavy-Makeup.", "figure_type": "plot", "evidence_from_figure": "Consistently higher FFVAE line across Δ_pf", "evidence_from_text": "Discussion section explaining FFVAE's method for flexible fair representations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_32_526", "query": "What does the black dashed line in Figure 6 represent according to the figure caption?", "answer": "The black dashed line indicates the mean MIG with outliers excluded.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_32", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg", "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "evidence_from_figure": "Black dashed line", "evidence_from_text": "Caption states 'Black dashed line indicates mean (with outliers excluded)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_32_527", "query": "According to the figure caption, which value of γ corresponds to the lightest colored line in Figure 6?", "answer": "γ = 100.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_32", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig31.jpg", "caption": "Figure 6. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 6a, each line is a different value of $\\gamma \\in [ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 7 0 , 1 0 0 ]$ , with brighter colors indicating larger values of $\\gamma$ . In Fig. 6b, each line is a different value of $\\alpha \\in [ 3 0 0 , 4 0 0 , 1 0 0 0 ]$ , with brighter colors indicating larger values of $\\alpha$ . Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (with outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "evidence_from_figure": "Lightest colored line", "evidence_from_text": "Caption states 'brighter colors indicating larger values of γ'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_34_528", "query": "What does the black dashed line represent in Figure 7, and how does it relate to the α values shown in the plot?", "answer": "The black dashed line represents the mean MIG (outliers excluded), serving as a central tendency measure across all α values (0, 50, 100, 150, 200) shown in the plot.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_34", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg", "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "evidence_from_figure": "black dashed line", "evidence_from_text": "Black dashed line indicates mean (outliers excluded)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_34_529", "query": "Which α value corresponds to the FactorVAE model in Figure 7, and what is its MIG at γ=50?", "answer": "α=0 corresponds to FactorVAE (text), and its MIG at γ=50 is approximately 0.5 (from the figure).", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_34", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg", "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "evidence_from_figure": "α=0 line's position at γ=50", "evidence_from_text": "α = 0 is equivalent to the FactorVAE", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1906.02589_1906.02589_fig_34_530", "query": "How does the MIG for α=100 change as γ increases from 0 to 100 in Figure 7, and why is this trend important for FFVAE performance?", "answer": "MIG decreases from ~0.6 to ~0.2 as γ increases (figure), indicating reduced disentanglement performance at higher γ values, which is critical for optimizing FFVAE's hyperparameters.", "doc_id": "1906.02589", "figure_id": "1906.02589_fig_34", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1906.02589/1906.02589/hybrid_auto/images/1906.02589_page0_fig33.jpg", "caption": "Figure 7. Mutual Information Gap (MIG) for various $( \\alpha , \\gamma )$ settings of the FFVAE. In Fig. 7a, each line is a different value of $\\alpha \\in [ 0 , 5 0 , 1 0 0 , 1 5 0 , 2 0 0 ]$ , with brighter colours indicating larger values of $\\alpha$ . In Fig. 7b, all combinations with $\\alpha , \\gamma > 0$ are shown. Models trained on DspritesUnfair, MIG calculated on Dsprites. Higher MIG is better. Black dashed line indicates mean (outliers excluded). $\\alpha = 0$ is equivalent to the FactorVAE.", "figure_type": "plot", "evidence_from_figure": "α=100 line's slope", "evidence_from_text": "Higher MIG is better; α values in FFVAE", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_2_531", "query": "Why does the overall recidivism rate for Black defendants (52%) exceed that of White defendants (39%) according to the figure and text?", "answer": "Black defendants are more frequently classified as Medium/High risk (58% vs. 33%), and the recidivism rate within each risk category is similar across races. The figure shows higher counts of Black defendants in the Medium/High risk category, which drives the higher overall rate.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg", "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .", "figure_type": "plot", "evidence_from_figure": "Higher total count of Black defendants in Medium/High risk category (taller stacked bars)", "evidence_from_text": "Text states Black are more likely to be classified as Medium/High risk (58% vs. 33%)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_2_532", "query": "How does the figure illustrate the claim that among non-reoffenders, Black defendants are more likely to be classified as Medium/High risk than White defendants?", "answer": "The 'Did not reoffend' (dark blue) segment in the Medium/High risk category is larger for Black defendants compared to White defendants in the figure, supporting the text's claim of 44.9% vs. 23.5% classification rates for non-reoffenders.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig1.jpg", "caption": "Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants ( $5 2 \\%$ vs. 39%), i.e. Y ✚⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. $Y \\perp \\perp A | \\hat { Y }$ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. $\\hat { Y } \\mathcal { M } A$ . Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to $2 3 . 5 \\%$ ). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28 $\\%$ ), i.e. ${ \\hat { Y } } { \\mathcal { H } } A | Y$ .", "figure_type": "plot", "evidence_from_figure": "Larger dark blue segment in Medium/High risk for Black vs. White", "evidence_from_text": "Text specifies 44.9% vs. 23.5% for non-reoffenders", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_3_533", "query": "In Figure 2, which nodes are the parents of Y, and how does this relate to the joint distribution formula described in the text?", "answer": "The parents of Y are F and M. According to the text, the joint distribution includes p(Y | F, M) as part of the product of conditional distributions.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg", "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.", "figure_type": "diagram", "evidence_from_figure": "Arrows pointing to Y (from F and M)", "evidence_from_text": "Definition of pa(X_i) and joint distribution as product of conditional distributions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_3_534", "query": "What is the conditional distribution for node F in Figure 2, based on the text's definition of pa(X_i)?", "answer": "The conditional distribution for F is p(F | A), as A is the parent of F according to the figure.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig2.jpg", "caption": "Fig. 2. Possible CBN underlying the dataset used for COMPAS.", "figure_type": "diagram", "evidence_from_figure": "Arrow from A to F", "evidence_from_text": "Text defines conditional distribution as p(X_i | pa(X_i))", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_4_535", "query": "In Figure 3(a), how does node C function as a confounder for the effect of A on Y, based on the arrow directions and the text's definition of confounders?", "answer": "Node C acts as a confounder because it has directed arrows to both A and Y (C→A and C→Y), creating a spurious association between A and Y. This aligns with the text's definition of confounders as variables influencing both the treatment (A) and outcome (Y).", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg", "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .", "figure_type": "diagram", "evidence_from_figure": "Arrows from C to A and C to Y", "evidence_from_text": "Definition of confounders as variables affecting both treatment and outcome", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_4_536", "query": "What is the joint probability distribution for the variables in Figure 3(a), using the conditional probabilities shown in the diagram and the text's description of Bayesian networks?", "answer": "The joint distribution is p(C) × p(A|C) × p(Y|C,A), as the text states that the joint distribution of all nodes in a Bayesian network is the product of all conditional distributions p(Xi | pa(Xi)).", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_4", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig3.jpg", "caption": "Fig. 3. (a): CBN with a confounder $C$ for the effect of $A$ on $Y$ . (b): Modified CBN resulting from intervening on $A$ .", "figure_type": "diagram", "evidence_from_figure": "Conditional probability labels p(C), p(A|C), and p(Y|C,A)", "evidence_from_text": "Text states joint distribution is product of conditional distributions", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_6_537", "query": "What specific paths are closed and opened by conditioning on C in Fig 4(a) as described in the caption?", "answer": "Conditioning on C closes paths A-C-X-Y and A-C-Y but opens path A-E-C-X-Y.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg", "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .", "figure_type": "diagram", "evidence_from_figure": "Arrows showing paths A→C→X→Y, A→C→Y, and A→E→C→X→Y", "evidence_from_text": "Caption stating 'conditioning on C closes the paths A C X Y and A C Y but opens the path A E C X Y'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_6_538", "query": "How does Fig 4(a) demonstrate the application of the back-door adjustment formula mentioned in the text?", "answer": "Conditioning on C blocks the back-door paths A-C-X-Y and A-C-Y, enabling the back-door adjustment formula to isolate the causal effect of A on Y.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_6", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig5.jpg", "caption": "Fig. 4. (a): CBN in which conditioning on $C$ closes the paths $A  C  X  Y$ and $A \\left. C \\right. Y$ but opens the path $A \\left. E \\right.$ $C \\left. X \\right. Y$ . (b): CBN with one direct and one indirect causal path from $A$ to $Y$ .", "figure_type": "diagram", "evidence_from_figure": "Structure showing paths affected by conditioning on C", "evidence_from_text": "Text stating 'i.e. we retrieve the back-door adjustment formula' and explanation of path blocking", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_8_539", "query": "In Figure 5 (bottom), how is the path-specific effect (PSE) calculated using the annotated path coefficients?", "answer": "The PSE is calculated by summing the products of path coefficients for all relevant causal paths (A→Y, A→M→Y, and A→M→L→Y), as specified in the text's explanation of path-specific effects.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg", "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).", "figure_type": "diagram", "evidence_from_figure": "Bottom diagram's annotated path coefficients (e.g., θ^m_C, θ^l_M, θ^y_L)", "evidence_from_text": "Text stating 'summing over the three causal paths of interest the product of all coefficients in each path'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_8_540", "query": "What do the red arrows in Figure 5 (top) represent in the context of path-specific effects?", "answer": "The red arrows highlight the direct path A→Y and indirect paths through M (A→M→Y and A→M→L→Y), which are the specific causal paths targeted for effect estimation in the paper's path-specific effect framework.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig7.jpg", "caption": "Fig. 5. Top: CBN with the direct path from $A$ to $Y$ and the indirect paths passing through $M$ highlighted in red. Bottom: CBN corresponding to Eq. (1).", "figure_type": "diagram", "evidence_from_figure": "Red arrows connecting A→Y, A→M→Y, and A→M→L→Y in the top diagram", "evidence_from_text": "Text describing isolation of effects along 'direct path A→Y and the indirect paths passing through M'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_10_541", "query": "In Fig. 6, why does the CBN show that Y is not independent of A, as mentioned in the text?", "answer": "The CBN shows an open back-door path A → X → Y, which causes Y to depend on A. The text explains this violates Y ⊥⊥ A due to the unblocked path from A to Y.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_10", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg", "caption": "Fig. 6. CBN underlying a music degree scenario.", "figure_type": "diagram", "evidence_from_figure": "Arrows A → X and X → Y", "evidence_from_text": "Text states 'Y ✚⊥⊥A due to the open back-door path from A to Y'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_10_542", "query": "According to the text, how does using A in addition to X help retrieve M in the CBN of Fig. 6?", "answer": "The text states that M is a common cause of X and Y; using A with X provides information about M, as M {AC} | X (implying conditional independence) but A helps disentangle M's influence on the model.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_10", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig9.jpg", "caption": "Fig. 6. CBN underlying a music degree scenario.", "figure_type": "diagram", "evidence_from_figure": "M → X and M → Y connections", "evidence_from_text": "Text states 'using A in addition to X can give information about M' and 'M {AC} | X'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_11_543", "query": "What does the dashed arrow from A to D in Figure 7 represent in the context of the twin Bayesian network described in the text?", "answer": "The dashed arrow represents the causal path where A influences D, which is part of the counterfactual analysis in the twin Bayesian network. The text explains that interventions on A→D (alongside A→Y) are used to model scenarios where A is set to different values (e.g., A = a vs. A = a̅) for fairness evaluation.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_11", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg", "caption": "Fig. 7. CBN underlying a college admission scenario.", "figure_type": "diagram", "evidence_from_figure": "Dashed arrow from A to D", "evidence_from_text": "Discussion of twin Bayesian network and interventions on A→D", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_11_544", "query": "Why is the arrow from A to Y colored red in Figure 7, and how does this relate to the fairness analysis in the text?", "answer": "The red arrow highlights the direct causal path from A to Y, which the text identifies as the path-specific effect (PSE) used to quantify unfairness. The text states that unfairness in this scenario is measured by the effect along this direct path, as it represents the portion of A's influence on Y not mediated through other variables like D.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_11", "figure_number": 7, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig10.jpg", "caption": "Fig. 7. CBN underlying a college admission scenario.", "figure_type": "diagram", "evidence_from_figure": "Red arrow from A to Y", "evidence_from_text": "Explanation of path-specific effect (PSE) along A→Y for fairness quantification", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_13_545", "query": "In Figure 8, what is the directed path from X₁ to X₄ as defined in the text?", "answer": "The directed path is X₁ → X₃ → X₄. The text defines a directed path as a sequence of linked nodes starting at X_i and ending at X_j.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_13", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg", "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.", "figure_type": "diagram", "evidence_from_figure": "Arrows from X₁ to X₃ and X₃ to X₄", "evidence_from_text": "Definition of directed path in the text before the figure", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.06430_1907.06430_fig_13_546", "query": "According to Appendix A, what property must a Bayesian network graph satisfy, and does Figure 8 meet this requirement?", "answer": "Bayesian networks require directed acyclic graphs (DAGs). Figure 8 is labeled as (a) acyclic in the caption, so it satisfies the requirement.", "doc_id": "1907.06430", "figure_id": "1907.06430_fig_13", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.06430/1907.06430/hybrid_auto/images/1907.06430_page0_fig12.jpg", "caption": "Fig. 8. Directed (a) acyclic and (b) cyclic graph.", "figure_type": "diagram", "evidence_from_figure": "Graph is acyclic (no cycles)", "evidence_from_text": "Appendix A context on Bayesian networks as directed graphs", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.09013_1907.09013_fig_1_547", "query": "In Figure 2, which CRISP-DM step is most associated with 'data issues' as a cause of classifier discrimination according to the text?", "answer": "Data Understanding, Data Preparation, and Modeling steps (as these are part of the 'building process' where data issues occur per the text)", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg", "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ", "figure_type": "diagram", "evidence_from_figure": "CRISP-DM steps listed in the diagram", "evidence_from_text": "Text states: 'For each procedural step of the building process, we categorize the discrimination by cause: either data issues or misspecification.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.09013_1907.09013_fig_1_548", "query": "How does the 'Deployment' step in Figure 2 relate to 'procedural failures' mentioned in the text?", "answer": "Deployment corresponds to 'procedural failures' as it is the phase where the classifier is used, and the text specifies procedural failures are part of the taxonomy for classifier usage.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_1", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig0.jpg", "caption": "Figure 2: Commonly cited causes of classifier discrimination.   ", "figure_type": "diagram", "evidence_from_figure": "Deployment step in the diagram", "evidence_from_text": "Text states: 'When we consider how the classifier is used, we include in our taxonomy 'procedural failures'...' (referring to the usage phase)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.09013_1907.09013_fig_2_549", "query": "How does the 'Discrim Potential?' decision point in Figure 3 address the text's warning about failing to test for discriminatory behavior?", "answer": "The 'Discrim Potential?' decision points (two instances in the flowchart) enforce mandatory discrimination testing at critical stages (after data unit tests and after classifier unit tests), directly addressing the text's warning that 'the greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior.'", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_2", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg", "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "figure_type": "diagram", "evidence_from_figure": "Two 'Discrim Potential?' diamond-shaped decision nodes in the flowchart", "evidence_from_text": "Text statement: 'The greatest error one can make during evaluation is to not test an algorithm for potential discriminatory behavior.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.09013_1907.09013_fig_2_550", "query": "What do the dotted lines in Figure 3 represent according to the text?", "answer": "The dotted lines represent optional workflows for pre-processing, in-processing, and post-processing techniques, as stated in the figure's caption: 'Dotted arrows and figures represent optional workflows, representing pre-processing, in-processing, and post-processing techniques, as suggested by the prior art.'", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_2", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg", "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "figure_type": "diagram", "evidence_from_figure": "Dotted arrows connecting elements like Discrim Aware Pre-processing and Discrim Reducing Post-Processing", "evidence_from_text": "Figure caption: 'Dotted arrows and figures represent optional workflows...'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.09013_1907.09013_fig_2_551", "query": "Which step in Figure 3 corresponds to the 'augmented cost functions' mentioned in the text?", "answer": "The 'Discrim Aware Training' step (under Standard Training) corresponds to augmented cost functions, as it represents in-processing modifications where fairness regularizers augment the standard loss function, aligning with the text's description of Zemel et al. introducing fairness regularizers.", "doc_id": "1907.09013", "figure_id": "1907.09013_fig_2", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.09013/1907.09013/hybrid_auto/images/1907.09013_page0_fig1.jpg", "caption": "Figure 3. Zemel et al. and Kamishima et al. both introduce augmented cost functions, where they augment a standard log-likelihood loss function with a ‘fairness’ regularizer11,19. These regularizers take into account differences in how the learning algorithm classifies protected vs. non-protected classes (i.e. $S ^ { I }$ vs $S ^ { \\boldsymbol { 0 } }$ ) and penalizes the total loss based on the extent of the difference. Much like in standard L1/L2 regularization, the penalization is controlled by a tunable hyper-parameter. Zemel et al give examples on ways to tune hyper-parameters to achieve different outcomes. Other in-processing techniques involve discrimination specific modifications of traditional learning algorithms, such as Naïve Bayes and Decision Trees9,10.", "figure_type": "diagram", "evidence_from_figure": "Dotted line connecting 'Discrim Aware Pre-processing' to 'Discrim Aware Training' box", "evidence_from_text": "Text: 'Zemel et al. and Kamishima et al. both introduce augmented cost functions... penalizes the total loss based on the extent of the difference.'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.12059_1907.12059_fig_8_552", "query": "In Figure 1's top panel, how do the Black males and White males histograms differ before training?", "answer": "Black males show a higher concentration of beliefs near 0.0 with a steeper peak, while White males have a more spread-out distribution with lower peak height, indicating initial bias.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_8", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg", "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ", "figure_type": "plot", "evidence_from_figure": "Visual comparison of histogram shapes for Black males vs White males in top panel", "evidence_from_text": "Text states groups' histograms converge to barycenter after training with α=0, β=100", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.12059_1907.12059_fig_8_553", "query": "What does the barycenter represent in the context of the Wasserstein-1 penalty method?", "answer": "The barycenter serves as the target distribution that each group's histogram converges to after training, enabling independence from sensitive attributes while minimizing model decision modifications.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_8", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg", "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ", "figure_type": "plot", "evidence_from_figure": "Barycenter line in initial state histogram", "evidence_from_text": "Text explains barycenter enables 'reaching independence with minimal modifications'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.12059_1907.12059_fig_8_554", "query": "What specific parameters caused the group histograms to match the barycenter in Figure 1's bottom panel?", "answer": "α=0 and β=100 were used during 10,000 training steps to align group histograms with the barycenter.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_8", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig7.jpg", "caption": "Figure 1: Histograms of model beliefs for groups of Black females, Black males, White females, and White males, and their barycenter on the Adult dataset using Wass-1 Penalty. Top: Initial state. Bottom: After 10,000 training steps with $\\alpha = 0 , \\beta = 1 0 0$ each group histogram matches the barycenter.   ", "figure_type": "plot", "evidence_from_figure": "Caption states 'after 10,000 training steps with α=0, β=100'", "evidence_from_text": "Text describes 'Wasserstein-1 constrained method' with these parameters", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.12059_1907.12059_fig_11_555", "query": "Why does integrating |f⁻¹ - g⁻¹| along the x-axis (left) and |f - g| along the y-axis (right) in Figure 3 both compute the same shaded area?", "answer": "The text states that both integrals compute the area of the shaded region between f and g, as they represent two equivalent ways of measuring the area (Eq. 12). The figure visually confirms the shaded region is identical for both integration methods.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_11", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg", "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).", "figure_type": "plot", "evidence_from_figure": "Shaded region between curves f and g", "evidence_from_text": "Text states 'both compute the area of the same shaded region, thus the equality in Eq. (12)'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1907.12059_1907.12059_fig_11_556", "query": "What property of invertible CDFs ensures the area calculation in Figure 3 is valid for both integrals?", "answer": "The text specifies that invertible CDFs are strictly increasing (bijective and non-decreasing), which ensures f⁻¹ and g⁻¹ exist and the area between f and g can be measured via either axis. The figure's curves visually demonstrate strict monotonicity.", "doc_id": "1907.12059", "figure_id": "1907.12059_fig_11", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1907.12059/1907.12059/hybrid_auto/images/1907.12059_page0_fig10.jpg", "caption": "Figure 3: Integrating $| f ^ { - 1 } - g ^ { - 1 } |$ along the $x$ axis (left) and integrating $| f - g |$ along the $y$ axis (right) both compute the area of the same shaded region, thus the equality in Eq. (12).", "figure_type": "plot", "evidence_from_figure": "Strictly increasing curves f and g", "evidence_from_text": "Text states 'Invertible CDFs f, g are strictly increasing functions due to being bijective and non-decreasing'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1909.05088_1909.05088_fig_1_557", "query": "What percentage of speakers in the 20-30 age group are female according to Figure 1, and why does the text state this age group represents only a small percentage of the total sentences?", "answer": "Figure 1 shows approximately 60% female speakers in the 20-30 age group. The text explains this group constitutes a small portion of the total dataset despite its high female representation, indicating underrepresentation in the overall data volume.", "doc_id": "1909.05088", "figure_id": "1909.05088_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg", "caption": "Figure 1: Percentage of female and male speakers per age group", "figure_type": "plot", "evidence_from_figure": "Blue segment in 20-30 bar (≈60% female)", "evidence_from_text": "Text states: 'with the exception of the youngest age group (20–30), which represents only a very small percentage of the total amount of sentences'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1909.05088_1909.05088_fig_1_558", "query": "How does the male dominance in the 70-80 age group (Figure 1) contribute to the gender imbalance in the Europarl dataset mentioned in the text?", "answer": "Figure 1 shows the 70-80 age group is approximately 80% male. This male dominance contributes to the dataset's gender imbalance, which the text states will cause MT systems trained on this data to produce biased translations favoring male speakers.", "doc_id": "1909.05088", "figure_id": "1909.05088_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1909.05088/1909.05088/hybrid_auto/images/1909.05088_page0_fig0.jpg", "caption": "Figure 1: Percentage of female and male speakers per age group", "figure_type": "plot", "evidence_from_figure": "Orange segment in 70-80 bar (≈80% male)", "evidence_from_text": "Text states: 'the gender unbalance in the Europarl dataset, which will be reflected in the translations that MT systems trained on this data produce'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_1_559", "query": "In Figure 1, which sentence shows a PERSON entity incorrectly tagged as MISC?", "answer": "Sentence 3 ('Isabel is sleeping') is tagged as MISC but should be PERSON.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg", "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.", "figure_type": "example", "evidence_from_figure": "Sentence 3 labeled with 'MISC' tag", "evidence_from_text": "Text states CoreNLP misclassifies PERSON entities (Introduction section)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_1_560", "query": "Which model is evaluated in Figure 1 according to the text?", "answer": "CoreNLP version 3.9 (as specified in the 'Models and Experiments' section).", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig0.jpg", "caption": "Figure 1: Examples of PERSON entities that are wrongfully tagged as non-PERSON or NULL entities by CoreNLP.", "figure_type": "example", "evidence_from_figure": "Figure caption mentions 'CoreNLP' as the model", "evidence_from_text": "Text lists CoreNLP version 3.9 in the 'Models and Experiments' section", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_31_561", "query": "What is the error rate for female names in 1955 according to Figure 3 using template #4?", "answer": "Approximately 0.35", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_31", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg", "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.", "figure_type": "plot", "evidence_from_figure": "The purple line at 1955 on the x-axis aligns with ~0.35 on the y-axis", "evidence_from_text": "Text specifies 'unweighted cases using template #4' as the data context", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_31_562", "query": "Why do female names have higher error rates than male names except for four marginal cases?", "answer": "The figure shows female error rates consistently above male rates, with four exceptions. The text attributes this to model performance differences in processing gendered names, though specific causes are not detailed in the provided text.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_31", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig30.jpg", "caption": "Figure 3: Results from different models over the 139-year history of baby names from the census data on different error types 0.020.031875 1895 1915 1935 1955 1975 1995 2015 1875 1895 1915 1935 1955 1975 1995 2015for the unweighted cases using template #4. Female names have higher error rates for all cases except four marginal cases. The 0.010.1 y axis shows the calculated error rates for each of the error types as described in their corresponding formulas, and the x axis 1875 1895 10.080.09represents the year in which the baby name was given.", "figure_type": "plot", "evidence_from_figure": "Female line (purple) dominates male line (cyan) across most years", "evidence_from_text": "Text states 'Female names have higher error rates for all cases except four marginal cases'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_35_563", "query": "In Figure 4, which TEMPL model shows the highest performance in 2015, and what does the text state about the implications of such trends regarding data bias?", "answer": "TEMPL5 shows the highest performance in 2015. The text states that data bias in training sets can lead to biased model behavior, and the observed trends indicate that datasets may not reflect real-world demographics.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_35", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg", "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.", "figure_type": "plot", "evidence_from_figure": "The light blue line (TEMPL5) peaks highest at 2015", "evidence_from_text": "The text explains that biased datasets directly affect fairness constraints and that results indicate datasets 'do not reflect the real world'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_35_564", "query": "How does the trend of TEMPL4's performance from 1875 to 2015 in Figure 4 align with the text's assertion that context in some templates helped some models but negatively affected others?", "answer": "TEMPL4's performance shows a steady decline until the 1970s followed by a slight increase. The text explains that context in templates can have varying effects, where some models benefit from certain templates while others are negatively impacted.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_35", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig34.jpg", "caption": "Figure 4: Performance of different models on different templates from our benchmark for female and male names collected for 0.25 Male CoreNLPover 139 years. Notice how context in some of the templates helped some models, but showed negative effects on other models.", "figure_type": "plot", "evidence_from_figure": "The yellow line (TEMPL4) shows a downward trend until 1975 then gradual rise", "evidence_from_text": "The text states 'context in some of the templates helped some models, but showed negative effects on other models'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_41_565", "query": "Why does Version 3.9 show higher error rates after 1995 compared to Version 3.8 for Error Type-2?", "answer": "Version 3.9's higher error rates stem from its attempt to tag more entities, which caused more PERSON entities to be misclassified as non-PERSON entities, degrading Error Type-2 performance as explained in the text.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_41", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg", "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy", "figure_type": "plot", "evidence_from_figure": "Steeper upward trend of Version 3.9 (orange line) after 1995", "evidence_from_text": "Text states the update caused more PERSON entities to be tagged as non-PERSON, degrading Error Type-2", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_41_566", "query": "How does the degradation in Error Type-2 performance affect female names versus male names?", "answer": "The degradation affected more female names than males on average, as stated in the text, while the figure shows the overall error rate increase for Version 3.9.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_41", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig40.jpg", "caption": "Figure 5: Version update in the CoreNLP model tried to tag more entities and thus a subtle boost in performance with regard to 1875 1895 1915 1935 1955 1975 1995 2015  1875 1895 1915 1935 1955 1975 1995 2015Error Type-3. However, this resulted in more PERSON entities being tagged as non-PERSON entities. This then degraded per-Male CoreNLP 0.2 Male CoreNLPformance with regard to Error Type-2 in the newer version, and overall no change in the Error Type-1 case which is considered 0.12 Version 3.8Version 3.9 0.18 Version 3.8Version 3.9to be the super-set. We observe how the degrade in Error Type-2 affected more female names than males on average.0.8 Female Medium Spacy", "figure_type": "plot", "evidence_from_figure": "Higher error rates for Version 3.9 post-1995 indicating degradation", "evidence_from_text": "Text explicitly states 'the degrade in Error Type-2 affected more female names than males on average'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_44_567", "query": "In Figure 6, how does the trend of the yellow line (V2.1) relate to the text's claim that Error Type-2 performance degraded in the newer version?", "answer": "The yellow line (V2.1) consistently shows higher bias against female names compared to V2.0, which aligns with the text stating that V2.1's degradation in Error Type-2 (related to PERSON entity tagging) caused increased bias against female names.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_44", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg", "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.", "figure_type": "plot", "evidence_from_figure": "Yellow line (V2.1) consistently above green line (V2.0) across all years", "evidence_from_text": "Text states 'degraded per-Male CoreNLP performance with regard to Error Type-2 in the newer version'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_44_568", "query": "How does the caption's claim that 'bias against female names was on average twice that of male names' in Figure 6 connect to the text's explanation of 'more PERSON entities being tagged as non-PERSON entities'?", "answer": "The increased bias shown in Figure 6 (V2.1's higher female bias) directly results from V2.1 tagging more PERSON entities as non-PERSON entities, as explained in the text, which disproportionately affected female name recognition.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_44", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig43.jpg", "caption": "Figure 6: Biased performance of version 2.1 over 2.0 in 0 1875 1895 1915 1935 1955 1975 1995 2015Spacy medium. The bias against female names was on average twice that of male names.", "figure_type": "plot", "evidence_from_figure": "Yellow line (V2.1) showing greater bias against female names", "evidence_from_text": "Text states 'this resulted in more PERSON entities being tagged as non-PERSON entities'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_60_569", "query": "What is the error rate for female names in 2015 according to Figure 8?", "answer": "Approximately 0.22", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_60", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg", "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.", "figure_type": "plot", "evidence_from_figure": "Female line at 2015 on y-axis", "evidence_from_text": "y-axis represents calculated error rates for weighted case using template #5", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_60_570", "query": "Does Figure 8 support the text's claim that female names have higher error rates for all cases?", "answer": "Yes, the female line remains above the male line across all years shown", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_60", "figure_number": 8, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig59.jpg", "caption": "Figure 8: Results from different models ran over 139-year history of baby names from the census data on different error types 1875 1895 1915 1935 1955 1975 1995 2015for the weighted case using template # 5. Female names have higher error rates for all the cases. The y axis shows the calculated 0.08  0.12 error rates for each of the error types as described in their corresponding formulas, and the x axis represents the year in which 0.07 the baby name was given.", "figure_type": "plot", "evidence_from_figure": "Female line consistently above male line", "evidence_from_text": "Text states 'Female names have higher error rates for all the cases'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_79_571", "query": "Why does the text state that 'Overall performance is more biased towards female names' based on Figure 9?", "answer": "The text states this because the female error rate line in Figure 9 is consistently higher than the male line across most years, indicating a systemic bias in error rates for female names.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_79", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg", "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "figure_type": "plot", "evidence_from_figure": "Female line above male line for most years", "evidence_from_text": "Overall performance is more biased towards female names", "requires_figure": true, "requires_text": true}
{"query_id": "l1_1910.10872_1910.10872_fig_79_572", "query": "What does the text mean by 'three marginal cases' where female names have lower error rates than male names in Figure 9?", "answer": "The text refers to three specific instances where female error rates dipped below male rates, though the figure does not explicitly identify these cases, implying they are minor exceptions to the overall trend.", "doc_id": "1910.10872", "figure_id": "1910.10872_fig_79", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/1910.10872/1910.10872/hybrid_auto/images/1910.10872_page0_fig78.jpg", "caption": "Figure 9: Results from different models ran over 139-year history of baby names from the census data on different error typesfor the unweighted case using template #5. Female names have higher error rates for all the cases except three marginal cases. 0.08 Overall performance is more biased towards female names. The y axis shows the calculated error rates for each of the error 1875 1895 1915 1935 1955 1975 1995 20150.07 types as described in their corresponding formulas, and the x axis represents the year in which the baby name was given.", "figure_type": "plot", "evidence_from_figure": "Female line mostly above male line", "evidence_from_text": "Female names have higher error rates for all the cases except three marginal cases", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_1_573", "query": "In Figure 1, how does the Equity panel's platform configuration differ from Equality, and what does this represent in the context of the text's definition of equity?", "answer": "The Equity panel adds a platform for the yellow figure to equalize apple access, while Equality uses identical platforms. This represents compensating for historical disadvantages (text: 'historically disadvantaged groups are compensated') to achieve equitable outcomes.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg", "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.", "figure_type": "diagram", "evidence_from_figure": "yellow figure's elevated platform in Equity vs. no platform in Equality", "evidence_from_text": "text: 'Equity is the distribution of resources among groups to overcome obstacles... historically disadvantaged groups are compensated'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_1_574", "query": "What do the equations under the Equity panel in Figure 1 formalize, and how does this align with the text's operationalization of equity?", "answer": "The equations formalize that p(Ŷ|A = color) = p(Y|A = color) for all groups, meaning predictions match actual outcomes after compensation. This aligns with the text's 'compensate for biases in historical data' approach to operationalize equity.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig0.jpg", "caption": "Figure 1: Notion of equality in fairness is depicted and formalized along with our newly formalized notion of equity.", "figure_type": "diagram", "evidence_from_figure": "p(Ŷ|A = blue) + p(Y|A = blue) = ... equations for Equity", "evidence_from_text": "text: 'compensate for them in the predictions generated by the model'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_3_575", "query": "At Beta=0.9, how does the accuracy of the Parity method compare to the Classifier method in Figure 2, and what does this imply about fairness gain according to the text?", "answer": "At Beta=0.9, the Parity method's accuracy (~83%) is lower than the Classifier's (~85%). According to the text, fairness gain measures effectiveness in reducing disparities; thus, lower accuracy for Parity suggests higher fairness gain as it likely enforces stronger fairness constraints.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg", "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.", "figure_type": "plot", "evidence_from_figure": "Parity's accuracy value at Beta=0.9 (red X marker)", "evidence_from_text": "Definition of fairness gain as 'measuring how effective a method was in reducing disparities among demographics compared to a classifier with no fairness constraint'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_3_576", "query": "Why are the 10 random splits mentioned in the text significant for interpreting the data points in Figure 2?", "answer": "The 10 random splits ensure consistent experimental conditions across different Beta values, allowing reliable comparison of results. Each data point in Figure 2 represents the average of 10 experiments on these splits, making the results robust and comparable.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig2.jpg", "caption": "Figure 2: Accuracy and fairness gain results for the COMPAS and Adult datasets over different $\\beta$ values. Top plots report the accuracy results, while bottom plots report the fairness gain results. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values. For details of these values along with standard deviation numbers refer to Tables 7 and 8 in the Appendixes section.", "figure_type": "plot", "evidence_from_figure": "Data points as averages of 10 experiments", "evidence_from_text": "Statement that 'the 10 random split sets are the same across different Beta values'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_9_577", "query": "In Figure 3, why does the Equity line show a more significant reduction in bias compared to Parity over iterations?", "answer": "The text states that Equity is more effective in reducing bias over iterations, which is visually evident as the Equity line consistently lies below the Parity line across all iterations.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg", "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "figure_type": "plot", "evidence_from_figure": "Equity line consistently below Parity line across iterations", "evidence_from_text": "Equity is more effective in reducing the bias over iterations", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_9_578", "query": "How many experiments were averaged to produce the data points in Figure 3?", "answer": "The caption states each data point is the average of 10 experiments performed on 10 random splits.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg", "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "figure_type": "plot", "evidence_from_figure": "Data points on the plot", "evidence_from_text": "Each point on the plots is the average value of 10 experiments performed on the 10 random splits", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_9_579", "query": "What does the caption state about the relationship between higher β values and bias reduction in Figure 3?", "answer": "The caption explains that higher β values result in more bias reduction for both Equity and Parity fairness objectives.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig8.jpg", "caption": "Figure 3: Simulation of the feedback loop phenomenon and results obtained in reduction of bias via different methods in COMPAS and Adult datasets. As expected higher $\\beta$ values result in reduction of more bias in the two fairness based objectives (Equity and Parity). It also shows how Equity is more effective in reducing the bias over iterations. Each point on the plots is the average value of 10 experiments performed on the 10 random splits. Notice that the 10 random split sets are the same across different $\\beta$ values.", "figure_type": "plot", "evidence_from_figure": "Figure 3 context", "evidence_from_text": "As expected higher β values result in reduction of more bias in the two fairness based objectives (Equity and Parity)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_13_580", "query": "Why does the Equity violin plot show a higher median human rating than Parity in Figure 4?", "answer": "The Equity plot's median line is positioned higher (closer to 3-4) than Parity's median (closer to 1-2), indicating stronger perceived fairness. This aligns with the text's context that Equity reduces bias in feedback loops, as supported by Table 4's Mann-Whitney U test results.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_13", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg", "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.", "figure_type": "plot", "evidence_from_figure": "Median line positions in Equity vs. Parity violin plots", "evidence_from_text": "Text describing Equity's effectiveness in reducing bias and Table 4's statistical significance", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2005.07293_2005.07293_fig_13_581", "query": "How does the distribution of human ratings for Equity in Figure 4 relate to the statistical significance in Table 4?", "answer": "The Equity plot's narrower distribution (less variability) and higher median ratings visually support the Mann-Whitney U test results in Table 4, which show significant improvements in fairness metrics for Equity compared to Parity.", "doc_id": "2005.07293", "figure_id": "2005.07293_fig_13", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2005.07293/2005.07293/hybrid_auto/images/2005.07293_page0_fig12.jpg", "caption": "Figure 4: Human ratings of equity and parity notions of fairness in different scenarios.", "figure_type": "plot", "evidence_from_figure": "Narrower distribution and higher median in Equity violin plot", "evidence_from_text": "Table 4's description of Mann-Whitney U test results for COMPAS/Adult datasets", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_2_582", "query": "Which category in Figure 1 shows the highest positive sentiment outlier, and how does this relate to the text's claim about overgeneralization issues?", "answer": "The 'Profession' category shows the highest positive sentiment outlier (near 30%). The text explains this as evidence of overgeneralization, where target groups like 'lawyer' exhibit extreme sentiment due to biased triples in ConceptNet.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg", "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.", "figure_type": "plot", "evidence_from_figure": "Blue box plot for 'Profession' with outlier near 30%", "evidence_from_text": "Text states 'outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_2_583", "query": "How does the variation in 'Gender' box plots in Figure 1 correspond to the text's explanation of 'large variation/disparity in the number of negative or positive triples'?", "answer": "The 'Gender' category shows significant spread in both negative and positive sentiment ranges (wide box plots). The text explains this variation as reflecting disparity in the number of triples, indicating inconsistent sentiment representation for gender-related targets.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_2", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig1.jpg", "caption": "Figure 1: Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.", "figure_type": "plot", "evidence_from_figure": "Wide box plots for 'Gender' with overlapping negative/positive ranges", "evidence_from_text": "Text states 'large variation/disparity in the number of negative or positive triples for groups in the same category'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_9_584", "query": "What specific negative and positive associations for 'woman' are shown in Figure 3's Gender category, and how does the text define overgeneralization in this context?", "answer": "The figure shows 'Slut is related to woman' (negative sentiment) and 'Beauty is related to woman' (positive sentiment). The text defines overgeneralization in the Gender category as both target groups having extreme associations (both negative and positive) detected by sentiment.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg", "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.", "figure_type": "plot", "evidence_from_figure": "Text annotations 'Slut is related to woman' (negative) and 'Beauty is related to woman' (positive) with corresponding sentiment bars", "evidence_from_text": "Text states 'in 'Gender' category both target groups are overgeneralized'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_9_585", "query": "How does the sentiment distribution for 'man' in Figure 3's Gender category support the text's claim that both target groups are overgeneralized?", "answer": "The figure shows 'Cock slave is related to man' (negative sentiment) and 'man is related to integrity' (positive sentiment), demonstrating that 'man' has both negative and positive associations, aligning with the text's claim of overgeneralization for both genders.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_9", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig8.jpg", "caption": "Figure 3: Four different representations from four categories each demonstrating a certain aspect of bias. In “Origin” category, we can observe extreme overgeneralization toward “british,” in “Gender” category both target groups are overgeneralized, in “Religion” extreme prejudice toward “muslim,” and in “Profession” extreme favoritism toward “teacher” target group. Each case is accompanied with an example of negative and positive associations detected by sentiment.", "figure_type": "plot", "evidence_from_figure": "Text annotations 'Cock slave is related to man' (negative) and 'man is related to integrity' (positive) with sentiment bars", "evidence_from_text": "Text states 'in 'Gender' category both target groups are overgeneralized'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_17_586", "query": "Which category in Figure 5 shows the highest variance in Negative regard (red box plots), and what does the text state about the effect of the mitigation technique on such variances?", "answer": "Religion; the text states that the mitigation technique reduces overgeneralization, which is indicated by variance in the figure.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_17", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg", "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.", "figure_type": "plot", "evidence_from_figure": "Religion's red box plot has the widest range (from ~10 to 50) and highest outlier.", "evidence_from_text": "Text states \"variances exist... which is an indication of disparity in overgeneralization\" and \"COMeT-Filtered is effective at reducing overgeneralization\"", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_17_587", "query": "How does the Positive regard distribution for the Gender category in Figure 5 relate to the concept of overgeneralization as explained in the text?", "answer": "The narrow distribution indicates low variance, suggesting minimal overgeneralization for the Gender category.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_17", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig16.jpg", "caption": "Figure 5: Negative and positive sentiment and regard results from COMeT and CSG.", "figure_type": "plot", "evidence_from_figure": "Gender's blue box plot has a small interquartile range with minimal outliers.", "evidence_from_text": "Text states \"variances exist... which is an indication of disparity in overgeneralization\"", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_20_588", "query": "Which relation types in Figure 6 exemplify representational harms as discussed in the text?", "answer": "The 'AtLocation' and 'CapableOf' relations in Figure 6 (e.g., 'child molesters → AtLocation → church') exemplify representational harms by encoding biased associations between groups and locations/attributes, as defined in the text's discussion of ConceptNet's role in machine learning bias.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_20", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg", "caption": "Figure 6: Examples from ConceptNet.   ", "figure_type": "example", "evidence_from_figure": "Presence of 'AtLocation' and 'CapableOf' relations in triples", "evidence_from_text": "Section discussing representational harms in Barocas et al. (2017)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_20_589", "query": "How do the 'AtLocation' examples in Figure 6 (e.g., 'corrupt politicians → AtLocation → Greece') relate to the mitigation framework mentioned after the figure?", "answer": "These examples illustrate how biased geographic associations (e.g., corrupt politicians linked to Greece) could be filtered by the COMet_Filtered framework described in the text, which aims to reduce representational harms through data filtering.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_20", "figure_number": 6, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig19.jpg", "caption": "Figure 6: Examples from ConceptNet.   ", "figure_type": "example", "evidence_from_figure": "Specific 'AtLocation' triples with geographic targets", "evidence_from_text": "Section on 'Mitigation Framework' referencing Figure 7", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_26_590", "query": "In Figure 9, why is 'girlfriend' categorized as 'Favoritism' despite high positive sentiment? What does the text imply about Favoritism's definition?", "answer": "The figure shows 'girlfriend' in the Favoritism region (blue) with high positive sentiment (~60%) and low negative sentiment. The text implies Favoritism corresponds to terms with significantly higher positive sentiment scores, indicating bias toward positive associations.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_26", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg", "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ", "figure_type": "plot", "evidence_from_figure": "Position of 'girlfriend' in high-positive/low-negative quadrant", "evidence_from_text": "Caption describing 'Favoritism' as a sentiment-based region", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_26_591", "query": "Which terms fall into the 'Both' category in Figure 9, and how does the text explain their sentiment profiles?", "answer": "Terms like 'son' and 'groom' fall into the 'Both' category (purple). The text implies this category represents terms with moderate positive and negative sentiment scores, indicating balanced bias in both directions.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_26", "figure_number": 9, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig25.jpg", "caption": "Figure 9: Examples of targets and the regions they fall under within each category considering sentiment as a measure. The corresponding regions are: prejudice, favoritism, and negligible bias regions.   ", "figure_type": "plot", "evidence_from_figure": "Purple-colored data points for 'son' and 'groom'", "evidence_from_text": "Caption mentioning 'Both' as a distinct sentiment region", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_30_592", "query": "In Figure 11, the box plot for Religion shows a significant disparity in triples between ConceptNet ('bible') and GenericsKB ('quora'). How does this disparity relate to the british-negative sentiment results in Table 11?", "answer": "The representation disparity (higher triples for ConceptNet in Religion) contributes to biased british-negative sentiment results, which Table 11 shows are mitigated by filtering techniques like COMeT_Filtered.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_30", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg", "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "figure_type": "plot", "evidence_from_figure": "Box plot for Religion showing disparity between 'bible' and 'quora'", "evidence_from_text": "Text mentions 'british-negative sentiment' and 'Table 11 contains detailed results for sentiment measures'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2103.11320_2103.11320_fig_30_593", "query": "How does the Origin category's box plot in Figure 11 illustrate representation disparity, and what is its connection to the mitigation framework described in the text?", "answer": "The Origin category box plot shows a larger disparity (higher ConceptNet triples), which the mitigation framework addresses via COMeT_Filtered to reduce biases, as evidenced by improved sentiment results in Table 11.", "doc_id": "2103.11320", "figure_id": "2103.11320_fig_30", "figure_number": 11, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2103.11320/2103.11320/hybrid_auto/images/2103.11320_page0_fig29.jpg", "caption": "Figure 11: Box plots demonstrating the representation disparity in terms of number of triples/sentences for Origin and Religion categories from ConceptNet and GenericsKB.", "figure_type": "plot", "evidence_from_figure": "Origin category box plot showing disparity between datasets", "evidence_from_text": "Text references 'COMeT vs COMet_Filtered comparisons' and 'mitigation framework'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_1_594", "query": "What is the dimensionality of the vector $r$ produced by the Attention layer in Figure 1?", "answer": "The vector $r$ has dimensionality $d^e$, as specified in the text where it states the Attention layer produces a $d^e$-dimensional vector representation.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg", "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.", "figure_type": "diagram", "evidence_from_figure": "The vector $r$ is shown as the output of the Attention layer.", "evidence_from_text": "The text states: 'the attention layer which produces a $d^e$-dimensional vector representation for the sample instance $i$'.", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_1_595", "query": "How does the Attribution framework modify the model structure shown in Figure 1 to assess feature importance?", "answer": "The Attribution framework zeros specific attention weights in the Attention layer (as shown in Figure 1) to evaluate the effect of features on accuracy and fairness, as described in the text.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_1", "figure_number": 1, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig0.jpg", "caption": "Figure 1: (a) In general classification model, for each feature $f _ { k }$ a vector representation $e _ { k }$ of length $d ^ { e }$ is learned. This is passed to the attention layer which produces a $d ^ { e }$ - dimensional vector representation for the sample instance $i$ which is passed to two dense layers to get the final classification output. (b) The Attribution framework has the same architecture as the general model. One outcome is obtained through the original model and another through the model that has some attention weights zeroed. The observed difference in accuracy and fairness measures will indicate the effect of the zeroed out features on accuracy and fairness.", "figure_type": "diagram", "evidence_from_figure": "The Attention layer is the critical component modified in the framework.", "evidence_from_text": "The text explains: 'the model that has some attention weights zeroed' to measure feature effects.", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_3_596", "query": "Why does the point labeled ŷ_z^2 in Figure 2 have a low accuracy value compared to ŷ_o, and what does this indicate about feature f_2?", "answer": "The low accuracy of ŷ_z^2 indicates feature f_2 is critical for model accuracy; removing its attention weights (zeroing out f_2) drastically reduces accuracy as explained in the text.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg", "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "evidence_from_figure": "Position of ŷ_z^2 on the x-axis shows low accuracy", "evidence_from_text": "Text states 'f_2 is correctly attributed to being responsible for accuracy and removing it hurts accuracy drastically'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_3_597", "query": "What does the red color of the point labeled ŷ_o in Figure 2 signify according to the figure caption?", "answer": "The red color signifies ŷ_o represents the original model outcome with all attention weights intact, as defined in the caption.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_3", "figure_number": 2, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig2.jpg", "caption": "Figure 2: Results from the synthetic datasets. Following the $\\hat { y } _ { o }$ and $\\hat { y } _ { z }$ notations, $\\hat { y } _ { o }$ represents the original model outcome with all the attention weights intact, while $\\hat { y } _ { z } ^ { k }$ represents the outcome of the model in which the attention weights corresponding to $k ^ { t h }$ feature are zeroed out (e.g. $\\hat { y } _ { z } ^ { 1 }$ represents when attention weights of feature $f _ { 1 }$ are zeroed out). The results show how the accuracy and fairness of the model (in terms of statistical parity difference) change by exclusion of each feature.", "figure_type": "plot", "evidence_from_figure": "Red color of ŷ_o point", "evidence_from_text": "Caption states 'ŷ_o represents the original model outcome with all attention weights intact'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_5_598", "query": "In Figure 3, which model shows the lowest statistical parity difference at an accuracy of 0.82, and how does this relate to the authors' claim about their approach's flexibility for non-tabular data?", "answer": "The 'Attention (Ours)' model shows the lowest statistical parity difference (~0.05) at 0.82 accuracy. This strong performance on tabular data supports the authors' claim that their approach is flexible for non-tabular data, as it demonstrates effective fairness control in a different modality (text-based biosbias dataset).", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg", "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "evidence_from_figure": "Green star markers (Attention) at x=0.82 with y≈0.05", "evidence_from_text": "Text states approach is flexible for non-tabular data (biosbias dataset)", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_5_599", "query": "What is the statistical parity difference for FCRL at accuracy 0.84 in Figure 3, and how does this compare to the authors' application to the biosbias dataset?", "answer": "FCRL shows a statistical parity difference of ~0.15 at 0.84 accuracy. This high fairness violation contrasts with the authors' successful application of their approach to the text-based biosbias dataset, highlighting the superiority of their method over baseline approaches like FCRL.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_5", "figure_number": 3, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig4.jpg", "caption": "Figure 3: Accuracy vs parity curves for UCI Adult and Heritage Health datasets.", "figure_type": "plot", "evidence_from_figure": "Red square markers (FCRL) at x=0.84 with y≈0.15", "evidence_from_text": "Text describes applying mitigation technique to biosbias dataset", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_7_600", "query": "What specific green-highlighted words in Figure 4's 'Not Debiased Model' column demonstrate the model's focus on gendered terms, and how does this align with the text's explanation of bias in job classification?", "answer": "The 'Not Debiased Model' column highlights 'She' and 'Rebekah' as green words, which are gendered terms. This aligns with the text's claim that the model focuses on gendered words rather than profession-based terms like 'R.N.' for correct job prediction.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg", "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”", "figure_type": "example", "evidence_from_figure": "green-highlighted words 'She' and 'Rebekah' in 'Not Debiased Model' column", "evidence_from_text": "text states 'Not Debiased Model mostly focuses on gendered words...'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_7_601", "query": "How does the Post-Processing (Ours) model's attention on 'R.N.' (highlighted in green) in Figure 4 contribute to its correct prediction of 'nurse' in the job classification task?", "answer": "The green-highlighted 'R.N.' (Registered Nurse) in the Post-Processing (Ours) column reflects the model's focus on profession-based terms, which aligns with the text's explanation that this approach correctly predicts 'nurse' by prioritizing job-relevant credentials over gendered terms.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_7", "figure_number": 4, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig6.jpg", "caption": "Figure 4: Qualitative results from the non-tabular data experiment on the job classification task based on bio texts. Green regions are the top three words used by the model for its prediction based on the attention weights. While the Not Debiased Model mostly focuses on gendered words, our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict “nurse.”", "figure_type": "example", "evidence_from_figure": "green-highlighted 'R.N.' in 'Post-Processing (Ours)' column", "evidence_from_text": "text explains 'our method focused on profession-based words, such as R.N. (Registered Nurse), to correctly predict 'nurse'", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_8_602", "query": "Why does the Attention (Ours) model show lower Equality Opportunity Difference values compared to LAFT and MIFR in Figure 5, especially in the accuracy range of 0.82-0.84?", "answer": "The Attention (Ours) model's lower Equality Opportunity Difference in this range likely stems from its attention mechanism effectively mitigating bias by focusing on relevant words (green regions) in bio texts, as explained in the context of job classification tasks where attention weights prioritize non-sensitive features.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg", "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ", "figure_type": "plot", "evidence_from_figure": "Purple data points (Attention (Ours)) have lower y-values than yellow/green points in 0.82-0.84 accuracy range", "evidence_from_text": "Text describes bio texts as non-tabular data for job classification, with attention weights selecting top words", "requires_figure": true, "requires_text": true}
{"query_id": "l1_2109.03952_2109.03952_fig_8_603", "query": "How do the green regions mentioned in the figure caption relate to the MIFR model's data points in Figure 5?", "answer": "The caption's 'green regions' refer to top words from attention weights (relevant to Attention (Ours)), not MIFR. The green markers for MIFR likely indicate a separate metric, creating a misalignment between the caption's description and the legend's color coding.", "doc_id": "2109.03952", "figure_id": "2109.03952_fig_8", "figure_number": 5, "image_path": "/projects/_hdd/myyyx1/data-process-test/data/mineru_output/2109.03952/2109.03952/hybrid_auto/images/2109.03952_page0_fig7.jpg", "caption": "Figure 5: Additional qualitative results from the non-tabular data experiment on the job classification task based on the bio texts. Green regions represent top three words that the model used for its prediction based on the attention weights.   ", "figure_type": "plot", "evidence_from_figure": "MIFR is labeled green in legend but caption associates green with attention weights", "evidence_from_text": "Text specifies Attention (Ours) uses attention weights for predictions", "requires_figure": true, "requires_text": true}
